
@article{ WOS:000815304400001,
Author = {Kritsis, Kosmas and Kiourt, Chairi and Stamouli, Spyridoula and
   Sevetlidis, Vasileios and Solomou, Alexandra and Karetsos, George and
   Katsouros, Vassilis and Pavlidis, George},
Title = {GRASP-125: A Dataset for Greek Vascular Plant Recognition in Natural
   Environment},
Journal = {SUSTAINABILITY},
Year = {2021},
Volume = {13},
Number = {21},
Month = {NOV},
Abstract = {Plant identification from images has become a rapidly developing
   research field in computer vision and is particularly challenging due to
   the morphological complexity of plants. The availability of large
   databases of plant images, and the research advancements in image
   processing, pattern recognition and machine learning, have resulted in a
   number of remarkably accurate and reliable image-based plant
   identification techniques, overcoming the time and expertise required
   for conventional plant identification, which is feasible only for expert
   botanists. In this paper, we introduce the GReek vAScular Plants (GRASP)
   dataset, a set of images composed of 125 classes of different species,
   for the automatic identification of vascular plants of Greece. In this
   context, we describe the methodology of data acquisition and dataset
   organization, along with the statistical features of the dataset.
   Furthermore, we present results of the application of popular deep
   learning architectures to the classification of the images in the
   dataset. Using transfer learning, we report 91\% top-1 and 98\% top-5
   accuracy.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Kritsis, K; Kiourt, C (Corresponding Author), Athena Res Ctr, Inst Language \& Speech Proc, Athens 15125, Greece.
   Kritsis, Kosmas; Kiourt, Chairi; Stamouli, Spyridoula; Sevetlidis, Vasileios; Katsouros, Vassilis; Pavlidis, George, Athena Res Ctr, Inst Language \& Speech Proc, Athens 15125, Greece.
   Solomou, Alexandra; Karetsos, George, Hellen Agr Org DEMETER, Inst Mediterranean Forest Ecosyst, Athens 11528, Greece.},
DOI = {10.3390/su132111865},
Article-Number = {11865},
EISSN = {2071-1050},
Keywords = {deep learning; image classification; plant identification; transfer
   learning},
Research-Areas = {Science \& Technology - Other Topics; Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Green \& Sustainable Science \& Technology; Environmental Sciences;
   Environmental Studies},
Author-Email = {kosmas.kritsis@athenarc.gr
   chairiq@athenarc.gr
   pstam@athenarc.gr
   vasiseve@athenarc.gr
   solomou@fria.gr
   karetsos@fria.gr
   vsk@athenarc.gr
   gpavlid@athenarc.gr},
Affiliations = {Institute for Language \& Speech Processing (ILSP)},
ResearcherID-Numbers = {Kritsis, Kosmas/AAR-3469-2021
   Pavlidis, George/L-5050-2013
   Kiourt, Chairi/U-8579-2017
   },
ORCID-Numbers = {Kritsis, Kosmas/0000-0003-4513-5040
   Pavlidis, George/0000-0002-9909-1584
   Kiourt, Chairi/0000-0001-8501-8899
   Katsouros, Vassilis/0000-0002-4185-2344
   SOLOMOU, Dr. ALEXANDRA/0000-0002-0014-1909
   Sevetlidis, Vasileios/0000-0001-9348-8786},
Funding-Acknowledgement = {European Union; Greek national funds through the Operational Programme
   ``Competitiveness, Entrepreneurship and Innovation{''}, under the call
   ``RESEARCH-CREATE-INNOVATE{''} {[}T1EDK-03844]},
Funding-Text = {This research has been co-financed by the European Union and Greek
   national funds through the Operational Programme ``Competitiveness,
   Entrepreneurship and Innovation{''}, under the call
   ``RESEARCH-CREATE-INNOVATE{''} (project code: T1EDK-03844, project
   title: AdVENt-Augmented Visitor Experience in National Parks). The
   authors would like also to thank their colleagues at the Institute of
   Mediterranean Forest Ecosystem for their valuable support in collecting,
   organizing and verifying field images from the considered plant species.},
Cited-References = {{[}Anonymous], P COMP VIS PROBL PLA, DOI 10.5244/C.29.CVPPP.1.
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097.
   Dimopoulos P., 2013, Englera, V31.
   Dimopoulos P, 2016, WILLDENOWIA, V46, P301, DOI 10.3372/wi.46.46303.
   Dobrescu A, 2019, IEEE COMPUT SOC CONF, P2600, DOI 10.1109/CVPRW.2019.00316.
   Dobrescu A, 2017, IEEE INT CONF COMP V, P2072, DOI 10.1109/ICCVW.2017.243.
   Fiel S., 2011, P COMP VIS WINT WORK.
   He K, 2015, C COMPUTER VISION PA.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Huang GL, 2017, IEEE ICC.
   Jadhav Sachin B., 2021, International Journal of Information Technology, P2461, DOI 10.1007/s41870-020-00437-5.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Kolhar Shrikrishna, 2023, Information Processing in Agriculture, P114, DOI 10.1016/j.inpa.2021.02.006.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   LEWIS WALTER, 1965, ANN MISSOURI BOT GARD, V52, P217, DOI 10.2307/2394871.
   Nilsback M.-E., 2006, P IEEE COMP SOC C CO, V2, P1447, DOI DOI 10.1109/CVPR.2006.42.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Rzanny M, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0462-4.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Schunck D, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0256340.
   Seeland M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170629.
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Smilkov D., 2017, ARXIV.
   Soderkvist O, 2001, COMPUTER VISION CLAS.
   Solomou A., 2020, P 9 INT C INF COMM T.
   Spaulding D.D., 2013, PHYTONEURON, V83, P1.
   Stefanovic S, 2003, SYST BOT, V28, P791.
   Sulc M, 2015, LECT NOTES COMPUT SC, V8928, P185, DOI 10.1007/978-3-319-16220-1\_14.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Torrey L., 2010, HDB RES MACHINE LEAR, P242, DOI {[}DOI 10.4018/978-1-60566-766-9.CH011, 10.4018/978-1-60566-766-9.ch011].
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wood JRI, 2015, PHYTOKEYS, V51, P1, DOI 10.3897/phytokeys.51.7104.
   Wu Stephen Gang, 2007, 2007 IEEE International Symposium on Signal Processing and Information Technology, P11, DOI 10.1109/ISSPIT.2007.4458016.
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907.},
Number-of-Cited-References = {41},
Times-Cited = {3},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Sustainability},
Doc-Delivery-Number = {2I9QC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000815304400001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000310945500032,
Author = {Valliammal, N. and Geethalakshmi, S. N.},
Editor = {Nagamalai, D and Renault, E and Dhanuskodi, M},
Title = {Leaf and Flower Recognition Using Preferential Image Segmentation
   Algorithm},
Booktitle = {TRENDS IN COMPUTER SCIENCE, ENGINEERING AND INFORMATION TECHNOLOGY},
Series = {Communications in Computer and Information Science},
Year = {2011},
Volume = {204},
Pages = {316-325},
Note = {1st International Conference on Computer Science, Engineering and
   Information Technology (CCSEIT 2011), Tirunelveli, INDIA, SEP 23-25,
   2011},
Organization = {Acad \& Ind Res Collaborat Ctr},
Abstract = {Automatic plant classification systems are essential for a wide range of
   applications including environment protection, plant resource survey, as
   well as for education. With the aid of advanced information technology,
   image processing and machine learning techniques, automatic plant
   identification and classification will enhance such systems with more
   functionality, such as automatic labeling and flexible searching. Image
   segmentation and object recognition are two aspects of digital image
   processing which are being increasingly used in many applications
   including leaf recognition. In this paper, the Preferential Image
   Segmentation (PIS) method is used to segment an object of interest from
   the original image. A probabilistic curve evolution method with particle
   filters is used to measure the similarity between shapes during matching
   process. The experimental results prove that the preferential image
   segmentation can be successfully applied in leaf recognition and
   segmentation from a plant image.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Valliammal, N (Corresponding Author), Avinashilingam Deemed Univ Women, Coimbatore 641043, Tamil Nadu, India.
   Valliammal, N.; Geethalakshmi, S. N., Avinashilingam Deemed Univ Women, Coimbatore 641043, Tamil Nadu, India.},
ISSN = {1865-0929},
EISSN = {1865-0937},
ISBN = {978-3-642-24042-3; 978-3-642-24043-0},
Keywords = {Preferential image segmentation; Tree of shapes; Filters; Segmentation},
Keywords-Plus = {ACTIVE CONTOURS},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \&
   Electronic},
Author-Email = {Valli.p.2008@gmail.com
   sngeethalakshmi@yahoo.com},
Affiliations = {Avinashilingam University for Women},
ResearcherID-Numbers = {, Valliammal.N/K-3807-2019
   },
ORCID-Numbers = {S N, Geethalakshmi/0000-0001-5468-0871},
Cited-References = {Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108.
   Aujol JF, 2003, IEEE T IMAGE PROCESS, V12, P1634, DOI 10.1109/TIP.2003.819309.
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043.
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291.
   Chen LJ, 2008, SIMUL-T SOC MOD SIM, V84, P185, DOI 10.1177/0037549708093374.
   Chen YM, 2002, INT J COMPUT VISION, V50, P315, DOI 10.1023/A:1020878408985.
   Cotton Incorporated USA, 2009, CLASS COTT.
   Cremers D, 2005, LECT NOTES COMPUT SC, V3752, P210.
   Cremers D, 2002, INT J COMPUT VISION, V50, P295, DOI 10.1023/A:1020826424915.
   Farzinfar M, 2010, COMPUT MED IMAG GRAP, V34, P354, DOI 10.1016/j.compmedimag.2009.12.006.
   Freedman D, 2004, IEEE T IMAGE PROCESS, V13, P518, DOI 10.1109/TIP.2003.821445.
   Kumar N, 2004, J BIOSCIENCES, V29, P309, DOI 10.1007/BF02702613.
   National Institute for Agricultural Botany, 2005, CHRYS LEAF CLASS.
   Tzionas P., 2005, 5 INT C ON TECHN AUT, P365.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {15},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BCP31},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000310945500032},
DA = {2023-08-12},
}

@article{ WOS:000579257900004,
Author = {Bisen, Dhananjay},
Title = {Deep convolutional neural network based plant species recognition
   through features of leaf},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2021},
Volume = {80},
Number = {4},
Pages = {6443-6456},
Month = {FEB},
Abstract = {In present scenario, the research under image processing has been
   rapidly transformed from machine learning to deep learning. The deep
   learning algorithms are usually applied in the various areas like images
   to be classified or identified more accurately. One of the application
   areas of deep learning is the plant identification through its leaf
   which helps to recognize plant species. Botanists consume most of time
   in identifying plant species by manually scrutinizing and finding its
   features. This paper proposes an automated plant identification system,
   for identifying the plants species through their leaf. This task is
   accomplished using deep convolutional neural network to achieve higher
   accuracy. Image pre-processing, feature extraction and recognition are
   three main identification steps which are taken under consideration.
   Proposed CNN classifier learns the features of plants such as
   classification of leafs by using hidden layers like convolutional layer,
   max pooling layer, dropout layers and fully connected layers. The model
   acquires a knowledge related to features of Swedish leaf dataset in
   which 15 tree classes are available, that helps to predict the correct
   category of unknown plant with accuracy of 97\% and minimum losses.
   Result is slightly better than the previous work that analyzes 93.75\%
   of accuracy.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Bisen, D (Corresponding Author), Rajkiya Engn Coll Banda, Atarra, UP, India.
   Bisen, Dhananjay, Rajkiya Engn Coll Banda, Atarra, UP, India.},
DOI = {10.1007/s11042-020-10038-w},
EarlyAccessDate = {OCT 2020},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Deep learning; Image pre-processing; Feature extraction; Leaf
   recognition; CNN classifier; Swedish leaf dataset},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {bisen.it2007@gmail.com},
ResearcherID-Numbers = {bisen, dhananjay/ABG-6485-2021},
ORCID-Numbers = {bisen, dhananjay/0000-0003-4165-3959},
Cited-References = {{[}Anonymous], 2020, PYTHON STANDARD LIB.
   Arafat SV, 2016, INT CONF DIGIT INFO, P136, DOI 10.1109/DICTAP.2016.7544015.
   Bambil Deborah, 2020, Environment Systems \& Decisions, V40, P480, DOI 10.1007/s10669-020-09769-w.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Beghin T., 2010, LECT NOTES COMPUTER.
   Chaki J, 2019, OPTIK, V181, P639, DOI 10.1016/j.ijleo.2018.12.107.
   Chollet F., 2018, DEEP LEARNING PYTHON, V190-200, P230, DOI DOI 10.1007/978-1-4842-2766-4.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231.
   Kadir A, 2011, INT J COMPUT TRENDS.
   Kan H. X., 2017, Pattern Recognition and Image Analysis, V27, P581, DOI 10.1134/S105466181703018X.
   Kumar N, 2012, LECT NOTES COMPUTER, V7573.
   Lukic M, 2017, 2017 IEEE 15TH INTERNATIONAL SYMPOSIUM ON APPLIED MACHINE INTELLIGENCE AND INFORMATICS (SAMI), P485, DOI 10.1109/SAMI.2017.7880358.
   Patil J. K., 2016, ENG AGR ENV FOOD, V10, P69, DOI DOI 10.1016/J.EAEF.2016.11.004.
   Kumar JP, 2020, MACH VISION APPL, V31, DOI 10.1007/s00138-019-01056-2.
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Swedish Leaf Dataset, 2020, SWEDISH LEAF DATASET.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.},
Number-of-Cited-References = {20},
Times-Cited = {25},
Usage-Count-Last-180-days = {7},
Usage-Count-Since-2013 = {30},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {QB1CT},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000579257900004},
DA = {2023-08-12},
}

@inproceedings{ WOS:000352485900069,
Author = {Winberg, Simon and Katz, Shaun and Mishra, Amit Kumar},
Editor = {Harit, G and Das, S},
Title = {Fynbos Leaf Online Plant Recognition Application Development and
   evaluation of an image pocessing method for fynbos plants},
Booktitle = {2013 FOURTH NATIONAL CONFERENCE ON COMPUTER VISION, PATTERN RECOGNITION,
   IMAGE PROCESSING AND GRAPHICS (NCVPRIPG)},
Series = {National Conference on Computer Vision Pattern Recognition Image
   Processing and Graphics},
Year = {2013},
Note = {4th National Conference on Computer Vision, Pattern Recognition, Image
   Processing and Graphics (NCVPRIPG), Jodhpur, INDIA, DEC 18-21, 2013},
Organization = {Indian Inst Technol Jodhpur; Indian Unit Pattern Recognit \& Artificial
   Intelligence},
Abstract = {Computer-aided plant identification combines computer vision and pattern
   recognition. The Cape Floristic Kingdom is the most varied of plant
   kingdoms, comprising thousands of species of fynbos plants. While it is
   easier to classify fynbos when they are flowering, mostly flower for
   only a few weeks in a year. This paper concerns an image processing
   application for automatic identification of certain fynbos using leaf
   photographs. The architecture of this application is overviewed prior to
   focusing on the leaf recognition operations, and how these were
   experimentally tested using a series of experiments, culminating in a
   comprehensive test to measure identification accuracy, effectiveness of
   the online user interface, and the processing speed. Our conclusions
   reflect on the overall effectiveness of the application and our plans to
   take it further.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Winberg, S (Corresponding Author), Univ Cape Town, ZA-7925 Cape Town, South Africa.
   Winberg, Simon; Katz, Shaun; Mishra, Amit Kumar, Univ Cape Town, ZA-7925 Cape Town, South Africa.},
ISSN = {2372-658X},
ISBN = {978-1-4799-1588-0},
Keywords = {image procesing; plant recognition; computer vision; leaf database;
   fynbos; digital morphological features},
Research-Areas = {Computer Science; Engineering; Imaging Science \& Photographic
   Technology},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic; Imaging Science \& Photographic Technology},
Author-Email = {simon.winberg@uct.ac.za
   shaun.katz@uct.ac.za
   amit.mishra@uct.ac.za},
Affiliations = {University of Cape Town},
Cited-References = {Abdulrahaman A., 2010, J HORTICULTURE FORES, V2, P112.
   BROWN NAC, 1993, NEW PHYTOL, V123, P575, DOI 10.1111/j.1469-8137.1993.tb03770.x.
   Cowling RM, 1996, TRENDS ECOL EVOL, V11, P362, DOI 10.1016/0169-5347(96)10044-6.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Fraser M., 2004, FYNBOS YEAR.
   Hansen Jesper Schmidt, 2011, GNU OCTAVE BEGINNERS.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Magoro MD, 2010, INT J AFR RENAISS ST, V5, P229, DOI 10.1080/18186874.2010.534842.
   OPLIN, 2013, WHAT TREE IS IT ID L.
   Otsu N., 1988, Proceedings of IAPR Workshop on Computer Vision: Special Hardware and Industrial Applications, P431.
   Paterson-Jones C., 2007, FIELD GUIDE FYNBOS.
   Shrestha B., 2010, CLASSIFICATION PLANT.
   Van Wyk B., 2000, PHOTOGUIDE WILD FLOW.
   Williams M.D., 2007, IDENTIFYING TREES AL.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {15},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BC4GJ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000352485900069},
DA = {2023-08-12},
}

@article{ WOS:000666845200001,
Author = {Yang, Xin and Ni, Haiming and Li, Jingkui and Lv, Jialuo and Mu, Hongbo
   and Qi, Dawei},
Title = {Leaf recognition using BP-RBF hybrid neural network},
Journal = {JOURNAL OF FORESTRY RESEARCH},
Year = {2022},
Volume = {33},
Number = {2},
Pages = {579-589},
Month = {APR},
Abstract = {Plant recognition has great potential in forestry research and
   management. A new method combined back propagation neural network and
   radial basis function neural network to identify tree species using a
   few features and samples. The process was carried out in three steps:
   image pretreatment, feature extraction, and leaf recognition. In the
   image pretreatment processing, an image segmentation method based on
   hue, saturation and value color space and connected component labeling
   was presented, which can obtain the complete leaf image without veins
   and background. The BP-RBF hybrid neural network was used to test the
   influence of shape and texture on species recognition. The recognition
   accuracy of different classifiers was used to compare classification
   performance. The accuracy of the BP-RBF hybrid neural network using nine
   dimensional features was 96.2\%, highest among all the classifiers.},
Publisher = {NORTHEAST FORESTRY UNIV},
Address = {NO 26 HEXING RD, XIANGFANG DISTRICT, HARBIN, 150040, PEOPLES R CHINA},
Type = {Article},
Language = {English},
Affiliation = {Mu, HB; Qi, DW (Corresponding Author), Northeast Forestry Univ, Coll Sci, Harbin 150040, Peoples R China.
   Yang, Xin; Ni, Haiming; Li, Jingkui; Lv, Jialuo; Mu, Hongbo; Qi, Dawei, Northeast Forestry Univ, Coll Sci, Harbin 150040, Peoples R China.},
DOI = {10.1007/s11676-021-01362-4},
EarlyAccessDate = {JUN 2021},
ISSN = {1007-662X},
EISSN = {1993-0607},
Keywords = {Leaf recognition; BP-RBF neural network; Image processing; Feature
   extraction; Machine learning},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Forestry},
Web-of-Science-Categories  = {Forestry},
Author-Email = {mhb506@nefu.edu.cn
   qidw9806@126.com},
Affiliations = {Northeast Forestry University - China},
ResearcherID-Numbers = {Mu, Hongbo/ABD-5224-2021},
Funding-Acknowledgement = {Fundamental Research Funds for the Central Universities {[}2572020BC07];
   Project of National Science Foundation of China {[}31570712]},
Funding-Text = {This work is supported by the Fundamental Research Funds for the Central
   Universities (No.2572020BC07) and the Project of National Science
   Foundation of China (No.31570712).},
Cited-References = {Ahmed A, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0237645.
   {[}Anonymous], 2017, P 2017 25 TELECOMMUN, DOI {[}10.1109/ICISC.2017.8068597, DOI 10.1109/ICISC.2017.8068597].
   Backes AR, 2009, PATTERN RECOGN, V42, P54, DOI 10.1016/j.patcog.2008.07.006.
   Chang ZY, 2018, J FORESTRY RES, V29, P1789, DOI 10.1007/s11676-017-0572-7.
   Crall AW, 2011, CONSERV LETT, V4, P433, DOI 10.1111/j.1755-263X.2011.00196.x.
   Espinosa D, 2006, J BIOGEOGR, V33, P1945, DOI 10.1111/j.1365-2699.2006.01566.x.
   Gong D.X., 2014, COMPUTER MODERNIZATI, V4, P12.
   Gray AN, 2005, ECOL INDIC, V5, P57, DOI 10.1016/j.ecolind.2004.09.001.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   MERRILL EK, 1978, BOT GAZ, V139, P447, DOI 10.1086/337020.
   MUHAMMAD AFA, 2019, COMPUTERS, V8, P77, DOI DOI 10.3390/COMPUTERS8040077.
   Nelson JDB, 2008, NEUROCOMPUTING, V72, P15, DOI 10.1016/j.neucom.2008.01.034.
   Nevalainen O, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9030185.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205.
   Rahman MM, 2019, SN APPL SCI, V1, DOI 10.1007/s42452-019-1682-y.
   Roy K, 2010, INT J PATTERN RECOGN, V24, P1209, DOI 10.1142/S0218001410008421.
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0.
   Sajo MG, 2002, BOT J LINN SOC, V138, P339, DOI 10.1046/j.1095-8339.2002.00025.x.
   Tarjoman Mana, 2013, Journal of Medical Engineering \& Technology, V37, P43, DOI 10.3109/03091902.2012.742157.
   Turkoglu M, 2019, APPL MATH COMPUT, V352, P1, DOI 10.1016/j.amc.2019.01.054.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xu ZH, 2020, J FORESTRY RES, V31, P107, DOI 10.1007/s11676-018-0832-1.
   Yang R, 2020, J APPL SPECTROSC+, V87, P184, DOI 10.1007/s10812-020-00981-9.
   Yu HL, 2019, J FORESTRY RES, V30, P2379, DOI 10.1007/s11676-018-00874-w.
   Zhang JW, 2018, J FORESTRY RES, V29, P557, DOI 10.1007/s11676-017-0448-x.
   Zhixuan Tang, 2020, Journal of Physics: Conference Series, V1592, DOI 10.1088/1742-6596/1592/1/012061.},
Number-of-Cited-References = {31},
Times-Cited = {1},
Usage-Count-Last-180-days = {8},
Usage-Count-Since-2013 = {44},
Journal-ISO = {J. For. Res.},
Doc-Delivery-Number = {0D3SS},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000666845200001},
OA = {hybrid},
DA = {2023-08-12},
}

@inproceedings{ WOS:000431664000058,
Author = {Imah, E. M. and Rahayu, Y. S. and Wintarti, A.},
Editor = {Abdullah, AG and Nandiyanto, ABD and Widiaty, I},
Title = {Plant Leaf Recognition Using Competitive Based Learning Algorithm},
Booktitle = {2ND ANNUAL APPLIED SCIENCE AND ENGINEERING CONFERENCE (AASEC 2017)},
Series = {IOP Conference Series-Materials Science and Engineering},
Year = {2018},
Volume = {288},
Note = {2nd Annual Applied Science and Engineering Conference (AASEC), Univ
   Pendidikan Indonesia, UPI Publicat Ctr, Bandung, INDONESIA, AUG 24, 2017},
Organization = {Univ Sumatera Utara; Univ Negeri Manado; Univ Negeri Surabaya; Univ
   Muhamadiyah Purwokerto},
Abstract = {Plant recognition based on digital leaf image has received as particular
   attention in computer vision and intelligence system, due its important
   implication in automatic plant identification. Plant species have the
   unique leaf characteristics such as the shape, texture, margin, and
   colour, which different each other. This study presents a novel method
   for automation plant recognition using Generalized Relevance Learning
   Vector Quantization (GRLVQ). GRLVQ is a competitive based learning
   algorithm which is integrating features extraction and classification
   phases. The experimental result shows that GRLVQ has better performance
   than the predecessor algorithm.},
Publisher = {IOP PUBLISHING LTD},
Address = {DIRAC HOUSE, TEMPLE BACK, BRISTOL BS1 6BE, ENGLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Imah, EM (Corresponding Author), Univ Negeri Surabaya, Math Dept, Surabaya, East Java, Indonesia.
   Imah, E. M.; Wintarti, A., Univ Negeri Surabaya, Math Dept, Surabaya, East Java, Indonesia.
   Rahayu, Y. S., Univ Negeri Surabaya, Biol Dept, Surabaya, East Java, Indonesia.},
DOI = {10.1088/1757-899X/288/1/012058},
Article-Number = {012058},
ISSN = {1757-8981},
Keywords-Plus = {SHAPE},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {ellymatul@unesa.ac.id},
Affiliations = {Universitas Negeri Surabaya; Universitas Negeri Surabaya},
ResearcherID-Numbers = {Imah, Elly Matul/AAE-7723-2021
   Rahayu, Yuni Sri/ABJ-9886-2022
   Imah, Elly Matul/D-1258-2015
   },
ORCID-Numbers = {Imah, Elly Matul/0000-0003-1008-4837
   Rahayu, Yuni Sri/0000-0002-2977-1162},
Cited-References = {Ahmed N, 2016, SCI INT, V28, DOI DOI 10.9790/0661-17134853.
   Belgacem N., 2012, INT J CRYPTOGR INF S, V2, P1, DOI DOI 10.5121/IJCIS.2012.2201.
   Cataron A, 2004, IEEE IJCNN, P1421.
   Chaki J, 2016, SMART INNOV SYST TEC, V43, P37, DOI 10.1007/978-81-322-2538-6\_5.
   Du JX, 2013, NEUROCOMPUTING, V116, P150, DOI 10.1016/j.neucom.2012.03.028.
   Hasim A, 2016, IOP C SER EARTH ENV.
   James O C, 2013, SIGNAL PROCESS PATTE, P3.
   Kadir A, 2013, NEURAL NETWORK APPL.
   Kastner M, 2012, NEUROCOMPUTING, P1.
   Lakshmi BV, 2017, J MOD APPL STAT METH, V16, P461, DOI 10.22237/jmasm/1493598420.
   Sato A, 1996, ADV NEUR IN, V8, P423.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   VijayaLakshmi B, 2016, COMPUT ELECTRON AGR, V125, P99, DOI 10.1016/j.compag.2016.04.033.
   Wang X, 2014, DIGIT SIGNAL PROCESS, V34, P101, DOI 10.1016/j.dsp.2014.08.005.
   Wang ZB, 2017, ARCH COMPUT METHOD E, V24, P637, DOI 10.1007/s11831-016-9181-4.},
Number-of-Cited-References = {15},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BK1AP},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000431664000058},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000576767300052,
Author = {Wu, Huisi and Xiang, Yongkui and Liu, Jingjing and Wen, Zhenkun},
Editor = {Liu, D and Xie, S and Li, Y and Zhao, D and ElAlfy, ESM},
Title = {Automatic Leaf Recognition Based on Deep Convolutional Networks},
Booktitle = {NEURAL INFORMATION PROCESSING (ICONIP 2017), PT III},
Series = {Lecture Notes in Computer Science},
Year = {2017},
Volume = {10636},
Number = {III},
Pages = {505-515},
Note = {24th International Conference on Neural Information Processing (ICONIP),
   Guangzhou, PEOPLES R CHINA, NOV 14-18, 2017},
Organization = {Chinese Acad Sci, Inst Automat; Guangdong Univ Technol; S China Univ
   Technol; Springers Lecture Notes Comp Sci; IEEE CAA Journal Automatica
   Sinica; Asia Pacific Neural Network Soc},
Abstract = {Leaf recognition remains a hot research topic receiving intensive
   attention in computer vision. In this paper, we propose deep
   convolutional networks with deep learning framework on the large scale
   of leaf databases. Different from the existing leaf recognition
   algorithms that mainly depend on traditional feature extractions and
   pattern matching operations, our method can achieve automatic leaf
   recognition based on deep convolutional networks without any explicit
   feature extraction or matching. Because it does not require any feature
   detection and selection, the advantages of our framework are obvious,
   especially for the large scale leaf databases. Specifically, we design
   deep convolutional networks structure and adopt fine-tuning strategy for
   our network initialization. In addition, we also develop a
   visualization-guided parameter tuning scheme to guarantee the accuracy
   of our deep learning framework. Our method is evaluated on several
   different databases with different scales. Comparison experiments are
   performed and demonstrate that the accuracy of our method outperforms
   traditional methods.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Wen, ZK (Corresponding Author), Shenzhen Univ, Coll Comp Sci \& Software Engn, Shenzhen, Peoples R China.
   Wu, Huisi; Xiang, Yongkui; Liu, Jingjing; Wen, Zhenkun, Shenzhen Univ, Coll Comp Sci \& Software Engn, Shenzhen, Peoples R China.},
DOI = {10.1007/978-3-319-70090-8\_52},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-70090-8; 978-3-319-70089-2},
Keywords = {Leaf recognition; Deep convolutional networks; Learning feature
   visualization},
Keywords-Plus = {SCALE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {wenzk@szu.edu.cn},
Affiliations = {Shenzhen University},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}61303101, 61572328];
   Shenzhen Research Foundation for Basic Research, China
   {[}JCYJ20150324140036846, JCYJ20170302153551588, CXZZ20140902 160818443,
   CXZZ20140902102350474, CXZZ20150813151056544, JCYJ20150630105452814,
   JCYJ20160331114551175, JCYJ20160608173051207]; Start-up Research Fund of
   Shenzhen University {[}2013-827-000009]; China-UK Visual Information
   Processing Laboratory (VIPL); Maternal and child health monitoring and
   early warning Engineering Technology Research Center (METRC) of
   Guangdong Province},
Funding-Text = {This work was supported in part by grants from the National Natural
   Science Foundation of China (Nos. 61303101, 61572328), the Shenzhen
   Research Foundation for Basic Research, China (Nos.
   JCYJ20150324140036846, JCYJ20170302153551588, CXZZ20140902 160818443,
   CXZZ20140902102350474, CXZZ20150813151056544, JCYJ20150630105452814,
   JCYJ20160331114551175, JCYJ20160608173051207), the Start-up Research
   Fund of Shenzhen University (Nos. 2013-827-000009), the China-UK Visual
   Information Processing Laboratory (VIPL) and Maternal and child health
   monitoring and early warning Engineering Technology Research Center
   (METRC) of Guangdong Province.},
Cited-References = {Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199.
   Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761.
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126.
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Reyes A.K., 2015, C LABS EV FOR SEPT, V1391, P9.
   Sainath TN, 2015, NEURAL NETWORKS, V64, P39, DOI 10.1016/j.neunet.2014.08.005.
   Tuama A, 2016, IEEE INT WORKS INFOR.
   Tymoshenko Kateryna, 2016, P 2016 C N AM CHAPT, P1268, DOI 10.18653/v1/N16-1152.
   Wang XF, 2005, LECT NOTES COMPUT SC, V3644, P87.
   Wu HS, 2014, ADV INTELL SYST, V277, P145, DOI 10.1007/978-3-642-54924-3\_14.
   ZAGORUYKO S, 2015, PROC CVPR IEEE, P4353, DOI DOI 10.1109/CVPR.2015.7299064.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474.
   Zhang L., 2012, 2012 19 IEEE INT C I.},
Number-of-Cited-References = {15},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BQ1LJ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000576767300052},
DA = {2023-08-12},
}

@inproceedings{ WOS:000458759100069,
Author = {Ali, Redha and Hardie, Russell and Essa, Almabrok},
Book-Group-Author = {IEEE},
Title = {A Leaf Recognition Approach to Plant Classification Using Machine
   Learning},
Booktitle = {NAECON 2018 - IEEE NATIONAL AEROSPACE AND ELECTRONICS CONFERENCE},
Series = {IEEE National Aerospace and Electronics Conference},
Year = {2018},
Pages = {431-434},
Note = {IEEE National Aerospace and Electronics Conference (NAECON), Dayton, OH,
   JUL 23-26, 2018},
Organization = {Universal Technol Corp; QUANSER; HARTMAAN Elect; ORION; OPAL RT Technol;
   IEEE Dayton Sect; Univ Dayton},
Abstract = {The identification of plants is a very important component of workflows
   in plant ecological research. This paper presents an automated leaf
   recognition method for plant identification. The proposed technique is
   simple and computationally efficient. It is based on a combination of
   two types of texture features, named Bag-of-features (BOF) and Local
   Binary Pattern (LBP). These features are utilized as inputs to a
   decision-making model that is based on a multiclass Support Vector
   Machine (SVM) classifier. The introduced method is evaluated on a
   publicly available leaf image database. The experimental results
   demonstrate that our proposed method is the highly efficient technique
   for plant recognition.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Ali, R (Corresponding Author), Univ Dayton, Dept Elect \& Comp Engn, 300 Coll Pk, Dayton, OH 45469 USA.
   Ali, Redha; Hardie, Russell, Univ Dayton, Dept Elect \& Comp Engn, 300 Coll Pk, Dayton, OH 45469 USA.
   Essa, Almabrok, Cleveland State Univ, Dept Elect Engn \& Comp Sci, 2121 Euclid Ave, Cleveland, OH 44115 USA.},
ISSN = {0547-3578},
ISBN = {978-1-5386-6557-2},
Keywords = {component; formatting; style; styling; insert},
Keywords-Plus = {SHAPE},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Aerospace; Engineering, Electrical \& Electronic},
Author-Email = {almahdir1@udayton.edu
   rhardie1@udayton.edu
   a.essa@csuohio.edu},
Affiliations = {University of Dayton; University System of Ohio; Cleveland State
   University},
ResearcherID-Numbers = {Ali, Redha/AAU-2317-2020},
ORCID-Numbers = {Ali, Redha/0000-0002-0379-7313},
Cited-References = {Aldaouab I, 2017, 2017 IEEE GREEN ENERGY AND SMART SYSTEMS CONFERENCE (IGESSC).
   Aldaouab I, 2018, 2018 IEEE TEXAS POWER AND ENERGY CONFERENCE (TPEC).
   Aldaouab I, 2017, 2017 IEEE TEXAS POWER AND ENERGY CONFERENCE (TPEC).
   Aldaouab Ibrahim, 2017, EN SUST C IESC 2017.
   Ali RA, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0177-2.
   Almahdi R, 2016, PROC NAECON IEEE NAT, P318, DOI 10.1109/NAECON.2016.7856822.
   {[}Anonymous], 2006, IEEE COMPUTER SOC C.
   BAY H, 2008, EUR C COMP VIS, V110, P346, DOI DOI 10.1016/J.CVIU.2007.09.014.
   Bosch A, 2007, IMAGE VISION COMPUT, V25, P778, DOI 10.1016/j.imavis.2006.07.015.
   Harris C. G., 1988, P ALVEY VISION C 198, V15, P10.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Kadir Abdul, 2012, Journal of Theoretical and Applied Information Technology, V41, P82.
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855.
   Khairnar K, 2014, INT J COMPUT APPL, V108, P36, DOI {[}DOI 10.5120/18973-0445, 10.5120/18973-0445].
   Lei YK, 2014, COMPUT VIS IMAGE UND, V119, P116, DOI 10.1016/j.cviu.2013.12.001.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x.
   Narayanan BN, 2016, PROC NAECON IEEE NAT, P338, DOI 10.1109/NAECON.2016.7856826.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490.
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
   Pedersen J. T, 2011, STUDY GROUP SURF FEA.
   Sakai N, 1996, J FOOD ENG, V27, P397, DOI 10.1016/0260-8774(95)00022-4.
   Sekeroglu B, 2016, PROCEDIA COMPUT SCI, V102, P578, DOI 10.1016/j.procs.2016.09.445.
   Soderkvist O., 2001, THESIS.
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018.
   Wang ZB, 2018, NEURAL PROCESS LETT, V47, P99, DOI 10.1007/s11063-017-9635-1.
   Wang ZB, 2016, ARCH COMPUT METHOD E, V23, P659, DOI 10.1007/s11831-015-9154-z.
   Zhang SW, 2013, PATTERN RECOGN, V46, P1891, DOI 10.1016/j.patcog.2013.01.015.},
Number-of-Cited-References = {30},
Times-Cited = {6},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BM0GZ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000458759100069},
DA = {2023-08-12},
}

@article{ WOS:000769842500001,
Author = {Ghosh, Aditi and Roy, Parthajit},
Title = {An automated model for leaf image-based plant recognition: an optimal
   feature-based machine learning approach},
Journal = {INNOVATIONS IN SYSTEMS AND SOFTWARE ENGINEERING},
Year = {2022},
Month = {2022 MAR 16},
Abstract = {Present study focuses on identifying plant species automatically from
   leaf images using optimal feature set. The need of classifying a plant
   species is increasing because of its numerous application domains. In
   this study an automatic leaf recognition model has been introduced based
   on image processing, pattern analysis and Machine Learning (ML)
   technology. The main focus of this study is to find out optimal feature
   set with higher accuracy. Two most popular datasets Flavia and Swedish
   have been taken for classification. Different types of features like
   Gray Level Co-occurrence Matrix (GLCM), Local Binary Pattern (LBP), Hu
   Invariant Moment have been used in different combinations to achieve
   higher accuracy. Accuracy obtained from different feature combinations
   have been given in the result section elaborately. Using this optimal
   feature set result obtained from different classifiers have also been
   given. In our previous research work the accuracy was 93.98 and 94.66\%,
   respectively, on this two dataset. A significant improvement has been
   achieved in accuracy in this present study.},
Publisher = {SPRINGER LONDON LTD},
Address = {236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND},
Type = {Article; Early Access},
Language = {English},
Affiliation = {Roy, P (Corresponding Author), Univ Burdwan, Dept Comp Sci, Purba Bardhaman 713104, W Bengal, India.
   Ghosh, Aditi; Roy, Parthajit, Univ Burdwan, Dept Comp Sci, Purba Bardhaman 713104, W Bengal, India.},
DOI = {10.1007/s11334-022-00440-y},
EarlyAccessDate = {MAR 2022},
ISSN = {1614-5046},
EISSN = {1614-5054},
Keywords = {Machine Learning; Artificial Neural Network; Deep Learning; Image
   Processing; Local Binary Pattern},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Software Engineering},
Author-Email = {aditi\_ghosh9@yahoo.com
   roy.parthajit@gmail.com},
Affiliations = {University of Burdwan},
ResearcherID-Numbers = {Roy, Parthajit/GRS-4111-2022},
Cited-References = {Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010.
   Gedraite ES, 2011, ELMAR PROC, P393.
   Ghosh A., 2021, SPRINGER BOOK SERIES, P243.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Hossain J., 2010, P 2010 13 INT C COMP.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Huang D, 2011, IEEE T SYST MAN CY C, V41, P765, DOI 10.1109/TSMCC.2011.2118750.
   Indolia Sakshi, 2018, Procedia Computer Science, V132, P679, DOI 10.1016/j.procs.2018.05.069.
   Iqbaldeep K., 2016, INT J COMP SCI TECHN, V7, P36.
   Khan S., 2020, ADV COMPUT TECHNOL A, P187.
   Klimek R, 2010, COMPUT SCI-AGH, V11, P115.
   Lavania S., 2015, 2014 IEEE INT C COMP.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Marcondes Francisco S., 2011, INNOV SYST SOFTW ENG.
   Mrazova I, 2012, PROCEDIA COMPUT SCI, V12, P194, DOI 10.1016/j.procs.2012.09.053.
   O'Shea K., 2015, INTRO CONVOLUTIONAL.
   Prasad S, 2013, 2013 IEEE SECOND INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P405, DOI 10.1109/ICIIP.2013.6707624.
   Ren XM, 2012, LECT NOTES ARTIF INT, V7390, P237, DOI 10.1007/978-3-642-31576-3\_31.
   Siravenha AC, 2016, 2016 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P45.
   Soderkvist O., 2001, THESIS.
   Szymkowski M, 2021, INNOV SYST SOFTW ENG, V17, P309, DOI 10.1007/s11334-021-00392-9.
   Bao TQ, 2020, J INFORM TELECOMMUN, V4, P140, DOI 10.1080/24751839.2019.1666625.
   Van Hieu N, 2020, ARXIVABS200502832.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang ZB, 2017, ARCH COMPUT METHOD E, V24, P637, DOI 10.1007/s11831-016-9181-4.
   Waykar, 2013, STUDY IMPORTANCE UML.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yamashita R, 2018, INSIGHTS IMAGING, V9, P611, DOI 10.1007/s13244-018-0639-9.},
Number-of-Cited-References = {30},
Times-Cited = {2},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Innov. Syst. Softw. Eng.},
Doc-Delivery-Number = {ZU4VY},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000769842500001},
DA = {2023-08-12},
}

@article{ WOS:000996490100002,
Author = {Ghosh, Sukanta and Singh, Amar and Kumar, Shakti},
Title = {PB3C-CNN: An integrated Parallel Big Bang-Big Crunch and CNN based
   approach for plant leaf classification},
Journal = {INTELIGENCIA ARTIFICIAL-IBEROAMERICAL JOURNAL OF ARTIFICIAL INTELLIGENCE},
Year = {2023},
Volume = {26},
Number = {72},
Pages = {15-29},
Month = {DEC},
Abstract = {Plant identification and classification are critical to understand,
   protect, and conserve biodiversity. Traditional plant classification
   requires years of intensive training and experience, making it difficult
   for others to classify plants. Plant leaf classification is a
   challenging issue as similar features appear in different plant species.
   With the development of automated image-based classification, machine
   learning (ML) is becoming very popular. Deep learning (DL) methods have
   significantly improved plant image identification and classification. In
   the last decade, convolutional neural networks (CNN) have entirely
   dominated the field of computer vision, showing outstanding feature
   extraction capabilities and significant identification and
   classification performance. The capability of CNN lies in its network.
   The primary strategy to continue this trend in the literature relies on
   further scaling networks in size. However due to increase in network
   size, costs increase rapidly, while performance improvements may be
   marginal. Hence, there is a need to optimize the CNN network to get the
   desired result with optimal size of machine learning model. This paper
   proposes a parallel big bang-big crunch (PB3C) based approach to
   automatically evolve the architecture of CNN. The proposed approach is
   validated on plant leaf classification application and compared with
   other existing machine learning-based approaches. From the comparision
   results we observed that the obtained it was found that the proposed
   approach was able to outperforms all the 11 existing state-of-the-art
   techniques.},
Publisher = {ASOC ESPANOLA INTELIGENCIA ARTIFICIAL},
Address = {FAC INFORMATICA, UNIV POLITECNICA VALENCIA, VALENCIA, SPAIN},
Type = {Article},
Language = {English},
Affiliation = {Singh, A (Corresponding Author), Lovely Profess Univ, Sch Comp Applicat, Phagwara, Punjab, India.
   Ghosh, Sukanta; Singh, Amar, Lovely Profess Univ, Sch Comp Applicat, Phagwara, Punjab, India.
   Kumar, Shakti, Panipat Inst Engn \& Technol, Off Director, Panipat, Haryana, India.},
DOI = {10.4114/intartif.vol26iss72pp15-29},
ISSN = {1137-3601},
EISSN = {1988-3064},
Keywords = {Image Classification; Plant Protection; Machine Learning; Deep Learning;
   Nature-Inspired Computing},
Keywords-Plus = {NEURAL-NETWORK},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {sukantaghoshmca@hotmail.com
   amar.23318@lpu.co.in
   shaktik@gmail.com},
Affiliations = {Lovely Professional University; Panipat Institute of Engineering \&
   Technology},
Cited-References = {Alhudhaif A, 2022, COMPUT SYST SCI ENG, V40, P223, DOI 10.32604/csse.2022.018430.
   Alotaibi A, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12101705.
   Asharf J, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9071177.
   Atila U, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2020.101182.
   Chan KY, 2022, NEURAL COMPUT APPL, V34, P15409, DOI 10.1007/s00521-022-07218-0.
   Charitha P. L., 2021, LECT NOTES NETWORKS, P523, DOI {[}10.1007/978-3-030-84760-9\_44, DOI 10.1007/978-3-030-84760-9\_44].
   Chen FC, 2018, IEEE T IND ELECTRON, V65, P4392, DOI 10.1109/TIE.2017.2764844.
   Cheng-Li Zhou, 2021, Journal of Physics: Conference Series, V1873, DOI 10.1088/1742-6596/1873/1/012002.
   Chouhan S. S., 2020, DATABASE LEAF IMAGES.
   Darwish A, 2020, SWARM EVOL COMPUT, V52, DOI 10.1016/j.swevo.2019.100616.
   Erol OK, 2006, ADV ENG SOFTW, V37, P106, DOI 10.1016/j.advengsoft.2005.04.005.
   Ghamisi P, 2016, IEEE GEOSCI REMOTE S, V13, P1537, DOI 10.1109/LGRS.2016.2595108.
   Ghosh A, 2022, INNOV SYST SOFTW ENG, DOI 10.1007/s11334-022-00440-y.
   Ghosh Sukanta, 2022, Emergent Converging Technologies and Biomedical Systems: Select Proceedings of ETBS 2021. Lecture Notes in Electrical Engineering (841), P133, DOI 10.1007/978-981-16-8774-7\_12.
   Ghosh Sukanta, 2020, Journal of Physics: Conference Series, V1531, DOI 10.1088/1742-6596/1531/1/012045.
   Ghosh S, 2022, CMC-COMPUT MATER CON, V71, P4257, DOI 10.32604/cmc.2022.023414.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Jin XJ, 2021, IEEE ACCESS, V9, P10940, DOI 10.1109/ACCESS.2021.3050296.
   Kanda PS, 2021, IEEE ACCESS, V9, P162590, DOI 10.1109/ACCESS.2021.3131726.
   Kumar R, 2021, INT J PATTERN RECOGN, V35, DOI 10.1142/S0218001421570044.
   Kumar S, 2018, WIRELESS PERS COMMUN, V100, P1601, DOI 10.1007/s11277-018-5656-y.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Liu SJ, 2021, IEEE T GEOSCI REMOTE, V59, P5085, DOI 10.1109/TGRS.2020.3018879.
   Liu Y., 2021, 2021 IEEE 21 INT C C, DOI {[}10.1109/icct52962.2021.9658028, DOI 10.1109/ICCT52962.2021.9658028].
   Mohakud R, 2022, J KING SAUD UNIV-COM, V34, P6280, DOI 10.1016/j.jksuci.2021.05.012.
   Oo YM., 2018, INT J RES ENG, V5, P516, DOI {[}DOI 10.21276/IJRE.2018.5.9.4, 10.21276/ijre.2018.5.9.4].
   Partel V, 2019, COMPUT ELECTRON AGR, V157, P339, DOI 10.1016/j.compag.2018.12.048.
   Quach BM, 2023, MULTIMED TOOLS APPL, V82, P777, DOI 10.1007/s11042-022-13199-y.
   Seward C., 2018, ARXIV.
   Singh P, 2021, SWARM EVOL COMPUT, V63, DOI 10.1016/j.swevo.2021.100863.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Wang P, 2021, FRONT PLANT SCI, V12, DOI 10.3389/fpls.2021.723294.
   Xiong JB, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10010081.},
Number-of-Cited-References = {34},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Inteligencia Artif.},
Doc-Delivery-Number = {H5PW1},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000996490100002},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000839140800002,
Author = {Roslan, Noor Aini Mohd and Diah, Norizan Mat and Ibrahim, Zaidah and
   Hanum, Haslizatul Mohamed and Ismail, Marina},
Book-Group-Author = {IEEE},
Title = {Automatic Plant Recognition: A Survey of Relevant Algorithms},
Booktitle = {2022 IEEE 18TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING \&
   APPLICATIONS (CSPA 2022)},
Year = {2022},
Pages = {5-9},
Note = {18th IEEE International Colloquium on Signal Processing and Applications
   (CSPA), Selangor, MALAYSIA, MAY 12, 2022},
Organization = {IEEE; Univ Teknologi Mara; IEEE Control Syst Soc; IEEE Control Syst Soc
   Malaysia Chapter},
Abstract = {Plants are one of the most important elements since they provide oxygen,
   which is necessary for human survival. Plant recognition applications
   have been widely developed, and these applications can help botanists
   tackle various real-world problems. This paper reviews machine learning
   and deep learning algorithms discussed for plant recognition. Different
   algorithms used for plant identification and recognition research
   between the year 2007 until the year 2020 are reviewed. The main
   algorithms discussed are Convolutional Neural Network (CNN), Support
   Vector Machine (SVM), Artificial Neural Network (ANN), and K-Nearest
   Neighbours (KNN). This paper also compares the performance between
   selected algorithms and proposes the best technique from the research
   outcomes.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Roslan, NAM (Corresponding Author), Univ Teknol MARA, Fac Comp \& Math Sci, Shah Alam 40450, Selangor, Malaysia.
   Roslan, Noor Aini Mohd; Diah, Norizan Mat; Ibrahim, Zaidah; Hanum, Haslizatul Mohamed; Ismail, Marina, Univ Teknol MARA, Fac Comp \& Math Sci, Shah Alam 40450, Selangor, Malaysia.},
DOI = {10.1109/CSPA55076.2022.9782022},
ISBN = {978-1-6654-8529-6},
Keywords = {Automatic plant detection; deep learning; machine learning; plant
   recognition; ANN; CNN; KNN; SVM},
Keywords-Plus = {NEURAL-NETWORK},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Software Engineering; Engineering, Electrical \&
   Electronic},
Author-Email = {noorainimohdroslan@gmail.com
   norizan@fskm.uitm.edu.my
   zaidah@fskm.uitm.edu.my
   fairuz@fskm.uitm.edu.my
   marina@fskm.uitm.edu.my},
Affiliations = {Universiti Teknologi MARA},
ResearcherID-Numbers = {Diah, Norizan Mat/AAL-5558-2020},
ORCID-Numbers = {Diah, Norizan Mat/0000-0002-3785-948X},
Funding-Acknowledgement = {Ministry of Education (MOE) Malaysia under FRGS-RACER Research Grant at
   Universiti Teknologi MARA, Shah Alam {[}600-IRMI/FRGS-RACER 5/3
   (082/2019)]},
Funding-Text = {The present research is funded by the Ministry of Education (MOE)
   Malaysia under FRGS-RACER Research Grant at Universiti Teknologi MARA,
   Shah Alam (600IRMI/FRGS-RACER 5/3 (082/2019).},
Cited-References = {Al-Hiary H., 2011, INT J COMPUT APPL, V17, P31, DOI DOI 10.5120/2183-2754.
   Alom MZ, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8030292.
   Azlah MAF, 2019, COMPUTERS, V8, DOI 10.3390/computers8040077.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Cap HQ, 2018, 2018 IEEE 14TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING \& ITS APPLICATIONS (CSPA 2018), P118, DOI 10.1109/CSPA.2018.8368697.
   Chaki S. B. J., 2015, RECOGNITION WHOLE DE.
   Chandramathi S., 2011, COMPUTER APPL, V31, P14, DOI DOI 10.5120/38365332.
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251.
   Habeeb A., 2018, ARTIF INTELL.
   Harish B. S, 2016, CLASSIFICATION PLANT, DOI {[}10.1109/ICACCI.2013.6637459, DOI 10.1109/ICACCI.2013.6637459].
   Herdiyeni Y., 2013, INT J ADV SCI ENG IN, V3, P23, DOI 10.18517/ijaseit.3.1.270.
   Janani R., 2013, P INT C ADV ELECT SY.
   Jye KS, 2017, FRONT LIFE SCI, V10, P98, DOI 10.1080/21553769.2017.1412361.
   Le N., 2020, RECOGNITION PLANT SP.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Liu NA, 2016, NEUROCOMPUTING, V216, P460, DOI 10.1016/j.neucom.2016.08.005.
   Ma JS, 2015, J CHEM INF MODEL, V55, P263, DOI 10.1021/ci500747n.
   Mookdarsanit L., 2019, SUAN SUNANDHA SCI TE, V06, P34, DOI 10.14456/ssstj.2019.8.
   Muneer A, 2020, IEEE ACCESS, V8, P196747, DOI 10.1109/ACCESS.2020.3034033.
   Muralidharan R, 2014, INT J INNOV RES COMP, V2, P5521.
   Nor A., 2017, DIGITAL IMAGE PROCES, P28.
   Pornpanomchai C., 2011, Kasetsart Journal, Natural Sciences, V45, P551.
   Priyadharshini R., 2018, DETECTION DIS PLANTS.
   Shruthi U, 2019, INT CONF ADVAN COMPU, P281, DOI 10.1109/ICACCS.2019.8728415.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Tangri N, 2008, NEPHROL DIAL TRANSPL, V23, P2972, DOI 10.1093/ndt/gfn187.
   Van Hieu N., 2020, AUTOMATIC PLANT IMAG, V68, P25.
   Yallur P. S., 2017, HYBRID INTELLIGENT S, DOI {[}10.9735/0975-2927.3.2.36-44, DOI 10.9735/0975-2927.3.2.36-44].
   Yamashita R, 2018, INSIGHTS IMAGING, V9, P611, DOI 10.1007/s13244-018-0639-9.
   Zhang JC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0093107.
   Zhao YY, 2020, IEEE SYS MAN CYBERN, P2565, DOI 10.1109/SMC42975.2020.9283189.
   Zhu HY, 2018, MULTIMED TOOLS APPL, V77, P29779, DOI 10.1007/s11042-017-5578-9.
   Zhu HY, 2017, MULTIMED TOOLS APPL, V76, P4599, DOI 10.1007/s11042-016-3538-4.},
Number-of-Cited-References = {33},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BT5WW},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000839140800002},
DA = {2023-08-12},
}

@inproceedings{ WOS:000851266300012,
Author = {Paco Ramos, Mery M. and Paco Ramos, Vanessa M. and Loaiza Fabian, Arnold
   and Osco Mamani, Erbert F.},
Editor = {Orjuela-Canon, AD and Figueroa-Garcia, JC and Arias-Londono, JD},
Title = {A Feature Extraction Method Based on Convolutional Autoencoder for Plant
   Leaves Classification},
Booktitle = {APPLICATIONS OF COMPUTATIONAL INTELLIGENCE, COLCACI 2019},
Series = {Communications in Computer and Information Science},
Year = {2019},
Volume = {1096},
Pages = {143-154},
Note = {2nd IEEE Colombian Conference on Computational Intelligence (ColCACI),
   Barranquilla, COLOMBIA, JUN 05-07, 2019},
Organization = {IEEE; IEEE Colombia Sect; IEEE Colombian Caribbean Sect; IEEE Computat
   Intelligence Colombian Chapter; IEEE Computat Intelligence Soc; Univ
   Norte; Univ Rosario; Univ Distrital Francisco Jose Caldas; Univ
   Antioquia},
Abstract = {In this research, we present an approach based on Convolutional
   Autoencoder (CAE) and Support Vector Machine (SVM) for leaves
   classification of different trees. While previous approaches relied on
   image processing and manual feature extraction, the proposed approach
   operates directly on the image pixels, without any preprocessing.
   Firstly, we use multiple layers of CAE to learn the features of leaf
   image dataset. Secondly, the extracted features were used to train a
   linear classifier based on SVM. Experimental results show that the
   classifiers using these features can improve their predictive value,
   reaching an accuracy rate of 94.74\%.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Ramos, MMP (Corresponding Author), Univ Nacl Jorge Basadre Grohmann, Tacna, Peru.
   Paco Ramos, Mery M.; Paco Ramos, Vanessa M.; Loaiza Fabian, Arnold; Osco Mamani, Erbert F., Univ Nacl Jorge Basadre Grohmann, Tacna, Peru.},
DOI = {10.1007/978-3-030-36211-9\_12},
ISSN = {1865-0929},
EISSN = {1865-0937},
ISBN = {978-3-030-36211-9; 978-3-030-36210-2},
Keywords = {Feature extraction; Image processing; Plant classification;
   Convolutional Autoencoder; Deep learning; Computer vision},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications},
Author-Email = {mpacor@unjbg.edu.pe
   vpacor@unjbg.edu.pe
   aloaizaf@unjbg.edu.pe
   eosco@unjbg.edu.pe},
Affiliations = {Universidad Nacional Jorge Basadre Grohmann},
ResearcherID-Numbers = {Loaiza Fabian, Arnold/ABD-5806-2021
   },
ORCID-Numbers = {Loaiza Fabian, Arnold/0000-0001-9302-4041
   Osco Mamani, Erbert Francisco/0000-0002-8492-5961},
Cited-References = {Ahmed N, 2016, SCI INT, V28, DOI DOI 10.9790/0661-17134853.
   Andrew N, 2011, CS294A LECT NOTES, V72, P1, DOI DOI 10.1371/JOURNAL.PONE.0006098.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Audebert N, 2018, ISPRS J PHOTOGRAMM, V140, P20, DOI 10.1016/j.isprsjprs.2017.11.011.
   Bama B. S., 2011, IND J COMP SCI ENG, V2, P202.
   Di Ruberto C, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS (VISAPP), VOL 1, P601.
   Gala Garcia Y, 2013, THESIS.
   Garcia-Garcia A, 2016, 3D OBJECT RECOGNITIO.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   Kadir Abdul, 2013, Arxiv, DOI arXiv:1401.4447.
   Kadir A, 2011, Arxiv, DOI arXiv:1110.1513.
   Kumar G, 2014, INT C ADV COMPUT COM, P5, DOI 10.1109/ACCT.2014.74.
   Kumar PSVVSR, 2016, PROCEEDINGS OF THE 2016 2ND INTERNATIONAL CONFERENCE ON CONTEMPORARY COMPUTING AND INFORMATICS (IC3I), P548, DOI 10.1109/IC3I.2016.7918024.
   Lagar-Cavilla H. A., 2009, 2012 INT C DIG IM CO, P1, DOI DOI 10.1109/DICTA.2012.6411702.
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7\_7.
   Mei XY, 2017, IEEE INT C BIOINF BI, P241, DOI {[}10.1109/BIBE.2017.00-48, 10.1109/BIBE.2017.00047].
   Meng QX, 2017, IEEE IJCNN, P364, DOI 10.1109/IJCNN.2017.7965877.
   Redolfi J.A., 2015, ARGENTINE S ARTIFICI.
   Schmid U., 2017, STACKED DENOISING ST.
   Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243.
   Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI {[}DOI 10.1145/1390156.1390294, 10.1145/1390156.1390294].
   Vincent P, 2010, J MACH LEARN RES, V11, P3371.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang ZB, 2014, IEEE IJCNN, P975, DOI 10.1109/IJCNN.2014.6889656.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Wu Y.-J., 2016, J IMAGE GRAPHICS, V4, P93.},
Number-of-Cited-References = {26},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BT7WC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000851266300012},
DA = {2023-08-12},
}

@article{ WOS:000374578800008,
Author = {Wang, Zhaobin and Sun, Xiaoguang and Zhang, Yaonan and Ying, Zhu and Ma,
   Yide},
Title = {Leaf recognition based on PCNN},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2016},
Volume = {27},
Number = {4},
Pages = {899-908},
Month = {MAY},
Abstract = {Plant is closely related to humans. How to quickly recognize an unknown
   plant without related professional knowledge is a huge challenge. With
   the development of image processing and pattern recognition, it is
   available for plant recognition based on the technique of image
   processing. Pulse-coupled neural network is a powerful tool for image
   processing. It is widely applied in the field of image segmentation,
   image fusion, feature extraction, etc. Support vector machine is an
   excellent classifier, which can finish the complex task of data
   exploration. Based on these two techniques, a novel plant recognition
   method is proposed in this paper. The key feature is the entropy
   sequence obtained by pulse-coupled neural network. Other ancillary
   features can be computed directly by mathematical and morphological
   methods. Both key feature and ancillary features are employed to
   represent the unique feature of one plant. Support vector machine in our
   method is taken as the classifier, which can implement the multi-class
   classification. Experimental results show that the proposed method can
   finish the task of plant recognition effectively. Compared with the
   existing methods, our proposed method has better recognition rate.},
Publisher = {SPRINGER LONDON LTD},
Address = {236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Zhang, YN (Corresponding Author), Chinese Acad Sci, Cold \& Arid Reg Environm \& Engn Res Inst, Lanzhou 730000, Gansu, Peoples R China.
   Wang, Zhaobin; Sun, Xiaoguang; Ma, Yide, Lanzhou Univ, Sch Informat Sci \& Engn, Lanzhou 730000, Gansu, Peoples R China.
   Wang, Zhaobin; Zhang, Yaonan, Chinese Acad Sci, Cold \& Arid Reg Environm \& Engn Res Inst, Lanzhou 730000, Gansu, Peoples R China.
   Ying, Zhu, Gansu Acad Sci, Inst Biol, Lanzhou 730000, Peoples R China.},
DOI = {10.1007/s00521-015-1904-1},
ISSN = {0941-0643},
EISSN = {1433-3058},
Keywords = {Feature extraction; Image processing; Plant recognition; PCNN},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {yaonan@lzb.ac.cn},
Affiliations = {Lanzhou University; Chinese Academy of Sciences; Cold \& Arid Regions
   Environmental \& Engineering Research Institute, CAS},
ResearcherID-Numbers = {liu, Jingyong/C-8626-2017
   zhang, yuanyuan/GYA-4428-2022},
Funding-Acknowledgement = {China Postdoctoral Science Foundation {[}2013M532097]; Fundamental
   Research Funds for the Central Universities {[}lzujbky-2014-52];
   National Science Foundation of China {[}61201421, 61175012,
   91125005/D011004]; Science Foundation of Gansu Province of China
   {[}1208RJYA058]; Incubation Foundation for Special Disciplines of the
   National Science Foundation of China {[}J1210003/J0109]},
Funding-Text = {The authors thank all the reviewers for their valuable comments, which
   further improved the quality of the paper. This work is jointly
   supported by China Postdoctoral Science Foundation (Grant No.
   2013M532097), Fundamental Research Funds for the Central Universities
   (lzujbky-2014-52), National Science Foundation of China (Grant Nos.
   61201421, 61175012 \& 91125005/D011004), Science Foundation of Gansu
   Province of China (Grant No. 1208RJYA058) and the Incubation Foundation
   for Special Disciplines of the National Science Foundation of China
   (Grant No. J1210003/J0109).},
Cited-References = {CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   Du JX, 2013, NEUROCOMPUTING, V116, P150, DOI 10.1016/j.neucom.2012.03.028.
   Eckhorn R, 1990, NEURAL COMPUT, V2, P293, DOI 10.1162/neco.1990.2.3.293.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   JOHNSON JL, 1994, APPL OPTICS, V33, P6239, DOI 10.1364/AO.33.006239.
   Kulkarni A.H., 2013, INT J ADV RES COMPUT, V2, P1.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Ma Y, 2002, CHINESE SCI BULL, V47, P167.
   Ma YD, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON INFORMATION ACQUISITION, VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P808, DOI 10.1109/ICIA.2006.305834.
   Miao ZJ, 2000, PATTERN RECOGN LETT, V21, P169, DOI 10.1016/S0167-8655(99)00144-0.
   Mouine S., 2012, P 2 ACM INT C MULT R, P1.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Shabanzade M., 2011, SIGNAL IMAGE PROCESS, V2, P23, DOI DOI 10.5121/sipij.2011.2303.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88.
   Uluturk Caner, 2012, INN INT SYST APPL IN.
   Valliammal N, 2012, INT J COMPUT COMMUN, V6, P406.
   Wang ZB, 2010, PATTERN RECOGN, V43, P2003, DOI 10.1016/j.patcog.2010.01.011.
   Wang ZB, 2010, IMAGE VISION COMPUT, V28, P5, DOI 10.1016/j.imavis.2009.06.007.
   Wu QF, 2007, ADVANCED COMPUTER TECHNOLOGY, NEW EDUCATION, PROCEEDINGS, P47.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhang SW, 2013, PATTERN RECOGN, V46, P1891, DOI 10.1016/j.patcog.2013.01.015.},
Number-of-Cited-References = {23},
Times-Cited = {33},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {38},
Journal-ISO = {Neural Comput. Appl.},
Doc-Delivery-Number = {DK0CA},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000374578800008},
DA = {2023-08-12},
}

@article{ WOS:000603258100006,
Author = {Suto, Jozsef},
Title = {Plant leaf recognition with shallow and deep learning: A comprehensive
   study},
Journal = {INTELLIGENT DATA ANALYSIS},
Year = {2020},
Volume = {24},
Number = {6},
Pages = {1311-1328},
Abstract = {Nowadays there are hundreds of thousands known plant species on the
   Earth and many are still unknown yet. The process of plant
   classification can be performed using different ways but the most
   popular approach is based on plant leaf characteristics. Most types of
   plants have unique leaf characteristics such as shape, color, and
   texture. Since machine learning and vision considerably developed in the
   past decade, automatic plant species (or leaf) recognition has become
   possible. Recently, the automated leaf classification is a standalone
   research area inside machine learning and several shallow and deep
   methods were proposed to recognize leaf types. From 2007 to present days
   several research papers have been published in this topic. In older
   studies the classifier was a shallow method while in current works many
   researchers applied deep networks for classification. During the
   overview of plant leaf classification literature, we found an
   interesting deficiency (lack of hyper-parameter search) and a key
   difference between studies (different test sets). This work gives an
   overall review about the efficiency of shallow and deep methods under
   different test conditions. It can be a basis to further research.},
Publisher = {IOS PRESS},
Address = {NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Suto, J (Corresponding Author), Univ Debrecen, Fac Informat, Dept Informat Syst \& Networks, Kassai St 26, H-4028 Debrecen, Hungary.
   Suto, Jozsef, Univ Debrecen, Fac Informat, Dept Informat Syst \& Networks, Kassai St 26, H-4028 Debrecen, Hungary.},
DOI = {10.3233/IDA-194821},
ISSN = {1088-467X},
EISSN = {1571-4128},
Keywords = {Artificial neural network; deep learning; feature engineering; plant
   leaf recognition},
Keywords-Plus = {SHAPE; HISTOGRAMS; FEATURES},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {suto.jozsef@inf.unideb.hu},
Affiliations = {University of Debrecen},
Funding-Acknowledgement = {Higher Education Institutional Excellence Programme of the Ministry of
   Human Capacities in Hungary {[}20428-3/2018/FEKUTSTRAT]; European Union;
   European Social Fund;  {[}EFOP-3.6.3-VEKOP-16-2017-00002]},
Funding-Text = {This work was supported by the construction
   EFOP-3.6.3-VEKOP-16-2017-00002. The project was supported by the
   European Union, co-financed by the European Social Fund. The research
   was also financed by the Higher Education Institutional Excellence
   Programme (20428-3/2018/FEKUTSTRAT) of the Ministry of Human Capacities
   in Hungary, within the framework of the 4. thematic programme of the
   University of Debrecen.},
Cited-References = {Anant Bhardwaj, 2013, International Journal of Innovation and Applied Studies, V3, P237.
   {[}Anonymous], 2011, P 22 ANN S PATT REC, P91.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Bergstra J, 2012, J MACH LEARN RES, V13, P281.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Chollet F., 2018, DEEP LEARNING PYTHON, V190-200, P230, DOI DOI 10.1007/978-1-4842-2766-4.
   Collobert R, 2011, J MACH LEARN RES, V12, P2493.
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
   Davies ER, 2012, COMPUTER AND MACHINE VISION: THEORY, ALGORITHMS, PRACTICALITIES, 4TH EDITION, P1.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Elhariri E, 2014, 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING \& SYSTEMS (ICCES), P271, DOI 10.1109/ICCES.2014.7030971.
   Hajjdiab H., 2011, 2011 Proceedings of IEEE International Conference on Imaging Systems and Techniques (IST 2011), P306, DOI 10.1109/IST.2011.5962205.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   Hasim A, 2016, IOP C SER EARTH ENV, V31, DOI 10.1088/1755-1315/31/1/012002.
   Jeon WS, 2017, INT J FUZZY LOG INTE, V17, P26, DOI 10.5391/IJFIS.2017.17.1.26.
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Kadir A., 2011, INT J COMPUT APPL, V29, P15, DOI DOI 10.5120/3592-4981.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Lee K.-B., 2013, INT J BIOSCI BIOTECH, V5, P57, DOI {[}10.14257/ijbsbt.2013.5.5.06, DOI 10.14257/IJBSBT.2013.5.5.06, 10.3724/sp.j.1087.2009.01707, DOI 10.1007/978-94-007-5857-5\_12].
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Mahdikhanlou K, 2014, IRAN CONF ELECTR ENG, P1690, DOI 10.1109/IranianCEE.2014.6999810.
   Mukundan R., 1998, MOMENT FUNCTIONS IMA.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Nielsen M.A., 2015, NEURAL NETWORKS DEEP, V25.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Suto J, 2019, COGN SYST RES, V54, P37, DOI 10.1016/j.cogsys.2018.11.009.
   Suto J, 2018, J AMB INTEL HUM COMP, V9, P1049, DOI 10.1007/s12652-017-0513-5.
   SUZUKI S, 1985, COMPUT VISION GRAPH, V30, P32, DOI 10.1016/0734-189X(85)90016-7.
   Tsolakidis DG, 2014, LECT NOTES ARTIF INT, V8445, P406, DOI 10.1007/978-3-319-07064-3\_33.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Wu HS, 2015, INT J INTELL SYST, V30, P871, DOI 10.1002/int.21729.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xia Q, 2014, LECT NOTES ARTIF INT, V8589, P369, DOI 10.1007/978-3-319-09339-0\_38.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zhang C., 2015, 2015 IEEE INT C COMP.
   Zhu XL, 2018, COGN SYST RES, V52, P223, DOI 10.1016/j.cogsys.2018.06.008.},
Number-of-Cited-References = {39},
Times-Cited = {4},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {8},
Journal-ISO = {Intell. Data Anal.},
Doc-Delivery-Number = {PL6VY},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000603258100006},
DA = {2023-08-12},
}

@inproceedings{ WOS:000412845400029,
Author = {Sabu, Amala and Sreekumar, K.},
Book-Group-Author = {IEEE},
Title = {Literature Review of Image Features and Classifiers Used in Leaf Based
   Plant Recognition Through Image Analysis Approach},
Booktitle = {PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON INVENTIVE
   COMMUNICATION AND COMPUTATIONAL TECHNOLOGIES (ICICCT)},
Year = {2017},
Pages = {145-149},
Note = {International Conference on Inventive Communication and Computational
   Technologies (ICICCT), Coimbatore, INDIA, MAR 10-11, 2017},
Organization = {Ranganathan Engn Coll; Elect Devices Soc; IEEE},
Abstract = {Plants play an important role in Earth's ecology by providing
   sustenance, shelter and maintaining a healthy atmosphere. Some of these
   plants have important medicinal properties. Automatic recognition of
   plant leaf is a challenging problem in the area of computer vision. An
   efficient Ayurvedic plant leaf recognition system will beneficial to
   many sectors of society which include medicinal field, botanic research
   etc. With the help of image processing and pattern recognition, we can
   easily recognize the leaf images. This paper gives a survey on different
   leaf recognition methods and classifications. Plant leaf classification
   is a technique where leaf is classified based on its different features.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Sabu, A (Corresponding Author), Coll Engn Poonjar, Dept Comp Sci, Poonjar, Kerala, India.
   Sabu, Amala; Sreekumar, K., Coll Engn Poonjar, Dept Comp Sci, Poonjar, Kerala, India.},
ISBN = {978-1-5090-5297-4},
Keywords = {ANN(Artificial Neural Network); KNN(k-nearest neighbors);
   PNN(Probablistic Neural Network); SVM(Support Vector Machine)},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems},
Author-Email = {amalasabu1992@gmail.com
   sree@cep.ac.in},
Cited-References = {BR Pushpa, 2016, INT J APPL ENG RES, V11.
   Chaki Jyotismita, 2016, PLANT LEAF RECOGNITI.
   E Sandeep Kumar., INDIAN J COMPUTER SC.
   Gopal A., 2012, CLASSIFICATION SELEC.
   Janani R, 2013, INT C ADV EL SYST.
   Lavania Shubham, 2014, IEEE INT C.
   Ngueyn Qunang-Khue, 2013, INT C ADV TECHN COMM.
   Panchal Shivaputra S., 2016, INT J ENG COMPUTER S.
   Sawant Shreepad S., 2015, INT J ADV RES COMPUT, V5.
   Sethulekshmi A V, 2014, INT J COMPUTER SCI I, V5.
   Shejwal Snehal, 2015, INT J COMPUTER SCI T.
   Tomar Divya, 2016, INT J IMAGE.
   Venkataraman D, 2016, LEAF RECOGNITION ALG.
   Xiao XY, 2010, HOG BASED APPROACH L.},
Number-of-Cited-References = {14},
Times-Cited = {13},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BI5UJ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000412845400029},
DA = {2023-08-12},
}

@inproceedings{ WOS:000428329100110,
Author = {Sabu, Amala and Sreekumar, K. and Nair, Rahul R.},
Editor = {Gupta, PK and Singh, AK and Tyagi, V and Ghrera, SP},
Title = {Recognition of Ayurvedic Medicinal Plants from Leaves: A Computer Vision
   Approach},
Booktitle = {2017 FOURTH INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING
   (ICIIP)},
Year = {2017},
Pages = {574-578},
Note = {4th International Conference on Image Information Processing (ICIIP),
   JAYPEE UNIV INFORMAT TECHNOL, WAKNAGHAT, INDIA, DEC 21-23, 2017},
Organization = {IEEE JUIT STUDENT BRANCH; Govt India, Minist Home Affairs; IEEE Delhi
   Sect; JAYPEE GRP; Comp Soc India; IEEE; Govt India, Minist External
   Affairs; JAYPEE UNIV INFORMAT TECHNOL, Dept CSE \& IT},
Abstract = {Plants are an indispensable part of our ecosystem and India has a long
   history of using plants as a source of medicines. Since the advent of
   modern allopathic medicine, the use of traditional medicine declined to
   a considerable extent. However, in recent years, traditional medicine
   has made a comeback for a variety of reasons like they are inexpensive,
   nontoxic and does not impact any side effect. Different kind of
   medicinal plant species are available on earth but it is very difficult
   to identify the plant. Considerable knowledge accumulated by the
   villagers and tribal on medicine from plants remains unknown to the
   scientists and urban people. This kind of knowledge is usually handed
   down through generations. Our immediate concern is to preserve this
   knowledge in digital form through the concepts of machine learning,
   pattern recognition and computer vision. A machine can identify a
   medicinal plant through the features extracted from the leaf images,
   together with a classification algorithm. This paper proposes a computer
   vision approach for the recognition of ayurvedic medicinal plant species
   found in Western Ghats of India The proposed system uses a combination
   of SURF and HOG features extracted from leaf images and a classification
   using k-NN classifier. Our experiments show results which seem to be
   sufficient for building apps for real life use.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Sabu, A (Corresponding Author), Coll Engn Poonjar, Dept Comp Sci, Poonjar, Kerala, India.
   Sabu, Amala; Sreekumar, K., Coll Engn Poonjar, Dept Comp Sci, Poonjar, Kerala, India.
   Nair, Rahul R., Tata Consultancy Serv, Robot \& Cognit Syst Grp, Kochi, Kerala, India.},
ISBN = {978-1-5090-6734-3},
Keywords = {Plant Recognition; Leaf Recognition; HOG; SURF; Computer Vision; Machine
   Learning},
Research-Areas = {Engineering; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Imaging Science \& Photographic
   Technology},
Author-Email = {amalasabu1992@gmail.com
   sree@cep.ac.in
   rahul7manu@gmail.com},
Affiliations = {Tata Sons; Tata Consultancy Services Limited (TCS)},
Cited-References = {ArunPriya C, 2012, P INT C PATT REC INF.
   BR Pushpa, 2016, INT J APPL ENG RES, V11.
   Chaki Jyotismita, 2016, PLANT LEAF RECOGNITI.
   E Sandeep Kumar., INDIAN J COMPUTER SC.
   Janani R, 2013, INT C ADV EL SYST.
   K Sreekumar, P 2013 IEEE 2 INT C.
   Lavania Shubham, 2014, IEEE INT C.
   Ngueyn Qunang-Khue, 2013, INT C ADV TECHN COMM.
   Nguyen Q., 2013, INT C ADV TECHN COMM.
   Reddy Prudhveeswar, 2012, CLASSIFICATION SELEC.
   Sawant Shreepad S., 2015, INT J ADV RES COMPUT, V5.
   Sekeroglua Boran, 2016, 12 INT C APPL FUZZ S.
   Shejwal Snehal, 2015, IJCST, V6.
   Tomar Divya, 2016, J IMAGE GRAPHICS, V16.
   Venkataraman D, 2016, LEAF RECOGNITION ALG.
   Xiao XY, 2010, HOG BASED APPROACH L.},
Number-of-Cited-References = {16},
Times-Cited = {8},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BJ8KD},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000428329100110},
DA = {2023-08-12},
}

@inproceedings{ WOS:000905024100115,
Author = {Lin, DeShu and Cheng, CaiFeng},
Book-Group-Author = {IEEE},
Title = {Research on flower image recognition algorithm},
Booktitle = {2022 INTERNATIONAL CONFERENCE ON BIG DATA, INFORMATION AND COMPUTER
   NETWORK (BDICN 2022)},
Year = {2022},
Pages = {646-649},
Note = {International Conference on Big Data, Information and Computer Network
   (BDICN), Sanya, PEOPLES R CHINA, JAN 20-22, 2022},
Abstract = {Flower recognition is one of the task to distinguish the species of
   flowers. Since the species of flowers with a great of variety, it is a
   hard task to distinguish the species by just using the manual features.
   Recently, with the development of deep learning technique, it can help
   us to discovery much more features hidden in the image and then help us
   to improve the precious of recognition. Thus, motived by the advantage
   machine learning technique on feature extraction and the depiction of
   handcraft features on image detail, we build our flower recognition
   model based on the features extracted from the following three types:
   feature extracted by using deep learning, dual-view features, and
   multi-modal features.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Cheng, CF (Corresponding Author), Guangdong Songshan Polytech, Sch Comp \& Informat Engn, Shaoguan, Peoples R China.
   Lin, DeShu; Cheng, CaiFeng, Guangdong Songshan Polytech, Sch Comp \& Informat Engn, Shaoguan, Peoples R China.},
DOI = {10.1109/BDICN55575.2022.00124},
ISBN = {978-1-6654-8476-3},
Keywords = {flower recognition; image recognition; ensemble learning; attention
   mechanism; feature fusion},
Research-Areas = {Computer Science; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Telecommunications},
Cited-References = {Chen P, 2019, IEEE ACCESS, V7, P118650, DOI 10.1109/ACCESS.2019.2934476.
   Cibuk M, 2019, MEASUREMENT, V137, P7, DOI 10.1016/j.measurement.2019.01.041.
   Nagarajan D, 2018, PATTERN RECOGN.
   Hoang ND, 2019, ADV ENG INFORM, V40, P110, DOI 10.1016/j.aei.2019.04.004.
   Patel S., 2020, INT J SCI TECHNOLOGY, V9, P5308.
   Simon P, 2020, PROCEDIA COMPUTER SC, V171, P680.
   Yu YC, 2018, NEUROCOMPUTING, V315, P190, DOI 10.1016/j.neucom.2018.07.016.
   Zhang SW, 2018, COMPUT ELECTRON AGR, V155, P150, DOI 10.1016/j.compag.2018.10.018.},
Number-of-Cited-References = {8},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BU4RF},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000905024100115},
DA = {2023-08-12},
}

@inproceedings{ WOS:000459239502036,
Author = {Hu, Mingyue and Feng, Hailin and Yang, Yinhui and Xia, Kai and Ren,
   Lijin},
Book-Group-Author = {IEEE},
Title = {Tree Species Identification Based on the Fusion of Multiple Deep
   Learning Models Transfer Learning},
Booktitle = {2018 CHINESE AUTOMATION CONGRESS (CAC)},
Series = {Chinese Automation Congress},
Year = {2018},
Pages = {2135-2140},
Note = {Chinese Automation Congress (CAC), Xian, PEOPLES R CHINA, NOV 30-DEC 02,
   2018},
Organization = {CAA; IEEE; IEEE Syst Man \& Cybernet Soc},
Abstract = {The automatic identification of tree species images is of great value in
   practical application. The conventional identification algorithm based
   on hand-crafted features has a complex process of feature extraction,
   and it is difficult to make an adaptive optimal adjustment based on
   actual data and specific identification tasks, which is not suitable for
   the accurate identification of tree species images with a complex
   background. A tree species identification method based on the fusion of
   multiple deep learning models transfer learning is proposed to solve the
   problem of tree species images identification with a complex background
   in natural scenes. A new tree image dataset called TreesNet was built.
   Based on the TreesNet dataset, a variety of experiments were designed,
   and the method proposed was compared with the conventional image
   classification methods. The experimental results show that the image
   identification accuracy of the tree species in the complex background
   with the method proposed in this paper reaches 93.75\%, which is much
   better than the conventional machine learning method.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Feng, HL (Corresponding Author), Zhejiang A\&F Univ, Sch Informat Engn, Hangzhou, Zhejiang, Peoples R China.
   Feng, HL (Corresponding Author), Key Lab Forestry Intelligent Monitoring \& Informa, Hangzhou, Zhejiang, Peoples R China.
   Hu, Mingyue; Feng, Hailin; Yang, Yinhui; Xia, Kai; Ren, Lijin, Zhejiang A\&F Univ, Sch Informat Engn, Hangzhou, Zhejiang, Peoples R China.
   Hu, Mingyue; Feng, Hailin; Yang, Yinhui; Xia, Kai; Ren, Lijin, Key Lab Forestry Intelligent Monitoring \& Informa, Hangzhou, Zhejiang, Peoples R China.},
ISSN = {2688-092X},
EISSN = {2688-0938},
ISBN = {978-1-7281-1312-8},
Keywords = {tree species identification; deep learning; transfer learning; image
   identification},
Research-Areas = {Automation \& Control Systems},
Web-of-Science-Categories  = {Automation \& Control Systems},
Author-Email = {huming.yue@foxmail.com
   hlfeng@zafu.edu.cn
   yhyang@zafu.edu.cn
   laxiakai@qq.com
   393229645@qq.com},
Affiliations = {Zhejiang A\&F University},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}61272313]; Zhejiang
   province key science and technology projects {[}2015C03008, 2018C02013];
   Natural Science Foundation of Zhejiang Province; Qingshanhu Science and
   Technology City Joint Fund {[}LQY18C160002]},
Funding-Text = {This work is jointly supported by National Natural Science Foundation of
   China (61272313), Zhejiang province key science and technology projects
   (2015C03008, 2018C02013), the Natural Science Foundation of Zhejiang
   Province and Qingshanhu Science and Technology City Joint Fund
   (LQY18C160002).},
Cited-References = {Abadi M., 2016, PROC USENIX OSDI, P265.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Guan Yin, 2018, COMPUTER ENG APPL, P1, DOI 10.1155/2018/8628531.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837.
   Jassmann T.J., 2015, P SOUTHEASTCON 2015, P1, DOI {[}10.1109/SECON.2015.7132978, DOI 10.1109/SECON.2015.7132978].
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Krizhevsky A, 2013, COMMUN ACM, V60, P2012.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lasseck M., 2017, CEUR WORKSHOP PROC, V1866.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Liang Long, 2016, FAST IDENTIFICATION, P55.
   Mouysset E., 2009, ARCH INVESTIGACION M.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Wang LiJun, 2015, Journal of Beijing Forestry University, V37, P55.
   Wu S. G., 2008, IEEE INT S SIGN PROC, P11.
   Zhang Shuai, 2016, J BEIJING FORESTRY U, P108.
   Zheng Yili, 2017, T CHINESE SOC AGR MA, P30.},
Number-of-Cited-References = {20},
Times-Cited = {2},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {13},
Doc-Delivery-Number = {BM0WI},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000459239502036},
DA = {2023-08-12},
}

@article{ WOS:000406987400001,
Author = {Lee, Sue Han and Chan, Chee Seng and Mayo, Simon Joseph and Remagnino,
   Paolo},
Title = {How deep learning extracts and learns leaf features for plant
   classification},
Journal = {PATTERN RECOGNITION},
Year = {2017},
Volume = {71},
Pages = {1-13},
Month = {NOV},
Abstract = {Plant identification systems developed by computer vision researchers
   have helped botanists to recognize and identify unknown plant species
   more rapidly. Hitherto, numerous studies have focused on procedures or
   algorithms that maximize the use of leaf databases for plant predictive
   modeling, but this results in leaf features which are liable to change
   with different leaf data and feature extraction techniques. In this
   paper, we learn useful leaf features directly from the raw
   representations of input data using Convolutional Neural Networks (CNN),
   and gain intuition of the chosen features based on a Deconvolutional
   Network (DN) approach. We report somewhat unexpected results: (1)
   different orders of venation are the best representative features
   compared to those of outline shape, and (2) we observe multi-level
   representation in leaf data, demonstrating the hierarchical
   transformation of features from lower-level to higher-level abstraction,
   corresponding to species classes. We show that these findings fit with
   the hierarchical botanical definitions of leaf characters. Through these
   findings, we gained insights into the design of new hybrid feature
   extraction models which are able to further improve the discriminative
   power of plant classification systems. The source code and models are
   available at: https://github.comics-chan/Deep-Plant. (C) 2017 Elsevier
   Ltd. All rights reserved.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Chan, CS (Corresponding Author), Univ Malaya, Fac Comp Sci \& Informat Technol, Ctr Image \& Signal Proc, Kuala Lumpur, Malaysia.
   Lee, Sue Han; Chan, Chee Seng, Univ Malaya, Fac Comp Sci \& Informat Technol, Ctr Image \& Signal Proc, Kuala Lumpur, Malaysia.
   Mayo, Simon Joseph, Royal Bot Gardens, Herbarium, Richmond TW9 3AE, Surrey, England.
   Remagnino, Paolo, Kingston Univ, Fac Sci Engn \& Comp, Kingston Upon Thames KT1 2EE, Surrey, England.},
DOI = {10.1016/j.patcog.2017.05.015},
ISSN = {0031-3203},
EISSN = {1873-5142},
Keywords = {Plant recognition; Deep learning; Feature visualisation},
Keywords-Plus = {AUTOMATIC CLASSIFICATION; VENATION; SHAPE; TEXTURE},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic},
Author-Email = {leesuehan@siswa.um.edu.my
   cs.chan@um.edu.my
   s.mayo@kew.org
   p.remagnino@kingston.ac.uk},
Affiliations = {Universiti Malaya; Royal Botanic Gardens, Kew; Kingston University},
ResearcherID-Numbers = {Chan, Chee Seng/B-9754-2011
   Lee, Sue Han/AAM-6250-2021
   Remagnino, Paolo/K-1829-2012},
ORCID-Numbers = {Chan, Chee Seng/0000-0001-7677-2865
   Remagnino, Paolo/0000-0002-9168-7746},
Funding-Acknowledgement = {Fundamental Research Grant Scheme (FRGS) MoHE from the Ministry of
   Education Malaysia {[}FP070-2015A]; NVIDIA Corporation; University of
   Malaya {[}PG007-2016A]; Institute for Information \& Communication
   Technology Planning \& Evaluation (IITP), Republic of Korea
   {[}B0126-16-1026] Funding Source: Korea Institute of Science \&
   Technology Information (KISTI), National Science \& Technology
   Information Service (NTIS)},
Funding-Text = {This research is supported by the Fundamental Research Grant Scheme
   (FRGS) MoHE Grant FP070-2015A, from the Ministry of Education Malaysia;
   and we gratefully acknowledge the support of NVIDIA Corporation with the
   donation of the Titan X GPU used for this research. S.H. Lee is
   supported by the Postgraduate Research (PPP) Grant PG007-2016A, from
   University of Malaya.},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   {[}Anonymous], 2016, ARXIV160207332.
   {[}Anonymous], 2013, ARXIV14014447.
   Ashley C., 2014, PLANT RECOGNITION BR.
   Backes AR, 2009, LECT NOTES COMPUT SC, V5716, P143, DOI 10.1007/978-3-642-04146-4\_17.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Berg A., 2010, LARGE SCALE VISUAL R, V2010.
   Candela H, 1999, DEV BIOL, V205, P205, DOI 10.1006/dbio.1998.9111.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Charters J., 2014, 2014 IEEE INT C MULT, P1, DOI {[}10.1109/ICMEW.2014.6890557, DOI 10.1109/ICMEW.2014.6890557].
   Clarke J, 2006, LECT NOTES COMPUT SC, V4292, P427.
   Cope JS, 2012, LECT NOTES COMPUT SC, V7517, P258, DOI 10.1007/978-3-642-33140-4\_23.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6474, P135, DOI 10.1007/978-3-642-17688-3\_14.
   Donahue J., 2013, ARXIV13101531.
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2\_13.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Ellis B., 2009, MANUAL LEAF ARCHITEC, P190, DOI DOI 10.1086/659936.
   Govaerts R, 2001, TAXON, V50, P1085, DOI 10.2307/1224723.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1\_48.
   Ma LH, 2013, LECT NOTES COMPUT SC, V7995, P106, DOI 10.1007/978-3-642-39479-9\_13.
   Mouine S., 2012, P 2 ACM INT C MULT R, P49.
   Mullen RJ, 2008, LECT NOTES COMPUT SC, V5217, P251, DOI 10.1007/978-3-540-87527-7\_24.
   Nagendra H, 2008, BIODIVERS CONSERV, V17, P3431, DOI 10.1007/s10531-008-9479-0.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Olsen A., 2015, DIG IM COMP TECHN AP, P1.
   Qi L., 2009, 2009 PES POW EN SOC, P1, DOI DOI 10.1109/WICOM.2009.5305792.
   Rashad M. Z., 2011, International Journal of Computer Science \& Information Technology, V3, P93, DOI 10.5121/ijcsit.2011.3407.
   Reyes A.K., 2015, CLEF 2015 C.
   Roth-Nebelsick A, 2001, ANN BOT-LONDON, V87, P553, DOI 10.1006/anbo.2001.1391.
   Runions A, 2005, ACM T GRAPHIC, V24, P702, DOI 10.1145/1073204.1073251.
   Simonyan K., 2014, 14091556V6 ARXIV, P1, DOI DOI 10.48550/ARXIV.1409.1556.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zhang K, 2018, IEEE T CIRC SYST VID, V28, P1303, DOI 10.1109/TCSVT.2017.2654543.
   Zhang SW, 2013, PATTERN RECOGN, V46, P1891, DOI 10.1016/j.patcog.2013.01.015.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhou B., 2016, ARXIV 1610 02055.},
Number-of-Cited-References = {55},
Times-Cited = {322},
Usage-Count-Last-180-days = {14},
Usage-Count-Since-2013 = {254},
Journal-ISO = {Pattern Recognit.},
Doc-Delivery-Number = {FC6YK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000406987400001},
OA = {Green Accepted},
DA = {2023-08-12},
}

@article{ WOS:000730453100001,
Author = {Kanda, Paul Shekonya and Xia, Kewen and Sanusi, Olanrewaju Hazzan},
Title = {A Deep Learning-Based Recognition Technique for Plant Leaf
   Classification},
Journal = {IEEE ACCESS},
Year = {2021},
Volume = {9},
Pages = {162590-162613},
Abstract = {In the practice of plant classification, the design of hand-crafted
   features is more dependent on the ability of computer vision experts to
   encode morphological characters that are predefined by botanists.
   However, the distinct features that each plant has as demonstrated by
   its leaves can be automatically learned based on the end-to-end
   advantage of Deep Learning algorithms. Therefore, Deep Learning based
   plant leaf recognition methods is an important approach nowadays. In
   this article, we are applying three technologies to achieve a model with
   high accuracy for plant classification. A Conditional Generative
   Adversarial Network was used to generate synthetic data, a Convolutional
   Neural Network was used for feature extraction and the rich extracted
   features were fed into a Logistic Regression classifier for efficient
   classification of the plant species. The effectiveness of this method
   can be seen in the wealth of plant datasets that it was tested on. The
   paper contains results on seven datasets with different modalities. We
   utilized both Deep Learning and Logistic regression in effectively
   classifying the plants using their leaf images with accuracies averaging
   96.1\% for about eight datasets used, but greater for the individual
   datasets from 99.0 to 100\% on some individual datasets. Extensive
   experiments on each of the datasets demonstrate the superiority of our
   method compared with others and are highlighted in our results.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Xia, KW (Corresponding Author), Hebei Univ Technol, Sch Elect \& Informat Engn, Tianjin 300401, Peoples R China.
   Kanda, Paul Shekonya; Xia, Kewen, Hebei Univ Technol, Sch Elect \& Informat Engn, Tianjin 300401, Peoples R China.
   Sanusi, Olanrewaju Hazzan, Tianjin Univ, Coll Intelligence \& Comp, Tianjin 300350, Peoples R China.},
DOI = {10.1109/ACCESS.2021.3131726},
ISSN = {2169-3536},
Keywords = {Feature extraction; Deep learning; Shape; Convolutional neural networks;
   Data mining; Veins; Task analysis; cGAN; classification; convolutional
   neural networks; deep learning; feature extraction; leaf images},
Keywords-Plus = {AUTOMATIC CLASSIFICATION; SHAPE; FEATURES},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {kwxia@hebut.edu.cn},
Affiliations = {Hebei University of Technology; Tianjin University},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}U1813222, 42075129];
   Hebei Province Natural Science Foundation {[}E2021202179]; Key Research
   and Development Project from Hebei Province {[}19210404D, 20351802D,
   21351803D]},
Funding-Text = {This work was supported in part by the National Natural Science
   Foundation of China under Grant U1813222 and Grant 42075129, in part by
   the Hebei Province Natural Science Foundation under Grant E2021202179,
   and in part by the Key Research and Development Project from Hebei
   Province under Grant 19210404D, Grant 20351802D, and Grant 21351803D.},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Asif, 2016, SCI INT, V28, P1.
   Bisen D, 2021, MULTIMED TOOLS APPL, V80, P6443, DOI 10.1007/s11042-020-10038-w.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Goeau H., 2016, WORKING NOTES CLEF 2, P428.
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622.
   Goyal N, 2021, MULTIMED TOOLS APPL, V80, P4533, DOI 10.1007/s11042-020-09899-y.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Gwo CY, 2013, APPL PLANT SCI, V1, DOI 10.3732/apps.1200005.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Isola P., 2017, PROC CVPR IEEE, P1125, DOI DOI 10.1109/CVPR.2017.632.
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632.
   Jin TS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0139482.
   Keivani M, 2020, TRAIT SIGNAL, V37, P17, DOI 10.18280/ts.370103.
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lavania S., 2014, P 2014 IEEE INT C CO, P1, DOI DOI 10.1109/ICCIC.2014.7238345.
   Lee MB, 2019, IEEE ACCESS, V7, P122134, DOI 10.1109/ACCESS.2019.2937809.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Mirza M., 2014, COMPUT SCI, DOI DOI 10.48550/ARXIV.1411.1784.
   Mouine S., 2012, ICMR, P1, DOI DOI 10.1145/2324796.2324853.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Nguyen QH, 2021, MATH PROBL ENG, V2021, DOI 10.1155/2021/4832864.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Pawara P, 2017, LECT NOTES COMPUT SC, V10617, P615, DOI 10.1007/978-3-319-70353-4\_52.
   Pearline SA, 2019, J INTELL FUZZY SYST, V36, P1997, DOI 10.3233/JIFS-169911.
   Ronneberger O., 2015, PROC INT C MED IMAGE, V2015, P234, DOI DOI 10.1007/978-3-319-24574-4\_28.
   Rosebrock A., 2019, DEEP LEARNING COMPUT.
   Santosa, 2013, INT J COMPUT TRENDS, Vabs, P226.
   Schlemper J, 2019, MED IMAGE ANAL, V53, P197, DOI 10.1016/j.media.2019.01.012.
   Sharon, 2020, AUTOMATING DATA AUGM.
   Soderkvist O., 2001, THESIS.
   Sujith A., 2021, Inventive Communication and Computational Technologies. Proceedings of ICICCT 2020. Lecture Notes in Networks and Systems (LNNS 145), P269, DOI 10.1007/978-981-15-7345-3\_22.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Tsolakidis DG, 2014, LECT NOTES ARTIF INT, V8445, P406, DOI 10.1007/978-3-319-07064-3\_33.
   Turkoglu M, 2019, PHYSICA A, V527, DOI 10.1016/j.physa.2019.121297.
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914.
   Vizcarra G, 2021, ECOL INFORM, V62, DOI 10.1016/j.ecoinf.2021.101268.
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113.
   Willis, 2017, STATE WORLDS PLANTS.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Zhang X, 2019, MULTIMED TOOLS APPL, V78, P27463, DOI 10.1007/s11042-019-07846-0.
   Zhang YN, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10155177.},
Number-of-Cited-References = {52},
Times-Cited = {3},
Usage-Count-Last-180-days = {12},
Usage-Count-Since-2013 = {24},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {XO8UE},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000730453100001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000803544900002,
Author = {Beikmohammadi, Ali and Faez, Karim and Motallebi, Ali},
Title = {SWP-LeafNET: A novel multistage approach for plant leaf identification
   based on deep CNN},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2022},
Volume = {202},
Month = {SEP 15},
Abstract = {Modern scientific and technological advances allow botanists to use
   computer vision-based approaches for plant identification tasks. These
   approaches have their own challenges. Leaf classification is a
   computer-vision task performed for the automated identification of plant
   species, a serious challenge due to variations in leaf morphology,
   including its size, texture, shape, and venation. Researchers have
   recently become more inclined toward deep learning-based methods rather
   than conventional feature-based methods due to the popularity and
   successful implementation of deep learning methods in image analysis,
   object recognition, and speech recognition. In this paper, to have an
   interpretable and reliable system, a botanist's behavior is modeled in
   leaf identification by proposing a highly-efficient method of maximum
   behavioral resemblance developed through three deep learning-based
   models. Different layers of the three models are visualized to ensure
   that the botanist's behavior is modeled accurately. The first and second
   models are designed from scratch. Regarding the third model, the
   pre-trained architecture MobileNetV2 is employed along with the
   transfer-learning technique. The proposed method is evaluated on two
   well-known datasets: Flavia and MalayaKew. According to a comparative
   analysis, the suggested approach is more accurate than hand-crafted
   feature extraction methods and other deep learning techniques in terms
   of 99.67\% and 99.81\% accuracy. Unlike conventional techniques that
   have their own specific complexities and depend on datasets, the
   proposed method requires no hand-crafted feature extraction. Also, it
   increases accuracy as compared with other deep learning techniques.
   Moreover, SWP-LeafNET is distributable and considerably faster than
   other methods because of using shallower models with fewer parameters
   asynchronously.},
Publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Beikmohammadi, A (Corresponding Author), Stockholm Univ, Dept Comp \& Syst Sci, Stockholm, Sweden.
   Beikmohammadi, Ali, Stockholm Univ, Dept Comp \& Syst Sci, Stockholm, Sweden.
   Faez, Karim; Motallebi, Ali, Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.},
DOI = {10.1016/j.eswa.2022.117470},
EarlyAccessDate = {MAY 2022},
Article-Number = {117470},
ISSN = {0957-4174},
EISSN = {1873-6793},
Keywords = {SWP-LeafNET; Deep learning; Plant leaf recognition; Convolutional neural
   network},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Computer Science; Engineering; Operations Research \& Management Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic; Operations Research \& Management Science},
Author-Email = {beikmohammadi@dsv.su.se
   kfaez@aut.ac.ir
   ali3motallebi@aut.ac.ir},
Affiliations = {Stockholm University; Amirkabir University of Technology},
ResearcherID-Numbers = {faez, karim/K-5117-2019},
ORCID-Numbers = {faez, karim/0000-0002-1159-4866},
Cited-References = {Angelov P, 2016, ESANN 2016, P489.
   {[}Anonymous], 2015, INT C MACHINE LEARN, DOI DOI 10.5555/3045118.3045167.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Beikmohammadi A, 2018, 2018 4TH IRANIAN CONFERENCE ON SIGNAL PROCESSING AND INTELLIGENT SYSTEMS (ICSPIS), P21, DOI 10.1109/ICSPIS.2018.8700547.
   Bodhwani Vinit, 2019, Procedia Computer Science, V152, P186, DOI 10.1016/j.procs.2019.05.042.
   Charters J., 2014, 2014 IEEE INT C MULT, P1, DOI {[}10.1109/ICMEW.2014.6890557, DOI 10.1109/ICMEW.2014.6890557].
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   Clarke J, 2006, LECT NOTES COMPUT SC, V4292, P427.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Du CB, 2017, IEEE ACCESS, V5, P15750, DOI 10.1109/ACCESS.2017.2735019.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Glorot X, 2010, P 13 INT C ARTIFICIA, P249.
   Goyal Neha, 2018, 2018 International Conference on Computing, Power and Communication Technologies (GUCON), P405, DOI 10.1109/GUCON.2018.8675114.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   Hedjazi Mohamed Abbas, 2017, IEEE T MED IMAG, V2, P1.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Mouine S., 2012, ICMR, P1, DOI DOI 10.1145/2324796.2324853.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Rasti R, 2018, IEEE T MED IMAGING, V37, P1024, DOI 10.1109/TMI.2017.2780115.
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Su Y. C., 2014, ARXIV PREPRINT ARXIV.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Wang ZB, 2016, NEURAL COMPUT APPL, V27, P899, DOI 10.1007/s00521-015-1904-1.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757.
   Zhiyu Liu, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9226, P115, DOI 10.1007/978-3-319-22186-1\_11.},
Number-of-Cited-References = {45},
Times-Cited = {9},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {15},
Journal-ISO = {Expert Syst. Appl.},
Doc-Delivery-Number = {1R7KN},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000803544900002},
OA = {hybrid, Green Published, Green Submitted},
DA = {2023-08-12},
}

@article{ WOS:000779045500002,
Author = {Tasci, Erdal and Ugur, Aybars},
Title = {A novel pattern recognition framework based on ensemble of handcrafted
   features on images},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2022},
Volume = {81},
Number = {21},
Pages = {30195-30218},
Month = {SEP},
Abstract = {Nowadays, with the advances and use of technological possibilities and
   devices, the number of digital images is increasing gradually.
   Computer-aided classification of image types is widely applied in many
   applications such as medicine, security, and automation. The feature
   extraction and selection stages have great importance in terms of
   improving the classification performance as sub-stages of the pattern
   recognition process. Researchers apply different feature extraction
   methods for their works due to the requirements. In this study, a novel
   pattern recognition framework combining diverse and large-scale
   handcrafted feature extraction methods (shape-based and texture-based)
   and the selection stage on images is developed. Genetic algorithms are
   also used for feature selection. In the experimental studies, Flavia
   leaf recognition, Caltech101 object classification image datasets, and
   five supervised classification models (random forest, ECOC-SVM,
   k-nearest neighbor, AdaBoost, classification tree) with different
   parameters' values are used. The experimental results show that the
   proposed method achieves 98.39\% and 82.77\% accuracy rates on Flavia
   and Caltech101 datasets with the ECOC-SVM model, respectively. The
   proposed framework is also competitive with the existing
   state-of-the-art methods in the related literature.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Tasci, E (Corresponding Author), Ege Univ, Comp Engn Dept, Bornova, Turkey.
   Tasci, Erdal; Ugur, Aybars, Ege Univ, Comp Engn Dept, Bornova, Turkey.},
DOI = {10.1007/s11042-022-12909-w},
EarlyAccessDate = {APR 2022},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Pattern recognition; Image processing; Feature extraction; Feature
   selection; Machine learning},
Keywords-Plus = {CLASSIFICATION; SHAPE; ALGORITHMS},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {arif.erdal.tasci@ege.edu.tr
   aybars.ugur@ege.edu.tr},
Affiliations = {Ege University},
ResearcherID-Numbers = {Taşcı, Erdal/AAB-7693-2020
   UGUR, Aybars/A-2962-2010
   },
ORCID-Numbers = {Tasci, Erdal/0000-0001-6754-2187},
Funding-Acknowledgement = {Scientific and Technological Research Council of Turkey (TUBITAK) 2211
   National Graduate Scholarship Program},
Funding-Text = {The author Erdal TASCI has been supported by the Scientific and
   Technological Research Council of Turkey (TUBITAK) 2211 National
   Graduate Scholarship Program.},
Cited-References = {Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244.
   {[}Anonymous], 2013, ARXIV14014447.
   {[}Anonymous], 2013, INT J ENG SCI TECHNO.
   Antipov G, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1263, DOI 10.1145/2733373.2806332.
   Bagheri Mohammad Ali, 2012, 2012 16th CSI International Symposium on Artificial Intelligence and Signal Processing (AISP), P508, DOI 10.1109/AISP.2012.6313800.
   Bansal M, 2021, MULTIMED TOOLS APPL, V80, P18839, DOI 10.1007/s11042-021-10646-0.
   Barczak Andre LC, 2011, INT C IM VIS COMP NZ.
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863.
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324.
   Busuttil Steven, 2004, Genome Inform, V15, P191.
   Caltech, 2018, CALTECH101.
   Chang C, 2001, PROC SPIE, V4315, P31, DOI 10.1117/12.410947.
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
   Dash M., 1997, Intelligent Data Analysis, V1.
   EHSANIRAD A., 2010, INT J COMPUTER SCI I, V8, P78.
   Eid HF, 2015, 2015 4TH INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION TECHNOLOGY AND SENSOR APPLICATION (AITS), P76, DOI 10.1109/AITS.2015.28.
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010.
   Guo GD, 2003, LECT NOTES COMPUT SC, V2888, P986.
   Gupta S, 2020, PATTERN ANAL APPL, V23, P1569, DOI {[}10.1007/s10044-020-00879-4, 10.1109/icecs49266.2020.9294790].
   Guyon I., 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Hossain J., 2010, Proceedings of the 13th International Conference on Computer and Information Technology (ICCIT 2010), P458, DOI 10.1109/ICCITECHN.2010.5723901.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Hwang W, 2018, MULTIMED TOOLS APPL, V77, P23429, DOI 10.1007/s11042-017-5571-3.
   Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819.
   Jeong D, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20071936.
   Jovic A, 2015, 2015 8TH INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO), P1200, DOI 10.1109/MIPRO.2015.7160458.
   KABBAI L, 2018, VISUAL COMPUT.
   Khan R, 2015, COMPUT VIS IMAGE UND, V132, P102, DOI 10.1016/j.cviu.2014.09.005.
   Kim JH, 2019, IEEE ACCESS, V7, P41273, DOI 10.1109/ACCESS.2019.2907327.
   Kim JH, 2018, DISPLAYS, V55, P38, DOI 10.1016/j.displa.2018.08.001.
   Kumar PSVVSR, 2016, PROCEEDINGS OF THE 2016 2ND INTERNATIONAL CONFERENCE ON CONTEMPORARY COMPUTING AND INFORMATICS (IC3I), P548, DOI 10.1109/IC3I.2016.7918024.
   Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, P2169, DOI DOI 10.1109/CVPR.2006.68.
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79.
   Li XC, 2008, ENG APPL ARTIF INTEL, V21, P785, DOI 10.1016/j.engappai.2007.07.001.
   Liao SX, 1998, IEEE T PATTERN ANAL, V20, P1358, DOI 10.1109/34.735809.
   Liu H, 2005, IEEE T KNOWL DATA EN, V17, P491, DOI 10.1109/TKDE.2005.66.
   Loh WY, 2011, WIRES DATA MIN KNOWL, V1, P14, DOI 10.1002/widm.8.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Mansourian L, 2017, MULTIMED TOOLS APPL, P1.
   Materka A., 1998, TEXTURE ANAL METHODS, P9.
   Mathworks, 2018, MEAS PROP IM REG.
   Mathworks, 2018, DCT2.
   Mingqiang Y., 2008, SURVEY SHAPE FEATURE.
   Mukherjee S., 2017, J MULTIMED INF SYST, V4, P233.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Mutch J, 2006, IEEE COMP SOC C COMP, P11, DOI DOI 10.1109/CVPR.2006.200.
   Nanni L, 2017, PATTERN RECOGN, V71, P158, DOI 10.1016/j.patcog.2017.05.025.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Pankaja K., 2020, Journal of the Institution of Engineers (India): Series B (Electrical, Electronics \& Telecommunication and Computer Engineering), V101, P597, DOI 10.1007/s40031-020-00470-9.
   Prasad S, 2017, MULTIMED TOOLS APPL, V76, P6915, DOI 10.1007/s11042-016-3309-2.
   Pyactlearn, 2018, MULTICL PERF METR.
   Rahman MM, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0178-1.
   Raji IK., 2016, INT J TELEMED CLIN P, V1, P265.
   Ramesh B, 2017, PATTERN RECOGN, V67, P380, DOI 10.1016/j.patcog.2017.02.024.
   Sargano AB, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7010110.
   Satpathy A, 2014, IEEE T IMAGE PROCESS, V24, P1953, DOI 10.1109/TIP.2014.2310123.
   Soh LK, 1999, IEEE T GEOSCI REMOTE, V37, P780, DOI 10.1109/36.752194.
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002.
   SRINIVAS M, 1994, COMPUTER, V27, P17, DOI 10.1109/2.294849.
   Tahmasbi A, 2011, COMPUT BIOL MED, V41, P726, DOI 10.1016/j.compbiomed.2011.06.009.
   Tang J., 2014, DATA CLASSIF ALGORIT, P37, DOI {[}10.1201/b17320, DOI 10.1201/B17320].
   Tasci E, 2015, J MED SYST, V39, DOI 10.1007/s10916-015-0231-5.
   TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920.
   TEH CH, 1988, IEEE T PATTERN ANAL, V10, P496, DOI 10.1109/34.3913.
   Thoke, 2015, INT J COMPUTER APPL, V116.
   Uluturk C., 2012, INT S INN INT SYST A, P1, DOI DOI 10.1109/INISTA.2012.6247030.
   Wang XG, 2011, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2011.5995696.
   Watanabe S., 1985, PATTERN RECOGN.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xue B, 2016, IEEE T EVOLUT COMPUT, V20, P606, DOI 10.1109/TEVC.2015.2504420.
   Yousefi E, 2017, COMPUT ELECTRON AGR, V140, P70, DOI 10.1016/j.compag.2017.05.031.
   Zhou JX, 2019, MULTIMED TOOLS APPL, V78, P6163, DOI 10.1007/s11042-018-6192-1.},
Number-of-Cited-References = {73},
Times-Cited = {2},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {16},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {3W1PB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000779045500002},
DA = {2023-08-12},
}

@article{ WOS:000418779200002,
Author = {Sulc, Milan and Matas, Jiri},
Title = {Fine-grained recognition of plants from images},
Journal = {PLANT METHODS},
Year = {2017},
Volume = {13},
Month = {DEC 21},
Abstract = {Background: Fine-grained recognition of plants from images is a
   challenging computer vision task, due to the diverse appearance and
   complex structure of plants, high intra-class variability and small
   inter-class differences. We review the state-of-the-art and discuss
   plant recognition tasks, from identification of plants from specific
   plant organs to general plant recognition ``in the wild{''}.
   Results: We propose texture analysis and deep learning methods for
   different plant recognition tasks. The methods are evaluated and
   compared them to the state-of-the-art. Texture analysis is only applied
   to images with unambiguous segmentation (bark and leaf recognition),
   whereas CNNs are only applied when sufficiently large datasets are
   available. The results provide an insight in the complexity of different
   plant recognition tasks. The proposed methods outperform the
   state-of-the-art in leaf and bark classification and achieve very
   competitive results in plant recognition `` in the wild{''}.
   Conclusions: The results suggest that recognition of segmented leaves is
   practically a solved problem, when high volumes of training data are
   available. The generality and higher capacity of state-of-the-art CNNs
   makes them suitable for plant recognition ``in the wild{''} where the
   views on plant organs or plants vary significantly and the difficulty is
   increased by occlusions and background clutter.},
Publisher = {BMC},
Address = {CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Sulc, M (Corresponding Author), FEE CTU Prague, Dept Cybernet, Karlovo Namesti 13, Prague 12135 2, Czech Republic.
   Sulc, Milan; Matas, Jiri, FEE CTU Prague, Dept Cybernet, Karlovo Namesti 13, Prague 12135 2, Czech Republic.},
DOI = {10.1186/s13007-017-0265-4},
Article-Number = {115},
EISSN = {1746-4811},
Keywords = {Computer vision; Plants; Leaves; Bark; Texture; Deep learning;
   Convolutional neural networks; SVM; Kernel maps},
Keywords-Plus = {INVARIANT TEXTURE CLASSIFICATION; NEURAL-NETWORKS; ROTATION; FEATURES;
   SCALE},
Research-Areas = {Biochemistry \& Molecular Biology; Plant Sciences},
Web-of-Science-Categories  = {Biochemical Research Methods; Plant Sciences},
Author-Email = {sulcmila@fel.cvut.cz},
Affiliations = {Czech Technical University Prague},
ResearcherID-Numbers = {, Matas/AAW-3282-2020
   Sonka, Milan/F-6227-2017
   },
ORCID-Numbers = {Sonka, Milan/0000-0002-9613-9968
   Sulc, Milan/0000-0002-6321-0131
   Matas, Jiri/0000-0003-0863-4844},
Funding-Acknowledgement = {CTU student Grant {[}SGS17/185/OHK3/3T/13]; Czech Science Foundation
   Project {[}GACR P103/12/G084]},
Funding-Text = {Milan Sulc was supported by the CTU student Grant SGS17/185/OHK3/3T/13.
   Jiri Matas was supported by The Czech Science Foundation Project GACR
   P103/12/G084.},
Cited-References = {Agarwal G, 2006, TAXON, V55, P597, DOI 10.2307/25065637.
   AHONEN T, 2009, PROCEEDINGS SCIA 09, V5575, P61.
   {[}Anonymous], 2009, HDB TEXTURE ANAL.
   {[}Anonymous], 1759, SYSTEMA NATURAE REGN.
   Barthelemy D, 2009, 13 WORLD FOR C.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Boudra S, 2015, LECT NOTES COMPUT SC, V9386, P764, DOI 10.1007/978-3-319-25903-1\_66.
   Champ J, 2015, CLEF 2015 C LABS EV.
   CHAPMAN AD, 2009, NUMBERS LIVING SPECI.
   Chen Ch, 2010, HDB PATTERN RECOGNIT.
   Chi Z, 2003, P ICNNSP, V2.
   Choi S., 2015, CLEF 2015 C LABS EV.
   CIMPOI M, 2015, PROC CVPR IEEE, P3828, DOI DOI 10.1109/CVPR.2015.7299007.
   Cimpoi M, 2015, ARXIV150702620.
   Cimpoi M, 2013, ARXIV13113618.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4.
   Fiel S, 2010, THESIS.
   Fiel S., 2011, P 16 COMP VIS WINT W, P67.
   Ge Z, 2015, CLEF 2015 C LABS EV.
   Ghazi MM, 2016, CLEF 2016 C LABS EV.
   Glorot X, 2010, P 13 INT C ARTIFICIA, P249.
   Goeau H., 2014, CLEF2014 WORKING NOT, P598.
   Goeau H, 2017, CLEF WORKING NOTES 2.
   Goeau H., 2013, PL NTNET MOBILE APP, P423, DOI {[}10.1145/2502081.2502251, DOI 10.1145/2502081.2502251].
   Goeau H, 2016, CLEF 2016 C LABS EV.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Goodfellow I.J., 2013, ICML.
   Grandvalet Y., 2006, SEMISUPERV LEARN, P151, DOI {[}DOI 10.7551/MITPRESS/9780262033589.001.0001, DOI 10.7551/MITPRESS/9780262033589.003.0009].
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957.
   Hang ST, 2016, CLEF 2016 C LABS EV.
   Hawkins JK, 1970, PICTURE PROCESSING P, P347.
   Howard A.G., 2017, ABS170404861 CORR.
   Huang ZK, 2006, LECT NOTES COMPUTER, V3972.
   Joly A, 2016, P CLEF 2016.
   Joly A., 2017, P CLEF 2017.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Kadir Abdul, 2012, Journal of Theoretical and Applied Information Technology, V41, P82.
   Kadir A., 2011, International Journal of Computer Science \& Information Technology, V3, P256, DOI 10.5121/ijcsit.2011.3318.
   Kadir A., 2014, INT J ADV SCI TECHNO, V44, P113.
   Kadir A., 2011, INT J COMPUT APPL, V29, P15, DOI DOI 10.5120/3592-4981.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Karuna G, 2013, INT J SCI ENG RES, V4, P703.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008.
   Lee K. B., 2012, 2012 INT C INF SCI T, V3, P133.
   Lee K-B, 2013, IMPLEMENTATION LEAF, V21, P152.
   Lin HT, 2007, MACH LEARN, V68, P267, DOI 10.1007/s10994-007-5018-6.
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1\_48.
   Lowe DG., 1999, P 7 IEEE INT C COMPU, V2, P1150, DOI {[}10.1109/iccv.1999.790410, DOI 10.1109/ICCV.1999.790410].
   Maenpaa T, 2003, LECT NOTES COMPUT SC, V2749, P885.
   Mao JH, 2014, LECT NOTES COMPUT SC, V8691, P140, DOI 10.1007/978-3-319-10578-9\_10.
   Nilsback ME, 2009, AUTOMATIC VISUAL FLO.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366.
   Otsu N., 1975, AUTOMATICA, V11, P23, DOI {[}10.1109/TSMC.1979.4310076, DOI 10.1109/TSMC.1979.4310076].
   Pietikainen M, 2000, PATTERN RECOGN, V33, P43, DOI 10.1016/S0031-3203(99)00032-1.
   Pietikainen M, 2014, COMPUTER VISION REFE, P789.
   Platt JC, 2000, ADV NEUR IN, P61.
   Pydipati R, 2006, COMPUT ELECTRON AGR, V52, P49, DOI 10.1016/j.compag.2006.01.004.
   Qi XB, 2012, LECT NOTES COMPUT SC, V7577, P158, DOI 10.1007/978-3-642-33783-3\_12.
   Reed S., 2014, ARXIV14126596.
   Reyes AK, 2015, CLEF 2015 C LABS EV.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003.
   Shalev-Shwartz S, 2012, ARXIV12091873.
   Sifre L, 2013, PROC CVPR IEEE, P1233, DOI 10.1109/CVPR.2013.163.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Soderkvist O., 2001, THESIS.
   Song J, 2004, P ISIMP.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   STEARN WT, 1959, SYST ZOOL, V8, P4, DOI 10.2307/2411603.
   Sulc M, 2015, LNCS, V8928, P181.
   Sulc M, 2017, CLEF 2017 C LABS EV.
   Sulc M, 2016, CLEF 2016 C LABS EV.
   Sulc M, 2015, LECT NOTES COMPUT SC, V8926, P47, DOI 10.1007/978-3-319-16181-5\_4.
   Sulc M, 2013, INT CONF IMAG VIS, P82, DOI 10.1109/IVCNZ.2013.6726996.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Vedaldi A., 2008, VLFEAT OPEN PORTABLE.
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153.
   Wan YY, 2004, P ISIMP.
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhang JG, 2002, PATTERN RECOGN, V35, P735, DOI 10.1016/S0031-3203(01)00074-7.
   Zhao GY, 2012, IEEE T IMAGE PROCESS, V21, P1465, DOI 10.1109/TIP.2011.2175739.},
Number-of-Cited-References = {89},
Times-Cited = {36},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {34},
Journal-ISO = {Plant Methods},
Doc-Delivery-Number = {FR0UM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000418779200002},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000402959500001,
Author = {Sun, Yu and Liu, Yuan and Wang, Guan and Zhang, Haiyan},
Title = {Deep Learning for Plant Identification in Natural Environment},
Journal = {COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE},
Year = {2017},
Volume = {2017},
Abstract = {Plant image identification has become an interdisciplinary focus in both
   botanical taxonomy and computer vision. The first plant image dataset
   collected by mobile phone in natural scene is presented, which contains
   10,000 images of 100 ornamental plant species in Beijing Forestry
   University campus. A 26-layer deep learning model consisting of 8
   residual building blocks is designed for large-scale plant
   classification in natural environment. The proposed model achieves a
   recognition rate of 91.78\% on the BJFU100 dataset, demonstrating that
   deep learning is a promising technology for smart forestry.},
Publisher = {HINDAWI LTD},
Address = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Zhang, HY (Corresponding Author), Beijing Forestry Univ, Sch Informat Sci \& Technol, Beijing 100083, Peoples R China.
   Sun, Yu; Liu, Yuan; Wang, Guan; Zhang, Haiyan, Beijing Forestry Univ, Sch Informat Sci \& Technol, Beijing 100083, Peoples R China.},
DOI = {10.1155/2017/7361042},
Article-Number = {7361042},
ISSN = {1687-5265},
EISSN = {1687-5273},
Research-Areas = {Mathematical \& Computational Biology; Neurosciences \& Neurology},
Web-of-Science-Categories  = {Mathematical \& Computational Biology; Neurosciences},
Author-Email = {zhyzml@bjfu.edu.cn},
Affiliations = {Beijing Forestry University},
ORCID-Numbers = {Sun, Yu/0000-0003-3206-9515},
Funding-Acknowledgement = {Fundamental Research Funds for the Central Universities {[}YX2014-17,
   TD2014-01]},
Funding-Text = {This work was supported by the Fundamental Research Funds for the
   Central Universities: YX2014-17 and TD2014-01.},
Cited-References = {Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50.
   Cerutti G, 2013, COMPUT VIS IMAGE UND, V117, P1482, DOI 10.1016/j.cviu.2013.07.003.
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343.
   Fu H., 2003, CHINESE B BOT, V21, P429.
   Goeau H., 2016, P CLEF WORK NOT 2016, V2016, P2016.
   Goeau H., 2015, P C LAB EV FOR CLEF.
   He K., 2016, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2016.90.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   He P., 2008, J AGR MECH RES, V6, P52.
   Huval B., EMPIRICAL EVALUATION.
   Ioffe S., BATCH NORMALIZATION.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Li Xin, 2012, Transactions of the Chinese Society of Agricultural Engineering, V28, P133.
   Li YF, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P885.
   Liu Nian, 2016, Journal of Beijing Forestry University, V38, P110.
   Nair V., 2010, ICML, P8, DOI DOI 10.5555/3104322.3104425.
   Nilsback ME, 2010, IMAGE VISION COMPUT, V28, P1049, DOI 10.1016/j.imavis.2009.10.001.
   oderkvist O. S, 2001, CHINESE B BOT.
   Sari Cihan, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P23.
   Wang YanJun, 2014, Journal of China Agricultural University, V19, P180.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhang CJ, 2013, SIGNAL PROCESS, V93, P2111, DOI 10.1016/j.sigpro.2012.09.007.},
Number-of-Cited-References = {26},
Times-Cited = {100},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {31},
Journal-ISO = {Comput. Intell. Neurosci.},
Doc-Delivery-Number = {EX1DC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000402959500001},
OA = {Green Published, gold, Green Submitted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000392155300029,
Author = {Louisse Codizar, Azeil and Solano, Geoffrey},
Book-Group-Author = {IEEE},
Title = {Plant Leaf Recognition by Venation and Shape Using Artificial Neural
   Networks},
Booktitle = {2016 7TH INTERNATIONAL CONFERENCE ON INFORMATION, INTELLIGENCE, SYSTEMS
   \& APPLICATIONS (IISA)},
Series = {International Conference Information Intelligence Systems and
   Applications},
Year = {2016},
Note = {7th International Conference on Information, Intelligence, Systems \&
   Applications (IISA), Chalkidiki, GREECE, JUL 13-15, 2016},
Organization = {Inst Elect \& Elect Engineers; BAIF; Univ Piraeus; Aristotle Univ
   Salonika; Technolog Educ Inst Western Macedonia; BAIF Aristotle Univ
   Thessaloniki},
Abstract = {The number of known and unknown plant species increases as time goes by.
   Research on plant species can be further advanced if there is a quick
   and accurate system that can identify plants and hasten the
   classification process. This system will not only help in accelerating
   plant classification, but will also allow people who are not
   morphological experts to conduct their own studies. LeaVes is an
   application designed to classify different plant species based on the
   leaf's shape and venation. This system uses different image processing
   and machine learning techniques including centroid-radii, moment
   invariance, canny edge detection, morphological operations, image
   difference and artificial neural networks.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Codizar, AL (Corresponding Author), Univ Philippines, Coll Arts \& Sci, Dept Phys Sci \& Math, Manila, Philippines.
   Louisse Codizar, Azeil; Solano, Geoffrey, Univ Philippines, Coll Arts \& Sci, Dept Phys Sci \& Math, Manila, Philippines.},
ISSN = {2379-3732},
ISBN = {978-1-5090-3429-1},
Keywords = {plant leaf recognition; leaf shape; venation; artificial neural networks},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic},
Affiliations = {University of the Philippines System; University of the Philippines
   Manila},
ResearcherID-Numbers = {Solano, Geoffrey Aserios/AAE-7446-2022
   Solano, Geoffrey/O-7876-2019},
ORCID-Numbers = {Solano, Geoffrey Aserios/0000-0002-4223-1355
   },
Funding-Acknowledgement = {Engineering Research and Development for Technology (ERDT) Scholarship
   Program of the Department of Science and Technology (DOST) Philippines},
Funding-Text = {G. Solano is supported by the Engineering Research and Development for
   Technology (ERDT) Scholarship Program of the Department of Science and
   Technology (DOST) Philippines.},
Cited-References = {{[}Anonymous], 2007, LEAF RECOGNITION ALG.
   Canny J., 1986, IEEE T PATTERN ANAL.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Cope J.S, 2011, PLANT SPECIES IDENTI.
   Du J.-X., 2007, APPL MATH COMPUTATIO.
   Gao Lin, 2010, 6 INT C NAT COMP.
   Gwo C.-Y., 2013, COMPUTERS ELECT AGR.
   Hajjdiab H., 2011, PLANT SPECIES RECOGN.
   Lee K., 2013, INT J BIOSCIENCE BIO.
   Noriega Leonardo, MULTILAYER PERCEPTRO.
   Sosa J., 2013, MATH COMPUTERS BIOL.
   Wu Q., 2006, ADV ARTIFICIAL INTEL.
   Zhang S., 2011, NEUROCOMPUTINGS.},
Number-of-Cited-References = {13},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BG8AT},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000392155300029},
DA = {2023-08-12},
}

@article{ WOS:000739024300001,
Author = {Woeber, Wilfried and Mehnen, Lars and Sykacek, Peter and Meimberg,
   Harald},
Title = {Investigating Explanatory Factors of Machine Learning Models for Plant
   Classification},
Journal = {PLANTS-BASEL},
Year = {2021},
Volume = {10},
Number = {12},
Month = {DEC},
Abstract = {Recent progress in machine learning and deep learning has enabled the
   implementation of plant and crop detection using systematic inspection
   of the leaf shapes and other morphological characters for identification
   systems for precision farming. However, the models used for this
   approach tend to become black-box models, in the sense that it is
   difficult to trace characters that are the base for the classification.
   The interpretability is therefore limited and the explanatory factors
   may not be based on reasonable visible characters. We investigate the
   explanatory factors of recent machine learning and deep learning models
   for plant classification tasks. Based on a Daucus carota and a Beta
   vulgaris image data set, we implement plant classification models and
   compare those models by their predictive performance as well as
   explainability. For comparison we implemented a feed forward
   convolutional neuronal network as a default model. To evaluate the
   performance, we trained an unsupervised Bayesian Gaussian process latent
   variable model as well as a convolutional autoencoder for feature
   extraction and rely on a support vector machine for classification. The
   explanatory factors of all models were extracted and analyzed. The
   experiments show, that feed forward convolutional neuronal networks
   (98.24\% and 96.10\% mean accuracy) outperforms the Bayesian Gaussian
   process latent variable pipeline (92.08\% and 94.31\% mean accuracy) as
   well as the convolutional autoenceoder pipeline (92.38\% and 93.28\%
   mean accuracy) based approaches in terms of classification accuracy,
   even though not significant for Beta vulgaris images. Additionally, we
   found that the neuronal network used biological uninterpretable image
   regions for the plant classification task. In contrast to that, the
   unsupervised learning models rely on explainable visual characters. We
   conclude that supervised convolutional neuronal networks must be used
   carefully to ensure biological interpretability. We recommend
   unsupervised machine learning, careful feature investigation, and
   statistical feature analysis for biological applications.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Woeber, W (Corresponding Author), Univ Nat Resources \& Life Sci, Inst Integrat Conservat Res, Dept Integrat Biol \& Biodivers Res, Gregor Mendel Str 33, A-1080 Vienna, Austria.
   Woeber, W (Corresponding Author), Univ Appl Sci Technikum Wien, Dept Ind Engn, Hochstadtpl 6, A-1200 Vienna, Austria.
   Woeber, Wilfried; Meimberg, Harald, Univ Nat Resources \& Life Sci, Inst Integrat Conservat Res, Dept Integrat Biol \& Biodivers Res, Gregor Mendel Str 33, A-1080 Vienna, Austria.
   Woeber, Wilfried, Univ Appl Sci Technikum Wien, Dept Ind Engn, Hochstadtpl 6, A-1200 Vienna, Austria.
   Mehnen, Lars, Univ Appl Sci Technikum Wien, Dept Comp Sci, Hochstadtpl 6, A-1200 Vienna, Austria.
   Sykacek, Peter, Univ Nat Resources \& Life Sci, Inst Computat Biol, Dept Biotechnol, Muthgasse 18, A-1190 Vienna, Austria.},
DOI = {10.3390/plants10122674},
Article-Number = {2674},
EISSN = {2223-7747},
Keywords = {deep learning; machine learning; plant leaf morphometrics; explainable
   AI},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {woeber@technikum-wien.at
   mehnen@technikum-wien.at
   sykacek@boku.ac.at
   meimberg@boku.ac.at},
Affiliations = {University of Natural Resources \& Life Sciences, Vienna; University of
   Natural Resources \& Life Sciences, Vienna},
ORCID-Numbers = {Sykacek, Peter/0000-0001-8800-8354
   Wober, Wilfried/0000-0002-0881-205X
   Mehnen, Lars/0000-0002-2228-0213},
Funding-Acknowledgement = {University of Applied Sciences Technikum Wien},
Funding-Text = {Open Access Funding by the University of Applied Sciences Technikum
   Wien.},
Cited-References = {Afifi A, 2021, PLANTS-BASEL, V10, DOI 10.3390/plants10010028.
   Alzubaidi L, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00444-8.
   Anders C.J., 2020, P ICML 20 WORKSHOP E.
   Anders CJ, 2022, INFORM FUSION, V77, P261, DOI 10.1016/j.inffus.2021.07.015.
   {[}Anonymous], 2003, STAT METHODS.
   Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140.
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50.
   Bergstra J, 2011, ADV NEURAL INFORM PR, V24, P2546, DOI {[}10.5555/2986459.2986743, DOI 10.5555/2986459.2986743].
   Bishop C.M., 2006, PATTERN RECOGN.
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773.
   Boulent J, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.00941.
   Bradski G, 2000, DR DOBBS J, V25, P120.
   Chollet F., 2015, KERAS.
   Damianou A., 2013, ARTIF INTELL, P207.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Digitalis Education Solutions, DIGITARIUM DATASETS.
   Dong GG, 2018, IEEE GEOSC REM SEN M, V6, P44, DOI 10.1109/MGRS.2018.2853555.
   Fawakherji M, 2019, 2019 THIRD IEEE INTERNATIONAL CONFERENCE ON ROBOTIC COMPUTING (IRC 2019), P146, DOI 10.1109/IRC.2019.00029.
   Genaev MA, 2021, PLANTS-BASEL, V10, DOI 10.3390/plants10081500.
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   GPy GPy, 2012, GAUSS PROC FRAM PYTH.
   Hasan RI, 2020, PLANTS-BASEL, V9, DOI 10.3390/plants9101302.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260.
   Koh PW, 2017, PR MACH LEARN RES, V70.
   Lapuschkin S, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-08987-4.
   Lawrence N, 2005, J MACH LEARN RES, V6, P1783.
   Lawrence ND, 2004, ADV NEUR IN, V16, P329.
   MacKay D. J. C., 2002, INFORM THEORY INFERE.
   Marcus G., 2018, ARXIV180100631CSAI, DOI DOI 10.48550/ARXIV.1801.00631.
   Meyer D., 2020, E1071 MISC FUNCTIONS.
   Milioto A, 2018, IEEE INT CONF ROBOT, P2229.
   Montavon G, 2018, DIGIT SIGNAL PROCESS, V73, P1, DOI 10.1016/j.dsp.2017.10.011.
   Nasiri A, 2021, PLANTS-BASEL, V10, DOI 10.3390/plants10081628.
   Nayak J., 2015, INT J DATABASE THEOR, V8, P169, DOI {[}10.14257/IJDTA.2015.8.1.18, 10.14257/ijdta.2015.8.1.18].
   Olsen A, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-38343-3.
   Pett M. A., 2015, NONPARAMETRIC STAT H.
   R Core Team, 2013, R LANG ENV STAT COMP.
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1.
   Redmon J, ARXIV.
   Saleem MH, 2020, PLANTS-BASEL, V9, DOI 10.3390/plants9111451.
   Samek W., 2019, LECT NOTES COMPUTER, P5, DOI {[}DOI 10.1007/978-3-030-28954-6\_1, 10.1007/978-3-030-28954-6].
   Samek W, 2018, ITU J ICT DISCOVERIE, V1, P39, DOI DOI 10.48550/ARXIV.1708.08296.
   Samek W, 2021, P IEEE, V109, P247, DOI 10.1109/JPROC.2021.3060483.
   Savicky P, 2014, PSPEARMAN SPEARMANS.
   Selvaraju R.R., 2016, ARXIV.
   Simonyan K., 2014, PROC WORKSHOP ICLR.
   Simonyan K., 2015, P 3 INT C LEARN REPR.
   SUN Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI DOI 10.1155/2017/7361042.
   Szegedy C., 2015, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2015.7298594.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Thi Thanh-Nhan Nguyen, 2019, International Journal of Machine Learning and Computing, V9, P26, DOI 10.18178/ijmlc.2019.9.1.761.
   Titsias M., 2009, ARTIF INTELL, P567.
   Titsias M., 2010, P INT C ARTIFICIAL I, P844.
   TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wober W, 2019, INT CONF CONNECT VEH, DOI 10.1109/iccve45908.2019.8964883.
   Wober W, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0249593.
   Yan Y., 2016, BAYESIAN OPTIMIZATIO.},
Number-of-Cited-References = {61},
Times-Cited = {5},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {9},
Journal-ISO = {Plants-Basel},
Doc-Delivery-Number = {YB5AE},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000739024300001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000413240500057,
Author = {Pawara, Pornntiwa and Okafor, Emmanuel and Surinta, Olarik and
   Schomaker, Lambert and Wiering, Marco},
Editor = {DeMarsico, M and DiBaja, GS and Fred, A},
Title = {Comparing Local Descriptors and Bags of Visual Words to Deep
   Convolutional Neural Networks for Plant Recognition},
Booktitle = {ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN
   RECOGNITION APPLICATIONS AND METHODS},
Year = {2017},
Pages = {479-486},
Note = {6th International Conference on Pattern Recognition Applications and
   Methods (ICPRAM), Porto, PORTUGAL, FEB 24-26, 2017},
Abstract = {The use of machine learning and computer vision methods for recognizing
   different plants from images has attracted lots of attention from the
   community. This paper aims at comparing local feature descriptors and
   bags of visual words with different classifiers to deep convolutional
   neural networks (CNNs) on three plant datasets; AgrilPlant, LeafSnap,
   and Folio. To achieve this, we study the use of both scratch and
   fine-tuned versions of the GoogleNet and the AlexNet architectures and
   compare them to a local feature descriptor with k-nearest neighbors and
   the bag of visual words with the histogram of oriented gradients
   combined with either support vector machines and multi-layer
   perceptrons. The results shows that the deep CNN methods outperform the
   hand-crafted features. The CNN techniques can also learn well on a
   relatively small dataset, Folio.},
Publisher = {SCITEPRESS},
Address = {AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Pawara, P (Corresponding Author), Univ Groningen, Inst Artificial Intelligence \& Cognit Engn ALICE, Nijenborgh 9, Groningen, Netherlands.
   Pawara, Pornntiwa; Okafor, Emmanuel; Schomaker, Lambert; Wiering, Marco, Univ Groningen, Inst Artificial Intelligence \& Cognit Engn ALICE, Nijenborgh 9, Groningen, Netherlands.
   Surinta, Olarik, Mahasarakham Univ, MISL, Maha Sarakham, Thailand.},
DOI = {10.5220/0006196204790486},
ISBN = {978-989-758-222-6},
Keywords = {Convolutional Neural Network; Deep Learning; Bags of Visual Words; Local
   Descriptor; Plant Classification},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {p.pawara@rug.nl
   e.okafor@rug.nl
   olarik.s@msu.ac.th
   l.r.b.schomaker@rug.nl
   m.a.wiering@rug.nl},
Affiliations = {University of Groningen; Mahasarakham University},
ResearcherID-Numbers = {surinta, olarik/B-9944-2012
   Wiering, Marco/C-5909-2012
   Schomaker, Lambert/GYU-5840-2022
   Pawara, Pornntiwa/IUM-7264-2023
   Okafor, Emmanuel/AAB-5120-2019
   Schomaker, Lambert RB/A-9489-2008
   },
ORCID-Numbers = {surinta, olarik/0000-0002-0644-1435
   Schomaker, Lambert RB/0000-0003-2351-930X
   Wiering, Marco/0000-0003-4331-7537
   Okafor, Emmanuel/0000-0001-6929-6880},
Cited-References = {Arora S, 2014, PR MACH LEARN RES, V32.
   Bertozzi M., 2007, 2007 IEEE Intelligent Transportation Systems Conference, P143, DOI 10.1109/ITSC.2007.4357692.
   Castelluccio M., 2015, ARXIV150800092, DOI DOI 10.1016/S1872-2032(08)60029-3.
   Chih-Fong Tsai, 2012, ISRN Artificial Intelligence, DOI 10.5402/2012/376804.
   Couchot JF, 2016, ARXIV160507946.
   Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678.
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
   Glorot X., 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1002/ECS2.1832.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Latte M. V., 2015, INT J SIGNAL PROCESS, V8, P287.
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI {[}10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954].
   Mohanty S. P., 2016, ABS160403169 CORR.
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Nilsback ME, 2010, IMAGE VISION COMPUT, V28, P1049, DOI 10.1016/j.imavis.2009.10.001.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Surinta O., 2015, ENG APPL NEURAL NETW, V517, P255, DOI {[}10.1007/978-3-319-23983-5, DOI 10.1007/978-3-319-23983-5].
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Wang ZB, 2014, IEEE IJCNN, P975, DOI 10.1109/IJCNN.2014.6889656.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Xing L., 2016, FINANC RES LETT, P1, DOI DOI 10.1016/J.FRL.2016.06.012.
   Xingxing Wang, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P572, DOI 10.1007/978-3-642-37431-9\_44.
   Yoo H. J., 2015, IEIE T SMART PROCESS, V4, P35, DOI {[}10.5573/ieiespc.2015.4.1.035, DOI 10.5573/IEIESPC.2015.4.1.035].
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {31},
Times-Cited = {48},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BI6GD},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000413240500057},
OA = {Green Submitted, hybrid},
DA = {2023-08-12},
}

@article{ WOS:000462724300007,
Author = {Pearline, S. Anubha and Kumar, V. Sathiesh and Harini, S.},
Title = {A study on plant recognition using conventional image processing and
   deep learning approaches},
Journal = {JOURNAL OF INTELLIGENT \& FUZZY SYSTEMS},
Year = {2019},
Volume = {36},
Number = {3},
Pages = {1997-2004},
Note = {4th International Symposium on Intelligent Systems Technologies and
   Applications (ISTA), Bangalore, INDIA, SEP 19-22, 2018},
Abstract = {Plant species recognition from images or videos is challenging due to a
   large diversity of plants, variation in orientation, viewpoint,
   background clutter, etc. In this paper, plant species recognition is
   carried out using two approaches, namely, traditional method and deep
   learning approach. In traditional method, feature extraction is carried
   out using Hu moments (shape features), Haralick texture, local binary
   pattern (LBP) (texture features) and color channel statistics (color
   features). The extracted features are classified using different
   classifiers (linear discriminant analysis, logistic regression,
   classification and regression tree, naive Bayes, k-nearest neighbor,
   random forest and bagging classifier). Also, different deep learning
   architectures are tested in the context of plant species recognition.
   Three standard datasets (Folio, Swedish leaf and Flavia) and one
   real-time dataset (Leaf12) is used. It is observed that, in traditional
   method, feature vector obtained by the combination of color channel
   statistics+LBP+Hu+Haralick with Random Forest classifier for Leaf12
   dataset resulted in a plant recognition accuracy (rank-1) of 82.38\%.
   VGG 16 Convolutional Neural Network (CNN) architecture with logistic
   regression resulted in an accuracy of 97.14\% for Leaf12 dataset. An
   accuracy of 96.53\%, 96.25\% and 99.41\% is obtained for Folio, Flavia
   and Swedish leaf datasets using VGG 19 CNN architecture with logistic
   regression as a classifier. It is also observed that the VGG (Very large
   Convolutional Neural Network) CNN models provided a higher accuracy rate
   compared to traditional methods.},
Publisher = {IOS PRESS},
Address = {NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS},
Type = {Article; Proceedings Paper},
Language = {English},
Affiliation = {Pearline, SA (Corresponding Author), Anna Univ, Madras Inst Technol, Dept Elect Engn, Chennai, Tamil Nadu, India.
   Pearline, S. Anubha; Kumar, V. Sathiesh; Harini, S., Anna Univ, Madras Inst Technol, Dept Elect Engn, Chennai, Tamil Nadu, India.},
DOI = {10.3233/JIFS-169911},
ISSN = {1064-1246},
EISSN = {1875-8967},
Keywords = {Plant species recognition; deep learning; convolutional neural network;
   machine learning classification},
Keywords-Plus = {CLASSIFICATION; FEATURES},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {anubhapearl@gmail.com},
Affiliations = {Anna University; Anna University Chennai; Madras Institute of Technology},
ResearcherID-Numbers = {Kumar, Sathiesh/AAW-8609-2020
   S, Anubha Pearline/AAR-6837-2020},
ORCID-Numbers = {Kumar, Sathiesh/0000-0002-0269-2333
   },
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   {[}Anonymous], 1997, MACHINE LEARNING.
   {[}Anonymous], 2017, P 31 AAAI C ART INT.
   Backes AR, 2009, INT J PATTERN RECOGN, V23, P1145, DOI 10.1142/S0218001409007508.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Chowdhury B., 2017, J METABOLIC SYNDROME, V06.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Katare C, 2014, J EVID-BASED INTEGR, V19, P112, DOI 10.1177/2156587214524229.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Marsland S., 2015, DECISION COMMITTEE E, DOI DOI 10.1016/S0967-2109(97)89838-9.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
   Pawara P, 2017, LECT NOTES COMPUT SC, V10617, P615, DOI 10.1007/978-3-319-70353-4\_52.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Soderkvist O., 2001, THESIS.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Thanh TKN, 2018, LECT NOTES ARTIF INT, V10751, P565, DOI 10.1007/978-3-319-75417-8\_53.
   Tomar D, 2016, INT J IMAGE GRAPH, V16, DOI 10.1142/S0219467816500121.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {29},
Times-Cited = {39},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {39},
Journal-ISO = {J. Intell. Fuzzy Syst.},
Doc-Delivery-Number = {HQ9CN},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000462724300007},
DA = {2023-08-12},
}

@article{ WOS:000459775000001,
Author = {Turkoglu, Muammer and Hanbay, Davut},
Title = {Recognition of plant leaves: An approach with hybrid features produced
   by dividing leaf images into two and four parts},
Journal = {APPLIED MATHEMATICS AND COMPUTATION},
Year = {2019},
Volume = {352},
Pages = {1-14},
Month = {JUL 1},
Abstract = {Plants play a crucial role in the lives of all living things. A risk of
   extinction exists for many plants, hence many botanists and scientists
   are working in order to protect plants and plant diversity. Plant
   identification is the most important part of studies carried out for
   this purpose. In order to identify plants more accurately, different
   approaches have been used in the studies to date. One of these
   approaches is plant identification through leaf recognition, and is the
   basis of many conducted studies. It can be used for automatic plant
   recognition in the area of botany, the food sector, industry, medicine,
   and in many more areas too. In this study, image processing based on
   feature extraction methods such as color features, vein features,
   Fourier Descriptors (FD), and Gray-Level Co-occurrence Matrix (GLCM)
   methods are used. This study suggests the use of features extracted from
   leaves divided into two or four parts, instead of extracting for the
   whole leaf. Both the individual and combined performances of each
   feature extraction method are calculated by Extreme Learning Machines
   (ELM) classifier. The suggested approach has been applied to the Flavia
   leaf dataset. 10-fold cross-validation was used to evaluate the accuracy
   of the proposed method, which was then compared and tabulated with
   methods from other studies. The evaluated accuracy of the proposed
   method on the Flavia leaf dataset was calculated as 99.10\%. (C) 2019
   Elsevier Inc. All rights reserved.},
Publisher = {ELSEVIER SCIENCE INC},
Address = {STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA},
Type = {Article},
Language = {English},
Affiliation = {Turkoglu, M (Corresponding Author), Bingol Univ, Dept Comp Engn, Fac Engn, TR-12000 Bingol, Turkey.
   Turkoglu, Muammer, Bingol Univ, Dept Comp Engn, Fac Engn, TR-12000 Bingol, Turkey.
   Hanbay, Davut, Inonu Univ, Dept Comp Engn, Fac Engn, TR-44280 Malatya, Turkey.},
DOI = {10.1016/j.amc.2019.01.054},
ISSN = {0096-3003},
EISSN = {1873-5649},
Keywords = {Leaf recognition; Image processing; Section process; Hybrid features;
   ELM},
Keywords-Plus = {EXTREME LEARNING-MACHINE; CLASSIFICATION},
Research-Areas = {Mathematics},
Web-of-Science-Categories  = {Mathematics, Applied},
Author-Email = {mturkoglu@bingol.edu.tr
   davut.hanbay@inonu.edu.tr},
Affiliations = {Bingol University; Inonu University},
ResearcherID-Numbers = {Hanbay, Davut/AAG-8511-2019},
ORCID-Numbers = {Hanbay, Davut/0000-0003-2271-7865},
Cited-References = {Abdul K., 2011, SIGNAL IMAGE PROCESS, V2, P1, DOI 10.5121/sipij.2011.2301.
   Alcin OF, 2016, NEUROCOMPUTING, V218, P251, DOI 10.1016/j.neucom.2016.08.050.
   Alcin OF, 2015, J FAC ENG ARCHIT GAZ, V30, P111.
   {[}Anonymous], 2014, DISCOVERING KNOWLEDG, DOI {[}DOI 10.1002/9781118874059, 10.1002/9781118874059].
   {[}Anonymous], 2012, INN INT SYST APPL IN.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Asrani  K., 2014, INT J COMPUT APPL, V95.
   Barreto GA, 2016, NEUROCOMPUTING, V176, P3, DOI 10.1016/j.neucom.2014.10.095.
   Benco M, 2007, RADIOENGINEERING, V16, P64.
   Cao JW, 2016, NEURAL NETWORKS, V81, P91, DOI 10.1016/j.neunet.2016.06.001.
   Chaki J, 2015, 2015 IEEE 2ND INTERNATIONAL CONFERENCE ON RECENT TRENDS IN INFORMATION SYSTEMS (RETIS), P189, DOI 10.1109/ReTIS.2015.7232876.
   Chen YL, 2017, ORE GEOL REV, V80, P200, DOI 10.1016/j.oregeorev.2016.06.033.
   Costa A. P. D., 2015, THESIS.
   Deng WY, 2009, 2009 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND DATA MINING, P389, DOI 10.1109/CIDM.2009.4938676.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   El Massie I, 2016, I C COMP GRAPH IM VI, P131, DOI 10.1109/CGiV.2016.34.
   Govender  N., 2014, P SCI COOP INT WORKS, P76.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Hasim  A., 2016, LEAF SHAPE RECOGNITI, V31.
   Hossain J., 2010, Proceedings of the 13th International Conference on Computer and Information Technology (ICCIT 2010), P458, DOI 10.1109/ICCITECHN.2010.5723901.
   Huang GB, 2004, IEEE IJCNN, P985.
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126.
   Iosifidis A, 2017, NEUROCOMPUTING, V219, P210, DOI 10.1016/j.neucom.2016.09.023.
   Iwata T, 2013, PROC SICE ANN CONF, P2489.
   Jain A, 2005, PATTERN RECOGN, V38, P2270, DOI 10.1016/j.patcog.2005.01.012.
   Kadir Abdul, 2012, Journal of Theoretical and Applied Information Technology, V41, P82.
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Kadir A., 2015, GATE COMPUTER VISION, V1, P3, DOI DOI 10.15579/GTCVPR.0101.003007.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Lee K.-B., 2013, INT J BIOSCI BIOTECH, V5, P57, DOI {[}10.14257/ijbsbt.2013.5.5.06, DOI 10.14257/IJBSBT.2013.5.5.06, 10.3724/sp.j.1087.2009.01707, DOI 10.1007/978-94-007-5857-5\_12].
   Liwen Gao, 2010, Proceedings 2010 Sixth International Conference on Natural Computation (ICNC 2010), P1038, DOI 10.1109/ICNC.2010.5582971.
   Mahdikhanlou K, 2014, IRAN CONF ELECTR ENG, P1690, DOI 10.1109/IranianCEE.2014.6999810.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Nijalingappa P, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON APPLIED AND THEORETICAL COMPUTING AND COMMUNICATION TECHNOLOGY (ICATCCT), P338, DOI 10.1109/ICATCCT.2015.7456906.
   Peng Y, 2016, NEUROCOMPUTING, V174, P265, DOI 10.1016/j.neucom.2015.03.118.
   Pornpanomchai C., 2011, Kasetsart Journal, Natural Sciences, V45, P551.
   Prasad S., 2011, 2011 2nd International Conference on Computer and Communication Technology, P646, DOI 10.1109/ICCCT.2011.6075212.
   Pushpa BR, 2016, INT J APPL ENG RES, V11, P5142.
   Rafael RH, 2016, P 2016 IEEE 13 INT C, P1.
   Rojanamontien M, 2016, INT CONF KNOWL SMART, P234, DOI 10.1109/KST.2016.7440521.
   Sari Cihan, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P23.
   Shabanzade M., 2011, SIGNAL IMAGE PROCESS, V2, P23, DOI DOI 10.5121/sipij.2011.2303.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Turkoglu  M., 2016, P INT C NAT SCI ENG.
   Turkoglu M, 2015, SIG PROCESS COMMUN, P2674, DOI 10.1109/SIU.2015.7130439.
   Varol G, 2015, EXPERT SYST APPL, V42, P8274, DOI 10.1016/j.eswa.2015.06.013.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wang ZB, 2014, IEEE IJCNN, P975, DOI 10.1109/IJCNN.2014.6889656.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xin Wang, 2009, 2009 Canadian Conference on Electrical and Computer Engineering (CCECE 2009), P104, DOI 10.1109/CCECE.2009.5090100.
   Yuan Q, 2011, EPILEPSY RES, V96, P29, DOI 10.1016/j.eplepsyres.2011.04.013.
   Zhang H., 2015, J COMPUTIONAL SYSTEM, V11, P141.
   Zhang L, 2016, NEUROCOMPUTING, V218, P103, DOI 10.1016/j.neucom.2016.08.066.},
Number-of-Cited-References = {55},
Times-Cited = {30},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {64},
Journal-ISO = {Appl. Math. Comput.},
Doc-Delivery-Number = {HM9AK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000459775000001},
DA = {2023-08-12},
}

@article{ WOS:000407182700007,
Author = {Yousefi, Ehsan and Baleghi, Yasser and Sakhaei, Sayed Mahmoud},
Title = {Rotation invariant wavelet descriptors, a new set of features to enhance
   plant leaves classification},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2017},
Volume = {140},
Pages = {70-76},
Month = {AUG},
Abstract = {Automatic plant leaf recognition can play an important role in plant
   classification due to leaf's availability, stable features and good
   potential to discriminate different kinds of species. Amongst many leaf
   features like leaf venation, margin, texture and lamina, leaf shape is
   the most important one due to its better discriminative power and ease
   of analysis. One of the most common leaf shape descriptors is Elliptic
   Fourier Descriptor (EFD). In this paper a new shape descriptor is
   introduced as ``Rotation Invariant Wavelet Descriptor{''} (RIWD). The
   performance of RIWD is compared with IEFD using Flavia dataset. MLP
   neural network is used as the classifier in this work. Results analysis
   shows better performance of the proposed feature in classification
   accuracy. Furthermore, an optimum feature vector is constructed using a
   set of textural and morphological features and the RIWD that reached
   97.5\% classification accuracy with low computational cost in comparison
   with many reported results in Flavia dataset. (C) 2017 Elsevier B.V. All
   rights reserved.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Baleghi, Y (Corresponding Author), Babol Noshirvani Univ Technol, Dept Elect \& Comp Engn, Babol Sar, Iran.
   Yousefi, Ehsan; Baleghi, Yasser; Sakhaei, Sayed Mahmoud, Babol Noshirvani Univ Technol, Dept Elect \& Comp Engn, Babol Sar, Iran.},
DOI = {10.1016/j.compag.2017.05.031},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Elliptic Fourier Descriptors; Feature extraction; Computer vision; Plant
   identification; Leaf recognition},
Keywords-Plus = {SHAPE; RETRIEVAL; REPRESENTATION},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {y.baleghi@nit.ac.ir},
Affiliations = {Babol Noshirvani University of Technology},
ResearcherID-Numbers = {Baleghi, Yasser/ABA-4026-2021
   Sakhaei, Sayed Mahmoud/AEW-6884-2022},
ORCID-Numbers = {Baleghi, Yasser/0000-0002-2882-4613
   },
Cited-References = {Adamek T, 2004, IEEE T CIRC SYST VID, V14, P742, DOI 10.1109/TCSVT.2004.826776.
   Alajlan N, 2007, PATTERN RECOGN, V40, P1911, DOI 10.1016/j.patcog.2006.12.005.
   {[}Anonymous], 2013, ARXIV14014447.
   {[}Anonymous], 1999, WAVELET TOUR SIGNAL.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   de Carvalho MR, 2007, EVOL BIOL, V34, P140, DOI 10.1007/s11692-007-9011-6.
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010.
   Grand-Brochier M, 2015, IEEE T IMAGE PROCESS, V24, P1549, DOI 10.1109/TIP.2015.2400214.
   Harish BS, 2013, 2013 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P1827, DOI 10.1109/ICACCI.2013.6637459.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Kociolek M., 2001, INT C SIGNALS ELECT, P99.
   KUHL FP, 1982, COMPUT VISION GRAPH, V18, P236, DOI 10.1016/0146-664X(82)90034-X.
   Kulkarni A.H., 2013, INT J ADV RES COMPUT, V2, P1.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Mancuso S., 2015, VITIS J GRAPEVINE RE, V38, P73.
   Mebatsion HK, 2012, COMPUT ELECTRON AGR, V80, P63, DOI 10.1016/j.compag.2011.10.016.
   Mebatsion HK, 2011, BIOSYST ENG, V108, P66, DOI 10.1016/j.biosystemseng.2010.10.011.
   Mebatsion HK, 2012, BIOSYST ENG, V111, P422, DOI 10.1016/j.biosystemseng.2012.01.009.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Powers D. M., 2010, J MACH LEARN TECHNOL, P37.
   Prasad S., 2016, 2016 23 INT C MECH M, P1.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Soldea O, 2010, PATTERN RECOGN LETT, V31, P1428, DOI 10.1016/j.patrec.2010.02.009.
   Wong WT, 2007, INFORM SCIENCES, V177, P1878, DOI 10.1016/j.ins.2006.10.008.
   Wang B, 2015, INFORM SCIENCES, V302, P132, DOI 10.1016/j.ins.2014.07.028.
   Wang B, 2014, IEEE T IMAGE PROCESS, V23, P4101, DOI 10.1109/TIP.2014.2343457.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yadav RB, 2008, OPT LASER TECHNOL, V40, P517, DOI 10.1016/j.optlastec.2007.08.007.
   Zhao ZQ, 2015, NEUROCOMPUTING, V151, P1112, DOI 10.1016/j.neucom.2014.02.077.},
Number-of-Cited-References = {30},
Times-Cited = {21},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {22},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {FC9SZ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000407182700007},
DA = {2023-08-12},
}

@article{ WOS:000988093500001,
Author = {Haq, Mohd Anul and Ahmed, Ahsan and Gyani, Jayadev},
Title = {Implementation of CNN for Plant Identification using UAV Imagery},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
Year = {2023},
Volume = {14},
Number = {4},
Pages = {369-378},
Month = {APR},
Abstract = {Plants are the world's most significant resource since they are the only
   natural source of oxygen. Additionally, plants are considered crucial
   since they are the major source of energy for humanity and have
   nutritional, therapeutic, and other benefits. Image identification has
   become more prominent in this technology-driven world, where many
   innovations are happening in this sphere. Image processing techniques
   are increasingly being used by researchers to identify plants. The
   capacity of Convolutional Neural Networks (CNN) to transfer weights
   learned with huge standard datasets to tasks with smaller collections or
   more particular data has improved over time. Several applications are
   made for image identification using deep learning, and Machine Learning
   (ML) algorithms. Plant image identification is a prominent part of such.
   The plant image dataset of about 300 images collected by mobile phone
   and camera from different places in the natural scenes with nine species
   of different plants are deployed for training. A five -layered
   convolution neural network (CNN) is applied for large-scale plant
   classification in a natural environment. The proposed work claims a
   higher accuracy in plant identification based on experimental data. The
   model achieves the utmost recognition rate of 96\% NU108 dataset and UAV
   images of NU101 have achieved an accuracy of 97.8\%.},
Publisher = {SCIENCE \& INFORMATION SAI ORGANIZATION LTD},
Address = {19 BOLLING RD, BRADFORD, WEST YORKSHIRE, 00000, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Gyani, J (Corresponding Author), Majmaah Univ, Coll Comp \& Informat Sci, Dept Comp Sci, Al Majmaah 11952, Saudi Arabia.
   Ahmed, A (Corresponding Author), Majmaah Univ, Coll Comp \& Informat Sci, Dept Informat Technol, Al Majmaah 11952, Saudi Arabia.
   Haq, Mohd Anul; Gyani, Jayadev, Majmaah Univ, Coll Comp \& Informat Sci, Dept Comp Sci, Al Majmaah 11952, Saudi Arabia.
   Ahmed, Ahsan, Majmaah Univ, Coll Comp \& Informat Sci, Dept Informat Technol, Al Majmaah 11952, Saudi Arabia.},
ISSN = {2158-107X},
EISSN = {2156-5570},
Keywords = {Convolutional Neural Networks (CNN); Machine Learning (ML) algorithms;
   plant image identification; plant image dataset},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Affiliations = {Majmaah University; Majmaah University},
ResearcherID-Numbers = {Haq, Mohd Anul/D-7802-2018},
ORCID-Numbers = {Haq, Mohd Anul/0000-0001-5913-5979},
Funding-Acknowledgement = {Deanship of Scientific Research at Majmaah University {[}R-2023-364]},
Funding-Text = {A CKNOWLEDGMENT Jayadev Gyani would like to thank Deanship of Scientific
   Research at Majmaah University for supporting this work under Project
   No. R-2023-364.},
Cited-References = {{[}Anonymous], CANADIAN J REMOTE SE.
   Backes AR, 2009, INT J PATTERN RECOGN, V23, P1145, DOI 10.1142/S0218001409007508.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Begue A, 2017, INT J ADV COMPUT SC, V8, P166.
   Bonnet P, 2018, MULTIMED SYST APPL, P131, DOI 10.1007/978-3-319-76445-0\_8.
   Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621.
   Farhadi F., 2017, THESIS ECOLE POLYTEC.
   Fu H., 2003, CHINESE B BOT, V21, P429.
   Haq Mohd Anul, 2021, Arabian Journal of Geosciences, V14, DOI 10.1007/s12517-021-07434-3.
   Haq MA, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-16665-7.
   Haq MA, 2022, COMPUT SYST SCI ENG, V42, P1031, DOI 10.32604/csse.2022.023221.
   Haq MA, 2022, COMPUT SYST SCI ENG, V42, P837, DOI 10.32604/csse.2022.023016.
   Haq MA, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21217416.
   Haq MA, 2022, CMC-COMPUT MATER CON, V70, P4599, DOI 10.32604/cmc.2022.020495.
   Haq MA, 2019, NAT RESOUR MODEL, V32, DOI 10.1111/nrm.12229.
   He P., 2008, J AGR MECH RES, V6, P168.
   iNaturalist, 2022, US.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Li Y, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105803.
   Li YF, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P885.
   Liu Nian, 2016, Journal of Beijing Forestry University, V38, P110.
   Microsoft Research, 2022, FLOW REC GAR PROJ RE.
   Militante SV, 2019, PROCEEDINGS OF THE 2019 IEEE EURASIA CONFERENCE ON IOT, COMMUNICATION AND ENGINEERING (ECICE), P579, DOI 10.1109/ecice47484.2019.8942686.
   Nair V., 2010, INT C MACH LEARN ICM.
   Nilsback ME, 2010, IMAGE VISION COMPUT, V28, P1049, DOI 10.1016/j.imavis.2009.10.001.
   O'Shea K, 2015, Arxiv, DOI {[}arXiv:1511.08458, DOI 10.48550/ARXIV.1511.08458].
   Plantsnap-Identify Plants Trees Mushrooms With An App, 2022, PLANTSNAP PLANT ID A.
   Popescu D, 2015, LECT NOTES COMPUT SC, V9386, P693, DOI 10.1007/978-3-319-25903-1\_60.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Wang JJ, 2018, J MANUF SYST, V48, P144, DOI 10.1016/j.jmsy.2018.01.003.
   Wu DZ, 2017, J MANUF SCI E-T ASME, V139, DOI 10.1115/1.4036350.
   Zhang CJ, 2013, SIGNAL PROCESS, V93, P2111, DOI 10.1016/j.sigpro.2012.09.007.},
Number-of-Cited-References = {38},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Int. J. Adv. Comput. Sci. Appl.},
Doc-Delivery-Number = {G3GY3},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000988093500001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000725251800034,
Author = {Yang, Meng-Meng and Phillips, Preetha and Wang, Shuihua and Zhang,
   Yudong},
Editor = {Huang, Y and Wu, H and Liu, H and Yin, Z},
Title = {Leaf Recognition for Plant Classification Based on Wavelet Entropy and
   Back Propagation Neural Network},
Booktitle = {INTELLIGENT ROBOTICS AND APPLICATIONS, ICIRA 2017, PT III},
Series = {Lecture Notes in Artificial Intelligence},
Year = {2017},
Volume = {10464},
Pages = {367-376},
Note = {10th International Conference on Intelligent Robotics and Applications
   (ICIRA, Huazhong Univ Sci \& Technol, Wuhan, PEOPLES R CHINA, AUG 16-18,
   2017},
Abstract = {In this paper, we proposed a method for plant classification, which aims
   to recognize the type of leaves from a set of image instances captured
   from same viewpoints. Firstly, for feature extraction, this paper
   adopted the 2-level wavelet transform and obtained in total 7 features.
   Secondly, the leaves were automatically recognized and classified by
   Back-Propagation neural network (BPNN). Meanwhile, we employed K-fold
   cross-validation to test the correctness of the algorithm. The accuracy
   of our method achieves 90.0\%. Further, by comparing with other methods,
   our method arrives at the highest accuracy.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zhang, YD (Corresponding Author), Nanjing Normal Univ, Sch Comp Sci \& Technol, Nanjing 210023, Jiangsu, Peoples R China.
   Zhang, YD (Corresponding Author), Jiangsu Key Lab Adv Mfg Technol, Huaiyin 223003, Jiangsu, Peoples R China.
   Yang, Meng-Meng; Wang, Shuihua; Zhang, Yudong, Nanjing Normal Univ, Sch Comp Sci \& Technol, Nanjing 210023, Jiangsu, Peoples R China.
   Phillips, Preetha, Shepherd Univ, Sch Nat Sci \& Math, Shepherdstown, WV 25443 USA.
   Phillips, Preetha, West Virginia Sch Osteopath Med, 400 N Lee St, Lewisburg, WV 24901 USA.
   Wang, Shuihua, CUNY City Coll, Dept Elect Engn, New York, NY 10031 USA.
   Zhang, Yudong, Jiangsu Key Lab Adv Mfg Technol, Huaiyin 223003, Jiangsu, Peoples R China.},
DOI = {10.1007/978-3-319-65298-6\_34},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-65298-6; 978-3-319-65297-9},
Keywords = {Feature extraction; Classification; Back-Propagation; K-fold
   crossvalidation; Pattern recognition},
Keywords-Plus = {DECISION TREE; TRANSFORM; EXTRACTION; MACHINE; IMAGES; MRI},
Research-Areas = {Automation \& Control Systems; Computer Science; Engineering; Robotics},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Artificial
   Intelligence; Engineering, Electrical \& Electronic; Robotics},
Author-Email = {yudongzhang@ieee.org},
Affiliations = {Nanjing Normal University; City University of New York (CUNY) System;
   City College of New York (CUNY)},
ResearcherID-Numbers = {Yang, Mengmeng/AAV-5795-2021
   Zhang, Yudong/I-7633-2013
   Wang, shuihua/G-7326-2016},
ORCID-Numbers = {Yang, Mengmeng/0000-0002-8988-269X
   Zhang, Yudong/0000-0002-4870-1493
   Wang, shuihua/0000-0003-4713-2791},
Funding-Acknowledgement = {Natural Science Foundation of China {[}61602250]; Natural Science
   Foundation of Jiangsu Province {[}BK20150983]},
Funding-Text = {This paper is financially supported by Natural Science Foundation of
   China (61602250), Natural Science Foundation of Jiangsu Province
   (BK20150983).},
Cited-References = {Anastasiu DC, 2016, PROCEEDINGS OF 2016 6TH WORKSHOP ON IRREGULAR APPLICATIONS: ARCHITECTURE AND ALGORITHMS (IA3), P50, DOI {[}10.1109/IA3.2016.013, 10.1109/IA3.2016.13].
   Bezawada S, 2017, J MECH DESIGN, V139, DOI 10.1115/1.4035428.
   Carro F, 2017, INTEGR ZOOL, V12, P49, DOI 10.1111/1749-4877.12212.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Garrido M, 2016, IEEE T CIRCUITS-II, V63, P868, DOI 10.1109/TCSII.2016.2534838.
   Gerdes M, 2017, EKSPLOAT NIEZAWODN, V19, P31, DOI 10.17531/ein.2017.1.5.
   HEYMANS BC, 1991, IEEE IJCNN, P2116, DOI 10.1109/IJCNN.1991.170700.
   Jeatrakul P, 2009, 2009 EIGHTH INTERNATIONAL SYMPOSIUM ON NATURAL LANGUAGE PROCESSING, PROCEEDINGS, P111, DOI 10.1109/SNLP.2009.5340935.
   Lim SH, 2016, SCI REP-UK, V6, DOI 10.1038/srep38728.
   Maleszka M, 2013, LECT NOTES COMPUT SC, V7803, P148, DOI 10.1007/978-3-642-36543-0\_16.
   Meier DC, 2017, J TEST EVAL, V45, P922, DOI 10.1520/JTE20150382.
   Nguyen HD, 2016, NEURAL COMPUT, V28, P2585, DOI 10.1162/NECO\_a\_00892.
   Saneva KHV, 2016, J INEQUAL APPL, DOI 10.1186/s13660-016-1065-5.
   Sharma B, 2016, ADV INTELL SYST, V437, P1, DOI 10.1007/978-981-10-0451-3\_1.
   Tiwari S, 2017, FOOD CHEM, V221, P47, DOI 10.1016/j.foodchem.2016.10.034.
   Wang SH, 2016, APPL SCI-BASEL, V6, DOI 10.3390/app6060169.
   Wang SH, 2015, ENTROPY-SWITZ, V17, P6663, DOI 10.3390/e17106663.
   Wang SH, 2015, ENTROPY-SWITZ, V17, P5711, DOI 10.3390/e17085711.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhang SW, 2016, PATTERN ANAL APPL, V19, P953, DOI 10.1007/s10044-015-0488-9.
   Zhang YD, 2016, SIMUL-T SOC MOD SIM, V92, P861, DOI 10.1177/0037549716666962.
   Zhang YD, 2015, J MED IMAG HEALTH IN, V5, P1395, DOI 10.1166/jmihi.2015.1542.
   Zhang YD, 2015, PROG ELECTROMAGN RES, V152, P41, DOI 10.2528/PIER15040602.
   Zhang YD, 2015, BIO-MED MATER ENG, V26, pS1283, DOI 10.3233/BME-151426.
   Zhang YD, 2015, ENTROPY-SWITZ, V17, P1795, DOI 10.3390/e17041795.
   Zhang YD, 2014, J ELECTROMAGNET WAVE, V28, P2327, DOI 10.1080/09205071.2014.967365.
   Zhang YD, 2014, KNOWL-BASED SYST, V64, P22, DOI 10.1016/j.knosys.2014.03.015.
   Zhang YD, 2010, J BIOL SYST, V18, P115, DOI 10.1142/S0218339010003652.},
Number-of-Cited-References = {30},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BS5BC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000725251800034},
DA = {2023-08-12},
}

@inproceedings{ WOS:000413813100121,
Author = {Hedjazi, Mohamed Abbas and Kourbane, Ikram and Genc, Yakup},
Book-Group-Author = {IEEE},
Title = {On Identifying Leaves: A Comparison of CNN with Classical ML Methods},
Booktitle = {2017 25TH SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS CONFERENCE
   (SIU)},
Series = {Signal Processing and Communications Applications Conference},
Year = {2017},
Note = {25th Signal Processing and Communications Applications Conference (SIU),
   Antalya, TURKEY, MAY 15-18, 2017},
Organization = {Turk Telekom; Arcelik A S; Aselsan; ARGENIT; HAVELSAN; NETAS;
   Adresgezgini; IEEE Turkey Sect; AVCR Informat Technologies; Cisco; i2i
   Syst; Integrated Syst \& Syst Design; ENOVAS; FiGES Engn; MS Spektral;
   Istanbul Teknik Univ},
Abstract = {Convolution neural networks (CNNs) eliminate the need for feature
   extraction which is one of the most important and time-consuming part of
   traditional machine learning (ML) methods. However, the challenge of
   training a deep CNN model with a limited amount of training data still
   remains. Transfer learning and parameter fine-tuning have emerged as
   solutions to this problem. Following the recent trends, we address the
   task of visual identification of leaves in images by modifying a trained
   model on a similar problem. In particular, we show that a pretrained CNN
   model on a large dataset (ImageNet) can be used to train a model from a
   small training set (ImageCLEF2013 Plant Identification). The resulting
   model outperforms the classical machine learning methods using local
   binary patterns (LBPs), a well utilized feature in the field.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Hedjazi, MA (Corresponding Author), Gebze Tech Univ, Dept Comp Engn, Kocaeli, Turkey.
   Hedjazi, Mohamed Abbas; Kourbane, Ikram; Genc, Yakup, Gebze Tech Univ, Dept Comp Engn, Kocaeli, Turkey.},
ISSN = {2165-0608},
ISBN = {978-1-5090-6494-6},
Keywords = {Machine Learning; Computer Vision; Convolution Neural Network; Transfer
   Learning},
Research-Areas = {Acoustics; Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Acoustics; Computer Science, Artificial Intelligence; Engineering,
   Electrical \& Electronic; Telecommunications},
Author-Email = {mahedjazi@gtu.edu.tr
   ikourbane@gtu.edu.tr
   yakup.genc@gtu.edu.tr},
Affiliations = {Gebze Technical University},
ResearcherID-Numbers = {Hedjazi, Mohamed Abbas/AAF-2495-2021
   Kourbane, Ikram/AAE-6367-2022
   Genc, Yakup/AAG-4668-2019},
ORCID-Numbers = {Hedjazi, Mohamed Abbas/0000-0001-9492-3719
   Genc, Yakup/0000-0002-6952-6735},
Cited-References = {Bakic V., 2013, CLEF ONL WORK NOT LA, V2013.
   Caputo B, 2013, LECT NOTES COMPUT SC, V8138, P250, DOI 10.1007/978-3-642-40802-1\_26.
   Chen Jie, 2013, BMVC.
   Deng L, 2013, IEEE T AUDIO SPEECH, V21, P1060, DOI 10.1109/TASL.2013.2244083.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   MacQueen J.B., 1967, 5 BERKELEY S MATH ST, P281, DOI DOI 10.1007/S11665-016-2173-6.
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Tin Kam Ho, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P278, DOI 10.1109/ICDAR.1995.598994.
   Yanikoglu B. A., 2013, CLEF WORKING NOTES.},
Number-of-Cited-References = {14},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BI6YK},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000413813100121},
DA = {2023-08-12},
}

@article{ WOS:000684707400022,
Author = {Nesteruk, Sergey and Shadrin, Dmitrii and Pukalchik, Mariia and Somov,
   Andrey and Zeidler, Conrad and Zabel, Paul and Schubert, Daniel},
Title = {Image Compression and Plants Classification Using Machine Learning in
   Controlled-Environment Agriculture: Antarctic Station Use Case},
Journal = {IEEE SENSORS JOURNAL},
Year = {2021},
Volume = {21},
Number = {16},
Pages = {17564-17572},
Month = {AUG 15},
Abstract = {In this article, we share our experience in the scope of
   controlled-environment agriculture automation in the Antarctic station
   greenhouse facility called EDEN ISS. For remote plant monitoring,
   control, and maintenance, we solve the problem of plant classification.
   Due to the inherent communication limitations between Antarctica and
   Europe, we first propose the image compression mechanism for the data
   collection. We show that we can compress the images, on average, 7.2
   times for efficient transmission over the weak channel. Moreover, we
   prove that decompressed images can be further used for computer vision
   applications. Upon decompressing images, we apply machine learning for
   the classification task. We achieve 92.6\% accuracy on an 18-classes
   unbalanced dataset. The proposed approach is promising for a number of
   agriculture related applications, including the plant classification,
   identification of plant diseases, and deviation of plant phenology.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Somov, A (Corresponding Author), Skolkovo Inst Sci \& Technol, Ctr Computat \& Data Intens Sci \& Engn CDISE, Moscow 121205, Russia.
   Nesteruk, Sergey; Shadrin, Dmitrii; Pukalchik, Mariia; Somov, Andrey, Skolkovo Inst Sci \& Technol, Ctr Computat \& Data Intens Sci \& Engn CDISE, Moscow 121205, Russia.
   Zeidler, Conrad; Zabel, Paul; Schubert, Daniel, German Aerosp Ctr DLR, Inst Space Syst, D-28359 Bremen, Germany.},
DOI = {10.1109/JSEN.2021.3050084},
ISSN = {1530-437X},
EISSN = {1558-1748},
Keywords = {Image coding; Cameras; Agriculture; Antarctica; Plants (biology);
   Monitoring; Machine learning; Classification; computer vision;
   controlled-environment agriculture; image compression; machine learning},
Keywords-Plus = {GREENHOUSE},
Research-Areas = {Engineering; Instruments \& Instrumentation; Physics},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Instruments \& Instrumentation;
   Physics, Applied},
Affiliations = {Skolkovo Institute of Science \& Technology; Helmholtz Association;
   German Aerospace Centre (DLR)},
ResearcherID-Numbers = {Zabel, Paul/HQZ-9029-2023
   Nesteruk, Sergey/AAQ-9698-2021
   Somov, Andrey/AAY-9278-2021},
ORCID-Numbers = {Zabel, Paul/0000-0001-7907-9230
   Nesteruk, Sergey/0000-0002-9740-6685
   Somov, Andrey/0000-0002-4615-3008},
Funding-Acknowledgement = {Ministry of Science and Higher Education of the Russian Federation
   {[}075-10-2020-091, 14.756.31.0001]; European Union {[}636501,
   COMPET-07-2014]},
Funding-Text = {This work was supported in part by the Ministry of Science and Higher
   Education of the Russian Federation under Agreement 075-10-2020-091 and
   Grant 14.756.31.0001; and in part by the European Union's Horizon 2020
   Research and Innovation Program via the COMPET-07-2014-Space
   Exploration-Life Support Subprogramme under Grant 636501. The associate
   editor coordinating the review of this article and approving it for
   publication was Dr. Hari P. Gupta.},
Cited-References = {Adeleke E, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10020206.
   Afonnikov DA, 2016, RUSS J GENET+, V52, P688, DOI 10.1134/S1022795416070024.
   Agustsson E, 2019, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2019.00031.
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150.
   Al-Turjman F, 2019, COMPUT ELECTRON AGR, V161, P4, DOI 10.1016/j.compag.2018.09.018.
   Balle J., 2018, ICLR.
   Cheng ZX, 2019, PROC CVPR IEEE, P10063, DOI 10.1109/CVPR.2019.01031.
   Costa C, 2019, FRONT PLANT SCI, V9, DOI 10.3389/fpls.2018.01933.
   Elijah O, 2018, IEEE INTERNET THINGS, V5, P3758, DOI 10.1109/JIOT.2018.2844296.
   Fiorani F., 2017, P INT C SUST AGR BIO.
   Franzen R., KODAK LOSSLESS TRUE.
   Ginesu G, 2012, SIGNAL PROCESS-IMAGE, V27, P867, DOI 10.1016/j.image.2012.01.011.
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Kingma D. P., 2014, INT C LEARNING REPRE.
   Mentzer F., 2020, ADV NEURAL INFORM PR.
   Minervini M, 2015, IEEE SIGNAL PROC MAG, V32, P126, DOI 10.1109/MSP.2015.2405111.
   Minnen D., 2018, ADV NEURAL INF PROCE, P10771.
   Pahuja R, 2013, IEEE PERVAS COMPUT, V12, P49, DOI 10.1109/MPRV.2013.26.
   Pflugfelder D, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0252-9.
   Rusyn B, 2016, PROCEEDINGS OF THE 2016 IEEE FIRST INTERNATIONAL CONFERENCE ON DATA STREAM MINING \& PROCESSING (DSMP), P195, DOI 10.1109/DSMP.2016.7583539.
   Scharr H, 2016, MACH VISION APPL, V27, P585, DOI 10.1007/s00138-015-0737-3.
   da Cruz SMS, 2019, COMPUT ELECTRON AGR, V161, P14, DOI 10.1016/j.compag.2019.01.044.
   Shadrin D, 2020, IEEE T INSTRUM MEAS, V69, P4103, DOI 10.1109/TIM.2019.2947125.
   Shadrin D, 2018, IEEE IMTC P, P251.
   Shadrin D, 2019, IEEE SENS J, V19, P11573, DOI 10.1109/JSEN.2019.2935812.
   Somov A, 2018, IEEE PERVAS COMPUT, V17, P65, DOI 10.1109/MPRV.2018.2873849.
   Taubman David, 2013, JPEG2000 IMAGE COMPR.
   Theis L., 2017, PROC INT C LEARN REP.
   Toderici G., 2015, ARXIV151106085.
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577.
   Tomita-Yokotani K, 2009, RAST 2009: PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN SPACE TECHNOLOGIES, P68, DOI 10.1109/RAST.2009.5158276.
   Wahba W. Z., 2016, TECH COMP STUDY, V3, P1.
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089.
   Walter A, 2015, PLANT METHODS, V11, DOI 10.1186/s13007-015-0056-8.
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861.
   Wasson AP, 2020, TRENDS PLANT SCI, V25, P119, DOI 10.1016/j.tplants.2019.10.011.},
Number-of-Cited-References = {38},
Times-Cited = {14},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {28},
Journal-ISO = {IEEE Sens. J.},
Doc-Delivery-Number = {TZ8FZ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000684707400022},
OA = {Green Accepted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000544256200071,
Author = {Pankaja, K. and Suma, V.},
Editor = {Satapathy, SC and Bhateja, V and Mohanty, JR and Udgata, SK},
Title = {Mango Leaves Recognition Using Deep Belief Network with MFO and
   Multi-feature Fusion},
Booktitle = {SMART INTELLIGENT COMPUTING AND APPLICATIONS, VOL 2},
Series = {Smart Innovation Systems and Technologies},
Year = {2020},
Volume = {160},
Pages = {557-565},
Note = {3rd International Conference on Smart Computing and Informatics (SCI),
   Bhubaneswar, INDIA, DEC 21-22, 2018},
Organization = {Kalinga Inst Ind Technol, Sch Comp Engn; Kalinga Inst Ind Technol, Sch
   Comp Applicat; KES Int},
Abstract = {In automatic plant classification, plant identification based on digital
   leaf images is a challenging task. This paper proposes a Moth-Flame
   Optimization (MFO) based Deep Belief Network (DBN) method for plant leaf
   recognition. Initially, a combination of texture and shape features is
   applied for extracting features from the preprocessed image. Further,
   for leaf classification, the MFO optimizes the DBN parameters to
   minimize error and are used as classifiers. The classifier has been
   applied to five different sets of mango leaf images and achieved an
   accuracy of 98.5\%. The experimental result indicates that it is
   feasible to automatically classify plants by using multi-feature
   extraction of plant leaf images in combination with MFO-based DBN.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Pankaja, K (Corresponding Author), VTU, Cambridge Inst Technol, Comp Sci \& Engn, Bengaluru, India.
   Pankaja, K., VTU, Cambridge Inst Technol, Comp Sci \& Engn, Bengaluru, India.
   Suma, V., VTU, Dayanand Sagar Coll Engn, Informat Sci \& Engn, Bengaluru, India.},
DOI = {10.1007/978-981-32-9690-9\_61},
ISSN = {2190-3018},
EISSN = {2190-3026},
ISBN = {978-981-32-9690-9; 978-981-32-9689-3},
Keywords = {Plant leaf; Feature extraction; Texture feature; MFO; DBNs},
Keywords-Plus = {ALGORITHM},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {pankaja.osr@gmail.com
   sumavdsce@gmail.com},
Affiliations = {Dayananda Sagar College of Engineering},
ORCID-Numbers = {V, Suma/0000-0003-1942-6741},
Cited-References = {Abdel-Zaher AM, 2016, EXPERT SYST APPL, V46, P139, DOI 10.1016/j.eswa.2015.10.015.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Deng YG, 2009, ICNMM 2009, PTS A-B, P253.
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527.
   Hinton Geoffrey E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8\_32.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Kahou SE, 2016, J MULTIMODAL USER IN, V10, P99, DOI 10.1007/s12193-015-0195-2.
   Lu JW, 2016, IEEE T CIRC SYST VID, V26, P529, DOI 10.1109/TCSVT.2015.2412831.
   Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134.
   Mirjalili S, 2015, KNOWL-BASED SYST, V89, P228, DOI 10.1016/j.knosys.2015.07.006.
   SHEN Lin-Lin, 2009, {[}自动化学报, Acta Automatica Sinica], V35, P350.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Tiwari D, 2016, MULTIDIM SYST SIGN P, V27, P563, DOI 10.1007/s11045-015-0319-6.
   Zhu XL, 2018, COGN SYST RES, V52, P223, DOI 10.1016/j.cogsys.2018.06.008.},
Number-of-Cited-References = {15},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BP2PT},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000544256200071},
DA = {2023-08-12},
}

@inproceedings{ WOS:000494940300022,
Author = {Han, Jing-Hua and Jin, Chen and Wu, Li-Sha},
Editor = {Ahram, T},
Title = {Research on Accuracy of Flower Recognition Application Based on
   Convolutional Neural Network},
Booktitle = {ADVANCES IN ARTIFICIAL INTELLIGENCE, SOFTWARE AND SYSTEMS ENGINEERING},
Series = {Advances in Intelligent Systems and Computing},
Year = {2020},
Volume = {965},
Pages = {224-232},
Note = {10th Int Conf on Appl Human Factors and Ergon (AHFE) / AHFE Int Conf
   Human Factors in Artificial Intelligence and Social Comp / AHFE Int Conf
   on Human Factors, Software, Serv and Syst Engn / AHFE Int Conf of Human
   Factors in Energy, Washington, DC, JUL 24-28, 2019},
Organization = {AHFE},
Abstract = {Compared with traditional flower recognition methods, the existing
   flower recognition applications on the market use advanced deep learning
   technology to improve the accuracy of plant recognition and solve the
   problem of plant recognition. The article studied the five applications
   that users commonly use, comparing and analyzing their recognition
   accuracy, and finally putting forward the feasibility advice for further
   improvement of flower recognition applications. The method of sampling
   survey was adopted, this paper divides the garden flowers and wild
   flowers into different levels according to their common degrees. Each
   type of flower was shot from 5 different angles and scenes, and
   recognized by these five applications separately. The results showed
   that the rankings of the five applications evaluated were Hua Bangzhu,
   Hua Banlv, Xing Se, Microsoft's Flower Recognition, and Find Flower
   Recognition. At pre-sent, it is necessary to continuously improve from
   the aspects of technology, products and plant libraries.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Han, JH (Corresponding Author), Beijing Forestry Univ, Beijing, Peoples R China.
   Han, Jing-Hua; Jin, Chen; Wu, Li-Sha, Beijing Forestry Univ, Beijing, Peoples R China.},
DOI = {10.1007/978-3-030-20454-9\_22},
ISSN = {2194-5357},
EISSN = {2194-5365},
ISBN = {978-3-030-20454-9; 978-3-030-20453-2},
Keywords = {Convolutional neural network; Deep learning; Flower recognition;
   Accuracy},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Software Engineering},
Author-Email = {hanjing013@126.com},
Affiliations = {Beijing Forestry University},
Funding-Acknowledgement = {Fundamental Research Funds for the Central Universities
   {[}2015ZCQ-YS-02]; Beijing Higher Education Young Elite Teacher Project
   {[}YETP0785]},
Funding-Text = {This work is supported by the Fundamental Research Funds for the Central
   Universities (2015ZCQ-YS-02) and Beijing Higher Education Young Elite
   Teacher Project (YETP0785).},
Cited-References = {Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Gardner MW, 1998, ATMOS ENVIRON, V32, P2627, DOI 10.1016/S1352-2310(97)00447-0.
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647.
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527.
   Huang YM, 2010, COMPUT EDUC, V54, P47, DOI 10.1016/j.compedu.2009.07.006.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   LeCun Y., 1990, P NEURAL INFORM PROC, P396.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.},
Number-of-Cited-References = {9},
Times-Cited = {0},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {24},
Doc-Delivery-Number = {BO1CL},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000494940300022},
DA = {2023-08-12},
}

@inproceedings{ WOS:000256344200023,
Author = {Wu, Stephen Gang and Bao, Forrest Sheng and Xu, Eric You and Wang,
   Yu-Xuan and Chang, Yi-Fan and Xiang, Qiao-Liang},
Book-Group-Author = {IEEE},
Title = {A leaf recognition algorithm for plant classification using
   Probabilistic Neural Network},
Booktitle = {2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION
   TECHNOLOGY, VOLS 1-3},
Year = {2007},
Pages = {120+},
Note = {7th IEEE International Symposium on Signal Processing and Information
   Technology, Cairo, EGYPT, DEC 15-18, 2007},
Organization = {IEEE; IEEE Signal Proc Soc; IEEE Comp Soc},
Abstract = {In this paper, we employ Probabilistic Neural Network (PNN) with image
   and data processing techniques to implement a general purpose automated
   leaf recognition for plant classification. 12 leaf features are
   extracted and orthogonalized into 5 principal variables which consist
   the input vector of the PNN. The PNN is trained by 1800 leaves to
   classify 32 kinds of plants with an accuracy greater than 90\%. Compared
   with other approaches, our algorithm is an accurate artificial
   intelligence approach which is fast in execution and easy in
   implementation.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Bao, FS (Corresponding Author), Texas Tech Univ, Dept Comp Sci, Lubbock, TX 79409 USA.
   Bao, Forrest Sheng, Texas Tech Univ, Dept Comp Sci, Lubbock, TX 79409 USA.
   Wu, Stephen Gang, Chinese Acad Sci, Inst Appl Chem, Beijing, Peoples R China.
   Xu, Eric You, Washington Univ, Dept Comp Sci \& Engn, St Louis, MO USA.
   Wang, Yu-Xuan; Xiang, Qiao-Liang, Nanjing Univ P\&T, Sch Informat \& Telecommun Engn, Nanjing, Peoples R China.
   Chang, Yi-Fan, Natl Taiwan Univ Sci \& Technol, Dept Elect Engn, Taipei, Taiwan.},
ISBN = {978-1-4244-1834-3},
Keywords = {Probabilistic Neural Network; feature extraction; leaf recognition;
   plant classification},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic},
Author-Email = {shengbao@ieee.org},
Affiliations = {Texas Tech University System; Texas Tech University; Chinese Academy of
   Sciences; Washington University (WUSTL); National Taiwan University of
   Science \& Technology},
ResearcherID-Numbers = {Bao, Forrest/C-6464-2011},
Cited-References = {{[}Anonymous], 2002, NEURAL NETWORK DESIG.
   BRENDEL T, 1995, P SPIE, V2345.
   DALLWITZ MJ, 1980, TAXON, V29, P41, DOI 10.2307/1219595.
   Du J., 2005, LNCS, V3497.
   DU JX, 2006, T I MEASUREMENT CONT, V28.
   DU XJ, 2007, APPL MATH COMPUTATIO, V185.
   Fu H., 2004, IEEE 2004 8 INT C CO.
   FU H, 2006, IEE P VIS IM SIGN PR, V153.
   FU H, 2003, P IEEE INT C NEUR NE.
   Gonzalez R.C., 2004, DIGITAL IMAGE PROCES.
   GOUVEIA F, 1997, P IEEE INT S IND EL.
   Gu X., 2005, LNCS.
   HEYMANS BC, 1991, P IEEE INT JOINT C N.
   HONG SM, 2005, COMPUTER ANIMATION V, V16.
   LI Y, 2005, P IEEE INT C NEUR NE.
   MASTER T, 1993, PRACTICAL NEURAL NET.
   {*}MATHWORKS INC, 2007, MATL NEUR NET TOOL D.
   MIAO Z, 2006, ENG APPL ARTIFICIAL, V19.
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724.
   NAM Y, 2005, LNCS, V3687.
   PLOTZE RD, 2005, CANADA J BOT, V83.
   QI H, 2003, P 2 INT C MACH LEARN.
   SAITOH TKT, 2000, P 15 INT C PATT REC, V2.
   Shlens J., 2005, TUTORIAL PRINCIPAL C.
   Specht D. F., 1988, P IEEE INT C NEUR NE, V1.
   SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q.
   Wang X. F., 2005, LNCS, V3644.
   WANG Z, 2003, IEE P VIS IM SIGN PR, V150.
   WARREN D, 1997, P IEE 6 INT C IM PRO.
   Ye Y., 2004, P 2004 INT S INT MUL.},
Number-of-Cited-References = {30},
Times-Cited = {356},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {19},
Doc-Delivery-Number = {BHT87},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000256344200023},
DA = {2023-08-12},
}

@article{ WOS:000464947900016,
Author = {Xue, Jinru and Fuentes, Sigfredo and Poblete-Echeverria, Carlos and
   Viejo, Claudia Gonzalez and Tongson, Eden and Du, Hejuan and Su, Baofeng},
Title = {Automated Chinese medicinal plants classification based on machine
   learning using leaf morpho-colorimetry, fractal dimension and
   visible/near infrared spectroscopy},
Journal = {INTERNATIONAL JOURNAL OF AGRICULTURAL AND BIOLOGICAL ENGINEERING},
Year = {2019},
Volume = {12},
Number = {2},
Pages = {123-131},
Month = {MAR},
Abstract = {The identification of Chinese medicinal plants was conducted to rely on
   ampelographic manual assessment by experts. More recently, machine
   learning algorithms for pattern recognition have been successfully
   applied to leaf recognition in other plant species. These new tools make
   the classification of Chinese medicinal plants easier, more efficient
   and cost effective. This study showed comparative results between
   machine learning models obtained from two methods: i) a
   morpho-colorimetric method and ii) a visible (VIS)/Near Infrared (NIR)
   spectral analysis from sampled leaves of 20 different Chinese medicinal
   plants. Specifically, the automated image analysis and VIS/NIR spectral
   based parameters obtained from leaves were used separately as inputs to
   construct customized artificial neural network (ANN) models. Results
   showed that the ANN model developed using the morpho-colorimetric
   parameters as inputs (Model A) had an accuracy of 98.3\% in the
   classification of leaves for the 20 medicinal plants studied. In the
   case of the model based on spectral data from leaves (Model B), the ANN
   model obtained using the averaged VIS/NIR spectra per leaf as inputs
   showed 92.5\% accuracy for the classification of all medicinal plants
   used. Model A has the advantage of being cost effective, requiring only
   a normal document scanner as measuring instrument. This method can be
   adapted for non-destructive assessment of leaves in-situ by using
   portable wireless scanners. Model B combines the fast, non-destructive
   advantages of VIS/NIR spectroscopy, which can be used for rapid and
   non-invasive identification of Chinese medicinal plants and other
   applications by analyzing specific light spectra overtones from leaves
   to assess concentration of pigments such as chlorophyll, anthocyanins
   and others that are related active compounds from the medicinal plants.},
Publisher = {CHINESE ACAD AGRICULTURAL ENGINEERING},
Address = {RM 506, NO 41, MAIZIDIAN ST, CHAOYANG DISTRICT, BEIJING, 100125, PEOPLES
   R CHINA},
Type = {Article},
Language = {English},
Affiliation = {Su, BF (Corresponding Author), Northwest A\&F Univ, Coll Mech \& Elect Engn, Yangling 712100, Shaanxi, Peoples R China.
   Xue, Jinru; Su, Baofeng, Northwest A\&F Univ, Coll Mech \& Elect Engn, Yangling 712100, Shaanxi, Peoples R China.
   Fuentes, Sigfredo; Viejo, Claudia Gonzalez; Tongson, Eden, Univ Melbourne, Sch Agr \& Food, Fac Vet \& Agr Sci, Melbourne, Vic 3010, Australia.
   Poblete-Echeverria, Carlos, Stellenbosch Univ, Fac AgriSci, ZA-7602 Stellenbosch, South Africa.
   Xue, Jinru; Su, Baofeng, Minist Agr \& Rural Affairs, Key Lab Agr Internet Things, Yangling 712100, Shaanxi, Peoples R China.
   Xue, Jinru; Su, Baofeng, Key Lab Agr Percept \& Intelligent Serv, Yangling 712100, Shaanxi, Peoples R China.
   Du, Hejuan, Tibet Nationality Univ, Coll Informat Engn, Xianyang 712089, Shaanxi, Peoples R China.},
DOI = {10.25165/j.ijabe.20191202.4637},
ISSN = {1934-6344},
EISSN = {1934-6352},
Keywords = {ampelography; computer vision; artificial neural networks; pattern
   recognition; Chinese medicinal plants},
Keywords-Plus = {CHLOROPHYLL CONTENT; IDENTIFICATION; SHAPE; LEAVES; REFLECTANCE;
   ALGORITHMS; FEATURES; TOOL},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agricultural Engineering},
Author-Email = {jinruxue@nwsuaf.edu.cn
   sfuentes@unimelb.edu.au
   sfuentes@unimelb.edu.au
   claudia.gonzalez@unimelb.edu.au
   Eden.tongson@unimelb.edu.au
   hejuandu@xzmu.edu.cn
   bfs@nwsuaf.edu.cn},
Affiliations = {Northwest A\&F University - China; University of Melbourne; Stellenbosch
   University; Ministry of Agriculture \& Rural Affairs},
ResearcherID-Numbers = {Echeverría, Carlos Poblete/T-1370-2019
   Viejo, Claudia Gonzalez/Q-9007-2019
   Fuentes, Sigfredo/J-6238-2015},
ORCID-Numbers = {Echeverría, Carlos Poblete/0000-0001-8025-5879
   Viejo, Claudia Gonzalez/0000-0001-9207-9307
   Fuentes, Sigfredo/0000-0002-0377-5085},
Funding-Acknowledgement = {National Key Research and Development Program of China
   {[}2017YFD0700402]; Key Science and Technology Program of Shaanxi
   Province, China {[}S2016YFNY0066]; Scientific Research Foundation for
   the Returned Overseas Chinese Scholars, State Education Ministry; Tibet
   Natural Science Foundation-The Study of Tibet Crop Condition Monitoring
   Based on Crop Growth Model and Multi-Source Remote Sensing Data
   {[}2016-ZR-15-18]; Digital Viticulture program - University of
   Melbourne's Networked Society Institute, Australia},
Funding-Text = {This study was supported by the National Key Research and Development
   Program of China (No. 2017YFD0700402), the Key Science and Technology
   Program of Shaanxi Province, China (No. S2016YFNY0066), the Scientific
   Research Foundation for the Returned Overseas Chinese Scholars, State
   Education Ministry, Tibet Natural Science Foundation-The Study of Tibet
   Crop Condition Monitoring Based on Crop Growth Model and Multi-Source
   Remote Sensing Data (2016-ZR-15-18); Part of this research was supported
   by the Digital Viticulture program funded by the University of
   Melbourne's Networked Society Institute, Australia.},
Cited-References = {Barbedo JGA, 2013, SPRINGERPLUS, V2, DOI 10.1186/2193-1801-2-660.
   Backhaus A, 2010, NEW PHYTOL, V187, P251, DOI 10.1111/j.1469-8137.2010.03266.x.
   Borkowski W, 1999, CAN J FOREST RES, V29, P1301, DOI 10.1139/cjfr-29-9-1301.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Chi XL, 2017, BIOL CONSERV, V210, P89, DOI 10.1016/j.biocon.2017.04.015.
   Chitwood DH, 2014, PLANT PHYSIOL, V164, P259, DOI 10.1104/pp.113.229708.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   De Bei R, 2011, AUST J GRAPE WINE R, V17, P62, DOI 10.1111/j.1755-0238.2010.00117.x.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   Edeoga HO, 2005, AFR J BIOTECHNOL, V4, P685, DOI 10.5897/AJB2005.000-3127.
   Foroutan-pour K, 1999, APPL MATH COMPUT, V105, P195, DOI 10.1016/S0096-3003(98)10096-6.
   Fu HD, 2015, J NANOMATER, V2015, DOI 10.1155/2015/473853.
   Fuentes S, 2018, COMPUT ELECTRON AGR, V151, P311, DOI 10.1016/j.compag.2018.06.035.
   Fuentes S, CANADIAN J FOREST RE.
   Gitelson A A, 2004, PRINCIPLES ALGORITHM, P78.
   Gitelson AA, 2003, J PLANT PHYSIOL, V160, P271, DOI 10.1078/0176-1617-00887.
   Gitelson AA, 2002, PHOTOCHEM PHOTOBIOL, V75, P272, DOI 10.1562/0031-8655(2002)075<0272:ACCIPL>2.0.CO;2.
   Jia Y F, 2016, INT J BIOMEDICAL ENG, V39, P222.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   LIEBOVITCH LS, 1989, PHYS LETT A, V141, P386, DOI 10.1016/0375-9601(89)90854-2.
   Pan J, 2008, 2008 INT C COMP SCI.
   Rossatto DR, 2011, PLANT SYST EVOL, V291, P103, DOI 10.1007/s00606-010-0366-2.
   Rzanny M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0245-8.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Vesali F, 2015, COMPUT ELECTRON AGR, V116, P211, DOI 10.1016/j.compag.2015.06.012.
   Viejo CG, 2018, J SCI FOOD AGR, V98, P618, DOI 10.1002/jsfa.8506.
   Viejo CG, 2016, FOOD RES INT, V89, P504, DOI 10.1016/j.foodres.2016.08.045.
   Wang P, 2014, SPECTROSC SPECT ANAL, V34, P58, DOI 10.3964/j.issn.1000-0593(2014)01-0058-06.
   Wei C S, 2017, CHINESE J EXPT TRADI, V23, P25.
   Wu S. G., 2007, IEEE INT S SIGN PROC.
   Yang Zhihui, 2016, MATH PRACTICE THEORY, V46, P170.
   Yuan L, 2014, FIELD CROP RES, V156, P199, DOI 10.1016/j.fcr.2013.11.012.
   Yuan Ming-Yang, 2014, Zhongguo Zhong Yao Za Zhi, V39, P267.
   Zhang H, 2000, CHINESE TRADITIONAL, V22, P101.
   Zhang ShuaiTang, 2017, Transactions of the Chinese Society of Agricultural Engineering, V33, P200.},
Number-of-Cited-References = {36},
Times-Cited = {14},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {19},
Journal-ISO = {Int. J. Agric. Biol. Eng.},
Doc-Delivery-Number = {HU0FX},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000464947900016},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000534427200001,
Author = {Jiang Huixian},
Title = {The Analysis of Plants Image Recognition Based on Deep Learning and
   Artificial Neural Network},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {68828-68841},
Abstract = {Classification and identification of plants are helpful for people to
   effectively understand and protect plants. The leaves of plants are the
   most important recognition organs. With the development of artificial
   intelligence and machine vision technology, plant leaf recognition
   technology based on image analysis is used to improve the knowledge of
   plant classification and protection. Deep learning is the abbreviation
   of deep neural network learning method and belongs to neural network
   structure. It can automatically learn features from big data and use
   artificial neural network based on back propagation algorithm to train
   and classify plant leaf samples. The main content of this paper is to
   extract plant leaf features and identify plant species based on image
   analysis. Firstly, plant leaf images are segmented by various methods,
   and then feature extraction algorithm is used to extract leaf shape and
   texture features from leaf sample images. Then the comprehensive
   characteristic information of plant leaves is formed according to the
   comprehensive characteristic information. In this paper, 50 plant leaf
   databases are tested and compared with KNN-based neighborhood
   classification, Kohonen network based on self-organizing feature mapping
   algorithm and SVM-based support vector machine. At the same time, the
   leaves of 7 different plants were compared and it was found that ginkgo
   leaves were easier to identify. For leaf images under complex
   background, good recognition effect has been achieved. Image samples of
   the test set are input into the learning model to obtain reconstruction
   errors. The class label of the test set can be obtained by
   reconstructing the deep learning model with the smallest error set. The
   results show that this method has the shortest recognition time and the
   highest correct recognition rate.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Jiang, HX (Corresponding Author), Fujian Normal Univ, Sch Geog Sci, Fuzhou 350007, Peoples R China.
   Jiang, HX (Corresponding Author), Fujian Prov Engn Res Ctr Monitoring \& Assessing T, Fuzhou 350007, Peoples R China.
   Jiang Huixian, Fujian Normal Univ, Sch Geog Sci, Fuzhou 350007, Peoples R China.
   Jiang Huixian, Fujian Prov Engn Res Ctr Monitoring \& Assessing T, Fuzhou 350007, Peoples R China.},
DOI = {10.1109/ACCESS.2020.2986946},
ISSN = {2169-3536},
Keywords = {Plant identification; leaf image segmentation; feature extraction;
   artificial neural network; deep learning},
Keywords-Plus = {TEXTURE; COLOR; SHAPE},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {Jhx155@163.com},
Affiliations = {Fujian Normal University},
Funding-Acknowledgement = {Key Projects of National Key Research and Development Plan
   {[}2016YFC0502905]; Key Education Reform Projects of Fujian Normal
   University {[}I201902008]; Natural Science Foundation of Fujian Province
   of China {[}2018J01740]},
Funding-Text = {This work was supported in part by the the Key Projects of National Key
   Research and Development Plan under Grant 2016YFC0502905, in part by the
   Key Education Reform Projects of Fujian Normal University under Grant
   I201902008, and in part by the Natural Science Foundation of Fujian
   Province of China under Grant 2018J01740.},
Cited-References = {Aptoula E, 2013, IEEE IMAGE PROC, P1496, DOI 10.1109/ICIP.2013.6738307.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chakravortty S., 2018, PROGR INTELLIGENT CO.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Dan W., 2019, COMPUT ENG APPL, V55, P129.
   Dewi AK, 2016, 2016 INTERNATIONAL CONFERENCE ON INFORMATICS AND COMPUTING (ICIC), P192, DOI 10.1109/IAC.2016.7905714.
   Guo JM, 2013, J VIS COMMUN IMAGE R, V24, P1360, DOI 10.1016/j.jvcir.2013.09.005.
   Kaneko T, 2018, PROC CVPR IEEE, P6606, DOI 10.1109/CVPR.2018.00691.
   Kzar AA, 2016, INT J ENV RES PUB HE, V13, DOI 10.3390/ijerph13010092.
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
   McLellan T, 2000, BOT J LINN SOC, V132, P79, DOI 10.1111/j.1095-8339.2000.tb01855.x.
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
   Patil P, 2016, PROCEEDINGS OF 2ND IEEE INTERNATIONAL CONFERENCE ON ENGINEERING \& TECHNOLOGY ICETECH-2016, P16, DOI 10.1109/ICETECH.2016.7569183.
   Petkov T., 2016, P 19 INT S EL APP TE, P1.
   Rahali M., 2016, P INT WORKSH SOFT CO, P474.
   Rossatto DR, 2011, PLANT SYST EVOL, V291, P103, DOI 10.1007/s00606-010-0366-2.
   Shanwen Z., 2019, JIANGSU AGR SCI, V47, P255.
   Shanwen Z., 2019, JIANGSU AGR SCI, V47, P273.
   Shengnan R., 2019, JIANGSU J AGR SCI, V47, P1061.
   Smith JR, 1996, P SOC PHOTO-OPT INS, V2670, P426, DOI 10.1117/12.234781.
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487.
   Vatamanu OA, 2013, E-HEALTH BIOENG CONF, DOI 10.1109/EHB.2013.6707396.
   Yang J., 2012, J COMPUT INF SYST, V8, P3369.
   Yang L, 2019, INT J CONTROL AUTOM, V17, P193, DOI 10.1007/s12555-017-0457-8.
   ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949.},
Number-of-Cited-References = {25},
Times-Cited = {30},
Usage-Count-Last-180-days = {7},
Usage-Count-Since-2013 = {37},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {LP6KG},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000534427200001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000389852900051,
Author = {Li, Jia-Xing and Zhang, De-Xiang and Zhang, Jing-Jing and Zhang, Jun and
   Xun, Li-Na and Yan, Qing},
Book-Group-Author = {Destech Publicat Inc},
Title = {Plant Leaf Recognition Based on Small Datasets Using Deep Learning
   Algorithm},
Booktitle = {2016 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND INFORMATION
   SECURITY (CSIS 2016)},
Year = {2016},
Pages = {351-355},
Note = {International Conference on Computer Science and Information Security
   (CSIS), Nanjing, PEOPLES R CHINA, APR 16-17, 2016},
Abstract = {Most of traditional classification methods for plant leaves are
   artificial feature extraction, which are difficult to extract features
   comprehensively and effectively. To achieve better recognition rate, an
   end to end classification method based on convolution neural network
   (CNN) is proposed. We establish a convolution neural network consisting
   five convolution layers, three subsampling layers and three full
   connection layers based on caffe platform. Through set the relevant
   parameters of the CNN, a better recognition rate is achieved.
   Experimental results show that the recognition accuracy of this
   algorithm is as high as 97.50\%. It can be seen that our algorithm has
   more advantages than other traditional classification algorithms.},
Publisher = {DESTECH PUBLICATIONS, INC},
Address = {439 DUKE STREET, LANCASTER, PA 17602-4967 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zhang, DX (Corresponding Author), Anhui Univ, Sch Elect Engn \& Automat, Hefei 230601, Peoples R China.
   Li, Jia-Xing; Zhang, De-Xiang; Zhang, Jing-Jing; Zhang, Jun; Xun, Li-Na; Yan, Qing, Anhui Univ, Sch Elect Engn \& Automat, Hefei 230601, Peoples R China.},
ISBN = {978-1-60595-341-0},
Keywords = {Plant Leaf Recognition; Deep Learning; Convolution Neural Network; Caffe},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Interdisciplinary Applications; Computer
   Science, Theory \& Methods},
Author-Email = {ahuljx@126.com
   dqxyzdx@126.com
   fannyzjj@sina.com
   wwwzhangjun@163.com
   xunlina@126.com
   rubby\_yan5996@sina.com},
Affiliations = {Anhui University},
ResearcherID-Numbers = {liu, jiajia/IUN-0901-2023
   yan, qing/GZN-2287-2022
   liu, jiajia/ISS-0316-2023},
Cited-References = {Abbasi S, 1997, LECT NOTES COMPUT SC, V1252, P284.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   Felzenszwalb P F, 2007, IEEE C COMP VIS PATT, V7, P1.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Soderkvist O, 2001, COMPUTER VISION CLAS.},
Number-of-Cited-References = {5},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BG6AO},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000389852900051},
DA = {2023-08-12},
}

@article{ WOS:000541106600003,
Author = {Unal, Zeynep},
Title = {Smart Farming Becomes Even Smarter With Deep Learning-A Bibliographical
   Analysis},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {105587-105609},
Abstract = {Smart farming is a new concept that makes agriculture more efficient and
   effective by using advanced information technologies. The latest
   advancements in connectivity, automation, and artificial intelligence
   enable farmers better to monitor all procedures and apply precise
   treatments determined by machines with superhuman accuracy. Farmers,
   data scientists and, engineers continue to work on techniques that allow
   optimizing the human labor required in farming. With valuable
   information resources improving day by day, smart farming turns into a
   learning system and becomes even smarter. Deep learning is a type of
   machine learning method, using artificial neural network principles. The
   main feature by which deep learning networks are distinguished from
   neural networks is their depth and that feature makes them capable of
   discovering latent structures within unlabeled, unstructured data. Deep
   learning networks that do not need human intervention while performing
   automatic feature extraction have a significant advantage over previous
   algorithms. The focus of this study is to explore the advantages of
   using deep learning in agricultural applications. This bibliography
   reviews the potential of using deep learning techniques in agricultural
   industries. The bibliography contains 120 papers from the database of
   the Science Citation Index on the subject that were published between
   2016 and 2019. These studies have been retrieved from 39 scientific
   journals. The papers are classified into the following categories as
   disease detection, plant classification, land cover identification,
   precision livestock farming, pest recognition, object recognition, smart
   irrigation, phenotyping, and weed detection.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Unal, Z (Corresponding Author), Nigtas Co, TR-51200 Nigde, Turkey.
   Unal, Zeynep, Nigtas Co, TR-51200 Nigde, Turkey.},
DOI = {10.1109/ACCESS.2020.3000175},
ISSN = {2169-3536},
Keywords = {Machine learning; internet of things; precision agriculture; artificial
   neural networks},
Keywords-Plus = {PLANT-DISEASE; NEURAL-NETWORKS; APPLE DETECTION; CLASSIFICATION;
   RECOGNITION; PREDICTION; IMAGES; IDENTIFICATION; DIAGNOSIS},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {zeynepunal1010@hotmail.com},
ResearcherID-Numbers = {Ünal, Zeynep/ABH-7808-2020},
ORCID-Numbers = {Ünal, Zeynep/0000-0002-9954-1151},
Cited-References = {Altuntas Y, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104874.
   AlZu'bi S, 2019, MULTIMED TOOLS APPL, V78, P29581, DOI 10.1007/s11042-019-7367-0.
   Ampatzidis Y, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11040410.
   {[}Anonymous], 2018, COMPUT IND, DOI DOI 10.1016/j.compind.2018.03.010.
   {[}Anonymous], 2019, J FIELD ROBOT.
   Arad B, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061390.
   Barbedo JGA, 2018, COMPUT ELECTRON AGR, V153, P46, DOI 10.1016/j.compag.2018.08.013.
   Arnpatzidis Y, 2019, COMPUT ELECTRON AGR, V164, DOI 10.1016/j.compag.2019.104900.
   Arredondo-Santoyo M, 2019, SOFT COMPUT, V23, P12799, DOI 10.1007/s00500-019-03832-8.
   Arsenovic M., 2019, SYMMETRY, V11, P1.
   Arsenovic M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11070939.
   Ayaz M, 2019, IEEE ACCESS, V7, P129551, DOI 10.1109/ACCESS.2019.2932609.
   Bah MD, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111690.
   Barth R, 2019, COMPUT ELECTRON AGR, V161, P291, DOI 10.1016/j.compag.2017.11.040.
   Bazzi H, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11151836.
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006.
   Bjerge K, 2019, COMPUT ELECTRON AGR, V164, DOI 10.1016/j.compag.2019.104898.
   Cheng X, 2017, COMPUT ELECTRON AGR, V141, P351, DOI 10.1016/j.compag.2017.08.005.
   Christiansen P., 2016, SENSORS, V16, P2.
   Coulibaly S, 2019, COMPUT IND, V108, P115, DOI 10.1016/j.compind.2019.02.003.
   Cruz A, 2019, COMPUT ELECTRON AGR, V157, P63, DOI 10.1016/j.compag.2018.12.028.
   d'Andrimont R, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081300.
   Deng L., 2014, FOND T SIGN PROC, V7, P197, DOI {[}10.1561/, 10.1561/2000000039, DOI 10.1561/2000000039].
   Despois J., MEMORIZING IS NOT LE.
   Dijkstra K, 2019, MACH VISION APPL, V30, P1, DOI 10.1007/s00138-018-0965-4.
   Ding WG, 2016, COMPUT ELECTRON AGR, V123, P17, DOI 10.1016/j.compag.2016.02.003.
   Dominguez C, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2082-9.
   Dong W, 2019, GEODERMA, V340, P234, DOI 10.1016/j.geoderma.2019.01.018.
   Du ZR, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11070888.
   Dyson J, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11161859.
   Emir S, 2013, THESIS.
   Espejo-Garcia B, 2019, COMPUT ELECTRON AGR, V162, P106, DOI 10.1016/j.compag.2019.03.027.
   Fabijanska A, 2018, COMPUT ELECTRON AGR, V150, P353, DOI 10.1016/j.compag.2018.05.005.
   Feng XP, 2020, PLANT J, V101, P1448, DOI 10.1111/tpj.14597.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Fernandes AM, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104855.
   Ferreira AD, 2017, COMPUT ELECTRON AGR, V143, P314, DOI 10.1016/j.compag.2017.10.027.
   Freeman J.A., 1991, NEURAL NETWORKS ALGO, P18.
   Fuentes A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17092022.
   Fyfe C., 2000, THESIS.
   Gadhiya T, 2018, IEEE GEOSCI REMOTE S, V15, P1720, DOI 10.1109/LGRS.2018.2861081.
   Gaetano R, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111746.
   Ganguly K., 2017, LEARNING GENERATIVE, P25.
   Gao LW, 2019, COMPUT ELECTRON AGR, V164, DOI 10.1016/j.compag.2019.104924.
   Garcia-Pedrero A, 2019, IEEE ACCESS, V7, P158223, DOI 10.1109/ACCESS.2019.2950371.
   Gene-Mola J, 2019, COMPUT ELECTRON AGR, V162, P689, DOI 10.1016/j.compag.2019.05.016.
   Ghosal S, 2018, P NATL ACAD SCI USA, V115, P4613, DOI 10.1073/pnas.1716999115.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Gorczyca MT, 2018, COMPUT ELECTRON AGR, V151, P286, DOI 10.1016/j.compag.2018.06.028.
   Graupe Daniel, 2007, PRINCIPLES ARTIFICIA, P1.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Gulli A., 2017, DEEP LEARNING KERAS, P68.
   Han ZZ, 2019, COMPUT ELECTRON AGR, V164, DOI 10.1016/j.compag.2019.104888.
   Heaton J., 2015, NEURAL NETWORKS DEEP, V3, P165.
   Hu GS, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104852.
   Huang W, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11050606.
   Huang XP, 2019, ANIMALS-BASEL, V9, DOI 10.3390/ani9070470.
   Jiang B, 2019, COMPUT ELECTRON AGR, V166, DOI 10.1016/j.compag.2019.104982.
   JIAO WW, 2019, ACTA MICROSC, V28, P1450.
   Kang HW, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19204599.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Kerkech M, 2018, COMPUT ELECTRON AGR, V155, P237, DOI 10.1016/j.compag.2018.10.006.
   Khan Z, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0287-6.
   Knoll FJ, 2018, COMPUT ELECTRON AGR, V153, P347, DOI 10.1016/j.compag.2018.08.032.
   Koirala A, 2019, PRECIS AGRIC, V20, P1107, DOI 10.1007/s11119-019-09642-0.
   Kounalakis T, 2019, COMPUT ELECTRON AGR, V165, DOI {[}10.1016/j.compag.2019.104973, 10.1016/j.compag.201].
   Krose B., 1996, INTRO NEURAL NETWORK, P15.
   Kussul N, 2020, INT J DIGIT EARTH, V13, P309, DOI 10.1080/17538947.2019.1610807.
   Kussul N, 2017, IEEE GEOSCI REMOTE S, V14, P778, DOI 10.1109/LGRS.2017.2681128.
   Kvam J, 2017, COMPUT ELECTRON AGR, V142, P521, DOI 10.1016/j.compag.2017.11.020.
   Li R., 2019, IEEE ACCESS, V7.
   Li WJ, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11010011.
   Li XY, 2019, COMPUT ELECTRON AGR, V164, DOI 10.1016/j.compag.2019.104885.
   Liang QK, 2019, COMPUT ELECTRON AGR, V157, P518, DOI 10.1016/j.compag.2019.01.034.
   Liu JZ, 2019, COMPUT ELECTRON AGR, V166, DOI 10.1016/j.compag.2019.105012.
   Liu ZZ, 2017, IEEE J-STSP, V11, P479, DOI 10.1109/JSTSP.2017.2679538.
   Lu J, 2017, COMPUT ELECTRON AGR, V142, P369, DOI 10.1016/j.compag.2017.09.012.
   Ma N, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18030693.
   Manzini N., SINGLE HIDDEN LAYER.
   Milella A, 2019, COMPUT ELECTRON AGR, V156, P293, DOI 10.1016/j.compag.2018.11.026.
   Moon T, 2019, COMPUT ELECTRON AGR, V166, DOI 10.1016/j.compag.2019.105023.
   Moshia ME, 2019, ACTA AGR SCAND B-S P, V69, P228, DOI 10.1080/09064710.2018.1536225.
   Nagasubramanian K, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0479-8.
   Nevavuori P, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104859.
   Ogucu M. O., 2006, THESIS.
   Ozkan K, 2019, J SCI FOOD AGR, V99, P4977, DOI 10.1002/jsfa.9732.
   Partel V, 2019, COMPUT ELECTRON AGR, V162, P328, DOI 10.1016/j.compag.2019.04.022.
   Partel V, 2019, COMPUT ELECTRON AGR, V157, P339, DOI 10.1016/j.compag.2018.12.048.
   Patterson J., 2017, DEEP LEARNING PRACTI, P102.
   Pereira DR, 2018, COMPUT ELECTRON AGR, V145, P35, DOI 10.1016/j.compag.2017.12.024.
   Persello C, 2019, REMOTE SENS ENVIRON, V231, DOI 10.1016/j.rse.2019.111253.
   Picon A, 2019, COMPUT ELECTRON AGR, V161, P280, DOI 10.1016/j.compag.2018.04.002.
   Przybylo J, 2019, COMPUT ELECTRON AGR, V156, P490, DOI 10.1016/j.compag.2018.12.001.
   Rahnemoonfar M, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17040905.
   Raschka S., 2017, MACHINE LEARNING DEE, P2.
   Rasmussen CB, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19163506.
   Remya S, 2019, SOFT COMPUT, V23, P8471, DOI 10.1007/s00500-019-03961-0.
   Ringland J, 2019, COMPUT ELECTRON AGR, V158, P36, DOI 10.1016/j.compag.2019.01.014.
   Rong D, 2019, COMPUT ELECTRON AGR, V162, P1001, DOI 10.1016/j.compag.2019.05.019.
   Sa J, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11020266.
   Saggi MK, 2019, COMPUT ELECTRON AGR, V156, P387, DOI 10.1016/j.compag.2018.11.031.
   Schuttelaar-partners.com, SMART FARM IS KEY FU.
   Sciforce, SMART FARM FUT AGR.
   Shahinfar S, 2019, COMPUT ELECTRON AGR, V156, P159, DOI 10.1016/j.compag.2018.11.021.
   Sharma A., UNDERSTANDING ACTIVA.
   Shen YF, 2018, COMPUT ELECTRON AGR, V145, P319, DOI 10.1016/j.compag.2017.11.039.
   Singh UP, 2019, IEEE ACCESS, V7, P43721, DOI 10.1109/ACCESS.2019.2907383.
   Sirsat MS, 2018, COMPUT ELECTRON AGR, V154, P120, DOI 10.1016/j.compag.2018.08.003.
   Skovsen S, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17122930.
   Song XD, 2016, J ARID LAND, V8, P734, DOI 10.1007/s40333-016-0049-0.
   Stewart EL, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11192209.
   Sulistyo SB, 2018, IEEE INTELL SYST, V33, P15, DOI 10.1109/MIS.2018.111144506.
   Sun YK, 2019, J DAIRY SCI, V102, P10140, DOI 10.3168/jds.2018-16164.
   Thenmozhi K, 2019, COMPUT ELECTRON AGR, V164, DOI 10.1016/j.compag.2019.104906.
   Tian MX, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.05.049.
   Tian YN, 2019, COMPUT ELECTRON AGR, V157, P417, DOI 10.1016/j.compag.2019.01.012.
   Too EC, 2019, COMPUT ELECTRON AGR, V161, P272, DOI 10.1016/j.compag.2018.03.032.
   Toussaint M., INTRO OPTIMIZATION S.
   Turkoglu M, 2019, TURK J ELECTR ENG CO, V27, P1636, DOI 10.3906/elk-1809-181.
   Uzal LC, 2018, COMPUT ELECTRON AGR, V150, P196, DOI 10.1016/j.compag.2018.04.024.
   Varghese R, 2018, PROCEEDINGS OF THE 2018 SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS), P645.
   Veeramani B, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2267-2.
   Waldmann P, 2018, GENET SEL EVOL, V50, DOI 10.1186/s12711-018-0439-1.
   Wang DW, 2019, J SCI FOOD AGR, V99, P4524, DOI 10.1002/jsfa.9689.
   Wang G, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/2917536.
   Wang ZD, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041126.
   Wu LH, 2019, COMPUT ELECTRON AGR, V166, DOI 10.1016/j.compag.2019.105002.
   Yamamoto K, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17112557.
   Yang HW, 2019, COMPUT ELECTRON AGR, V162, P739, DOI 10.1016/j.compag.2019.05.003.
   Yang Q, 2019, FIELD CROP RES, V235, P142, DOI 10.1016/j.fcr.2019.02.022.
   Yu Y, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.06.001.
   Yuksel ME, 2018, J INTELL FUZZY SYST, V34, P2273, DOI 10.3233/JIFS-171307.
   Zambrano F, 2018, REMOTE SENS ENVIRON, V219, P15, DOI 10.1016/j.rse.2018.10.006.
   Zhang WY, 2019, COMPUT ELECTRON AGR, V166, DOI 10.1016/j.compag.2019.104985.
   Zhang YQ, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104884.
   Zhang ZQ, 2019, COMPUT ELECTRON AGR, V166, DOI 10.1016/j.compag.2019.104978.
   Zhao S, 2020, GISCI REMOTE SENS, V57, P37, DOI 10.1080/15481603.2019.1658960.
   Zhao Y, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11040399.
   Zheng C, 2018, COMPUT ELECTRON AGR, V147, P51, DOI 10.1016/j.compag.2018.01.023.
   Zheng YY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19051058.
   Zhong LH, 2019, REMOTE SENS ENVIRON, V221, P430, DOI 10.1016/j.rse.2018.11.032.
   Zhou YN, 2019, GISCI REMOTE SENS, V56, P1170, DOI 10.1080/15481603.2019.1628412.
   Zhu LQ, 2017, ORIENT INSECTS, V51, P79, DOI 10.1080/00305316.2016.1252805.},
Number-of-Cited-References = {143},
Times-Cited = {30},
Usage-Count-Last-180-days = {7},
Usage-Count-Since-2013 = {56},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {LZ3DA},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000541106600003},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000412726800076,
Author = {Di Ruberto, Cecilia and Putzu, Lorenzo},
Editor = {Battiato, S and Braz, J},
Title = {A Fast Leaf Recognition Algorithm based on SVM Classifier and High
   Dimensional Feature Vector},
Booktitle = {PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION
   THEORY AND APPLICATIONS (VISAPP), VOL 1},
Year = {2014},
Pages = {601-609},
Note = {9th International Conference on Computer Vision Theory and Applications
   (VISAPP), Lisbon, PORTUGAL, JAN 05-08, 2014},
Organization = {Inst Syst \& Technologies Informat, Control \& Commun; IEEE Comp Soc;
   IEEE Tech Community Visualizat \& Graph},
Abstract = {Plants are fundamental for human beings, so it's very important to
   catalog and preserve all the plants species. Identifying an unknown
   plant species is not a simple task. Automatic image processing
   techniques based on leaves recognition can help to find the best
   features useful for plant representation and classification. Many
   methods present in literature use only a small and complex set of
   features, often extracted from the binary images or the boundary of the
   leaf. In this work we propose a leaf recognition method which uses a new
   features set that incorporates shape, color and texture features. A
   total of 138 features are extracted and used for training a SVM model.
   The method has been tested on Flavia dataset (Wu et al., 2007), showing
   excellent performance both in terms of accuracy that often reaches
   100\%, and in terms of speed, less than a second to process and extract
   features from an image.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Di Ruberto, C (Corresponding Author), Univ Cagliari, Dept Math \& Comp Sci, Via Osped 72, I-09124 Cagliari, Italy.
   Di Ruberto, Cecilia; Putzu, Lorenzo, Univ Cagliari, Dept Math \& Comp Sci, Via Osped 72, I-09124 Cagliari, Italy.},
ISBN = {978-9-8975-8133-5},
Keywords = {Image Analysis; Feature Extraction; Leaf Recognition; Plant
   Classification; Support Vector Machine},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory \& Methods},
Author-Email = {dirubert@unica.it
   lorenzo.putzu@gmail.com},
Affiliations = {University of Cagliari},
ResearcherID-Numbers = {PUTZU, LORENZO/J-5200-2019
   Di Ruberto, Cecilia/G-6915-2014},
ORCID-Numbers = {PUTZU, LORENZO/0000-0001-5361-8793
   Di Ruberto, Cecilia/0000-0003-4641-0307},
Funding-Acknowledgement = {Regione Autonoma della Sardegna (R.A.S.) {[}CRP-17615]; Sardinia
   Regional Government},
Funding-Text = {This work has been funded by Regione Autonoma della Sardegna (R.A.S.)
   Project CRP-17615 DE-NIS: Dataspace Enhancing Next Internet in Sardinia.
   Lorenzo Putzu gratefully acknowledges Sardinia Regional Government for
   the financial support of his PhD scholarship (P.O.R. Sardegna F.S.E.
   Operational Programme of the Autonomous Region of Sardinia, European
   Social Fund 2007-2013 - Axis IV Human Resources, Objective l.3, Line of
   Activity l.3.1.). We wish to thank Wu et al. for having made available
   the Dataset on which we could test our method.},
Cited-References = {Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Cheng SC, 2007, LECT NOTES ARTIF INT, V4570, P834.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   EHSANIRAD A., 2010, INT J COMPUTER SCI I, V8, P78.
   Gonzalez R.C., 2004, DIGITAL IMAGE PROCES.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Huang Lin, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P871, DOI 10.1109/CSSE.2008.1333.
   Arribas JI, 2011, COMPUT ELECTRON AGR, V78, P9, DOI 10.1016/j.compag.2011.05.007.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   Kadir A., 2011, International Journal of Computer Science \& Information Technology, V3, P256, DOI 10.5121/ijcsit.2011.3318.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P82.
   Lee CL, 2006, INT J IMAG SYST TECH, V16, P15, DOI 10.1002/ima.20063.
   Lee K.-B., 2013, INT J BIOSCI BIOTECH, V5, P57, DOI {[}10.14257/ijbsbt.2013.5.5.06, DOI 10.14257/IJBSBT.2013.5.5.06, 10.3724/sp.j.1087.2009.01707, DOI 10.1007/978-94-007-5857-5\_12].
   Liwen Gao, 2010, Proceedings 3rd International Congress on Image and Signal Processing (CISP 2010), P2732, DOI 10.1109/CISP.2010.5647617.
   Liwen Gao, 2010, Proceedings 2010 Sixth International Conference on Natural Computation (ICNC 2010), P1038, DOI 10.1109/ICNC.2010.5582971.
   Machado B. B., 2013, J PHYS C SERIES, V410.
   Man QK, 2008, COMM COM INF SC, V15, P192.
   Pauwels EJ, 2009, ENG APPL ARTIF INTEL, V22, P26, DOI 10.1016/j.engappai.2008.04.017.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Valliammal N., 2011, HYBRID IMAGE SEGMENT, P1, DOI {[}DOI 10.1109/PACC.2011.5978883, DOI 10.1109/PACC2011.5978883].
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Wu Q., 2006, ADV ARTIFICIAL INTEL, V3, P5.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhang SW, 2011, NEUROCOMPUTING, V74, P2284, DOI 10.1016/j.neucom.2011.03.007.
   Zhang XH, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 2, PROCEEDINGS, P773, DOI 10.1109/CISP.2008.88.
   Zulkifli Z., 2011, Proceedings of the 2011 11th International Conference on Hybrid Intelligent Systems (HIS 2011), P430, DOI 10.1109/HIS.2011.6122144.},
Number-of-Cited-References = {28},
Times-Cited = {14},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BI5RD},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000412726800076},
DA = {2023-08-12},
}

@inproceedings{ WOS:000465799100388,
Author = {Yusof, Rubiyah and Khairuddin, Uswah and Rosli, Nenny Ruthfalydia and
   Ghafar, Hafizza Abdul and Azmi, Nik Mohamad Aizuddin Nik and Ahmad,
   Azlin and Khairuddin, Anis Salwa},
Book-Group-Author = {IEEE},
Title = {A Study of Feature Extraction and Classifier Methods for Tropical Wood
   Recognition System},
Booktitle = {PROCEEDINGS OF TENCON 2018 - 2018 IEEE REGION 10 CONFERENCE},
Series = {TENCON IEEE Region 10 Conference Proceedings},
Year = {2018},
Pages = {2034-2039},
Note = {IEEE-Region-10 Conference (IEEE TENCON), IEEE Reg 10, SOUTH KOREA, OCT
   28-31, 2018},
Organization = {IEEE Korea Council; IEIE; KICS; KIEE; KIEES; KIPS},
Abstract = {Tropical wood recognition is a very challenging task due to the lack of
   discriminative features among some species of the wood, and also some
   very discriminative features among inter class species. Moreover, noises
   due to illuminations, or the uncontrolled environment as well as the
   wood features such as the size of pores, the density of pores, etc.,
   which depend very much on the age, weather and other factors,
   contributing to the irregularities of the features. In this paper, we
   explore the use of feature extraction techniques, classification
   techniques for better accuracy of the system. In particular, we explore
   the use of one of the deep learning method residual network based CNN
   (Res-Net), noting the capability of the network to learn the features of
   images and its ability of generalization. Results have shown that good
   feature extraction methods can give a much better accuracy for all the
   datasets tested, and Res-Net performed badly due to lack of data, which
   cause the problem of overfitting.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Yusof, R (Corresponding Author), Univ Teknol Malaysia, MJIIT, Ctr Artificial Intelligence \& Robot CAIRO, Kuala Lumpur, Malaysia.
   Yusof, Rubiyah; Khairuddin, Uswah; Rosli, Nenny Ruthfalydia; Ghafar, Hafizza Abdul; Azmi, Nik Mohamad Aizuddin Nik, Univ Teknol Malaysia, MJIIT, Ctr Artificial Intelligence \& Robot CAIRO, Kuala Lumpur, Malaysia.
   Ahmad, Azlin, Univ Teknol MARA, Fac Comp \& Math Sci, AAEC, Shah Alam, Malaysia.
   Khairuddin, Anis Salwa, Univ Malaya, Dept Elect Engn, Kuala Lumpur, Malaysia.},
ISSN = {2159-3442},
ISBN = {978-1-5386-5457-6},
Keywords = {Wood Recognition System; Wood Species; Classification; Deep Learning;
   Backpropagation Neural Network},
Keywords-Plus = {GENETIC ALGORITHM},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {rubiyah.kl@utm.my
   uswah.kl@utm.my
   nenny.kl@utm.my
   nik-mohamad-aizuddin@yandex.com
   azlin@tmsk.uitm.edu.my
   anissalwa@um.edu.my},
Affiliations = {Universiti Teknologi Malaysia; Universiti Teknologi MARA; Universiti
   Malaya},
ResearcherID-Numbers = {yusof, rubiyah/AAV-9212-2020
   KHAIRUDDIN, ANIS SALWA MOHD/B-5340-2010
   Ahmad, Azlin/ABA-7891-2021
   Khairuddin, Uswah/AAW-1762-2020},
ORCID-Numbers = {KHAIRUDDIN, ANIS SALWA MOHD/0000-0002-9873-4779
   Khairuddin, Uswah/0000-0002-1035-3745},
Funding-Acknowledgement = {Ministry of Education Malaysia {[}600-RMI/PERDANA 513 BESTARI
   (059/2018)]},
Funding-Text = {The authors would like to thank Ministry of Education Malaysia for
   funding this research project through a Research University Grant;
   Bestari Perdana 2018 Grant, project titled ``Modified Clustering
   Algorithm for Analyzing and Visualizing the Structured and Unstructured
   Data{''} (600-RMI/PERDANA 513 BESTARI (059/2018)). Also appreciation
   goes to the Research Management Center (RMC) of UiTM for providing an
   excellent research environment in completing this research work.},
Cited-References = {Abdi Masoud, 2016, ARXIV160905672CS.
   Ahmad A, 2017, 2017 COMPUTING CONFERENCE, P751, DOI 10.1109/SAI.2017.8252180.
   Ahmad A, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY \& INTERNET-BASED SYSTEMS (SITIS), P720, DOI 10.1109/SITIS.2013.117.
   Ahmad A, 2013, ADV INTEL SYS RES, V42, P16.
   {[}Anonymous], 2016, ARXIV160305027.
   {[}Anonymous], 2018, BRIEF BIOINFORM, DOI DOI 10.1093/bib/bbx044.
   Hafizudin Wan Afiq, 2015, NAT C WOOD BAS TECHN.
   Jia YH, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1217, DOI 10.1109/ITSC.2016.7795712.
   Khairuddin A. S. M., 2011, USING 2 STAGE CLASSI.
   Khairuddin U., 2015, 17 INT C EL CONTR AU, P2695.
   Khairuddin U., 2011, ICIC EXPRESS LETT B, V2, P441.
   Lew Y. L., 2005, DESIGN INTELLIGENT W.
   Nadaraj M, 2008, INT J SIMUL SYST SCI, V9, P9.
   Qin Xuejie, 2004, P 2004 IEEE COMP SOC, V1.
   Ren Zhang, 2014, 2014 International Joint Conference on Neural Networks (IJCNN), P366, DOI 10.1109/IJCNN.2014.6889448.
   Rosli N. R., 2008, 2008 STUD C RES DEV.
   Shamsuddin Mohd Razif, 2015, RECENT ADV MATH COMP.
   Tacconi L, 2004, LEARNING LESSONS PRO.
   Teknomo Kardi, DISCR AN TUT.
   Wang Z, 2018, IEEE ACCESS, V6, P17905, DOI 10.1109/ACCESS.2018.2812208.
   Yusof R., 2016, 4 KUAL LUMP INT AGR, P162.
   Yusof R., 2009, 2009 2 INT C.
   YUSOF R, 2009, 2ND INTERNATIONAL CO, P2705.
   Yusof R, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY \& INTERNET-BASED SYSTEMS (SITIS), P737, DOI 10.1109/SITIS.2013.120.
   Yusof R, 2013, COMPUT ELECTRON AGR, V93, P68, DOI 10.1016/j.compag.2013.01.007.
   Yusof R, 2012, INT J INNOV COMPUT I, V8, P7363.
   Yusoff Marina, 2015, INCIEC 2014, P1141.
   Yusoff Marina, 2016, J ENG APPL SCI, V11, P1682.
   2010, 2010 12 INT C COMP, P289.
   2010, 2010 2 INT C COMP, P308, DOI DOI 10.1109/CICSYN.2010.27.},
Number-of-Cited-References = {30},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BM5ZA},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000465799100388},
DA = {2023-08-12},
}

@article{ WOS:000835336100008,
Author = {Dourado Filho, Luciano Araujo and Calumby, Rodrigo Tripodi},
Title = {Data Augmentation policies and heuristics effects over dataset imbalance
   for developing plant identification systems based on Deep Learning: A
   case study},
Journal = {REVISTA BRASILEIRA DE COMPUTACAO APLICADA},
Year = {2022},
Volume = {14},
Number = {2},
Pages = {85-94},
Month = {JUL},
Abstract = {Data augmentation (DA) is a widely known strategy for effectiveness
   improvement in computer vision models such as Deep Convolutional Neural
   Networks (DCNN). Although it enables improving model generalization by
   increasing data diversity, in this work we propose to investigate its
   effects with respect to two different sources of dataset imbalance
   (i.e., Content and Sampling imbalance) in a plant species recognition
   task. We systematically evaluated several techniques to generate the
   augmented datasets used to train the DCNN models that enabled a thorough
   investigation over the effects of DA in terms of imbalance attenuation.
   The results allowed inferring that data augmentation enables mitigating
   the negative effects related to underrepresentation mainly caused by the
   dataset imbalance.},
Publisher = {UNIV PASSO FUNDO},
Address = {CAIXA POSTAL 611, PASSO FUNDO, RS 99001-970, BRAZIL},
Type = {Article},
Language = {English},
Affiliation = {Dourado, LA (Corresponding Author), Univ Feira Santana, Feira De Santana, BA, Brazil.
   Dourado Filho, Luciano Araujo; Calumby, Rodrigo Tripodi, Univ Feira Santana, Feira De Santana, BA, Brazil.},
DOI = {10.5335/rbca.v14i2.13487},
ISSN = {2176-6649},
Keywords = {Data Augmentation; Deep Learning; Plant Recognition},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications},
Author-Email = {lucianoadfilho@ecomp.uefs.br
   rtcalumby@uefs.br},
ORCID-Numbers = {Dourado, Luciano/0000-0002-0507-2201},
Funding-Acknowledgement = {National Council for Scientific and Technological Development (CNPq)
   {[}124989/2021-7]; Quadro(R)},
Funding-Text = {This work was partially supported by the National Council for Scientific
   and Technological Development (CNPq grant no. 124989/2021-7) and a
   Quadro (R) ? P6000 GPU donation by NVIDIA T M Corporation.},
Cited-References = {Beech E, 2017, J SUSTAIN FOREST, V36, P454, DOI 10.1080/10549811.2017.1310049.
   Bonnet P, 2018, MULTIMED SYST APPL, P131, DOI 10.1007/978-3-319-76445-0\_8.
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011.
   Carranza-Rojas J., 2017, PROC TDWG, V1, DOI DOI 10.3897/TDWGPROCEEDINGS.1.20302.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Chatfield K, 2014, Arxiv, DOI arXiv:1405.3531.
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020.
   Dourado Filho L. A., 2021, ANAIS 13 CONGRESSO B, P136.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Glotin, 2018, CLEF WORKING NOTES.
   Goeau H, 2013, P 2 ACM INT WORKSH M, P23, DOI DOI 10.1145/2509896.2509902.
   Goeau H., 2019, CLEF 2019 C LABS EVA.
   Graves SJ, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8020161.
   Haupt J., 2018, CLEF WORKING NOTES.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Pandian JA, 2019, INT CONF ADV COMPU, P199, DOI 10.1109/IACC48062.2019.8971580.
   Pawara P, 2017, LECT NOTES COMPUT SC, V10617, P615, DOI 10.1007/978-3-319-70353-4\_52.
   Pawara P, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P479, DOI 10.5220/0006196204790486.
   Perez L, 2017, Arxiv, DOI {[}arXiv:1712.04621, DOI 10.48550/ARXIV.1712.04621].
   Picek L., 2019, CLEF WORKING NOTES.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Seeland M, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-018-2474-x.
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0.
   Soltis PS, 2017, AM J BOT, V104, P1281, DOI 10.3732/ajb.1700281.
   Sulc M., 2018, CLEF WORKING NOTES.
   Taylor L, 2018, 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P1542, DOI 10.1109/SSCI.2018.8628742.},
Number-of-Cited-References = {29},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Journal-ISO = {Rev. Bras. Comput. Apl.},
Doc-Delivery-Number = {3M3BQ},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000835336100008},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000865969000008,
Author = {Saxena, Deepak Kumar and Jhanwar, Deepak and Gautam, Diwakar},
Editor = {Tiwari, M and Maddila, RK and Garg, AK and Kumar, A and Yupapin, P},
Title = {Classification of Leaf Disease on Using Triangular Thresholding Method
   and Machine Learning},
Booktitle = {OPTICAL AND WIRELESS TECHNOLOGIES, OWT 2020},
Series = {Lecture Notes in Electrical Engineering},
Year = {2022},
Volume = {771},
Pages = {77-88},
Note = {4th International Conference on Optical and Wireless Technologies (OWT),
   Malaviya Natl Inst Technol Jaipur, ELECTR NETWORK, OCT 03-04, 2020},
Abstract = {Plant Leaves recognition and estimation is one of the most problematic
   companies in image processing techniques. This article subtleties
   techniques and methods are investigated to distinguish and estimate the
   severity of the disease caused by growth patterns on leaves of plants
   using threshold triangular strategy. Four set of images obtained from
   several species of plants and proposed analysis led to identify and
   measure the extent of the damage caused by disease-causing organisms to
   the leaves. Experimental research performed using machine learning
   techniques and final classification result being 97\% accuracy.},
Publisher = {SPRINGER-VERLAG SINGAPORE PTE LTD},
Address = {152 BEACH ROAD, \#21-01/04 GATEWAY EAST, SINGAPORE, 189721, SINGAPORE},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Gautam, D (Corresponding Author), Engn Coll, Elect \& Commun Engn, Ajmer, India.
   Saxena, Deepak Kumar; Jhanwar, Deepak; Gautam, Diwakar, Engn Coll, Elect \& Commun Engn, Ajmer, India.},
DOI = {10.1007/978-981-16-2818-4\_8},
ISSN = {1876-1100},
EISSN = {1876-1119},
ISBN = {978-981-16-2818-4; 978-981-16-2817-7},
Keywords = {Division; Thresholding; Picture securing; Leaf sickness; AI},
Research-Areas = {Engineering; Optics; Telecommunications},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Optics; Telecommunications},
Author-Email = {diwakargautam@ecajmer.ac.in},
ResearcherID-Numbers = {Gautam, Diwakar/AAA-7875-2022
   Saxena, Deepak/AAH-7395-2019},
ORCID-Numbers = {Gautam, Diwakar/0000-0002-4355-4383
   Saxena, Deepak/0000-0003-0563-4259},
Cited-References = {Al Bashish D, 2011, DETECTION CLASSIFICA.
   Babu P, 2007, EXPERT ADVISORY SYST.
   Dae Gwan Kim, 2009, International Journal of Agricultural and Biological Engineering, V2, P41, DOI 10.3965/j.issn.1934-6344.2009.03.041-050.
   Doudkin AA, 2007, J RES APPL AGR ENG, V52.
   El Helly M, INTEGRATED IMAGE PRO.
   Hu Chunhua, 2004, Computer Measurement \& Control, V12, P859.
   Jain A.K., 1989, FUNDAMENTALS DIGITAL.
   Kondou H., 2002, AFITA 2002: Asian agricultural information technology \& management. Proceedings of the Third Asian Conference for Information Technology in Agriculture, Beijing, China, 26-28 October, 2002, P586.
   Li ZhouGM, 2009, TRANSCSAE, V2, P213.
   Lu C, 2010, LEAF AREA MEASUREMEN, P580.
   Maliappis KP, 2008, WORLD J AGR SCI, V4, P640.
   Marcon M, 2011, REV BRAS ENG AGR AMB, V15, P96, DOI 10.1590/S1415-43662011000100014.
   Rao KMM, 1996, TRENDS IN NDE SCIENCE AND TECHNOLOGY - PROCEEDINGS OF THE 14TH WORLD CONFERENCE ON NDT (14TH WCNDT), VOLS 1-5, P1219.
   Rao KMM., 1997, INDIAN J TECHNOL, V15.
   Rao KMM., 1995, P WORKSHOP MEDICAL I.
   Siddiqi MH, REAL TIME SPECIFIC W.
   Tzionas P., 2005, 5 INT C ON TECHN AUT, P365.
   Wang KeRu, 2006, Acta Agronomica Sinica, V32, P34.
   Woodford BJ, 1999, P ICONIPANZIISANNES.},
Number-of-Cited-References = {19},
Times-Cited = {1},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BT9ZB},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000865969000008},
DA = {2023-08-12},
}

@inproceedings{ WOS:000457928800017,
Author = {Kwok, Jessica and Sun, Yu},
Book-Group-Author = {ACM},
Title = {A Smart IoT-Based Irrigation System with Automated Plant Recognition
   using Deep Learning},
Booktitle = {PROCEEDINGS OF THE 10TH INTERNATIONAL CONFERENCE ON COMPUTER MODELING
   AND SIMULATION (ICCMS 2018)},
Year = {2017},
Pages = {87-91},
Note = {10th International Conference on Computer Modeling and Simulation
   (ICCMS), Sydney, AUSTRALIA, JAN 08-10, 2018},
Organization = {Int Assoc Comp Sci \& Informat Technol},
Abstract = {Machine Learning allows systems to learn and improve automatically from
   experiences without hand-coding. Thus, in recent years, many technology
   companies have been developing such application if Artificial
   Intelligence, from face recognition by Facebook, to the AlphaGo program
   by Google. The irrigation systems in the market nowadays mostly allow
   users to set them to a certain amount of water and at specific time
   intervals. However, there are usually more than one type of plants in a
   garden, and each species requires different amount of water. In order to
   resolve this issue, in this paper, we have developed an irrigation
   system, with the use of deep learning, that is able to adjust the
   amounts of water foe each type pf plant through plants recognition.
   There are two main parts of the solution, the software and the hardware.
   The prior is connected with cameras to undergo plant recognition, and
   utilizes database to find the suitable amount of water; the latter
   controls the amount of water that is able to flow out.},
Publisher = {ASSOC COMPUTING MACHINERY},
Address = {1515 BROADWAY, NEW YORK, NY 10036-9998 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kwok, J (Corresponding Author), Claremont High Sch, 1601 N Indian Blvd, Claremont, CA 91711 USA.
   Kwok, Jessica, Claremont High Sch, 1601 N Indian Blvd, Claremont, CA 91711 USA.
   Sun, Yu, Calif State Polytech Univ Pomona, Dept Comp Sci, Pomona, CA 91768 USA.},
DOI = {10.1145/3177457.3177506},
ISBN = {978-1-4503-6339-6},
Keywords = {Machine Learning; Irrigation System; Image Classification; Soil Moisture
   Content},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \&
   Electronic},
Author-Email = {jessica.kwok19@gmail.com
   yusun@cpp.edu},
Affiliations = {California State University System; California State Polytechnic
   University Pomona},
Cited-References = {{[}Anonymous], 2017, ARD UN REV3.
   {[}Anonymous], 2017, TYP IRR SYST.
   Demay Vincent, 2014, MEASURE SOIL MOISTUR.
   Docker, 2017, WHAT IS DOCK.
   Law Inc. WP, 2013, MOST COMM PROBL FARM.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Momoh O., 2016, DEEP LEARNING.
   Russakovsky O., 2015, INT J COMPUT VISION, V115, P211.
   Torres AF, 2011, AGR WATER MANAGE, V98, P553, DOI 10.1016/j.agwat.2010.10.012.
   Yu Sun, 2017, TENSORFLOW MOBILE EX.},
Number-of-Cited-References = {10},
Times-Cited = {10},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {9},
Doc-Delivery-Number = {BL9OI},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000457928800017},
DA = {2023-08-12},
}

@article{ WOS:000595472800002,
Author = {Alkhonin, Abdulrahman and Almutairi, Abdulelah and Alburaidi,
   Abdulmajeed and Saudagar, Abdul Khader Jilani},
Title = {Recognition of flowers using convolutional neural networks},
Journal = {INTERNATIONAL JOURNAL OF INTELLIGENT ENGINEERING INFORMATICS},
Year = {2020},
Volume = {8},
Number = {3},
Pages = {186-197},
Abstract = {Every human has curiosity about what's around them. Most of the people
   love the nature and visits different places like parks, flower shows
   etc., with family and children during free time. But due to lack of
   enough knowledge and information it is very difficult to decide which
   flowers are beneficial, non-poisonous and edible to mankind. To solve
   this problem, this work developed a mobile application which capture
   flower images and helps in recognising the flowers and categorise them
   into different categories using deep learning algorithms. This work uses
   a dataset which contains four different flowers (Sunflower, Dandelion,
   Rose, and Tulip) for training purpose and tested with a sample of
   flowers over the trained model. The percentage of overall accuracy
   achieved in recognition of flowers is approximately 83.13\%.},
Publisher = {INDERSCIENCE ENTERPRISES LTD},
Address = {WORLD TRADE CENTER BLDG, 29 ROUTE DE PRE-BOIS, CASE POSTALE 856, CH-1215
   GENEVA, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Saudagar, AKJ (Corresponding Author), Imam Mohammad Ibn Saud Islamic Univ IMSIU, Coll Comp \& Informat Sci, Informat Syst Dept, Riyadh 11432, Saudi Arabia.
   Alkhonin, Abdulrahman; Almutairi, Abdulelah; Alburaidi, Abdulmajeed; Saudagar, Abdul Khader Jilani, Imam Mohammad Ibn Saud Islamic Univ IMSIU, Coll Comp \& Informat Sci, Informat Syst Dept, Riyadh 11432, Saudi Arabia.},
DOI = {10.1504/IJIEI.2020.111246},
ISSN = {1758-8715},
EISSN = {1758-8723},
Keywords = {flower recognition; deep learning; keras; mobile application;
   TensorFlow; convolutional neural networks; accuracy; scalability;
   computer vision; image processing},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications},
Author-Email = {abdulrahman.s.alkhonin@gmail.com
   abdulelah.dnd@gmail.com
   majeeed96@gmail.com
   aksaudagar@imamu.edu.sa},
Affiliations = {Imam Mohammad Ibn Saud Islamic University (IMSIU)},
ResearcherID-Numbers = {Saudagar, Abdul/HHY-9939-2022},
Cited-References = {Abd El Rahman S, 2017, INT J INTELL ENG INF, V5, P167, DOI 10.17148/IJIREEICE.2017.5431.
   Affonso C, 2017, EXPERT SYST APPL, V85, P114, DOI 10.1016/j.eswa.2017.05.039.
   Al Shammary NM, 2015, INT J ADV COMPUT SC, V6, P49.
   Ali MAS, 2017, INT J INTELL ENG INF, V5, P327, DOI 10.1504/IJIEI.2017.087938.
   {[}Anonymous], 2018, SMART ID PLANT INS D.
   Charrada A, 2018, INT J INTELL ENG INF, V6, P295, DOI 10.1504/IJIEI.2018.091869.
   Dwivedi P., 2019, MEDIUM.
   Garden Answers, FREE PLANT ID GARD A.
   Gogul I., 2017, 4 INT C SIGN PROC CO, P1, DOI DOI 10.1109/ICSCN.2017.8085675.
   Huddar MG, 2020, INT J INTELL ENG INF, V8, P1.
   Jyothi V. K., 2018, Procedia Computer Science, V132, P1533, DOI 10.1016/j.procs.2018.05.117.
   Khatir N, 2019, INT J INTELL ENG INF, V7, P323.
   Liu D, 2018, PATTERN RECOGN LETT, V110, P16, DOI 10.1016/j.patrec.2018.03.015.
   Petrushevski S., 2018, KERAS RESNET 50.
   Rashaideh H, 2019, INT J INTELL ENG INF, V7, P399, DOI 10.1504/IJIEI.2019.103623.
   ReQtest, 2019, WHAT IS WAT METH LON.
   Singh AK, 2018, TRENDS PLANT SCI, V23, P883, DOI 10.1016/j.tplants.2018.07.004.
   TensorFlow, 2020, TENSORFLOW LIT CONV.
   TensorFlow, 2020, KER TENSORFLOW COR.
   van Lamsweerde A., 2009, REQUIREMENTS ENG SYS.
   Wang T, 2020, PHYTOKEYS, P1, DOI 10.3897/phytokeys.161.54912.
   Zawbaa HM, 2014, 2014 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P895, DOI 10.1109/ICACCI.2014.6968612.},
Number-of-Cited-References = {22},
Times-Cited = {6},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {8},
Journal-ISO = {Int. J. Intell. Eng. Inform.},
Doc-Delivery-Number = {PA2OI},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000595472800002},
DA = {2023-08-12},
}

@inproceedings{ WOS:000518826800005,
Author = {Beikmohammadi, Ali and Faez, Karim},
Editor = {Taheri, H},
Title = {Leaf Classification for Plant Recognition with Deep Transfer Learning},
Booktitle = {2018 4TH IRANIAN CONFERENCE ON SIGNAL PROCESSING AND INTELLIGENT SYSTEMS
   (ICSPIS)},
Year = {2018},
Pages = {21-26},
Note = {4th Iranian Conference of Signal Processing and Intelligent Systems
   (ICSPIS), Amirkabir Univ Technol, Tehran, IRAN, DEC 25-27, 2018},
Organization = {IEEE; Ctr Excellence Digital Proc Syst},
Abstract = {Plant recognition systems that developed by computer vision researchers,
   help botanists in faster recognition and detection of unknown plant
   species. Until now, multiple studies focused on the process or
   algorithms that maximize use of botanical datasets for plants prediction
   modeling, but this method depends on leaf characteristics which can be
   changed with botanical data and different feature extraction techniques.
   On the other hand, recently, due to the popularity and successful
   implementation of deep learning-based methods in various areas such as
   image classification, object detection, and speech recognition, the
   researchers directed from traditional feature-based methods to deep
   learning. In this research, one more efficient method presented that use
   transfer learning to recognize plant for leaf classification, which
   first uses a pre-trained deep neural network model for learning useful
   leaf characteristics directly from the input data representation. Then
   use a logistic regression classifier for leaf classification. It is seen
   that transfer learning from a large dataset to limited botanical dataset
   in plant recognition task is well done. The proposed method is evaluated
   on two well-known botanical datasets, i.e., Flavia with 32 classes and
   Leafsnap with 184 classes, and has succeeded in achieving an accuracy of
   99.6\% and 90.54\%, respectively. The results show that despite the
   large change in the number of classes in these two datasets, the
   proposed method, have a good performance and show the better result than
   methods based on the hand-crafted feature and other methods based on the
   deep learning in terms of memory and precision.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Beikmohammadi, A (Corresponding Author), Amirkabir Univ Technol, Dept Elect Engn, Tehran Polytech, Tehran, Iran.
   Beikmohammadi, Ali; Faez, Karim, Amirkabir Univ Technol, Dept Elect Engn, Tehran Polytech, Tehran, Iran.},
ISBN = {978-1-7281-1194-0},
Keywords = {leaf classification; plant recognition; deep learning; transfer
   learning; MobileNet},
Keywords-Plus = {AUTOMATIC CLASSIFICATION; TEXTURE; SHAPE; VENATION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic},
Author-Email = {dr.abm@aut.ac.ir
   kfaez@aut.ac.ir},
Affiliations = {Amirkabir University of Technology},
ResearcherID-Numbers = {Beikmohammadi, Ali/ABE-4519-2021
   faez, karim/K-5117-2019},
ORCID-Numbers = {faez, karim/0000-0002-1159-4866},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   {[}Anonymous], 2013, ARXIV14014447.
   Azizpour Hossein, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P36, DOI 10.1109/CVPRW.2015.7301270.
   Backes AR, 2009, LECT NOTES COMPUT SC, V5716, P143, DOI 10.1007/978-3-642-04146-4\_17.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Bux A., 2017, HUMAN ACTION RECOGNI.
   Cao XB, 2013, NEUROCOMPUTING, V100, P51, DOI 10.1016/j.neucom.2011.12.043.
   Chaki J., 2011, INT J ADV COMPUTER S, V2.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Charters J., 2014, 2014 IEEE INT C MULT, P1, DOI {[}10.1109/ICMEW.2014.6890557, DOI 10.1109/ICMEW.2014.6890557].
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76.
   Chiu T.-H., 2014, ARXIV14094127.
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   Clarke J, 2006, LECT NOTES COMPUT SC, V4292, P427.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6474, P135, DOI 10.1007/978-3-642-17688-3\_14.
   Fei-Fei L., 2006, P INT C DEV LEARN IC, P11.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Girshick R., 2014, PROC IEEE C COMPUT V, P580, DOI DOI 10.1109/CVPR.2014.81.
   Gray Douglas, 2007, P IEEE INT WORKSHOP, V3, P1.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   Howard A.G., 2017, ABS170404861 CORR.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
   Mouine S., 2012, P 2 ACM INT C MULT R, P49.
   NAM H, 2016, PROC CVPR IEEE, P4293, DOI DOI 10.1109/CVPR.2016.465.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178.
   Olsen A., 2015, 2015 INT C DIG IM CO, P1, DOI {[}10.1109/DICTA.2015.7371274, DOI 10.1109/DICTA.2015.7371274].
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222.
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131.
   Sargano AB, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7010110.
   Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1609/AAAI.V31I1.11231.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Wu D., 2012, 2012 IEEE COMP SOC C, P7, DOI DOI 10.1109/CVPRW.2012.6239179.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zhiyu Liu, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9226, P115, DOI 10.1007/978-3-319-22186-1\_11.
   Zhu F, 2016, IMAGE VISION COMPUT, V55, P42, DOI 10.1016/j.imavis.2016.06.007.
   Zhu J., 2017, IEEE T CIRCUITS SYST.},
Number-of-Cited-References = {56},
Times-Cited = {10},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {9},
Doc-Delivery-Number = {BO5TZ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000518826800005},
DA = {2023-08-12},
}

@article{ WOS:000669498400003,
Author = {Cao, Shuai and Song, Biao},
Title = {Visual attentional-driven deep learning method for flower recognition},
Journal = {MATHEMATICAL BIOSCIENCES AND ENGINEERING},
Year = {2021},
Volume = {18},
Number = {3},
Pages = {1981-1991},
Abstract = {As a typical fine-grained image recognition task, flower category
   recognition is one of the most popular research topics in the field of
   computer vision and forestry informatization. Although the image
   recognition method based on Deep Convolutional Neural Network (DCNNs)
   has achieved acceptable performance on natural scene image, there are
   still shortcomings such as lack of training samples, intra-class
   similarity and low accuracy in flowers category recognition. In this
   paper, we study deep learning-based flowers' category recognition
   problem, and propose a novel attention-driven deep learning model to
   solve it. Specifically, since training the deep learning model usually
   requires massive training samples, we perform image augmentation for the
   training sample by using image rotation and cropping. The augmented
   images and the original image are merged as a training set. Then,
   inspired by the mechanism of human visual attention, we propose a visual
   attention-driven deep residual neural network, which is composed of
   multiple weighted visual attention learning blocks. Each visual
   attention learning block is composed by a residual connection and an
   attention connection to enhance the learning ability and discriminating
   ability of the whole network. Finally, the model is training in the
   fusion training set and recognize flowers in the testing set. We verify
   the performance of our new method on public Flowers 17 dataset and it
   achieves the recognition accuracy of 85.7\%.},
Publisher = {AMER INST MATHEMATICAL SCIENCES-AIMS},
Address = {PO BOX 2604, SPRINGFIELD, MO 65801-2604, UNITED STATES},
Type = {Article},
Language = {English},
Affiliation = {Song, B (Corresponding Author), Nanjing Univ Informat Sci \& Technol, Nanjing 210044, Peoples R China.
   Cao, Shuai, Lanzhou Univ, Sch Informat Sci \& Engn, Lanzhou 730000, Peoples R China.
   Song, Biao, Nanjing Univ Informat Sci \& Technol, Nanjing 210044, Peoples R China.},
DOI = {10.3934/mbe.2021103},
ISSN = {1547-1063},
EISSN = {1551-0018},
Keywords = {deep learning; feature extraction; attention learning; flower
   recognition},
Research-Areas = {Mathematical \& Computational Biology},
Web-of-Science-Categories  = {Mathematical \& Computational Biology},
Author-Email = {caosh18@lzu.edu.cn},
Affiliations = {Lanzhou University; Nanjing University of Information Science \&
   Technology},
Funding-Acknowledgement = {Startup Foundation for Introducing Talent of Nanjing University of
   Information Science and Technology {[}2019r030]},
Funding-Text = {This work was supported by Startup Foundation for Introducing Talent of
   Nanjing University of Information Science and Technology (Grant
   No.2019r030).},
Cited-References = {Cheng G, 2018, IEEE T GEOSCI REMOTE, V56, P6712, DOI 10.1109/TGRS.2018.2841823.
   Cheng YW, 2020, KNOWL-BASED SYST, V210, DOI 10.1016/j.knosys.2020.106488.
   Cheng YW, 2020, NEURAL NETWORKS, V125, P313, DOI 10.1016/j.neunet.2020.02.002.
   Cibuk M, 2019, MEASUREMENT, V137, P7, DOI 10.1016/j.measurement.2019.01.041.
   Fu L., 2020, IEEE T NEURAL NETWOR.
   Fu L., 2020, IEEE T GEOSCI ELECT.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123.
   Kim B, 2018, J COASTAL RES, P1176, DOI 10.2112/SI85-236.1.
   Li Kang, 2018, Journal of Forestry Engineering, V3, P16.
   Lin M., ARXIV13124400.
   Liu Ying, 2019, Journal of Forestry Engineering, V4, P115.
   Nilsback M. E., 2006, 2006 IEEE COMP SOC C.
   Pereira DR, 2018, COMPUT ELECTRON AGR, V145, P35, DOI 10.1016/j.compag.2017.12.024.
   Ravanelli M, 2019, INT CONF ACOUST SPEE, P6465, DOI 10.1109/ICASSP.2019.8683713.
   Sercu T, 2016, INT CONF ACOUST SPEE, P4955, DOI 10.1109/ICASSP.2016.7472620.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Wachinger C, 2018, NEUROIMAGE, V170, P434, DOI 10.1016/j.neuroimage.2017.02.035.
   Ye Q., 2019, J FOR ENG, V4.
   Ye QL, 2019, IEEE T NEUR NET LEAR, V30, P3818, DOI 10.1109/TNNLS.2019.2944869.
   Ye QL, 2018, IEEE T NEUR NET LEAR, V29, P4494, DOI 10.1109/TNNLS.2017.2749428.
   Ye QL, 2018, IEEE T CIRC SYST VID, V28, P114, DOI 10.1109/TCSVT.2016.2596158.
   Zhang JJ, 2018, J VIS COMMUN IMAGE R, V55, P640, DOI 10.1016/j.jvcir.2018.07.011.
   Zhang SS, 2018, PROC CVPR IEEE, P6995, DOI 10.1109/CVPR.2018.00731.
   Zhu HY, 2018, MULTIMED TOOLS APPL, V77, P29779, DOI 10.1007/s11042-017-5578-9.},
Number-of-Cited-References = {25},
Times-Cited = {2},
Usage-Count-Last-180-days = {17},
Usage-Count-Since-2013 = {57},
Journal-ISO = {Math. Biosci. Eng.},
Doc-Delivery-Number = {TD7JM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000669498400003},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000407567300001,
Author = {Carranza-Rojas, Jose and Goeau, Herve and Bonnet, Pierre and
   Mata-Montero, Erick and Joly, Alexis},
Title = {Going deeper in the automated identification of Herbarium specimens},
Journal = {BMC EVOLUTIONARY BIOLOGY},
Year = {2017},
Volume = {17},
Pages = {1-14},
Month = {AUG 11},
Abstract = {Background: Hundreds of herbarium collections have accumulated a
   valuable heritage and knowledge of plants over several centuries. Recent
   initiatives started ambitious preservation plans to digitize this
   information and make it available to botanists and the general public
   through web portals. However, thousands of sheets are still unidentified
   at the species level while numerous sheets should be reviewed and
   updated following more recent taxonomic knowledge. These annotations and
   revisions require an unrealistic amount of work for botanists to carry
   out in a reasonable time. Computer vision and machine learning
   approaches applied to herbarium sheets are promising but are still not
   well studied compared to automated species identification from leaf
   scans or pictures of plants in the field.
   Results: In this work, we propose to study and evaluate the accuracy
   with which herbarium images can be potentially exploited for species
   identification with deep learning technology. In addition, we propose to
   study if the combination of herbarium sheets with photos of plants in
   the field is relevant in terms of accuracy, and finally, we explore if
   herbarium images from one region that has one specific flora can be used
   to do transfer learning to another region with other species; for
   example, on a region under-represented in terms of collected data.
   Conclusions: This is, to our knowledge, the first study that uses deep
   learning to analyze a big dataset with thousands of species from
   herbaria. Results show the potential of Deep Learning on herbarium
   species identification, particularly by training and testing across
   different datasets from different herbaria. This could potentially lead
   to the creation of a semi, or even fully automated system to help
   taxonomists and experts with their annotation, classification, and
   revision works.},
Publisher = {BMC},
Address = {CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Carranza-Rojas, J (Corresponding Author), Costa Rica Inst Technol, Cartago, Costa Rica.
   Carranza-Rojas, Jose; Mata-Montero, Erick, Costa Rica Inst Technol, Cartago, Costa Rica.
   Goeau, Herve; Bonnet, Pierre, CIRAD Amap, Montpellier, France.
   Joly, Alexis, INRIA, Montpellier, France.},
DOI = {10.1186/s12862-017-1014-z},
Article-Number = {181},
ISSN = {1471-2148},
Keywords = {Biodiversity informatics; Computer vision; Deep learning; Plant
   identification; Herbaria},
Keywords-Plus = {DIGITIZATION; IMAGES},
Research-Areas = {Evolutionary Biology; Genetics \& Heredity},
Web-of-Science-Categories  = {Evolutionary Biology; Genetics \& Heredity},
Author-Email = {jcarranza@itcr.ac.cr},
Affiliations = {Instituto Tecnologico de Costa Rica; CIRAD; Universite de Montpellier;
   Inria},
ResearcherID-Numbers = {joly, alexis/AAV-3101-2021
   Bonnet, Pierre/AAG-6819-2020
   },
ORCID-Numbers = {joly, alexis/0000-0002-2161-9940
   Bonnet, Pierre/0000-0002-2828-4389
   Mata-Montero, Erick/0000-0001-5471-164X},
Funding-Acknowledgement = {Costa Rica Institute of Technology},
Funding-Text = {This work was partially supported by the Costa Rica Institute of
   Technology.},
Cited-References = {Bebber DP, 2010, P NATL ACAD SCI USA, V107, P22169, DOI 10.1073/pnas.1011841108.
   Casanova D, 2009, INT J IMAG SYST TECH, V19, P236, DOI 10.1002/ima.20201.
   Cerutti G, 2013, COMPUT VIS IMAGE UND, V117, P1482, DOI 10.1016/j.cviu.2013.07.003.
   Corney DPA, 2012, TAXON, V61, P231, DOI 10.1002/tax.611016.
   Duckworth WD, 1993, SOC PRESERVATION NAT.
   Ellwood ER, 2015, BIOSCIENCE, V65, P383, DOI 10.1093/biosci/biv005.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Goeau H, 2013, P 2 ACM INT WORKSH M, P23, DOI DOI 10.1145/2509896.2509902.
   Goeau H, 2015, CLEF2015.
   Goeau H., 2016, WORKING NOTES CLEF 2, P428.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Ioffe S., 2015, INT C MACHINE LEARNI, DOI DOI 10.1007/S13398-014-0173-7.2.
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Joly A, 2016, P 2016 ACM MULT C, P958, DOI DOI 10.1145/2964284.2976762.
   Joly A, 2016, MULTIMEDIA SYST, V22, P751, DOI 10.1007/s00530-015-0462-9.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   LeCun Y., 1995, CONVOLUTIONAL NETWOR, P255, DOI {[}10.5555/303568.303704, DOI 10.5555/303568.303704].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Mata-Montero E., 2016, PROC IFIP WORLD INFO, P26, DOI {[}10.1007/978-3-319-44447-5\_3, DOI 10.1007/978-3-319-44447-5\_3].
   Mata-Montero E, 2015, PROC LAT AM COMPUT C, P41.
   MONONEN T, 2014, BIODIVERS INFORM, V9, P18.
   NICOLSON N, 2017, BMC EVOL BIOL, V17.
   Page LM, 2015, BIOSCIENCE, V65, P841, DOI 10.1093/biosci/biv104.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Suhrbier L, 2017, DATABASE-OXFORD, P1, DOI 10.1093/database/bax018.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Thiers B, 2020, INDEX HERBARIORUM GL, DOI DOI 10.1093/nar/22.22.4673.
   Thiers BM, 2016, BRITTONIA, V68, P324, DOI 10.1007/s12228-016-9423-7.
   Tomaszewski D, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0153071.
   Tschope O, 2013, TAXON, V62, P1248, DOI 10.12705/626.4.
   Unger J, 2016, BMC EVOL BIOL, V16, DOI 10.1186/s12862-016-0827-5.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wijesingha D., 2011, Tropical Agricultural Research, V23, P42.
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197.},
Number-of-Cited-References = {40},
Times-Cited = {114},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {47},
Journal-ISO = {BMC Evol. Biol.},
Doc-Delivery-Number = {FD5JS},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000407567300001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000751661100001,
Author = {Ariyapadath, Sujith},
Title = {Plant Leaf Classification and Comparative Analysis of Combined Feature
   Set Using Machine Learning Techniques},
Journal = {TRAITEMENT DU SIGNAL},
Year = {2021},
Volume = {38},
Number = {6},
Pages = {1587-1598},
Month = {DEC},
Abstract = {The main purpose of this research work is to apply machine learning and
   image processing techniques for plant classification efficiently. In the
   plant classification system, the conventional method is time-consuming
   and needs to apply expensive analytical instruments. The automated plant
   classification system helps to predict plant classes easily. The most
   challenging part of the automated plant classification research is to
   extract unique features of leaves. This paper proposes a plant
   classification model using an optimal feature set with combined
   features. The proposed model is used to extract features from leaf
   images and applied to image classification algorithms. After the
   evaluation process, it is found that GIST, Local Binary Pattern and
   Pyramid Histogram Oriented Gradient have better results than others in
   this particular application. Combined these three features extraction
   techniques and selected the optimal feature set through Neighbourhood
   Component Analysis. The optimal feature set helps classify plants with
   maximum accuracy in minimal time. Here performed an extensive
   experimental comparison of the proposed optimal feature set and other
   feature extraction methods using different classifiers and tested on
   different data sets (Swedish Leaves, Flavia, D-Leaf). The results
   confirm that this optimal feature set with NCA using ANN classifier
   leads to better classification achieved 98.99\% accuracy in 353.39
   seconds.},
Publisher = {INT INFORMATION \& ENGINEERING TECHNOLOGY ASSOC},
Address = {\#2020, SCOTIA PLACE TOWER ONE, 10060 JASPER AVE, EDMONTON, AB T5J 3R8,
   CANADA},
Type = {Article},
Language = {English},
Affiliation = {Ariyapadath, S (Corresponding Author), Univ Kerala, Dept Comp Sci, Res Ctr, Thiruvananthapuram 695581, Kerala, India.
   Ariyapadath, Sujith, Univ Kerala, Dept Comp Sci, Res Ctr, Thiruvananthapuram 695581, Kerala, India.},
DOI = {10.18280/ts.380603},
ISSN = {0765-0019},
EISSN = {1958-5608},
Keywords = {plant classification; optimal feature set; GIST; local binary pattern;
   pyramid; histogram oriented gradient; machine; learning; neighbourhood
   component; analysis; artificial neural network},
Keywords-Plus = {MEDICINAL-PLANTS; RECOGNITION; SHAPE; SELECTION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic},
Author-Email = {sujithmail.dcb@keralauniversity.ac.in},
Affiliations = {University of Kerala},
ResearcherID-Numbers = {A, Sujith/ISU-7427-2023},
Cited-References = {Afifi A, 2021, PLANTS-BASEL, V10, DOI 10.3390/plants10010028.
   Ahmed N., 2021, ARXIV210204515.
   Ali R, 2018, PROC NAECON IEEE NAT, P431, DOI 10.1109/NAECON.2018.8556785.
   Amirshahi SA, 2012, PROC SPIE, V8291, DOI 10.1117/12.911973.
   {[}Anonymous], 2014, THE J.
   Ashwinkumar S., 2021, MATER TODAY-PROC.
   Begue A, 2017, INT J ADV COMPUT SC, V8, P166.
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
   Ding C, 2003, PROCEEDINGS OF THE 2003 IEEE BIOINFORMATICS CONFERENCE, P523, DOI 10.1109/CSB.2003.1227396.
   Ellis B., 2009, MANUAL LEAF ARCHITEC, DOI {[}DOI 10.1079/9781845935849.0000, 10.1079/9781845935849.0000].
   Hamuda E, 2016, COMPUT ELECTRON AGR, V125, P184, DOI 10.1016/j.compag.2016.04.024.
   Hewitt C., 2018, ARXIV PREPRINT ARXIV, V1811, P08398, DOI {[}10.48550/arXiv.1811.08398, DOI 10.48550/ARXIV.1811.08398].
   Kan H. X., 2017, Pattern Recognition and Image Analysis, V27, P581, DOI 10.1134/S105466181703018X.
   Keivani M, 2020, TRAIT SIGNAL, V37, P17, DOI 10.18280/ts.370103.
   Kheirkhah FM, 2019, IET COMPUT VIS, V13, P369, DOI 10.1049/iet-cvi.2018.5028.
   Kuang HL, 2018, CHIN CONT DECIS CONF, P3341, DOI 10.1109/CCDC.2018.8407701.
   Kurmi Y, 2021, SIGNAL IMAGE VIDEO P, V15, P589, DOI 10.1007/s11760-020-01780-7.
   Manit J., 2011, 2011 6th International Conference on Broadband and Biomedical Communications (IB2Com), P86, DOI 10.1109/IB2Com.2011.6217897.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Mythili C., 2011, RES B JORDAN ACM, V2, P41.
   Narayan V, 2014, INT ARAB J INF TECHN, V11, P447.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Nigam S, 2020, INDIAN J AGR SCI, V90, P249.
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724.
   Pacifico Luciano D. S., 2019, 2019 8th Brazilian Conference on Intelligent Systems (BRACIS). Proceedings, P741, DOI 10.1109/BRACIS.2019.00133.
   Pal M, 2003, INT GEOSCI REMOTE SE, P3510.
   Prakasa E., 2016, J INKOM, V9, P45, DOI 10.14203/j.inkom.420.
   Robnik-Sikonja M, 2003, MACH LEARN, V53, P23, DOI 10.1023/A:1025667309714.
   Rzanny M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0245-8.
   Salve P., 2018, INT S SIGN PROC INT, P15, DOI {[}10.1007/978-981-13-5758-9\_2, DOI 10.1007/978-981-13-5758-9\_2].
   Sekeroglu B, 2016, PROCEDIA COMPUT SCI, V102, P578, DOI 10.1016/j.procs.2016.09.445.
   Shrivastava VK, 2021, J PLANT PATHOL, V103, P17, DOI 10.1007/s42161-020-00683-3.
   Soderkvist O., 2001, THESIS.
   Soofivand MA, 2014, 2014 4TH INTERNATIONAL CONFERENCE ON COMPUTER AND KNOWLEDGE ENGINEERING (ICCKE), P559, DOI 10.1109/ICCKE.2014.6993395.
   Sujith A., 2021, Inventive Communication and Computational Technologies. Proceedings of ICICCT 2020. Lecture Notes in Networks and Systems (LNNS 145), P269, DOI 10.1007/978-981-15-7345-3\_22.
   Tan J.W., 2018, D LEAF DATASET, DOI {[}10.6084/m9.figshare.5732955.v1, DOI 10.6084/M9.FIGSHARE.5732955.V1].
   Taslim A., 2021, B ELECT ENG ANDINFOR, V10, P3341.
   Vi Nguyen Thanh Le, 2019, Information Processing in Agriculture, V6, P116, DOI 10.1016/j.inpa.2018.08.002.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yang CZ, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107809.
   Yang CZ, 2016, FRONT ARTIF INTEL AP, V285, P269, DOI 10.3233/978-1-61499-672-9-269.
   Zhang SC, 2018, IEEE T NEUR NET LEAR, V29, P1774, DOI 10.1109/TNNLS.2017.2673241.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {44},
Times-Cited = {4},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {8},
Journal-ISO = {Trait. Signal},
Doc-Delivery-Number = {YT9GG},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000751661100001},
OA = {Bronze},
DA = {2023-08-12},
}

@inproceedings{ WOS:000627321900012,
Author = {Santos, Luis and Santos, Filipe N. and Oliveira, Paulo Moura and Shinde,
   Pranjali},
Editor = {Silva, MF and Lima, JL and Reis, LP and Sanfeliu, A and Tardioli, D},
Title = {Deep Learning Applications in Agriculture: A Short Review},
Booktitle = {FOURTH IBERIAN ROBOTICS CONFERENCE: ADVANCES IN ROBOTICS, ROBOT 2019,
   VOL 1},
Series = {Advances in Intelligent Systems and Computing},
Year = {2020},
Volume = {1092},
Pages = {139-151},
Note = {4th Iberian Robotics Conference (Robot) - Advances in Robotics, Porto,
   PORTUGAL, NOV 20-22, 2019},
Organization = {Portuguese Soc Robot; Spanish Soc Res \& Dev Robot},
Abstract = {Deep learning (DL) incorporates a modern technique for image processing
   and big data analysis with large potential. Deep learning is a recent
   tool in the agricultural domain, being already successfully applied to
   other domains. This article performs a survey of different deep learning
   techniques applied to various agricultural problems, such as disease
   detection/identification, fruit/plants classification and fruit counting
   among other domains. The paper analyses the specific employed models,
   the source of the data, the performance of each study, the employed
   hardware and the possibility of real-time application to study eventual
   integration with autonomous robotic platforms. The conclusions indicate
   that deep learning provides high accuracy results, surpassing, with
   occasional exceptions, alternative traditional image processing
   techniques in terms of accuracy.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Santos, L (Corresponding Author), INESC TEC INESC Technol \& Sci, Porto, Portugal.
   Santos, L (Corresponding Author), UTAD Univ Tras Os Montes \& Alto Douro, Vila Real, Portugal.
   Santos, Luis; Santos, Filipe N.; Oliveira, Paulo Moura; Shinde, Pranjali, INESC TEC INESC Technol \& Sci, Porto, Portugal.
   Santos, Luis; Oliveira, Paulo Moura, UTAD Univ Tras Os Montes \& Alto Douro, Vila Real, Portugal.},
DOI = {10.1007/978-3-030-35990-4\_12},
ISSN = {2194-5357},
EISSN = {2194-5365},
ISBN = {978-3-030-35990-4; 978-3-030-35989-8},
Keywords = {Deep learning; Agriculture; Image processing; Survey},
Keywords-Plus = {NEURAL-NETWORKS; WEED CLASSIFICATION; RECOGNITION},
Research-Areas = {Automation \& Control Systems; Computer Science; Engineering; Robotics},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Artificial
   Intelligence; Engineering, Electrical \& Electronic; Robotics},
Author-Email = {luis.c.santos@inesctec.pt
   fbnsantos@inesctec.pt
   oliveira@utad.pt
   pranjali.shinde@inesctec.pt},
Affiliations = {INESC TEC; University of Tras-os-Montes \& Alto Douro},
ResearcherID-Numbers = {Oliveira, Paulo Moura/N-1816-2013
   Baptista Neves dos Santos, Filipe/J-2662-2015
   },
ORCID-Numbers = {Oliveira, Paulo Moura/0000-0003-4283-1243
   Baptista Neves dos Santos, Filipe/0000-0002-8486-6113
   Santos, Luis/0000-0002-0255-5005},
Funding-Acknowledgement = {European Regional Development Fund (ERDF) through the Interreg V-A
   Espanha-Portugal Programme (POCTEP) 2014-2020 {[}0095\_BIOTECFOR\_1\_P];
   ERDF European Regional Development Fund through the Operational
   Programme for Competitiveness and Internationalisation - COMPETE 2020
   under the PORTUGAL 2020 Partnership Agreement; ERDF European Regional
   Development Fund through the Portuguese National Innovation Agency (ANI)
   {[}ROMOVI: POCI-01-0247-FEDER-017945]},
Funding-Text = {This work is co-financed by the European Regional Development Fund
   (ERDF) through the Interreg V-A Espanha-Portugal Programme (POCTEP)
   2014-2020 within project 0095\_BIOTECFOR\_1\_P. This work also was
   cofinanced by the ERDF European Regional Development Fund through the
   Operational Programme for Competitiveness and Internationalisation -
   COMPETE 2020 under the PORTUGAL 2020 Partnership Agreement, and through
   the Portuguese National Innovation Agency (ANI) as a part of project
   ``ROMOVI: POCI-01-0247-FEDER-017945{''} The opinions included in this
   paper shall be the sole responsibility of their authors. The European
   Commission and the Authorities of the Programme aren't responsible for
   the use of information contained therein.},
Cited-References = {Atzberger C, 2013, REMOTE SENS-BASEL, V5, P949, DOI 10.3390/rs5020949.
   Azevedo F., 2019, P 2019 IEEE INT C AU, P1.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Baweja H.S., 2018, FIELD SERVICE ROBOTI, P271, DOI {[}DOI 10.1007/978-3-319-67361-5\_18, 10.1007/978-3-319-67361-5\_18].
   Cui HG, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901323.
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039.
   Ding J, 2016, IEEE GEOSCI REMOTE S, V13, P364, DOI 10.1109/LGRS.2015.2513754.
   Douarre C, 2018, J IMAGING, V4, DOI 10.3390/jimaging4050065.
   Espejo-Garcia B, 2019, COMPUT ELECTRON AGR, V162, P106, DOI 10.1016/j.compag.2019.03.027.
   Farooq A, 2019, IEEE GEOSCI REMOTE S, V16, P183, DOI 10.1109/LGRS.2018.2869879.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Ferreira AD, 2017, COMPUT ELECTRON AGR, V143, P314, DOI 10.1016/j.compag.2017.10.027.
   Fuentes A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17092022.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Han XB, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9080848.
   Heinrich K., 2019, YIELD PROGNOSIS AGRA.
   Heo YJ, 2018, IEEE ROBOT AUTOM LET, V3, P3035, DOI 10.1109/LRA.2018.2849513.
   Kamilaris A, 2018, J AGR SCI-CAMBRIDGE, V156, P312, DOI 10.1017/S0021859618000436.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Kamilaris A, 2017, COMPUT ELECTRON AGR, V143, P23, DOI 10.1016/j.compag.2017.09.037.
   Kitzes J, 2008, PHILOS T R SOC B, V363, P467, DOI 10.1098/rstb.2007.2164.
   Koirala A, 2019, PRECIS AGRIC, V20, P1107, DOI 10.1007/s11119-019-09642-0.
   Lammie C, 2019, IEEE ACCESS, V7, P51171, DOI 10.1109/ACCESS.2019.2911709.
   Lavreniuk M, 2018, 2018 IEEE 38TH INTERNATIONAL CONFERENCE ON ELECTRONICS AND NANOTECHNOLOGY (ELNANO), P239, DOI 10.1109/ELNANO.2018.8477525.
   Li XG, 2015, INT CONF ACOUST SPEE, P4520, DOI 10.1109/ICASSP.2015.7178826.
   Liakos KG, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18082674.
   Liu X, 2018, IEEE INT C INT ROBOT, P1045, DOI 10.1109/IROS.2018.8594239.
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
   Lu Y, 2017, NEUROCOMPUTING, V267, P378, DOI 10.1016/j.neucom.2017.06.023.
   Ma JC, 2018, COMPUT ELECTRON AGR, V154, P18, DOI 10.1016/j.compag.2018.08.048.
   McBratney Alex, 2005, Precision Agriculture, V6, P7, DOI 10.1007/s11119-005-0681-8.
   McNicoll G, 2002, POPUL DEV REV, V28, P144.
   Mendes J, 2016, IEEE INT CONF AUTON, P1, DOI 10.1109/ICARSC.2016.68.
   Patil K. A., 2016, 2016 International Conference on Global Trends in Signal Processing, Information Computing and Communication (ICGTSPICC). Proceedings, P543, DOI 10.1109/ICGTSPICC.2016.7955360.
   Perry M., 2015, APPL EC FINANCE, V2, P76.
   Rancon F, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11010001.
   Rivas A, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18072048.
   Russwurm M, 2017, INT ARCH PHOTOGRAMM, V42-1, P551, DOI 10.5194/isprs-archives-XLII-1-W1-551-2017.
   Sant L, 2019, 2019 27TH AUSTROCHIP WORKSHOP ON MICROELECTRONICS (AUSTROCHIP), P1, DOI 10.1109/Austrochip.2019.00012.
   Santos L, 2018, IEEE INT CONF AUTON, P250, DOI 10.1109/ICARSC.2018.8374191.
   Saxena L., 2014, SURVEY IMAGE PROCESS.
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003.
   Schmitz A., 2015, MECH AGR MACHINE ADO.
   Smith AG, 2020, PLANT METHODS, V16, DOI 10.1186/s13007-020-0563-0.
   Tseng D, 2018, IEEE INT CON AUTO SC, P284, DOI 10.1109/COASE.2018.8560431.
   Uzal LC, 2018, COMPUT ELECTRON AGR, V150, P196, DOI 10.1016/j.compag.2018.04.024.
   Walter A, 2017, P NATL ACAD SCI USA, V114, P6148, DOI 10.1073/pnas.1707462114.
   Wang C, 2017, IEEE T COMPUT AID D, V36, P513, DOI 10.1109/TCAD.2016.2587683.
   Yalcin H, 2016, INT CONF AGRO-GEOINF, P233.
   Younis S, 2018, BOT LETT, V165, P377, DOI 10.1080/23818107.2018.1446357.
   Zhang XH, 2018, IEEE ACCESS, V6, P30370, DOI 10.1109/ACCESS.2018.2844405.
   Zhang YD, 2019, MULTIMED TOOLS APPL, V78, P3613, DOI 10.1007/s11042-017-5243-3.
   Zhong LH, 2019, REMOTE SENS ENVIRON, V221, P430, DOI 10.1016/j.rse.2018.11.032.},
Number-of-Cited-References = {53},
Times-Cited = {30},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {21},
Doc-Delivery-Number = {BQ9XW},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000627321900012},
DA = {2023-08-12},
}

@inproceedings{ WOS:000406005700085,
Author = {Lukic, Marko and Tuba, Eva and Tuba, Milan},
Book-Group-Author = {IEEE},
Title = {Leaf Recognition Algorithm using Support Vector Machine with Hu Moments
   and Local Binary Patterns},
Booktitle = {2017 IEEE 15TH INTERNATIONAL SYMPOSIUM ON APPLIED MACHINE INTELLIGENCE
   AND INFORMATICS (SAMI)},
Year = {2017},
Pages = {485-490},
Note = {15th IEEE International Symposium on Applied Machine Intelligence and
   Informatics (SAMI), Herlany, SLOVAKIA, JAN 26-28, 2017},
Organization = {IEEE},
Abstract = {Leaf recognition is convenient for plant classification and it is an
   important subfield of pattern recognition. Different leaf features such
   as color, shape and texture are used as well as different classifiers
   including artificial neural networks, k-nearest neighbor and support
   vector machines. In this paper we propose an algorithm based on tuned
   support vector machine as a classifier and Hu moments and uniform local
   binary pattern histogram parameters as features. Our proposed algorithm
   was tested on leaf images from standard benchmark database and compared
   with other approaches from literature where it proved to be more
   successful (higher recognition percentage).},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Lukic, M (Corresponding Author), John Naisbitt Univ, Grad Sch Comp Sci, Bulevar Umetnosti 29, Belgrade, Serbia.
   Lukic, Marko; Tuba, Milan, John Naisbitt Univ, Grad Sch Comp Sci, Bulevar Umetnosti 29, Belgrade, Serbia.
   Tuba, Eva, Univ Belgrade, Fac Math, Studentski Trg 16, Belgrade, Serbia.},
ISBN = {978-1-5090-5655-2},
Keywords = {Leaf recognition; plant classification; support vector machines; Hu
   moments; local binary patterns},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Interdisciplinary Applications},
Author-Email = {lukic235@gmail.com
   etuba@acm.org
   tuba@ieee.org},
Affiliations = {University of Belgrade},
ResearcherID-Numbers = {Tuba, Milan/B-6779-2009
   Tuba, Eva/U-2526-2017},
ORCID-Numbers = {Tuba, Milan/0000-0003-3794-3056
   Tuba, Eva/0000-0003-4866-9048},
Funding-Acknowledgement = {Ministry of Education, Science and Technological Development of Republic
   of Serbia {[}III-44006]},
Funding-Text = {This research is supported by the Ministry of Education, Science and
   Technological Development of Republic of Serbia, Grant No. III-44006.},
Cited-References = {Alihodzic A, 2014, SCI WORLD J, DOI 10.1155/2014/176718.
   Anant Bhardwaj, 2013, International Journal of Innovation and Applied Studies, V3, P237.
   {[}Anonymous], 2014, THE J.
   {[}Anonymous], 2013, SIGNAL PROCESS PATTE.
   Aptoula E, 2013, IEEE IMAGE PROC, P1496, DOI 10.1109/ICIP.2013.6738307.
   Aranda M. C., 2010, P ACM INT C IM VID R, P327, DOI {[}10.1145/1816041.1816089, DOI 10.1145/1816041.1816089].
   Arsic A, 2015, 2015 23RD TELECOMMUNICATIONS FORUM TELFOR (TELFOR), P472, DOI 10.1109/TELFOR.2015.7377509.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Aschwanden MJ, 2010, SOL PHYS, V262, P235, DOI 10.1007/s11207-009-9474-y.
   Backes AR, 2009, INT J PATTERN RECOGN, V23, P1145, DOI 10.1142/S0218001409007508.
   Ban Y, 2014, PATTERN RECOGN, V47, P1573, DOI 10.1016/j.patcog.2013.11.005.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Brajevic I., 2014, CUCKOO SEARCH FIREFL, V516, P115, DOI DOI 10.1007/978-3-319-02141-6\_6.
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199.
   Christlein V, 2012, IEEE T INF FOREN SEC, V7, P1841, DOI 10.1109/TIFS.2012.2218597.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   EHSANIRAD A., 2010, INT J COMPUTER SCI I, V8, P78.
   Gunda NSK, 2011, J POWER SOURCES, V196, P3592, DOI 10.1016/j.jpowsour.2010.12.042.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Jordanski M, 2015, 2015 23RD TELECOMMUNICATIONS FORUM TELFOR (TELFOR), P819, DOI 10.1109/TELFOR.2015.7377591.
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Kumar R, 2012, IEEE T BIO-MED ENG, V59, P355, DOI 10.1109/TBME.2011.2172438.
   Liu D, 2014, FOOD BIOPROCESS TECH, V7, P307, DOI 10.1007/s11947-013-1193-6.
   Liu Z., 2015, HYBRID DEEP LEARNING, P115.
   Mokhtarian F, 2004, IEEE T IMAGE PROCESS, V13, P653, DOI 10.1109/TIP.2004.826126.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Mouine S., 2012, P 2 ACM INT C MULT R, P49.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Singh K, 2014, PATTERN RECOGN LETT, V36, P10, DOI 10.1016/j.patrec.2013.08.024.
   Tuba E, 2016, 24 INT C CENTR EUR C, P369.
   Tuba E, 2015, 2015 23RD TELECOMMUNICATIONS FORUM TELFOR (TELFOR), P464, DOI 10.1109/TELFOR.2015.7377507.
   Tuba M, 2014, COMPUT SCI J MOLD, V22, P318.
   Tuba M, 2015, 2015 25TH INTERNATIONAL CONFERENCE RADIOELEKTRONIKA (RADIOELEKTRONIKA), P326, DOI 10.1109/RADIOELEK.2015.7129057.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182.
   Zhang SW, 2009, LECT NOTES COMPUT SC, V5754, P948, DOI 10.1007/978-3-642-04070-2\_100.},
Number-of-Cited-References = {40},
Times-Cited = {20},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BI1IL},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000406005700085},
DA = {2023-08-12},
}

@inproceedings{ WOS:000461058100077,
Author = {Imanov, Elbrus and Alzouhbi, Abdallah Khaled},
Editor = {Aliev, RA and Kacprzyk, J and Pedrycz, W and Jamshidi, M and Sadikoglu, FM},
Title = {Machine Learning Comparative Analysis for Plant Classification},
Booktitle = {13TH INTERNATIONAL CONFERENCE ON THEORY AND APPLICATION OF FUZZY SYSTEMS
   AND SOFT COMPUTING - ICAFS-2018},
Series = {Advances in Intelligent Systems and Computing},
Year = {2019},
Volume = {896},
Pages = {586-593},
Note = {13th International Conference on Application of Fuzzy Systems and Soft
   Computing (ICAFS), Warsaw, POLAND, AUG 27-28, 2018},
Organization = {Azerbaijan Assoc Zadehs Legacy \& Artificial Intelligence; Azerbaijan
   State Oil \& Ind Univ; Berkeley Initiat Soft Comp; Georgia State Univ;
   Near E Univ; TOBB Econ \& Technol Univ; Univ Alberta; Univ Siegen; Univ
   Texas; Univ Toronto},
Abstract = {Nowadays, digital image processing, artificial neural network and
   machine visualization have been pettishly progressing, and they cover a
   significant side of artificial cleverness and the rule among human
   beings and electro-mechanical devices. These technologies have been
   utilized in a wide range of agricultural operations, medicine and
   manufacturing. By this research the preparation of some functions has
   been conducted.
   In this paper we introduce the classification of maize leaves from
   pictures that reveal many conditions, opening among pictures, by
   pre-processing, taking out, plant feature recognition, matching and
   training, and lastly getting the outcomes executed by Matlab, neural
   network pattern recognition application. These given features are
   separated to leaf maturity and picture interpretations, rotary motions
   and calibration, and they are calculated to develop an approach that
   gives us better classification algorithm results. A plant scientist may
   be introduced with a plant for recognition of its classes revealed in
   its natural home ground, to gather an in-depth recognition.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Imanov, E (Corresponding Author), Near East Univ, Dept Comp Engn, TR-10 Mersin, North Cyprus, Turkey.
   Imanov, Elbrus, Near East Univ, Dept Comp Engn, TR-10 Mersin, North Cyprus, Turkey.
   Alzouhbi, Abdallah Khaled, Dept Water Recycling, Machha 1032, Akkar, Lebanon.},
DOI = {10.1007/978-3-030-04164-9\_77},
ISSN = {2194-5357},
EISSN = {2194-5365},
ISBN = {978-3-030-04164-9},
Keywords = {Artificial neural network; Digital image processing; Machine
   visualization classification K-nearest neighbor; Support vector machine;
   Machine learning},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {elbrus.imanov@neu.edu.tr
   abdallazohbi@gmail.com},
Affiliations = {Near East University},
Cited-References = {Aliev R. A., 2004, SOFT COMPUTING ITS A, P388.
   Alzouhbi A., 2017, THESIS.
   Bermmert D., 2005, DISCRETE COMPUT GEOM, V33, P583.
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555.
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761.
   DALLWITZ MJ, 1980, TAXON, V29, P41, DOI 10.2307/1219595.
   Ding C, 2004, P 21 INT C MACHINE L, P29, DOI DOI 10.1145/1015330.1015408.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Gouk H. G., 2014, 2014 P 29 INT C IM V, P114.
   Karray F. O., 2004, SOFT COMPUTING INTEL, p{[}4, 223].
   Kim J., 2012, APPL MATH ELECT COMP, P133.
   Pujari Jagadeesh Devdas, 2013, INT J ADV SCI TECHNO, V52, P121.
   Tran QA, 2003, 2003 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-5, PROCEEDINGS, P1245.
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5.},
Number-of-Cited-References = {14},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {6},
Doc-Delivery-Number = {BM2HT},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000461058100077},
DA = {2023-08-12},
}

@inproceedings{ WOS:000427968000039,
Author = {Isnanto, R. Rizal and Hidayatno, Achmad and Zahra, Ajub Ajulian and
   Eskanesiari and Bagaskara, Aditya Indra and Septiana, Risma},
Editor = {Facta, M and Riyadi, MA and Prasetijo, AB and Widianto, ED and Eridani, D},
Title = {Herb Leaves Recognition Using Combinations of Hu's Moment Variants -
   Backpropagation Neural Network and 2-D Gabor Filter - Learning Vector
   Quantization (LVQ)},
Booktitle = {2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY, COMPUTER,
   AND ELECTRICAL ENGINEERING (ICITACEE)},
Year = {2017},
Pages = {219-224},
Note = {4th International Conference on Information Technology, Computer, and
   Electrical Engineering (ICITACEE), Semarang, INDONESIA, OCT 18-19, 2017},
Organization = {IEEE; Diponegoro Univ, Fac Engn, Dept Comp Engn},
Abstract = {Indonesian people use herbs as an alternative choice to heal many kinds
   of diseases. The lack of information and knowledge about the merit of
   herbs will make the recognition turns difficult. The herbs can be
   recognized by the shape of the leaf. Capturing the leaf as an image
   allows constructing an automatic system of herbs recognition using image
   processing. Two steps of image processing for recognition are feature
   extraction and classification. 2D Gabor filter and Hu's moment
   invariants are used to feature extraction process. 2D Gabor Filters are
   influenced by some vectors that have different values for each feature
   of leaves. The hu's moment invariants are influenced by the geometric
   operation. The next step is classification grouping the result of the
   feature extraction into the right cluster. In this research, two
   classification methods will be combined with the two feature extraction
   methods. The first is hu's moment invariants and backpropagation neural
   network and the second is 2D Gabor filter and Learning Vector
   Quantization. Each of combination gives the different results. The first
   combination has recognition rate is about 81.4\% and the second
   combination is about 80\%.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Isnanto, RR (Corresponding Author), Diponegoro Univ, Dept Comp Engn, Fac Engn, Semarang, Indonesia.
   Isnanto, R. Rizal; Septiana, Risma, Diponegoro Univ, Dept Comp Engn, Fac Engn, Semarang, Indonesia.
   Hidayatno, Achmad; Zahra, Ajub Ajulian; Eskanesiari; Bagaskara, Aditya Indra, Diponegoro Univ, Dept Elect Engn, Fac Engn, Semarang, Indonesia.},
ISBN = {978-1-5386-3947-4},
Keywords = {herbs; image processing; feature extraction; classification; leaf
   recognition},
Keywords-Plus = {IMAGE-PROCESSING TECHNIQUES},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {rizal\_isnanto@yahoo.com
   achmad@undip.ac.id
   ayub.ayul1an@gmail.com
   eskanesiari@gmail.com
   aditya.indra.b@gmail.com
   rismaseptiana@live.undip.ac.id},
Affiliations = {Diponegoro University; Diponegoro University},
ResearcherID-Numbers = {Isnanto, R Rizal/ABH-8272-2020
   },
ORCID-Numbers = {Hidayatno, Achmad/0000-0001-8282-0413
   Isnanto, R. Rizal/0000-0002-6044-0644
   Septiana, Risma/0000-0002-5139-9037},
Cited-References = {Bashyal S, 2008, ENG APPL ARTIF INTEL, V21, P1056, DOI 10.1016/j.engappai.2007.11.010.
   Hamuda E, 2016, COMPUT ELECTRON AGR, V125, P184, DOI 10.1016/j.compag.2016.04.024.
   Herlambang RGANP, 2015, 2015 2ND INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY, COMPUTER, AND ELECTRICAL ENGINEERING (ICITACEE), P123, DOI 10.1109/ICITACEE.2015.7437783.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Husin Z, 2012, COMPUT ELECTRON AGR, V89, P18, DOI 10.1016/j.compag.2012.07.009.
   Isnanto R. R., 2015, P SEM NAS HAS HAS PE, P102.
   Isnanto R. R., 2016, INT J SOFTW ENG ITS, V10, P131.
   Jong JekSiang Jaringan Syaraf Tinian dan, 2005, JARINGAN SYARAF TIRU.
   Ramirez-Quintana JA, 2012, ENG LET, V20, P68.
   Saeedkia D, 2013, WOODH PUB SER ELECT, P1, DOI 10.1533/9780857096494.
   Setiawan R. A., 2013, IDENTIFIKASI DIRI BE.
   The Mathwork Inc, 2001, IM PROC TOOLB US MAT.},
Number-of-Cited-References = {12},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BJ8CF},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000427968000039},
DA = {2023-08-12},
}

@article{ WOS:000868633600005,
Author = {Oppong, Stephen Opoku and Twum, Frimpong and Hayfron-Acquah, James Ben
   and Missah, Yaw Marfo},
Title = {A Novel Computer Vision Model for Medicinal Plant Identification Using
   Log-Gabor Filters and Deep Learning Algorithms},
Journal = {COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE},
Year = {2022},
Volume = {2022},
Month = {SEP 27},
Abstract = {Computer vision is the science that enables computers and machines to
   see and perceive image content on a semantic level. It combines
   concepts, techniques, and ideas from various fields such as digital
   image processing, pattern matching, artificial intelligence, and
   computer graphics. A computer vision system is designed to model the
   human visual system on a functional basis as closely as possible. Deep
   learning and Convolutional Neural Networks (CNNs) in particular which
   are biologically inspired have significantly contributed to computer
   vision studies. This research develops a computer vision system that
   uses CNNs and handcrafted filters from Log-Gabor filters to identify
   medicinal plants based on their leaf textural features in an ensemble
   manner. The system was tested on a dataset developed from the Centre of
   Plant Medicine Research, Ghana (MyDataset) consisting of forty-nine (49)
   plant species. Using the concept of transfer learning, ten pretrained
   networks including Alexnet, GoogLeNet, DenseNet201, Inceptionv3,
   Mobilenetv2, Restnet18, Resnet50, Resnet101, vgg16, and vgg19 were used
   as feature extractors. The DenseNet201 architecture resulted with the
   best outcome of 87\% accuracy and GoogLeNet with 79\% preforming the
   worse averaged across six supervised learning algorithms. The proposed
   model (OTAMNet), created by fusing a Log-Gabor layer into the transition
   layers of the DenseNet201 architecture achieved 98\% accuracy when
   tested on MyDataset. OTAMNet was tested on other benchmark datasets;
   Flavia, Swedish Leaf, MD2020, and the Folio dataset. The Flavia dataset
   achieved 99\%, Swedish Leaf 100\%, MD2020 99\%, and the Folio dataset
   97\%. A false-positive rate of less than 0.1\% was achieved in all
   cases.},
Publisher = {HINDAWI LTD},
Address = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Oppong, SO (Corresponding Author), Univ Educ, Dept ICT Educ, Winneba, Ghana.
   Oppong, Stephen Opoku, Univ Educ, Dept ICT Educ, Winneba, Ghana.
   Twum, Frimpong; Hayfron-Acquah, James Ben; Missah, Yaw Marfo, Kwame Nkrumah Univ Sci \& Technol, Dept Comp Sci, Kumasi, Ghana.},
DOI = {10.1155/2022/1189509},
Article-Number = {1189509},
ISSN = {1687-5265},
EISSN = {1687-5273},
Keywords-Plus = {CONVOLUTIONAL NEURAL-NETWORKS; RECOGNITION},
Research-Areas = {Mathematical \& Computational Biology; Neurosciences \& Neurology},
Web-of-Science-Categories  = {Mathematical \& Computational Biology; Neurosciences},
Author-Email = {sooppong@uew.edu.gh
   twumf@yahoo.co.uk
   jbha@yahoo.com
   ymissah@gmail.com},
Affiliations = {Kwame Nkrumah University Science \& Technology},
ORCID-Numbers = {Oppong, Stephen Opoku/0000-0002-1024-5680},
Cited-References = {Abiodun OI, 2018, HELIYON, V4, DOI 10.1016/j.heliyon.2018.e00938.
   Adeniyi A, 2018, J HERB MED, V14, P76, DOI 10.1016/j.hermed.2018.02.001.
   Adetiba E., 2021, IEEE T PATTERN ANAL, V17, P349, DOI {[}10.3844/JCSSP.2021.349.363, DOI 10.3844/JCSSP.2021.349.363].
   Adinortey MB, 2019, EVID-BASED COMPL ALT, V2019, DOI 10.1155/2019/6021209.
   Alzubaidi L, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00444-8.
   Amancio DR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0094137.
   Banzi J, 2021, TANZANIA J FORESTRY, V90, P93.
   Barimah KB, 2015, J COMMUNITY PSYCHOL, V43, P99, DOI 10.1002/jcop.21687.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Begue A, 2017, INT J ADV COMPUT SC, V8, P166.
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50.
   Elizabeth CPB, 2023, IETE J RES, V69, P1772, DOI 10.1080/03772063.2021.2016504.
   Boadu AA, 2017, EVID-BASED COMPL ALT, V2017, DOI 10.1155/2017/3043061.
   Britto L., 2019, ANAIS 16 ENCONTRO NA, P13.
   Brodzicki A, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20236713.
   Cao JF, 2021, EURASIP J ADV SIG PR, V2021, DOI 10.1186/s13634-021-00740-8.
   Chung Y, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18030961.
   Dahigaonkar T.D., 2018, INT RES J ENG TECHNO, V5, P351.
   de Luna RG, 2017, I C HUMANOID NANOTEC.
   Demsar J, 2021, PLOS COMPUT BIOL, V17, DOI 10.1371/journal.pcbi.1008671.
   Dissanayake D.M.C., 2021, SRI LANKAN J TECHNOL, P60, DOI {[}10.33851/JMIS.2019.6.2.49, DOI 10.33851/JMIS.2019.6.2.49].
   Ezekwesili-Ofili J.O., 2019, HERBAL MED AFRICAN T, DOI {[}10.5772/intechopen.80348, DOI 10.5772/INTECHOPEN.80348].
   Fangfei Liu, 2021, Journal of Physics: Conference Series, V1915, DOI 10.1088/1742-6596/1915/2/022005.
   Farrell MH, 2021, ECONOMETRICA, V89, P181, DOI 10.3982/ECTA16901.
   Fernandes AFA, 2020, FRONT VET SCI, V7, DOI 10.3389/fvets.2020.551269.
   FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379.
   Gadekallu TR, 2021, J REAL-TIME IMAGE PR, V18, P1383, DOI 10.1007/s11554-020-00987-8.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Glegola W, 2021, PROCEDIA COMPUT SCI, V192, P2249, DOI 10.1016/j.procs.2021.08.238.
   Herrera F., 2016, MULTIPLE INSTANCE LE, P67.
   Huynh HX, 2020, VIETNAM J COMPUT SCI, V7, P197, DOI 10.1142/S2196888820500116.
   Huang G, 2022, IEEE T PATTERN ANAL, V44, P8704, DOI 10.1109/TPAMI.2019.2918284.
   Jaiganesh M., 2020, J CRIT REV, V7, P923.
   Kan H. X., 2017, Pattern Recognition and Image Analysis, V27, P581, DOI 10.1134/S105466181703018X.
   Kanagalakshmi K., 2012, EUR J SCI RES, V74, P563.
   Karahan T, 2021, PAMUKKALE U J ENG SC, V27, P638, DOI 10.5505/pajes.2020.84042.
   Kaur P.P., 2020, SSRN ELECT J, DOI {[}10.2139/ssrn.3565850, DOI 10.2139/SSRN.3565850].
   Kaur S., 2019, J MULTIMEDIA INFORM, V6, P49, DOI DOI 10.33851/JMIS.2019.6.2.49.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Kern C, 2019, SURV RES METHODS-GER, V13, P73, DOI 10.18148/srm/2019.v13i1.7395.
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6.
   Krause J, 2018, ICMR `18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P517, DOI 10.1145/3206025.3206089.
   Kuka\~{}cka J., 2017, ARXIV171010686.
   Lasseck M., 2017, CEUR WORKSHOP PROC, V1866.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lee Y, 2021, INT J MOL SCI, V22, DOI 10.3390/ijms22147721.
   Lewis M, 2019, FRONT NEURAL CIRCUIT, V13, DOI 10.3389/fncir.2019.00022.
   Li C, 2020, IEEE INT CONF CLOUD, P414, DOI 10.1109/CLOUD49709.2020.00063.
   Ma C., 2020, QUALITATIVE STUDY DY, V145, P1.
   Ma Y., 2 INT C COMPUTER VIS, DOI {[}10.1117/12.2604526, DOI 10.1117/12.2604526].
   Marin I, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10217817.
   Mishra RK, 2021, MATH PROBL ENG, V2021, DOI 10.1155/2021/5548884.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Naeem S, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11020263.
   Olson RS, 2017, BIODATA MIN, V10, DOI 10.1186/s13040-017-0154-4.
   Oppong S.O., 2022, GHANAIAN LEAF DATASE.
   Panigrahi R, 2021, MATHEMATICS-BASEL, V9, DOI 10.3390/math9060690.
   Pawara P, 2017, LECT NOTES COMPUT SC, V10617, P615, DOI 10.1007/978-3-319-70353-4\_52.
   Pearline SA, 2019, J INTELL FUZZY SYST, V36, P1997, DOI 10.3233/JIFS-169911.
   Perin G, 2021, LECT NOTES COMPUT SC, V12804, P615, DOI 10.1007/978-3-030-81652-0\_24.
   Postalcioglu S, 2020, INT J PATTERN RECOGN, V34, DOI 10.1142/S0218001420510039.
   Pravin A., 2021, TURKISH J COMP MATHE, V12, P6740.
   Pushpanathan K, 2021, ARTIF INTELL REV, V54, P305, DOI 10.1007/s10462-020-09847-0.
   Rahman MM, 2019, SN APPL SCI, V1, DOI 10.1007/s42452-019-1682-y.
   Rainey C., 2021, PROCESSES RADIOGRAPH.
   Roopashree S., 2020, MED LEAF DATASET, DOI {[}10.17632/nnytj2v3n5.1, DOI 10.17632/NNYTJ2V3N5.1].
   Sarkar A, 2020, DATABASE-OXFORD, DOI 10.1093/database/baz117.
   Sarker Iqbal H, 2021, SN Comput Sci, V2, P420, DOI 10.1007/s42979-021-00815-1.
   Sarraf A, 2021, AM ACAD SCI RES J EN, V77, P1.
   Schmidt R.M., 2021, P P 38 INT C MACHINE, VVolume 139, P9367.
   Sfar AR, 2013, PROC CVPR IEEE, P835, DOI 10.1109/CVPR.2013.113.
   Shrestha A, 2019, IEEE ACCESS, V7, P53040, DOI 10.1109/ACCESS.2019.2912200.
   Simpson M.G., 2019, PLANT SYSTEMATICS, P537, DOI {[}10.1016/b978-0-12-812628-8.50010-9, DOI 10.1016/B978-0-12-812628-8.50010-9].
   Singh M.M., 2021, REV GEST O INOVA O T, V11, P3191, DOI {[}10.47059/revistageintec.v11i4.2362, DOI 10.47059/REVISTAGEINTEC.V11I4.2362].
   Singh R., 2018, NATURAL PRODUCTS DRU, P119, DOI {[}DOI 10.1016/B978-0-08-102081-4.00006-X, 10.1016/B978-0-08-102081-4.00006-X].
   Sliti O., 2014, P 3 INT C PATT REC A, P687, DOI {[}10.5220/0004829306870694, DOI 10.5220/0004829306870694].
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Stephens CR, 2018, MACH LEARN, V107, P397, DOI 10.1007/s10994-017-5658-0.
   Sulc M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0265-4.
   Sulc M, 2015, LECT NOTES COMPUT SC, V8928, P185, DOI 10.1007/978-3-319-16220-1\_14.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Tharwat A., 2018, APPL COMPUTING INFOR, P1, DOI DOI 10.1016/J.ACI.2018.08.003.
   Thi Thanh-Nhan Nguyen, 2019, International Journal of Machine Learning and Computing, V9, P26, DOI 10.18178/ijmlc.2019.9.1.761.
   Bao TQ, 2020, J INFORM TELECOMMUN, V4, P140, DOI 10.1080/24751839.2019.1666625.
   Twum F, 2022, IEEE ACCESS, V10, P83204, DOI 10.1109/ACCESS.2022.3196788.
   Vijayashree T, 2018, RES J PHARM BIOL CHE, V9, P1221.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Welchman AE, 2016, ANNU REV VIS SCI, V2, P345, DOI 10.1146/annurev-vision-111815-114605.
   World Health Organization, 2019, WHO GLOB REP TRAD CO.
   Wu J, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20247080.
   Wu K., 2021, GEN DEEP LEARNING IM, P3, DOI {[}10.1142/9789811218842\_0001, DOI 10.1142/9789811218842\_0001].
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xu, 2020, ADV APPL DEEP LEARNI, P45, DOI DOI 10.5772/INTECHOPEN.94072.
   Xue JR, 2019, INT J AGR BIOL ENG, V12, P123, DOI 10.25165/j.ijabe.20191202.4637.
   Yamashita R, 2018, INSIGHTS IMAGING, V9, P611, DOI 10.1007/s13244-018-0639-9.
   Yang Y, 2021, COMPUT BIOL MED, V139, DOI 10.1016/j.compbiomed.2021.104887.
   Zhang CY, 2015, CIT/IUCC/DASC/PICOM 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY - UBIQUITOUS COMPUTING AND COMMUNICATIONS - DEPENDABLE, AUTONOMIC AND SECURE COMPUTING - PERVASIVE INTELLIGENCE AND COMPUTING, P2147, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Zhang YN, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10155177.
   Zhang ZY, 2021, NATL SCI REV, V8, DOI 10.1093/nsr/nwaa159.
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI {[}10.1109/TNNLS.2018.2876865, 10.23977/icamcs.2018.001].
   Zheng Yi, 2018, Journal of Physics: Conference Series, V1087, DOI 10.1088/1742-6596/1087/6/062018.
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555.},
Number-of-Cited-References = {102},
Times-Cited = {1},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {7},
Journal-ISO = {Comput. Intell. Neurosci.},
Doc-Delivery-Number = {5I8XU},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000868633600005},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000342558400001,
Author = {Gwo, Chih-Ying and Wei, Chia-Hung},
Title = {PLANT IDENTIFICATION THROUGH IMAGES: USING FEATURE EXTRACTION OF KEY
   POINTS ON LEAF CONTOURS},
Journal = {APPLICATIONS IN PLANT SCIENCES},
Year = {2013},
Volume = {1},
Number = {11},
Month = {NOV},
Abstract = {Premise of the study: Because plant identification demands extensive
   knowledge and complex terminologies, even professional botanists require
   significant time in the field for mastery of the subject. As plant
   leaves are normally regarded as possessing useful characteristics for
   species identification, leaf recognition through images can be
   considered an important research issue for plant recognition.
   Methods: This study proposes a feature extraction method for leaf
   contours, which describes the lines between the centroid and each
   contour point on an image. A length histogram is created to represent
   the distribution of distances in the leaf contour. Thereafter, a
   classifier is applied from a statistical model to calculate the matching
   score of the template and query leaf.
   Results: The experimental results show that the top value achieves
   92.7\% and the first two values can achieve 97.3\%. In the scale
   invariance test, those 45 correlation coefficients fall between the
   minimal value of 0.98611 and the maximal value of 0.99992. Like the
   scale invariance test, the rotation invariance test performed 45
   comparison sets. The correlation coefficients range between 0.98071 and
   0.99988.
   Discussion: This study shows that the extracted features from leaf
   images are invariant to scale and rotation because those features are
   close to positive correlation in terms of coefficient correlation.
   Moreover, the experimental results indicated that the proposed method
   outperforms two other methods, Zernike moments and curvature scale
   space.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Wei, CH (Corresponding Author), Chien Hsin Univ Sci \& Technol, Dept Informat Management, 229 Chien Hsin Rd, Taoyuan 320, Taiwan.
   Gwo, Chih-Ying; Wei, Chia-Hung, Chien Hsin Univ Sci \& Technol, Dept Informat Management, Taoyuan 320, Taiwan.
   Wei, Chia-Hung, Taipei Med Univ, Grad Inst Biomed Informat, Taipei, Taiwan.},
DOI = {10.3732/apps.1200005},
Article-Number = {1200005},
ISSN = {2168-0450},
Keywords = {classifier of statistical model; edge detection; feature extraction;
   leaf recognition},
Keywords-Plus = {CURVATURE SCALE-SPACE; SHAPE; DESCRIPTORS},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {rogerwei@uch.edu.tw},
Affiliations = {Chien Hsin University of Science \& Technology; Taipei Medical
   University},
Cited-References = {{[}Anonymous], 2000, WORLD PAT INF, DOI DOI 10.1016/S0172-2190(00)00083-1.
   Bober M., 2002, INTRO MPEG 7 MULTIME, P231.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   EHSANIRAD A., 2010, INT J COMPUTER SCI I, V8, P78.
   Gailing O, 2012, PLANT SYST EVOL, V298, P1533, DOI 10.1007/s00606-012-0656-y.
   Gu X, 2005, LECT NOTES COMPUT SC, V3644, P253.
   Hajjdiab H., 2011, 2011 Proceedings of IEEE International Conference on Imaging Systems and Techniques (IST 2011), P306, DOI 10.1109/IST.2011.5962205.
   Huang Lin, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P871, DOI 10.1109/CSSE.2008.1333.
   Liwen Gao, 2010, Proceedings 3rd International Congress on Image and Signal Processing (CISP 2010), P2732, DOI 10.1109/CISP.2010.5647617.
   Liwen Gao, 2010, Proceedings 2010 Sixth International Conference on Natural Computation (ICNC 2010), P1038, DOI 10.1109/ICNC.2010.5582971.
   Lukasiewicz T, 2009, INT J APPROX REASON, V50, P837, DOI 10.1016/j.ijar.2009.03.004.
   Marcysiak K, 2012, PLANT SYST EVOL, V298, P1597, DOI 10.1007/s00606-012-0662-0.
   Mokhtarian F, 2005, COMPUT GRAPH-UK, V29, P961, DOI 10.1016/j.cag.2005.09.012.
   Tsukaya H, 2006, ANNU REV PLANT BIOL, V57, P477, DOI 10.1146/annurev.arplant.57.032905.105320.
   Wee CY, 2007, IMAGE VISION COMPUT, V25, P967, DOI 10.1016/j.imavis.2006.07.010.
   Wu Q., 2006, ADV ARTIFICIAL INTEL, V3, P5.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yu-Ping Liao, 2010, 2010 International Symposium on Computer, Communication, Control and Automation (3CA), P334, DOI 10.1109/3CA.2010.5533816.
   Zhang DS, 2003, MULTIMEDIA SYST, V9, P15, DOI 10.1007/s00530-002-0075-y.
   Zhang DS, 2003, J VIS COMMUN IMAGE R, V14, P41, DOI 10.1016/S1047-3203(03)00003-8.},
Number-of-Cited-References = {21},
Times-Cited = {19},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {10},
Journal-ISO = {Appl. Plant Sci.},
Doc-Delivery-Number = {AQ1RJ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000342558400001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000736861100008,
Author = {Salve, Pradip and Yannawar, Pravin and Sardesai, Milind},
Title = {Multimodal plant recognition through hybrid feature fusion technique
   using imaging and non-imaging hyper-spectral data},
Journal = {JOURNAL OF KING SAUD UNIVERSITY-COMPUTER AND INFORMATION SCIENCES},
Year = {2022},
Volume = {34},
Number = {1},
Pages = {1361-1369},
Month = {JAN},
Abstract = {Automatic classification of the plants is growing area of association
   with computer science and Botany, it has attracted many researchers to
   subsidize plant classification using image processing and machine
   learning techniques. Plants can be classified using number of traits
   such as leaf color, flowers, leaves, roots, leaf shape, leaf size etc.
   highly depends upon feature selection methods. However extraction of
   features from selected trait is most significant state in
   classification. State-of-the-art classification can be achieved by using
   leaf characteristics such as leaf venation patterns, leaf spectral
   signatures, leaf color, leaf shape, etc. This paper describes multimodal
   plant classification system using leaf venation patterns and its
   spectral signatures as a significant features. This paper shows that the
   feature fusion can be used to achieve efficient plant identification.
   The accuracy of identification for leaf spectral data, leaf venation
   features and HOG features is validated, it signifies that feature fusion
   technique performs better than that of non-imaging spectral signatures
   features only, with recognition result of 98.03\% GAR and 93.51\% GAR,
   respectively. (c) 2018 The Authors. Production and hosting by Elsevier
   B.V. on behalf of King Saud University. This is an open access article
   under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Salve, P (Corresponding Author), Dr Babasa Heb Ambedkar Marathwada Univ, Dept Comp Sci \& IT, Aurangabad 431004, Maharashtra, India.
   Salve, Pradip; Yannawar, Pravin, Dr Babasaheb Ambedkar Marathwada Univ, Dept Comp Sci \& IT, Vis \& Intelligence Lab, Aurangabad, Maharashtra, India.
   Sardesai, Milind, Savitribai Phule Pune Univ, Dept Bot, Florist Res Lab, Pune, Maharashtra, India.},
DOI = {10.1016/j.jksuci.2018.09.018},
EarlyAccessDate = {DEC 2021},
ISSN = {1319-1578},
EISSN = {2213-1248},
Keywords = {Leaf venation; Plant identification; Multimodal plant; Leaf spectral
   signature},
Keywords-Plus = {LEAF; EXTRACTION; VENATION; IDENTIFICATION; PATTERN; SHAPE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems},
Author-Email = {pradipslv@gmail.com},
Affiliations = {Dr. Babasaheb Ambedkar Marathwada University (BAMU); Savitribai Phule
   Pune University},
ResearcherID-Numbers = {Yannawar, Pravin/AAC-4327-2021
   Salve, Pradip/CAG-3923-2022},
ORCID-Numbers = {Yannawar, Pravin/0000-0002-3398-0565
   },
Funding-Acknowledgement = {UGC-MANF fellowship},
Funding-Text = {Authors would like to acknowledge UGC-MANF fellowship for financial
   support and technical supports of GIS \& Remote sensing Lab of
   Department of Computer Science \& IT, Dr. Babasaheb Ambedkar Marathwada
   University, Aurangabad, Maharashtra, India.},
Cited-References = {Ali MMH, 2017, PROCEDIA COMPUT SCI, V115, P482, DOI 10.1016/j.procs.2017.09.091.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6474, P135, DOI 10.1007/978-3-642-17688-3\_14.
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
   Fu H, 2006, IEE P-VIS IMAGE SIGN, V153, P881, DOI 10.1049/ip-vis:20060061.
   Green WA, 2014, APPL PLANT SCI, V2, DOI 10.3732/apps.1400006.
   Jamil N, 2015, PROCEDIA COMPUT SCI, V76, P436, DOI 10.1016/j.procs.2015.12.287.
   Kirchgessner N., 2002, 2 IASTED INT C VIS I.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Mounsef Jinane, 2011, COMPUTATIONAL INTELL, P1.
   Park J, 2008, J SYST SOFTWARE, V81, P71, DOI 10.1016/j.jss.2007.05.001.
   Price CA, 2011, PLANT PHYSIOL, V155, P236, DOI 10.1104/pp.110.162834.
   Rolland-Lagan AG, 2009, PLANT J, V57, P195, DOI 10.1111/j.1365-313X.2008.03678.x.
   Salve P, 2016, ADV INTELL SYST, V379, P85, DOI 10.1007/978-81-322-2517-1\_10.
   Sekulska-Nalewajko Joanna, 2010, AUTOMATYKA AKAD GORN, V14, P359.
   Shejwal S., 2015, INT J COMPUT SCI TEC, P93.
   Siravenha AC, 2016, 2016 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P45.
   Tang XD, 2009, PROCEEDINGS OF THE 2009 2ND INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOLS 1-9, P1102.
   Varpe AB, 2015, PROCEEDINGS 2015 INTERNATIONAL CONFERENCE ON MAN AND MACHINE INTERFACING (MAMI).
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Yachao Wang, 2014, Journal of Software, V9, P1532, DOI 10.4304/jsw.9.6.1532-1537.},
Number-of-Cited-References = {21},
Times-Cited = {10},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {11},
Journal-ISO = {J. King Saud Univ.-Comput. Inf. Sci.},
Doc-Delivery-Number = {XY3EX},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000736861100008},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000883360200001,
Author = {Azadnia, Rahim and Al-Amidi, Mohammed Maitham and Mohammadi, Hamed and
   Cifci, Mehmet Akif and Daryab, Avat and Cavallo, Eugenio},
Title = {An AI Based Approach for Medicinal Plant Identification Using Deep CNN
   Based on Global Average Pooling},
Journal = {AGRONOMY-BASEL},
Year = {2022},
Volume = {12},
Number = {11},
Month = {NOV},
Abstract = {Medicinal plants have always been studied and considered due to their
   high importance for preserving human health. However, identifying
   medicinal plants is very time-consuming, tedious and requires an
   experienced specialist. Hence, a vision-based system can support
   researchers and ordinary people in recognising herb plants quickly and
   accurately. Thus, this study proposes an intelligent vision-based system
   to identify herb plants by developing an automatic Convolutional Neural
   Network (CNN). The proposed Deep Learning (DL) model consists of a CNN
   block for feature extraction and a classifier block for classifying the
   extracted features. The classifier block includes a Global Average
   Pooling (GAP) layer, a dense layer, a dropout layer, and a softmax
   layer. The solution has been tested on 3 levels of definitions (64 x 64,
   128 x 128 and 256 x 256 pixel) of images for leaf recognition of five
   different medicinal plants. As a result, the vision-based system
   achieved more than 99.3\% accuracy for all the image definitions. Hence,
   the proposed method effectively identifies medicinal plants in real-time
   and is capable of replacing traditional methods.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Cavallo, E (Corresponding Author), Natl Res Council CNR Italy, Inst Sci \& Technol Sustainable Energy \& Mobil STE, I-10129 Turin, Italy.
   Azadnia, Rahim, Univ Tehran, Dept Biosyst Engn, Karaj 3158777871, Iran.
   Al-Amidi, Mohammed Maitham, Al Mustaqbal Univ Coll, Informat Technol, Babylon 51001, Iraq.
   Mohammadi, Hamed, Univ Cent Florida, Dept Ind Engn \& Management Syst, Orlando, FL 32816 USA.
   Cifci, Mehmet Akif, Bandirma Onyedi Eylul Univ, Dept Comp Engn, TR-10200 Balikesir, Turkey.
   Daryab, Avat, Islamic Azad Univ, Dept Agr, Karaj Branch, Karaj 3158777871, Iran.
   Cavallo, Eugenio, Natl Res Council CNR Italy, Inst Sci \& Technol Sustainable Energy \& Mobil STE, I-10129 Turin, Italy.},
DOI = {10.3390/agronomy12112723},
Article-Number = {2723},
EISSN = {2073-4395},
Keywords = {medicinal plant; identification; image processing; Global Average
   Pooling (GAP); Convolutional Neural Network (CNN)},
Keywords-Plus = {ATTENTION; NETWORK; MODELS},
Research-Areas = {Agriculture; Plant Sciences},
Web-of-Science-Categories  = {Agronomy; Plant Sciences},
Author-Email = {eugenio.cavallo@cnr.it},
Affiliations = {University of Tehran; Al-Mustaqbal University College; State University
   System of Florida; University of Central Florida; Balikesir University;
   Bandirma Onyedi Eylul University; Islamic Azad University},
ResearcherID-Numbers = {Cavallo, Eugenio/F-2820-2014
   cifci, Mehmet Akif/V-2885-2018},
ORCID-Numbers = {Cavallo, Eugenio/0000-0002-2759-9629
   cifci, Mehmet Akif/0000-0002-6439-8826},
Cited-References = {Altemimi A, 2017, PLANTS-BASEL, V6, DOI 10.3390/plants6040042.
   Amenu E, 2007, THESIS ADDIS ABABA U.
   Amit Kumar, 2013, International Journal of Agronomy and Plant Production, V4, P1580.
   Amuthalingeswaran C., 2019, 2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI). Proceedings, P886, DOI 10.1109/ICOEI.2019.8862765.
   Apolo-Apolo OE, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.01086.
   Barbedo JGA, 2019, BIOSYST ENG, V180, P96, DOI 10.1016/j.biosystemseng.2019.02.002.
   Arsenovic M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11070939.
   Azadnia R, 2022, MEASUREMENT, V190, DOI 10.1016/j.measurement.2021.110669.
   Azadnia R, 2021, J APPL RES MED AROMA, V25, DOI 10.1016/j.jarmap.2021.100327.
   Azizi A, 2020, SOIL TILL RES, V199, DOI 10.1016/j.still.2020.104586.
   Baliga MS, 2011, FOOD RES INT, V44, P1768, DOI 10.1016/j.foodres.2011.02.008.
   Bedi P, 2021, ARTIF INTELL AGR, V5, P90, DOI 10.1016/j.aiia.2021.05.002.
   Bhardwaj RL, 2015, NUTR FOOD SCI, V45, P895, DOI 10.1108/NFS-05-2015-0058.
   Bhattacharyya P, 2013, ANTI-CANCER DRUG, V24, P659, DOI 10.1097/CAD.0b013e328361aca1.
   Bisen D, 2021, MULTIMED TOOLS APPL, V80, P6443, DOI 10.1007/s11042-020-10038-w.
   Bodhwani Vinit, 2019, Procedia Computer Science, V152, P186, DOI 10.1016/j.procs.2019.05.042.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Chen YH, 2018, NEUROCOMPUTING, V294, P61, DOI 10.1016/j.neucom.2018.03.014.
   Chukwuma E. C., 2015, J MED PLANTS STUDIES, V3, P23.
   Crini G, 2020, ENVIRON CHEM LETT, V18, P1451, DOI 10.1007/s10311-020-01029-2.
   Farooq M, 2017, LECT N BIOINFORMAT, V10208, P464, DOI 10.1007/978-3-319-56148-6\_41.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013.
   Herro E, 2010, DERMATITIS, V21, P327, DOI 10.2310/6620.2011.10080.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Hu RC, 2020, J ETHNOBIOL ETHNOMED, V16, DOI 10.1186/s13002-020-00387-z.
   Jahanbakhshi A, 2020, SCI HORTIC-AMSTERDAM, V263, DOI 10.1016/j.scienta.2019.109133.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2.
   Mahendran G, 2020, PHYTOTHER RES, V34, P2088, DOI 10.1002/ptr.6664.
   Mimica-Dukic N, 2004, J AGR FOOD CHEM, V52, P2485, DOI 10.1021/jf030698a.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Mukherjee G, 2021, SOFT COMPUT, V25, P14119, DOI 10.1007/s00500-021-06139-9.
   Muneer A, 2020, IEEE ACCESS, V8, P196747, DOI 10.1109/ACCESS.2020.3034033.
   Nabavi SF, 2015, NUTRIENTS, V7, P7729, DOI 10.3390/nu7095359.
   Naeem S, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11020263.
   Naserifar R, 2017, INT J ADV BIOTECHNOL, V8, P1330.
   Nasiri A, 2021, PLANTS-BASEL, V10, DOI 10.3390/plants10081628.
   Nasiri A, 2019, POSTHARVEST BIOL TEC, V153, P133, DOI 10.1016/j.postharvbio.2019.04.003.
   Ozioma E.-O.J., 2019, HERBAL MED, V10, P191, DOI 10.5772/intechopen.80348.
   Paulson Anu, 2020, 2020 Advanced Computing and Communication Technologies for High Performance Applications (ACCTHPA), P57, DOI 10.1109/ACCTHPA49271.2020.9213224.
   Pearline SA, 2019, J INTELL FUZZY SYST, V36, P1997, DOI 10.3233/JIFS-169911.
   Reddy SRG, 2021, INT J SPEECH TECHNOL, DOI 10.1007/s10772-021-09843-x.
   REITER M, 1985, ARZNEIMITTEL-FORSCH, V35, P408.
   Rincon F, 2012, TRANSL STROKE RES, V3, pS10, DOI 10.1007/s12975-012-0175-8.
   Roopashree S, 2021, IEEE ACCESS, V9, P135927, DOI 10.1109/ACCESS.2021.3116207.
   Russel NS, 2022, NEURAL COMPUT APPL, V34, P19217, DOI 10.1007/s00521-022-07521-w.
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0.
   Singh Vijai, 2017, Information Processing in Agriculture, V4, P41, DOI 10.1016/j.inpa.2016.10.005.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Swati Madan, 2010, Indian Journal of Natural Products and Resources, V1, P267.
   Tagashira M, 1998, PLANTA MED, V64, P555, DOI 10.1055/s-2006-957513.
   Tang YC, 2015, Arxiv, DOI arXiv:1306.0239.
   Vala H. J., 2013, COMPUT SCI, V2, P387, DOI {[}DOI 10.1007/S11548-009-0389-8, 10.4028/www.scientific.net/AMR.989-994.1959].
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang JQ, 2022, FRONT NEUROSCI-SWITZ, V16, DOI 10.3389/fnins.2022.920820.
   Yamashita R, 2018, INSIGHTS IMAGING, V9, P611, DOI 10.1007/s13244-018-0639-9.
   Zhu YX, 2019, NEUROCOMPUTING, V365, P191, DOI 10.1016/j.neucom.2019.07.016.
   Ziyaee P, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11050873.},
Number-of-Cited-References = {61},
Times-Cited = {1},
Usage-Count-Last-180-days = {10},
Usage-Count-Since-2013 = {14},
Journal-ISO = {Agronomy-Basel},
Doc-Delivery-Number = {6E4OL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000883360200001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000524986200003,
Author = {Keivani, Mohammad and Mazloum, Jalil and Sedaghatfar, Ezatollah and
   Tavakoli, Mohammad Bagher},
Title = {Automated Analysis of Leaf Shape, Texture, and Color Features for Plant
   Classification},
Journal = {TRAITEMENT DU SIGNAL},
Year = {2020},
Volume = {37},
Number = {1},
Pages = {17-28},
Month = {FEB},
Abstract = {The main purpose of this research is to apply image processing for plant
   identification in agriculture. This application field has so far
   received less attention rather than the other image processing
   applications domains. This is called the plant identification system. In
   the plant identification system, the conventional technique is dealt
   with looking at the leaves and fruits of the plants. However, it does
   not take into account as a cost effective approach because of its time
   consumption. The image processing technique can lead to identify the
   specimens more quickly and classify them through a visual machine
   method. This paper proposes a methodology for identifying the plant leaf
   images through several items including GIST and Local Binary Pattern
   (LBP) features, three kinds of geometric features, as well as color
   moments, vein features, and texture features based on lacunarity. After
   completion of the processing phase, the features are normalized, and
   then Pbest-guide binary particle swarm optimization (PBPSO) is developed
   as a novel method for reduction of the features. In the next phase,
   these features are employed for classification of the plant species.
   Different machine learning classifiers are evaluated including k-nearest
   neighbor, decision tree, naive Bayes, and multi-SVM. We tested our
   proposed technique on Flavia and Folio leaf datasets. The final results
   demonstrated that the decision tree has the best performance. The
   results of the experiments reveal that the proposed algorithm shows the
   accuracy of 98.58\% and 90.02\% for the ``Flavia{''} and ``Folio{''}
   datasets, respectively.},
Publisher = {INT INFORMATION \& ENGINEERING TECHNOLOGY ASSOC},
Address = {\#2020, SCOTIA PLACE TOWER ONE, 10060 JASPER AVE, EDMONTON, AB T5J 3R8,
   CANADA},
Type = {Article},
Language = {English},
Affiliation = {Keivani, M (Corresponding Author), Islamic Azad Univ Arak, Fac Engn, Dept Elect \& Elect Engn, Arak 3836119131, Iran.
   Keivani, Mohammad; Tavakoli, Mohammad Bagher, Islamic Azad Univ Arak, Fac Engn, Dept Elect \& Elect Engn, Arak 3836119131, Iran.
   Mazloum, Jalil, Shahid Sattari Aeronaut Univ Sci \& Technol, Fac Engn, Dept Elect \& Elect Engn, Tehran 1384663113, Iran.
   Sedaghatfar, Ezatollah, Islamic Azad Univ Arak, Fac Agr, Dept Plant Pathol, Arak 3836119131, Iran.},
DOI = {10.18280/ts.370103},
ISSN = {0765-0019},
EISSN = {1958-5608},
Keywords = {plants; GIST; best-guide binary particle swarm optimization; geometrics;
   machine learning},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic},
Author-Email = {Mkeivani94@iau-arak.ac.ir},
Affiliations = {Islamic Azad University; Islamic Azad University},
ResearcherID-Numbers = {Mazloum, Jalil/AAV-2810-2020
   Sedaghatfar, Ezatollah/AAU-2038-2021
   },
ORCID-Numbers = {Mazloum, Jalil/0000-0001-7847-5493
   Sedaghatfar, Ezatollah/0000-0001-9834-2743
   Tavakoli, Mohammadbagher/0000-0001-7829-7885},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Acharya T, 2005, IMAGE PROCESSING: PRINCIPLES AND APPLICATIONS, P1, DOI 10.1002/0471745790.
   Anant Bhardwaj, 2013, International Journal of Innovation and Applied Studies, V3, P237.
   {[}Anonymous], 1998, MACHINE LEARNING ECM, DOI DOI 10.1007/BFB0026666.
   {[}Anonymous], 2013, INT J ENG SCI TECHNO.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   FAYYAD UM, 1992, MACH LEARN, V8, P87, DOI 10.1023/A:1022638503176.
   Gwo CY, 2013, APPL PLANT SCI, V1, DOI 10.3732/apps.1200005.
   Islam M. A., 2019, INT J COMPUTER IJC, V33, P26.
   Iwata T, 2013, PROC SICE ANN CONF, P2489.
   Kadir Abdul, 2012, Journal of Theoretical and Applied Information Technology, V41, P82.
   Kadir A., 2013, INT J COMPUT TRENDS, V1, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Kheirkhah FM, 2019, IET COMPUT VIS, V13, P369, DOI 10.1049/iet-cvi.2018.5028.
   Lavania S., 2014, P 2014 IEEE INT C CO, P1, DOI DOI 10.1109/ICCIC.2014.7238345.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
   Ryszard S, 2007, ELE COM ENG, V1, P6.
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   SINGH P, 2010, INT J SIGNAL PROCESS, V3, P67.
   Too J, 2019, COMPUTATION, V7, DOI 10.3390/computation7010012.
   VijayaLakshmi B, 2016, COMPUT ELECTRON AGR, V125, P99, DOI 10.1016/j.compag.2016.04.033.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zheng X, 2009, IEEE POW ENER SOC GE, P810.},
Number-of-Cited-References = {23},
Times-Cited = {9},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Trait. Signal},
Doc-Delivery-Number = {LB9YB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000524986200003},
OA = {Bronze},
DA = {2023-08-12},
}

@inproceedings{ WOS:000484469700008,
Author = {Zhang, Shanwen and Wang, Zhen and Shi, Yun},
Editor = {Huang, DS and Jo, KH and Zhang, XL},
Title = {Multi-modal Plant Leaf Recognition Based on Centroid-Contour Distance
   and Local Discriminant Canonical Correlation Analysis},
Booktitle = {INTELLIGENT COMPUTING THEORIES AND APPLICATION, PT II},
Series = {Lecture Notes in Computer Science},
Year = {2018},
Volume = {10955},
Pages = {61-66},
Note = {14th International Conference on Intelligent Computing (ICIC), Wuhan,
   PEOPLES R CHINA, AUG 15-18, 2018},
Organization = {IEEE Computat Intelligence Soc; Int Neural Network Soc; Natl Sci Fdn
   China; Tongji Univ; Wuhan Univ Sci \& Technol; Wuhan Inst Technol},
Abstract = {Leaf based plant species recognition plays an important research, but it
   is a challenging work because of the complexity and diversity of plant
   leaves. A multi-modal plant leaf recognition method is proposed based on
   centroid-contour distance (CCD) and local discriminant canonical
   correlation analysis (LDCCA). First, the CCD feature vector is extracted
   from each leaf image. Second, the extracted feature vectors of any two
   within-class leaves are integrated by LDCCA. Final, K-nearest neighbor
   classifier is applied to plant recognition. The experiment results on a
   public dataset validated the effectiveness of the proposed method.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Wang, Z (Corresponding Author), Xijing Univ, Dept Informat Engn, Xian 710123, Shaanxi, Peoples R China.
   Zhang, Shanwen; Wang, Zhen; Shi, Yun, Xijing Univ, Dept Informat Engn, Xian 710123, Shaanxi, Peoples R China.},
DOI = {10.1007/978-3-319-95933-7\_8},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-95932-0; 978-3-319-95933-7},
Keywords = {Plant recognition; Centroid-contour distance; Local discriminant
   canonical correlation analysis (LDCCA); Feature extraction},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {wjdw716@163.com},
Affiliations = {Xijing University},
ResearcherID-Numbers = {Wang, Zhen/A-2309-2011},
ORCID-Numbers = {Wang, Zhen/0000-0002-9182-4421},
Funding-Acknowledgement = {China National Natural Science Foundation {[}61473237]; Shaanxi
   Department of Science and Technology {[}2017ZDXM-NY-088, 2016GY-141]},
Funding-Text = {This work is supported by the China National Natural Science Foundation
   under grant Nos. 61473237, key research and development projects
   (2017ZDXM-NY-088), Key project (2016GY-141) of Shaanxi Department of
   Science and Technology. The authors would like to thank all the editors
   and anonymous reviewers for their constructive advice.},
Cited-References = {Arai Kohei, 2013, International Journal of Advanced Research in Artificial Intelligence, V2, P16.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Du JX, 2016, NEUROCOMPUTING, V188, P131, DOI 10.1016/j.neucom.2014.10.113.
   Fern BM, 2014, ADV SCI LETT, V20, P209, DOI 10.1166/asl.2014.5300.
   Hasim A, 2016, IOP C SER EARTH ENV, V31, DOI 10.1088/1755-1315/31/1/012002.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Huang XY, 2017, IEEE GEOSCI REMOTE S, V14, P2102, DOI 10.1109/LGRS.2017.2752800.
   Khmag A, 2017, IEEE ST CONF RES DEV, P467, DOI 10.1109/SCORED.2017.8305438.
   Mouine S, 2013, LECT NOTES COMPUT SC, V7950, P205, DOI 10.1007/978-3-642-39094-4\_24.
   Sabu A, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON INVENTIVE COMMUNICATION AND COMPUTATIONAL TECHNOLOGIES (ICICCT), P145, DOI 10.1109/ICICCT.2017.7975176.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Zeng Q.M., 2017, MULTIMED TOOLS APPL, V76, P1.
   Zhang H., 2015, J COMPUTIONAL SYSTEM, V11, P141.},
Number-of-Cited-References = {13},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BN5PR},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000484469700008},
DA = {2023-08-12},
}

@article{ WOS:000426688200006,
Author = {Wang, Zhaobin and Sun, Xiaoguang and Yang, Zekun and Zhang, Yaonan and
   Zhu, Ying and Ma, Yide},
Title = {Leaf Recognition Based on DPCNN and BOW},
Journal = {NEURAL PROCESSING LETTERS},
Year = {2018},
Volume = {47},
Number = {1},
Pages = {99-115},
Month = {FEB},
Abstract = {Leaf classification is an interesting and important research. Current
   work focuses mainly on feature extraction, especially on textural
   feature extraction. In this case, we propose a new method of leaf
   recognition based on bag of words (BOW) and entropy sequence (EnS). In
   our method, EnS is firstly obtained by dual-output pulse-coupled neural
   network and then it is improved by BOW. Locality-constrained linear
   coding method is used for sparse coding. Then, the classification system
   is built where the linear support vector machine is taken as classifier.
   Some representative datasets and existing methods are employed to
   evaluate the effect of the proposed method. Finally, experimental
   results show that the accuracy of our proposed method is better than
   existing methods.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Wang, ZB (Corresponding Author), Lanzhou Univ Lanzhou, Sch Informat Sci \& Engn, Lanzhou 730000, Gansu, Peoples R China.
   Wang, ZB (Corresponding Author), Chinese Acad Sci Lanzhou, Cold \& Arid Reg Environm \& Engn Res Inst, Lanzhou 730000, Gansu, Peoples R China.
   Wang, Zhaobin; Sun, Xiaoguang; Yang, Zekun; Ma, Yide, Lanzhou Univ Lanzhou, Sch Informat Sci \& Engn, Lanzhou 730000, Gansu, Peoples R China.
   Wang, Zhaobin; Zhang, Yaonan, Chinese Acad Sci Lanzhou, Cold \& Arid Reg Environm \& Engn Res Inst, Lanzhou 730000, Gansu, Peoples R China.
   Zhu, Ying, Gansu Acad Sci, Inst Biol, Lanzhou 730000, Peoples R China.},
DOI = {10.1007/s11063-017-9635-1},
ISSN = {1370-4621},
EISSN = {1573-773X},
Keywords = {Feature extraction; Plant recognition; SVM; DPCNN; BOW},
Keywords-Plus = {IMAGE SEGMENTATION; PLANT; REPRESENTATION; CLASSIFICATION; TEXTURE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {zhaobin\_wang@hotmail.com},
Affiliations = {Chinese Academy of Sciences; Cold \& Arid Regions Environmental \&
   Engineering Research Institute, CAS},
ResearcherID-Numbers = {zhang, yuanyuan/GYA-4428-2022},
Funding-Acknowledgement = {Fundamental Research Funds for the Central Universities
   {[}lzujbky-2015-197]; China Postdoctoral Science Foundation
   {[}2013M532097]; National Science Foundation of China {[}61201421];
   Science Foundation of Gansu Province of China {[}1208RJYA058]},
Funding-Text = {This work was jointly supported by Fundamental Research Funds for the
   Central Universities (Grant No. lzujbky-2015-197), China Postdoctoral
   Science Foundation (Grant No. 2013M532097), National Science Foundation
   of China (Grant No. 61201421) and Science Foundation of Gansu Province
   of China (No. 1208RJYA058).},
Cited-References = {Ahmad A, 2007, DATA KNOWL ENG, V63, P503, DOI 10.1016/j.datak.2007.03.016.
   {[}Anonymous], 1994, LOS ALAMOS SCI, DOI DOI 10.3724/SP.J.1087.2010.01536.
   {[}Anonymous], 2013, INT J ENG SCI TECHNO.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Daliri MR, 2008, PATTERN RECOGN, V41, P1782, DOI 10.1016/j.patcog.2007.10.020.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   Du JX, 2013, NEUROCOMPUTING, V116, P150, DOI 10.1016/j.neucom.2012.03.028.
   Ebied HM, 2013, NEURAL COMPUT APPL, V22, P1211, DOI 10.1007/s00521-012-0889-2.
   Ekblad U, 2004, NUCL INSTRUM METH A, V525, P392, DOI 10.1016/j.nima.2004.03.102.
   Fan RE, 2008, J MACH LEARN RES, V9, P1871.
   Felzenszwalb PF, 2007, P 2007 IEEE C COMP V.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Hall D, 2015, P IEEE WINT C APPL C.
   Hsiao J-K, 2014, P IEEE SCI INF C LON.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Im C, 1998, P IAPR WORKSH MACH V.
   Kadir Abdul, 2012, Journal of Theoretical and Applied Information Technology, V41, P82.
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Kim Seong-Mo, 2012, Evid Based Complement Alternat Med, V2012, P578497, DOI 10.1155/2012/578497.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Kumar N, 2012, P EUR C COMP VIS FLO.
   Kuntimad G, 1999, IEEE T NEURAL NETWOR, V10, P591, DOI 10.1109/72.761716.
   Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, P2169, DOI DOI 10.1109/CVPR.2006.68.
   Lei YK, 2014, COMPUT VIS IMAGE UND, V119, P116, DOI 10.1016/j.cviu.2013.12.001.
   Li XJ, 2012, NEURAL COMPUT, V24, P194, DOI 10.1162/NECO\_a\_00194.
   Lin F-Y, 2008, P INT C INT COMP.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Ma Y, 2002, CHINESE SCI BULL, V47, P167.
   Mahmoud MA, 2016, BIOTECHNOL BIOTEC EQ, V30, P1090, DOI 10.1080/13102818.2016.1228480.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Saitoh T, 2004, J APPL PHYS, V66, P3014.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Suk T, 2013, COMP LEAF RECOGNITIO.
   Syahputra Hermawan, 2014, Journal of Computer Science, V10, P697, DOI 10.3844/jcssp.2014.697.704.
   Tolba MF, 2010, CAN J ARTIF INTELL M, V1, P1.
   Tsolakidis DG, 2014, P HELL C ART INT IOA.
   Uluturk C, 2012, P 2012 INT S INN INT.
   Valliammal N, 2012, INT J COMPUT COMMUN, V6, P1.
   Wang J, 2010, P 2010 IEEE C COMP V.
   Wang Q, 2015, NEUROCOMPUTING, V158, P174, DOI 10.1016/j.neucom.2015.01.054.
   Wang XG, 2014, PATTERN RECOGN, V47, P2116, DOI 10.1016/j.patcog.2013.12.008.
   Wang Z., 2016, INT C MACHINE LEARNI, P2939, DOI DOI 10.1007/S11831-016-9181-4.
   Wang Z, 2011, P 2011 INT C DIG IM.
   Wang Z, 2014, P 2014 INT JOINT C N.
   Wang ZB, 2016, ARCH COMPUT METHOD E, V23, P659, DOI 10.1007/s11831-015-9154-z.
   Wang ZB, 2010, IMAGE VISION COMPUT, V28, P5, DOI 10.1016/j.imavis.2009.06.007.
   Wei S, 2011, NEUROCOMPUTING, V74, P1485, DOI 10.1016/j.neucom.2011.01.005.
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yuan Y, 2017, IEEE T INTELL TRANSP, V18, P1918, DOI 10.1109/TITS.2016.2614548.
   Yuan Y, 2016, IEEE T CYBERNETICS, V46, P2966, DOI 10.1109/TCYB.2015.2484324.
   Yuan Y, 2015, IEEE T CYBERNETICS, V45, P562, DOI 10.1109/TCYB.2014.2330853.
   Zhan K, 2009, IEEE T NEURAL NETWOR, V20, P1980, DOI 10.1109/TNN.2009.2030585.
   Zhang H-J, 2008, P 2007 INT S INT SIG.
   Zhang SW, 2013, PATTERN RECOGN, V46, P1891, DOI 10.1016/j.patcog.2013.01.015.
   Zhang SW, 2011, NEUROCOMPUTING, V74, P2284, DOI 10.1016/j.neucom.2011.03.007.
   Zheru C, 2003, P 2003 INT C NEUR NE.},
Number-of-Cited-References = {61},
Times-Cited = {4},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {21},
Journal-ISO = {Neural Process. Lett.},
Doc-Delivery-Number = {FY3AC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000426688200006},
DA = {2023-08-12},
}

@article{ WOS:000695176900001,
Author = {Heidary-Sharifabad, Ahmad and Zarchi, Mohsen Sardari and Emadi, Sima and
   Zarei, Gholamreza},
Title = {Efficient Deep Learning Models for Categorizing Chenopodiaceae in the
   Wild},
Journal = {INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE},
Year = {2021},
Volume = {35},
Number = {10},
Month = {AUG},
Abstract = {The Chenopodiaceae species are ecologically and financially important,
   and play a significant role in biodiversity around the world.
   Biodiversity protection is critical for the survival and sustainability
   of each ecosystem and since plant species recognition in their natural
   habitats is the first process in plant diversity protection, an
   automatic species classification in the wild would greatly help the
   species analysis and consequently biodiversity protection on earth.
   Computer vision approaches can be used for automatic species analysis.
   Modern computer vision approaches are based on deep learning techniques.
   A standard dataset is essential in order to perform a deep learning
   algorithm. Hence, the main goal of this research is to provide a
   standard dataset of Chenopodiaceae images. This dataset is called ACHENY
   and contains 27030 images of 30 Chenopodiaceae species in their natural
   habitats. The other goal of this study is to investigate the
   applicability of ACHENY dataset by using deep learning models.
   Therefore, two novel deep learning models based on ACHENY dataset are
   introduced: First, a lightweight deep model which is trained from
   scratch and is designed innovatively to be agile and fast. Second, a
   model based on the EfficientNet-B1 architecture, which is pre-trained on
   ImageNet and is fine-tuned on ACHENY. The experimental results show that
   the two proposed models can do Chenopodiaceae fine-grained species
   recognition with promising accuracy. To evaluate our models, their
   performance was compared with the well-known VGG-16 model after
   fine-tuning it on ACHENY. Both VGG-16 and our first model achieved about
   80\% accuracy while the size of VGG-16 is about 16x larger than the
   first model. Our second model has an accuracy of about 90\% and
   outperforms the other models where its number of parameters is 5x than
   the first model but it is still about one-third of the VGG-16
   parameters.},
Publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
Address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
Type = {Article},
Language = {English},
Affiliation = {Zarchi, MS (Corresponding Author), Meybod Univ, Dept Comp Engn, Meybod, Iran.
   Heidary-Sharifabad, Ahmad; Emadi, Sima, Islamic Azad Univ, Dept Comp Engn, Yazd Branch, Yazd, Iran.
   Zarchi, Mohsen Sardari, Meybod Univ, Dept Comp Engn, Meybod, Iran.
   Zarei, Gholamreza, Islamic Azad Univ, Maybod Branch, Dept Agron, Maybod, Iran.},
DOI = {10.1142/S0218001421520157},
Article-Number = {2152015},
ISSN = {0218-0014},
EISSN = {1793-6381},
Keywords = {Biodiversity protection; Chenopodiaceae; deep learning; convolutional
   neural networks; image classification; plant classification; standard
   dataset},
Keywords-Plus = {PLANT-IDENTIFICATION; CLASSIFICATION; RECOGNITION; FEATURES; VISION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {ahmad.heidary@maybodiau.ac.ir
   sardari@meybod.ac.ir
   emadi@iauyazd.ac.ir
   zareigholamreza@maybodiau.ac.ir},
Affiliations = {Islamic Azad University; Islamic Azad University},
ResearcherID-Numbers = {Zarei, Gholamreza/HZJ-8437-2023
   Zarchi, Mohsen Sardari/AAA-7240-2022
   },
ORCID-Numbers = {Zarchi, Mohsen Sardari/0000-0003-0831-3426
   , Ahmad/0000-0002-3356-4334},
Cited-References = {{[}Anonymous], NATURE, DOI 10.1038/nature14539.
   Backes AR, 2009, INT J PATTERN RECOGN, V23, P1145, DOI 10.1142/S0218001409007508.
   Buschbacher K, 2020, ECOL INFORM, V55, DOI 10.1016/j.ecoinf.2019.101017.
   Casati P, 1999, PHYTOCHEMISTRY, V52, P985, DOI 10.1016/S0031-9422(99)00355-6.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chollet F., 2018, DEEP LEARNING PYTHON.
   Dayrat B, 2005, BIOL J LINN SOC, V85, P407, DOI 10.1111/j.1095-8312.2005.00503.x.
   Fu H., 2004, CHIN B BOT, V21, P429.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Geetharamani G, 2019, COMPUT ELECTR ENG, V76, P323, DOI 10.1016/j.compeleceng.2019.04.011.
   Goeau H., 2012, P 1 ACM INT WORKSH M, P41.
   Goeau H., 2015, CEUR WORKSH P.
   Goeau H., 2017, CLEF 2017 C LABS EVA, P1.
   Goeau H., 2016, WORKING NOTES CLEF 2, P428.
   GUYER DE, 1986, T ASAE, V29, P1500.
   He K., 2016, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2016.90.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448.
   Jeon WS, 2017, INT J FUZZY LOG INTE, V17, P26, DOI 10.5391/IJFIS.2017.17.1.26.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Karami E., ARXIV171002726.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Kelleher JD, 2015, FUNDAMENTALS OF MACHINE LEARNING FOR PREDICTIVE DATA ANALYTICS: ALGORITHMS, WORKED EXAMPLES, AND CASE STUDIES, P1.
   King DB, 2015, ACS SYM SER, V1214, P1.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee HH, 2017, IMAGE VISION COMPUT, V61, P98, DOI 10.1016/j.imavis.2017.01.013.
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Liu NA, 2016, NEUROCOMPUTING, V216, P460, DOI 10.1016/j.neucom.2016.08.005.
   Lopez-Jimenez E, 2019, ECOL INFORM, V52, P131, DOI 10.1016/j.ecoinf.2019.05.005.
   Montufar G., 2014, ADV NEURAL INF PROCE, V4, P2924, DOI {[}10.48550/arxiv.1402.1869, DOI 10.48550/ARXIV.1402.1869].
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Nilsback M.-E., 2006, P IEEE COMP SOC C CO, V2, P1447, DOI DOI 10.1109/CVPR.2006.42.
   Nilsback ME, 2010, IMAGE VISION COMPUT, V28, P1049, DOI 10.1016/j.imavis.2009.10.001.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Ozturk S, 2019, COMPUT ELECTR ENG, V76, P299, DOI 10.1016/j.compeleceng.2019.04.012.
   Passalis N, 2017, PATTERN RECOGN, V64, P277, DOI 10.1016/j.patcog.2016.11.014.
   Pawara P, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P479, DOI 10.5220/0006196204790486.
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009.
   Plotze RD, 2009, INT J PATTERN RECOGN, V23, P247, DOI 10.1142/S0218001409007156.
   Postalcioglu S, 2020, INT J PATTERN RECOGN, V34, DOI 10.1142/S0218001420510039.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Seeland M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170629.
   Sekeroglu B, 2016, PROCEDIA COMPUT SCI, V102, P578, DOI 10.1016/j.procs.2016.09.445.
   Sfar AR, 2015, INT J COMPUT VISION, V111, P255, DOI 10.1007/s11263-014-0743-3.
   Simonyan K., 2014, 14091556V6 ARXIV, P1, DOI DOI 10.48550/ARXIV.1409.1556.
   Soderkvist O., 2001, THESIS.
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002.
   Tan M., ARXIV180711626.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Nguyen TTN, 2018, COMPUT ELECTRON AGR, V155, P412, DOI 10.1016/j.compag.2018.10.042.
   Tieleman T., 2012, COURSERA NEURAL NETW.
   Torresani M, 2019, ECOL INFORM, V52, P26, DOI 10.1016/j.ecoinf.2019.04.001.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang YanJun, 2014, Journal of China Agricultural University, V19, P180.
   Wang Y, 2018, INT J PATTERN RECOGN, V32, DOI 10.1142/S0218001418500453.
   Wu A, 2020, COMPUT ELECTR ENG, V81, DOI 10.1016/j.compeleceng.2019.106454.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xiao QG, 2018, ECOL INFORM, V48, P117, DOI 10.1016/j.ecoinf.2018.09.001.
   Yuan X., 2011, IEEE INT C IM PROC.
   Zarchi MS, 2018, INT J MED INFORM, V114, P81, DOI 10.1016/j.ijmedinf.2018.03.003.
   Zarchi MS, 2016, PATTERN RECOGN, V53, P174, DOI 10.1016/j.patcog.2015.11.010.
   Zarchi MS, 2015, COMPUT ELECTR ENG, V46, P303, DOI 10.1016/j.compeleceng.2015.06.018.
   Zhang D., 2019, INT J PATTERN RECOGN, V33.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhong LH, 2019, REMOTE SENS ENVIRON, V221, P430, DOI 10.1016/j.rse.2018.11.032.
   Zhu HY, 2018, MULTIMED TOOLS APPL, V77, P29779, DOI 10.1007/s11042-017-5578-9.
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907.},
Number-of-Cited-References = {69},
Times-Cited = {5},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {17},
Journal-ISO = {Int. J. Pattern Recognit. Artif. Intell.},
Doc-Delivery-Number = {UP1WI},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000695176900001},
DA = {2023-08-12},
}

@article{ WOS:000234724400008,
Author = {Miao, ZJ and Gandelin, MH and Yuan, BZ},
Title = {An OOPR-based rose variety recognition system},
Journal = {ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE},
Year = {2006},
Volume = {19},
Number = {1},
Pages = {79-101},
Month = {FEB},
Abstract = {This paper describes a rose analysis and recognition system and presents
   the main principles used to realize the recognition system. The major
   principles presented in this paper are the mathematical description
   methods for rose features such as shape, size and color of the flower,
   petal, leaf, etc, and the object-oriented pattern recognition (OOPR)
   approach which mathematically deals with how to comprehensively use all
   different rose features rationally in the recognition scheme. The
   recognition system is described and some of its experimental results are
   given which demonstrate the efficiency of our methods. (c) 2005 Elsevier
   Ltd. All rights reserved.},
Publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Miao, ZJ (Corresponding Author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.},
DOI = {10.1016/j.engappai.2005.05.009},
ISSN = {0952-1976},
Keywords = {pattern recognition; intelligent systems; image processing; color image;
   artificial intelligence; image segmentation},
Keywords-Plus = {COMPUTER VISION; MACHINE VISION; IMAGE-ANALYSIS; PLANT-IDENTIFICATION;
   CLASSIFICATION; DISCRIMINATION; SEGMENTATION; AGRICULTURE; WHEAT},
Research-Areas = {Automation \& Control Systems; Computer Science; Engineering},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Artificial
   Intelligence; Engineering, Multidisciplinary; Engineering, Electrical \&
   Electronic},
Author-Email = {zjmiao@center.njtu.edu.cn},
Affiliations = {Beijing Jiaotong University},
Cited-References = {{[}Anonymous], 1987, FUZZY SETS DECISION.
   {[}Anonymous], EVIDENCE THEORY ITS.
   {[}Anonymous], EVIDENCE THEORY ITS.
   BARNETT JA, 1991, IEEE T PATTERN ANAL, V13, P599, DOI 10.1109/34.87345.
   BEVERIDGE JR, 1989, INT J COMPUT VISION, V2, P311, DOI 10.1007/BF00158168.
   DRAPER S R, 1989, Plant Varieties and Seeds, V2, P53.
   Fixsen D, 1997, IEEE T SYST MAN CY A, V27, P96, DOI 10.1109/3468.553228.
   FU KS, 1981, PATTERN RECOGN, V13, P3, DOI 10.1016/0031-3203(81)90028-5.
   GUNASEKARAN S, 1988, T ASAE, V31, P1264.
   GUYER DE, 1986, T ASAE, V29, P1500.
   GUYER DE, 1993, T ASAE, V36, P163.
   HAMALAINEN JJ, 1993, BIOTECHNOL BIOENG, V41, P35, DOI 10.1002/bit.260410106.
   HARALICK RM, 1985, COMPUT VISION GRAPH, V29, P100, DOI 10.1016/S0734-189X(85)90153-7.
   Jain A.K., 1989, FUNDAMENTALS DIGITAL.
   KRANZLER GA, 1985, AGR ENG, V66, P11.
   LUO Z, 1994, P 1994 IEEE INT C MU, P403.
   Mahler RPS, 1996, IEEE T SYST MAN CY A, V26, P27, DOI 10.1109/3468.477858.
   MILLER BK, 1989, ACTA HORTIC, V254, P161.
   NEUMAN MR, 1989, J CEREAL SCI, V10, P183, DOI 10.1016/S0733-5210(89)80047-5.
   PRICE TV, 1990, CRIT REV PLANT SCI, V9, P235, DOI 10.1080/07352689009382289.
   REHKUGLER GE, 1986, T ASAE, V29, P1388.
   SAPIRSTEIN HD, 1987, J CEREAL SCI, V6, P3, DOI 10.1016/S0733-5210(87)80035-8.
   SARKAR N, 1985, T ASAE, V28, P1714.
   Terano T., 1992, FUZZY SYSTEMS THEORY.
   {*}UPOV, 1990, GUID COND TESTS DIST.
   VANDEVOOREN JG, 1991, EUPHYTICA, V57, P245, DOI 10.1007/BF00039670.
   VOORBRAAK F, 1991, ARTIF INTELL, V48, P171, DOI 10.1016/0004-3702(91)90060-W.
   WILHOIT JH, 1990, T ASAE, V33, P1736.
   YOUNG TY, 1986, HDB PATTERN RECOGNIT.
   ZAYAS I, 1989, CEREAL CHEM, V66, P233.
   ZHENJIANG M, 2005, 2005 JOINT C INF SCI, V4.
   ZHENJIANG M, 1997, COMPUTERIZED ROSE VA.
   ZHENJIANG M, 2000, PATTERN RECOGNIT LET, V21, P169.
   ZHENJIANG M, 2002, P 6 INT C SIGN PROC.
   ZHENJIANG M, 1994, ENG APPL ARTIF INTEL, V7, P593.
   ZHENJIANG M, 1995, P 6 INT C SIGN PROC.
   ZHENJIANG M, 1997, P 6 IEEE INT C FUZZ, V2, P1083.
   ZHONGXIONG H, 1985, FUZZY MATH ITS APPL.
   Zimmermann H. J., 1991, FUZZY SETS THEORY IT.},
Number-of-Cited-References = {39},
Times-Cited = {7},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Eng. Appl. Artif. Intell.},
Doc-Delivery-Number = {004AM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000234724400008},
DA = {2023-08-12},
}

@inproceedings{ WOS:000189460400050,
Author = {Fu, H and Chi, ZR},
Book-Group-Author = {IEEE},
Title = {A two-stage approach for leaf vein extraction},
Booktitle = {PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS \&
   SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2},
Year = {2003},
Pages = {208-211},
Note = {International Conference on Neural Networks and Signal Processing,
   Nanjing, PEOPLES R CHINA, DEC 14-17, 2003},
Organization = {IEEE CASS Shanghai Chapter; IEEE CASS; IEEE Beijing Sect; Chinese Neural
   Networks Council; Jiangsu Elect Soc, Circuits \& Informat Proc Comm;
   IEEE R 10 CASS; CASS CIE, Neural Networks \& Signal Proc Comm; SE Univ},
Abstract = {Living plant recognition is a promising but challenging task in the
   fields of pattern recognition and computer vision. As an inherent trait
   the leaf vein definitely contains the important information for plant
   species recognition despite of its complex modality. In this paper, an
   efficient two-stage approach is presented for leaf vein extraction. At
   the first stage, a preliminary segmentation based on the intensity
   histogram of the leaf image is carried out to estimate the rough regions
   of vein pixels. This is followed at the second stage by a fine checking
   using a trained artificial neural network (ANN) classifier. Ten features
   distilled from a window centered at the pixel are used as the input to
   tram the ANN classifier Compared with conventional edge detection
   methods, experimental results show that the proposed method is capable
   of extracting more precise venation modality of the leaf for the
   subsequent leaf recognition.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Chi, ZR (Corresponding Author), Hong Kong Polytech Univ, Ctr Multimedia Signal Proc, Dept Elect \& Informat Eng, Kowloon, Hong Kong, Peoples R China.
   Hong Kong Polytech Univ, Ctr Multimedia Signal Proc, Dept Elect \& Informat Eng, Kowloon, Hong Kong, Peoples R China.},
ISBN = {0-7803-7702-8},
Research-Areas = {Computer Science; Engineering; Imaging Science \& Photographic
   Technology},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic; Imaging Science \& Photographic Technology},
Author-Email = {enzheru@polyu.edu.hk},
Affiliations = {Hong Kong Polytechnic University},
ORCID-Numbers = {fu, hong/0000-0003-2246-7552},
Cited-References = {BOUVEIA F, 1997, ISIE 97, P757.
   Chi Z., 1996, FUZZY ALGORITHMS APP.
   Haykin S., 1994, NEURAL NETWORKS COMP.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   Pratt W, 2013, DIGITAL IMAGE PROCES.
   Soille P, 2000, IMAGE VISION COMPUT, V18, P1025, DOI 10.1016/S0262-8856(00)00043-3.
   WANG Z, 2000, LECT NOTES COMPUTER, P477.},
Number-of-Cited-References = {7},
Times-Cited = {15},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BY77R},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000189460400050},
DA = {2023-08-12},
}

@inproceedings{ WOS:000333669300075,
Author = {Prasad, Shitala and Peddoju, Sateesh K. and Ghosh, Debashis},
Book-Group-Author = {IEEE},
Title = {Mobile Plant Species Classification: A Low Computational Aproach},
Booktitle = {2013 IEEE SECOND INTERNATIONAL CONFERENCE ON IMAGE INFORMATION
   PROCESSING (ICIIP)},
Year = {2013},
Pages = {405-409},
Note = {IEEE 2nd International Conference on Image Information Processing
   (ICIIP), Jaypee Univ Informat Technol, Shimla, INDIA, DEC 09-11, 2013},
Organization = {IEEE; IEEE Delhi Sect; IEEE Jaypee Univ Informat Technol Student Branch;
   Jaypee Grp; Comp Soc India Special Interest Grp Cyber Forens; Jaypee
   Univ Informat Technol, Dept CSE; Jaypee Univ Informat Technol, Dept ICT},
Abstract = {In this paper a reduced shape and color feature extraction method is
   proposed for a mobile device based plant classification system. For
   scientists, botanists, farmers, and others plant identification is a
   useful and important task. The original image captured is reduced to
   similar aspect ratio which does not affect the shape information but
   reduces the computation cost nearly up to half of the total cost. The
   algorithm first calculates the geometric feature and then polar Fourier
   transform and trained using k-NN classifier. Then two nearest classes
   were selected on the basics of smallest distance which is further
   rectified by the color features using a decision tree. The algorithm
   proves to be better in performance compared to other already existing
   algorithms.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Prasad, S (Corresponding Author), Indian Inst Technol, Dept Comp Sci \& Engn, Roorkee, Haridwar, India.
   Prasad, Shitala; Peddoju, Sateesh K., Indian Inst Technol, Dept Comp Sci \& Engn, Roorkee, Haridwar, India.
   Ghosh, Debashis, Indian Inst Technol, Dept Elect Commun Engn, Roorkee, Uttar Pradesh, India.},
ISBN = {978-1-4673-6099-9; 978-1-4673-6100-2},
Keywords = {Polar Fourier Transform; Plant Identification; Leaf Recognition; Color
   Features; Geometric Shape Features; k-NN; Low Computation},
Keywords-Plus = {SHAPE},
Research-Areas = {Computer Science; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Computer Science, Information Systems; Imaging Science \& Photographic
   Technology},
Author-Email = {shitala@ieee.org
   sateesh@ieee.org
   ghoshfec@iitr.ac.in},
Affiliations = {Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Roorkee},
ResearcherID-Numbers = {Prasad, Shitala/AAI-8449-2020
   Peddoju, Sateesh K/I-7249-2016},
ORCID-Numbers = {Peddoju, Sateesh K/0000-0003-0202-3196},
Cited-References = {{[}Anonymous], IEEE 7 INT S SIGN PR.
   Aptoula E, 2009, IEEE T IMAGE PROCESS, V18, P2505, DOI 10.1109/TIP.2009.2027363.
   Arivazhagan S., 2013, Agricultural Engineering International: CIGR Journal, V15, P211.
   Backes AR, 2010, PATTERN RECOGN, V43, P685, DOI 10.1016/j.patcog.2009.07.017.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Burrascano P., 2010, P 7 INT C ENG APPL N.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   Gang L., 2009, INT C GEOSP SOL EM M, P25.
   Man QK, 2008, COMM COM INF SC, V15, P192.
   Prasad S., 2011, 2011 2nd International Conference on Computer and Communication Technology, P646, DOI 10.1109/ICCCT.2011.6075212.
   Prasad S., 2011, P 2011 INT C COMM CO, P343, DOI {[}10.1145/1947940.1948012, DOI 10.1145/1947940.1948012].
   White S, 2006, IEEE SYMPOSIUM ON 3D USER INTERFACES 2006, PROCEEDINGS, P119, DOI 10.1109/TRIDUI.2006.1618281.
   Wu Q., 2006, ADV ARTIFICIAL INTEL, V3, P5.
   Zhang Dengsheng, 2002, THESIS.},
Number-of-Cited-References = {15},
Times-Cited = {7},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BA2MP},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000333669300075},
DA = {2023-08-12},
}

@inproceedings{ WOS:000528794100005,
Author = {Mamani Diaz, Carlos A. and Medina Castaneda, Edgar E. and Mugruza
   Vassallo, Carlos A.},
Editor = {Risnandar},
Title = {DEEP LEARNING FOR PLANT CLASSIFICATION IN PRECISION AGRICULTURE},
Booktitle = {2019 INTERNATIONAL CONFERENCE ON COMPUTER, CONTROL, INFORMATICS AND ITS
   APPLICATIONS (IC3INA)},
Year = {2019},
Pages = {9-13},
Note = {7th Annual International Conference on Computer, Control, Informatics
   and its Applications (IC3INA), Tangerang, INDONESIA, OCT 23-24, 2019},
Organization = {Indonesian Inst Sci, Res Ctr Informat; IEEE Indonesia Sect},
Abstract = {Deep learning has emerged with big data technologies and
   high-performance computing to create new opportunities for data
   intensive science in the multidisciplinary agriculture technologies
   domain. In this research, we present a deep learning classification
   system of diverse plants, in order to enable precision agriculture
   applications. This classification problem was achieved thanks to the
   public dataset ``Plant Seedlings Dataset{''}, which contains images of
   approximately 960 unique plants belonging to 12 species at several
   growth stages. The database has been from Aarhus University Flakkebjerg
   Research Station in collaboration between the University of Southern
   Denmark and Aarhus University. A classification comparison was used to
   determinate which of three pre-trained models; InceptionV3, VGG16 and
   Xception; reach the best accuracy performance for the database used in
   this work. Results determined that (1) Xception was the best model for
   plant classification obtaining 86.21\%, overcoming other networks in
   7.37\% with a time processing around 741 seconds. (2) GPU hardware
   changes the classification model results impacting strongly in their
   accuracy score.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Diaz, CAM (Corresponding Author), Natl Univ Technol South Lima UNTELS, Dept Elect \& Telecommun Engn, Lima, Peru.
   Mamani Diaz, Carlos A., Natl Univ Technol South Lima UNTELS, Dept Elect \& Telecommun Engn, Lima, Peru.
   Medina Castaneda, Edgar E., Univ Fed Rio de Janeiro, Dept Elect Engn, Rio De Janeiro, Brazil.
   Mugruza Vassallo, Carlos A., Natl Univ Technol South Lima UNTELS, Res Grp Comp \& Cognit Neurosci, Lima, Peru.},
DOI = {10.1109/ic3ina48034.2019.8949612},
ISBN = {978-1-7281-5540-1},
Keywords = {Precision Agriculture; Deep Learning; Machine Learning; Xception;
   Inception V3; VGG 16},
Keywords-Plus = {MACHINES},
Research-Areas = {Automation \& Control Systems; Computer Science; Engineering},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Theory \& Methods;
   Engineering, Electrical \& Electronic},
Author-Email = {carlosmdiaz59@gmail.com
   emedina@pads.ufrj.br
   cmugruza@yahoo.com},
Affiliations = {Universidade Federal do Rio de Janeiro},
ORCID-Numbers = {Mugruza Vassallo, Carlos/0000-0002-9262-7198},
Cited-References = {AgroSmart, 2018, TECNOLOGY.
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   FarmersEdge, 2005, SMART SOL.
   Guerrero JM, 2012, EXPERT SYST APPL, V39, P11149, DOI 10.1016/j.eswa.2012.03.040.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Kamilaris A, 2017, COMPUT ELECTRON AGR, V143, P23, DOI 10.1016/j.compag.2017.09.037.
   Louridas P, 2016, IEEE SOFTWARE, V33, P110, DOI 10.1109/MS.2016.114.
   Mello U., 2018, BRINGING POWER WATSO.
   Mochida K, 2019, GIGASCIENCE, V8, DOI 10.1093/gigascience/giy153.
   MOSGAARD T, 2017, ARXIV171105458.
   Peng B, 2018, GEOPHYS RES LETT, V45, P9662, DOI 10.1029/2018GL079291.
   Pierce FJ, 1999, ADV AGRON, V67, P1, DOI 10.1016/S0065-2113(08)60513-1.
   Ray DK, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0066428.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Sailog, 2019, AGRIO.
   Sciforce, 2019, SMART FARMING FUTURE.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   SISTLER FE, 1987, IEEE T ROBOTIC AUTOM, V3, P3, DOI 10.1109/JRA.1987.1087074.
   Sumi L, 2016, 2016 FOURTH INTERNATIONAL CONFERENCE ON PARALLEL, DISTRIBUTED AND GRID COMPUTING (PDGC), P295, DOI 10.1109/PDGC.2016.7913163.
   Szegedy C., 2015, P IEEE C COMP VIS PA.},
Number-of-Cited-References = {20},
Times-Cited = {7},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BO8OE},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000528794100005},
DA = {2023-08-12},
}

@article{ WOS:000790539700009,
Author = {Jabir, Brahim and Rabhi, Loubna and Falih, Noureddine},
Title = {RNN- and CNN-based weed detection for crop improvement: An overview},
Journal = {FOODS AND RAW MATERIALS},
Year = {2021},
Volume = {9},
Number = {2},
Pages = {387-396},
Abstract = {Introduction. Deep learning is a modern technique for image processing
   and data analysis with promising results and great potential.
   Successfully applied in various fields, it has recently entered the
   field of agriculture to address such agricultural problems as disease
   identification, fruit/plant classification, fruit counting, pest
   identification, and weed detection. The latter was the subject of our
   work. Weeds are harmful plants that grow in crops, competing for things
   like sunlight and water and causing crop yield losses. Traditional data
   processing techniques have several limitations and consume a lot of
   time. Therefore, we aimed to take inventory of deep learning networks
   used in agriculture and conduct experiments to reveal the most efficient
   ones for weed control.
   Study objects and methods. We used new advanced algorithms based on deep
   learning to process data in real time with high precision and
   efficiency. These algorithms were trained on a dataset containing real
   images of weeds taken from Moroccan fields.
   Results and discussion. The analysis of deep learning methods and
   algorithms trained to detect weeds showed that the Convolutional Neural
   Network is the most widely used in agriculture and the most efficient in
   weed detection compared to others, such as the Recurrent Neural Network.
   Conclusion. Since the Convolutional Neural Network demonstrated
   excellent accuracy in weed detention, we adopted it in building a smart
   system for detecting weeds and spraying them in place.},
Publisher = {KEMEROVO STATE UNIV},
Address = {STREET KRASNAYA, 6, KEMEROVO, 650000, RUSSIA},
Type = {Article},
Language = {English},
Affiliation = {Jabir, B (Corresponding Author), Sultan Moulay Slimane Univ, Beni Mellal, Morocco.
   Jabir, Brahim; Rabhi, Loubna; Falih, Noureddine, Sultan Moulay Slimane Univ, Beni Mellal, Morocco.},
DOI = {10.21603/2308-4057-2021-2-387-396},
ISSN = {2308-4057},
EISSN = {2310-9599},
Keywords = {Digital agriculture; weed detection; machine learning; deep learning;
   Convolutional Neural Network (CNN); Recurren Neural Network (RNN)},
Keywords-Plus = {NEURAL-NETWORKS; CLASSIFICATION},
Research-Areas = {Food Science \& Technology},
Web-of-Science-Categories  = {Food Science \& Technology},
Author-Email = {ibra.jabir@gmail.com},
Affiliations = {Sultan Moulay Slimane University of Beni Mellal},
ResearcherID-Numbers = {Jabir, Brahim/AAA-5797-2022},
ORCID-Numbers = {Jabir, Brahim/0000-0002-8762-9199},
Cited-References = {Alloghani M., 2020, SUPERVISED UNSUPERVI, P3, DOI {[}10.1007/978-3-030-22475-2\_1, DOI 10.1007/978-3-030-22475-2\_1].
   {[}Anonymous], NATURE, DOI 10.1038/nature14539.
   Bousetouane F, 2015, LECT NOTES COMPUT SC, V9475, P379, DOI 10.1007/978-3-319-27863-6\_35.
   Duda P, 2020, J ARTIF INTELL SOFT, V10, P15, DOI 10.2478/jaiscr-2020-0002.
   Farooq A, 2019, IEEE GEOSCI REMOTE S, V16, P183, DOI 10.1109/LGRS.2018.2869879.
   Ferguson AL, 2018, J PHYS-CONDENS MAT, V30, DOI 10.1088/1361-648X/aa98bd.
   Ferreira AD, 2017, COMPUT ELECTRON AGR, V143, P314, DOI 10.1016/j.compag.2017.10.027.
   Ganai AF, 2019, 2019 FIFTH INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP 2019), P469, DOI 10.1109/ICIIP47207.2019.8985885.
   Gupta S, 2015, PR MACH LEARN RES, V37, P1737.
   Harsono IW, 2020, J KING SAUD UNIV-COM, V34, P567, DOI 10.1016/j.jksuci.2020.03.013.
   Huu PN, 2021, IND NETWORKS INTELLI, P315, DOI {[}10.1007/978-3-030-77424-0\_26, DOI 10.1007/978-3-030-77424-0\_26].
   Jabir B., 2021, AGRIS On-line Papers in Economics and Informatics, V13, P49, DOI 10.7160/aol.2021.130104.
   Jabir B., 2020, 2020 IEEE 6 INT C OP, DOI DOI 10.1109/ICOA49421.2020.9094450.
   Kamilaris A, 2018, J AGR SCI-CAMBRIDGE, V156, P312, DOI 10.1017/S0021859618000436.
   Kulkarni A, 2021, TURK J COMPUT MATH E, V10, P129, DOI {[}10.17762/turcomat.v12i10.4060, DOI 10.17762/TURCOMAT.V12I10.4060].
   Lammie C, 2019, IEEE ACCESS, V7, P51171, DOI 10.1109/ACCESS.2019.2911709.
   Lin JCW, 2021, KNOWL-BASED SYST, V212, DOI 10.1016/j.knosys.2020.106548.
   Momennejad I, 2020, CURR OPIN BEHAV SCI, V32, P155, DOI 10.1016/j.cobeha.2020.02.017.
   Nash W, 2018, NPJ MAT DEGRAD, V2, DOI 10.1038/s41529-018-0058-x.
   Nowicki RK, 2020, J ARTIF INTELL SOFT, V10, P47, DOI 10.2478/jaiscr-2020-0004.
   Pak M.S., 2017, INT CONF COMP APPL I, P1.
   Peng SL, 2022, IEEE T NEUR NET LEAR, V33, P7020, DOI 10.1109/TNNLS.2021.3085433.
   Salloum Said A., 2020, Proceedings of the International Conference on Artificial Intelligence and Computer Vision (AICV2020). Advances in Intelligent Systems and Computing (AISC 1153), P50, DOI 10.1007/978-3-030-44289-7\_5.
   Shen J, 2021, MOB INF SYST, V2021, DOI 10.1155/2021/8878364.
   Sherstinsky A, 2020, PHYSICA D, V404, DOI 10.1016/j.physd.2019.132306.
   Araujo VJS, 2019, MACH LEARN KNOW EXTR, V1, P466, DOI 10.3390/make1010028.
   Zhang CY, 2019, PATTERN RECOGN LETT, V123, P82, DOI 10.1016/j.patrec.2019.03.015.},
Number-of-Cited-References = {27},
Times-Cited = {2},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {9},
Journal-ISO = {Food Raw Mater.},
Doc-Delivery-Number = {0Y7AO},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000790539700009},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000380459600068,
Author = {Mata-Montero, Erick and Carranza-Rojas, Jose},
Editor = {Cancela, H and Cuadros-Vargas, A and Cuadros-Vargas, E},
Title = {A Texture and Curvature Bimodal Leaf Recognition Model for
   Identification of Costa Rican Plant Species},
Booktitle = {2015 XLI LATIN AMERICAN COMPUTING CONFERENCE (CLEI)},
Series = {Proceedings of the Latin American Computing Conference},
Year = {2015},
Pages = {41-52},
Note = {Latin American Computing Conference (CLEI), SPC, UNSA, UCSM, UCSP, ULS,
   Arequipa, PERU, OCT 19-23, 2015},
Abstract = {In the last decade, research in Computer Vision has developed several
   algorithms to help botanists and non-experts to classify plants based on
   images of their leaves. LeafSnap is a mobile application that uses a
   multiscale curvature model of the leaf margin to classify leaf images
   into species. It has achieved high levels of accuracy on 184 tree
   species from Northeast US. We extend the research that led to the
   development of LeafSnap along two lines. First, LeafSnap's underlying
   algorithms are applied to a set of 66 tree species from Costa Rica.
   Then, texture is used as an additional criterion to measure the level of
   improvement achieved in the automatic identification of Costa Rica tree
   species. A 25.6\% improvement was achieved for a Costa Rican clean image
   dataset and 42.5\% for a Costa Rican noisy image dataset. In both cases,
   our results show this increment as statistically significant.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Mata-Montero, E (Corresponding Author), Costa Rica Inst Technol, Dept Comp Engn, Cartago, Costa Rica.
   Mata-Montero, Erick; Carranza-Rojas, Jose, Costa Rica Inst Technol, Dept Comp Engn, Cartago, Costa Rica.},
ISSN = {2381-1609},
ISBN = {978-1-4673-9143-6},
Keywords = {Biodiversity Informatics; Computer Vision; Image Processing; Leaf
   Recognition},
Keywords-Plus = {CLASSIFICATION; SHAPE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Author-Email = {emata@itcr.ac.cr
   jcarranza@itcr.ac.cr},
Affiliations = {Instituto Tecnologico de Costa Rica},
Cited-References = {Agrawal K., 2012, J SIGNAL INF PROCESS, V03, P146, DOI DOI 10.4236/JSIP.2012.32019.
   Anant Bhardwaj, 2013, International Journal of Innovation and Applied Studies, V3, P237.
   Andreopoulos A, 2013, COMPUT VIS IMAGE UND, V117, P827, DOI 10.1016/j.cviu.2013.04.005.
   {[}Anonymous], 2011, P 22 ANN S PATT REC, P91.
   Arun C. H, 2013, INT J COMPUTER APPL, V62.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Bradski G., 2000, DOBBS J SOFTWARE TOO.
   Carranza-Rojas J., 2014, THESIS.
   Clarke J, 2006, LECT NOTES COMPUT SC, V4292, P427.
   Coelho LP, 2013, J OPEN RES SOFTWARE, DOI DOI 10.5334/JORS.AC/.ACCESSED.
   de Carvalho MR, 2007, EVOL BIOL, V34, P140, DOI 10.1007/s11692-007-9011-6.
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x.
   el-Desouky B.S., 2011, INT J COMPUTER SCI I, V3.
   Herdiyeni Y, 2013, INT C ADV COMP SCI I, P353, DOI 10.1109/ICACSIS.2013.6761601.
   Herdiyeni Y, 2012, 2012 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER SCIENCE AND INFORMATION SYSTEMS, P255.
   Kadir A., 2011, INT J COMPUTER TREND.
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee K.-B., 2013, LNEE, V214, P109.
   Lee K.-B., 2013, INT J BIOSCI BIOTECH, V5, P57, DOI {[}10.14257/ijbsbt.2013.5.5.06, DOI 10.14257/IJBSBT.2013.5.5.06, 10.3724/sp.j.1087.2009.01707, DOI 10.1007/978-94-007-5857-5\_12].
   Li Y, 2006, IEEE SYS MAN CYBERN, P3890, DOI 10.1109/ICSMC.2006.384738.
   Manay S, 2006, IEEE T PATTERN ANAL, V28, P1602, DOI 10.1109/TPAMI.2006.208.
   Marikar F., 2012, TROPICAL AGR RES, V23.
   Nguyen Q., 2013, INT C ADV TECHN COMM.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Oliphant T., 2006, NUMPY GUIDE NUMPY.
   SUZUKI S, 1985, COMPUT VISION GRAPH, V30, P32, DOI 10.1016/0734-189X(85)90016-7.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zamora N., 2014, COMMUNICATION.
   Zhu XT, 2014, PROC CVPR IEEE, P1450, DOI 10.1109/CVPR.2014.188.},
Number-of-Cited-References = {31},
Times-Cited = {6},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BF2EC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000380459600068},
DA = {2023-08-12},
}

@inproceedings{ WOS:000839140800039,
Author = {Khalid, Fatimah and Abdullah, Azfar Husna and Abdullah, Lili Nurliyana},
Book-Group-Author = {IEEE},
Title = {SMARTFLORA Mobile Flower Recognition Application Using Machine Learning
   Tools},
Booktitle = {2022 IEEE 18TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING \&
   APPLICATIONS (CSPA 2022)},
Year = {2022},
Pages = {204-209},
Note = {18th IEEE International Colloquium on Signal Processing and Applications
   (CSPA), Selangor, MALAYSIA, MAY 12, 2022},
Organization = {IEEE; Univ Teknologi Mara; IEEE Control Syst Soc; IEEE Control Syst Soc
   Malaysia Chapter},
Abstract = {There are around 369,000 flowering plant species documented globally.
   However, the majority of people have difficulties telling these blooms
   apart. Usually, people often consult specialists, study floral reference
   books, or do keyword searches on relevant web resources. Therefore, this
   flower recognition mobile application was proposed to ease those people
   to recognize types of flowers without using any computer or machine. In
   this paper, a system architecture is designed based on Teachable Machine
   Learning platform, Tensorflow Lite Model and Android Studio to develop a
   SMARTFLORA Mobile Flower Recognition application that allows users to
   identify three types of flower species: daisies, roses, and sunflowers.
   Kaggle dataset has been used and the accuracy was 88\%.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Khalid, F (Corresponding Author), Univ Putra Malaysia, Fac Comp Sci \& Informat Technol, Serdang, Selangor, Malaysia.
   Khalid, Fatimah; Abdullah, Azfar Husna; Abdullah, Lili Nurliyana, Univ Putra Malaysia, Fac Comp Sci \& Informat Technol, Serdang, Selangor, Malaysia.},
DOI = {10.1109/CSPA55076.2022.9781961},
ISBN = {978-1-6654-8529-6},
Keywords = {Flower classification; Teachable Machine Learning; Tensorflow Lite;
   Machine Learning},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Software Engineering; Engineering, Electrical \&
   Electronic},
Author-Email = {fatimalik@upm.edu.my
   azfarhusna13@gmail.com
   liyana@upm.edu.my},
Affiliations = {Universiti Putra Malaysia},
ResearcherID-Numbers = {Abdullah, Lili Nurliyana/AAC-6423-2020},
ORCID-Numbers = {Abdullah, Lili Nurliyana/0000-0001-8704-2390},
Cited-References = {Alexander M., FLOWERS RECOGNITION.
   Andrew G., 2017, MOBILENETS EFFICIENT, P1.
   Bulbul M. F., 2018, ADV IMAGE VIDEO PROC, V6, P8.
   Data School, 2014, SIMPL GUID CONF MATR.
   Guo TM, 2017, 2017 IEEE 2ND INTERNATIONAL CONFERENCE ON BIG DATA ANALYSIS (ICBDA), P721, DOI 10.1109/ICBDA.2017.8078730.
   Hiary H, 2018, IET COMPUT VIS, V12, P855, DOI 10.1049/iet-cvi.2017.0155.
   Liu YY, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON FUNCTIONAL-STRUCTURAL PLANT GROWTH MODELING, SIMULATION, VISUALIZATION AND APPLICATIONS (FSPMA), P110, DOI 10.1109/FSPMA.2016.7818296.
   Mete B. R., 2019, 2019 3 INT S MULTIDI, DOI {[}DOI 10.1109/ISMSIT.2019.8932908, 10.1109/ismsit.2019.8932908].
   play.google, PLANTNET PLANT ID.
   play.google, PLANTSNAP PLANT ID.
   Sveta C., INTEGRATE MACHINE LE.
   Weed and More, 2021, PICT THIS ID FLOW PL.},
Number-of-Cited-References = {12},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BT5WW},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000839140800039},
DA = {2023-08-12},
}

@inproceedings{ WOS:000405573700125,
Author = {Wable, Pradip B. and Chilveri, Purushottam G.},
Book-Group-Author = {IEEE},
Title = {Neural Network Based Leaf Recognition},
Booktitle = {2016 INTERNATIONAL CONFERENCE ON AUTOMATIC CONTROL AND DYNAMIC
   OPTIMIZATION TECHNIQUES (ICACDOT)},
Year = {2016},
Pages = {645-648},
Note = {International Conference on Automatic Control and Dynamic Optimization
   Techniques (ICACDOT), Int Inst Informat Technol, Pune, INDIA, SEP 09-10,
   2016},
Organization = {IEEE Pune Sect},
Abstract = {Plants are related to human. Recognize an unfamiliar plant correctly
   without any expert understanding is big task. Due to Improvement in
   image processing, it is likely to know leaf image rapidly from which
   species it is. Pulse coupled neural network is a helpful tool for
   feature extraction. Entropy sequence is key feature which is obtained
   from pulse-coupled neural network. Along with entropy sequence other
   features such as aspect ratio, Zernike moments, Hu's invariants, Form
   factor, Rectangularity, circularity, area. Artificial neural network is
   taken as classifier. Proposed method gives better recognition rate
   compared to existing methods.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Wable, PB (Corresponding Author), SKNCOE Vadgaon Bk, Pune, Maharashtra, India.
   Wable, Pradip B.; Chilveri, Purushottam G., SKNCOE Vadgaon Bk, Pune, Maharashtra, India.},
ISBN = {978-1-5090-2080-5},
Keywords = {Neural Network; Image Processing; Feature Extraction; ANN Classification},
Research-Areas = {Automation \& Control Systems},
Web-of-Science-Categories  = {Automation \& Control Systems},
Author-Email = {pwable91@gmail.com
   pgchilveri@gmail.com},
Cited-References = {Du JX, 2013, NEUROCOMPUTING, V116, P150, DOI 10.1016/j.neucom.2012.03.028.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Kulkarni A.H., 2013, INT J ADV RES COMPUT, V2, P1.
   Wang ZB, 2010, IMAGE VISION COMPUT, V28, P5, DOI 10.1016/j.imavis.2009.06.007.
   Wang Zhaobin, 2015, NEURAL COMPUTATION A.
   Wu Stephen Gang, 2007, IEEE INT S SIGN PROC, V3, P11.},
Number-of-Cited-References = {6},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BI1DA},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000405573700125},
DA = {2023-08-12},
}

@article{ WOS:000558995300001,
Author = {Zhang, Yaonan and Cui, Jing and Wang, Zhaobin and Kang, Jianfang and
   Min, Yufang},
Title = {Leaf Image Recognition Based on Bag of Features},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2020},
Volume = {10},
Number = {15},
Month = {AUG},
Abstract = {Plants are ubiquitous in human life. Recognizing an unknown plant by its
   leaf image quickly is a very interesting and challenging research. With
   the development of image processing and pattern recognition, plant
   recognition based on image processing has become possible. Bag of
   features (BOF) is one of the most powerful models for classification,
   which has been used for many projects and studies. Dual-output
   pulse-coupled neural network (DPCNN) has shown a good ability for
   texture features in image processing such as image segmentation. In this
   paper, a method based on BOF and DPCNN (BOF\_DP) is proposed for leaf
   classification. BOF\_DP achieved satisfactory results in many leaf image
   datasets. As it is hard to get a satisfactory effect on the large
   dataset by a single feature, a method (BOF\_SC) improved from bag of
   contour fragments is used for shape feature extraction. BOF\_DP and LDA
   (linear discriminant analysis) algorithms are, respectively, employed
   for textual feature extraction and reducing the feature dimensionality.
   Finally, both features are used for classification by a linear support
   vector machine (SVM), and the proposed method obtained higher accuracy
   on several typical leaf datasets than existing methods.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Wang, ZB (Corresponding Author), Chinese Acad Sci, Northwest Inst Ecoenvironm \& Resources, Lanzhou 730000, Peoples R China.
   Wang, ZB (Corresponding Author), Natl Cryosphere Desert Data Ctr, Lanzhou 730000, Peoples R China.
   Wang, ZB (Corresponding Author), Lanzhou Univ, Sch Informat Sci \& Engn, Lanzhou 730000, Peoples R China.
   Zhang, Yaonan; Wang, Zhaobin; Kang, Jianfang; Min, Yufang, Chinese Acad Sci, Northwest Inst Ecoenvironm \& Resources, Lanzhou 730000, Peoples R China.
   Zhang, Yaonan; Wang, Zhaobin; Kang, Jianfang; Min, Yufang, Natl Cryosphere Desert Data Ctr, Lanzhou 730000, Peoples R China.
   Cui, Jing; Wang, Zhaobin, Lanzhou Univ, Sch Informat Sci \& Engn, Lanzhou 730000, Peoples R China.},
DOI = {10.3390/app10155177},
Article-Number = {5177},
EISSN = {2076-3417},
Keywords = {feature extraction; shape context; plant recognition; DPCNN; BOF},
Keywords-Plus = {SHAPE-FEATURES; PLANT; CLASSIFICATION; TEXTURE; DESCRIPTORS; PROJECTION;
   RETRIEVAL; ROTATION; SCALE; LBP},
Research-Areas = {Chemistry; Engineering; Materials Science; Physics},
Web-of-Science-Categories  = {Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied},
Author-Email = {yaonan@lzb.ac.cn
   cuij18@lzu.edu.cn
   wangzhb@lzu.edu.cn
   kangjf@lzb.ac.cn
   myf@lzb.ac.cn},
Affiliations = {Chinese Academy of Sciences; Lanzhou University},
ORCID-Numbers = {zhang, yaonan/0000-0001-8905-9006
   Min, Yufang/0000-0003-2600-2517},
Funding-Acknowledgement = {China Postdoctoral Science Foundation {[}2013M532097]; National Natural
   Science Foundation of China {[}61201421]; Foundation of National
   Glaciology Geocryology Desert Data Center {[}Y929830201]; 13th Five-year
   Informatization Plan of the Chinese Academy of Sciences {[}XXH13506]},
Funding-Text = {This study was jointly funded by China Postdoctoral Science Foundation
   (Grant No. 2013M532097), National Natural Science Foundation of China
   (Grant No. 61201421), the Foundation of National Glaciology Geocryology
   Desert Data Center (Grant No. Y929830201), and the 13th Five-year
   Informatization Plan of the Chinese Academy of Sciences(Grant No.
   XXH13506).},
Cited-References = {Alvarsson J, 2016, J CHEMINFORMATICS, V8, DOI 10.1186/s13321-016-0151-5.
   {[}Anonymous], 2006, IEEE COMPUTER SOC C.
   Chaki J, 2019, OPTIK, V181, P639, DOI 10.1016/j.ijleo.2018.12.107.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199.
   Chi ZR, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS \& SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P1035.
   Demisse GG, 2018, IEEE T PATTERN ANAL, V40, P1338, DOI 10.1109/TPAMI.2017.2711607.
   Dev S, 2015, IEEE IMAGE PROC, P422, DOI 10.1109/ICIP.2015.7350833.
   Fan RE, 2008, J MACH LEARN RES, V9, P1871.
   Fu B, 2019, J PHYS CONF SER, V1176, DOI 10.1088/1742-6596/1176/3/032053.
   Fuentes S, 2018, COMPUT ELECTRON AGR, V151, P311, DOI 10.1016/j.compag.2018.06.035.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   Hasim A, 2016, IOP C SER EARTH ENV, V31, DOI 10.1088/1755-1315/31/1/012002.
   Hsiao JK, 2014, 2014 SCIENCE AND INFORMATION CONFERENCE (SAI), P389, DOI 10.1109/SAI.2014.6918216.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Kadir Abdul, 2012, Journal of Theoretical and Applied Information Technology, V41, P82.
   Kala JR, 2018, IMAGE ANAL STEREOL, V37, P119, DOI 10.5566/ias.1821.
   Kalyoncu C, 2016, IET COMPUT VIS, V10, P700, DOI 10.1049/iet-cvi.2015.0414.
   Kheirkhah FM, 2019, IET COMPUT VIS, V13, P369, DOI 10.1049/iet-cvi.2018.5028.
   Kolivand H, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0191447.
   Kristin J.D., 2018, SYNTH LECT COMPUT VI, V8, P113.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Li XJ, 2012, NEURAL COMPUT, V24, P194, DOI 10.1162/NECO\_a\_00194.
   Lin FY, 2008, COMM COM INF SC, V15, P432.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Liu NA, 2016, NEUROCOMPUTING, V216, P460, DOI 10.1016/j.neucom.2016.08.005.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Pearline SA, 2019, J INTELL FUZZY SYST, V36, P1997, DOI 10.3233/JIFS-169911.
   Prasad S, 2017, MULTIMED TOOLS APPL, V76, P21339, DOI 10.1007/s11042-016-4040-8.
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   Shao Y, 2019, COMPUT ELECTRON AGR, V158, P102, DOI 10.1016/j.compag.2019.01.022.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Soderkvist O., 2001, THESIS.
   Thanh TKN, 2018, LECT NOTES ARTIF INT, V10751, P565, DOI 10.1007/978-3-319-75417-8\_53.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Tharwat A, 2017, J APPL LOGIC, V24, P15, DOI 10.1016/j.jal.2016.11.021.
   Tsolakidis DG, 2014, LECT NOTES ARTIF INT, V8445, P406, DOI 10.1007/978-3-319-07064-3\_33.
   Turkoglu M, 2019, PHYSICA A, V527, DOI 10.1016/j.physa.2019.121297.
   Valliammal N., 2012, INT J COMPUT COMMUN, V6, P440.
   Wang B, 2019, IEEE ACCESS, V7, P151754, DOI 10.1109/ACCESS.2019.2947510.
   Wang B, 2015, INFORM SCIENCES, V302, P132, DOI 10.1016/j.ins.2014.07.028.
   Wang B, 2014, IEEE T IMAGE PROCESS, V23, P4101, DOI 10.1109/TIP.2014.2343457.
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018.
   Wang XG, 2014, PATTERN RECOGN, V47, P2116, DOI 10.1016/j.patcog.2013.12.008.
   Wang ZB, 2016, NEURAL COMPUT APPL, V27, P899, DOI 10.1007/s00521-015-1904-1.
   Wang ZB, 2014, IEEE IJCNN, P975, DOI 10.1109/IJCNN.2014.6889656.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yang CZ, 2019, SIGNAL PROCESS-IMAGE, V71, P110, DOI 10.1016/j.image.2018.11.004.
   Yousefi E, 2017, COMPUT ELECTRON AGR, V140, P70, DOI 10.1016/j.compag.2017.05.031.
   Yu H, 2001, PATTERN RECOGN, V34, P2067, DOI 10.1016/S0031-3203(00)00162-X.
   Yu XH, 2019, IEEE ACCESS, V7, P68087, DOI 10.1109/ACCESS.2019.2918263.
   Zeng JX, 2019, IEEE ACCESS, V7, P57163, DOI 10.1109/ACCESS.2019.2913688.
   Zhang SW, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/9581292.
   Zhang SW, 2013, PATTERN RECOGN, V46, P1891, DOI 10.1016/j.patcog.2013.01.015.
   Zhang X, 2019, MULTIMED TOOLS APPL, V78, P27463, DOI 10.1007/s11042-019-07846-0.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {59},
Times-Cited = {9},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Appl. Sci.-Basel},
Doc-Delivery-Number = {MZ3AP},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000558995300001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000503314100046,
Author = {Knoll, Florian J. and Czymmek, Vitali and Harders, Leif O. and Hussmann,
   Stephan},
Title = {Real-time classification of weeds in organic carrot production using
   deep learning algorithms},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2019},
Volume = {167},
Month = {DEC},
Abstract = {This paper proposes a real-time machine learning approach for carrot
   plants classification in organic farming using Convolutional Neural
   Network. Artificial neural networks become increasingly popular for
   image processing tasks, e.g. the classification of complex structures in
   images. The problem is not very often the accuracy of the
   classification, but the speed of calculation. The core of this paper
   presents a real-time calculation flow for the neural networks, in which
   all the individual steps are summarized. It also briefly discusses the
   used sensors, which are suitable for the Convolutional Neural Network
   and the pre-processing which extracts the plants from the background in
   order to keep the load on the neural network as low as possible.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Knoll, FJ (Corresponding Author), West Coast Univ Appl Sci, Fac Engn, Fritz Thiedemann Ring 20, D-25746 Heide, Germany.
   Knoll, Florian J.; Czymmek, Vitali; Harders, Leif O.; Hussmann, Stephan, West Coast Univ Appl Sci, Fac Engn, Fritz Thiedemann Ring 20, D-25746 Heide, Germany.},
DOI = {10.1016/j.compag.2019.105097},
Article-Number = {105097},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Convolution Neural Network (CNN); Deep leaming; Visual sensors; Colour
   room processing; Random forest classifier; Real-time performance;
   Organic farming},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {knoll@fh-westkueste.de},
Funding-Acknowledgement = {Federal State of Schleswig-Holstein, Germany},
Funding-Text = {This work was supported by the Federal State of Schleswig-Holstein,
   Germany. The authors are grateful for the financial support.},
Cited-References = {Ak R, 2016, IEEE T NEUR NET LEAR, V27, P1734, DOI 10.1109/TNNLS.2015.2418739.
   Chen L, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P695, DOI 10.1109/ACPR.2015.7486592.
   Chen L, 2016, CHIN CONTR CONF, P6967, DOI 10.1109/ChiCC.2016.7554454.
   Haug S, 2014, IEEE WINT CONF APPL, P1142, DOI 10.1109/WACV.2014.6835733.
   Jung S, 2016, INT CONF UBIQ ROBOT, P31, DOI 10.1109/URAI.2016.7734014.
   Kargar Amir H. B., 2013, 2013 First RSI/ISM International Conference on Robotics and Mechatronics (ICRoM 2013). Proceedings, P468, DOI 10.1109/ICRoM.2013.6510152.
   Knoll F., 2017, P SOC PHOTO-OPT INS, V10335.
   Knoll F, 2016, IEEE IMTC P, P1024.
   Knoll FJ, 2018, COMPUT ELECTRON AGR, V153, P347, DOI 10.1016/j.compag.2018.08.032.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Li J, 2017, IEEE T NEUR NET LEAR, V28, P690, DOI 10.1109/TNNLS.2016.2522428.
   Liu H, 2016, C HUM SYST INTERACT, P197, DOI 10.1109/HSI.2016.7529631.
   Michaels A, 2015, IEEE INT C INT ROBOT, P5498, DOI 10.1109/IROS.2015.7354156.
   Perez-Ortiz M, 2015, APPL SOFT COMPUT, V37, P533, DOI 10.1016/j.asoc.2015.08.027.
   Pouladzadeh P, 2016, IEEE IMTC P, P1243.
   Wong WK, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON ROBOTICS AND MANUFACTURING AUTOMATION (ROMA), P63, DOI 10.1109/ROMA.2014.7295863.
   Zheng CF, 2016, IEEE INT POWER ELEC, P37, DOI 10.1109/IPEMC.2016.7512258.
   Zheng YJ, 2017, IEEE T NEUR NET LEAR, V28, P2911, DOI 10.1109/TNNLS.2016.2609437.},
Number-of-Cited-References = {18},
Times-Cited = {21},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {27},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {JW8RL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000503314100046},
DA = {2023-08-12},
}

@inproceedings{ WOS:000450093300134,
Author = {Kang, Euncheol and Oh, Il-Seok},
Book-Group-Author = {IEEE},
Title = {Weak Constraint Leaf Image Recognition based on Convolutional Neural
   Network},
Booktitle = {2018 INTERNATIONAL CONFERENCE ON ELECTRONICS, INFORMATION, AND
   COMMUNICATION (ICEIC)},
Year = {2018},
Pages = {435-438},
Note = {17th Annual International Conference on Electronics, Information, and
   Communication (ICEIC), Honolulu, HI, JAN 24-27, 2018},
Organization = {IEEE; Inst Elect \& Informat Engineers; Consumer Elect Soc},
Abstract = {Recently the computer vision and machine learning research communities
   pay a great attention to the leaf image recognition problem. Our
   literature survey focusing on the user interaction aspect reveals that
   two schemes of image acquisition have been used, one with strong
   constraint and the other with no constraint. The strong constraint
   interaction asks users to capture images by placing a leaf on a uniform
   background such as white paper while the unconstrained interaction
   allows any form of image capturing. The former one gets a high
   performance sacrificing the user convenience while the latter one
   provides a great convenience sacrificing the recognition performance.
   Our scheme is weakly constrained in the middle of two extremes. The
   proposed interaction scheme only asks users to center the leaf on
   smartphone camera screen. The leaf may be on the tree or off the tree.
   When the leaf is picked off the tree, it is recommended to place it
   against rather uniform background such as sky, soil, or tree bark. By
   fine-tuning the pre-trained CNNs(Convolutional Neural Network), we
   obtained a practical performance, 96.08\% top-1 and 99.81\% top-5
   accuracies. The dataset is publicly open and the recognition system is
   released as an Android App.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kang, E (Corresponding Author), Chonbuk Natl Univ, Div Comp Sci \& Engn, Jeonju, South Korea.
   Kang, Euncheol; Oh, Il-Seok, Chonbuk Natl Univ, Div Comp Sci \& Engn, Jeonju, South Korea.},
ISBN = {978-1-5386-4754-7},
Keywords = {Automatic leaf recognition; convolutional neural network; deep learning;
   fine tuning},
Research-Areas = {Engineering; Telecommunications},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Telecommunications},
Author-Email = {ec.kang@jbnu.ac.kr
   isoh@jbnu.ac.kr},
Affiliations = {Jeonbuk National University},
Cited-References = {Agarwal G, 2006, TAXON, V55, P597, DOI 10.2307/25065637.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Cugu I, 2017, ARXIV170108291.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Grand-Brochier M, 2015, IEEE T IMAGE PROCESS, V24, P1549, DOI 10.1109/TIP.2015.2400214.
   Hang ST, 2016, CLEF 2016 C.
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Joly A, 2016, LECT NOTES COMPUT SC, V9822, P286, DOI 10.1007/978-3-319-44564-9\_26.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yosinski J, 2015, UNDERSTANDING NEURAL.
   Zhao ZQ, 2015, LECT NOTES COMPUT SC, V9004, P348, DOI 10.1007/978-3-319-16808-1\_24.},
Number-of-Cited-References = {18},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BL3XN},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000450093300134},
DA = {2023-08-12},
}

@article{ WOS:000900395300001,
Author = {Minowa, Yasushi and Shigematsu, Koharu and Takahara, Hikaru},
Title = {A Deep Learning-Based Model for Tree Species Identification Using Pollen
   Grain Images},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2022},
Volume = {12},
Number = {24},
Month = {DEC},
Abstract = {The objective of this study was to develop a deep learning-based tree
   species identification model using pollen grain images taken with a
   camera mounted on an optical microscope. From five focal points, we took
   photographs of pollen collected from tree species widely distributed in
   the Japanese archipelago, and we used these to produce pollen images. We
   used Caffe as the deep learning framework and AlexNet and GoogLeNet as
   the deep learning algorithms. We constructed four learning models that
   combined two learning patterns, one for focal point images with data
   augmentation, for which the training and test data were the same, and
   the other without data augmentation, for which they were not the same.
   The performance of the proposed model was evaluated according to the MCC
   and F score. The most accurate classification model was based on the
   GoogLeNet algorithm, with data augmentation after 200 epochs. Tree
   species identification accuracy varied depending on the focal point,
   even for the same pollen grain, and images focusing on the pollen
   surface tended to be more accurately classified than those focusing on
   the pollen outline and membrane structure. Castanea crenata, Fraxinus
   sieboldiana, and Quercus crispula pollen grains were classified with the
   highest accuracy, whereas Gamblea innovans, Carpinus tschonoskii, Cornus
   controversa, Fagus japonica, Quercus serrata, and Quercus sessilifolia
   showed the lowest classification accuracy. Future studies should
   consider application to fossil pollen in sediments and state-of-the-art
   deep learning algorithms.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Minowa, Y (Corresponding Author), Kyoto Prefectural Univ, Grad Sch Life \& Environm Sci, Kyoto 6068522, Japan.
   Minowa, Yasushi; Takahara, Hikaru, Kyoto Prefectural Univ, Grad Sch Life \& Environm Sci, Kyoto 6068522, Japan.
   Shigematsu, Koharu, Kyoto Prefectural Univ, Fac Life \& Environm Sci, Kyoto 6068522, Japan.},
DOI = {10.3390/app122412626},
Article-Number = {12626},
EISSN = {2076-3417},
Keywords = {AlexNet; Caffe; deep learning; F score; focal point; GoogLeNet; MCC;
   pollen grain images; tree species identification},
Research-Areas = {Chemistry; Engineering; Materials Science; Physics},
Web-of-Science-Categories  = {Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied},
Author-Email = {sharmy@uf.kpu.ac.jp},
Affiliations = {Kyoto Prefectural University; Kyoto Prefectural University},
Cited-References = {{[}Anonymous], 2004, IMAGEJ.
   Battiato S, 2020, IEEE IMAGE PROC, P2456, DOI 10.1109/ICIP40778.2020.9190776.
   Boldeanu M, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app112411707.
   Chen XY, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12147126.
   Daood A, 2016, INT C PATT RECOG, P3091, DOI 10.1109/ICPR.2016.7900109.
   Faegri K., 1975, Textbook of pollen analysis..
   France I, 2000, QUATERNARY SCI REV, V19, P537, DOI 10.1016/S0277-3791(99)00021-9.
   Gallardo-Caballero R, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19163583.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   go.jp, 2015, GBIF SURVEY.
   Goeau H., 2014, P WORKING NOTES CLEF.
   Goncalves AB, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0157044.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Holt KA, 2014, NEW PHYTOL, V203, P735, DOI 10.1111/nph.12848.
   Huang G., 2016, P P IEEE C COMPUTER.
   Jia Y., 2014, ACM INT C MULT, DOI {[}DOI 10.1145/2647868.2654889, 10.1145/2647868.2654889].
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kubera E, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21103526.
   Li P, 1999, GRANA, V38, P59, DOI 10.1080/001731300750044717.
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI {[}10.1109/TPAMI.2017.2773081, 10.1109/TCC.2017.2764082].
   Mahbod A., 2021, P ICPR 2021 PATTERN.
   Makino K., 2018, DEEP LEARNING BEGIN.
   MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9.
   Minowa Y., 2020, Journal of Forest Planning, V26, P1, DOI 10.20659/jfp.2020.001.
   Minowa Y, 2022, FORESTS, V13, DOI 10.3390/f13060943.
   Minowa Y, 2022, J FOREST RES-JPN, V27, P246, DOI 10.1080/13416979.2021.2021640.
   Morita Y., 1994, CLASSIFICATION MORPH.
   Motoda H., 2006, FUNDAMENTALS DATA MI.
   Nakae M., 2019, JPN SOC DEGIT ARCH, V3, P345.
   Nakamura J., 1967, POLLEN ANAL.
   NVIDIA, 2016, US.
   Okatani T., 2015, J ROBOTICS SOC JPN, V33, P92, DOI {[}10.7210/jrsj.33.92, DOI 10.7210/JRSJ.33.92].
   Perez L., 2017, ARXIV.
   quantitative-plant.org, 2013, POLEN23E.
   Raschka S., 2018, PYTHON MACHINE LEARN.
   Ruiz-Varela JM, 2022, IEEE LAT AM T, V20, P22, DOI 10.1109/TLA.2022.9662170.
   scikit-learn.org, 2007, SCIKIT LEARN.
   Sevillano V, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0229751.
   Sevillano V, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0201807.
   Shavlik J., 2009, TRANSFER LEARNING.
   Stillman EC, 1996, QUATERNARY SCI REV, V15, P1, DOI 10.1016/0277-3791(95)00076-3.
   Szegedy C., 2014, P IEEE C COMPUTER VI, P1.
   Takagi M., 2004, HDB IMAGE ANAL REVIS.
   Marcos JV, 2015, MICRON, V68, P36, DOI 10.1016/j.micron.2014.09.002.
   Witten IH, 2011, MOR KAUF D, P1.
   Yamashita T., 2016, ILLUSTRATED GUIDE DE.},
Number-of-Cited-References = {46},
Times-Cited = {0},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Appl. Sci.-Basel},
Doc-Delivery-Number = {7D3LD},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000900395300001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000461263400003,
Author = {Kaya, Aydin and Keceli, Ali Seydi and Catal, Cagatay and Yalic, Hamdi
   Yalin and Temucin, Huseyin and Tekinerdogan, Bedir},
Title = {Analysis of transfer learning for deep neural network based plant
   classification models},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2019},
Volume = {158},
Pages = {20-29},
Month = {MAR},
Abstract = {Plant species classification is crucial for biodiversity protection and
   conservation. Manual classification is time-consuming, expensive, and
   requires experienced experts who are often limited available. To cope
   with these issues, various machine learning algorithms have been
   proposed to support the automated classification of plant species. Among
   these machine learning algorithms, Deep Neural Networks (DNNs) have been
   applied to different data sets. DNNs have been however often applied in
   isolation and no effort has been made to reuse and transfer the
   knowledge of different applications of DNNs. Transfer learning in the
   context of machine learning implies the usage of the results of multiple
   applications of DNNs. In this article, the results of the effect of four
   different transfer learning models for deep neural network-based plant
   classification is investigated on four public datasets. Our experimental
   study demonstrates that transfer learning can provide important benefits
   for automated plant identification and can improve low-performance plant
   classification models.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Kaya, A (Corresponding Author), Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
   Kaya, Aydin; Keceli, Ali Seydi; Yalic, Hamdi Yalin; Temucin, Huseyin, Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
   Catal, Cagatay; Tekinerdogan, Bedir, Wageningen Univ, Informat Technol Grp, Wageningen, Netherlands.},
DOI = {10.1016/j.compag.2019.01.041},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Plant classification; Transfer learning; Deep neural networks;
   Fine-tuning; Convolutional neural networks},
Keywords-Plus = {COLOR TEXTURE FEATURES; WEED-DETECTION; REAL-TIME; IDENTIFICATION;
   RECOGNITION; IMAGE; VENATION; SYSTEM; WHEAT; SHAPE},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {aydinkaya@cs.hacettepe.edu.tr
   aliseydi@cs.hacettepe.edu.tr
   cagatay.catal@wur.nl
   yalinyalic@cs.hacettepe.edu.tr
   htemucin@cs.hacettepe.edu.tr
   bedir.tekinerdogan@wur.nl},
Affiliations = {Hacettepe University; Wageningen University \& Research},
ResearcherID-Numbers = {Kaya, AydÄn/AAR-1028-2020
   Tekinerdogan, Bedir/K-3639-2019
   Keçeli, Ali Seydi/M-3158-2018
   Temuçin, Hüseyin/M-8046-2018
   Tekinerdogan, Bedir/AAU-8909-2020
   Catal, Cagatay/AAF-3929-2019},
ORCID-Numbers = {Kaya, AydÄn/0000-0001-6175-7769
   Tekinerdogan, Bedir/0000-0002-8538-7261
   Keçeli, Ali Seydi/0000-0001-6531-8464
   Temuçin, Hüseyin/0000-0001-6688-1996
   Tekinerdogan, Bedir/0000-0002-8538-7261
   Catal, Cagatay/0000-0003-0959-2930},
Cited-References = {Agarwal G, 2006, TAXON, V55, P597, DOI 10.2307/25065637.
   {[}Anonymous], PLOS BIOL.
   {[}Anonymous], 2016, P 5 INT C AGR AGR JU.
   {[}Anonymous], 2000, EXPT SOFTWARE ENG IN.
   {[}Anonymous], 2007, 2007 IEEE INT S SIGN.
   {[}Anonymous], IEEE T SOFTW ENG.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Burks TF, 2000, T ASAE, V43, P441, DOI 10.13031/2013.2723.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Cook D, 2013, KNOWL INF SYST, V36, P537, DOI 10.1007/s10115-013-0665-3.
   Day Oscar, 2017, Journal of Big Data, V4, DOI 10.1186/s40537-017-0089-0.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Ferreira AD, 2017, COMPUT ELECTRON AGR, V143, P314, DOI 10.1016/j.compag.2017.10.027.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Gerhards R, 2003, WEED RES, V43, P385, DOI 10.1046/j.1365-3180.2003.00349.x.
   Govaerts R, 2001, TAXON, V50, P1085, DOI 10.2307/1224723.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Hopkins GW, 2002, ANIM CONSERV, V5, P245, DOI 10.1017/S1367943002002299.
   Horaisova K, 2016, BIOSYST ENG, V142, P83, DOI 10.1016/j.biosystemseng.2015.12.007.
   Husin Z, 2012, COMPUT ELECTRON AGR, V89, P18, DOI 10.1016/j.compag.2012.07.009.
   Jeon WS, 2017, INT J FUZZY LOG INTE, V17, P26, DOI 10.5391/IJFIS.2017.17.1.26.
   Karpathy A., 2015, ANDREJ KARPATHY BLOG, V21, P23.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Larese MG, 2014, EXPERT SYST APPL, V41, P4638, DOI 10.1016/j.eswa.2014.01.029.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Mattila H, 2013, PRECIS AGRIC, V14, P621, DOI 10.1007/s11119-013-9320-y.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Murat M, 2017, PEERJ, V5, DOI 10.7717/peerj.3792.
   Muthevi A, 2017, IEEE INT ADV COMPUT, P870, DOI {[}10.1109/IACC.2017.169, 10.1109/IACC.2017.0178].
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Pahikkala T, 2015, COMPUT ELECTRON AGR, V118, P186, DOI 10.1016/j.compag.2015.09.003.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Pydipati R, 2006, COMPUT ELECTRON AGR, V52, P49, DOI 10.1016/j.compag.2006.01.004.
   Sack L, 2008, P NATL ACAD SCI USA, V105, P1567, DOI 10.1073/pnas.0709333105.
   Scoffoni C, 2011, PLANT PHYSIOL, V156, P832, DOI 10.1104/pp.111.173856.
   Scotland RW, 2003, TAXON, V52, P101, DOI 10.2307/3647306.
   Shie CK, 2015, IEEE ENG MED BIO, P711, DOI 10.1109/EMBC.2015.7318461.
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162.
   Silva PFB, 2013, LECT NOTES COMPUT SC, V7950, P197, DOI 10.1007/978-3-642-39094-4\_23.
   Simonyan K., 2014, 14091556V6 ARXIV, P1, DOI DOI 10.48550/ARXIV.1409.1556.
   Soderkvist O, 2001, COMPUTER VISION CLAS.
   Strothmann W, 2017, COMPUT ELECTRON AGR, V134, P79, DOI 10.1016/j.compag.2017.01.003.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Tyystjarvi E, 2011, PRECIS AGRIC, V12, P546, DOI 10.1007/s11119-010-9201-6.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Wang Chang, 2011, 22 INT JOINT C ARTIF, P1541.
   Wang N, 2007, BIOSYST ENG, V98, P276, DOI 10.1016/j.biosystemseng.2007.08.007.
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6.
   Wilkinson MD, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.18.
   WOEBBECKE DM, 1995, T ASAE, V38, P259, DOI 10.13031/2013.27838.
   Yousefi E, 2017, COMPUT ELECTRON AGR, V140, P70, DOI 10.1016/j.compag.2017.05.031.
   Yu Xin, 2016, 2016 European Conference on Networks and Communications (EuCNC), P1, DOI 10.1109/EuCNC.2016.7561046.},
Number-of-Cited-References = {54},
Times-Cited = {172},
Usage-Count-Last-180-days = {38},
Usage-Count-Since-2013 = {176},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {HO9EE},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000461263400003},
DA = {2023-08-12},
}

@article{ WOS:000787912400001,
Author = {Figueroa-Mata, Geovanni and Mata-Montero, Erick and Carlos
   Valverde-Otarola, Juan and Arias-Aguilar, Dagoberto and
   Zamora-Villalobos, Nelson},
Title = {Using Deep Learning to Identify Costa Rican Native Tree Species From
   Wood Cut Images},
Journal = {FRONTIERS IN PLANT SCIENCE},
Year = {2022},
Volume = {13},
Month = {APR 1},
Abstract = {Tree species identification is critical to support their conservation,
   sustainable management and, particularly, the fight against illegal
   logging. Therefore, it is very important to develop fast and accurate
   identification systems even for non-experts. In this research we have
   achieved three main results. First, we developed-from scratch and using
   new sample collecting and processing protocols-an dataset called
   CRTreeCuts that comprises macroscopic cross-section images of 147 native
   tree species from Costa Rica. Secondly, we implemented a CNN for
   automated tree species identification based on macroscopic images of
   cross-sections of wood. For this CNN we apply the fine-tuning technique
   with VGG16 as a base model, pre-trained with the ImageNet data set. This
   model is trained and tested with a subset of 75 species from CRTreeCuts.
   The top-1 and top-3 accuracies achieved in the testing phase are 70.5\%
   and 80.3\%, respectively. The Same-Specimen-Picture Bias (SSPB), which
   is known to erroneously increase accuracy, is absent in all experiments.
   Finally, the third result is Cocobolo, an Android mobile application
   that uses the developed CNN as back-end to identify Costa Rican tree
   species from images of cross-sections of wood.},
Publisher = {FRONTIERS MEDIA SA},
Address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Figueroa-Mata, G (Corresponding Author), Costa Rica Inst Technol, Sch Math, Cartago, Costa Rica.
   Figueroa-Mata, Geovanni, Costa Rica Inst Technol, Sch Math, Cartago, Costa Rica.
   Mata-Montero, Erick, Costa Rica Inst Technol, Sch Comp, Cartago, Costa Rica.
   Carlos Valverde-Otarola, Juan; Arias-Aguilar, Dagoberto; Zamora-Villalobos, Nelson, Costa Rica Inst Technol, Sch Forestry Engn, Cartago, Costa Rica.
   Carlos Valverde-Otarola, Juan, Univ Concepcion, Fac Ciencias Forestales, Cooperat Productividad Forestal, Concepcion, Chile.},
DOI = {10.3389/fpls.2022.789227},
Article-Number = {789227},
ISSN = {1664-462X},
Keywords = {deep learning; convolutional neural network; plant classification;
   automated image-based tree species identification; costa rican tree
   species; xylotheques},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {gfigueroa@itcr.ac.cr},
Affiliations = {Instituto Tecnologico de Costa Rica; Instituto Tecnologico de Costa
   Rica; Instituto Tecnologico de Costa Rica; Universidad de Concepcion},
ORCID-Numbers = {Valverde, Juan Carlos/0000-0002-3181-1346},
Cited-References = {{[}Anonymous], 2000, UNDERSTANDING WOOD C.
   Apolinario M, 2019, IEEE LAT AM T, V17, P2005, DOI 10.1109/TLA.2019.9011545.
   Apolinario M., 2018, 2018 IEEE 25 INT C E, P1.
   BALDI P, 1993, NEURAL COMPUT, V5, P402, DOI 10.1162/neco.1993.5.3.402.
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339.
   Carranza Rojas J., 2018, 2018 IEEE INT WORK C, P1, DOI 10.1109/IWOBI.2018.8464187.
   Carranza-Rojas J, 2017, BMC EVOL BIOL, V17, P1, DOI 10.1186/s12862-017-1014-z.
   Diaz-Vaz O. J. E., 1979, Bosque, V3, P15.
   Figueroa-Mata G, 2018, INT WORK C BIOINSPIR, P1, DOI DOI 10.1109/IWOBI.2018.8464206.
   Figueroa-Mata G., 2018, 2018 IEEE INT WORK C, P1.
   Figueroa-Mata G, 2020, BIOMIMETICS-BASEL, V5, DOI 10.3390/biomimetics5010008.
   Fournier Origgi L., 2016, BIOCENOSIS, V19, P35.
   Goeau H., 2017, CLEF 2017 C LABS EVA, P1.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Hernandez-Castro F, 2016, METODOLOGIA ANALISIS.
   Holland O, 2016, 2016 23RD INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT), DOI 10.1109/ICT.2016.7500442.
   Jinmei Yang, 2019, 2019 IEEE International Conference on Computer Science and Educational Informatization (CSEI), P124.
   Koch G., 2015, PROC ICML DEEP LEARN.
   Kwon OhKyung, 2017, Mokchae Konghak = Journal of the Korean Wood Science and Technology, V45, P797.
   Larochelle H., 2008, AAAI.
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79.
   Lopes DJV, 2020, FORESTS, V11, DOI 10.3390/f11030298.
   Mata-Montero E., 2018, BIODIVERSITY INF SCI, V2, DOI DOI 10.3897/BISS.2.25260.
   Okataria A.S., 2019, J INF TECHNOL COMPUT, V4, P274, DOI {[}10.25126/jitecs.201943155, DOI 10.25126/JITECS.201943155].
   ONF, 2020, DECR EJ POL CRIT PRI.
   Paula PL, 2014, MACH VISION APPL, V25, P1019, DOI 10.1007/s00138-014-0592-7.
   Quesada-Monje R, 2004, REV FORESTAL MESOAME, V1, P84.
   Ravindran P., 2019, 33 C NEUR INF PROC S, P1.
   Ravindran P, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0292-9.
   Ren C, 2010, EUR J INORG CHEM, P5545, DOI 10.1002/ejic.201000731.
   SCIJ, 1996, DECR EJ VED N 25700.
   Siew K.F., 2017, P SPIE, V10443, P10443.
   Valverde JC, 2020, REV CUB CIENC FOR, V8, P439.
   Vinyals O., 2016, ADV NEURAL INFORM PR, P3630.
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252.
   Wiedenhoeft A., 2011, CARACTERES BASICOS U.
   Wiedenhoeft A., 2011, CLAVE IDENTIFICACION.
   Yusof Rubiyah, 2020, Computational and Experimental Simulations in Engineering. Proceedings of ICCES2019. Mechanisms and Machine Science (MMS 75), P1225, DOI 10.1007/978-3-030-27053-7\_104.},
Number-of-Cited-References = {38},
Times-Cited = {4},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {14},
Journal-ISO = {Front. Plant Sci.},
Doc-Delivery-Number = {0U8QK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000787912400001},
OA = {gold, Green Published},
DA = {2023-08-12},
}

@inproceedings{ WOS:000435133900076,
Author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M. T.},
Book-Group-Author = {IEEE},
Title = {Whether color, shape and texture of leaves are the key features for
   image processing based plant recognition? An analysis!},
Booktitle = {2017 RECENT DEVELOPMENTS IN CONTROL, AUTOMATION AND POWER ENGINEERING
   (RDCAPE)},
Year = {2017},
Pages = {404-409},
Note = {Conference on Recent Developments in Control, Automation and Power
   Engineering (RDCAPE), Amity Sch Engn \& Technol, Noida, INDIA, OCT
   26-27, 2017},
Organization = {IEEE; IEEE UP Sect},
Abstract = {Studies on plant identification through image processing consider shape,
   color and texture features of leafs. But botanist's uses leaf
   morphology, leaf arrangement, types of venation, leave shapes, leave
   bases, leaf margins and leaf apices for recognizing a plant. This paper
   introduces the leaf venation, leaf margin, leaf apies, and leaf bases
   models for improving plant leaf identification. These new features along
   with shape, color and feature increases the accuracy of the plant
   identification.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Thanikkal, JG (Corresponding Author), Amity Univ Uttar Pradesh, Dept Comp Sci \& Engn, Noida, India.
   Thanikkal, Jibi G., Amity Univ Uttar Pradesh, Dept Comp Sci \& Engn, Noida, India.
   Dubey, Ashwani Kumar, Amity Univ Uttar Pradesh, Dept Elect \& Commun Engn, Noida, India.
   Thomas, M. T., St Thomas Coll, Dept Bot, Trichur, Kerala, India.},
ISBN = {978-1-5090-3978-4},
Keywords = {Plant recognition; Image processing; Shape matching; Morphological
   features; Leaf image},
Keywords-Plus = {LEAF; CLASSIFICATION; MORPHOMETRICS},
Research-Areas = {Automation \& Control Systems; Energy \& Fuels; Engineering},
Web-of-Science-Categories  = {Automation \& Control Systems; Energy \& Fuels; Engineering, Electrical
   \& Electronic},
Author-Email = {jibimary@gmail.com
   dubeylak@gmail.com
   thomastbgri@gmail.com},
Affiliations = {Amity University Noida; Amity University Noida},
ResearcherID-Numbers = {Thomas, M T/G-5518-2011
   Dubey, Ashwani Kumar/ABI-1337-2020
   Thanikkal, Jibi/ABG-9262-2021
   },
ORCID-Numbers = {Dubey, Ashwani Kumar/0000-0003-0778-9262
   Thanikkal, Jibi/0000-0002-5577-1158
   M T, Thomas/0000-0001-5952-5125},
Cited-References = {Abbasi S, 1999, MULTIMEDIA SYST, V7, P467, DOI 10.1007/s005300050147.
   Abdolvahab E., 2010, LEAF RECOGNITION PLA.
   Abdul K., 2012, J THEORETICAL APPL I, V41, P83.
   Agarwal G, 2006, TAXON, V55, P597, DOI 10.2307/25065637.
   {[}Anonymous], 1995, STORAGE RETRIEVAL IM, DOI {[}10.1117/12.205308, DOI 10.1117/12.205308].
   Arun C. H., 2013, INT J COMPUTER APPL, V62, P1, DOI 10.5120/10129-4920.
   Backes AR, 2010, PATTERN RECOGN, V43, P685, DOI 10.1016/j.patcog.2009.07.017.
   Backes AR, 2009, INT J PATTERN RECOGN, V23, P1145, DOI 10.1142/S0218001409007508.
   Beghin T., 2010, SHAPE TEXTURE BASED.
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
   Bober M., 2002, INTRO MPEG 7 MULTIME, P231.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Chaki J, 2015, LETTERS, V58, P61.
   Chi ZR, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS \& SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P1035.
   Clark J. Y., 2012, 2012 IEEE S COMP INT, P342.
   Clark JY, 2009, BOT J LINN SOC, V159, P300, DOI 10.1111/j.1095-8339.2008.00891.x.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Corney DPA, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0042112.
   Du J.-X., 2006, T I MEASUREMENT CONT.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   El-Ghazal A, 2007, LECT NOTES COMPUT SC, V4633, P650.
   El-ghazal A, ICIAR, P650.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Gupta C., 2015, INT JURNL IRAE, V2.
   Hajjdiab H., 2011, P IEEE INT C IM SYST, P17.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Ingrouille MJ, 1986, LON NAT, V65, P35, DOI DOI 10.6041/J.ISSN.1000-1298.2017.03.004.
   Jamil N, 2015, PROCEDIA COMPUT SCI, V76, P436, DOI 10.1016/j.procs.2015.12.287.
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Kepapci H., 2011, OXFORD JURNLS, V54.
   Kim T. Y., 2004, LECT NOTES COMPUT SC, P146.
   Kumar N., 2011, LEAFSNAP COMPUT VISI.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lexer C, 2009, TAXON, V58, P349, DOI 10.1002/tax.582003.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Meade C, 2003, BOT J LINN SOC, V143, P231, DOI 10.1046/j.1095-8339.2003.00223.x.
   Mokhtarian F, 2005, COMPUT GRAPH-UK, V29, P961, DOI 10.1016/j.cag.2005.09.012.
   Mokhtarian F, 2004, IEEE T IMAGE PROCESS, V13, P653, DOI 10.1109/TIP.2004.826126.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Park J, 2008, J SYST SOFTWARE, V81, P71, DOI 10.1016/j.jss.2007.05.001.
   Prasad S., 2011, 2011 2nd International Conference on Computer and Communication Technology, P646, DOI 10.1109/ICCCT.2011.6075212.
   Ramos E, 2009, ECOL INFORM, V4, P177, DOI 10.1016/j.ecoinf.2009.06.003.
   Rashad M. Z., 2011, International Journal of Computer Science \& Information Technology, V3, P93, DOI 10.5121/ijcsit.2011.3407.
   Sa JJD, 2011, BOTANY, V89, P467, DOI {[}10.1139/B11-038, 10.1139/b11-038].
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972.
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710.
   Sumathi C., 2012, INT J COMPUT SCI TEL, V3, P6.
   Tico M., 2000, NORSIG2000. Nordic Signal Processing Symposium, P157.
   Venters C., 2000, TECHNICAL REPORT.
   Wang F, 2015, PLANT METHODS, V11, DOI 10.1186/s13007-015-0049-7.
   Wang X., 2008, APPL MATH COMPUT, V205, P16.
   Wang XF, 2005, LECT NOTES COMPUT SC, V3644, P87.
   Wang Z., 2003, IEE P VISION IMAGE S, V150.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yusof R, 2010, 2010 12TH INTERNATIONAL CONFERENCE ON COMPUTER MODELLING AND SIMULATION (UKSIM), P289, DOI 10.1109/UKSIM.2010.61.
   Zhang DS, 2003, MULTIMEDIA SYST, V9, P15, DOI 10.1007/s00530-002-0075-y.
   Zhang L, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 5, PROCEEDINGS, P90, DOI 10.1109/ICNC.2008.253.
   Zhao ZQ, 2015, NEUROCOMPUTING, V151, P1112, DOI 10.1016/j.neucom.2014.02.077.
   {[}郑小东 Zheng Xiaodong], 2011, {[}中国农学通报, Chinese Agricultural Science Bulletin], V27, P174.},
Number-of-Cited-References = {59},
Times-Cited = {6},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BK3KE},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000435133900076},
DA = {2023-08-12},
}

@inproceedings{ WOS:000249782700011,
Author = {Wu Qingfeng and Lin Kunhui and Zhou Changle},
Editor = {Li, M},
Title = {Feature extraction and automatic recognition of plant leaf using
   artificial neural network},
Booktitle = {ADVANCED COMPUTER TECHNOLOGY, NEW EDUCATION, PROCEEDINGS},
Year = {2007},
Pages = {47-50},
Note = {2nd International Conference on Computer Science and Education, Wuhan,
   PEOPLES R CHINA, JUL 25-27, 2007},
Organization = {Zhongnan Univ Econ \& Law; Xiamen Univ; IEEE Control Syst Soc, Singapore
   Chapter; Univ Melbourne; Univ Virginia; Natl Univ Singapore},
Abstract = {Plant recognition is an important and challenging task. Leaf recognition
   plays an important role in plant recognition and its key issue lies in
   whether selected features are stable and have good ability to
   discriminate different kinds of leaves. From the view of plant leaf
   morphology (such as shape, dent, margin, vein and so on), domain-related
   visual features of plant leaf are analyzed and extracted first. On such
   a basis, an approach for recognizing plant leaf using artificial neural
   network is brought forward. The prototype system has been implemented.
   Experiment results prove the effectiveness and superiority of this
   method.},
Publisher = {XIAMEN UNIV PRESS},
Address = {XIAMEN, FUJIAN, 361005, PEOPLES R CHINA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Wu, QF (Corresponding Author), Xiamen Univ, Coll Software, Xiamen 361005, Fujian, Peoples R China.
   Xiamen Univ, Coll Software, Xiamen 361005, Fujian, Peoples R China.},
ISBN = {978-7-5615-2825-9},
Keywords = {feature extraction; plant leaf; artificial neural network},
Keywords-Plus = {IMAGE RETRIEVAL},
Research-Areas = {Computer Science; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Interdisciplinary Applications; Computer
   Science, Software Engineering; Telecommunications},
Author-Email = {qfwu@xmu.edu.cn
   khlin@xmu.edu.cn
   dozero@xmu.edu.cn},
Affiliations = {Xiamen University},
ResearcherID-Numbers = {Lozano, Jose A. A/F-5120-2010
   Rodriguez-Aguilar, Juan A/H-1952-2015
   Wu, QF/G-3372-2010},
ORCID-Numbers = {Lozano, Jose A. A/0000-0002-4683-8111
   Rodriguez-Aguilar, Juan A/0000-0002-2940-6886
   },
Cited-References = {ASH AW, 1999, MANUAL LEAF ARCHITEC, P26.
   {[}傅弘 Fu Hong], 2004, {[}植物学通报, Chinese Bulletin of Botany], V21, P429.
   Han JW, 2003, SIGNAL PROCESS-IMAGE, V18, P141, DOI 10.1016/S0923-5965(02)00116-9.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Hussein E., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P335, DOI 10.1109/ICPR.1996.546845.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   Jain A.K., 1989, FUNDAMENTALS DIGITAL.
   Mehrotra K, 1997, ELEMENTS ARTIFICIAL.
   {[}祁亨年 Qi Hengnian], 2003, {[}浙江林学院学报, Journal of Zhejiang Forestry College], V20, P281.
   Ripley BD., 2007, PATTERN RECOGN.
   SAITOH T, 2000, P IEEE INT C PATT RE, V2, P507, DOI DOI 10.1109/ICPR.2000.906123.
   Wang Z, 1992, ADV VISUAL INFORM SY, P477.
   WEIER TE, 1982, BOTANY INTRO PLANT B.
   Zhang B, 2002, COMPUTER ENG APPL, V07, P203.
   Zhou XS, 2001, PATTERN RECOGN LETT, V22, P457, DOI 10.1016/S0167-8655(00)00124-0.},
Number-of-Cited-References = {15},
Times-Cited = {4},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BGQ59},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000249782700011},
DA = {2023-08-12},
}

@article{ WOS:000516827100004,
Author = {Tiwari, Shamik},
Title = {A Comparative Study of Deep Learning Models With Handcraft Features and
   Non-Handcraft Features for Automatic Plant Species Identification},
Journal = {INTERNATIONAL JOURNAL OF AGRICULTURAL AND ENVIRONMENTAL INFORMATION
   SYSTEMS},
Year = {2020},
Volume = {11},
Number = {2},
Pages = {44-57},
Month = {APR-JUN},
Abstract = {The classification of plants is one of the most important aims for
   botanists since plants have a significant part in the natural life
   cycle. In this work, a leaf-based automatic plant classification
   framework is investigated. The aim is to compare two different deep
   learning approaches named Deep Neural Network (DNN) and deep
   Convolutional Neural Network (CNN). In the case of deep neural network,
   hybrid shapes and texture features are utilized as hand-crafted features
   while in the case of the convolution non-handcraft, features are applied
   for classification. The offered frameworks are evaluated with a public
   leaf database. From the simulation results, it is confirmed that the
   deep CNN-based deep learning framework demonstrates superior
   classification performance than the handcraft feature based approach.},
Publisher = {IGI GLOBAL},
Address = {701 E CHOCOLATE AVE, STE 200, HERSHEY, PA 17033-1240 USA},
Type = {Article},
Language = {English},
Affiliation = {Tiwari, S (Corresponding Author), UPES Univ, Dehra Dun, India.
   Tiwari, Shamik, UPES Univ, Dehra Dun, India.},
DOI = {10.4018/IJAEIS.2020040104},
ISSN = {1947-3192},
EISSN = {1947-3206},
Keywords = {CNN; Deep Learning; Feature Extraction; Plant Classification},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications},
Affiliations = {University of Petroleum \& Energy Studies (UPES)},
ResearcherID-Numbers = {tiwari, shamik/AAT-5917-2020
   tiwari, shamik/AAR-2040-2021
   Tiwari, Shamik/AAG-8726-2021
   tiwari, shamik/AAO-2551-2021},
ORCID-Numbers = {tiwari, shamik/0000-0002-5987-7101
   },
Cited-References = {{[}Anonymous], 2014, THE J.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Du JX, 2013, NEUROCOMPUTING, V116, P150, DOI 10.1016/j.neucom.2012.03.028.
   Elhariri E, 2014, 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING \& SYSTEMS (ICCES), P271, DOI 10.1109/ICCES.2014.7030971.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Gaur P., 2014, INT J RES ADVENT TEC, V2, P433.
   Haque F, 2018, ICTACT J IMAGE VIDEO, V9.
   Kadir A, 2013, NEURAL NETWORK APPL.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Raal A., 2005, TRAMES J HUMANITIES, V9.
   Riaz A., 2018, NUCLEUS, V55, P1.
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162.
   Shupeng Liu, 2018, 2018 14th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD), P235, DOI 10.1109/FSKD.2018.8687165.
   Silva F, 2013, OCCUPATIONAL SAFETY AND HYGIENE, P197.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Tiwari S., 2017, INDONESIAN J ELECT E, V5, P162.
   Tiwari S, 2018, INT J INF SYST MODEL, V9, DOI 10.4018/IJISMD.2018100101.},
Number-of-Cited-References = {19},
Times-Cited = {12},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {12},
Journal-ISO = {Int. J. Agric. Environ. Inf. Syst.},
Doc-Delivery-Number = {KQ3LD},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000516827100004},
OA = {Bronze},
DA = {2023-08-12},
}

@inproceedings{ WOS:000380396300055,
Author = {Bressane, Adriano and Frutuoso Roveda, Jose Arnaldo and Germano Martins,
   Antonio Cesar},
Editor = {Vellasco, MMBR and Valdivia, YJT and Lopes, HS},
Title = {Pattern recognition in trunk images based on co-occurrence descriptors:
   a proposal applied to tree species identification},
Booktitle = {2015 LATIN AMERICA CONGRESS ON COMPUTATIONAL INTELLIGENCE (LA-CCI)},
Year = {2015},
Note = {Latin America Congress on Computational Intelligence (LA-CCI), Curitiba,
   BRAZIL, OCT 13-16, 2015},
Organization = {Universidade Tecnologica Federal Parana; Associacao Brasileira
   Inteligencia Computacional; Conselho Nacional Desenvolvimento Cientifico
   Tecnologico; Coordenacao Aperfeicoamento Pessoal Nivel Super; IEEE Comp
   Int Soc; Universidade Tecnologica Federal Parana},
Abstract = {Tree species identification is required for many applications. However,
   current techniques are dependent on the presence of morphological
   structures such as leaves, which restricts its use in certain situations
   and seasons. In this context, the use of trunk images can be an
   alternative. Therefore, the present study developed a pattern
   recognition based on co-occurrence descriptors, aiming evaluate its
   performance in the identification of 8 tree species from the Brazilian
   deciduous native forest, achieving promising results, with precision
   better than 0.8 for most of them, accuracy equivalent to 0.77 and
   average area under curve by Receiver Operating Characteristic of 0.88,
   during the tests with cross-validation sets.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Bressane, A (Corresponding Author), UNESP Univ Estadual Paulista, Environm Sci Grad Program, Sorocaba City, SP, Brazil.
   Bressane, Adriano; Frutuoso Roveda, Jose Arnaldo; Germano Martins, Antonio Cesar, UNESP Univ Estadual Paulista, Environm Sci Grad Program, Sorocaba City, SP, Brazil.},
ISBN = {978-1-4673-8418-6},
Keywords = {Image processing; Decision Tree; Co-occurrence descriptors; Trunk
   images; Brazilian forest},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {adriano.bressane@posgrad.sorocaba.unesp.br},
Affiliations = {Universidade Estadual Paulista},
ResearcherID-Numbers = {Martins, Antonio C G/L-9202-2013},
Cited-References = {{[}Anonymous], THESIS.
   {[}Anonymous], 2013, INT J INNOV RES SCI.
   {[}Anonymous], 1995, MACHINE VISION.
   {[}Anonymous], 2009, DIGITAL IMAGE PROCES.
   {[}Anonymous], 2008, COMPUT ANAL VIS TEXT.
   ArunPriya C., 2012, INT J COMPUTER TECHN, V3, P1132.
   Bressane A., 2015, ENVIRON MONIT ASSESS, V187, P1.
   Casanova D., 2011, P 2011 C LABS EV FOR.
   Sa JJD, 2013, ECOL INFORM, V15, P34, DOI 10.1016/j.ecoinf.2013.02.007.
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Landgrebe TCW, 2007, PATTERN RECOGN LETT, V28, P1747, DOI 10.1016/j.patrec.2007.05.001.
   Machado BB, 2013, J PHYS CONF SER, V410, DOI 10.1088/1742-6596/410/1/012066.
   MORET BME, 1982, COMPUT SURV, V14, P593, DOI 10.1145/356893.356898.
   Provost F, 1998, MACH LEARN, V30, P127, DOI 10.1023/A:1007442505281.
   Rossatto DR, 2011, PLANT SYST EVOL, V291, P103, DOI 10.1007/s00606-010-0366-2.
   Silva NR, 2014, J PHYS CONF SER, V490, DOI 10.1088/1742-6596/490/1/012085.
   van Diepen M, 2006, INFORM SYST, V31, P814, DOI 10.1016/j.is.2005.03.002.},
Number-of-Cited-References = {19},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BF1HA},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000380396300055},
DA = {2023-08-12},
}

@article{ WOS:000864244800001,
Author = {Pradipkumar, Vaghela Himali and Raja, R. A. Alagu},
Title = {Automatic Identification of Tree Species from UAV Images Using Machine
   Learning Approaches},
Journal = {JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING},
Year = {2022},
Volume = {50},
Number = {12},
Pages = {2447-2464},
Month = {DEC},
Abstract = {Today, the identification of tree species has become essential in order
   to protect biodiversity and ecological equilibrium and fulfil medicinal
   purposes. It is beneficial for large parts of society, foresters,
   farmers, biologists, conservationists, and landscape architects. The
   process of identifying tree species by conventional methods is
   time-consuming and prone to human error. So, a method based on automatic
   tree species identification is required to reduce human error as well as
   time. This can be achieved by using artificial intelligence and image
   processing techniques. This paper deals with machine learning
   approaches, a subset of artificial intelligence, where different
   supervised classifiers such as support vector machine (SVM), k-nearest
   neighbour (kNN), decision tree (DT), random forest (RF), and eXtreme
   gradient boosting (XGBoost) are used to identify different tree species
   such as banana, coconut, mango, oil palm, and papaya. To implement each
   machine learning model, features are extracted from training images
   using the histogram of oriented gradient (HOG) descriptor. These feature
   vectors are fed as inputs to the classifiers so that the generated model
   can identify the unlabelled tree species. To demonstrate the
   effectiveness of the HOG descriptor in identifying tree species, two
   additional feature descriptors, gray-level co-occurrence matrix and
   linear binary pattern, are used. The performance of each model is
   assessed using the confusion matrix. The overall accuracy obtained for
   the classifiers SVM, kNN, DT, RF, and XGBoost using the HOG is 97.05\%,
   92.05\%, 75.00\%, 95.00\%, and 97.94\%, respectively. XGBoost gives
   higher accuracy because it combines the estimates of individual
   classifiers.},
Publisher = {SPRINGER},
Address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
Type = {Article},
Language = {English},
Affiliation = {Pradipkumar, VH (Corresponding Author), Thiagarajar Coll Engn, Remote Sensing \& GIS Lab, Madurai 625015, Tamil Nadu, India.
   Pradipkumar, Vaghela Himali; Raja, R. A. Alagu, Thiagarajar Coll Engn, Remote Sensing \& GIS Lab, Madurai 625015, Tamil Nadu, India.},
DOI = {10.1007/s12524-022-01608-6},
EarlyAccessDate = {OCT 2022},
ISSN = {0255-660X},
EISSN = {0974-3006},
Keywords = {HOG feature descriptor; Machine learning; Tree species identification;
   XGBoost classifier},
Keywords-Plus = {CROP CLASSIFICATION; RANDOM FOREST; TIME-SERIES; CLASSIFIERS},
Research-Areas = {Environmental Sciences \& Ecology; Remote Sensing},
Web-of-Science-Categories  = {Environmental Sciences; Remote Sensing},
Author-Email = {hpvaghela10@gmail.com
   alaguraja@tce.edu},
Affiliations = {Thiagarajar College of Engineering},
ORCID-Numbers = {, ALAGU RAJA RA/0000-0001-5524-1221
   Vaghela, Himali/0000-0002-3743-1555},
Funding-Acknowledgement = {All India Council for Technical Education (AICTE), New Delhi, Government
   of India},
Funding-Text = {We thank the All India Council for Technical Education (AICTE), New
   Delhi, Government of India, for the National Doctoral Fellowship
   (NDF/ADF) scheme.},
Cited-References = {Alzubaidi L, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00444-8.
   Azlah MAF, 2019, COMPUTERS, V8, DOI 10.3390/computers8040077.
   Berni JAJ, 2009, IEEE T GEOSCI REMOTE, V47, P722, DOI 10.1109/TGRS.2008.2010457.
   Brester, 2019, AERIAL PAPAYA PLANTA.
   Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785.
   Conrad C, 2011, INT J REMOTE SENS, V32, P8763, DOI 10.1080/01431161.2010.550647.
   Dzeroski S, 2004, MACH LEARN, V54, P255, DOI 10.1023/B.MAC.0000015881.36452.6e.
   Feret JB, 2013, IEEE T GEOSCI REMOTE, V51, P73, DOI 10.1109/TGRS.2012.2199323.
   Fricker GA, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11192326.
   Guo GD, 2003, LECT NOTES COMPUT SC, V2888, P986.
   Hardy M, 2010, MATH INTELL, V32, P38, DOI 10.1007/s00283-010-9159-2.
   Humeau-Heurtier A, 2019, IEEE ACCESS, V7, P8975, DOI 10.1109/ACCESS.2018.2890743.
   Jones, 2017, AERIAL VIEW MANGO OR.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   KUSNETZOVA TV, 1988, FLORA, V181, P1.
   Ma L, 2019, ISPRS J PHOTOGRAMM, V152, P166, DOI 10.1016/j.isprsjprs.2019.04.015.
   Muller, 2014, AERIAL VIEW LARGE BA.
   Nandyal SS., 2013, INT J COMPUT VIS ROB, V3, P197, DOI {[}10.1504/IJCVR.2013.056040, DOI 10.1504/IJCVR.2013.056040].
   Ojala T., 2001, Advances in Pattern Recognition - ICAPR 2001. Second International Conference. Proceedings (Lecture Notes in Computer Science Vol.2013), P397.
   Onishi M, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-020-79653-9.
   Pal M, 2005, INT J REMOTE SENS, V26, P217, DOI 10.1080/01431160412331269698.
   Pal M, 2005, INT J REMOTE SENS, V26, P1007, DOI 10.1080/01431160512331314083.
   Prakobkit, 2014, AERIAL VIEW COCONUT.
   Raczko E, 2017, EUR J REMOTE SENS, V50, P144, DOI 10.1080/22797254.2017.1299557.
   Runions A, 2005, ACM T GRAPHIC, V24, P702, DOI 10.1145/1073204.1073251.
   SAFAVIAN SR, 1991, IEEE T SYST MAN CYB, V21, P660, DOI 10.1109/21.97458.
   Satpathy A, 2014, IEEE T IMAGE PROCESS, V24, P1953, DOI 10.1109/TIP.2014.2310123.
   SUN Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI DOI 10.1155/2017/7361042.
   Torrione PA, 2014, IEEE T GEOSCI REMOTE, V52, P1539, DOI 10.1109/TGRS.2013.2252016.
   Bao TQ, 2020, J INFORM TELECOMMUN, V4, P140, DOI 10.1080/24751839.2019.1666625.
   Tsolakidis DG, 2014, LECT NOTES ARTIF INT, V8445, P406, DOI 10.1007/978-3-319-07064-3\_33.
   Wang YR, 2019, INT J REMOTE SENS, V40, P7356, DOI 10.1080/01431161.2018.1513669.
   Wardlow BD, 2007, REMOTE SENS ENVIRON, V108, P290, DOI 10.1016/j.rse.2006.11.021.
   Wongsa, 2012, NATURE PATTERN PALM.
   Xiong JT, 2020, BIOSYST ENG, V194, P261, DOI 10.1016/j.biosystemseng.2020.04.006.},
Number-of-Cited-References = {35},
Times-Cited = {0},
Usage-Count-Last-180-days = {9},
Usage-Count-Since-2013 = {15},
Journal-ISO = {J. Indian Soc. Remote Sens.},
Doc-Delivery-Number = {6N7UP},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000864244800001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000799474100059,
Author = {Shafi, Muhammed K. and Ismail, Mohammed B.},
Book-Group-Author = {IEEE},
Title = {Systematic Study on Different Methods Used for Identification and
   Classification of Plant Leaves},
Booktitle = {2021 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS,
   COMMUNICATION, COMPUTER TECHNOLOGIES AND OPTIMIZATION TECHNIQUES
   (ICEECCOT)},
Year = {2021},
Pages = {328-332},
Note = {5th International Conference on Electrical, Electronics, Communication,
   Computer Technologies and Optimization Techniques (ICEECCOT), Mysuru,
   INDIA, DEC 10-11, 2021},
Organization = {Geetha Shishu Shikshana Sangha Inst Engn \& Technol Women; IEEE
   Bangalore Sect; IEEE Mysore Subsect; NBA; NACC; I GAUGE},
Abstract = {Plants can be recognized based on its different parts such as leaves,
   flowers, seeds, fruits, roots etc. Plants are identified for different
   purposes. Wide varieties of plant species are available in nature.
   Botanists are research plants to classify plant species in the wild and
   take their benefits out. There is a lot of demand for identifying plant
   species manually or visually inspecting their characteristics, study
   their availability on various geographical locations to reap their
   benefits effectively. Based on the wide variety of flora available it is
   practically impossible to manually or visually classify, differentiate
   or record their features. Hence automated or computer aided methods are
   developed assisting botanists to accurately feature wise differentiate
   the plant species. The difficulties faced by a botanist are that some
   plants having the same characteristics require extra features to be
   identified. Automated plant identification helps common people to
   identify and use them having no professional prior knowledge about the
   plant. Plant identification is done in a variety of ways by researchers.
   The aim of this paper is to investigate and summarize a systematic study
   of various plant classification methodologies.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Shafi, KM (Corresponding Author), Kannur Univ, Dept Informat Technol, Mangattuparamba, India.
   Shafi, Muhammed K.; Ismail, Mohammed B., Kannur Univ, Dept Informat Technol, Mangattuparamba, India.},
DOI = {10.1109/ICEECCOT52851.2021.9708023},
ISBN = {978-1-6654-3272-6},
Keywords = {Neural Network; Machine learning; Plant Morphology},
Keywords-Plus = {LEAF RECOGNITION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \&
   Electronic},
Author-Email = {muhammedshafik@gmail.com
   mohammedismailb@karmuruniv.ac.in},
ResearcherID-Numbers = {B, Mohammed Ismail/U-6139-2018
   b, m/HPE-0989-2023},
ORCID-Numbers = {B, Mohammed Ismail/0000-0003-4480-3801
   },
Cited-References = {Azlah MAF, 2019, COMPUTERS, V8, DOI 10.3390/computers8040077.
   Bambil Deborah, 2020, Environment Systems \& Decisions, V40, P480, DOI 10.1007/s10669-020-09769-w.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Begue A, 2017, INT J ADV COMPUT SC, V8, P166.
   Bisen D, 2021, MULTIMED TOOLS APPL, V80, P6443, DOI 10.1007/s11042-020-10038-w.
   Figueroa-Mata Geovanni, 2018, 2018 IEEE INT WORK C.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Igbineweka K., 2020, IMPLEMENTATIONS APPL, P197, DOI 10.1007/978-3-030-37830-1\_8.
   Ismail Mohammad, 2019, INT J INNOVATIVE TEC, V8, P1529.
   Ismail BM, 2020, INT CONF ELECTRO INF, P228, DOI 10.1109/EIT48999.2020.9208240.
   Ismail BM, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS, VISION AND INFORMATION SECURITY (CGVIS), P284, DOI 10.1109/CGVIS.2015.7449938.
   Kansara Meera, 2020, INDIAN AYURVEDIC PLA.
   Lakshmi K.Naga, 2019, INT J INNOVATIVE TEC, V8, P1742.
   Lukic M, 2017, 2017 IEEE 15TH INTERNATIONAL SYMPOSIUM ON APPLIED MACHINE INTELLIGENCE AND INFORMATICS (SAMI), P485, DOI 10.1109/SAMI.2017.7880358.
   Mohammed Ismail B., 2020, INT J EMERG TRENDS E, V8, P5693, DOI {[}10.30534/ijeter/2020/127892020, DOI 10.30534/IJETER/2020/127892020].
   Mohammed Ismail B, 2016, IEEE 2016 INT C ELEC.
   Pushpanathan K, 2021, ARTIF INTELL REV, V54, P305, DOI 10.1007/s10462-020-09847-0.
   Sabarinathan C., 2018, INT J GLOB ENG, V1, P120.
   Shahane R., 2019, J COMPUT THEOR NANOS, V16, P5078.
   Shaik GA, 2020, INT CONF ADVAN COMPU, P510, DOI {[}10.1109/icaccs48705.2020.9074352, 10.1109/ICACCS48705.2020.9074352].
   Singh Vijai, 2017, Information Processing in Agriculture, V4, P41, DOI 10.1016/j.inpa.2016.10.005.
   Suto J, 2020, INTELL DATA ANAL, V24, P1311, DOI 10.3233/IDA-194821.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Vo Anh H., 2019, International Journal of Machine Learning and Computing, V9, P363, DOI 10.18178/ijmlc.2019.9.3.811.
   Wu Huisi, 2017, INT C NEURAL INFORM.
   Xiao QG, 2018, ECOL INFORM, V48, P117, DOI 10.1016/j.ecoinf.2018.09.001.
   Yang CZ, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107809.
   Zhang SW, 2017, LECT NOTES COMPUT SC, V10361, P282, DOI 10.1007/978-3-319-63309-1\_26.
   Zhu HY, 2018, MULTIMED TOOLS APPL, V77, P29779, DOI 10.1007/s11042-017-5578-9.
   Zhu XL, 2018, COGN SYST RES, V52, P223, DOI 10.1016/j.cogsys.2018.06.008.
   Zhu YX, 2019, NEUROCOMPUTING, V365, P191, DOI 10.1016/j.neucom.2019.07.016.},
Number-of-Cited-References = {32},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BT1OA},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000799474100059},
DA = {2023-08-12},
}

@inproceedings{ WOS:000275935000146,
Author = {Kim, Jung-Hyun and Huang, Rong-Guo and Jin, Sang-Hyeon and Hong,
   Kwang-Seok},
Editor = {Luo, Q and Zhu, M},
Title = {Mobile-Based Flower Recognition System},
Booktitle = {2009 THIRD INTERNATIONAL SYMPOSIUM ON INTELLIGENT INFORMATION TECHNOLOGY
   APPLICATION, VOL 3, PROCEEDINGS},
Year = {2009},
Pages = {580-583},
Note = {3rd International Symposium on Intelligent Information Technology
   Application, Nanchang, PEOPLES R CHINA, NOV 21-22, 2009},
Organization = {IEEE Circuits \& Syst Soc (CAS); Intelligent Informat Technol Applicat
   Res Assoc; Nanchang Univ},
Abstract = {Conventional flower or leaf recognition studies have some restrictions
   and limitations. These include a sharp drop in recognition rate due to
   the varying positions and number of relevant objects in the original
   object image. Hence, this paper suggests and implements a mobile-based
   flower recognition system using Difference Image Entropy (DIE) and
   contour features of the flower from the original image with multi-flower
   objects. In system framework includes 1) WiBro Net.-based transmission
   and designation module of the relevant flower object by drawing the
   flower region of the user's interest, 2) contour feature extraction
   module, and 3) DIE-based flower recognition module. The system was
   evaluated using ten species of flowers with each ten samples.
   Experimental results achieved an optimum average recognition rate of
   95\% and average response run-time of 9,033ms, for a set of ten images
   per species.},
Publisher = {IEEE COMPUTER SOC},
Address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kim, JH (Corresponding Author), Sungkyunkwan Univ, Sch Informat \& Commun Engn, Suwon 440746, South Korea.
   Kim, Jung-Hyun; Huang, Rong-Guo; Jin, Sang-Hyeon; Hong, Kwang-Seok, Sungkyunkwan Univ, Sch Informat \& Commun Engn, Suwon 440746, South Korea.},
DOI = {10.1109/IITA.2009.407},
ISBN = {978-0-7695-3859-4},
Keywords = {flower recognition; difference image entropy; contour feature},
Research-Areas = {Automation \& Control Systems; Computer Science},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Artificial
   Intelligence; Computer Science, Interdisciplinary Applications},
Author-Email = {kjh0328@skku.edu
   hrg316@skku.edu
   jinjinsh@skku.edu
   kshong@skku.ac.kr},
Affiliations = {Sungkyunkwan University (SKKU)},
Cited-References = {Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Harmsen SR, 2009, COMPUT ELECTRON AGR, V65, P7, DOI 10.1016/j.compag.2008.07.004.
   JEON JB, 2008, 3 INT C CONV HYBR IN, V2, P967.
   Jeon JB, 2009, LECT NOTES COMPUT SC, V5620, P36, DOI 10.1007/978-3-642-02809-0\_5.
   Miao ZJ, 2006, IMAGE VISION COMPUT, V24, P1115, DOI 10.1016/j.imavis.2006.04.004.
   Miao ZJ, 2006, ENG APPL ARTIF INTEL, V19, P79, DOI 10.1016/j.engappai.2005.05.009.
   Saitoh T, 2004, INT C PATT RECOG, P27, DOI 10.1109/ICPR.2004.1333997.
   Zou J, 2004, INT C PATT RECOG, P311, DOI 10.1109/ICPR.2004.1334185.},
Number-of-Cited-References = {8},
Times-Cited = {5},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BNZ36},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000275935000146},
DA = {2023-08-12},
}

@article{ WOS:000818334900001,
Author = {Minowa, Yasushi and Kubota, Yuhsuke and Nakatsukasa, Shun},
Title = {Verification of a Deep Learning-Based Tree Species Identification Model
   Using Images of Broadleaf and Coniferous Tree Leaves},
Journal = {FORESTS},
Year = {2022},
Volume = {13},
Number = {6},
Month = {JUN},
Abstract = {The objective of this study was to verify the accuracy of tree species
   identification using deep learning with leaf images of broadleaf and
   coniferous trees in outdoor photographs. For each of 12 broadleaf and
   eight coniferous tree species, we acquired 300 photographs of leaves and
   used those to produce 72,000 256 x 256-pixel images. We used Caffe as
   the deep learning framework and AlexNet and GoogLeNet as the deep
   learning algorithms. We constructed four learning models that combined
   two learning patterns: one for individual classification of 20 species
   and the other for two-group classification (broadleaf vs. coniferous
   trees), with and without data augmentation, respectively. The
   performance of the proposed model was evaluated according to the MCC and
   F-score. Both classification models exhibited very high accuracy for all
   learning patterns; the highest MCC was 0.997 for GoogLeNet with data
   augmentation. The classification accuracy was higher for broadleaf trees
   when the model was trained using broadleaf only; for coniferous trees,
   the classification accuracy was higher when the model was trained using
   both tree types simultaneously than when it was trained using coniferous
   trees only.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Minowa, Y (Corresponding Author), Kyoto Prefectural Univ, Grad Sch Life \& Environm Sci, Kyoto 6068522, Japan.
   Minowa, Yasushi; Nakatsukasa, Shun, Kyoto Prefectural Univ, Grad Sch Life \& Environm Sci, Kyoto 6068522, Japan.
   Kubota, Yuhsuke, Kyoto Prefectural Univ, Fac Life \& Environm Sci, Kyoto 6068522, Japan.},
DOI = {10.3390/f13060943},
Article-Number = {943},
EISSN = {1999-4907},
Keywords = {AlexNet; broadleaf trees; Caffe; coniferous trees; deep learning;
   F-score; GoogLeNet; MCC; tree species identification},
Keywords-Plus = {SHAPE; RETRIEVAL; EXTRACTION; SCHEME},
Research-Areas = {Forestry},
Web-of-Science-Categories  = {Forestry},
Author-Email = {nakatsushun@gmail.com
   terayu1819@gmail.com
   sharmy@uf.kpu.ac.jp},
Affiliations = {Kyoto Prefectural University; Kyoto Prefectural University},
ORCID-Numbers = {Minowa, Yasushi/0000-0001-7487-5692},
Cited-References = {{[}Anonymous], 2016, SWEDISH LEAF DATASET.
   {[}Anonymous], ML KIT 2018.
   {[}Anonymous], 2004, NIH IMAGEJ.
   {[}Anonymous], 2016, NIKON COOLPIX A900.
   {[}Anonymous], 2018, SAMSUNG GALAXY S9.
   {[}Anonymous], 2012, IMAGECLEF2012.
   {[}Anonymous], 2007, SCIKIT LEARN.
   Aptoula E, 2013, IEEE IMAGE PROC, P1496, DOI 10.1109/ICIP.2013.6738307.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Bisen D, 2021, MULTIMED TOOLS APPL, V80, P6443, DOI 10.1007/s11042-020-10038-w.
   Boulent J, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.00941.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6474, P135, DOI 10.1007/978-3-642-17688-3\_14.
   CoreML, 2017, US.
   DARMINESH JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI DOI 10.1016/J.COMPAG.2005.09.004.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Goeau H, 2015, LIFECLEF PLANT IDENT.
   Gyires-Toth BP, 2019, CYBERN INF TECHNOL, V19, P88, DOI 10.2478/cait-2019-0005.
   Hamrouni L, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13091705.
   Han K., 2020, ARXIV.
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140.
   Jia Y., 2014, CAFFE CONVOLUTIONAL.
   Kiso A, 2020, HEPATOL COMMUN, V4, P255, DOI 10.1002/hep4.1466.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Li H, 2021, FORESTS, V12, DOI 10.3390/f12121697.
   Li Y, 2006, IEEE SYS MAN CYBERN, P3890, DOI 10.1109/ICSMC.2006.384738.
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1\_48.
   Liu M., 2014, ARXIV.
   MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9.
   Minowa, 2020, JPN J PLANN, V53, P43, DOI {[}10.20659/jjfp.53.2\_43, DOI 10.20659/JJFP.53.2\_43].
   Minowa Y., 2020, Journal of Forest Planning, V26, P1, DOI 10.20659/jfp.2020.001.
   Minowa Y., 2011, Journal of Forest Planning, V17, P31, DOI 10.20659/jfp.17.1\_31.
   Minowa Y., 2019, JPN J PLANN, V53, P1, DOI {[}10.20659/jjfp.53.1\_1, DOI 10.20659/JJFP.53.1\_1].
   Minowa Y., 2021, JPN J PLANN, V13, P162.
   Minowa Y, 2022, J FOREST RES-JPN, V27, P246, DOI 10.1080/13416979.2021.2021640.
   Motoda H., 2006, FUNDAMENTALS DATA MI.
   Nam YY, 2005, LECT NOTES COMPUT SC, V3767, P876.
   Nezami S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12071070.
   NVIDIA, 2016, US.
   Perez L., 2017, ARXIV.
   Raschka S., 2018, PYTHON MACHINE LEARN.
   Shen YC, 2005, INT FED INFO PROC, V187, P711.
   Shi Y, 2021, IEEE ACCESS, V9, P13643, DOI 10.1109/ACCESS.2021.3051015.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Szegedy C., 2014, P IEEE C COMPUTER VI, P1.
   Torrey L., 2010, HDB RES MACHINE LEAR, P242, DOI DOI 10.4018/978-1-60566-766-9.CH011.
   Bao TQ, 2020, J INFORM TELECOMMUN, V4, P140, DOI 10.1080/24751839.2019.1666625.
   Wang ZY, 2000, LECT NOTES COMPUT SC, V1929, P477.
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113.
   Witten I. H., 2011, PRACTICAL MACHINE LE.
   Yamashita T., 2016, ILLUSTRATED GUIDE DE.
   Yang KL, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10111721.
   Yunbin Deng, 2019, Proceedings of the SPIE, V10993, DOI 10.1117/12.2518469.
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009.},
Number-of-Cited-References = {55},
Times-Cited = {4},
Usage-Count-Last-180-days = {12},
Usage-Count-Since-2013 = {27},
Journal-ISO = {Forests},
Doc-Delivery-Number = {2N4ER},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000818334900001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000441208400026,
Author = {Zhang, Shanwen and Zhang, Chuanlei},
Editor = {Huang, DS and Bevilacqua, V and Premaratne, P and Gupta, P},
Title = {Plant Species Recognition Based on Deep Convolutional Neural Networks},
Booktitle = {INTELLIGENT COMPUTING THEORIES AND APPLICATION, ICIC 2017, PT I},
Series = {Lecture Notes in Computer Science},
Year = {2017},
Volume = {10361},
Pages = {282-289},
Note = {13th International Conference on Intelligent Computing (ICIC),
   Liverpool, ENGLAND, AUG 07-10, 2017},
Organization = {IEEE Computat Intelligence Soc; Int Neural Network Soc; Natl Sci Fdn
   China; Tongji Univ; Liverpool John Moores Univ},
Abstract = {A number of the existing leaf based plan leaf recognition methods rely
   on the hand-crafted features of color, texture and shape, and other
   various features. One drawback of these methods is poor convergence and
   generalization. To overcome this problem, a deep convolutional neural
   network (DCNN) is applied to plant species recognition. The proposed
   method is different from the existing feature extraction based
   recognition approaches. The high-level features can be extracted by
   DCNN. The experimental results clearly demonstrate the effectiveness and
   efficiency of the proposed model in leaf identification in comparison
   with current state-of-the-art.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zhang, CL (Corresponding Author), Tianjin Univ Sci \& Technol, Tianjin 300222, Peoples R China.
   Zhang, Shanwen, Xijing Univ, Dept Informat Engn, Xian 710123, Shaanxi, Peoples R China.
   Zhang, Chuanlei, Tianjin Univ Sci \& Technol, Tianjin 300222, Peoples R China.},
DOI = {10.1007/978-3-319-63309-1\_26},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-63309-1; 978-3-319-63308-4},
Keywords = {Automatic plant species identification; Deep convolutional neural
   networks (DCNN); Feature extraction; Deep learning (DL)},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {wjdw716@163.com
   a17647@gmail.com},
Affiliations = {Xijing University; Tianjin University Science \& Technology},
Funding-Acknowledgement = {China National Natural Science Foundation {[}61473237]; Tianjin Research
   Program of Application Foundation and Advanced Technology
   {[}14JCYBJC42500]; Tianjin science and technology correspondent project
   {[}16JCTPJC47300]},
Funding-Text = {This work is partially supported by the China National Natural Science
   Foundation under grant No. 61473237. It is also supported by Tianjin
   Research Program of Application Foundation and Advanced Technology under
   grant No. 14JCYBJC42500, and Tianjin science and technology
   correspondent project 16JCTPJC47300. The authors would like to thank all
   the editors and anonymous reviewers for their constructive advices},
Cited-References = {Atabay H. A., 2016, IIOAB J, V7, P326.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   de Souza MMS, 2016, EXPERT SYST APPL, V63, P375, DOI 10.1016/j.eswa.2016.07.016.
   Hasim A., 2016, IOP C SER EARTH ENV, V31, P1.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Jin TS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0139482.
   Kadir A., 2013, INT J COMPUT TRENDS, V1, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003.
   Simonyan K., 2014, 14091556V6 ARXIV, P1, DOI DOI 10.48550/ARXIV.1409.1556.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Wu YH, 2016, LECT NOTES COMPUT SC, V9771, P12, DOI 10.1007/978-3-319-42291-6\_2.
   Xu Q, 2014, IEEE T IMAGE PROCESS, V23, P2944, DOI 10.1109/TIP.2014.2311656.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {17},
Times-Cited = {6},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BK7EI},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000441208400026},
DA = {2023-08-12},
}

@inproceedings{ WOS:000371465701010,
Author = {Wang, Zhaobin and Sun, Xiaoguang and Ma, Yide and Zhang, Hongjuan and
   Ma, Yurun and Xie, Weiying and Wang, Zhaobin and Zhang, Yaonan},
Book-Group-Author = {IEEE},
Title = {Plant Recognition Based on Intersecting Cortical Model},
Booktitle = {PROCEEDINGS OF THE 2014 INTERNATIONAL JOINT CONFERENCE ON NEURAL
   NETWORKS (IJCNN)},
Series = {IEEE International Joint Conference on Neural Networks (IJCNN)},
Year = {2014},
Pages = {975-980},
Note = {International Joint Conference on Neural Networks (IJCNN), Beijing,
   PEOPLES R CHINA, JUL 06-11, 2014},
Organization = {IEEE},
Abstract = {plant recognition recently becomes more and more attractive in computer
   vision and pattern recognition. Although some researchers have proposed
   several methods, their accuracy is not satisfactory. Therefore, a novel
   method of plant recognition based on leaf image is proposed in the
   paper. Both shape and texture features are employed in the proposed
   method. Texture feature is extracted by intersecting cortical model, and
   shape feature is obtained by the representation of center distance
   sequence. Support vector machine is employed for the classifier. The
   leaf image is preprocessed to get better quality for extracting
   features, and then entropy sequence and center distance sequence are
   obtained by intersecting cortical model and center distance transform,
   respectively. Redundant data of entropy sequence vector and center
   distance are reduced by principal component analysis. Finally, feature
   vector is imported into the classifier for classification. In order to
   evaluate the performance, several existing methods are used to compare
   with the proposed method and three leaf image datasets are taken as test
   samples. The experimental result shows the proposed method gets the
   better accuracy of recognition than other methods.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Wang, ZB (Corresponding Author), Lanzhou Univ, Sch Informat Sci \& Engn, Lanzhou 730000, Peoples R China.
   Wang, Zhaobin; Sun, Xiaoguang; Ma, Yide; Zhang, Hongjuan; Ma, Yurun; Xie, Weiying, Lanzhou Univ, Sch Informat Sci \& Engn, Lanzhou 730000, Peoples R China.
   Wang, Zhaobin; Zhang, Yaonan, Chinese Acad Sci, Cold \& Arid Reg Environm \& Engn Res Inst, Lanzhou, Peoples R China.},
ISSN = {2161-4393},
ISBN = {978-1-4799-1484-5},
Keywords = {plant recognition; ICM; feature extraction; leaf image; classification},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {wangzhb@lzu.edu.cn},
Affiliations = {Lanzhou University; Chinese Academy of Sciences; Cold \& Arid Regions
   Environmental \& Engineering Research Institute, CAS},
ResearcherID-Numbers = {zhang, yuanyuan/GYA-4428-2022
   Xie, Weiying/AAP-3832-2020},
Cited-References = {{[}Anonymous], 2010, APPL PULSE COUPLED N.
   {[}Anonymous], 2007, SIGN PROC INF TECHN.
   Cerutti G, 2013, COMPUT VIS IMAGE UND, V117, P1482, DOI 10.1016/j.cviu.2013.07.003.
   Corney DPA, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0042112.
   Eckhorn R, 1990, NEURAL COMPUT, V2, P293, DOI 10.1162/neco.1990.2.3.293.
   Ekblad U, 2004, NUCL INSTRUM METH A, V525, P392, DOI 10.1016/j.nima.2004.03.102.
   Kadir Abdul, 2012, EXPT ZERNIKE MOMENTS.
   Kinser J. M., 2001, P 4 IASTED INT C COM.
   Kinser Jason M., 1996, AEROSPACE DEFENSE SE.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Ma Y, 2002, CHINESE SCI BULL, V47, P167.
   Ma Yi-de, 2007, COMM INF TECHN 2007.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Satti vijay, 2013, INT J ENG SCI TECHNO, V5.4.
   Singh Krishna, 2010, INT J SIGNAL PROCESS, V3.4.
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88.
   Uluturk Caner, 2012, INN INT SYST APPL IN.
   Valliammal N., 2012, INT J COMPUTER COMMU, V6.
   Wang Zhaobin, 2006, INF ACQ 2006 IEEE IN.
   Zhang SW, 2011, NEUROCOMPUTING, V74, P2284, DOI 10.1016/j.neucom.2011.03.007.
   Zulkifli Zalikha, 2011, HYBR INT SYST HIS 20.},
Number-of-Cited-References = {21},
Times-Cited = {18},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BE4BF},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000371465701010},
DA = {2023-08-12},
}

@inproceedings{ WOS:000367072600049,
Author = {Hsiao, Jou-Ken and Kang, Li-Wei and Chang, Ching-Long and Lin, Chih-Yang},
Book-Group-Author = {IEEE},
Title = {Comparative Study of Leaf Image Recognition with a Novel Learning-based
   Approach},
Booktitle = {2014 SCIENCE AND INFORMATION CONFERENCE (SAI)},
Year = {2014},
Pages = {389-393},
Note = {Science and Information Conference (SAI), Sci \& Informat Org, London,
   ENGLAND, AUG 27-29, 2014},
Organization = {Microsoft; RK Trans2Cloud; Springer; IEEE Comp Soc, UKRI Sect; IEEE
   Computat Intelligence Soc, UKRI Sect; IEEE},
Abstract = {Automatic plant identification via computer vision techniques has been
   greatly important for a number of professionals, such as environmental
   protectors, land managers, and foresters. In this paper, we conduct a
   comparative study on leaf image recognition and propose a novel
   learning-based leaf image recognition technique via sparse
   representation (or sparse coding) for automatic plant identification. In
   our learning-based method, in order to model leaf images, we learn an
   overcomplete dictionary for sparsely representing the training images of
   each leaf species. Each dictionary is learned using a set of descriptors
   extracted from the training images in such a way that each descriptor is
   represented by linear combination of a small number of dictionary atoms.
   Moreover, we also implement a general bag-of-words (BoW) model-based
   recognition system for leaf images, used for comparison. We
   experimentally compare the two approaches and show unique
   characteristics of our sparse coding-based framework. As a result,
   efficient leaf recognition can be achieved on public leaf image dataset
   based on the two evaluated methods, where the proposed sparse
   coding-based framework can perform better.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kang, LW (Corresponding Author), Natl Yunlin Univ Sci \& Technol, Dept Comp Sci \& Informat Engn, Yunlin, Taiwan.
   Hsiao, Jou-Ken; Kang, Li-Wei; Chang, Ching-Long, Natl Yunlin Univ Sci \& Technol, Dept Comp Sci \& Informat Engn, Yunlin, Taiwan.
   Kang, Li-Wei, Natl Yunlin Univ Sci \& Technol, Grad Sch Engn Sci \& Technol Doctoral Program, Yunlin, Taiwan.
   Lin, Chih-Yang, Asia Univ, Dept Comp Sci \& Informat Engn, Taichung, Taiwan.},
ISBN = {978-0-9893193-1-7},
Keywords = {plant identification; leaf recognition; dictionary learning;
   bag-of-words; classification},
Keywords-Plus = {SHAPE; RETRIEVAL},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications; Engineering,
   Multidisciplinary},
Author-Email = {lwkang@yuntech.edu.tw},
Affiliations = {National Yunlin University Science \& Technology; National Yunlin
   University Science \& Technology; Asia University Taiwan},
ResearcherID-Numbers = {Lin, Chih-Yang/HOF-2583-2023},
ORCID-Numbers = {Lin, Chih-Yang/0000-0002-0401-8473},
Cited-References = {Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199.
   Aranda M. C., 2010, P ACM INT C IM VID R, P327, DOI {[}10.1145/1816041.1816089, DOI 10.1145/1816041.1816089].
   Bruckstein AM, 2009, SIAM REV, V51, P34, DOI 10.1137/060657704.
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199.
   Csurka G., 2004, P ECCV INT WORK STAT.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Du JX, 2013, NEUROCOMPUTING, V116, P150, DOI 10.1016/j.neucom.2012.03.028.
   Fotopoulou F, 2013, PATTERN ANAL APPL, V16, P381, DOI 10.1007/s10044-011-0254-6.
   Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253.
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830.
   Hossain J., 2010, Proceedings of the 13th International Conference on Computer and Information Technology (ICCIT 2010), P458, DOI 10.1109/ICCITECHN.2010.5723901.
   Hsu CY, 2013, IEEE INT CON MULTI.
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759.
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057.
   Kebapci H, 2011, COMPUT J, V54, P1475, DOI 10.1093/comjnl/bxq037.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, P2169, DOI DOI 10.1109/CVPR.2006.68.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Mairal J, 2010, J MACH LEARN RES, V11, P19.
   Mouine S., 2013, P ACM INT C MULT RET.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Mouine S., 2012, P ACM INT C MULT RET.
   Mzoughi O, 2013, IEEE IMAGE PROC, P3967, DOI 10.1109/ICIP.2013.6738817.
   Mzoughi O, 2012, IEEE IMAGE PROC, P1033, DOI 10.1109/ICIP.2012.6467039.
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0.
   Wagner A, 2012, IEEE T PATTERN ANAL, V34, P372, DOI 10.1109/TPAMI.2011.112.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yahiaoui I., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P254, DOI 10.1109/ICME.2012.130.
   Yang J., 2007, P INT WORKSHOP MULTI, P197, DOI DOI 10.1145/1290082.1290111.
   Yeh C.-H., J VIS COMM IN PRESS.},
Number-of-Cited-References = {32},
Times-Cited = {19},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BE0UQ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000367072600049},
DA = {2023-08-12},
}

@article{ WOS:000438270400003,
Author = {Kala, Jules R. and Viriri, Serestina},
Title = {PLANT SPECIE CLASSIFICATION USING SINUOSITY COEFFICIENTS OF LEAVES},
Journal = {IMAGE ANALYSIS \& STEREOLOGY},
Year = {2018},
Volume = {37},
Number = {2},
Pages = {119-126},
Abstract = {Forests are the lungs of our planet. Conserving the plants may require
   the development of an automated system that will identify plants using
   leaf features such as shape, color, and texture. In this paper, a leaf
   shape descriptor based on sinuosity coefficients is proposed. The
   sinuosity coefficients are defined using the sinuosity measure, which is
   a measure expressing the degree of meandering of a curve. The initial
   empirical experiments performed on the LeafSnap dataset on the usage of
   four sinuosity coefficients to characterize the leaf images using the
   Radial Basis Function Neural Network (RBF) and Multilayer Perceptron
   (MLP) classifiers achieved accurate classification rates of 88\% and
   65\%, respectively. The proposed feature extraction technique is further
   enhanced through the addition of leaf geometrical features, and the
   accurate classification rates of 93\% and 82\% were achieved using RBF
   and MLP, respectively. The overall results achieved showed that the
   proposed feature extraction technique based on the sinuosity
   coefficients of leaves, complemented with geometrical features improve
   the accuracy rate of plant classification using leaf recognition.},
Publisher = {INT SOC STEREOLOGY},
Address = {INST ANATOMY, MEDICAL FACULTY, KORYTKOVA 2, LJUBJANA, SL-1000, SLOVENIA},
Type = {Article},
Language = {English},
Affiliation = {Kala, JR (Corresponding Author), Univ KwaZulu Natal, Sch Math Stat \& Comp Sci, ZA-4000 Durban, South Africa.
   Kala, Jules R.; Viriri, Serestina, Univ KwaZulu Natal, Sch Math Stat \& Comp Sci, ZA-4000 Durban, South Africa.},
DOI = {10.5566/ias.1821},
ISSN = {1580-3139},
Keywords = {leaf recognition; plant classification; sinuosity coefficients;
   sinuosity measure},
Keywords-Plus = {LEAF; IDENTIFICATION},
Research-Areas = {Materials Science; Mathematics; Mechanics; Imaging Science \&
   Photographic Technology},
Web-of-Science-Categories  = {Materials Science, Multidisciplinary; Mathematics, Applied; Mechanics;
   Imaging Science \& Photographic Technology},
Author-Email = {raymondkalal@gmail.com
   viriris@ukzn.ac.za},
Affiliations = {University of Kwazulu Natal},
ResearcherID-Numbers = {Viriri, Serestina/AAN-3882-2020},
ORCID-Numbers = {Viriri, Serestina/0000-0002-2850-8645},
Cited-References = {Adetiba E, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0143542.
   Aickelin U, 2003, LECT NOTES COMPUT SC, V2787, P147.
   Bendiab E., 2011, INT J DIGITAL INFORM, V1, P284.
   Cerutti Guillaume, 2013, International Conference on Computer Vision Theory and Applications (VISAPP 2013). Proceedings, P277.
   Cerutti G, 2014, PATTERN RECOGN LETT, V49, P177, DOI 10.1016/j.patrec.2014.07.016.
   Chaudhuri D, 2007, PATTERN RECOGN, V40, P1981, DOI 10.1016/j.patcog.2006.08.003.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Fu H, 2004, I C CONT AUTOMAT ROB, P681.
   Gonzalez RC., 1992, DIGITAL IMAGE PROCES.
   Jaekel M, 2007, P NATL ACAD SCI USA, V104, P20437, DOI 10.1073/pnas.0710216105.
   Kala JR, 2014, P 6 IEEE INT C AD SC, P1.
   Kala JR, 2016, ADV INTELL SYST, V419, P141, DOI 10.1007/978-3-319-27400-3\_13.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   KUHL FP, 1982, COMPUT VISION GRAPH, V18, P236, DOI 10.1016/0146-664X(82)90034-X.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Langbein WB, 1966, 422H USGS.
   Lazarus ED, 2013, P NATL ACAD SCI USA, V110, P8447, DOI 10.1073/pnas.1214074110.
   Lei YK, 2014, COMPUT VIS IMAGE UND, V119, P116, DOI 10.1016/j.cviu.2013.12.001.
   Mallah C, 2013, SIGNAL PROCESS PATTE, P279.
   Naing L, 2006, ARCH OROFAC SCI, V1, P9.
   Partington C. F., 1837, BRIT CYCLOPAEDIA NAT.
   Tanguy A, 2002, RES SPINAL DEFORMITI, V3.
   von Linne Carl, 1788, SYSTEMA NATURAE REGN, V1.
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.},
Number-of-Cited-References = {26},
Times-Cited = {5},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {3},
Journal-ISO = {Image Anal. Stereol.},
Doc-Delivery-Number = {GM6IT},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000438270400003},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000510238000002,
Author = {Kumar, Munish and Gupta, Surbhi and Gao, Xiao-Zhi and Singh, Amitoj},
Title = {Plant Species Recognition Using Morphological Features and Adaptive
   Boosting Methodology},
Journal = {IEEE ACCESS},
Year = {2019},
Volume = {7},
Pages = {163912-163918},
Abstract = {Plant species detection aims at the automatic identification of plants.
   Although a lot of aspects like leaf, flowers, fruits, seeds could
   contribute to the decision, but leaf features are the most significant.
   As a plant leaf is always more accessible as compared to other parts of
   the plants, it is obvious to study it for plant identification. The
   present paper introduced a novel plant species classifier based on the
   extraction of morphological features using a Multilayer Perceptron with
   Adaboosting. The proposed framework comprises pre-processing, feature
   extraction, feature selection, and classification. Initially, some
   pre-processing techniques are used to set up a leaf image for the
   feature extraction process. Various morphological features, i.e.,
   centroid, major axis length, minor axis length, solidity, perimeter, and
   orientation are extracted from the digital images of various categories
   of leaves. Different classifiers, i.e., k-NN, Decision Tree and
   Multilayer perceptron are employed to test the accuracy of the
   algorithm. AdaBoost methodology is explored for improving the precision
   rate of the proposed system. Experimental results are obtained on a
   public dataset (FLAVIA) downloaded from http://flavia.sourceforge.net/.
   A precision rate of 95.42\% has been achieved using the proposed machine
   learning classifier, which outperformed the state-of-the-art algorithms.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Kumar, M (Corresponding Author), Maharaja Ranjit Singh Punjab Tech Univ, Dept Computat Sci, Bathinda 151001, India.
   Kumar, Munish; Singh, Amitoj, Maharaja Ranjit Singh Punjab Tech Univ, Dept Computat Sci, Bathinda 151001, India.
   Gupta, Surbhi, Gokaraju Rangaraju Inst Engn \& Technol, Dept Comp Sci \& Engn, Hyderabad 500090, India.
   Gao, Xiao-Zhi, Univ Eastern Finland, Sch Comp, Kuopio 70211, Finland.},
DOI = {10.1109/ACCESS.2019.2952176},
ISSN = {2169-3536},
Keywords = {Leaf recognition; feature extraction; k-NN; decision tree; multilayer
   perceptron; plant leaf classification; plant species identification;
   AdaBoost},
Keywords-Plus = {SHAPE; TEXTURE; LEAVES; COLOR},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {munishcse@gmail.com},
Affiliations = {Gokaraju Rangaraju Institute of Engineering \& Technology; University of
   Eastern Finland},
ResearcherID-Numbers = {Kumar, Munish/P-7756-2018
   Gupta, Surbhi/AAS-3570-2020
   gupta, surbhi/AAY-7465-2020
   Gao, Xiao-Zhi/Q-3038-2016},
ORCID-Numbers = {Kumar, Munish/0000-0003-0115-1620
   Gupta, Surbhi/0000-0003-0618-8369
   Singh, amitoj/0000-0002-5884-3145
   Gao, Xiao-Zhi/0000-0002-0078-5675},
Cited-References = {Abbasi S, 1997, LECT NOTES COMPUT SC, V1252, P284.
   Ananthi C., 2014, J NANOSCI NANOTECHNO, V2, P214.
   {[}Anonymous], 2010, INT J COMP APPL.
   {[}Anonymous], 2014, THE J.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Bama B. S., 2011, IND J COMP SCI ENG, V2, P202.
   Bong M. F., 2013, IJCSI INT J COMPUTER, V10, P477.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Cholhong Im, 1998, Proceedings of IAPR Workshop on Machine Vision Applications, P158.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Dell'Aquila A., 2004, Italian Journal of Agronomy, V8, P51.
   Ekshinge S, 2014, ASIAN J ENG TECHNOL, V10, P16.
   Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771.
   Hossain J., 2010, Proceedings of the 13th International Conference on Computer and Information Technology (ICCIT 2010), P458, DOI 10.1109/ICCITECHN.2010.5723901.
   Kadir A., 2014, INT J ADV SCI TECHNO, V44, P113.
   Kebapci H, 2011, COMPUT J, V54, P1475, DOI 10.1093/comjnl/bxq037.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Kumar S., 2012, INDIAN J COMPUTER SC, V3, P436.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Maheshwari C.V., 2013, INT J INNOVAT RES CO, V1, P1107.
   Man QK, 2008, COMM COM INF SC, V15, P192.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Olsen A., 2015, DIG IM COMP TECHN AP, P1.
   Perez AJ, 2000, COMPUT ELECTRON AGR, V25, P197, DOI 10.1016/S0168-1699(99)00068-X.
   Pietikainen M., 2002, P 2 INT WORKSH TEXT, P109.
   Prasvita D.S., 2013, INT J ADV SCI ENG IN, V3, P5, DOI {[}10.18517/ijaseit.3.2.287, DOI 10.18517/IJASEIT.3.2.287].
   Timmermans AJM, 1996, COMPUT ELECTRON AGR, V15, P41, DOI 10.1016/0168-1699(95)00056-9.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yadav B.K., 2011, COMPUT ELECTRON AGR, V33, P19.
   Yang CC, 2000, CAN AGR ENG, V42, P195.
   Zhang H, 2011, REPROD BIOL, V11, P99, DOI 10.1016/S1642-431X(12)60048-5.
   Zhang SW, 2017, CLUSTER COMPUT, V20, P1517, DOI 10.1007/s10586-017-0859-7.
   Zulkifli Z., 2009, THESIS.},
Number-of-Cited-References = {34},
Times-Cited = {32},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {6},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {KG8YS},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000510238000002},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000615938700006,
Author = {Yang, Chengzhuan},
Title = {Plant leaf recognition by integrating shape and texture features},
Journal = {PATTERN RECOGNITION},
Year = {2021},
Volume = {112},
Month = {APR},
Abstract = {Plant leaf identification is a significant challenge in the fields of
   computer vision and pattern recognition. This article presents a new
   approach to plant leaf identification, one that integrates shape and
   texture characteristics. First, we introduce the shape and texture
   features used by the proposed plant leaf recognition method. The
   proposed multiscale triangle descriptor (MTD) is employed to
   characterize the shape information of a plant leaf, and the local binary
   pattern histogram Fourier (LBP-HF) is used as the texture feature. Then,
   the shape and texture features of a leaf image are combined by weighted
   distance measurement, where L 1 distance and chi-square distance are
   used for shape and texture features, respectively. The proposed approach
   provides a robust descriptor for the task of plant leaf recognition by
   combining the complementary MTD and LBP-HF features. The proposed
   approach has been thoroughly evaluated on three benchmark leaf datasets,
   including the Flavia, Swedish and MEW2012 leaf datasets. Our method
   achieves 77.6\%, 85.7\%, and 67.5\% retrieval accuracy on the Flavia,
   Swedish and MEW2012 leaf datasets, respectively, while the corresponding
   classification accuracy is 99.1\%, 98.4\%, 95.6\%. The recognition
   performance of our method is better or comparable to prior
   state-of-the-art plant leaf recognition method. (c) 2021 Elsevier Ltd.
   All rights reserved.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Yang, CZ (Corresponding Author), Zhejiang Normal Univ, Sch Math \& Comp Sci, 688 Yingbin Rd, Jinhua 321004, Zhejiang, Peoples R China.
   Yang, Chengzhuan, Zhejiang Normal Univ, Sch Math \& Comp Sci, 688 Yingbin Rd, Jinhua 321004, Zhejiang, Peoples R China.},
DOI = {10.1016/j.patcog.2020.107809},
EarlyAccessDate = {JAN 2021},
Article-Number = {107809},
ISSN = {0031-3203},
EISSN = {1873-5142},
Keywords = {Plant leaf recognition; Multiscale triangle descriptor; Local binary
   pattern; Shape descriptor; Texture feature},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic},
Author-Email = {chengzhuan\_yang@163.com},
Affiliations = {Zhejiang Normal University},
ResearcherID-Numbers = {Yang, Chengzhuan/GXF-1310-2022},
Funding-Acknowledgement = {Zhejiang Provincial Natural Science Foundation of China {[}LQ19F0200 03,
   LY18F020023]; National Natural Science Foundation of China {[}61375122];
   Shanghai Science and Technology Development Funds {[}13dz2260200,
   13511504300]},
Funding-Text = {We would like to thank the anonymous reviewers for their helpful
   suggestions. This work was supported by Zhejiang Provincial Natural
   Science Foundation of China (Project nos. LQ19F0200 03, LY18F020023) and
   the National Natural Science Foundation of China (Project no. 61375122),
   (in part) by Shanghai Science and Technology Development Funds (Project
   nos. 13dz2260200, 13511504300). We also thank LetPub
   (http://www.letpub.com) for its linguistic assistance during the
   preparation of this manuscript.},
Cited-References = {Ahonen T, 2009, LECT NOTES COMPUT SC, V5575, P61, DOI 10.1007/978-3-642-02230-2\_7.
   Alajlan N, 2007, PATTERN RECOGN, V40, P1911, DOI 10.1016/j.patcog.2006.12.005.
   Alamoudi S, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207386.
   Backes AR, 2009, LECT NOTES COMPUT SC, V5716, P143, DOI 10.1007/978-3-642-04146-4\_17.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Han J., 2005, DATA MINING CONCEPTS.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Kaplan C, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107468.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Li ZY, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2020.107371.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Lukic M, 2017, 2017 IEEE 15TH INTERNATIONAL SYMPOSIUM ON APPLIED MACHINE INTELLIGENCE AND INFORMATICS (SAMI), P485, DOI 10.1109/SAMI.2017.7880358.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Olsen A, 2016, INT C DIG IM COMP TE, P1.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Pawara P, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107528.
   Rashad M. Z., 2011, International Journal of Computer Science \& Information Technology, V3, P93, DOI 10.5121/ijcsit.2011.3407.
   Scotland RW, 2003, TAXON, V52, P101, DOI 10.2307/3647306.
   Shah MP, 2017, IEEE IMAGE PROC, P860, DOI 10.1109/ICIP.2017.8296403.
   Soderkvist O.J.O., 2001, SE581, V83.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Tsolakidis DG, 2014, LECT NOTES ARTIF INT, V8445, P406, DOI 10.1007/978-3-319-07064-3\_33.
   Wang B, 2017, P IEEE C COMP VIS PA, P6119.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xu YX, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107558.
   Yang CZ, 2019, SIGNAL PROCESS-IMAGE, V71, P110, DOI 10.1016/j.image.2018.11.004.
   Yang CZ, 2016, FRONT ARTIF INTEL AP, V285, P269, DOI 10.3233/978-1-61499-672-9-269.
   Zhang X, 2019, MULTIMED TOOLS APPL, V78, P27463, DOI 10.1007/s11042-019-07846-0.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhao ZQ, 2015, LECT NOTES COMPUT SC, V9004, P348, DOI 10.1007/978-3-319-16808-1\_24.
   Zhiyu Liu, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9226, P115, DOI 10.1007/978-3-319-22186-1\_11.
   Zisserman A, 2015, ICLR 2015 INT C LEAR, P1097.},
Number-of-Cited-References = {40},
Times-Cited = {31},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {66},
Journal-ISO = {Pattern Recognit.},
Doc-Delivery-Number = {QE1AL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000615938700006},
DA = {2023-08-12},
}

@article{ WOS:000419413200010,
Author = {Zeng, Shaoning and Zhang, Bob and Du, Yong},
Title = {Joint distances by sparse representation and locality-constrained
   dictionary learning for robust leaf recognition},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2017},
Volume = {142},
Number = {B},
Pages = {563-571},
Month = {NOV},
Abstract = {Plant species recognition has been a difficult and important task in
   agriculture, where computer techniques like image processing and pattern
   recognition can commendably facilitate plant recognition based on leaf
   images. The locality-constrained models produced by sparse
   representation and dictionary learning are a few of the prevailing
   feature models for leaf image recognition. Previous studies demonstrated
   that sparsity in representation plays an important role in the
   recognition, while sparsity constraints are the keys to solve the
   dictionary learning problems. Many of them focused on improving the
   sparsity, which is hard, but using large atoms in dictionary learning
   for high accuracy consumed more training time. Actually, sparse
   representation and dictionary learning are both based on distance
   calculation, e.g., Euclidean distance, which is also an aspect possible
   to obtain an improvement. On the premise of unchanged sparsity, this
   paper proposed a novel distance based method fusing Sparse
   Representation and Locality-Constrained Dictionary Learning (SRLC-DL)
   for robust leaf recognition. Integrating the distances obtained by
   dictionary learning and naive sparse representation can generate robust
   and high performance leaf recognition. In the fusion of distances, the
   number of atoms was not necessarily large as conventional methods, and
   even using smaller atoms produced more promising recognition at times.
   Therefore, not only has the leaf recognition accuracy by sparse
   representation been advanced, but the recognition speed also remains
   fast enough. A series of experiments had been conducted on five
   benchmark leaf datasets, including Caltech Leaves, Leaf, Herbarium,
   Swedish Leaf and Flavia. The experimental results demonstrated that
   SRLC-DL produced a higher accuracy in leaf image recognition and
   outperformed many other state-of-the-art methods.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Zhang, B (Corresponding Author), Univ Macau, Dept Comp \& Informat Sci, Ave Univ, Taipa, Macau, Peoples R China.
   Zeng, Shaoning; Zhang, Bob, Univ Macau, Dept Comp \& Informat Sci, Ave Univ, Taipa, Macau, Peoples R China.
   Zeng, Shaoning, Huizhou Univ, Sch Informat Sci \& Technol, Huizhou, Guangdong, Peoples R China.
   Du, Yong, Northeast Agr Univ, Dept Elect \& Informat Engn, Harbin, Heilongjiang, Peoples R China.
   Du, Yong, Tianjin Univ, Dept Comp Sci \& Technol, Tianjin, Peoples R China.},
DOI = {10.1016/j.compag.2017.11.013},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Sparse representation; Dictionary learning; Feature integration; Leaf
   image recognition},
Keywords-Plus = {COLLABORATIVE REPRESENTATION; FACE RECOGNITION; FUSION},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {zsn@outlook.com
   bobzhang@umac.mo
   duyonghit@126.com},
Affiliations = {University of Macau; Huizhou University; Northeast Agricultural
   University - China; Tianjin University},
ResearcherID-Numbers = {Zhang, Bob/ABD-5926-2021
   Zhang, Bob/HIR-3656-2022
   Peng, Xi/B-9002-2012},
ORCID-Numbers = {Zhang, Bob/0000-0003-2497-9519
   Zhang, Bob/0000-0001-6512-0474
   Peng, Xi/0000-0002-5727-2790},
Funding-Acknowledgement = {Natural Science Foundation of Jiangsu Province of China {[}BK20150522];
   China Postdoctoral Science Foundation {[}2014M551024]; Foundation of
   Education Department of Heilongjiang Province {[}12541050]; Science and
   Technology Program of Huizhou City {[}2016X0422037]; Science and
   Technology Development Fund (FDCT) of Macao SAR {[}124/2014/A3];
   National Natural Science Foundation of China (NSFC) {[}51308096,
   61502208, 61602540]},
Funding-Text = {This work was supported in part by National Natural Science Foundation
   of China (NSFC) (Grant No. 51308096, 61502208, 61602540), Foundation of
   Education Department of Heilongjiang Province (Grant No. 12541050),
   China Postdoctoral Science Foundation (Grant No. 2014M551024), Natural
   Science Foundation of Jiangsu Province of China (Grant No. BK20150522),
   Science and Technology Development Fund (FDCT) of Macao SAR (Grant No.
   124/2014/A3) and Science and Technology Program of Huizhou City (Grant
   No. 2016X0422037).},
Cited-References = {Akhtar N, 2017, PATTERN RECOGN, V65, P136, DOI 10.1016/j.patcog.2016.12.017.
   {[}Anonymous], EVALUATION FEATURES.
   Bao CL, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487966.
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542.
   Cai S, 2016, ADV DIFFER EQU, V2016, P1.
   Cheng H, 2013, SIGNAL PROCESS, V93, P1408, DOI 10.1016/j.sigpro.2012.09.011.
   Donoho DL, 2012, IEEE T INFORM THEORY, V58, P1094, DOI 10.1109/TIT.2011.2173241.
   Fan ZZ, 2014, IEEE T NEUR NET LEAR, V25, P1538, DOI 10.1109/TNNLS.2013.2294492.
   Fei LK, 2016, NEUROCOMPUTING, V218, P264, DOI 10.1016/j.neucom.2016.08.048.
   Gandhi A., 2002, HERBARIUM LEAVES DAT.
   Goncalves H, 2014, IEEE IMAGE PROC, P4907, DOI 10.1109/ICIP.2014.7025994.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hsiao J. K., 2015, LEARNING BASED LEAF.
   Jiang ZL, 2011, PROC CVPR IEEE, P1697, DOI 10.1109/CVPR.2011.5995354.
   Kim SJ, 2007, IEEE J-STSP, V1, P606, DOI 10.1109/JSTSP.2007.910971.
   Kreutz-Delgado K, 2003, NEURAL COMPUT, V15, P349, DOI 10.1162/089976603762552951.
   Li ZM, 2017, IEEE T NEUR NET LEAR, V28, P278, DOI 10.1109/TNNLS.2015.2508025.
   Liu NA, 2016, NEUROCOMPUTING, V216, P460, DOI 10.1016/j.neucom.2016.08.005.
   Ma L, 2012, PROC CVPR IEEE, P2586, DOI 10.1109/CVPR.2012.6247977.
   Matiz S, 2016, IEEE IMAGE PROC, P1888, DOI 10.1109/ICIP.2016.7532686.
   Patel VM, 2012, IEEE T INF FOREN SEC, V7, P954, DOI 10.1109/TIFS.2012.2189205.
   Peng X, 2017, IEEE T CYBERNETICS, V47, P1053, DOI 10.1109/TCYB.2016.2536752.
   Peng X, 2014, PATTERN RECOGN, V47, P2794, DOI 10.1016/j.patcog.2014.03.013.
   Quan YH, 2016, PATTERN RECOGN, V55, P247, DOI 10.1016/j.patcog.2016.01.028.
   Rubinstein R., 2008, CS TECHNION, V40.
   Rubinstein R, 2013, IEEE T SIGNAL PROCES, V61, DOI 10.1109/TSP.2012.2226445.
   Sderkvist O., 2010, COMPUTER VISION CLAS.
   Silva P. F. B., 2013, AUSTRALAS EMERG NURS, V11, P203.
   Skretting K, 2010, IEEE T SIGNAL PROCES, V58, P2121, DOI 10.1109/TSP.2010.2040671.
   Wang QC, 2016, 2016 12TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (ICNC-FSKD), P1744, DOI 10.1109/FSKD.2016.7603441.
   Weber M., 1999, CALTECH LEAVES 1999.
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xu Y., 2012, INT J INNOV COMPUT I, V8, P1349.
   Xu Y, 2017, IEEE T NEUR NET LEAR, V28, P2233, DOI 10.1109/TNNLS.2016.2580572.
   Xu Y, 2017, INFORM SCIENCES, V375, P171, DOI 10.1016/j.ins.2016.09.059.
   Xu Y, 2014, IEEE T CYBERNETICS, V44, P1738, DOI 10.1109/TCYB.2013.2293391.
   Xu Y, 2013, NEURAL COMPUT APPL, V23, P1251, DOI 10.1007/s00521-012-1066-3.
   Xu Y, 2013, PATTERN RECOGN LETT, V34, P980, DOI 10.1016/j.patrec.2013.01.028.
   Yang AY, 2013, IEEE T IMAGE PROCESS, V22, P3234, DOI 10.1109/TIP.2013.2262292.
   Yang M, 2014, PROC CVPR IEEE, P4138, DOI 10.1109/CVPR.2014.527.
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286.
   Zeng S., 2017, OPTIK INT J LIGHT EL.
   Zeng SN, 2017, MULTIMED TOOLS APPL, V76, P20889, DOI 10.1007/s11042-016-4035-5.
   Zeng SN, 2016, CAAI T INTELL TECHNO, V1, P189, DOI 10.1016/j.trit.2016.09.002.
   Zeng SN, 2018, NEURAL COMPUT APPL, V30, P2965, DOI 10.1007/s00521-017-2900-4.
   Zeng SN, 2017, EXPERT SYST APPL, V82, P1, DOI 10.1016/j.eswa.2017.04.001.
   Zhang BC, 2016, NEUROCOMPUTING, V171, P387, DOI 10.1016/j.neucom.2015.06.052.
   Zhang HZ, 2016, EXPERT SYST APPL, V45, P352, DOI 10.1016/j.eswa.2015.09.058.
   Zhang L, 2012, COLLABORATIVE REPRES.
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277.
   Zhang S., 2017, 2 STAGE PLANT SPECIE.
   Zhang SW, 2017, COMPUT ELECTRON AGR, V134, P135, DOI 10.1016/j.compag.2017.01.014.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhu HY, 2017, MULTIMED TOOLS APPL, V76, P4599, DOI 10.1007/s11042-016-3538-4.
   Zhu Q, 2013, NEURAL COMPUT APPL, V23, P169, DOI 10.1007/s00521-012-0851-3.},
Number-of-Cited-References = {56},
Times-Cited = {12},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {30},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {FR9SK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000419413200010},
DA = {2023-08-12},
}

@article{ WOS:000615177300001,
Author = {Chung, Yi and Chou, Chih-Ang and Li, Chih-Yang},
Title = {Central Attention and a Dual Path Convolutional Neural Network in
   Real-World Tree Species Recognition},
Journal = {INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH},
Year = {2021},
Volume = {18},
Number = {3},
Month = {FEB},
Abstract = {Identifying plants is not only the job of professionals, but also useful
   or essential for the plant lover and the general public. Although deep
   learning approaches for plant recognition are promising, driven by the
   success of convolutional neural networks (CNN), their performances are
   still far from the requirements of an in-field scenario. First, we
   propose a central attention concept that helps focus on the target
   instead of backgrounds in the image for tree species recognition. It
   could prevent model training from confused vision by establishing a dual
   path CNN deep learning framework, in which the central attention model
   combined with the CNN model based on InceptionV3 were employed to
   automatically extract the features. These two models were then learned
   together with a shared classification layer. Experimental results
   assessed the effectiveness of our proposed approach which outperformed
   each uni-path alone, and existing methods in the whole plant recognition
   system. Additionally, we created our own tree image database where each
   photo contained a wealth of information on the entire tree instead of an
   individual plant organ. Lastly, we developed a prototype system of an
   online/offline available tree species identification working on a
   consumer mobile platform that can identify the tree species not only by
   image recognition, but also detection and classification in real-time
   remotely.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Chung, Y (Corresponding Author), Natl Taipei Univ Nursing \& Hlth Sci, Coll Human Dev \& Hlth, Taipei 11219, Taiwan.
   Chung, Yi, Natl Taipei Univ Nursing \& Hlth Sci, Coll Human Dev \& Hlth, Taipei 11219, Taiwan.
   Chou, Chih-Ang, Xin Ji Int Co, New Taipei 234014, Taiwan.
   Li, Chih-Yang, Natl Taiwan Univ, Dept Comp Sci \& Informat Engn, Taipei 10617, Taiwan.},
DOI = {10.3390/ijerph18030961},
Article-Number = {961},
EISSN = {1660-4601},
Keywords = {plant recognition; deep learning; dual path convolutional neural
   network; visual attention; mobile application},
Keywords-Plus = {IDENTIFICATION; SEGMENTATION; IMAGES; SYSTEM; MODEL; RGB},
Research-Areas = {Environmental Sciences \& Ecology; Public, Environmental \& Occupational
   Health},
Web-of-Science-Categories  = {Environmental Sciences; Public, Environmental \& Occupational Health},
Author-Email = {m9306009@gmail.com
   catchsob@gmail.com
   taipingeric@gmail.com},
Affiliations = {National Taipei University of Nursing \& Health Science (NTUNHS);
   National Taiwan University},
ORCID-Numbers = {Chung, Yi/0000-0001-7733-6459},
Funding-Acknowledgement = {National Taipei University of Nursing and Health Sciences
   {[}110ntunhs-NT-01]},
Funding-Text = {This research was funded by National Taipei University of Nursing and
   Health Sciences (110ntunhs-NT-01).},
Cited-References = {Abadi M., 2016, TENSORFLOW LARGE SCA.
   Affouard A., 2017, P ICLR INT C LEARN R.
   Agarwal G, 2006, TAXON, V55, P597, DOI 10.2307/25065637.
   Al-Qurran R, 2018, INT ARAB CONF INF TE, P148.
   Altintas I., ARXIV18090624.
   Anh H.N., TRAINING DETECTING O.
   {[}Anonymous], 1860, Br Foreign Med Chir Rev, V25, P367.
   Austen GE, 2016, SCI REP-UK, V6, DOI 10.1038/srep33634.
   Azlah MAF, 2019, COMPUTERS, V8, DOI 10.3390/computers8040077.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Bertrand S, 2018, ECOL INFORM, V46, P57, DOI 10.1016/j.ecoinf.2018.05.007.
   Bodhwani Vinit, 2019, Procedia Computer Science, V152, P186, DOI 10.1016/j.procs.2019.05.042.
   Bugwood, SE EARLY DETECTION N.
   Carranza-Rojas Jose, 2016, CLEIej, V19, P7.
   Ceballos G, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1400253.
   Cerutti G, 2013, COMPUT VIS IMAGE UND, V117, P1482, DOI 10.1016/j.cviu.2013.07.003.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Chen CY, 2018, ASIAPAC SIGN INFO PR, P1995, DOI 10.23919/APSIPA.2018.8659562.
   Chen JS, 2016, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2016.61.
   Chien SC, 2013, HOLZFORSCHUNG, V67, P345, DOI 10.1515/hf-2012-0086.
   Chong K., 2018, P 16 AUSTR SEC MAN C, P41.
   Chopra M., 2015, TREEID IMAGE RECOGNI.
   Columbia University University of Maryland Smithsonian Institution, LEAFSNAP.
   Crocker E, 2020, PLANTS PEOPLE PLANET, V2, P47, DOI 10.1002/ppp3.41.
   Elphick CS, 2008, J APPL ECOL, V45, P1313, DOI 10.1111/j.1365-2664.2008.01545.x.
   Feitoza Marcondes Coelho, 2019, P 15 BRAZ S INF SYST, P1.
   Gao M, 2017, P IEEE INT C E-SCI, P29, DOI 10.1109/eScience.2017.15.
   Gee S., INATURALIST LAUNCHES.
   Glority Software Ltd, XINGSE.
   Goeau H., 2014, CLEF2014 WORKING NOT, P598.
   Goeau H., 2019, CLEF 2019 C LABS EVA.
   Goeau H., 2013, PL NTNET MOBILE APP, P423, DOI {[}10.1145/2502081.2502251, DOI 10.1145/2502081.2502251].
   Griffin G., 2007, CALTECH 256 OBJECT C.
   Ha Q, 2017, IEEE INT C INT ROBOT, P5108, DOI 10.1109/IROS.2017.8206396.
   He GQ, 2018, J PHYS CONF SER, V1004, DOI 10.1088/1742-6596/1004/1/012015.
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0\_38.
   He X, 2020, IEEE T GEOSCI REMOTE, V58, P3246, DOI 10.1109/TGRS.2019.2951445.
   Hopkins GW, 2002, ANIM CONSERV, V5, P245, DOI 10.1017/S1367943002002299.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710.
   Joly A., 2014, EMR ICMR, P7.
   Kesifler Dunyasi LTD, DISCOVERY GREEN LAB.
   Kosbar K, 2018, INT TELEMETER C PROC, P54.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lasseck Mario, 2017, CLEF.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Levesquea C, 2016, J FOREST, V114, P170.
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147.
   Li RH, 2018, IEEE T AUTOM SCI ENG, V15, P651, DOI 10.1109/TASE.2017.2664920.
   Li X, 2017, IEEE J-STARS, V10, P2022, DOI 10.1109/JSTARS.2016.2646138.
   Liu N, 2015, PROC CVPR IEEE, P362, DOI 10.1109/CVPR.2015.7298633.
   Lu J.C., 2000, TAIWAN TREE COMMENTA.
   Ma LH, 2013, LECT NOTES COMPUT SC, V7995, P106, DOI 10.1007/978-3-642-39479-9\_13.
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467.
   Medela Alfonso, 2020, J Pathol Inform, V11, P38, DOI 10.4103/jpi.jpi\_41\_20.
   Mignotte N., 2018, WILDSNAP MOBILE ANDR.
   Mishra P. K., 2012, 2012 International Conference on Advances in Engineering, Science and Management (ICAESM), P68.
   Muller H., 2018, OVERVIEW LIFECLEF 20, P247.
   Pan J, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19143111.
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71.
   Parks and Street Lights Office Public Works Department, STREET TREES INFORM.
   Pearline SA, 2019, J INTELL FUZZY SYST, V36, P1997, DOI 10.3233/JIFS-169911.
   Prasad S, 2013, 2013 IEEE SECOND INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P405, DOI 10.1109/ICIIP.2013.6707624.
   Priyankara HAC, 2015, 2015 MORATUWA ENGINEERING RESEARCH CONFERENCE (MERCON), P148, DOI 10.1109/MERCon.2015.7112336.
   Qi YK, 2019, IEEE T PATTERN ANAL, V41, P1116, DOI 10.1109/TPAMI.2018.2828817.
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606.
   qqwweee, KER IMPL YOLOV3 TENS.
   Nguyen QK, 2013, PROC INT CONF ADV, P404, DOI 10.1109/ATC.2013.6698145.
   Ralls E., 2018, Google Patents, Patent No. {[}2020056148A1, 2020056148].
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767.
   Reyes A.K., 2015, C LABS EV FOR SEPT, V1391, P9.
   Richoz S, 2019, UBICOMP/ISWC'19 ADJUNCT: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2019 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P803, DOI 10.1145/3341162.3344849.
   Rizk S., 2019, THESIS NOTRE DAME U.
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI {[}10.1109/ICCV.2017.74, 10.1007/s11263-019-01228-7].
   Seon-Jong Kim, 2011, SICE 2011 - 50th Annual Conference of the Society of Instrument and Control Engineers of Japan, P1147.
   Shah MP, 2017, IEEE IMAGE PROC, P860, DOI 10.1109/ICIP.2017.8296403.
   Shao K.T., CATALOGUE LIFE TAIWA.
   Soderkvist O., 2001, THESIS.
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655.
   SONG YN, 1994, J NAT PROD, V57, P1670, DOI 10.1021/np50114a008.
   Su Y., ARXIV14094127.
   Sulc M., 2018, CLEF WORKING NOTES.
   Sulc M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0265-4.
   Sun J, 2013, INT J COMPUT VISION, V104, P135, DOI 10.1007/s11263-013-0618-z.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Sun YX, 2021, IEEE T AUTOM SCI ENG, V18, P1000, DOI 10.1109/TASE.2020.2993143.
   Sun YX, 2019, IEEE ROBOT AUTOM LET, V4, P2576, DOI 10.1109/LRA.2019.2904733.
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   TensorFlow, MODEL OPTIMIZATION.
   TensorFlow, MOD OPT.
   Toma A., 2017, P CLEF WORK NOT DUB.
   Bao TQ, 2020, J INFORM TELECOMMUN, V4, P140, DOI 10.1080/24751839.2019.1666625.
   Tzutalin, LAB GIT COD.
   Uddin B., 2018, THESIS DAFFOD INT U.
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914.
   Vasu B., RE DEEP LEARNING TRA.
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang B, 2013, IEEE IMAGE PROC, P4417, DOI 10.1109/ICIP.2013.6738910.
   Wang HL, 2019, IEEE ROBOT AUTOM LET, V4, P4386, DOI 10.1109/LRA.2019.2932874.
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005.
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784.
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013.
   {[}翁杨 Weng Yang], 2019, {[}中国科学. 生命科学, Scientia Sinica Vitae], V49, P698.
   Wick C., 2017, ARXIV.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Wu YH, 2016, LECT NOTES COMPUT SC, V9771, P12, DOI 10.1007/978-3-319-42291-6\_2.
   Xiao D, 2019, IEEE ACCESS, V7, P123757, DOI 10.1109/ACCESS.2019.2928603.
   Xiao QG, 2018, ECOL INFORM, V48, P117, DOI 10.1016/j.ecoinf.2018.09.001.
   Zhao ZQ, 2015, NEUROCOMPUTING, V151, P1112, DOI 10.1016/j.neucom.2014.02.077.
   Zhu HY, 2018, MULTIMED TOOLS APPL, V77, P29779, DOI 10.1007/s11042-017-5578-9.},
Number-of-Cited-References = {117},
Times-Cited = {5},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {16},
Journal-ISO = {Int. J. Environ. Res. Public Health},
Doc-Delivery-Number = {QC9VF},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000615177300001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000426990900041,
Author = {Gogul, I. and Kumar, V. Sathiesh},
Book-Group-Author = {IEEE},
Title = {Flower Species Recognition System using Convolution Neural Networks and
   Transfer Learning},
Booktitle = {2017 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION
   AND NETWORKING (ICSCN)},
Year = {2017},
Note = {4th International Conference on Signal Processing, Communication and
   Networking (ICSCN), Chennai, INDIA, MAR 16-18, 2017},
Abstract = {Automatic identification and recognition of medicinal plant species in
   environments such as forests, mountains and dense regions is necessary
   to know about their existence. In recent years, plant species
   recognition is carried out based on the shape, geometry and texture of
   various plant parts such as leaves, stem, flowers etc. Flower based
   plant species identification systems are widely used. While modern
   search engines provide methods to visually search for a query image that
   contains a flower, it lacks in robustness because of the intra-class
   variation among millions of flower species around the world. Hence in
   this proposed research work, a Deep learning approach using
   Convolutional Neural Networks (CNN) is used to recognize flower species
   with high accuracy. Images of the plant species are acquired using the
   built-in camera module of a mobile phone. Feature extraction of flower
   images is performed using a Transfer Learning approach (i.e. extraction
   of complex features from a pre-trained network). A machine learning
   classifier such as Logistic Regression or Random Forest is used on top
   of it to yield a higher accuracy rate. This approach helps in minimizing
   the hardware requirement needed to perform the computationally intensive
   task of training a CNN. It is observed that, CNN combined with Transfer
   Learning approach as feature extractor outperforms all the handcrafted
   feature extraction methods such as Local Binary Pattern (LBP), Color
   Channel Statistics, Color Histograms, Haralick Texture, Hu Moments and
   Zernike Moments. CNN combined with Transfer Learning approach yields
   impressive Rank-1 accuracies of 73.05\%, 93.41\% and 90.60\% using
   OverFeat, Inception-v3 and Xception architectures, respectively as
   Feature Extractors on FLOWERS102 dataset.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Gogul, I (Corresponding Author), Anna Univ, Madras Inst Technol, Dept Elect Engn, Madras 600044, Tamil Nadu, India.
   Gogul, I.; Kumar, V. Sathiesh, Anna Univ, Madras Inst Technol, Dept Elect Engn, Madras 600044, Tamil Nadu, India.},
ISBN = {978-1-5090-4740-6},
Keywords = {Deep Learning; Artificial Intelligence; Convolutional Neural Networks;
   Transfer Learning; Flower Recognition},
Research-Areas = {Engineering; Optics},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Optics},
Author-Email = {gogulilangoswami@gmail.com},
Affiliations = {Anna University; Anna University Chennai; Madras Institute of Technology},
ResearcherID-Numbers = {Kumar, Sathiesh/AAW-8609-2020},
ORCID-Numbers = {Kumar, Sathiesh/0000-0002-0269-2333},
Cited-References = {Barthelemy D, 2009, 13 WORLD FOR C.
   Cerutti G., 2012, C LABS EV FOR.
   Chollet F., 2016, ARXIV161002357CSCV.
   Du J. X., 2007, APPL MATH COMPUTATIO, V185.
   Eigen D., 2014, 2 INT C LEARN REPR.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Nam Y, 2005, LECT NOTES COMPUT SC, V3815, P139.
   Nilsback, 2006, COMP VIS PATT REC IE, V2.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   SAITOH T, 2000, P IEEE INT C PATT RE, V2, P507, DOI DOI 10.1109/ICPR.2000.906123.
   Szegedy C., 2015, P IEEE C COMP VIS PA, P1.
   Yusof Shahrul Azmi Mohd, 2010, COMP INT MOD SIM CIM.},
Number-of-Cited-References = {14},
Times-Cited = {36},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BJ7AX},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000426990900041},
DA = {2023-08-12},
}

@article{ WOS:000315255500015,
Author = {Gwo, Chih-Ying and Wei, Chia-Hung and Li, Yue},
Title = {Rotary matching of edge features for leaf recognition},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2013},
Volume = {91},
Pages = {124-134},
Month = {FEB},
Abstract = {With advances in cloud computing technology, handheld computers and
   smartphones can now perform plant recognition by taking a photograph of
   a plant. This study proposes novel features to describe leaf edge
   variation. The Bayes theorem is used to calculate the maximal matching
   score for rotary matching. The Viterbi training algorithm is then
   applied to find the model parameters of rotary matching. The
   experimental results show that the top one of 13-tuple reaches 94.4\%
   and the first two can also achieve 100\% in the test set. The results
   have verified that the proposed features are invariant to translation,
   rotation and size. (C) 2012 Elsevier B.V. All rights reserved.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Wei, CH (Corresponding Author), Chien Hsin Univ Sci \& Technol, Dept Informat Management, 229 Chien Hsin Rd, Tao Yuan 320, Taiwan.
   Gwo, Chih-Ying; Wei, Chia-Hung, Chien Hsin Univ Sci \& Technol, Dept Informat Management, Tao Yuan 320, Taiwan.
   Li, Yue, Nankai Univ, Coll Software, Tianjin 300071, Peoples R China.},
DOI = {10.1016/j.compag.2012.12.005},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Leaf recognition; Bayes theorem; Pattern recognition; Viterbi training
   algorithm},
Keywords-Plus = {SHAPE; CLASSIFICATION},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {ericgwo@uch.edu.tw
   rogerwei@uch.edu.tw
   Liyue80@nankai.edu.cn},
Affiliations = {Chien Hsin University of Science \& Technology; Nankai University},
Cited-References = {{[}Anonymous], 2000, WORLD PAT INF, DOI DOI 10.1016/S0172-2190(00)00083-1.
   Camargo A, 2009, COMPUT ELECTRON AGR, V66, P121, DOI 10.1016/j.compag.2009.01.003.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   EHSANIRAD A., 2010, INT J COMPUTER SCI I, V8, P78.
   Hajjdiab H., 2011, 2011 Proceedings of IEEE International Conference on Imaging Systems and Techniques (IST 2011), P306, DOI 10.1109/IST.2011.5962205.
   Huang Lin, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P871, DOI 10.1109/CSSE.2008.1333.
   Arribas JI, 2011, COMPUT ELECTRON AGR, V78, P9, DOI 10.1016/j.compag.2011.05.007.
   Ip E., 2010, INT ENCY ED, P142.
   Liu J, 2009, LECT NOTES ARTIF INT, V5755, P645.
   Liwen Gao, 2010, Proceedings 3rd International Congress on Image and Signal Processing (CISP 2010), P2732, DOI 10.1109/CISP.2010.5647617.
   Liwen Gao, 2010, Proceedings 2010 Sixth International Conference on Natural Computation (ICNC 2010), P1038, DOI 10.1109/ICNC.2010.5582971.
   Nam Y, 2008, COMPUT VIS IMAGE UND, V110, P245, DOI 10.1016/j.cviu.2007.08.002.
   Valliammai N., 2011, WORLD COMPUTER SCI I, V1, P370.
   VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wu Q., 2006, ADV ARTIFICIAL INTEL, V3, P5.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yu-Ping Liao, 2010, 2010 International Symposium on Computer, Communication, Control and Automation (3CA), P334, DOI 10.1109/3CA.2010.5533816.
   Zhang SW, 2011, NEUROCOMPUTING, V74, P2284, DOI 10.1016/j.neucom.2011.03.007.},
Number-of-Cited-References = {20},
Times-Cited = {13},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {16},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {094IF},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000315255500015},
DA = {2023-08-12},
}

@article{ WOS:000439697800002,
Author = {Gao, Zhi-Yong and Xie, Heng-Xing and Li, Ji-Feng and Liu, Shi-Li},
Title = {Spatial-Structure Siamese Network for Plant Identification},
Journal = {INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE},
Year = {2018},
Volume = {32},
Number = {11},
Month = {NOV},
Abstract = {Plant identification is now attracting considerable attention due to its
   important applications in agriculture automation and ecosystems.
   Recently, deep learning-based plant identification methods have drawn
   increasing interest and shown favorable performance. However, existing
   methods do not consider plant spatial structure and their similarities
   explicitly. In this paper, we propose a robust spatial-structure siamese
   network (3SN) for plant identification, which has the following
   advantages: (1) It models the spatial structure of a plant by recurrent
   neural networks exploiting their capability to capture long-range
   dependencies among sequential data, which enables it to capture even a
   slight difference between a specific plant and distractors. (2) The
   plant similarity modeling is achieved effectively by a siamese network
   with large numbers of image pairs. In this way, the plant classification
   task and siamese learning task are learned jointly in a unified
   framework, where both can enhance and complement each other. Extensive
   experimental results show that the proposed 3SN method outperforms the
   state-of-the-art methods consistently.},
Publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
Address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
Type = {Article},
Language = {English},
Affiliation = {Gao, ZY (Corresponding Author), Weinan Normal Univ, Sch Chem \& Environm, Weinan 714099, Shaanxi, Peoples R China.
   Gao, ZY (Corresponding Author), Key Lab Ecol \& Environm River Wetlands Shaanxi Pr, Weinan 714099, Shaanxi, Peoples R China.
   Gao, Zhi-Yong; Xie, Heng-Xing; Li, Ji-Feng; Liu, Shi-Li, Weinan Normal Univ, Sch Chem \& Environm, Weinan 714099, Shaanxi, Peoples R China.
   Gao, Zhi-Yong; Xie, Heng-Xing; Li, Ji-Feng, Key Lab Ecol \& Environm River Wetlands Shaanxi Pr, Weinan 714099, Shaanxi, Peoples R China.},
DOI = {10.1142/S0218001418500350},
Article-Number = {1850035},
ISSN = {0218-0014},
EISSN = {1793-6381},
Keywords = {Plant identification; deep learning; RNNs; siamese network},
Keywords-Plus = {IMAGE CLASSIFICATION; LEAF},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {shandonggaozhiyong@126.com},
Affiliations = {Weinan Normal University},
Funding-Acknowledgement = {2016 Key Scientific Research Project of Shaanxi Department of Education
   {[}16JS031]; Science and Engineering Talents Foundation of Weinan Normal
   University {[}2015ZRRC02]; 2018 Military and Civil Integration Research
   Fund Project of Shaanxi Province {[}18JMR50]},
Funding-Text = {This work was supported by the 2016 Key Scientific Research Project of
   Shaanxi Department of Education (16JS031), the Science and Engineering
   Talents Foundation of Weinan Normal University (2015ZRRC02), and the
   2018 Military and Civil Integration Research Fund Project of Shaanxi
   Province (18JMR50).},
Cited-References = {Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259.
   Bonnet P, 2016, MULTIMED TOOLS APPL, V75, P1647, DOI 10.1007/s11042-015-2607-4.
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339.
   Champ J., 2015, CLEF 2015, V1391, P8.
   Charters J., 2014, 2014 IEEE INT C MULT, P1, DOI {[}10.1109/ICMEW.2014.6890557, DOI 10.1109/ICMEW.2014.6890557].
   Chen Q, 2014, CLEF WORKING NOTES, P693.
   Cho K, 2014, P 2014 C EMPIRICAL M, P1724, DOI DOI 10.3115/V1/D14-1179.
   Choi S., 2015, CLEF 2015 C, P1.
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6474, P135, DOI 10.1007/978-3-642-17688-3\_14.
   Cugu I., ARXIV170108291.
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
   Di Cicco M., ARXIV161203019.
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878.
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714.
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402\_1.
   Goeau H., 2012, P 1 ACM INT WORKSH M, P41.
   Goeau H., 2011, P 19 INT C MULT SCOT, DOI {[}10.1145/2072298.2072472, DOI 10.1145/2072298.2072472].
   Goring C, 2014, PROC CVPR IEEE, P2489, DOI 10.1109/CVPR.2014.319.
   Graves A., 2008, ADV NEURAL INFORM PR, V21, P545, DOI DOI 10.1007/978-1-4471-4072-6.
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948.
   He A., 2016, 2016 IEEE INT C SYST.
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123.
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI {[}10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2].
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee SH, 2016, J SENSORS, V2016, DOI 10.1155/2016/1859292.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lin TY, 2015, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2015.7299135.
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
   LUO WJ, 2016, PROC CVPR IEEE, P5695, DOI DOI 10.1109/CVPR.2016.614.
   Maji S., ARXIV13065151.
   McCool C., 2015, P C LABS EV FOR JAN.
   Parkhi OM., 2015, PROC BRIT MACH VIS C, P6, DOI {[}10.5244/C.29.41, DOI 10.5244/C.29.41].
   Paulus S, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-238.
   Pinheiro PO, 2014, PR MACH LEARN RES, V32.
   Potena C, 2017, ADV INTELL SYST COMP, V531, P105, DOI 10.1007/978-3-319-48036-7\_9.
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Reyes A. K., 2015, CLEF 2015 C, P1.
   Rodner E., ARXIV150700913.
   Sanchez J, 2011, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2011.5995504.
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682.
   Shuai B., 2016, CVPR, P1026.
   Shuai B, 2015, IEEE SIGNAL PROC LET, V22, P1990, DOI 10.1109/LSP.2015.2441781.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Socher R, 2011, ICML.
   Stark M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.36.
   Stollenga Marijn F., 2015, NIPS.
   Sun J, 2013, IEEE I CONF COMP VIS, P3400, DOI 10.1109/ICCV.2013.422.
   Sutskever I., 2014, P 27 INT C NEURAL IN, V2, P3104.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Szucs G., 2014, CLEF 2014 C, V1180, P15.
   Tai K.S., 2015, ARXIV150300075.
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220.
   Nguyen TTN, 2017, STUD COMPUT INTELL, V710, P223, DOI 10.1007/978-3-319-56660-3\_20.
   Varior Rahul Rama, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P135, DOI 10.1007/978-3-319-46478-7\_9.
   WANG JJ, 2010, PROC CVPR IEEE, P3360, DOI DOI 10.1109/CVPR.2010.5540018.
   Wang YM, 2016, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2016.131.
   Xie SN, 2015, PROC CVPR IEEE, P2645, DOI 10.1109/CVPR.2015.7298880.
   Yang Z., ARXIV151102274.
   ZAGORUYKO S, 2015, PROC CVPR IEEE, P4353, DOI DOI 10.1109/CVPR.2015.7299064.
   Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767.
   Zhang CJ, 2017, IEEE T NEUR NET LEAR, V28, P1550, DOI 10.1109/TNNLS.2016.2545112.
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1\_54.
   Zhou F, 2016, PROC CVPR IEEE, P1124, DOI 10.1109/CVPR.2016.127.
   Zuo Z, 2016, IEEE T IMAGE PROCESS, V25, P2983, DOI 10.1109/TIP.2016.2548241.},
Number-of-Cited-References = {77},
Times-Cited = {4},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {37},
Journal-ISO = {Int. J. Pattern Recognit. Artif. Intell.},
Doc-Delivery-Number = {GO1GM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000439697800002},
DA = {2023-08-12},
}

@inproceedings{ WOS:000977250500020,
Author = {Zhang, Yang and Wang, Yizhen and Tang, Zhicheng and Zhai, Zhenduo and
   Shang, Yi and Viegut, Reid},
Book-Group-Author = {IEEE},
Title = {Deep Learning Methods for Tree Detection and Classification},
Booktitle = {2022 IEEE 4TH INTERNATIONAL CONFERENCE ON COGNITIVE MACHINE
   INTELLIGENCE, COGMI},
Year = {2022},
Pages = {148-155},
Note = {IEEE 4th International Conference on Cognitive Machine Intelligence
   (CogMI), ELECTR NETWORK, DEC 14-16, 2022},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {This paper presents the results of our deep learning methods for tree
   detection and classification on aerial images in the Plant Recognition
   University Challenge sponsored by Ameren in 2021-2022. The task was to
   locate the trees in an aerial image and predict their family, genus, and
   species. For tree detection, we applied various supervised learning
   methods with labeled training data as well as semi-supervised learning
   methods with the addition of unlabeled data. Our experimental results
   show that the semi-supervised learning method outperformed the
   supervised learning methods, improving the f1-score by an average of
   three percent on the set of images used in the final Plant Challenge
   competition. For tree classification, We applied various machine
   learning methods and deep learning models for image classification to
   predict family, genus and species on the portions of images detected of
   trees by the detection models. By considering the relationships between
   family, genus and species, we developed a multi-head ResNet18-based
   neural network and increased mean accuracy by two percent over the
   baseline ResNet18. Finally, our team ranked first among all teams in the
   Plant Challenge competition.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zhang, Y (Corresponding Author), Univ Missouri, Dept Elect Engn \& Comp Sci EECS, Columbia, MO 65211 USA.
   Zhang, Yang; Wang, Yizhen; Tang, Zhicheng; Zhai, Zhenduo; Shang, Yi, Univ Missouri, Dept Elect Engn \& Comp Sci EECS, Columbia, MO 65211 USA.
   Viegut, Reid, Univ Missouri, Sch Nat Resources, Columbia, MO USA.},
DOI = {10.1109/CogMI56440.2022.00030},
ISBN = {978-1-6654-7406-1},
Keywords = {aerial image; RGB image; multi-spectral image; tree detection; tree
   classification; supervised learning; semi-supervised learning; deep
   learning},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory \& Methods},
Author-Email = {zhangy1@missouri.edu
   ywf3m@missouri.edu
   zt253@missouri.edu
   zz7z9@missouri.edu
   shangy@missouri.edu
   rav3pt@missouri.edu},
Affiliations = {University of Missouri System; University of Missouri Columbia;
   University of Missouri System; University of Missouri Columbia},
Funding-Acknowledgement = {Ameren},
Funding-Text = {This work is partially funded by a grant from Ameren. Special thanks to
   Jason Wibbenmeyer, the manager of Technology Transfer at Ameren Missouri
   who contacted teams and organized this impressive competition. Thanks to
   all the Ameren Unmanned Aerial Systems members who answered our
   questions patiently. Thanks to the Missouri Botanical Garden for
   providing a test area and plant records from their Living Collections
   Management System. We also thank Dr. Andy Raedeke and Mr. Joel Sartwell
   from Missouri Department of Conservation for giving us valuable comments
   and suggestions.},
Cited-References = {He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hoffmann R., 2011, P 49 ANN M ASS COMP, V1, P541, DOI DOI 10.5555/2002472.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826.
   Noble WS, 2006, NAT BIOTECHNOL, V24, P1565, DOI 10.1038/nbt1206-1565.
   Secord J, 2007, IEEE GEOSCI REMOTE S, V4, P196, DOI 10.1109/LGRS.2006.888107.
   Sohn K, 2020, Arxiv, DOI arXiv:2005.04757.
   Weinstein B, 2020, METHODS ECOL EVOL, V11, P1743, DOI 10.1111/2041-210X.13472.
   Wu Yuxin, 2019, DETECTRON2.
   Xu MD, 2021, Arxiv, DOI arXiv:2106.09018.
   Yang L., 2009, P 17 ACM SIGSPATIAL, P131, DOI DOI 10.1145/1653771.1653792.},
Number-of-Cited-References = {11},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BV0SS},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000977250500020},
DA = {2023-08-12},
}

@inproceedings{ WOS:000348509400089,
Author = {Lv, Feng-hua and Wang, Hang-jun},
Editor = {Wen, Z and Li, T},
Title = {Graph Cuts-Based Feature Extraction of Plant Leaf},
Booktitle = {PRACTICAL APPLICATIONS OF INTELLIGENT SYSTEMS, ISKE 2013},
Series = {Advances in Intelligent Systems and Computing},
Year = {2014},
Volume = {279},
Pages = {927+},
Note = {8th International Conference on Intelligent Systems and Knowledge
   Engineering (ISKE), Shenzhen, PEOPLES R CHINA, NOV 20-23, 2013},
Organization = {Shenzhen Univ; Sci China Press; Chinese Acad Sci; IEEE Computat
   Intelligence Soc; Chinese Assoc Artificial Intelligence; State Key Lab
   Complex Elect Syst Simulat; Sci \& Technol Integrated Informat Syst Lab;
   SW Jiaotong Univ; Univ Technol},
Abstract = {As leaf is one of the most important organs in a plant, contour features
   of plant leaves are important for the identification of plant species.
   So researchers have proposed many methods to improve the progress of the
   plant identification. In this paper, we present a graph cuts-based
   method using Min-Cut/Max Flow algorithm to obtain the leaf blade
   section. Then, five basic features are computed to further obtain six
   digital morphological features. These experimental results show that the
   graph cuts algorithm and the presented leaf features are important for
   leaf recognition.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Wang, HJ (Corresponding Author), Zhejiang A\&F Univ, Tianmu Coll, Linan 311300, Peoples R China.
   Wang, Hang-jun, Zhejiang A\&F Univ, Tianmu Coll, Linan 311300, Peoples R China.
   Lv, Feng-hua, Jinhua Polytech, Jinhua 321007, Peoples R China.},
DOI = {10.1007/978-3-642-54927-4\_89},
ISSN = {2194-5357},
EISSN = {2194-5365},
ISBN = {978-3-642-54927-4; 978-3-642-54926-7},
Keywords = {Plant leaf; Graph cuts; Statistical features; Feature extraction},
Keywords-Plus = {SEGMENTATION},
Research-Areas = {Computer Science; Robotics},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Robotics},
Author-Email = {213837053@qq.com
   whj@zafue.edu.cn},
Affiliations = {Zhejiang A\&F University; Jinhua Polytechnic},
Funding-Acknowledgement = {Jinhua Polytechnic {[}2011S002]; Talent Start-up Foundation of Zhejiang
   A F University {[}2013FR059]},
Funding-Text = {The work reported in this paper was supported by Jinhua Polytechnic
   under the research grant 2011S002, and the Talent Start-up Foundation of
   Zhejiang A\& F University under grant No.2013FR059.},
Cited-References = {Baker B, 1996, J PLANT PHYSIOL, V148, P530, DOI 10.1016/S0176-1617(96)80072-1.
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114.
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60.
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5.
   Chien CF, 2002, T ASAE, V45, P1669.
   Daly DC, 2009, MANUAL LEAF ARCHITEC.
   Dinic E.A., 1970, SOVIET MATH DOKLAD, V1, P1277.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Eriksson Anders, 2006, IMAGE SEGMENTATION U.
   Fu H, 2006, IEE P-VIS IMAGE SIGN, V153, P881, DOI 10.1049/ip-vis:20060061.
   Iwata H, 2002, BREEDING SCI, V52, P89, DOI 10.1270/jsbbs.52.89.
   Kim D, 2012, ELECTRON LETT, V48, P1198, DOI 10.1049/el.2012.0740.
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177.
   Peng B, 2013, PATTERN RECOGN, V46, P1020, DOI 10.1016/j.patcog.2012.09.015.
   Yang Y, 2013, PATTERN RECOGN, V46, P1101, DOI 10.1016/j.patcog.2012.09.024.
   Ye YH, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P723, DOI 10.1109/ISIMP.2004.1434166.
   Zheng Q, 2013, SIGNAL PROCESS, V93, P961, DOI 10.1016/j.sigpro.2012.10.005.
   Zhou HL, 2013, PATTERN RECOGN, V46, P1719, DOI 10.1016/j.patcog.2012.12.005.},
Number-of-Cited-References = {18},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BB9ND},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000348509400089},
DA = {2023-08-12},
}

@article{ WOS:000488143100032,
Author = {Kounalakis, Tsampikos and Triantafyllidis, Georgios A. and Nalpantidis,
   Lazaros},
Title = {Deep learning-based visual recognition of rumex for robotic precision
   farming},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2019},
Volume = {165},
Month = {OCT},
Abstract = {In this paper we address the problem of recognising the Broad-leaved
   dock (Rumex obtusifolius L.) in grasslands from high-resolution 2D
   images. We discuss and present the determining factors for developing
   and implementing weed visual recognition algorithms using deep learning.
   This analysis, leads to the formulation of the proposed algorithm. Our
   implementation exploits Transfer Learning techniques for deep
   learning-based feature extraction, in combination with a classifier for
   weed recognition. A prototype robotic platform has been used to make
   available an image dataset from a dairy farm containing broad-leaved
   docks. The evaluation of the proposed algorithm on this dataset shows
   that it outperforms competing weed/plant recognition methods in
   recognition accuracy, while producing low false-positive rates under
   real-world operation conditions.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Kounalakis, T (Corresponding Author), Danish Technol Inst, Dept Robot Technol, Serv Robot, Odense, Denmark.
   Kounalakis, Tsampikos, Danish Technol Inst, Dept Robot Technol, Serv Robot, Odense, Denmark.
   Triantafyllidis, Georgios A., Aalborg Univ, Dept Architecture Design \& Media Technol, Copenhagen, Denmark.
   Nalpantidis, Lazaros, Tech Univ Denmark, Dept Elect Engn, Lyngby, Denmark.},
DOI = {10.1016/j.compag.2019.104973},
Article-Number = {104973},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Weed recognition; Deep learning visual recognition; Agricultural
   robotics; Precision fanning},
Keywords-Plus = {OBTUSIFOLIUS; CROP},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {tsko@teknologisk.dk
   gt@create.aau.dk
   lanalpa@elektro.dtu.dk},
Affiliations = {Danish Technological Institute; Aalborg University; Technical University
   of Denmark},
ResearcherID-Numbers = {Triantafyllidis, Georgios/HTR-3327-2023
   Nalpantidis, Lazaros/AAV-9414-2021},
ORCID-Numbers = {Triantafyllidis, Georgios/0000-0002-0495-777X
   Nalpantidis, Lazaros/0000-0002-3620-4123},
Funding-Acknowledgement = {European Union {[}30079, 618123 {[}ICT-AGRI 2]]; Ministry of Economic
   Affairs (The Netherlands); Federal Office for Agriculture (Switzerland);
   Innovation Fund Denmark; Ministry of Science, Innovation and Higher
   Education (Denmark)},
Funding-Text = {This work has been supported by the DockWeeder project (project ID:
   30079), administered through the European Union's Seventh Framework
   Programme for research, technological development and demonstration
   under grant agreement no 618123 {[}ICT-AGRI 2]. The project has received
   funding from the Ministry of Economic Affairs (The Netherlands), from
   the Federal Office for Agriculture (Switzerland), and from Innovation
   Fund Denmark, the Ministry of Science, Innovation and Higher Education
   (Denmark).},
Cited-References = {Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615.
   Bengio Y., 2012, P ICML WORKSHOP UNSU, V27, P17, DOI DOI 10.5555/3045796.3045800.
   Binch A, 2017, COMPUT ELECTRON AGR, V140, P123, DOI 10.1016/j.compag.2017.05.018.
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76.
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953.
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Di Febbo P, 2018, KCNN EXTREMELY EFFIC.
   Fan RE, 2008, J MACH LEARN RES, V9, P1871.
   Georgakis G, 2018, PROC CVPR IEEE, P1965, DOI 10.1109/CVPR.2018.00210.
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1.
   Grimstad L, 2017, ROBOTICS, V6, DOI 10.3390/robotics6040024.
   He HB, 2008, IEEE IJCNN, P1322, DOI 10.1109/IJCNN.2008.4633969.
   Joly A., 2014, LNCS, P229, DOI DOI 10.1007/978-3-319-11382-1\_20.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Kazmi W, 2015, COMPUT ELECTRON AGR, V118, P290, DOI 10.1016/j.compag.2015.08.023.
   Kounalakis T, 2018, IEEE CONF IMAGING SY, P185.
   Kounalakis T, 2018, MULTIMED TOOLS APPL, V77, P9567, DOI 10.1007/s11042-017-5337-y.
   Kounalakis T, 2017, LECT NOTES COMPUT SC, V10528, P485, DOI 10.1007/978-3-319-68345-4\_43.
   Kounalakis T, 2016, IEEE CONF IMAGING SY, P466, DOI 10.1109/IST.2016.7738271.
   Krizhevsky A, 2012, NEURAL NETWORKS.
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
   Lottes P, 2016, IEEE INT CONF ROBOT, P5157, DOI 10.1109/ICRA.2016.7487720.
   Lottes P., 2017, 2017 IEEE INT C ROB, P3024, DOI DOI 10.1109/ICRA.2017.7989347.
   Lottes P, 2018, IEEE ROBOT AUTOM LET, V3, P2870, DOI 10.1109/LRA.2018.2846289.
   Michaels A, 2015, IEEE INT C INT ROBOT, P5498, DOI 10.1109/IROS.2015.7354156.
   Mordvintsev A., 2015, INCEPTIONISM GOING D, V14, P5.
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222.
   Perez-Ortiz M, 2016, EXPERT SYST APPL, V47, P85, DOI 10.1016/j.eswa.2015.10.043.
   Pinto L, 2016, IEEE INT CONF ROBOT, P3406, DOI {[}arXiv:1509.06825, 10.1109/ICRA.2016.7487517].
   Potena C, 2016, 14TH INTERNATIONAL C.
   Redmon J, CORR ABS 1804 02767.
   Ren S, CORR ABS 1506 01497.
   Reyes A. K, 2015, CEUR WORKSHOP PROCEE, V1391.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Schwarz M, 2015, IEEE INT CONF ROBOT, P1329, DOI 10.1109/ICRA.2015.7139363.
   Sermanet P, 2014, ICLR.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sunderhauf N, 2014, WORKING NOTES OF CLE.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   van Evert FK, 2009, WEED RES, V49, P164, DOI 10.1111/j.1365-3180.2008.00682.x.
   van Evert FK, 2011, J FIELD ROBOT, V28, P264, DOI 10.1002/rob.20377.
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412.},
Number-of-Cited-References = {43},
Times-Cited = {36},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {28},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {JA9AT},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000488143100032},
OA = {Green Submitted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000452511200052,
Author = {Al-Otaibi, Manar Bati and Ashour, Amira S. and Dey, Nilanjan and Al
   Quthami, Rahaf Abdullah and Al-Nufaei, Asrar Abdullah and Shi, Fuqian},
Editor = {Balas, VE and Jain, LC and Zhao, X and Shi, F},
Title = {Statistical Image Analysis Based Automated Leaves Classification},
Booktitle = {INFORMATION TECHNOLOGY AND INTELLIGENT TRANSPORTATION SYSTEMS (ITITS
   2017)},
Series = {Frontiers in Artificial Intelligence and Applications},
Year = {2017},
Volume = {296},
Pages = {469-479},
Note = {2nd International Conference on Information Technology and Intelligent
   Transportation Systems (ITITS), Xian, PEOPLES R CHINA, JUN 10-11, 2017},
Organization = {Shaanxi Comp Soc; Changan Univ; Xian Univ Technol; NW Polytechn Univ;
   CAS; Shaanxi Sirui Adv Mat Co Ltd; Special Aircraft Engn Res Inst},
Abstract = {Plants recognition and classification is a challenging process due to
   the high variability in the plants' features and shapes. Numerous
   methodologies incorporating image processing were improved to tackle
   this process for early stage recognition of diseases. Leaf recognition
   has popular/wide range of agriculture practical applications.
   Consequently, the current work is interested in the recognition and
   classification of parsley and basil leaves along with the recognition of
   their infected parts. An image analysis is used to extract different
   statistical features from the leaves' dataset. From such statistical
   features a recognition/classification processes are performed to
   classify the fresh and infected leaves in each leaf type as well as to
   classify the two-leave species. The classification process was performed
   using neural network. The experimental results depicted that the
   classification accuracies for the three tested cases, namely
   fresh/infected basil, fresh/infected parsley, and fresh basil/parsley
   were 80\%, 80.0\%, and 100.0\%; respectively.},
Publisher = {IOS PRESS},
Address = {NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Al-Otaibi, MB (Corresponding Author), Taif Univ, Comp Sci Dept, CIT Coll, At Taif, Saudi Arabia.
   Al-Otaibi, Manar Bati; Ashour, Amira S.; Al Quthami, Rahaf Abdullah; Al-Nufaei, Asrar Abdullah, Taif Univ, Comp Sci Dept, CIT Coll, At Taif, Saudi Arabia.
   Ashour, Amira S., Tanta Univ, Dept Elect \& Elect Commun Engn, Fac Engn, Tanta, Egypt.
   Dey, Nilanjan, Techno India Coll Technol, Dept Informat Technol, Kolkata, India.
   Shi, Fuqian, Wenzhou Med Univ, Coll Informat \& Engn, Wenzhou, Peoples R China.},
DOI = {10.3233/978-1-61499-785-6-469},
ISSN = {0922-6389},
EISSN = {1879-8314},
ISBN = {978-1-61499-785-6; 978-1-61499-784-9},
Keywords = {Image processing; image analysis; de-noising; leaf recognition;
   classification; neural network},
Research-Areas = {Computer Science; Transportation},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Transportation Science \&
   Technology},
Affiliations = {Taif University; Egyptian Knowledge Bank (EKB); Tanta University;
   Wenzhou Medical University},
ResearcherID-Numbers = {Ashour, Amira S./T-5454-2019},
ORCID-Numbers = {Ashour, Amira S./0000-0003-3217-6185},
Cited-References = {{[}Anonymous], P EUR C COMP VIS SEP.
   {[}Anonymous], 2009, ENCY DISTANCES.
   Arias-Castro E, 2009, ANN STAT, V37, P1172, DOI 10.1214/08-AOS604.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Ashour AS, 2016, NEURAL COMPUT APPL, P1.
   Beagum S., 2016, CLASSIFICATION CLUST, V1.
   Bishop C., 2006, MACH LEARN, V128, P1.
   Chaki J., 2011, INT J ADV COMPUTER S, V2.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   Elazouni AM, 1997, J COMPUT CIVIL ENG, V11, P217, DOI 10.1061/(ASCE)0887-3801(1997)11:4(217).
   Erdem H, 2010, ADV ENG SOFTW, V41, P270, DOI 10.1016/j.advengsoft.2009.07.006.
   Fabijanska A., 2009, AUTOMATYKA AKAD HUTN, V13, P807.
   Ferreira T., 2012, IMAGEJ FIJI, V1.
   Gwo CY, 2013, APPL PLANT SCI, V1, DOI 10.3732/apps.1200005.
   Hakim SJS, 2014, SMART STRUCT SYST, V14, P159, DOI 10.12989/sss.2014.14.2.159.
   Hore S, 2016, STRUCT ENG MECH, V58, P459, DOI 10.12989/sem.2016.58.3.459.
   Howitt  D., 2008, STAT PSYCHOL.
   Kamaruzzaman S.N., 2017, NAT INV INN EXH COMP, P1.
   Kaur G., 2012, INT J ENG RES DEV, V1, P35.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Lee K.-B., 2013, INT J BIOSCI BIOTECH, V5, P57, DOI {[}10.14257/ijbsbt.2013.5.5.06, DOI 10.14257/IJBSBT.2013.5.5.06, 10.3724/sp.j.1087.2009.01707, DOI 10.1007/978-94-007-5857-5\_12].
   Lim L, 2011, THESIS.
   Marikar F., 2012, TROPICAL AGR RES, V23.
   Oerke EC, 2004, CROP PROT, V23, P275, DOI 10.1016/j.cropro.2003.10.001.
   Pearson K, 1895, PHILOS T R SOC A, V186, P343, DOI {[}10.1098/rsta.1895.0010, DOI 10.1098/RSTA.1895.0010].
   Rathi V. P., 1895, BRAIN TUMOR MRI IMAG.
   Sosa J. P. M., 2013, MATH COMPUTERS BIOL, P53.
   Wang D., 2016, IEEE SENSORS J.
   Wang D, 2017, IEEE ACCESS, V5, P4887, DOI 10.1109/ACCESS.2017.2677950.
   Zhang YD, 2011, ENTROPY-SWITZ, V13, P841, DOI 10.3390/e13040841.
   Zongmei Tian, 2018, Neural Computing and Applications, V30, P3733, DOI 10.1007/s00521-017-2955-2.},
Number-of-Cited-References = {31},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BL5RA},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000452511200052},
DA = {2023-08-12},
}

@inproceedings{ WOS:000426071600004,
Author = {Gao, Min and Lin, Lang and Sinnott, Richard O.},
Book-Group-Author = {IEEE},
Title = {A Mobile Application for Plant Recognition through Deep Learning},
Booktitle = {2017 IEEE 13TH INTERNATIONAL CONFERENCE ON E-SCIENCE (E-SCIENCE)},
Series = {Proceeding IEEE International Conference on e-Science (e-Science)},
Year = {2017},
Pages = {29-38},
Note = {13th Annual IEEE International Conference on e-Science (e-Science),
   Auckland, NEW ZEALAND, OCT 24-27, 2017},
Organization = {New Zealand eScience Infrastructure; IEEE; Microsoft; CRAY; figshare;
   IEEE Comp Soc; eScience Steering Comm},
Abstract = {It is a simple task for humans to visually identify objects. However,
   computer-based image recognition remains challenging. In this paper we
   describe an approach for image recognition with specific focus on
   automated recognition of plants and flowers. The approach taken utilizes
   deep learning capabilities and unlike other approaches that focus on
   static images for feature classification, we utilize video data that
   compensates for the information that would otherwise be lost when
   comparing a static image with many others images of plants and flowers.
   We describe the steps taken in data collection, data cleaning and data
   purification, and the deep learning algorithms that were subsequently
   applied. We describe the mobile (iOS) application that was designed and
   finally we present the overall results that show that in the work
   undertaken thus far, the approach is able to identify 122/125 plants and
   47/50 genera selected with degrees of confidence up to 95\%. We also
   describe the performance speed up through the use of Cloud-based
   resources.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Gao, M (Corresponding Author), Univ Melbourne, Sch Comp \& Informat Syst, Melbourne, Vic, Australia.
   Gao, Min; Lin, Lang; Sinnott, Richard O., Univ Melbourne, Sch Comp \& Informat Syst, Melbourne, Vic, Australia.},
DOI = {10.1109/eScience.2017.15},
ISSN = {2325-372X},
ISBN = {978-1-5386-2686-3},
Keywords = {Deep Learning; Convolutional neural networks; Computer Vision; Data
   Mining; Image Recognition},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications},
Author-Email = {ming1@student.unimelb.edu.au
   langl2@student.unimelb.edu.au
   rsinnott@unimelb.edu.au},
Affiliations = {University of Melbourne},
Cited-References = {Abadi M., 2016, TENSORFLOW LARGE SCA, DOI {[}DOI 10.1038/NN.3331, DOI 10.5555/3026877.3026899].
   Ahmed A, 50 MOST BEAUTIFUL FL.
   {[}Anonymous], 2016, RES BLOG.
   {[}Anonymous], 1993, ADV NEURAL INFORM PR.
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Donahue J, 2014, PR MACH LEARN RES, V32.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Graf H. P., 2005, ADV NEURAL INFORM PR, P521.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hinton G., 2015, DISTILLING KNOWLEDGE.
   Huval B., 2015, AN EMPIRICAL EVALUAT, DOI DOI 10.1002/PI.4767.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI {[}10.1007/BF02478259, DOI 10.1007/BF02478259].
   Mitchell, 2013, MACHINE LEARNING ART.
   Nair V., 2010, ICML, P8, DOI DOI 10.5555/3104322.3104425.
   Norouzi M., 2012, P 25 INT C NEUR INF, V1, P1061, DOI DOI 10.5555/2999134.2999253.
   Prashanth H. S., 2009, Proceedings of the 2009 International Conference on Advances in Computing, Control, \& Telecommunication Technologies (ACT 2009), P859, DOI 10.1109/ACT.2009.218.
   Press W.H, 1996, NUMERICAL RECIPES C, V2, P123.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Sherrington CS, 1906, J PHYSIOL-LONDON, V34, P1.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sinnott R.O., 2016, 22 IEEE INT C PAR DI.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Wu Y., 2016, CORR.},
Number-of-Cited-References = {30},
Times-Cited = {8},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BJ5NN},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000426071600004},
DA = {2023-08-12},
}

@article{ WOS:000342120700011,
Author = {Wang, Xuan and Liang, Junhua and Guo, Fangxia},
Title = {Feature extraction algorithm based on dual-scale decomposition and local
   binary descriptors for plant leaf recognition},
Journal = {DIGITAL SIGNAL PROCESSING},
Year = {2014},
Volume = {34},
Pages = {101-107},
Month = {NOV},
Abstract = {Plant leaf recognition is very important and necessary to agricultural
   information and ecological protection. Unfortunately, the robustness and
   discriminability of the existing methods are insufficient. This paper
   describes a novel plant leaf recognition method. In order to extract
   distinctive features from plant leaf images and reduce the probability
   of disruption by occlusion, clutter, or noise, a novel feature
   extraction algorithm based on dual-scale decomposition and local binary
   descriptors is proposed. The dual-scale decomposition consists of two
   phases. In the first phase, a plant leaf image is decomposed into
   several subbands with an adaptive lifting wavelet scheme. In the second
   phase, each subband is filtered using a group of variable-scale Gaussian
   filters. Local binary descriptors are extracted from the filtered
   subbands to capture both shape and texture characteristics, and then the
   histograms of the local binary descriptors at different scales and
   different subbands are determined and regarded as features. In order to
   improve the robustness and discriminability of plant leaf recognition
   further, a fuzzy k-nearest neighbors' classifier is introduced for
   matching. Experimental results show that the proposed approach yields a
   better performance in terms of the classification accuracies compared
   with the state of the art methods. It is also shown that this method is
   relatively robust to noise, occlusion and smoothing. (C) 2014 Elsevier
   Inc. All rights reserved.},
Publisher = {ACADEMIC PRESS INC ELSEVIER SCIENCE},
Address = {525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA},
Type = {Article},
Language = {English},
Affiliation = {Wang, X (Corresponding Author), Shaanxi Normal Univ, Sch Phys \& Informat Technol, Xian 710062, Shaanxi, Peoples R China.
   Wang, Xuan; Guo, Fangxia, Shaanxi Normal Univ, Sch Phys \& Informat Technol, Xian 710062, Shaanxi, Peoples R China.
   Liang, Junhua, Hebei North Univ, Sch Informat Sci \& Engn, Zhangjiakou 075000, Hebei, Peoples R China.},
DOI = {10.1016/j.dsp.2014.08.005},
ISSN = {1051-2004},
EISSN = {1095-4333},
Keywords = {Dual-scale decomposition; Adaptive lifting wavelet scheme; Local binary
   pattern; Fuzzy k-nearest neighbors' classifier; Plant recognition},
Keywords-Plus = {WAVELET TRANSFORMS; CLASSIFICATION; IDENTIFICATION; EFFICIENT},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {wxuan@snnu.edu.cn},
Affiliations = {Shaanxi Normal University; Hebei North University},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}60633020, 61373083]},
Funding-Text = {This work is supported by Major Program of National Natural Science
   Foundation of China (60633020), the National Natural Science Foundation
   of China under Grant no. 61373083. The authors would like to thank the
   anonymous reviewers and the editor for their constructive comments to
   further improve the quality of this paper.},
Cited-References = {Abbasi S, 1997, LECT NOTES COMPUT SC, V1252, P284.
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244.
   Amiri M, 2010, PATTERN RECOGN, V43, P2485, DOI 10.1016/j.patcog.2009.12.014.
   {[}Anonymous], PLANT LEAF DAT.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Casanova D, 2009, INT J IMAG SYST TECH, V19, P236, DOI 10.1002/ima.20201.
   Claypoole RL, 2003, IEEE T IMAGE PROCESS, V12, P1449, DOI 10.1109/TIP.2003.817237.
   Cope J. S., 2009, INT S VIS COMP LAS V.
   Cope J. S., 2010, ADV CONCEPTS INTELLI.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Nguyen DT, 2013, PATTERN RECOGN, V46, P1485, DOI 10.1016/j.patcog.2012.10.024.
   Fu H, 2006, IEE P-VIS IMAGE SIGN, V153, P881, DOI 10.1049/ip-vis:20060061.
   Heikkila M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68.
   Heikkila M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Im C., 1999, Proceedings Vision Interface `99, P397.
   Jia YQ, 2009, IEEE T NEURAL NETWOR, V20, P729, DOI 10.1109/TNN.2009.2015760.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Kuzume K, 2004, SIGNAL PROCESS, V84, P1931, DOI 10.1016/j.sigpro.2004.06.020.
   Li HF, 2006, IEEE T NEURAL NETWOR, V17, P157, DOI 10.1109/TNN.2005.860852.
   Ling H., 2009, SEMANTIC MINING TECH.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Liu JD, 2009, LECT NOTES COMPUT SC, V5754, P253.
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Park J, 2008, J SYST SOFTWARE, V81, P71, DOI 10.1016/j.jss.2007.05.001.
   Piella G, 2005, SIGNAL PROCESS-IMAGE, V20, P813, DOI 10.1016/j.image.2005.03.016.
   Pietikainen M, 2004, PATTERN RECOGN, V37, P313, DOI 10.1016/S0031-3203(03)00231-0.
   Rejeb A., 2013, ACM C INT C MULT RET.
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323.
   Saitoh T, 2000, INT C PATT RECOG, P507, DOI 10.1109/ICPR.2000.906123.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Sweldens W, 1998, SIAM J MATH ANAL, V29, P511, DOI 10.1137/S0036141095289051.
   Tenenbaum JB, 2002, SCIENCE, V295.
   Wang X, 2013, KNOWL-BASED SYST, V42, P68, DOI 10.1016/j.knosys.2013.01.013.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Wang ZY, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOL 1 \& 2, P372, DOI 10.1109/FUZZ.2002.1005019.
   Zhang SW, 2011, NEUROCOMPUTING, V74, P2284, DOI 10.1016/j.neucom.2011.03.007.},
Number-of-Cited-References = {39},
Times-Cited = {24},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {30},
Journal-ISO = {Digit. Signal Prog.},
Doc-Delivery-Number = {AP5LM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000342120700011},
DA = {2023-08-12},
}

@inproceedings{ WOS:000405324000020,
Author = {Thi Thanh-Nhan Nguyen and Thi-Lan Le and Hai Vu and Huy-Hoang Nguyen and
   Van-Sam Hoang},
Editor = {Krol, D and Nguyen, NT and Shirai, K},
Title = {A Combination of Deep Learning and Hand-Designed Feature for Plant
   Identification Based on Leaf and Flower Images},
Booktitle = {ADVANCED TOPICS IN INTELLIGENT INFORMATION AND DATABASE SYSTEMS},
Series = {Studies in Computational Intelligence},
Year = {2017},
Volume = {710},
Pages = {223-233},
Note = {9th Asian Conference on Intelligent Information and Database Systems
   (ACIIDS), Kanazawa, JAPAN, APR 03-05, 2017},
Organization = {Japan Adv Inst Sci \& Technol; Wroclaw Univ Sci \& Technol; IEEE SMC
   Tech Comm Computat Collect Intelligence; Quang Binh Univ; Yeungnam Univ;
   Bina Nusantara Univ; Univ Teknologi Malaysia; Univ Newcastle},
Abstract = {This paper proposes a combination of deep learning and hand-designed
   feature for plant identification based on leaf and flower images. The
   contributions of this paper are two-fold. First, for each organ image,
   we have performed a comparative evaluation of deep learning and
   hand-designed feature for plant identification. Two approaches for deep
   learning and hand-designed feature that are convolutional neuron network
   (CNN) and kernel descriptor (KDES) are chosen in our experiments.
   Second, based on the results of the first contribution, we propose a
   method for plant identification by late fusing the identification
   results of leaf and flower. Experimental results on ImageClef 2015
   dataset show that hand designed feature outperforms deep learning for
   well-constrained cases (leaf captured on simple background). However,
   deep learning shows its robustness in natural situations. Moreover, the
   combination of leaf and flower images improves significantly the
   identification when comparing leaf-based plant identification.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Nguyen, TTN (Corresponding Author), Hanoi Univ Sci \& Technol, Int Res Inst MICA, HUST CNRS UMI GRENOBLE INP 2954, Hanoi, Vietnam.
   Thi Thanh-Nhan Nguyen; Thi-Lan Le; Hai Vu; Huy-Hoang Nguyen, Hanoi Univ Sci \& Technol, Int Res Inst MICA, HUST CNRS UMI GRENOBLE INP 2954, Hanoi, Vietnam.
   Thi Thanh-Nhan Nguyen, Thai Nguyen Univ Informat \& Commun Technol, Thai Nguyen, Vietnam.
   Van-Sam Hoang, Vietnam Forestry Univ, Hanoi, Vietnam.},
DOI = {10.1007/978-3-319-56660-3\_20},
ISSN = {1860-949X},
EISSN = {1860-9503},
ISBN = {978-3-319-56660-3},
Keywords = {CNN; KDES; Plant identification},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory \& Methods},
Author-Email = {nttnhan@ictu.edu.vn},
Affiliations = {Hanoi University of Science \& Technology (HUST); Thai Nguyen
   University; Vietnam National University of Agriculture (VNUA)},
ResearcherID-Numbers = {Le, Thi-Lan/AAA-5855-2020
   },
ORCID-Numbers = {Le, Thi-Lan/0000-0001-9541-3905
   Vu, Hai/0000-0003-2880-4417},
Funding-Acknowledgement = {ASEAN University Network (Aun-Seed/Net) {[}HUST/CRC/1501]},
Funding-Text = {The authors thank Collaborative Research Program for Common Regional
   Issue (CRC) funded by ASEAN University Network (Aun-Seed/Net), under the
   grant reference HUST/CRC/1501.},
Cited-References = {Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596.
   Bo L., 2010, ADV NEURAL INFORM PR, P244.
   Choi Sungbin, 2015, CLEF 2015 C.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Goeau H., 2015, CEUR WORKSH P.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Le T. L., 2015, P 2 INT WORKSHOP ENV, DOI {[}10.1145/2764873.2764877, DOI 10.1145/2764873.2764877].
   Le Thi-Lan, 2015, CLEF 2015 C.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Nilsback M.-E., 2006, P IEEE COMP SOC C CO, V2, P1447, DOI DOI 10.1109/CVPR.2006.42.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Yoo H. J., 2015, IEIE T SMART PROCESS, V4, P35, DOI {[}10.5573/ieiespc.2015.4.1.035, DOI 10.5573/IEIESPC.2015.4.1.035].},
Number-of-Cited-References = {13},
Times-Cited = {9},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {11},
Doc-Delivery-Number = {BI0YW},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000405324000020},
DA = {2023-08-12},
}

@article{ WOS:000842409900001,
Author = {Kayhan, Gokhan},
Title = {Comparison of the performance of different learning algorithms in leaf
   feature extraction and recognition using convolution neural network},
Journal = {CONCURRENCY AND COMPUTATION-PRACTICE \& EXPERIENCE},
Year = {2022},
Volume = {34},
Number = {26},
Month = {NOV 30},
Abstract = {Plant identification with computer systems has been developed with image
   processing tools and has helped researchers to identify unknown plant
   species with high accuracy. In this study, the leaves of five different
   plants were classified according to their shapes using deep learning. A
   database was created with leaf images of mint, echinacea, St. John's
   wort, melissa, and thyme plants. Images in this database were classified
   with a convolution neural network (CNN). For this classification, 70\%
   training and 30\% testing were randomly selected in the database. The
   parameters of the CNN layer consist of a set of 120x160x3\$\$
   120\textbackslash{}times 160\textbackslash{}times 3 \$\$ learnable
   filters. In the CNN, 10 3x3\$\$ 3\textbackslash{}times 3 \$\$ kernel
   matrices with stride {[}1 1] were used. A rectified linear unit was
   chosen as the activation function. Maximum pooling was performed using a
   2x2\$\$ 2\textbackslash{}times 2 \$\$ filter with stride {[}2 2]. In
   this classification, five fully connected layers were created. Using
   CNN, the performance of different learning algorithms was compared. It
   was observed that CNN achieved more successful results than traditional
   attribute methods.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Kayhan, G (Corresponding Author), Ondokuz Mayis Univ, Dept Comp Engn, TR-55139 Samsun, Turkey.
   Kayhan, Gokhan, Ondokuz Mayis Univ, Dept Comp Engn, TR-55139 Samsun, Turkey.},
DOI = {10.1002/cpe.7294},
EarlyAccessDate = {AUG 2022},
Article-Number = {e7294},
ISSN = {1532-0626},
EISSN = {1532-0634},
Keywords = {classification; convolution neural network; feature extraction; image
   processing},
Keywords-Plus = {PLANT; CLASSIFICATION; PREDICTION; AREA},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Software Engineering; Computer Science, Theory \&
   Methods},
Author-Email = {gkayhan@omu.edu.tr},
Affiliations = {Ondokuz Mayis University},
ResearcherID-Numbers = {Kayhan, Gökhan/HGU-2449-2022},
ORCID-Numbers = {Kayhan, Gökhan/0000-0003-3391-0097},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Bosilj P, 2020, J FIELD ROBOT, V37, P7, DOI 10.1002/rob.21869.
   Caliskan O, 2017, OPEN AGRIC, V2, P589, DOI 10.1515/opag-2017-0062.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Kayhan G., 2020, BALKAN J ELECT COMPU, V8, P81, DOI {[}10.17694/bajece.651286, DOI 10.17694/BAJECE.651286].
   LeCun Y., 1995, CONVOLUTIONAL NETWOR, P255, DOI {[}10.5555/303568.303704, DOI 10.5555/303568.303704].
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Li P., OPTIMIZATION ALGORIT.
   Liu JC, 2018, CHIN AUTOM CONGR, P3165, DOI 10.1109/CAC.2018.8623427.
   Odabas MS, 2017, J CIRCUIT SYST COMP, V26, DOI 10.1142/S0218126617500268.
   Odabas MS, 2016, ZEMDIRBYSTE, V103, P193, DOI 10.13080/z-a.2016.103.025.
   Odabas MS, 2009, TURK J FIELD CROPS, V14, P144.
   Oner F, 2011, PHOTOSYNTHETICA, V49, P637, DOI 10.1007/s11099-011-0069-0.
   Roy D, 2015, IEEE IJCNN.
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Tsukaya H, 2018, SEMIN CELL DEV BIOL, V79, P48, DOI 10.1016/j.semcdb.2017.11.035.
   Wang HD, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9183907.
   Yi D, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10031073.
   Zhou T., ARRAY, V3, DOI {[}10.1016/j.array.2019.100004, DOI 10.1016/J.ARRAY.2019.100004].},
Number-of-Cited-References = {23},
Times-Cited = {0},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Concurr. Comput.-Pract. Exp.},
Doc-Delivery-Number = {5N0BU},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000842409900001},
DA = {2023-08-12},
}

@article{ WOS:000704816200001,
Author = {Roopashree, S. and Anitha, J.},
Title = {DeepHerb: A Vision Based System for Medicinal Plants Using Xception
   Features},
Journal = {IEEE ACCESS},
Year = {2021},
Volume = {9},
Pages = {135927-135941},
Abstract = {The conservation of biodiversity is crucial as many plant species are
   critically under extinction. The traditional medicinal system, an
   alternative to synthetic drugs, promote healthy living and mainly
   depends on the wide repository of plants. A vision-based automatic
   medicinal plant identification system is proposed using different neural
   network techniques in computer vision and deep learning. The challenge
   lies in the unavailability of the medicinal herb dataset. The paper
   showcases a novel medicinal leaf dataset entitled DeepHerb dataset
   comprising of 2515 leaf images from 40 varied species of Indian herbs.
   The efficacy of the dataset is revealed by comparing pre-trained deep
   convolution neural network architectures such as VGG16, VGG19,
   InceptionV3 and Xception. The work concentrates on adopting the transfer
   learning technique on the pre-trained models to extract features and
   classify using Artificial Neural Network (ANN) and Support Vector
   Machine (SVM). The SVM hyperparameters are tuned further by Bayesian
   optimization to achieve a better performance model. The proposed
   DeepHerb model learned from Xception and ANN outperformed by 97.5\%
   accuracy. A cross-platform mobile application entitled HerbSnap
   developed integrating the DeepHerb model identifies the herb image with
   a prediction time of 1 second per image and reveals the pertinent
   details of herbs from the database. This research will further focus on
   expanding the dataset to benefit stakeholders and thus, enriches society
   with the knowledge of herbs and their medicinal properties.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Roopashree, S (Corresponding Author), JAIN Deemed Be Univ, Fac Engn \& Technol, Dept Comp Sci \& Engn AI\&ML \& Cybersecur, Bengaluru 560041, India.
   Roopashree, S (Corresponding Author), Visvesvaraya Technol Univ, VTU RRC, Belagavi 590018, India.
   Roopashree, S., JAIN Deemed Be Univ, Fac Engn \& Technol, Dept Comp Sci \& Engn AI\&ML \& Cybersecur, Bengaluru 560041, India.
   Roopashree, S., Visvesvaraya Technol Univ, VTU RRC, Belagavi 590018, India.
   Anitha, J., RV Inst Technol \& Management, Bengaluru 560076, India.},
DOI = {10.1109/ACCESS.2021.3116207},
ISSN = {2169-3536},
Keywords = {Feature extraction; Biomedical imaging; Deep learning; Support vector
   machines; Neural networks; Transfer learning; Convolutional neural
   networks; Bayesian optimization; computer vision; deep learning;
   medicinal plants; support vector machine; transfer learning; Xception},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {roopashaily@gmail.com},
Affiliations = {Jain University; Visvesvaraya Technological University},
ResearcherID-Numbers = {Shailendra, Roopashree/GRJ-7607-2022},
ORCID-Numbers = {Shailendra, Roopashree/0000-0003-1327-1267},
Cited-References = {Abdullahi H. S., 2017, P 7 INT C INN COMP T.
   Althnian A, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11020796.
   {[}Anonymous], 2016, P 5 INT C AGR AGR JU.
   {[}Anonymous], NATURE, DOI 10.1038/nature14539.
   {[}Anonymous], 2013, ARXIV14014447.
   Arsenovic M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11070939.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Begue A, 2017, INT J ADV COMPUT SC, V8, P166.
   Beikmohammadi A., ARXIV200905139.
   Bodhwani Vinit, 2019, Procedia Computer Science, V152, P186, DOI 10.1016/j.procs.2019.05.042.
   Brochu E., 2010, ARXIV10122599.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Cheng Q, 2018, PROCEEDINGS OF 2018 IEEE 4TH INFORMATION TECHNOLOGY AND MECHATRONICS ENGINEERING CONFERENCE (ITOEC 2018), P60, DOI 10.1109/ITOEC.2018.8740771.
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   De L. C., 2016, ADV PLANTS AGR RES, V5.
   Dileep MR, 2019, TENCON IEEE REGION, P321, DOI 10.1109/TENCON.2019.8929394.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Habiba S. U., 2019, PUB INT PUB ACCOUNTA, P1.
   Husin Z, 2012, COMPUT ELECTRON AGR, V89, P18, DOI 10.1016/j.compag.2012.07.009.
   Jasitha P., 2019, P 4 INT C REC TRENDS, P715.
   Jeon WS, 2017, INT J FUZZY LOG INTE, V17, P26, DOI 10.5391/IJFIS.2017.17.1.26.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Kuo CCJ, 2016, J VIS COMMUN IMAGE R, V41, P406, DOI 10.1016/j.jvcir.2016.11.003.
   Lee JH, 2019, IEEE GLOBE WORK, DOI {[}10.1109/ICAIIC.2019.8669002, 10.1109/gcwkshps45667.2019.9024382].
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Mahajan S, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13020356.
   Mukti I.Z., 2019, 2019 4 INT C ELECT I, P1, DOI DOI 10.1109/EICT48899.2019.9068805.
   Muneer A, 2020, IEEE ACCESS, V8, P196747, DOI 10.1109/ACCESS.2020.3034033.
   Patra J.K., 2019, ETHNOPHARMACOLOGY BI.
   Prasad S, 2017, APPL COGNITIVE COMPU, P37.
   Prasvita D.S., 2013, INT J ADV SCI ENG IN, V3, P5, DOI {[}10.18517/ijaseit.3.2.287, DOI 10.18517/IJASEIT.3.2.287].
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Saad B, 2006, EVID-BASED COMPL ALT, V3, P433, DOI 10.1093/ecam/nel058.
   Sabarinathan C., 2018, INT J GLOB ENG, V1, P120.
   Shaheen S, 2019, ADULTERATION IN HERBAL DRUGS: A BURNING ISSUE, P35, DOI 10.1007/978-3-030-28034-5\_4.
   Simonyan K., 2014, 14091556V6 ARXIV, P1, DOI DOI 10.48550/ARXIV.1409.1556.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zaidah Ibrahim, 2018, INDONES J ELECT ENG, V9, P152.},
Number-of-Cited-References = {46},
Times-Cited = {6},
Usage-Count-Last-180-days = {7},
Usage-Count-Since-2013 = {13},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {WD2ZU},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000704816200001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000474423800017,
Author = {Nghia Duong-Trung and Luyl-Da Quach and Minh-Hoang Nguyen and Chi-Ngon
   Nguyen},
Book-Group-Author = {ACM},
Title = {A Combination of Transfer Learning and Deep Learning for Medicinal Plant
   Classification},
Booktitle = {PROCEEDINGS OF 2019 4TH INTERNATIONAL CONFERENCE ON INTELLIGENT
   INFORMATION TECHNOLOGY (ICIIT 2019)},
Year = {2019},
Pages = {83-90},
Note = {4th International Conference on Intelligent Information Technology
   (ICIIT), Da Nang, VIETNAM, FEB 20-23, 2019},
Abstract = {Medicinal plants are an important element of indigenous medical systems
   in Viet Nam. These resources are usually regarded as a part of culture's
   traditional knowledge. One of the prerequisites for any medical
   recommendation systems and/ or applications is accurate identification
   and classification of medicinal plants. Hence, leveraging technology in
   automatic classification of these curative herbs has become essential.
   Unfortunately, building and training a machine learning model from
   scratch is next to impossible due to the lack of hardware infrastructure
   and finance support. It painfully restricts the requirements of rapid
   solutions to deal with the demand. For this purpose, this paper exploits
   the idea of transfer learning which is the improvement of learning in a
   new prediction task through the transferability of knowledge from a
   related prediction task that has already been learned. By utilizing
   state-of-the-art deep networks re-trained with our collected data, our
   extensive experiments show that the proposed combination performs
   perfectly and achieves the classification accuracy of 98.7\% within the
   acceptable training time.},
Publisher = {ASSOC COMPUTING MACHINERY},
Address = {1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Duong-Trung, N (Corresponding Author), FPT Univ, Can Tho City, Vietnam.
   Nghia Duong-Trung; Luyl-Da Quach, FPT Univ, Can Tho City, Vietnam.
   Minh-Hoang Nguyen, Univ Trento, Dept Informat Engn \& Comp Sci, Trento, Italy.
   Chi-Ngon Nguyen, Can Tho Univ, Coll Engn Technol, Can Tho City, Vietnam.},
DOI = {10.1145/3321454.3321464},
ISBN = {978-1-4503-6633-5},
Keywords = {Medicinal Plant Classification; Transfer Learning; Deep Learning;
   MobileNets},
Keywords-Plus = {CONVOLUTIONAL NEURAL-NETWORKS},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Author-Email = {duong-trung@ismll.de
   luyldaquach@gmail.com
   minhhoang.nguyen@studenti.unitn.it
   ncngon@ctu.edu.vn},
Affiliations = {FPT University; University of Trento; Can Tho University},
ResearcherID-Numbers = {Nguyen, Chi/GZG-5146-2022
   Duong-Trung, Nghia/N-5525-2019
   Nguyen, Chi/GWR-1592-2022
   Duong-Trung, Nghia/K-6960-2017
   },
ORCID-Numbers = {Duong-Trung, Nghia/0000-0002-7402-4166
   Duong-Trung, Nghia/0000-0002-7402-4166
   Luyl-Da, Quach/0000-0002-5661-4250
   Nguyen, Chi-Ngon/0000-0002-9638-7259},
Cited-References = {Abadi M., 2016, TENSORFLOW LARGE SCA, DOI {[}DOI 10.1038/NN.3331, DOI 10.5555/3026877.3026899].
   {[}Anonymous], 2016, GOOGLEFIS SHIFT MOBI.
   {[}Anonymous], 2016, 3 780 VIETNAMESE MED.
   {[}Anonymous], 1999, AFR J ROOT TUBER CRO.
   {[}Anonymous], 2012, IMAGENET LARGE SCALE.
   {[}Anonymous], 2018, PREPARE MOBILE 1 WOR.
   {[}Anonymous], P INT S DISTR COMP A.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4.
   Farnsworth N.R., 1991, CONSERVATION MED PLA, P25.
   Griffin G., 2007, CALTECH 256 OBJECT C.
   Hamilton AC, 2004, BIODIVERS CONSERV, V13, P1477, DOI 10.1023/B:BIOC.0000021333.23413.42.
   Ho Quang-Thai, 2018, ANAL BIOCH.
   Howard A.G., 2017, ABS170404861 CORR.
   Ioffe S., 2015, INT C MACHINE LEARNI, DOI DOI 10.1007/S13398-014-0173-7.2.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Krizhevsky A., 2010, CONVOLUTIONAL DEEP B.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   Loi Do Tat, 2004, NHUNG CAY THUOC VAFI.
   Maggiori E, 2017, IEEE T GEOSCI REMOTE, V55, P645, DOI 10.1109/TGRS.2016.2612821.
   Mamedov N., 2012, MED AROMAT PLANTS, V1, P1000, DOI {[}10.4172/2167-0412.1000e133, DOI 10.4172/2167-0412.1000E133].
   Nguyen Van Duong, 1991, MED PLANTS VIETNAM.
   Le NQK, 2017, J COMPUT CHEM, V38, P2000, DOI 10.1002/jcc.24842.
   Ogle BM, 2003, ECON BOT, V57, P103, DOI 10.1663/0013-0001(2003)057{[}0103:FFOMTM]2.0.CO;2.
   Okwu D. E., 2001, Global Journal of Pure and Applied Sciences, V7, P455.
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222.
   Padulosi S., 2002, Journal of Herbs, Spices \& Medicinal Plants, V9, P243, DOI 10.1300/J044v09n04\_01.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Russakovsky O, 2013, IEEE I CONF COMP VIS, P2064, DOI 10.1109/ICCV.2013.258.
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162.
   Sifre L., 2014, THESIS.
   Simonyan Karen, 2014, THESIS.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128.
   Torrey L., 2010, HDB RES MACHINE LEAR, P242, DOI DOI 10.4018/978-1-60566-766-9.CH011.
   Vernin Gaston, 2016, CHEM OF GINGER, P107.
   Viet Nam Ministry of Health, 2014, QUYET DINH VIEC BAN.
   Wahlberg A, 2006, HEALTH-LONDON, V10, P123, DOI 10.1177/1363459306061784.
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197.
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907.},
Number-of-Cited-References = {41},
Times-Cited = {7},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BN1GN},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000474423800017},
DA = {2023-08-12},
}

@inproceedings{ WOS:000855969800128,
Author = {Kursun, Ramazan and Cinar, Ilkay and Taspinar, Y. Selim and Koklu, Murat},
Book-Group-Author = {IEEE},
Title = {Flower Recognition System with Optimized Features for Deep Features},
Booktitle = {2022 11TH MEDITERRANEAN CONFERENCE ON EMBEDDED COMPUTING (MECO)},
Series = {Mediterranean Conference on Embedded Computing},
Year = {2022},
Pages = {635-638},
Note = {11th Mediterranean Conference on Embedded Computing (MECO) / 3rd Summer
   School on Cyber-Physical + Systems and Internet of Things (CPS and IoT),
   Budva, MONTENEGRO, JUN 07-10, 2022},
Organization = {EUROMICRO; MANT; MECOnet; Univ Montenegro; Eindhoven Tech Univ; Univ
   Zagreb; Univ Belgrade; Ind Syst Inst; Minist Montenegro; Cikom; Elkon;
   SMART4ALL},
Abstract = {Looking at nature, flowers are everywhere. Classification is a difficult
   task, as the flowers have a large number of species that are very
   similar to each other in shape, appearance and color. Classification of
   flowers can be used in various fields of application such as product
   monitoring, flower identification, medicinal flowers, floriculture
   industry, plant taxonomy. In the study, a dataset with 4317 images from
   5 types of flowers was used. In the classification study carried out in
   three stages, deep features were extracted from images with the
   SqueezeNet deep learning architecture of the transfer learning approach
   in the first stage. In the second stage the 1000 extracted features were
   classified using Neural Network and Logistic Regression methods from
   machine learning techniques. In the third stage, the deep features
   extracted were optimized with the help of particle swarm algorithm and
   the 488 features obtained were classified using machine learning Neural
   Network and Logistic Regression methods again. When the results obtained
   at both stages were compared, it was observed that the classification
   with the optimized features improved the success performance. The
   classification success of the features obtained by deep feature
   extraction was obtained as 85.1\% by Neural Network and 79.7\% by
   Logistic Regression method. In the classification results performed with
   the optimized features, the classification success was determined as
   90.1\% for Neural Network and 84.2\% for Logistic Regression. The effect
   of optimized features on classification success is also understood in
   the study.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kursun, R (Corresponding Author), Selcuk Univ, Guneysinir Vocat Sch, Konya, Turkey.
   Kursun, Ramazan, Selcuk Univ, Guneysinir Vocat Sch, Konya, Turkey.
   Cinar, Ilkay; Koklu, Murat, Selcuk Univ, Dept Comp Engn, Konya, Turkey.
   Taspinar, Y. Selim, Selcuk Univ, Doganhisar Vocat Sch, Konya, Turkey.},
DOI = {10.1109/MECO55406.2022.9797103},
ISSN = {2377-5475},
ISBN = {978-1-6654-6828-2},
Keywords = {Flowers; logistic regression; neural network; feature selection;
   particle swarm optimization},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications; Computer Science,
   Theory \& Methods},
Author-Email = {rkursun@selcuk.edu.tr
   ilkay.cinar@selcuk.edu.tr
   ytaspinar@selcuk.edu.tr
   mkoklu@selcuk.edu.tr},
Affiliations = {Selcuk University; Selcuk University; Selcuk University},
ResearcherID-Numbers = {cinar, ilkay/GLS-2427-2022
   KOKLU, Murat/Y-7354-2018
   KURSUN, Ramazan/ACG-4351-2022
   Taspinar, Yavuz Selim/AAZ-9537-2021},
ORCID-Numbers = {cinar, ilkay/0000-0003-0611-3316
   KOKLU, Murat/0000-0002-2737-2360
   KURSUN, Ramazan/0000-0002-6729-1055
   Taspinar, Yavuz Selim/0000-0002-7278-4241},
Funding-Acknowledgement = {Selcuk University Scientific Research Coordinatorship},
Funding-Text = {Thanks to Selcuk University Scientific Research Coordinatorship for
   their support.},
Cited-References = {Adebayo I. A., 2020, GLOBAL FLORICULTURE, P1, DOI DOI 10.1201/9781003000723.
   {[}Anonymous], 2022, FLOWERS RECOGNITION.
   Cinar I, 2022, J AGR SCI-TARIM BILI, V28, P307, DOI 10.15832/ankutbd.862482.
   Guru DS., 2010, INT J COMPUTERS APPL, V1, P21.
   Ito S., 2010, COMPUTER VISION ECCV, DOI {[}10.1007/978-3-642-15555-0\_51, DOI 10.1007/978-3-642-15555-0\_51].
   Kanan C, 2010, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2010.5539947.
   Koklu M, 2022, MEASUREMENT, V188, DOI 10.1016/j.measurement.2021.110425.
   Koklu M, 2021, MATH PROBL ENG, V2021, DOI 10.1155/2021/4793293.
   Koklu M, 2022, BIOMED SIGNAL PROCES, V71, DOI 10.1016/j.bspc.2021.103216.
   Koklu M, 2021, IEEE ACCESS, V9, P86207, DOI 10.1109/ACCESS.2021.3088612.
   Koklu M, 2012, INT J INNOV COMPUT I, V8, P6303.
   Iandola FN, 2016, Arxiv, DOI {[}arXiv:1602.07360, DOI 10.48550/ARXIV.1602.07360].
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Ozsaglam Y.M., 2008, POLITEKNIK DERGISI, V11, P299.
   Patel S, 2019, INT J ENG ADV TECHNO, V8, P277.
   Ropelewska E, 2021, J FOOD PROCESS ENG, V44, DOI 10.1111/jfpe.13724.
   Singh D, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11070981.
   Taspinar YS, 2022, J X-RAY SCI TECHNOL, V30, P73, DOI 10.3233/XST-211031.
   Taspinar YS., 2021, SELCUK U J ENG SCI, V20, P11.
   Tiay T, 2014, 2014 THIRD ICT INTERNATIONAL STUDENT PROJECT CONFERENCE (ICT-ISPC), P99, DOI 10.1109/ICT-ISPC.2014.6923227.
   Togacar M, 2020, MEASUREMENT, V158, DOI 10.1016/j.measurement.2020.107703.
   Ucar F, 2020, MED HYPOTHESES, V140, DOI 10.1016/j.mehy.2020.109761.
   Unlersen MF, 2022, ARAB J SCI ENG, V47, P2639, DOI 10.1007/s13369-021-05700-w.},
Number-of-Cited-References = {23},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BT8XN},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000855969800128},
DA = {2023-08-12},
}

@inproceedings{ WOS:000465018800029,
Author = {Amlekar, Manisha M. and Gaikwad, Ashok T.},
Editor = {Balas, VE and Sharma, N and Chakrabarti, A},
Title = {Plant Classification Using Image Processing and Neural Network},
Booktitle = {DATA MANAGEMENT, ANALYTICS AND INNOVATION, ICDMAI 2018, VOL 2},
Series = {Advances in Intelligent Systems and Computing},
Year = {2019},
Volume = {839},
Pages = {375-384},
Note = {2nd International Conference on Data Management, Analytics and
   Innovation (ICDMAI), Pune, INDIA, JAN 19-21, 2018},
Organization = {Comp Soc India, Div II \& Pune Sect; Inst Ind \& Comp Management \& Res},
Abstract = {This paper is presenting here the plant classification method using
   imaging technology which is useful for classifying the plants by
   providing the leaf image as an input. The proposed method performs
   classification by automatically extracting shape patterns and features
   performing the image processing techniques and neural network model.
   This method gets the leaf image as an input, performs the leaf image
   processing tasks, and automatically extracts the leaf shape pattern and
   leaf shape features. It performs the classification with the help of
   leaf shape features using neural network techniques. This method
   presents plant classification using the leaf shape features and feed
   forward back propagation neural network model. This method results in up
   to 99\% accuracy of classification. This method extracts the leaf shape
   features and patterns automatically using image processing techniques.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Amlekar, MM (Corresponding Author), Dr Babasaheb Ambedkar Marathwada Univ, Aurangabad 431001, Maharashtra, India.
   Amlekar, Manisha M.; Gaikwad, Ashok T., Dr Babasaheb Ambedkar Marathwada Univ, Aurangabad 431001, Maharashtra, India.},
DOI = {10.1007/978-981-13-1274-8\_29},
ISSN = {2194-5357},
EISSN = {2194-5365},
ISBN = {978-981-13-1274-8; 978-981-13-1273-1},
Keywords = {Feature extraction; Feed forward neural network; Leaf image processing;
   Shape extraction},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory \& Methods},
Author-Email = {manishaak2012@gmail.com
   drashokgaikwad@gmail.com},
Affiliations = {Dr. Babasaheb Ambedkar Marathwada University (BAMU)},
Cited-References = {Amlekar M., 2015, IEEE INT C PERV COMP.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Deokar S. R., 2013, INT J ENG RES TECHNO, V2.
   Du J., 2005, LNCS, V3497.
   Du J. X., 2007, APPL MATH COMPUTATIO, V185.
   Gu X., 2005, LNCS.
   Hossain J., 2010, Proceedings of the 13th International Conference on Computer and Information Technology (ICCIT 2010), P458, DOI 10.1109/ICCITECHN.2010.5723901.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Pan Jiazhi, 2008, INT C COMP SCI SOFTW, V4, P906.
   Suman S. G, 2017, INT J INNOVATIVE RES, V5.
   Wang X. F., 2005, LNCS, V3644.
   Wu S. G., 2007, ARXIV07074289V1CSAI.},
Number-of-Cited-References = {12},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BM5IJ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000465018800029},
DA = {2023-08-12},
}

@inproceedings{ WOS:000463335100052,
Author = {Pawara, Pornntiwa and Okafor, Emmanuel and Schomaker, Lambert and
   Wiering, Marco},
Editor = {BlancTalon, J and Penne, R and Philips, W and Popescu, D and Scheunders, P},
Title = {Data Augmentation for Plant Classification},
Booktitle = {ADVANCED CONCEPTS FOR INTELLIGENT VISION SYSTEMS (ACIVS 2017)},
Series = {Lecture Notes in Computer Science},
Year = {2017},
Volume = {10617},
Pages = {615-626},
Note = {18th International Conference on Advanced Concepts for Intelligent
   Vision Systems (ACIVS), Antwerp, BELGIUM, SEP 18-21, 2017},
Organization = {Antwerp Univ; Commonwealth Sci \& Ind Res Org; Ghent Univ},
Abstract = {Data augmentation plays a crucial role in increasing the number of
   training images, which often aids to improve classification performances
   of deep learning techniques for computer vision problems. In this paper,
   we employ the deep learning framework and determine the effects of
   several data-augmentation (DA) techniques for plant classification
   problems. For this, we use two convolutional neural network (CNN)
   architectures, AlexNet and GoogleNet trained from scratch or using
   pre-trained weights. These CNN models are then trained and tested on
   both original and data-augmented image datasets for three plant
   classification problems: Folio, AgrilPlant, and the Swedish leaf
   dataset. We evaluate the utility of six individual DA techniques
   (rotation, blur, contrast, scaling, illumination, and projective
   transformation) and several combinations of these techniques, resulting
   in a total of 12 data-augmentation methods. The results show that the
   CNN methods with particular data-augmented datasets yield the highest
   accuracies, which also surpass previous results on the three datasets.
   Furthermore, the CNN models trained from scratch profit a lot from data
   augmentation, whereas the fine-tuned CNN models do not really profit
   from data augmentation. Finally, we observed that data-augmentation
   using combinations of rotation and different illuminations or different
   contrasts helped most for getting high performances with the scratch CNN
   models.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Wiering, M (Corresponding Author), Univ Groningen, Inst Artificial Intelligence \& Cognit Engn ALICE, Nijenborgh 9, Groningen, Netherlands.
   Pawara, Pornntiwa; Okafor, Emmanuel; Schomaker, Lambert; Wiering, Marco, Univ Groningen, Inst Artificial Intelligence \& Cognit Engn ALICE, Nijenborgh 9, Groningen, Netherlands.},
DOI = {10.1007/978-3-319-70353-4\_52},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-70353-4; 978-3-319-70352-7},
Keywords = {Plant classification; Data augmentation; Deep convolutional neural
   networks},
Keywords-Plus = {SHAPE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {p.pawara@rug.nl
   e.okafor@rug.nl
   l.r.b.schomaker@rug.nl
   m.a.wiering@rug.nl},
Affiliations = {University of Groningen},
ResearcherID-Numbers = {Okafor, Emmanuel/AAB-5120-2019
   Schomaker, Lambert RB/A-9489-2008
   Schomaker, Lambert/GYU-5840-2022
   },
ORCID-Numbers = {Schomaker, Lambert RB/0000-0003-2351-930X
   Wiering, Marco/0000-0003-4331-7537
   Okafor, Emmanuel/0000-0001-6929-6880},
Cited-References = {Atabay H. A, 2016, IIOAB J, V7, P226.
   Bama B. S., 2011, IND J COMP SCI ENG, V2, P202.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116.
   Hsiao JK, 2014, 2014 SCIENCE AND INFORMATION CONFERENCE (SAI), P389, DOI 10.1109/SAI.2014.6918216.
   Kadir A., 2013, ARXIV13115829.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee S. H., 2016, CLEF 2016 C.
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI {[}10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954].
   McFee Brian, 2015, P 16 INT SOC MUS INF, P248.
   Mohanty S. P., 2016, CORR.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Okafor E., 2017, P IEEE INT C INN INT.
   Pawara P, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P479, DOI 10.5220/0006196204790486.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Salamon J, 2016, ARXIV160804363.
   Sato Ikuro, 2015, ARXIV150503229.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Soderkvist O., 2001, THESIS.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   VijayaLakshmi B, 2016, COMPUT ELECTRON AGR, V125, P99, DOI 10.1016/j.compag.2016.04.033.
   Wang X, 2014, DIGIT SIGNAL PROCESS, V34, P101, DOI 10.1016/j.dsp.2014.08.005.
   Wang ZS, 2011, 2011 INTERNATIONAL CONFERENCE ON ECONOMIC, EDUCATION AND MANAGEMENT (ICEEM2011), VOL II, P650, DOI 10.1109/ISWREP.2011.5893091.
   Zhang SW, 2016, PATTERN ANAL APPL, V19, P953, DOI 10.1007/s10044-015-0488-9.},
Number-of-Cited-References = {29},
Times-Cited = {47},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {14},
Doc-Delivery-Number = {BM4HP},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000463335100052},
OA = {Green Submitted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000631791500037,
Author = {Lim, Marcus Guozong and Chuah, Joon Huang},
Book-Group-Author = {IEEE},
Title = {Durian Types Recognition Using Deep Learning Techniques},
Booktitle = {2018 9TH IEEE CONTROL AND SYSTEM GRADUATE RESEARCH COLLOQUIUM
   (ICSGRC2018)},
Year = {2018},
Pages = {183-187},
Note = {9th IEEE Control and System Graduate Research Colloquium (ICSGRC), Shah
   Alam, MALAYSIA, AUG 03-04, 2018},
Organization = {IEEE; Univ Teknologi Mara; IEEE Malaysia Sect Control Syst Soc Chapter;
   IEEE Control Syst Soc; Univ Teknologi Mara, Fac Elect Engn},
Abstract = {Fruit or plant recognition is a very pragmatic and specific application
   of deep-learning technique. As compared to conventional method, the
   technique requires a larger quantity of data for training while at the
   same time promises a higher level of accuracy. Among various classes of
   neural network, convolutional neural network (CNN) is arguably the most
   commonly used method in image classification. The aim of this research
   work is to develop an effective method to classify the various cultivars
   of Durio zibethinus (or commonly known as durian) based on the crop's
   visual features via the application of CNN to improve the accuracy and
   speed of the cultivars recognition. Meanwhile, a reliable database
   consisting of labelled durian cultivars has been created. A total of 800
   images consisting of the bottom view of 3 classes of cultivars and
   non-durian images are used during the training process of the neural
   network. The research work starts with the pre-processing and conversion
   of the images then followed by one-hot labelling of the data,
   construction of the network architecture, training and validation of the
   model then lastly exporting the trained model for general application.
   Important system parameters and prediction accuracy are obtained,
   including the graphs of loss function and accuracy against the number of
   epochs, confusion matrix, miss-classified images, the effect of network
   architecture on prediction performance, etc. The prediction accuracy of
   the trained model on the perfect bottom-view images of Durio zibethinus
   is 82.50\%. With the addition of non-durian images, the prediction
   accuracy is slightly dropped to 81.25\%.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Lim, MG (Corresponding Author), Univ Malaya, VIP Res Lab, Dept Elect Engn, Fac Engn, Kuala Lumpur 50603, Malaysia.
   Lim, Marcus Guozong; Chuah, Joon Huang, Univ Malaya, VIP Res Lab, Dept Elect Engn, Fac Engn, Kuala Lumpur 50603, Malaysia.},
ISBN = {978-1-5386-6321-9},
Keywords = {Image Processing; Machine Learning; Deep Learning; Convolutional Neural
   Network (CNN); Durio zibethinus; Durian Classification; Durian Types
   Recognition},
Keywords-Plus = {DROPOUT},
Research-Areas = {Automation \& Control Systems; Engineering},
Web-of-Science-Categories  = {Automation \& Control Systems; Engineering, Electrical \& Electronic},
Author-Email = {marcuslimguozong@gmail.com
   jhchuah@um.edu.my},
Affiliations = {Universiti Malaya},
ResearcherID-Numbers = {Chuah, Joon Huang/F-9990-2010},
ORCID-Numbers = {Chuah, Joon Huang/0000-0001-9058-3497},
Funding-Acknowledgement = {Fundamental Research Grant Scheme (FRGS) grant from Malaysian Ministry
   of Higher Education {[}FRGS/2/2014/TK03/UM/02/6]},
Funding-Text = {Special thanks to the ImageNet Database which provided the non-durian
   images database. This research is supported by the Fundamental Research
   Grant Scheme (FRGS) grant from Malaysian Ministry of Higher Education
   with the grant no. FRGS/2/2014/TK03/UM/02/6.},
Cited-References = {Aghdam HH., 2017, GUIDE CONVOLUTIONAL.
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110.
   Demuth H. B., 2014, NEURAL NETWORK DESIG.
   Egbert H., 1992, BIOLOGICAL METAPHORS.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8\_2.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   Masci J., 2011, P 22 INT JOINT C ART.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Wu HB, 2015, NEURAL NETWORKS, V71, P1, DOI 10.1016/j.neunet.2015.07.007.},
Number-of-Cited-References = {12},
Times-Cited = {7},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BR1EH},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000631791500037},
DA = {2023-08-12},
}

@article{ WOS:000702923300011,
Author = {Castro, Pedro and Luz, Eduardo and Moreira, Gladston},
Title = {Dataset for Hop varieties classification},
Journal = {DATA IN BRIEF},
Year = {2021},
Volume = {38},
Month = {OCT},
Abstract = {Humulus lupulus L., also known as hops, is a vine whose flowers are a
   major component in brewing. It delivers flavor, bitterness, and aroma to
   beer and also aids in foam stabilization. Furthermore, it plays an
   important role in beer conservation due to its antimicrobial and
   antioxidant properties, which have recently been studied for food
   preservation. Hops can also be found in the production of cosmetics and
   is considered healthy food.
   There are more than 250 cataloged varieties of hops, and among the main
   attributes that differ from each other are alpha-acids, beta-acids, and
   essential oils. Those components give the beer a unique combination of
   characteristics, and may even influence its category. There are many
   ways to identify the hop variety from its acids and essential oils using
   methods such as chromatography, mass spectrometry, capillary
   electrophoresis, and nuclear magnetic resonance. However, these methods
   demand expensive and complex equipment, inaccessible or unavailable to
   most beer producers. In this work, we present a database that includes
   1592 images of hop leaves, from 12 popular hop varieties in southeastern
   Brazil. From these images, it is possible to explore methods of pattern
   recognition and machine learning to classify hop varieties (C) 2021 The
   Authors. Published by Elsevier Inc.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article; Data Paper},
Language = {English},
Affiliation = {Luz, E (Corresponding Author), Univ Fed Ouro Preto, Dept Comp, BR-35400000 Ouro Preto, MG, Brazil.
   Castro, Pedro; Luz, Eduardo; Moreira, Gladston, Univ Fed Ouro Preto, Dept Comp, BR-35400000 Ouro Preto, MG, Brazil.},
DOI = {10.1016/j.dib.2021.107312},
EarlyAccessDate = {AUG 2021},
Article-Number = {107312},
ISSN = {2352-3409},
Keywords = {Hop varieties; Plant recognition; Leaf recognition},
Keywords-Plus = {HUMULUS-LUPULUS L.},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {eduluz@ufop.edu.br},
Affiliations = {Universidade Federal de Ouro Preto},
ResearcherID-Numbers = {Luz, Eduardo J. da S./AAZ-1274-2020
   Moreira, Gladston J. P./H-9396-2012
   },
ORCID-Numbers = {Luz, Eduardo J. da S./0000-0001-5249-1559
   Moreira, Gladston J. P./0000-0001-7747-5926
   Castro, Pedro/0000-0001-5006-6508},
Funding-Acknowledgement = {UFOP},
Funding-Text = {The authors would like to thank Atlantica Hops, Hops Brasil and Brazuca
   Lupulos for providing the images, Lupulo Zona da Mata for technical
   contribution and UFOP for their financial support.},
Cited-References = {Almaguer C, 2014, J I BREWING, V120, P289, DOI 10.1002/jib.160.
   Astray G, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10155074.
   Chadwick LR, 2006, PHYTOMEDICINE, V13, P119, DOI 10.1016/j.phymed.2004.07.006.
   Duarte LM, 2018, ELECTROPHORESIS, V39, P1399, DOI 10.1002/elps.201700420.
   Everingham M., 2011, TECH REP, V8.
   Farag MA, 2014, METABOLOMICS, V10, P21, DOI 10.1007/s11306-013-0547-4.
   Healey J., 2016, HOPS LIST 265 BEER H, V1.
   Kovacevic M, 2002, FOOD CHEM, V77, P489, DOI 10.1016/S0308-8146(02)00114-0.
   Nuutinen T, 2018, EUR J MED CHEM, V157, P198, DOI 10.1016/j.ejmech.2018.07.076.
   Shellie RA, 2009, J SEP SCI, V32, P3720, DOI 10.1002/jssc.200900422.
   Steenackers B, 2015, FOOD CHEM, V172, P742, DOI 10.1016/j.foodchem.2014.09.139.
   Zanoli P, 2008, J ETHNOPHARMACOL, V116, P383, DOI 10.1016/j.jep.2008.01.011.},
Number-of-Cited-References = {12},
Times-Cited = {1},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {9},
Journal-ISO = {Data Brief},
Doc-Delivery-Number = {WA5KG},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000702923300011},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000756019400001,
Author = {Reedha, Reenul and Dericquebourg, Eric and Canals, Raphael and Hafiane,
   Adel},
Title = {Transformer Neural Network for Weed and Crop Classification of High
   Resolution UAV Images},
Journal = {REMOTE SENSING},
Year = {2022},
Volume = {14},
Number = {3},
Month = {FEB},
Abstract = {Monitoring crops and weeds is a major challenge in agriculture and food
   production today. Weeds compete directly with crops for moisture,
   nutrients, and sunlight. They therefore have a significant negative
   impact on crop yield if not sufficiently controlled. Weed detection and
   mapping is an essential step in weed control. Many existing research
   studies recognize the importance of remote sensing systems and machine
   learning algorithms in weed management. Deep learning approaches have
   shown good performance in many agriculture-related remote sensing tasks,
   such as plant classification, disease detection, etc. However, despite
   the success of these approaches, they still face many challenges such as
   high computation cost, the need of large labelled datasets, intra-class
   discrimination (in growing phase weeds and crops share many attributes
   similarity as color, texture, and shape), etc. This paper aims to show
   that the attention-based deep network is a promising approach to address
   the forementioned problems, in the context of weeds and crops
   recognition with drone system. The specific objective of this study was
   to investigate visual transformers (ViT) and apply them to plant
   classification in Unmanned Aerial Vehicles (UAV) images. Data were
   collected using a high-resolution camera mounted on a UAV, which was
   deployed in beet, parsley and spinach fields. The acquired data were
   augmented to build larger dataset, since ViT requires large sample sets
   for better performance, we also adopted the transfer learning strategy.
   Experiments were set out to assess the effect of training and validation
   dataset size, as well as the effect of increasing the test set while
   reducing the training set. The results show that with a small labeled
   training dataset, the ViT models outperform state-of-the-art models such
   as EfficientNet and ResNet. The results of this study are promising and
   show the potential of ViT to be applied to a wide range of remote
   sensing image analysis tasks.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Dericquebourg, E (Corresponding Author), Univ Orleans, INSA CVL, PRISME Lab EA 4229, F-18022 Bourges, France.
   Reedha, Reenul; Dericquebourg, Eric; Hafiane, Adel, Univ Orleans, INSA CVL, PRISME Lab EA 4229, F-18022 Bourges, France.
   Canals, Raphael, Univ Orleans, PRISME Lab EA 4229, INSA CVL, F-45067 Orleans, France.},
DOI = {10.3390/rs14030592},
Article-Number = {592},
EISSN = {2072-4292},
Keywords = {computer vision; deep learning; self-attention; vision transformers;
   remote sensing; drone; image classification; agriculture},
Research-Areas = {Environmental Sciences \& Ecology; Geology; Remote Sensing; Imaging
   Science \& Photographic Technology},
Web-of-Science-Categories  = {Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science \& Photographic Technology},
Author-Email = {reenul.reedha@insa-cvl.fr
   eric.dericquebourg@insa-cvl.fr
   raphael.canals@univ-orleans.fr
   adel.hafiane@insa-cvl.fr},
Affiliations = {Universite de Orleans; Universite de Orleans},
ORCID-Numbers = {Canals, Raphael/0000-0001-9100-7539
   Hafiane, Adel/0000-0003-3185-9996},
Cited-References = {Arlot S, 2010, STAT SURV, V4, P40, DOI 10.1214/09-SS054.
   Bah MD, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111690.
   Bah MD, 2020, IEEE ACCESS, V8, P5189, DOI 10.1109/ACCESS.2019.2960873.
   Chen JZ, 2016, PROCEEDINGS OF 2016 12TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P551, DOI {[}10.1109/CIS.2016.133, 10.1109/CIS.2016.0134].
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359.
   Donmez C, 2021, COMPUT ELECTRON AGR, V187, DOI 10.1016/j.compag.2021.106273.
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.1412.11929.
   Ferreira AD, 2017, COMPUT ELECTRON AGR, V143, P314, DOI 10.1016/j.compag.2017.10.027.
   Hamuda E, 2016, COMPUT ELECTRON AGR, V125, P184, DOI 10.1016/j.compag.2016.04.024.
   Hasan ASMM, 2021, COMPUT ELECTRON AGR, V184, DOI 10.1016/j.compag.2021.106067.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI {[}10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2].
   Hu DC, 2020, ADV INTELL SYST COMP, V1038, P432, DOI 10.1007/978-3-030-29513-4\_31.
   Huang HS, 2020, INT J REMOTE SENS, V41, P3446, DOI 10.1080/01431161.2019.1706112.
   Huang HS, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0196302.
   Iqbal N, 2019, ARCH AGRON SOIL SCI, V65, P1885, DOI 10.1080/03650340.2019.1579904.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Kang J, 2021, COMPUT ELECTRON AGR, V189, DOI 10.1016/j.compag.2021.106370.
   Kerkech M, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105446.
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   LECUN Y, 1989, CONNECTIONISM IN PERSPECTIVE, P143.
   LeCun Y., 1990, P NEURAL INFORM PROC, P396.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Milioto A, 2017, ISPRS ANN PHOTO REM, V4-2, P41, DOI 10.5194/isprs-annals-IV-2-W3-41-2017.
   Molinaro AM, 2005, BIOINFORMATICS, V21, P3301, DOI 10.1093/bioinformatics/bti499.
   Niu ZY, 2021, NEUROCOMPUTING, V452, P48, DOI 10.1016/j.neucom.2021.03.091.
   Nkemelu D.K., 2018, ARXIV181108404.
   Osorio K, 2020, AGRIENGINEERING, V2, P471, DOI 10.3390/agriengineering2030032.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Patel D.D., 2016, J PHARM SCI BIOSCI R, V6, P453.
   Patidar S., 2020, P 2020 INT C EL SUST.
   Petrich L, 2020, PRECIS AGRIC, V21, P1291, DOI 10.1007/s11119-020-09721-7.
   Radoglou-Grammatikis P, 2020, COMPUT NETW, V172, DOI 10.1016/j.comnet.2020.107148.
   Ramirez W., 2020, INT ARCH PHOTOGRAMM, DOI {[}10.5194/isprs-archives-XLII-3-W12-2020-551-2020, DOI 10.5194/ISPRS-ARCHIVES-XLII-3-W12-2020-551-2020].
   Sa I, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10091423.
   Sa I, 2018, IEEE ROBOT AUTOM LET, V3, P588, DOI 10.1109/LRA.2017.2774979.
   Sabzi S, 2020, HELIYON, V6, DOI 10.1016/j.heliyon.2020.e03685.
   Saha D., 2019, THESIS S DAKOTA STAT, P49.
   Simonyan K., 2014, 14091556V6 ARXIV, P1, DOI DOI 10.48550/ARXIV.1409.1556.
   Sokolova M, 2007, LECT NOTES COMPUT SC, V4509, P159.
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002.
   Suh HK, 2018, BIOSYST ENG, V174, P50, DOI 10.1016/j.biosystemseng.2018.06.017.
   Szegedy C., 2015, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2015.7298594.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Tharwat A., 2018, APPL COMPUTING INFOR, P1, DOI DOI 10.1016/J.ACI.2018.08.003.
   Ustuner T, 2020, PLOS ONE, V10, P633, DOI 10.29322/IJSRP.10.08.2020.p10480.
   Vaswani A., 2017, P ANN C NEUR INF PRO, P5998, DOI DOI 10.48550/ARXIV.1706.03762.
   Sivakumar ANV, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12132136.
   Vrbnianin S., 2017, HERBICIDE RESISTANCE, DOI {[}10.5772/67979, DOI 10.5772/67979].
   Wang AC, 2019, COMPUT ELECTRON AGR, V158, P226, DOI 10.1016/j.compag.2019.02.005.
   Wu XL, 2020, J FIELD ROBOT, V37, P322, DOI 10.1002/rob.21938.
   Wu ZN, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21113647.
   Zhang H, 2019, PR MACH LEARN RES, V97.},
Number-of-Cited-References = {55},
Times-Cited = {30},
Usage-Count-Last-180-days = {64},
Usage-Count-Since-2013 = {114},
Journal-ISO = {Remote Sens.},
Doc-Delivery-Number = {ZA2SN},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000756019400001},
OA = {gold, Green Submitted},
DA = {2023-08-12},
}

@article{ WOS:000729992600003,
Author = {Homan, Dewald and du Preez, Johan A.},
Title = {Automated feature-specific tree species identification from natural
   images using deep semi-supervised learning},
Journal = {ECOLOGICAL INFORMATICS},
Year = {2021},
Volume = {66},
Month = {DEC},
Abstract = {Prior work on plant species classification predominantly focuses on
   building models from isolated plant attri-butes. Hence, there is a need
   for tools that can assist in species identification in the natural
   world. We present a novel and robust two-fold approach capable of
   identifying trees in a real-world natural setting. Additionally, we
   leverage unlabelled data through deep semi-supervised learning and
   demonstrate superior performance to su-pervised learning. Our single-GPU
   implementation for featu r e recognition uses minimal annotated data and
   achieves accuracies of 93.96\% and 93.11\% for leaves and bark,
   respectively. Further, we extract feature-specific datasets of 50
   species by employing this technique. Finally, ou r semi-supervised
   species classification method attains 94.04\% top-5 accuracy for leaves
   and 83.04\% top-5 accuracy for bark.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Homan, D (Corresponding Author), Stellenbosch Univ, Fac Engn, ZA-7602 Stellenbosch, South Africa.
   Homan, Dewald; du Preez, Johan A., Stellenbosch Univ, Fac Engn, ZA-7602 Stellenbosch, South Africa.},
DOI = {10.1016/j.ecoinf.2021.101475},
EarlyAccessDate = {NOV 2021},
Article-Number = {101475},
ISSN = {1574-9541},
EISSN = {1878-0512},
Keywords = {Species classification; Deep learning Semi-supervised learning; Computer
   vision; Neural networ k; Natural images; Transfer learning},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Ecology},
Author-Email = {dewald.homan@gmail.com
   dupreez@sun.ac.za},
Affiliations = {Stellenbosch University},
ORCID-Numbers = {du Preez, Johan/0000-0001-6775-9220},
Funding-Acknowledgement = {NVIDIA Corporation},
Funding-Text = {We gratefully acknowledge the support of NVIDIA Corporation with the
   donation of the Titan X Pascal GPU used for this research.},
Cited-References = {Abadi M., 2016, TENSORFLOW LARGE SCA, DOI {[}DOI 10.1038/NN.3331, DOI 10.5555/3026877.3026899].
   Arazo E., 2020, IEEE IJCNN, P1.
   Belkin M, 2006, J MACH LEARN RES, V7, P2399.
   Berthelot D., 2019, INT C LEARN REPR.
   Bourlard H, 1996, SPEECH COMMUN, V18, P205, DOI 10.1016/0167-6393(96)00003-9.
   Carpentier M, 2018, IEEE INT C INT ROBOT, P1075, DOI 10.1109/IROS.2018.8593514.
   Chicco D, 2020, BMC GENOMICS, V21, DOI 10.1186/s12864-019-6413-7.
   Dalponte M, 2015, ISPRS J PHOTOGRAMM, V110, P77, DOI 10.1016/j.isprsjprs.2015.10.010.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Grandvalet Y., 2005, PROC CAP, P529.
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0\_38.
   Howard A.G., 2017, ABS170404861 CORR.
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Le Q. V., 2019, ARXIV PREPRINT ARXIV.
   Lee D., 2013, INT C MACH LEARN WOR.
   LIU X, 2018, J ELECTR COMPUT ENG.
   Loshchilov I., 2017, PROC INT C LEARNING, P1.
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655.
   MORRIS CW, 1992, MYCOL RES, V96, P697, DOI 10.1016/S0953-7562(09)80501-7.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Oliver A, 2018, ADV NEUR IN, V31.
   Pawara P, 2017, LECT NOTES COMPUT SC, V10617, P615, DOI 10.1007/978-3-319-70353-4\_52.
   Pawara P, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P479, DOI 10.5220/0006196204790486.
   Picard M., 2011, IMAGECLEF 2011 PLANT.
   Pimm SL, 2014, SCIENCE, V344, P987, DOI 10.1126/science.1246752.
   Pise NN, 2008, 2008 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY, VOLS 1 AND 2, PROCEEDINGS, P593.
   RATAJCZAK R, 2019, INT C COMPUTER VISIO.
   Ren Z., 2020, ADV NEURAL INFORM PR, V33, P21786.
   Rizve Mamshad Nayeem, 2021, INT C LEARNING REPRE.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI {[}10.1109/ICCV.2017.74, 10.1007/s11263-019-01228-7].
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   SIMPSON R, 1992, MAR ECOL PROG SER, V79, P303.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Sohn K., 2020, NEURIPS, V33, P596.
   Song, 2019, ADV NEURAL INFORM PR, P15663.
   Sulc M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0265-4.
   SUN Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI DOI 10.1155/2017/7361042.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Tzutalin, 2015, LAB GIT COD.
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Weinstein BG, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111309.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zagoruyko S., 2016, ARXIV160507146, P1.
   Zhiyong Wang, 2011, Proceedings of the 2011 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2011), P650, DOI 10.1109/DICTA.2011.115.},
Number-of-Cited-References = {53},
Times-Cited = {2},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {21},
Journal-ISO = {Ecol. Inform.},
Doc-Delivery-Number = {XO2AA},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000729992600003},
OA = {Green Submitted},
DA = {2023-08-12},
}

@article{ WOS:000667276600006,
Author = {Hiep Xuan Huynh and Bao Quoc Truong and Kiet Tan Nguyen Thanh and Dinh
   Quoc Truong},
Title = {Plant Identification Using New Architecture Convolutional Neural
   Networks Combine with Replacing the Red of Color Channel Image by Vein
   Morphology Leaf},
Journal = {VIETNAM JOURNAL OF COMPUTER SCIENCE},
Year = {2020},
Volume = {7},
Number = {2},
Pages = {197-208},
Month = {MAY},
Abstract = {The determination of plant species from field observation requires
   substantial botanical expertise, which puts it beyond the reach of most
   nature enthusiasts. Traditional plant species identification is almost
   impossible for the general public and challenging even for professionals
   who deal with botanical problems daily such as conservationists,
   farmers, foresters, and landscape architects. Even for botanists
   themselves, species identification is often a difficult task. This paper
   proposes a model deep learning with a new architecture Convolutional
   Neural Network (CNN) for leaves classifier based on leaf pre-processing
   extract vein shape data replaced for the red channel of colors. This
   replacement improves the accuracy of the model significantly. This model
   experimented on collector leaves data set Flavia leaf data set and the
   Swedish leaf data set. The classification results indicate that the
   proposed CNN model is effective for leaf recognition with the best
   accuracy greater than 98.22\%.},
Publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
Address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
Type = {Article},
Language = {English},
Affiliation = {Huynh, HX (Corresponding Author), Can Tho Univ, Coll Informat Commun Technol, Can Tho City, Vietnam.
   Hiep Xuan Huynh; Kiet Tan Nguyen Thanh; Dinh Quoc Truong, Can Tho Univ, Coll Informat Commun Technol, Can Tho City, Vietnam.
   Bao Quoc Truong, Can Tho Univ, Coll Engn Technol, Can Tho City, Vietnam.},
DOI = {10.1142/S2196888820500116},
ISSN = {2196-8888},
EISSN = {2196-8896},
Keywords = {Deep learning; convolutional neural networks; leaf classification},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory \& Methods},
Author-Email = {hxhiep@ctu.edu.com
   tqbao@ctu.edu.com
   tqdinh@ctu.edu.com},
Affiliations = {Can Tho University; Can Tho University},
Cited-References = {Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6474, P135, DOI 10.1007/978-3-642-17688-3\_14.
   Hsiao JK, 2014, 2014 SCIENCE AND INFORMATION CONFERENCE (SAI), P389, DOI 10.1109/SAI.2014.6918216.
   Jassmann T.J., 2015, P SOUTHEASTCON 2015, P1, DOI {[}10.1109/SECON.2015.7132978, DOI 10.1109/SECON.2015.7132978].
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lavania S., 2014, P 2014 IEEE INT C CO, P1, DOI DOI 10.1109/ICCIC.2014.7238345.
   Li Y, 2006, IEEE SYS MAN CYBERN, P3890, DOI 10.1109/ICSMC.2006.384738.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Pham NH, 2013, 2013 INTERNATIONAL CONFERENCE ON COMPUTING, MANAGEMENT AND TELECOMMUNICATIONS (COMMANTEL), P134, DOI 10.1109/ComManTel.2013.6482379.
   Oide M, 2000, COMPUT ELECTRON AGR, V29, P59, DOI 10.1016/S0168-1699(00)00136-8.
   Priyankara HAC, 2015, 2015 MORATUWA ENGINEERING RESEARCH CONFERENCE (MERCON), P148, DOI 10.1109/MERCon.2015.7112336.
   Nguyen QK, 2013, PROC INT CONF ADV, P404, DOI 10.1109/ATC.2013.6698145.
   Ren X.M., 2012, P 8 INT C INT COMP T, P37.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Soderkvist O, 2001, COMPUTER VISION CLAS.
   Vini Katyal A, 2012, INT J ADV RES COMPUT, V3, P258.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wu Stephen Gang, 2007, 2007 IEEE International Symposium on Signal Processing and Information Technology, P11, DOI 10.1109/ISSPIT.2007.4458016.
   Wu YH, 2016, LECT NOTES COMPUT SC, V9771, P12, DOI 10.1007/978-3-319-42291-6\_2.
   Zheng X., 2010, INT J IMAGE GRAPH, V2, P25, DOI {[}10.5815/ijigsp.2010.02.04, DOI 10.5815/ijigsp.2010.02.04].},
Number-of-Cited-References = {22},
Times-Cited = {7},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Journal-ISO = {Vietnam J. Comput. Sci.},
Doc-Delivery-Number = {TA5GF},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000667276600006},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000987795200001,
Author = {Chen, Caiyan and Jing, Linhai and Li, Hui and Tang, Yunwei and Chen,
   Fulong},
Title = {Individual Tree Species Identification Based on a Combination of Deep
   Learning and Traditional Features},
Journal = {REMOTE SENSING},
Year = {2023},
Volume = {15},
Number = {9},
Month = {APR 27},
Abstract = {Accurate identification of individual tree species (ITS) is crucial to
   forest management. However, current ITS identification methods are
   mainly based on traditional image features or deep learning. Traditional
   image features are more interpretative, but the generalization and
   robustness of such methods are inferior. In contrast, deep learning
   based approaches are more generalizable, but the extracted features are
   not interpreted; moreover, the methods can hardly be applied to limited
   sample sets. In this study, to further improve ITS identification,
   typical spectral and texture image features were weighted to assist deep
   learning models for ITS identification. To validate the hybrid models,
   two experiments were conducted; one on the dense forests of the
   Huangshan Mountains, Anhui Province and one on the Gaofeng forest farm,
   Guangxi Province, China. The experimental results demonstrated that with
   the addition of image features, different deep learning ITS
   identification models, such as DenseNet, AlexNet, U-Net, and LeNet, with
   different limited sample sizes (480, 420, 360), were all enhanced in
   both study areas. For example, the accuracy of DenseNet model with a
   sample size of 480 were improved to 87.67\% from 85.41\% in Huangshan.
   This hybrid model can effectively improve ITS identification accuracy,
   especially for UAV aerial imagery or limited sample sets, providing the
   possibility to classify ITS accurately in sample-poor areas.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Jing, LH (Corresponding Author), Int Res Ctr Big Data Sustainable Dev Goals, Beijing 100094, Peoples R China.
   Chen, Caiyan; Li, Hui; Chen, Fulong, Chinese Acad Sci, Aerosp Informat Res Inst, Key Lab Digital Earth Sci, Beijing 100094, Peoples R China.
   Chen, Caiyan; Jing, Linhai; Tang, Yunwei; Chen, Fulong, Int Res Ctr Big Data Sustainable Dev Goals, Beijing 100094, Peoples R China.
   Chen, Caiyan, Univ Chinese Acad Sci, Sch Elect Elect \& Commun Engn, Beijing 100049, Peoples R China.
   Chen, Fulong, Jiangxi Normal Univ, Minist Educ, Key Lab Poyang Lake Wetland \& Watershed Res, Nanchang 330022, Peoples R China.
   Chen, Fulong, Jiangxi Normal Univ, Sch Geog \& Environm, Nanchang 330022, Peoples R China.},
DOI = {10.3390/rs15092301},
Article-Number = {2301},
EISSN = {2072-4292},
Keywords = {remote sensing; individual tree species identification; deep learning;
   spectral feature; texture feature},
Keywords-Plus = {LIDAR DATA; WORLDVIEW-2 IMAGERY; FOREST; CLASSIFICATION; DELINEATION;
   FUSION; CROWNS},
Research-Areas = {Environmental Sciences \& Ecology; Geology; Remote Sensing; Imaging
   Science \& Photographic Technology},
Web-of-Science-Categories  = {Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science \& Photographic Technology},
Author-Email = {jinglh@aircas.ac.cn},
Affiliations = {Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Jiangxi Normal University; Jiangxi
   Normal University},
ORCID-Numbers = {li, hui/0000-0002-3565-1773},
Funding-Acknowledgement = {Innovative Research Program of the International Research Center of Big
   Data for Sustainable Development Goals {[}CBAS2022IRP03]; National
   Natural Science Foundation of China {[}41972308]; Jiangxi Provincial
   Technology Innovation Guidance Program (National Science and Technology
   Award Reserve Project Cultivation Program) {[}20212AEI91006]; Second
   Tibetan Plateau Scientific Expedition and Research {[}2019QZKK0806]},
Funding-Text = {This research was supported by the Innovative Research Program of the
   International Research Center of Big Data for Sustainable Development
   Goals (CBAS2022IRP03); the National Natural Science Foundation of China
   (41972308); the Jiangxi Provincial Technology Innovation Guidance
   Program (National Science and Technology Award Reserve Project
   Cultivation Program) (20212AEI91006); and the Second Tibetan Plateau
   Scientific Expedition and Research (2019QZKK0806).},
Cited-References = {Al-Azzawi A, 2020, BMC BIOINFORMATICS, V21, DOI 10.1186/s12859-020-03809-7.
   Alzubaidi L, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00444-8.
   Anitha K, 2010, ECOL COMPLEX, V7, P217, DOI 10.1016/j.ecocom.2010.02.005.
   {[}Anonymous], CANADIAN J REMOTE SE.
   Bakour K, 2021, NEURAL COMPUT APPL, V33, P11499, DOI 10.1007/s00521-021-05816-y.
   Cao KL, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12071128.
   Caudullo G., 2016, EUROPEAN ATLAS FORES, P114.
   Chenari A, 2017, INT ARCH PHOTOGRAMM, V42-4, P43, DOI 10.5194/isprs-archives-XLII-4-W4-43-2017.
   Cho MA, 2015, INT J APPL EARTH OBS, V38, P349, DOI 10.1016/j.jag.2015.01.015.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Culvenor DS, 2002, COMPUT GEOSCI-UK, V28, P33, DOI 10.1016/S0098-3004(00)00110-2.
   Dalponte M, 2014, REMOTE SENS ENVIRON, V140, P306, DOI 10.1016/j.rse.2013.09.006.
   Dalponte M, 2012, REMOTE SENS ENVIRON, V123, P258, DOI 10.1016/j.rse.2012.03.013.
   Dey N, 2021, PATTERN RECOGN LETT, V143, P67, DOI 10.1016/j.patrec.2020.12.010.
   DIXON RK, 1994, SCIENCE, V263, P185, DOI 10.1126/science.263.5144.185.
   Egli S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12233892.
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013.
   Fedrigo M, 2018, ISPRS J PHOTOGRAMM, V136, P106, DOI 10.1016/j.isprsjprs.2017.11.018.
   Franco-Lopez H, 2001, REMOTE SENS ENVIRON, V77, P251, DOI 10.1016/S0034-4257(01)00209-7.
   Fujimoto A, 2019, FORESTS, V10, DOI 10.3390/f10080680.
   FUNG T, 1988, PHOTOGRAMMETRIC ENG, V54, P1449.
   Grabska E, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11101197.
   Hamraz H, 2017, ISPRS J PHOTOGRAMM, V130, P385, DOI 10.1016/j.isprsjprs.2017.07.001.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Holmgren J, 2004, REMOTE SENS ENVIRON, V90, P415, DOI 10.1016/S0034-4257(03)00140-8.
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.
   Immitzer M, 2012, REMOTE SENS-BASEL, V4, P2661, DOI 10.3390/rs4092661.
   Jin WQ, 2021, COMPUT BIOL MED, V131, DOI 10.1016/j.compbiomed.2021.104252.
   Jing L., 2013, P 35 INT S REM SENS.
   Jing LH, 2012, PHOTOGRAMM ENG REM S, V78, P1275, DOI 10.14358/PERS.78.11.1275.
   Jing L, 2009, INT J REMOTE SENS, V30, P2119, DOI 10.1080/01431160802549260.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   Lee J, 2016, IEEE J-STARS, V9, P2554, DOI 10.1109/JSTARS.2016.2569408.
   Li GQ, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107610.
   Lu D, 2007, INT J REMOTE SENS, V28, P823, DOI 10.1080/01431160600746456.
   Madonsela S, 2017, INT J APPL EARTH OBS, V58, P65, DOI 10.1016/j.jag.2017.01.018.
   Mallinis G, 2008, ISPRS J PHOTOGRAMM, V63, P237, DOI 10.1016/j.isprsjprs.2007.08.007.
   Maschler J, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081218.
   Miraki M, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2020.101207.
   Mishra NB, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7110445.
   Modzelewska A, 2020, INT J APPL EARTH OBS, V84, DOI 10.1016/j.jag.2019.101960.
   Naidoo L, 2012, ISPRS J PHOTOGRAMM, V69, P167, DOI 10.1016/j.isprsjprs.2012.03.005.
   Nezami S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12071070.
   Nijland W, 2015, FOREST ECOL MANAG, V357, P239, DOI 10.1016/j.foreco.2015.08.027.
   Oreti L, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13132508.
   Osco LP, 2021, ISPRS J PHOTOGRAMM, V174, P1, DOI 10.1016/j.isprsjprs.2021.01.024.
   Ouyang G, 2021, LASER OPTOELECTRON P, V58, DOI {[}10.3788/L0P202158.0228002, 10.3788/LOP202158.0228002].
   Pant P, 2013, REMOTE SENS ENVIRON, V138, P27, DOI 10.1016/j.rse.2013.07.016.
   Qiu L, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12030585.
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4\_28.
   Saini M, 2021, MULTIMED TOOLS APPL, V80, P20821, DOI 10.1007/s11042-021-10612-w.
   Sedliak M, 2017, CENT EURO FOR J, V63, P1, DOI 10.1515/forj-2017-0002.
   Shi WX, 2023, INT J DIGIT EARTH, V16, P486, DOI 10.1080/17538947.2023.2181992.
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0.
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002.
   Sothe C, 2020, GISCI REMOTE SENS, V57, P369, DOI 10.1080/15481603.2020.1712102.
   Varin M, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12183092.
   Wan HM, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13010144.
   Wang T, 2020, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.324.
   Wang XF, 2021, URBAN FOR URBAN GREE, V58, DOI 10.1016/j.ufug.2020.126958.
   Xu Z, 2020, INT J APPL EARTH OBS, V92, DOI 10.1016/j.jag.2020.102173.
   Yan SJ, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13030479.
   Yin DM, 2016, INT J REMOTE SENS, V37, P4521, DOI 10.1080/01431161.2016.1214302.
   Zhang CY, 2012, PHOTOGRAMM ENG REM S, V78, P1079, DOI 10.14358/PERS.78.10.1079.
   Zhang C, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14040874.
   Zhou DX, 2020, NEURAL NETWORKS, V124, P319, DOI 10.1016/j.neunet.2020.01.018.},
Number-of-Cited-References = {69},
Times-Cited = {0},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Remote Sens.},
Doc-Delivery-Number = {G2VM1},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000987795200001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000304220000006,
Author = {Zhang, Yun and Slaughter, David C. and Staab, Erik S.},
Title = {Robust hyperspectral vision-based classification for multi-season weed
   mapping},
Journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
Year = {2012},
Volume = {69},
Pages = {65-73},
Month = {APR},
Abstract = {This study investigated the robustness of hyperspectral image-based
   plant recognition to seasonal variability in a natural farming
   environment in the context of automated in-row weed control. A machine
   vision system was developed and equipped with a CCD camera integrated
   with a line-imaging spectrograph for close-range weed sensing and
   mapping. Three canonical Bayesian classifiers were developed using
   canopy reflectance (400-795 nm) collected over three seasons for tomato
   and weeds. The performance of the three season-specific classifiers was
   tested by changing environmental conditions, resulting in an increase in
   total error rate of up to 36\%. Global calibration across the complete
   span of the three seasons produced overall classification accuracies of
   85.0\%, 90\% and 92.7\%, respectively, for 2005, 2006 and 2008. To
   improve the stability of global classifier over multiple seasons, a
   multiclassifier system was constructed with three canonical Bayesian
   classifiers optimized for the three seasons individually. This system
   was tested on a data set simulating an upcoming season with field
   conditions similar to that in 2005. The system increased the total
   discrimination accuracy to 95.8\% for the tested season under
   simulation. This method provided an innovative direction for achieving
   robust plant recognition over multiple seasons by integrating expert
   knowledge from historical data that most closely matched the new field
   environment. (C) 2012 International Society for Photogrammetry and
   Remote Sensing, Inc. (ISPRS) Published by Elsevier B.V. All rights
   reserved.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Zhang, Y (Corresponding Author), Univ Calif Davis, Dept Biol \& Agr Engn, 1 Shields Ave, Davis, CA 95616 USA.
   Zhang, Yun; Slaughter, David C.; Staab, Erik S., Univ Calif Davis, Dept Biol \& Agr Engn, Davis, CA 95616 USA.},
DOI = {10.1016/j.isprsjprs.2012.02.006},
ISSN = {0924-2716},
EISSN = {1872-8235},
Keywords = {Computer vision; Plant recognition; Machine learning; Multiclassifier
   system; Weed control; Seasonal variability},
Keywords-Plus = {PLANT-IDENTIFICATION; MACHINE VISION; DISCRIMINATION; DIFFERENTIATION;
   REFLECTANCE; LEAVES; EARTH},
Research-Areas = {Physical Geography; Geology; Remote Sensing; Imaging Science \&
   Photographic Technology},
Web-of-Science-Categories  = {Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science \& Photographic Technology},
Author-Email = {yunzhang@ucdavis.edu},
Affiliations = {University of California System; University of California Davis},
Funding-Acknowledgement = {California Tomato Research Institute},
Funding-Text = {This project has been partially funded by the California Tomato Research
   Institute. The authors also would like to express their gratitude to
   Chris Gliever, Burt Vannucci and Jim Jackson from University of
   California, Davis for providing technical assistance and support.},
Cited-References = {Aitkenhead MJ, 2003, COMPUT ELECTRON AGR, V39, P157, DOI 10.1016/S0168-1699(03)00076-0.
   ANDERSON GL, 1993, WEED TECHNOL, V7, P865, DOI 10.1017/S0890037X00037908.
   Biller RH, 1998, J AGR ENG RES, V71, P357, DOI 10.1006/jaer.1998.0334.
   Borregaard T, 2000, J AGR ENG RES, V75, P389, DOI 10.1006/jaer.1999.0519.
   Brown RB, 2005, WEED SCI, V53, P252, DOI 10.1614/WS-04-068R1.
   Burks TF, 2000, T ASAE, V43, P1029, DOI 10.13031/2013.2971.
   Chi YT, 2003, T ASAE, V46, P175.
   Christensen S, 2009, WEED RES, V49, P233, DOI 10.1111/j.1365-3180.2009.00696.x.
   Everitt B., 1978, GRAPHICAL TECHNIQUES.
   Feyaerts F, 2001, PATTERN RECOGN LETT, V22, P667, DOI 10.1016/S0167-8655(01)00006-X.
   FRANZ E, 1991, T ASAE, V34, P673.
   GOETZ AFH, 1985, SCIENCE, V228, P1147, DOI 10.1126/science.228.4704.1147.
   Goetz AFH, 2009, REMOTE SENS ENVIRON, V113, pS5, DOI 10.1016/j.rse.2007.12.014.
   Govender M, 2007, WATER SA, V33, P145.
   GUYER DE, 1986, T ASAE, V29, P1500.
   GUYOT G, 1990, EAST SCH AG, P19.
   Kutner M. H., 2005, APPL LINEAR STAT MOD.
   Lamb DW, 2001, J AGR ENG RES, V78, P117, DOI 10.1006/jaer.2000.0630.
   Manolakis D, 2002, IEEE SIGNAL PROC MAG, V19, P29, DOI 10.1109/79.974724.
   MENGES RM, 1985, WEED SCI, V33, P569, DOI 10.1017/S0043174500082862.
   Meyer GE, 1998, T ASAE, V41, P1189, DOI 10.13031/2013.17244.
   MURRAY I, 1987, NEAR INFRARED TECHNO, P17.
   PETERS AJ, 1992, WEED TECHNOL, V6, P1015, DOI 10.1017/S0890037X00036642.
   {*}SAS I INC, 2007, SAS STAT US GUID.
   SAVITZKY A, 1964, ANAL CHEM, V36, P1627, DOI 10.1021/ac60214a047.
   Scotford IM, 2005, BIOSYST ENG, V90, P235, DOI 10.1016/j.biosystemseng.2004.11.010.
   SHEARER SA, 1990, T ASAE, V33, P2037.
   Shenk JS., 2008, HDB NEAR INFRARED AN, P347.
   Shiraishi M, 1996, J MANUF SCI E-T ASME, V118, P382, DOI 10.1115/1.2831041.
   Slaughter DC, 2008, COMPUT ELECTRON AGR, V61, P63, DOI 10.1016/j.compag.2007.05.008.
   Slaughter DC, 2008, WEED TECHNOL, V22, P378, DOI 10.1614/WT-07-104.1.
   SPECIM, 2003, IMSP IM SPECTR US MA.
   STEINIER J, 1972, ANAL CHEM, V44, P1906, DOI 10.1021/ac60319a045.
   Stork David G., 2001, PATTERN CLASSIFICATI, V2.
   ULLAH E, 1989, Plant Protection Quarterly, V4, P155.
   {*}USDA, 2010, VEG 2009 SUMM.
   WOEBBECKE DM, 1995, T ASAE, V38, P271, DOI 10.13031/2013.27839.
   Zhang Y, 2011, COMPUT ELECTRON AGR, V77, P95, DOI 10.1016/j.compag.2011.04.001.
   Zhang Y., 2009, P ASABE ANN INT M RE.
   Zwiggelaar R, 1998, CROP PROT, V17, P189, DOI 10.1016/S0261-2194(98)00009-X.},
Number-of-Cited-References = {40},
Times-Cited = {33},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {43},
Journal-ISO = {ISPRS-J. Photogramm. Remote Sens.},
Doc-Delivery-Number = {944QX},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000304220000006},
DA = {2023-08-12},
}

@article{ WOS:001012880500001,
Author = {Borkatulla, Bijly and Ferdous, Jannatul and Uddin, Abdul Hasib and
   Mahmud, Prince},
Title = {Bangladeshi medicinal plant dataset},
Journal = {DATA IN BRIEF},
Year = {2023},
Volume = {48},
Month = {JUN},
Abstract = {Medicinal plants have been used to treat diseases since an-cient times.
   Plants used as raw materials for herbal medicine are known as medicinal
   plants {[}2] . The U. S. Forest Service estimates that 40\% of
   pharmaceutical drugs in the Western world are derived from plants {[}1]
   . Seven thousand medi-cal compounds are derived from plants in the
   modern phar-macopeia. Herbal medicine combines traditional empirical
   knowledge with modern science {[}2] . A medicinal plant is considered an
   important source of prevention against vari-ous diseases {[}2] . The
   essential medicine component is ex-tracted from different parts of the
   plants {[}8] . In underdevel-oped countries, people use medicinal plants
   as a substitute for medicine. There are various species of plants in the
   world. Herbs are one of them, which are of different shapes, col-ors,
   and leaves {[}5] . It is difficult for ordinary people to rec-ognize
   these species of herbs. People use more than 50 0 0 0 plants in the
   world for medicinal purposes. There are 80 0 0 medicinal plants in India
   with evidence of medicinal proper-ties {[}7] . Automatic classification
   of these plant species is im-portant because it requires intensive
   domain knowledge to manually classify the proper species. Machine
   learning tech-niques are extensively used in classifying medicinal plant
   species from photographs, which is challenging but intrigu-ing to
   academics. Artificial Neural Network classifiers' effec-tive performance
   depends on the quality of the image dataset {[}4] . This article
   represents a medicinal plant dataset: an im-age dataset of ten different
   Bangladeshi plant species. Images of medicinal plant leaves were from
   various gardens, includ-ing the Pharmacy Garden at Khwaja Yunus Ali
   University and the Khwaja Yunus Ali Medical College \& Hospital in
   Siraj-ganj, Bangladesh. Images were collected by taking pictures with
   high-resolution mobile phone cameras. Ten medicinal species, 500 images
   per species are included in the data set, namely, Nayantara
   (Catharanthus roseus), Pathor kuchi (Kalanchoe pinnata), Gynura
   procumbens ( Longevity spinach ), Bohera (Terminalia bellirica),
   Haritaki (Terminalia chebula), Thankuni (Centella asiatica), Neem
   (Azadirachta indica), Tulsi ( Ocimum tenniflorum), Lemon grass
   (Cymbopogon citratus), and Devil backbone (Euphorbia tithymaloides).
   This dataset will benefit researchers applying machine learning and
   com-puter vision algorithms in several ways. For example, training and
   evaluation of machine learning models with this well-curated
   high-quality dataset, development of new computer vision algorithms,
   automatic medicinal plant identification in the field of botany and
   pharmacology for drug discovery and conservation, and data augmentation.
   Overall, this medicinal plant image dataset can provide researchers in
   the field of machine learning and computer vision with a valuable
   re-source to develop and evaluate algorithms for plant pheno-typing,
   disease detection, plant identification, drug develop-ment, and other
   tasks related to medicinal plants. \& COPY; 2023 The Author(s).
   Published by Elsevier Inc. This is an open access article under the CC
   BY license ( http://creativecommons.org/licenses/by/4.0/ )},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article; Data Paper},
Language = {English},
Affiliation = {Uddin, AH (Corresponding Author), Khwaja Yunus Ali Univ, Dept Comp Sci \& Engn, Enayetpur 6751, Sirajganj, Bangladesh.
   Borkatulla, Bijly; Uddin, Abdul Hasib; Mahmud, Prince, Khwaja Yunus Ali Univ, Dept Comp Sci \& Engn, Enayetpur 6751, Sirajganj, Bangladesh.
   Ferdous, Jannatul, Jannat Ara Henry Sci \& Technol Coll, Dept Comp Sci \& Engn, Sirajganj, Bangladesh.},
DOI = {10.1016/j.dib.2023.109211},
EarlyAccessDate = {MAY 2023},
Article-Number = {109211},
ISSN = {2352-3409},
Keywords = {Medicinal plant; Image classification; Image processing; DenseNet201;
   Feature visualization},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {abdulhasibuddin@gmail.com},
Cited-References = {{[}Anonymous], MED BOT.
   Ansari A.K., 2015, INT J APPL RES, V1, P148.
   Begue A, 2017, INT J ADV COMPUT SC, V8, P166.
   Bijly B., 2022, KAGGLE, DOI {[}10.34740/KAGGLE/DSV/4510170, DOI 10.34740/KAGGLE/DSV/4510170].
   Herdiyeni Y, 2015, INT CONF SOFT COMPUT, P218, DOI 10.1109/SOCPAR.2015.7492810.
   Ibrahim Z., 2018, P 6 IIAE INT C INT S, P327.
   Manojkumar P, 2017, 2017 THIRD IEEE INTERNATIONAL CONFERENCE ON RESEARCH IN COMPUTATIONAL INTELLIGENCE AND COMMUNICATION NETWORKS (ICRCICN), P231, DOI 10.1109/ICRCICN.2017.8234512.
   Mohammed A.H., 2019, RES PHARM HLTH SCI, V5, P124, DOI DOI 10.32463/RPHS.2019.V05I02.01.
   Ogidan O. K., 2019, THERAPEUTIC PROPERTI, P271.
   Putri Yuanita A., 2021, Journal of Physics: Conference Series, V1845, DOI 10.1088/1742-6596/1845/1/012026.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {11},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Journal-ISO = {Data Brief},
Doc-Delivery-Number = {J9PT6},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:001012880500001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000407661100006,
Author = {Barre, Pierre and Stoever, Ben C. and Mueller, Kai F. and Steinhage,
   Volker},
Title = {LeafNet: A computer vision system for automatic plant species
   identification},
Journal = {ECOLOGICAL INFORMATICS},
Year = {2017},
Volume = {40},
Pages = {50-56},
Month = {JUL},
Abstract = {Aims: Taxon identification is an important step in many plant ecological
   studies. Its efficiency and reproducibility might greatly benefit from
   partly automating this task. Image-based identification systems exist,
   but mostly rely on hand-crafted algorithms to extract sets of features
   chosen a priori to identify species of selected taxa. In consequence,
   such systems are restricted to these taxa and additionally require
   involving experts that provide taxonomical knowledge for developing such
   customized systems. The aim of this study was to develop a deep learning
   system to learn discriminative features from leaf images along with a
   classifier for species identification of plants. By comparing our
   results with customized systems like LeafSnap we can show that learning
   the features by a convolutional neural network (CNN) can provide better
   feature representation for leaf images compared to hand-crafted
   features.
   Methods: We developed Lea/Net, a CNN-based plant identification system.
   For evaluation, we utilized the publicly available LeafSnap, Flavia and
   Foliage datasets.
   Results: Evaluating the recognition accuracies of LeafNet on the
   LeafSnap, Flavia and Foliage datasets reveals a better performance of
   LeafNet compared to hand-crafted customized systems.
   Conclusions: Given the overall species diversity of plants, the goal of
   a complete automatisation of visual plant species identification is
   unlikely to be met solely by continually gathering assemblies of
   customized, specialized and hand-crafted (and therefore expensive)
   identification systems. Deep Learning CNN approaches offer a self
   learning state-of-the-art alternative that allows adaption to different
   taxa just by presenting new training data instead of developing new
   software systems.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Barre, P (Corresponding Author), Univ Bonn, Inst Comp Sci 4, Friedrich Ebert Allee 144, D-53113 Bonn, Germany.
   Barre, Pierre; Steinhage, Volker, Univ Bonn, Inst Comp Sci 4, Friedrich Ebert Allee 144, D-53113 Bonn, Germany.
   Stoever, Ben C.; Mueller, Kai F., Westfalische Wilhelms Univ Munster, Inst Evolut \& Biodivers, Hufferstr 1, D-48149 Munster, Germany.
   Stoever, Ben C.; Mueller, Kai F., Bot Garden, Hufferstr 1, D-48149 Munster, Germany.},
DOI = {10.1016/j.ecoinf.2017.05.005},
ISSN = {1574-9541},
EISSN = {1878-0512},
Keywords = {Plant classification; Feature representation; Deep learning;
   Convolutional neural network; Convolutional layers; Feature maps},
Research-Areas = {Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Ecology},
Author-Email = {barre@cs.uni-bonn.de
   stoever@bioinfweb.info
   kaimueller@uni-muenster.de
   steinhag@cs.uni-bonn.de},
Affiliations = {University of Bonn; University of Munster},
ORCID-Numbers = {Stover, Ben/0000-0003-4443-7733},
Funding-Acknowledgement = {German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) {[}STE
   806/2-1]},
Funding-Text = {This study was partially funded by the German Research Foundation
   (Deutsche Forschungsgemeinschaft, DFG, grant no. STE 806/2-1 to V.S.,
   DFG had no influence on study design).},
Cited-References = {Abdul Kadir, 2014, Research Journal of Pharmaceutical, Biological and Chemical Sciences, V5, P1.
   Ahmed N, 2016, SCI INT, V28, DOI DOI 10.9790/0661-17134853.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Farnsworth EJ, 2013, BIOSCIENCE, V63, P891, DOI 10.1525/bio.2013.63.11.8.
   Goeau H., 2013, P 2 ACM INT WORKSH M, P23.
   Goeau H., 2016, WORKING NOTES CLEF 2, P428.
   Hang S. T., 2016, PLANT IDENTIFICATION.
   Jia Y., 2014, ARXIVPREPRARXIV14085.
   Jin T., 2015, PLOS ONE, V10.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Lu H., 2016, IEEE INT C HIGH VOLT, P1, DOI {[}DOI 10.14257/IJAST.2016.91.01, 10.1109/ICHVE.2016.7800604, DOI 10.1109/ICHVE.2016.7800604].
   MacLeod N, 2010, NATURE, V467, P154, DOI 10.1038/467154a.
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131.
   Sermanet P., 2013, ARXIVPREPRARXIV13126.
   Simonyan K., 2014, ARXIVPREPRARXIV14091.
   Stevenson RD, 2003, CONSERV ECOL, V7.
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.},
Number-of-Cited-References = {25},
Times-Cited = {134},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {47},
Journal-ISO = {Ecol. Inform.},
Doc-Delivery-Number = {FD6TP},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000407661100006},
DA = {2023-08-12},
}

@inproceedings{ WOS:000351597604005,
Author = {Mzoughi, Olfa and Yahiaoui, Itheri and Boujemaa, Nozha and Zagrouba,
   Ezzeddine},
Book-Group-Author = {IEEE},
Title = {ADVANCED TREE SPECIES IDENTIFICATION USING MULTIPLE LEAF PARTS IMAGE
   QUERIES},
Booktitle = {2013 20TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP 2013)},
Series = {IEEE International Conference on Image Processing ICIP},
Year = {2013},
Pages = {3967-3971},
Note = {20th IEEE International Conference on Image Processing (ICIP),
   Melbourne, AUSTRALIA, SEP 15-18, 2013},
Organization = {Inst Elect \& Elect Engineers; IEEE Signal Proc Soc},
Abstract = {There has recently been increasing interest in using advanced computer
   vision techniques for automatic plant identification. Most of the
   approaches proposed are based on an analysis of leaf characteristics.
   Nevertheless, two aspects have still not been well exploited: (1)
   domain-specific or botanical knowledge (2) the extraction of meaningful
   and relevant leaf parts. In this paper, we describe a new automated
   technique for leaf image retrieval that attempts to take these
   particularities into account. The proposed method is based on local
   representation of leaf parts. The part-based decomposition is defined
   and usually used by botanists. The global image query is a combination
   of part sub-images queries. Experiments carried out on real world leaf
   images, the Pl@ntLeaves scan images (3070 images totalling 70 species),
   show an increase in performance compared to global leaf representation.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Mzoughi, O (Corresponding Author), INRIA, Yvelines, France.
   Mzoughi, Olfa; Yahiaoui, Itheri; Boujemaa, Nozha, INRIA, Yvelines, France.
   Mzoughi, Olfa; Zagrouba, Ezzeddine, Univ Tunis El Manar, Intitut Super Informat, SIIVA RIADI, El Manar, Tunisia.
   Yahiaoui, Itheri, Univ Reims, CReSTIC, F-51100 Reims, France.},
ISSN = {1522-4880},
ISBN = {978-1-4799-2341-0},
Keywords = {Plant identification; botanical knowledge; leaf parts; local
   representation; partial similarities},
Keywords-Plus = {SHAPE; MULTISCALE},
Research-Areas = {Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Imaging Science \& Photographic Technology},
Affiliations = {Inria; Universite de Tunis-El-Manar; Universite de Reims
   Champagne-Ardenne},
ResearcherID-Numbers = {zagrouba, ezzeddine/AAO-7281-2020
   Zagrouba, Ezzeddine/D-7896-2014
   mzoughi, olfa/HMW-1127-2023},
ORCID-Numbers = {zagrouba, ezzeddine/0000-0002-2574-9080
   Zagrouba, Ezzeddine/0000-0002-2574-9080
   },
Cited-References = {Agarwal G, 2006, TAXON, V55, P597, DOI 10.2307/25065637.
   Backes AR, 2010, LECT NOTES COMPUT SC, V6134, P463, DOI 10.1007/978-3-642-13681-8\_54.
   Bama B.S., 2011, INDIAN J COMPUTER SC.
   Barthelemy D, 2009, 13 WORLD FOR C.
   Cerutti Guillaume, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P202.
   Cerutti G., 2012, C LABS EV FOR.
   Chi YT, 2003, T ASAE, V46, P175.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Ferecatu M., 2005, THESIS U SAINT QUENT.
   Goeau Herve, 2011, ACM MULT 2011.
   Gouveia F, 1997, ISIE `97 - PROCEEDINGS OF THE IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS, VOLS 1-3, P757, DOI 10.1109/ISIE.1997.648634.
   Hearn DJ, 2009, TAXON, V58, P934, DOI 10.1002/tax.583021.
   Kapila P., 2008, THESIS.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Leaf Architecture Working Group, 1999, MANUAL LEAF ARCHITEC.
   MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591.
   Mouine S., 2012, P 2 ACM INT C MULT R.
   Mzoughi Olfa, 2012, ICIAR.
   Nam Y, 2005, LECT NOTES COMPUT SC, V3815, P139.
   Nam Y, 2008, COMPUT VIS IMAGE UND, V110, P245, DOI 10.1016/j.cviu.2007.08.002.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Prasad S., 2011, P 2011 INT C COMM CO, P343, DOI {[}10.1145/1947940.1948012, DOI 10.1145/1947940.1948012].
   Qian RJ, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P125, DOI 10.1109/ICME.2000.869560.
   Tanase Mirela, 2005, P 13 ANN ACM INT C M.
   TRAISER C, 2005, ENV SIGNALS LEAVES P, V166, P465.
   WANG SH, 2006, ISWC, V4273, P668.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yahiaoui Itheri, 2012, ICME2012.
   Ye LX, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P947.},
Number-of-Cited-References = {31},
Times-Cited = {10},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BC3FD},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000351597604005},
DA = {2023-08-12},
}

@inproceedings{ WOS:000524284000107,
Author = {Feng, Jing and Wang, Zhiwen and Zha, Min and Cao, Xinliang},
Book-Group-Author = {ACM},
Title = {Flower Recognition Based on Transfer Learning and Adam Deep Learning
   Optimization Algorithm},
Booktitle = {PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON ROBOTICS,
   INTELLIGENT CONTROL AND ARTIFICIAL INTELLIGENCE (RICAI 2019)},
Year = {2019},
Pages = {598-604},
Note = {International Conference on Robotics, Intelligent Control and Artificial
   Intelligence (RICAI), Shanghai Univ Engn Sci, Shanghai, PEOPLES R CHINA,
   SEP 20-22, 2019},
Organization = {Natl Robot Test \& Assessment Ctr; Soochow Univ; Shanghai Robot Indl
   Technol Res Inst; Shanghai Jiaotong Univ, Robot Inst; Ind Alliance
   Intelligent Mfg \& Robot; Innovat Alliance Robot Ind Platform; Global
   Sci Res Assoc},
Abstract = {Due to the complex background of flowers and the similarity between
   their own categories, the traditional method of image recognition is to
   extract features manually, which can not solve this problem well. With
   the development and progress of science and technology, deep learning
   has gradually entered the image recognition problem and achieved good
   results. This paper proposes the flower recognition based on transfer
   learning and Adam deep learning optimization algorithm for the defects
   of the current mainstream convolutional neural network with deep depth
   and long parameters, long training time and slow convergence. The VGG16
   model is modified and supplemented. At the same time, the transfer
   learning method and the Adam optimization algorithm are used to
   accelerate network convergence. Thirty kinds of flower image data sets
   were established by 102 Category Flower Dataset partial images and 17
   Category Flower Dataset. The experimental results show that the accuracy
   of the test set in this paper is 98.99\%. Compared with the traditional
   image recognition algorithm, it has the characteristics of fast
   convergence and high recognition accuracy.},
Publisher = {ASSOC COMPUTING MACHINERY},
Address = {1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Feng, J (Corresponding Author), Guangxi Univ Sci \& Technol, Elect \& Informat Engn, Liu Zhou, Guangxi, Peoples R China.
   Feng, Jing; Wang, Zhiwen; Zha, Min; Cao, Xinliang, Guangxi Univ Sci \& Technol, Elect \& Informat Engn, Liu Zhou, Guangxi, Peoples R China.},
DOI = {10.1145/3366194.3366301},
ISBN = {978-1-4503-7298-5},
Keywords = {Deep learning; Transfer learning; VGG16; Flower recognition},
Research-Areas = {Automation \& Control Systems; Computer Science; Robotics},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Artificial
   Intelligence; Robotics},
Author-Email = {980400464@qq.com
   wzw69@126.com
   771333396@qq.com
   18860478731@163.com},
Affiliations = {Guangxi University of Science \& Technology},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}61462008, 61751213,
   61866004, 61962007]; Key projects of Guangxi Natural Science Foundation
   {[}2018GXNSFDA294001, 2018GXNSFDA281009]; Natural Science Foundation of
   Guangxi {[}2018GXNSFAA294050, 2017GXNSFAA198365]; 2015 Innovation Team
   Project of Guangxi University of Science and Technology
   {[}gxkjdx201504]; Innovation Project for College Students of Guangxi
   University of Science and Technology {[}GKYC201708]; Innovation Project
   of Guangxi Graduate Education {[}YCSW2019206]},
Funding-Text = {The authors are very grateful for the support provided by the National
   Natural Science Foundation of China (61462008, 61751213, 61866004,
   61962007), the Key projects of Guangxi Natural Science Foundation
   (2018GXNSFDA294001, 2018GXNSFDA281009), the Natural Science Foundation
   of Guangxi (2018GXNSFAA294050, 2017GXNSFAA198365), 2015 Innovation Team
   Project of Guangxi University of Science and Technology(gxkjdx201504),
   Innovation Project for College Students of Guangxi University of Science
   and Technology (GKYC201708), Innovation Project of Guangxi Graduate
   Education(YCSW2019206),},
Cited-References = {{[}常亮 Chang Liang], 2016, {[}自动化学报, Acta Automatica Sinica], V42, P1300.
   Girshick R., 2014, PROC IEEE C COMPUT V, P580, DOI DOI 10.1109/CVPR.2014.81.
   {[}李旭冬 Li Xudong], 2017, {[}计算机应用研究, Application Research of Computers], V34, P2881.
   Li Yandong, 2016, Journal of Computer Applications, V36, P2508, DOI 10.11772/j.issn.1001-9081.2016.09.2508.
   Liu Fangyuan, 2017, NEW IND, V7, P40.
   Miao Jinquan, 2014, J IMAGE GRAPHICS, V19, P1630.
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Xia Wei, 2018, J COMPUTER APPL, V38, P2141.
   Xu Xudong, 2018, COMPUT APPL, V38, P290.
   Yang Ge, 2015, APPL ELECT TECHNIQUE, V41, P120.
   Yin Bao-cai, 2015, Journal of Beijing University of Technology, V41, P48, DOI 10.11936/bjutxb2014100026.
   Zhai J., 2017, J HEBEI UNIV NAT SCI, V37, P640, DOI DOI 10.3969/j.issn.1000-1565.2017.06.012.
   Zhou Junyu, 2017, Computer Engineering and Applications, V53, P34, DOI 10.3778/j.issn.1002-8331.1703-0362.
   Zhou Z.H., 2016, MACH LEARN, P121.},
Number-of-Cited-References = {15},
Times-Cited = {8},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {9},
Doc-Delivery-Number = {BO7GP},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000524284000107},
DA = {2023-08-12},
}

@inproceedings{ WOS:000412179000058,
Author = {Lavania, Shubham and Matey, Palash Sushil},
Editor = {Krishnan, N and Karthikeyan, M},
Title = {Leaf Recognition using Contour based Edge Detection and SIFT Algorithm},
Booktitle = {2014 IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND
   COMPUTING RESEARCH (IEEE ICCIC)},
Series = {IEEE International Conference on Computational Intelligence and
   Computing Research},
Year = {2014},
Pages = {275-278},
Note = {5th IEEE International Conference on Computational Intelligence and
   Computing Research (IEEE ICCIC), Park Coll Engn \& Tekhnol, Coimbatore,
   INDIA, DEC 18-20, 2014},
Organization = {IEEE; IEEE Podhigai Subsect; IEEE SIPCICOM; PARK Grp Inst},
Abstract = {The paper presents two advanced methods for comparative study in the
   field of computer vision. The first method involves the implementation
   of the Scalar Invariant Fourier Transform (SIFT) algorithm for the leaf
   recognition based on the key descriptors value. The second method
   involves the contour-based corner detection and classification which is
   done with the help of Mean Projection algorithm. The advantage of this
   system over the other Curvature Scale Space (CSS) systems is that there
   are fewer false-positive (FP) and false-negative (FN) points compared
   with recent standard corner detection techniques. The performance
   analysis of both the algorithm was done on the flavia database.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Lavania, S (Corresponding Author), VIT Univ, Sch Elect Engn, Vellore, Tamil Nadu, India.
   Lavania, Shubham; Matey, Palash Sushil, VIT Univ, Sch Elect Engn, Vellore, Tamil Nadu, India.},
ISSN = {2471-7851},
ISBN = {978-1-4799-3975-6},
Keywords = {SIFT; corner detection; contour-based corner detector; mean projection
   transform; leaf recognition},
Keywords-Plus = {CORNER DETECTION; CLASSIFICATION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications; Computer Science,
   Theory \& Methods; Engineering, Electrical \& Electronic},
Author-Email = {shubham.lavania2011@vit.ac.in
   mateypalash.sushil2011@vit.ac.in},
Affiliations = {Vellore Institute of Technology (VIT); VIT Vellore},
Cited-References = {Abbasi S, 1997, LECT NOTES COMPUT SC, V1252, P284.
   Awrangjeb M, 2008, THESIS.
   Awrangjeb M, 2012, IEEE T IMAGE PROCESS, V21, P4167, DOI 10.1109/TIP.2012.2200493.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Clarke J, 2006, LECT NOTES COMPUT SC, V4292, P427.
   de la Escalera A, 2010, SENSORS-BASEL, V10, P2027, DOI 10.3390/s100302027.
   Du J. X., 2007, APPL MATH COMPUTATIO, V185.
   Forlenza L, 2012, SENSORS-BASEL, V12, P863, DOI 10.3390/s120100863.
   Fu H, 2006, IEE P-VIS IMAGE SIGN, V153, P881, DOI 10.1049/ip-vis:20060061.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   Kahaki SMM, 2014, SENSORS-BASEL, V14, P4126, DOI 10.3390/s140304126.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188.
   Mokhtarian F, 1998, IEEE T PATTERN ANAL, V20, P1376, DOI 10.1109/34.735812.
   Olague G, 2005, PATTERN RECOGN LETT, V26, P27, DOI 10.1016/j.patrec.2004.08.026.
   Papadimitriou CH., 1998, COMBINATORIAL OPTIMI.
   Shui PL, 2013, IEEE T IMAGE PROCESS, V22, P3204, DOI 10.1109/TIP.2013.2259834.
   Sinzinger ED, 2008, PATTERN RECOGN, V41, P494, DOI 10.1016/j.patcog.2007.06.032.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Ye Y., 2004, P 2004 INT S INT MUL.
   Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9.},
Number-of-Cited-References = {22},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BI5AB},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000412179000058},
DA = {2023-08-12},
}

@inproceedings{ WOS:000678409202097,
Author = {Parashar, J. and Bhandarkar, S. M. and Simon, J. and Hopkinson, B. M.
   and Pennings, S. C.},
Book-Group-Author = {IEEE COMP SOC},
Title = {Estimation of Abundance and Distribution of Salt Marsh Plants from
   Images Using Deep Learning},
Booktitle = {2020 25TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)},
Series = {International Conference on Pattern Recognition},
Year = {2021},
Pages = {2635-2642},
Note = {25th International Conference on Pattern Recognition (ICPR), ELECTR
   NETWORK, JAN 10-15, 2021},
Organization = {Int Assoc Pattern Recognit; IEEE Comp Soc; Italian Assoc Comp Vis
   Pattern Recognit \& Machine Learning},
Abstract = {Recent advances in computer vision and machine learning, most notably
   deep convolutional neural networks (CNNs), are exploited to identify and
   localize various plant species in salt marsh images. Three different
   approaches are explored that provide estimations of abundance and
   spatial distribution at varying levels of granularity defined by spatial
   resolution. In the coarsest-grained approach, CNNs are tasked with
   identifying which of six plant species are present/absent in large
   patches within the salt marsh images. CNNs with diverse topological
   properties and attention mechanisms are shown capable of providing
   accurate estimations with > 90\% precision and recall for the more
   abundant plant species and reduced performance for less common plant
   species. Estimation of percent cover of each plant species is performed
   at a finer spatial resolution, where smaller image patches are extracted
   and the CNNs tasked with identifying the plant species or substrate at
   the center of the image patch. For the percent cover estimation task,
   the CNNs are observed to exhibit a performance profile similar to that
   for the presence/absence estimation task, but with an approximate to
   5\%-10\% reduction in precision and recall. Finally, fine-grained
   estimation of the spatial distribution of the various plant species is
   performed via semantic segmentation. The DeepLab-V3 semantic
   segmentation architecture is observed to provide very accurate
   estimations for abundant plant species, but with significant performance
   degradation for less abundant plant species; in extreme cases, rare
   plant classes are seen to be ignored entirely. Overall, a clear
   trade-off is observed between the CNN estimation quality and the spatial
   resolution of the underlying estimation thereby offering guidance for
   ecological applications of CNN-based approaches to automated plant
   identification and localization in salt marsh images.},
Publisher = {IEEE COMPUTER SOC},
Address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Parashar, J (Corresponding Author), Univ Georgia, Inst AI, Athens, GA 30602 USA.
   Parashar, J.; Bhandarkar, S. M., Univ Georgia, Inst AI, Athens, GA 30602 USA.
   Bhandarkar, S. M., Univ Georgia, Dept Comp Sci, Athens, GA 30602 USA.
   Simon, J.; Hopkinson, B. M., Univ Georgia, Dept Marine Sci, Athens, GA 30602 USA.
   Pennings, S. C., Univ Houston, Dept Biol \& Biochem, Houston, TX 77204 USA.},
DOI = {10.1109/ICPR48806.2021.9412264},
ISSN = {1051-4651},
ISBN = {978-1-7281-8808-9},
Keywords = {Salt marsh monitoring; convolutional neural networks; network topology;
   attention mechanism; deep learning; ecological monitoring},
Research-Areas = {Computer Science; Engineering; Imaging Science \& Photographic
   Technology},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic; Imaging Science \& Photographic Technology},
Author-Email = {jayant.parashar@uga.edu
   suchi@uga.edu
   jacob.simon25@uga.edu
   bmhopkin@uga.edu
   scpennin@central.uh.edu},
Affiliations = {University System of Georgia; University of Georgia; University System
   of Georgia; University of Georgia; University System of Georgia;
   University of Georgia; University of Houston System; University of
   Houston},
ResearcherID-Numbers = {Pennings, Steven C/A-8326-2013
   Parashar, Jetendra/ACZ-4927-2022},
ORCID-Numbers = {Pennings, Steven C/0000-0003-4757-7125
   Parashar, Jetendra/0000-0002-1505-3960},
Funding-Acknowledgement = {National Science Foundation {[}DBI-2016741]; Georgia Coastal Ecosystems
   Long-Term Ecological Research program {[}OCE-1832178]},
Funding-Text = {The research described in this paper was supported by the National
   Science Foundation under Grant No. DBI-2016741 and the Georgia Coastal
   Ecosystems Long-Term Ecological Research program under Grant No.
   OCE-1832178.},
Cited-References = {Ayrey E, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10040649.
   Beaulieu S, 2020, FRONT ARTIF INTEL AP, V325, P992, DOI 10.3233/FAIA200193.
   Beijbom O, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130312.
   Beijbom O, 2012, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2012.6247798.
   BERTNESS MD, 1987, ECOL MONOGR, V57, P129, DOI 10.2307/1942621.
   Bowley C, 2016, P IEEE INT C E-SCI, P251, DOI 10.1109/eScience.2016.7870906.
   Brodrick PG, 2019, TRENDS ECOL EVOL, V34, P734, DOI 10.1016/j.tree.2019.03.006.
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Du S, 2017, CHIN CONTR CONF, P4470, DOI 10.23919/ChiCC.2017.8028062.
   He K, 2015, C COMPUTER VISION PA.
   Heinzel J, 2012, INT J APPL EARTH OBS, V18, P101, DOI 10.1016/j.jag.2012.01.025.
   Huang G., 2017, P 2017 IEEE C COMP V, P4700, DOI {[}DOI 10.1109/CVPR.2017.243, 10.1109/CVPR.2017.243].
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6.
   King A, 2018, P IEEE C COMP VIS PA, P1394, DOI DOI 10.1109/CVPRW.2018.00188.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541.
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI {[}10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954].
   Mao HZ, 2017, IEEE COMPUT SOC CONF, P1927, DOI 10.1109/CVPRW.2017.241.
   Mcleod E, 2011, FRONT ECOL ENVIRON, V9, P552, DOI 10.1890/110004.
   Mitsch W.J., 2000, WETLANDS, Vthird.
   Pennings SC, 2005, J ECOL, V93, P159, DOI 10.1111/j.1365-2745.2004.00959.x.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Soltani R., 2016, ARXIV PREPRINT ARXIV.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Touvron Hugo, 2019, ADV NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1906.06423.
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683.
   Wasson K, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2813.
   Weinstein BG, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111309.
   Weinstein BG, 2018, METHODS ECOL EVOL, V9, P1435, DOI 10.1111/2041-210X.13011.
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780.
   Williams ID, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00222.
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634.},
Number-of-Cited-References = {34},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {6},
Doc-Delivery-Number = {BR9MO},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000678409202097},
DA = {2023-08-12},
}

@article{ WOS:000525324500011,
Author = {Espejo-Garcia, Borja and Mylonas, Nikos and Athanasakos, Loukas and
   Fountas, Spyros and Vasilakoglou, Ioannis},
Title = {Towards weeds identification assistance through transfer learning},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2020},
Volume = {171},
Month = {APR},
Abstract = {Reducing the use of pesticides through selective spraying is an
   important component towards a more sustainable computer-assisted
   agriculture. Weed identification at early growth stage contributes to
   reduced herbicide rates. However, while computer vision alongside deep
   learning have overcome the performance of approaches that use
   hand-crafted features, there are still some open challenges in the
   development of a reliable automatic plant identification system. These
   type of systems have to take into account different sources of
   variability, such as growth stages and soil conditions, with the added
   constraint of the limited size of usual datasets. This study proposes a
   novel crop/weed identification system that relies on a combination of
   fine-tuning pre-trained convolutional networks (Xception,
   Inception-Resnet, VGNets, Mobilenet and Densenet) with the
   ``traditional{''} machine learning classifiers (Support Vector Machines,
   XGBoost and Logistic Regression) trained with the previously deep
   extracted features. The aim of this approach was to avoid overfitting
   and to obtain a robust and consistent performance. To evaluate this
   approach, an open access dataset of two crop {[}tomato (Solanum
   lycopersicum L.) and cotton (Gossypium hirsutum L.)] and two weed
   species {[}black nightshade (Solanum nigrum L.) and velvetleaf (Abutilon
   theophrasti Medik.)] was generated. The pictures were taken by different
   production sites across Greece under natural variable light conditions
   from RGB cameras. The results revealed that a combination of fine-tuned
   Densenet and Support Vector Machine achieved a micro F-1 score of
   99.29\% with a very low performance difference between train and test
   sets. Other evaluated approaches also obtained repeatedly more than 95\%
   F-1 score. Additionally, our results analysis provides some heuristics
   for designing transfer-learning based systems to avoid overfitting
   without decreasing performance.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Espejo-Garcia, B (Corresponding Author), Agr Univ Athens, Athens, Greece.
   Espejo-Garcia, Borja; Mylonas, Nikos; Athanasakos, Loukas; Fountas, Spyros, Agr Univ Athens, Athens, Greece.
   Vasilakoglou, Ioannis, Inst Thessaly, Thessaly, Greece.},
DOI = {10.1016/j.compag.2020.105306},
Article-Number = {105306},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Weed identification; Deep learning; Transfer learning; Open data;
   Precision agriculture},
Keywords-Plus = {DISEASE DETECTION; NEURAL-NETWORKS; SUGAR-BEET; CLASSIFICATION},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {borjaeg@aua.gr},
Affiliations = {Agricultural University of Athens},
Funding-Acknowledgement = {Government of Aragon; European Social Fund {[}C38/2015]; Spanish
   Government {[}TIN2017-88002R]; Aragon Government; EU FEDER program
   {[}T59\_17R]; Corteva Agriscience(TM)},
Funding-Text = {The work of Borja Espejo-Garcia has been partially supported by the
   Government of Aragon and the European Social Fund through the grant
   number {[}C38/2015]. This work has also been supported by the Spanish
   Government (project TIN2017-88002R); and Aragon Government and EU FEDER
   program project T59\_17R. Field surveys and agronomic support has been
   kindly sponsored by Corteva Agriscience (TM).},
Cited-References = {{[}Anonymous], P IEEE INT C ROB AUT.
   {[}Anonymous], 2016 IEEE INT C IM S.
   {[}Anonymous], 2008, ELEMENTS STAT LEARNI.
   {[}Anonymous], NATURE, DOI 10.1038/nature14539.
   {[}Anonymous], 2018, FUNDAMENTALS WEED SC, DOI DOI 10.1016/B978-0-12-811143-7.00014-7.
   {[}Anonymous], BIOSYST ENG.
   {[}Anonymous], AUTOMATION ICRA.
   {[}Anonymous], NIPS.
   {[}Anonymous], 2017, COMPUTER VISION PATT.
   {[}Anonymous], 8 INT C AGR.
   {[}Anonymous], EUR C COMP VIS.
   {[}Anonymous], STAT FOOD AGR FOOD S.
   {[}Anonymous], 2014, IEEE WINTER C APPL C.
   {[}Anonymous], 32 INT FLOR ART INT.
   {[}Anonymous], 2017, EUR CROP PROT PEST.
   {[}Anonymous], 1995, TECHNICAL REPORT.
   Bakhshipour A, 2018, COMPUT ELECTRON AGR, V145, P153, DOI 10.1016/j.compag.2017.12.032.
   Barbedo JGA, 2018, BIOSYST ENG, V172, P84, DOI 10.1016/j.biosystemseng.2018.05.013.
   Bengio Yoshua, 2012, P ICML WORKSH UNS TR, P17.
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Dimitrakopoulos GN, 2018, 10TH HELLENIC CONFERENCE ON ARTIFICIAL INTELLIGENCE (SETN 2018), DOI 10.1145/3200947.3201029.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Gebbers R, 2010, SCIENCE, V327, P828, DOI 10.1126/science.1183899.
   Holland O, 2016, 2016 23RD INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT), DOI 10.1109/ICT.2016.7500442.
   Howard A.G., 2017, ARXIV170404861 CS.
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.
   Kitzes J, 2008, PHILOS T R SOC B, V363, P467, DOI 10.1098/rstb.2007.2164.
   Kleinbaum D. G., 1994, LOGISTIC REGRESSION.
   Kounalakis T, 2019, COMPUT ELECTRON AGR, V165, DOI {[}10.1016/j.compag.2019.104973, 10.1016/j.compag.201].
   Kounalakis T, 2018, MULTIMED TOOLS APPL, V77, P9567, DOI 10.1007/s11042-017-5337-y.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   Lottes P, 2017, J FIELD ROBOT, V34, P1160, DOI 10.1002/rob.21675.
   McCool C, 2017, IEEE ROBOT AUTOM LET, V2, P1344, DOI 10.1109/LRA.2017.2667039.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Oerke EC, 2006, J AGR SCI-CAMBRIDGE, V144, P31, DOI 10.1017/S0021859605005708.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Potena C, 2017, ADV INTELL SYST COMP, V531, P105, DOI 10.1007/978-3-319-48036-7\_9.
   Rizzardi Mauro Antônio, 2004, Cienc. Rural, V34, P13, DOI 10.1590/S0103-84782004000100003.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Suh HK, 2018, BIOSYST ENG, V174, P50, DOI 10.1016/j.biosystemseng.2018.06.017.
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   United Nations, 2019, WORLD POPULATION PRO.
   Wang G, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/2917536.
   WOEBBECKE DM, 1995, T ASAE, V38, P259, DOI 10.13031/2013.27838.
   Zhou R, 2014, COMPUT ELECTRON AGR, V108, P58, DOI 10.1016/j.compag.2014.07.004.},
Number-of-Cited-References = {47},
Times-Cited = {84},
Usage-Count-Last-180-days = {8},
Usage-Count-Since-2013 = {72},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {LC4VS},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000525324500011},
DA = {2023-08-12},
}

@inproceedings{ WOS:000651422600052,
Author = {Wang, Qiyao and He, Guiqing and Li, Feng and Zhang, Haixi},
Book-Group-Author = {IEEE},
Title = {A novel database for plant diseases and pests classification},
Booktitle = {2020 IEEE INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATIONS
   AND COMPUTING (IEEE ICSPCC 2020)},
Year = {2020},
Note = {10th IEEE International Conference on Signal Processing, Communications
   and Computing (IEEE ICSPCC), Univ Macau, Macau, PEOPLES R CHINA, AUG
   21-23, 2020},
Organization = {IEEE; IEEE Xian Sect; IEEE Hong Kong Sect; IEEE ComSoc Chapter; NW
   Polytechn Univ},
Abstract = {In agricultural field, the research, detection, and treatment of plant
   diseases and pests play a very important role. Prevention or early
   treatment of diseases and pests can significantly increase crop yields.
   With rich variety, plants form a mature hierarchical structure based on
   taxonomic methods. Thus, in the process of computer vision, plant
   classification and identification have attracted many researchers, and
   then the detection system of plant diseases and pests came into being.
   In this paper, the keyword retrieval is used to obtain images of plant
   diseases and pests from the keyword search engine to build a novel
   database. After that, a hierarchical multitask learning is proposed to
   classify plant diseases and pests by leveraging the relationship between
   different plant species and pests. The experimental results confirmed
   the feasibility and reliability of the classification of plant diseases
   and pests using deep learning model.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {He, GQ (Corresponding Author), Northwestern Polytech Univ, Sch Elect \& Informat, Xian, Peoples R China.
   Wang, Qiyao; He, Guiqing; Li, Feng; Zhang, Haixi, Northwestern Polytech Univ, Sch Elect \& Informat, Xian, Peoples R China.},
ISBN = {978-1-7281-7201-9},
Keywords = {Plant Diseases and Pests Database; Deep Learning; Hierarchical
   Multi-task Learning},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {wang\_qiyao@mail.nwpu.edu.cn
   guiqing\_he@nwpu.edu.cn
   feng\_li@mail.nwpu.edu.cn
   zh.haixi@gmail.com},
Affiliations = {Northwestern Polytechnical University},
ResearcherID-Numbers = {Zhu, Jie/HPI-1935-2023
   Zhang, Haixi/HGB-7131-2022
   wang, qi/IAN-4150-2023
   LI, QI/IUM-8577-2023},
ORCID-Numbers = {wang, qi/0000-0002-2794-6897
   },
Funding-Acknowledgement = {National Nature Science Foundation of China {[}61402368]; Aerospace
   Science and Technology Innovation Foundation of China {[}2017ZD53047];
   Aerospace Support Foundation of China {[}2019-HT-XGD]; Key Research and
   Development Project of Shaanxi Province {[}2018GY-038]},
Funding-Text = {This research was funded by the National Nature Science Foundation of
   China (NO.61402368), Aerospace Science and Technology Innovation
   Foundation of China (NO. 2017ZD53047), Aerospace Support Foundation of
   China (NO. 2019-HT-XGD), Key Research and Development Project of Shaanxi
   Province(NO. 2018GY-038).},
Cited-References = {Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3\_7.
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.
   Jagadeesh Vignesh, 2016, US Patent App, Patent No. {[}14/582,059, 14582059].
   Johannes A, 2017, COMPUT ELECTRON AGR, V138, P200, DOI 10.1016/j.compag.2017.04.013.
   Krizhevsky, 2014, ARXIV PREPRINT ARXIV.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kuang ZZ, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P310, DOI 10.1109/BigMM.2017.72.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   Lu J, 2017, COMPUT ELECTRON AGR, V142, P369, DOI 10.1016/j.compag.2017.09.012.
   Lu Y, 2017, NEUROCOMPUTING, V267, P378, DOI 10.1016/j.neucom.2017.06.023.
   Roy D., 2018, ARXIV PREPRINT ARXIV.
   Shun Zhang, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9916, P497, DOI 10.1007/978-3-319-48890-5\_49.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Singh Vijai, 2017, Information Processing in Agriculture, V4, P41, DOI 10.1016/j.inpa.2016.10.005.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Too EC, 2019, COMPUT ELECTRON AGR, V161, P272, DOI 10.1016/j.compag.2018.03.032.
   Wang C, 2017, IEEE INT CONF COMP V, P1907, DOI 10.1109/ICCVW.2017.225.},
Number-of-Cited-References = {22},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BR4HU},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000651422600052},
DA = {2023-08-12},
}

@article{ WOS:000741200400008,
Author = {Kainat, Jaweria and Ullah, Syed Sajid and Alharithi, Fahd S. and
   Alroobaea, Roobaea and Hussain, Saddam and Nazir, Shah},
Title = {Blended Features Classification of Leaf-Based Cucumber Disease Using
   Image Processing Techniques},
Journal = {COMPLEXITY},
Year = {2021},
Volume = {2021},
Month = {DEC 30},
Abstract = {Existing plant leaf disease detection approaches are based on features
   of extracting algorithms. These algorithms have some limits in feature
   selection for the diseased portion, but they can be used in conjunction
   with other image processing methods. Diseases of a plant can be
   classified from their symptoms. We proposed a cucumber leaf recognition
   approach, consisting of five steps: preprocessing, normalization,
   features extraction, features fusion, and classification. Otsu's
   thresholding is implemented in preprocessing and Tan-Triggs
   normalization is applied for normalizing the dataset. During the
   features extraction step, texture and shape features are extracted. In
   addition, increasing the instances improves some characteristics.
   Through a principal component analysis approach, serial feature fusion
   is employed to provide a feature score. Fused features can be classified
   through a support vector machine. The accuracy of the Fine KNN is
   94.30\%, which is higher than the previous work in past papers.},
Publisher = {WILEY-HINDAWI},
Address = {ADAM HOUSE, 3RD FL, 1 FITZROY SQ, LONDON, WIT 5HE, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Hussain, S (Corresponding Author), Univ Brunei Darussalam, Sch Digital Sci, Jalan Tungku Link, BE-1410 Gadong, Brunei.
   Nazir, S (Corresponding Author), Univ Swabi, Dept Comp Sci, Swabi, Khyber Pakhtunk, Pakistan.
   Kainat, Jaweria, COMSATS Univ Islamabad, Dept Comp Sci, Wah Cantt, Pakistan.
   Ullah, Syed Sajid, Villanova Univ, Dept Elect \& Comp Engn, Villanova, PA 19085 USA.
   Alharithi, Fahd S.; Alroobaea, Roobaea, Taif Univ, Coll Comp \& Informat Technol, Dept Comp Sci, POB 11099, At Taif 21944, Saudi Arabia.
   Hussain, Saddam, Univ Brunei Darussalam, Sch Digital Sci, Jalan Tungku Link, BE-1410 Gadong, Brunei.
   Nazir, Shah, Univ Swabi, Dept Comp Sci, Swabi, Khyber Pakhtunk, Pakistan.},
DOI = {10.1155/2021/9736179},
Article-Number = {9736179},
ISSN = {1076-2787},
EISSN = {1099-0526},
Keywords-Plus = {LOCAL BINARY PATTERNS; SYSTEMIC RESISTANCE; MILDEW DEVELOPMENT;
   RECOGNITION; LEAVES; SEGMENTATION; INDUCTION; FUSION; SPOT},
Research-Areas = {Mathematics; Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences},
Author-Email = {saddamicup1993@gmail.com
   shahnazir@uoswabi.edu.pk},
Affiliations = {COMSATS University Islamabad (CUI); Villanova University; Taif
   University; University Brunei Darussalam},
ResearcherID-Numbers = {Hussain, Saddam/AAU-7597-2020
   Nazir, Shah/D-2020-2015
   Alroobaea, Roobaea/M-3894-2019
   Sajid Ullah, Syed/AAU-7591-2020
   Alharithi, Fahd S/ABF-1981-2021},
ORCID-Numbers = {Hussain, Saddam/0000-0003-1523-1330
   Nazir, Shah/0000-0003-0126-9944
   Alroobaea, Roobaea/0000-0003-1585-2962
   Sajid Ullah, Syed/0000-0002-5406-0389
   Alharithi, Fahd S/0000-0003-2166-8168},
Funding-Acknowledgement = {Taif University, Taif, Saudi Arabia {[}TURSP-2020/347]},
Funding-Text = {This study was supported by Taif University Researchers Supporting
   Project (no. TURSP-2020/347), Taif University, Taif, Saudi Arabia.},
Cited-References = {Chemura A, 2018, TROP PLANT PATHOL, V43, P117, DOI 10.1007/s40858-017-0187-8.
   DAAYF F, 1995, PLANT DIS, V79, P577, DOI 10.1094/PD-79-0577.
   GOTTSTEIN HD, 1989, PHYTOPATHOLOGY, V79, P176, DOI 10.1094/Phyto-79-176.
   Kwon MK, 2003, PLANT PATHOL, V52, P424, DOI 10.1046/j.1365-3059.2003.00846.x.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Li W, 2015, IEEE T GEOSCI REMOTE, V53, P3681, DOI 10.1109/TGRS.2014.2381602.
   Lindenthal M, 2005, PHYTOPATHOLOGY, V95, P233, DOI 10.1094/PHYTO-95-0233.
   Liu J, 2021, PLANT METHODS, V17, DOI 10.1186/s13007-021-00722-9.
   LIU L, 1995, PHYTOPATHOLOGY, V85, P843, DOI 10.1094/Phyto-85-843.
   Ma JC, 2018, COMPUT ELECTRON AGR, V154, P18, DOI 10.1016/j.compag.2018.08.048.
   Mehta R, 2016, PATTERN RECOGN LETT, V71, P16, DOI 10.1016/j.patrec.2015.11.019.
   Ngugi Lawrence C., 2021, Information Processing in Agriculture, V8, P27, DOI 10.1016/j.inpa.2020.04.004.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Pantazi XE, 2019, COMPUT ELECTRON AGR, V156, P96, DOI 10.1016/j.compag.2018.11.005.
   Pantazi XE, 2016, IFIP ADV INF COMM TE, V475, P319, DOI 10.1007/978-3-319-44944-9\_27.
   Sharif M, 2018, COMPUT ELECTRON AGR, V150, P220, DOI 10.1016/j.compag.2018.04.023.
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645.
   Tsamardinos I, 2018, MACH LEARN, V107, P1895, DOI 10.1007/s10994-018-5714-4.
   Wang ZJ, 2005, IEEE T GEOSCI REMOTE, V43, P1391, DOI 10.1109/TGRS.2005.846874.
   Xia ZH, 2018, IEEE ACCESS, V6, P30392, DOI 10.1109/ACCESS.2018.2845456.
   Xie CQ, 2017, COMPUT ELECTRON AGR, V135, P154, DOI 10.1016/j.compag.2016.12.015.
   Xie WD, 2018, COMP M BIO BIO E-IV, V6, P283, DOI 10.1080/21681163.2016.1149104.
   Zapata PAM, 2021, BIOINFORMATICS, V37, P861, DOI 10.1093/bioinformatics/btaa905.
   Zhang SW, 2018, OPTIK, V157, P866, DOI 10.1016/j.ijleo.2017.11.190.
   Zhang SW, 2016, NEUROCOMPUTING, V205, P341, DOI 10.1016/j.neucom.2016.04.034.},
Number-of-Cited-References = {25},
Times-Cited = {6},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {13},
Journal-ISO = {Complexity},
Doc-Delivery-Number = {YE5YH},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000741200400008},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000939106800009,
Author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M. T.},
Title = {Deep-Morpho Algorithm (DMA) for medicinal leaves features extraction},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2023},
Volume = {82},
Number = {18},
Pages = {27905-27925},
Month = {JUL},
Abstract = {Presently, for the identification and classification of images, various
   deep learning techniques are being used. In these techniques, the whole
   image is considered to produce similar feature sets for many images. As
   a result, this mechanism loses many of its features at the final stage.
   Therefore, to analyze and identify medicinal leaves through an
   artificial eye of botanists, it was emphasized that the leaf image
   features should remain preserved till the final stage of classification
   for better accuracy. The existing plant identification approaches are
   trained using the leaf images. So leaf features are lost in the
   different stages of the convolution process and the same feature values
   are generated for similar type leaf images. This raises ambiguity in the
   results and affects the accuracy of leaf image identification. But here,
   in this proposed deep learning-based plant leaves morphological feature
   recognition system, leaf morphological features are used to train the
   system. Morphological features are identified to recognize a plant leaf.
   Here, morphological features of medicinal plant leaves, venation,
   shapes, apices, and bases are extracted and analyzed to predict the
   image class. So, the leaf features remain persevered until the final
   stage. The proposed feature recognition analysis improves the accuracy
   of the leaf identification method. In this, more than 300 leaves from 18
   different plant families are collected and trained to build the deep
   learning classifier and achieve 96\% accuracy. The performance
   evaluation was also conducted over ``Flavia{''}, ``Swedish{''} and
   ``Leaf{''} data set and obtained 91\%, 87\% and 91\% accuracy. The
   performance of image classification and feature preservation algorithms
   with less computational power are indicating the potential applicability
   of the proposed Deep - Morpho Algorithm (DMA) in medicinal plants and
   leaves identification.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Dubey, AK (Corresponding Author), Amity Univ Uttar Pradesh, Amity Sch Engn \& Technol, Dept Elect \& Commun Engn, Noida 201313, UP, India.
   Thanikkal, Jibi G., Amity Univ Uttar Pradesh, Amity Sch Engn \& Technol, Dept Comp Sci \& Engn, Noida 201313, UP, India.
   Dubey, Ashwani Kumar, Amity Univ Uttar Pradesh, Amity Sch Engn \& Technol, Dept Elect \& Commun Engn, Noida 201313, UP, India.
   Thomas, M. T., St Thomas Coll, Dept Bot, Trichur, Kerala, India.},
DOI = {10.1007/s11042-023-14567-y},
EarlyAccessDate = {FEB 2023},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Deep learning; Leaf morphology; Medicinal plant; Plant recognition;
   Feature recognition},
Keywords-Plus = {SHAPE},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {jibimary@gmail.com
   dubey1ak@gmail.com
   thomastbgri@gmail.com},
Affiliations = {Amity University Noida; Amity University Noida},
ResearcherID-Numbers = {Dubey, Ashwani Kumar/ABI-1337-2020},
ORCID-Numbers = {Dubey, Ashwani Kumar/0000-0003-0778-9262},
Cited-References = {Amlekar MM, 2015, INT C PERV COMP, P1, DOI DOI 10.1109/PERVASIVE.2015.7087088.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Atoum Y, 2016, PATTERN RECOGN, V53, P287, DOI 10.1016/j.patcog.2015.11.021.
   Cohen Marc Maurice, 2014, J Ayurveda Integr Med, V5, P251, DOI 10.4103/0975-9476.146554.
   Demisse GG, 2018, IEEE T PATTERN ANAL, V40, P1338, DOI 10.1109/TPAMI.2017.2711607.
   George Juby, 2017, 2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS), P2216, DOI 10.1109/ICECDS.2017.8389846.
   Howard A.G., 2017, ABS170404861 CORR.
   Janahiraman TV, 2019, 2019 7TH INTERNATIONAL CONFERENCE ON SMART COMPUTING \& COMMUNICATIONS (ICSCC), P79.
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Kadir A., 2014, INT J ADV SCI TECHNO, V44, P113.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Kebapci H, 2011, COMPUT J, V54, P1475, DOI 10.1093/comjnl/bxq037.
   Kheirkhah FM, 2019, IET COMPUT VIS, V13, P369, DOI 10.1049/iet-cvi.2018.5028.
   Kim W, 2012, IEEE SIGNAL PROC LET, V19, P127, DOI 10.1109/LSP.2011.2182648.
   Kolivand H, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0191447.
   Kumar M, 2019, IEEE ACCESS, V7, P163912, DOI 10.1109/ACCESS.2019.2952176.
   Kumar S., 2012, INDIAN J COMPUTER SC, V3, P436.
   Lam BSY, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107153.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Leong KK, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC CONTROL AND INTELLIGENT SYSTEMS (I2CACIS 2020), P39, DOI {[}10.1109/i2cacis49202.2020.9140103, 10.1109/I2CACIS49202.2020.9140103].
   Liu XY, 2019, IEEE ACCESS, V7, P139635, DOI 10.1109/ACCESS.2019.2942144.
   Lopez A, 2017, IEEE LAT AM T, V15, P2185, DOI 10.1109/TLA.2017.8070425.
   Mall PK, 2022, INT J SYST ASSUR ENG, V13, P658, DOI 10.1007/s13198-021-01580-3.
   Mzoughi O, 2012, LECT NOTES COMPUT SC, V7324, P348, DOI 10.1007/978-3-642-31295-3\_41.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Pawara P, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107528.
   Randrianasoa JF, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107667.
   Raut SP, 2018, PROCEEDINGS OF THE 2018 SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS), P1579, DOI 10.1109/ICCONS.2018.8663028.
   Salima A, 2015, INT C ADV COMP SCI I, P275, DOI 10.1109/ICACSIS.2015.7415152.
   Shu H, 2019, AAAI CONF ARTIF INTE, P4943.
   Silva PFB, 2013, LECT NOTES COMPUT SC, V7950, P197, DOI 10.1007/978-3-642-39094-4\_23.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Soderkvist OJO, 2001, COMPUTER VISION CLAS, P1.
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Thanikkal JG, 2018, 7 INT C RELIABILITY, P1.
   Thanikkal JG, 2020, IEEE SENS J, V20, P13103, DOI 10.1109/JSEN.2020.3002909.
   Thanikkal JG, 2017, 2017 RECENT DEVELOPMENTS IN CONTROL, AUTOMATION AND POWER ENGINEERING (RDCAPE), P404, DOI 10.1109/RDCAPE.2017.8358305.
   Wang B, 2019, IEEE T IMAGE PROCESS, V28, P5963, DOI 10.1109/TIP.2019.2921526.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yang CZ, 2019, IEEE ACCESS, V7, P178108, DOI 10.1109/ACCESS.2019.2958416.
   Zeng JX, 2019, IEEE ACCESS, V7, P57163, DOI 10.1109/ACCESS.2019.2913688.
   Zhang J, 2019, PATTERN RECOGN, V91, P175, DOI 10.1016/j.patcog.2019.02.024.
   Zhang SW, 2013, PATTERN RECOGN, V46, P1891, DOI 10.1016/j.patcog.2013.01.015.
   Zhang Y, 2020, IEEE ACCESS, V8, P56607, DOI 10.1109/ACCESS.2020.2982456.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhao TM, 2019, IEEE ACCESS, V7, P49691, DOI 10.1109/ACCESS.2019.2911056.},
Number-of-Cited-References = {48},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {L4OB9},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000939106800009},
DA = {2023-08-12},
}

@article{ WOS:000311245300003,
Author = {Husin, Z. and Shakaff, A. Y. M. and Aziz, A. H. A. and Farook, R. S. M.
   and Jaafar, M. N. and Hashim, U. and Harun, A.},
Title = {Embedded portable device for herb leaves recognition using image
   processing techniques and neural network algorithm},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2012},
Volume = {89},
Pages = {18-29},
Month = {NOV},
Abstract = {Herbs have been widely used in food preparation, medicine and cosmetic
   industry. Knowing which herbs to be used would be very critical in these
   applications. Nevertheless, the current way of identification and
   determination of the types of herbs is still being done manually and
   prone to human error. Designing a convenient and automatic recognition
   system of herbs species is essential since this will improve herb
   species classification efficiency. This research focus on recognition
   approach to the shape and texture features of the herbs leaves. It aims
   to realize the computerized method to classify the herbs plants in a
   very convenient way. Portable herb leaves recognition system through
   image and data processing techniques is implemented as automated herb
   plant classification system. It is very easy to use and inexpensive
   system designed especially for helping scientist in agricultural field.
   The proposed system employs neural networks algorithm and image
   processing techniques to perform recognition on twenty species of herbs.
   One hundred samples for each species went through the system and the
   recognition accuracy was at 98.9\%. Most importantly the system is
   capable of identifying the herbs leaves species even though they are
   dried, wet, torn or deformed. The efficiency and effectiveness of the
   proposed method in recognizing and classifying the different herbs
   species is demonstrated by experiments. (C) 2012 Elsevier B.V. All
   rights reserved.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Husin, Z (Corresponding Author), Univ Malaysia Perlis, Sch Comp \& Commun Engn, Perlis, Malaysia.
   Husin, Z.; Aziz, A. H. A.; Farook, R. S. M., Univ Malaysia Perlis, Sch Comp \& Commun Engn, Perlis, Malaysia.
   Jaafar, M. N., Univ Malaysia Perlis, Agrotechnol Unit, Perlis, Malaysia.
   Hashim, U., Univ Malaysia Perlis, Inst Nano Elect Engn, Perlis, Malaysia.},
DOI = {10.1016/j.compag.2012.07.009},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Embedded portable device; Herbs leaves database; Herbs leaves
   recognition; Neural network algorithm; Singular Value Decomposition
   (SVD)},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {zulhusin@unimap.edu.my
   aliyeon@unimap.edu.my
   abdulhallis@unimap.edu.my
   rohani@unimap.edu.my
   mahmad@unimap.edu.my
   uda@unimap.edu.my
   zz\_harun@yahoo.com},
Affiliations = {Universiti Malaysia Perlis; Universiti Malaysia Perlis; Universiti
   Malaysia Perlis},
ResearcherID-Numbers = {Harun, Azizi/V-6892-2019
   Aziz, Abdul Hallis Abdul/AAA-1798-2019
   },
ORCID-Numbers = {Scopus, UniMAP/0000-0003-1327-8215
   hashim, uda/0000-0001-5118-6069},
Funding-Acknowledgement = {Ministry of Science, Technology \& Innovation (MOSTI); Universiti
   Malaysia Perlis},
Funding-Text = {This study was supported by the Ministry of Science, Technology \&
   Innovation (MOSTI) and Universiti Malaysia Perlis.},
Cited-References = {Abadi Mohamed, 2010, P 4 INT C IM SIGN PR.
   Abrahamsen Adam, 2001, IMAGE COMPRESSION US.
   {[}Anonymous], IMAGE PROCESSING C A.
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851.
   Daramola Samuel Adebayoand, 2011, INT J VIDEO IMAGE PR, V11, P28.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Gao Liwen, 2010, INT C IM SIGN PROC C.
   Gonzalez R.C., 2004, DIGITAL IMAGE PROCES.
   Gonzalez R. C., 2008, DIGITAL IMAGE PROCES.
   Haykin Simon, 2000, NEURAL NETWORK COMPR.
   Kalman Dan, 1996, COLL MATH J, V27, P2.
   Kumar Satish, 2010, NEURAL NETWORKS CLAS.
   Malepati Hazarathaiah, 2010, DIGITAL MEDIA PROCES.
   Myler Harley R., 1993, POCKET HDB IMAGE PRO.
   Parker, 1997, ALGORITHMS IMAGE PRO.
   Qidwai Uvais, 2010, DIGITAL IMAGE PROCES.},
Number-of-Cited-References = {16},
Times-Cited = {28},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {38},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {039KS},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000311245300003},
DA = {2023-08-12},
}

@article{ WOS:000755636500040,
Author = {Apriyanti, Diah Harnoni and Spreeuwers, Luuk J. and Lucas, Peter J. F.
   and Veldhuis, Raymond N. J.},
Title = {Automated color detection in orchids using color labels and deep
   learning},
Journal = {PLOS ONE},
Year = {2021},
Volume = {16},
Number = {10},
Month = {OCT 27},
Abstract = {The color of particular parts of a flower is often employed as one of
   the features to differentiate between flower types. Thus, color is also
   used in flower-image classification. Color labels, such as `green',
   `red', and `yellow', are used by taxonomists and lay people alike to
   describe the color of plants. Flower image datasets usually only consist
   of images and do not contain flower descriptions. In this research, we
   have built a flower-image dataset, especially regarding orchid species,
   which consists of human-friendly textual descriptions of features of
   specific flowers, on the one hand, and digital photographs indicating
   how a flower looks like, on the other hand. Using this dataset, a new
   automated color detection model was developed. It is the first research
   of its kind using color labels and deep learning for color detection in
   flower recognition. As deep learning often excels in pattern recognition
   in digital images, we applied transfer learning with various amounts of
   unfreezing of layers with five different neural network architectures
   (VGG16, Inception, Resnet50, Xception, Nasnet) to determine which
   architecture and which scheme of transfer learning performs best. In
   addition, various color scheme scenarios were tested, including the use
   of primary and secondary color together, and, in addition, the
   effectiveness of dealing with multi-class classification using
   multi-class, combined binary, and, finally, ensemble classifiers were
   studied. The best overall performance was achieved by the ensemble
   classifier. The results show that the proposed method can detect the
   color of flower and labellum very well without having to perform image
   segmentation. The result of this study can act as a foundation for the
   development of an image-based plant recognition system that is able to
   offer an explanation of a provided classification.},
Publisher = {PUBLIC LIBRARY SCIENCE},
Address = {1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA},
Type = {Article},
Language = {English},
Affiliation = {Apriyanti, DH (Corresponding Author), Univ Twente, Fac EEMCS, Enschede, Netherlands.
   Apriyanti, DH (Corresponding Author), Indonesian Inst Sci LIPI, Jakarta, Indonesia.
   Apriyanti, Diah Harnoni; Spreeuwers, Luuk J.; Lucas, Peter J. F.; Veldhuis, Raymond N. J., Univ Twente, Fac EEMCS, Enschede, Netherlands.
   Apriyanti, Diah Harnoni, Indonesian Inst Sci LIPI, Jakarta, Indonesia.
   Lucas, Peter J. F., Leiden Univ, LIACS, Leiden, Netherlands.},
DOI = {10.1371/journal.pone.0259036},
Article-Number = {e0259036},
ISSN = {1932-6203},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {d.h.apriyanti@utwente.nl},
Affiliations = {University of Twente; National Research \& Innovation Agency of
   Indonesia (BRIN); Indonesian Institute of Sciences (LIPI); Leiden
   University; Leiden University - Excl LUMC},
ResearcherID-Numbers = {Apriyanti, Diah Harnoni/I-8809-2019
   },
ORCID-Numbers = {Apriyanti, Diah Harnoni/0000-0002-1673-7870
   Lucas, Peter/0000-0001-5454-2428},
Funding-Acknowledgement = {Research and Innovation in Science and Technology Project (RISET-Pro) of
   the Ministry of Research, Technology, and Higher Education of Republic
   Indonesia {[}8245]; Indonesian Institute of Sciences (LIPI)},
Funding-Text = {The research described in this paper was funded by Research and
   Innovation in Science and Technology Project (RISET-Pro) of the Ministry
   of Research, Technology, and Higher Education of Republic Indonesia
   (World Bank Loan No.8245-ID) and supported by Indonesian Institute of
   Sciences (LIPI). The funder had no role in study design, data collection
   and analysis, decision to publish, or preparation of the manuscript.},
Cited-References = {Abadi M., 2016, TENSORFLOW LARGE SCA, DOI {[}DOI 10.1038/NN.3331, DOI 10.5555/3026877.3026899].
   Apriyanti DH, 2013, 2013 INTERNATIONAL CONFERENCE ON COMPUTER, CONTROL, INFORMATICS AND ITS APPLICATIONS (IC3INA), P53, DOI 10.1109/IC3INA.2013.6819148.
   Arwatchananukul S, 2020, WIRELESS PERS COMMUN, V115, P3275, DOI 10.1007/s11277-020-07463-3.
   Basha SHS, 2021, NEURAL COMPUT APPL, V33, P8055, DOI 10.1007/s00521-020-05549-4.
   Benavente R, 2006, COLOR RES APPL, V31, P48, DOI 10.1002/col.20172.
   Chollet F., 2015, KERAS.
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1.
   Genesereth MR., 1987, LOGICAL FDN ARTIFICI.
   Gogul I, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION AND NETWORKING (ICSCN).
   Gonzalez R. C., 2018, DIGITAL IMAGE PROCES.
   Gurnani A, 2017, FLOWER CATEGORIZATIO.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hiary H, 2018, IET COMPUT VIS, V12, P855, DOI 10.1049/iet-cvi.2017.0155.
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649.
   Leggett R, 2011, AOB PLANTS, DOI 10.1093/aobpla/plr004.
   Liu ZL, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0532-7.
   Lodh A, 2017, PROCEEDINGS OF 2ND INTERNATIONAL CONFERENCE ON 2017 DEVICES FOR INTEGRATED CIRCUIT (DEVIC), P790, DOI 10.1109/DEVIC.2017.8074061.
   Menegaz G, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/29125.
   Menegaz G, 2007, 14TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING WORKSHOPS, PROCEEDINGS, P216, DOI 10.1109/ICIAPW.2007.41.
   Radhika K., 2020, NAT INSPIRED COMPUT, V871, P57, DOI DOI 10.1007/978-3-030-33820-6\_3.
   Seeland M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170629.
   Seo Seon-Won, 2017, KOREAN JOURNAL OF PLANT TAXONOMY, V47, P124, DOI 10.11110/kjpt.2017.47.2.124.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Witten IH, 2011, MOR KAUF D, P1.
   Xia XL, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P783, DOI 10.1109/ICIVC.2017.7984661.
   Zhang HX, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P25, DOI 10.1109/MIPR.2018.00013.
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907.},
Number-of-Cited-References = {32},
Times-Cited = {3},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {16},
Journal-ISO = {PLoS One},
Doc-Delivery-Number = {YZ7EW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000755636500040},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000386326100143,
Author = {Rojas-Hernandez, Rafael and Lopez-Chau, Asdrubal and Trujillo-Mora,
   Valentin and Rojas-Hernandez, Carlos A.},
Book-Group-Author = {IEEE},
Title = {Plant Identification using New Geometric Features with Standard Data
   Mining Methods},
Booktitle = {2016 IEEE 13TH INTERNATIONAL CONFERENCE ON NETWORKING, SENSING, AND
   CONTROL (ICNSC)},
Series = {IEEE International Conference on Networking Sensing and Control},
Year = {2016},
Note = {13th IEEE International Conference on Networking, Sensing, and Control
   (ICNSC), Mexico City, MEXICO, APR 28-30, 2016},
Organization = {Consejo Nacl Ciencia Tecnologia; Cinvestav; IEEE Advancing Technol
   Human; IEEE Syst Man \& Cybernet Soc; IEEE},
Abstract = {Plant identification belongs to a specific application domain of data
   mining. Images of plant leaves are usually used as the main element to
   distinguish a plant from another. For proper identification, feature
   extraction is necessary. In the literature, most plant recognition
   systems use the features along with a classification method, which has
   been adapted or modified to face this type of application.
   In this paper, we propose three new geometric features that describe the
   vertical and horizontal symmetry of leaves. These features are simple to
   extract from images. According to the results of experiments, when these
   features are used in conjunction with other well-known geometric
   characteristics, the performance of classical classification methods is
   remarkably improved. To show the effectiveness of the proposal, we test
   seven classifiers with images of leaves publicly available on the
   Internet.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Rojas-Hernandez, R (Corresponding Author), Univ Autonoma Estado Mexico, CU UAEM Zumpango, Zumpango 55600, Estado De Mexic, Mexico.
   Rojas-Hernandez, Rafael; Lopez-Chau, Asdrubal; Trujillo-Mora, Valentin; Rojas-Hernandez, Carlos A., Univ Autonoma Estado Mexico, CU UAEM Zumpango, Zumpango 55600, Estado De Mexic, Mexico.},
ISSN = {1810-7869},
ISBN = {978-1-4673-9975-3},
Keywords = {Plant recognition; feature extraction; leaf identification; geometric
   feature},
Keywords-Plus = {LEAF; CLASSIFICATION},
Research-Areas = {Automation \& Control Systems; Computer Science; Engineering; Remote
   Sensing},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Hardware \&
   Architecture; Engineering, Electrical \& Electronic; Remote Sensing},
Author-Email = {rrojashe@uaemex.mx
   alchau@uaemex.mx
   vtrujillom@uaemex.mx
   carojash@uaemex.mx},
ResearcherID-Numbers = {López Chau, Asdrúbal/B-7737-2016
   Rojas, Rafael/GPK-7621-2022},
ORCID-Numbers = {López Chau, Asdrúbal/0000-0001-5254-0939
   },
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   {[}Anonymous], 2012, INN INT SYST APPL IN.
   Cerutti G, 2014, PATTERN RECOGN LETT, V49, P177, DOI 10.1016/j.patrec.2014.07.016.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Gwo CY, 2013, COMPUT ELECTRON AGR, V91, P124, DOI 10.1016/j.compag.2012.12.005.
   Hall Mark, 2009, ACM SIGKDD EXPLORATI, V11, P10, DOI {[}10.1145/1656274.1656278, DOI 10.1145/1656274.1656278].
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Arribas JI, 2011, COMPUT ELECTRON AGR, V78, P9, DOI 10.1016/j.compag.2011.05.007.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Priyankara HAC, 2015, 2015 MORATUWA ENGINEERING RESEARCH CONFERENCE (MERCON), P148, DOI 10.1109/MERCon.2015.7112336.
   Soderkvist O, 2001, COMPUTER VISION CLAS.
   Watcharabutsarakham S., 2012, INF SCI SERV SCI DAT.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {14},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BG0LU},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000386326100143},
DA = {2023-08-12},
}

@article{ WOS:000763502300009,
Author = {Cai, Xingquan and Huo, Yuqing and Chen, Yunbo and Xi, Mengyao and Tu,
   Yuxin and Sun, Chen and Sun, Haiyan},
Title = {Real-Time Leaf Recognition Method Based on Image Segmentation and
   Feature Extraction},
Journal = {INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE},
Year = {2022},
Volume = {36},
Number = {01},
Month = {JAN},
Abstract = {Leaf recognition has been an important research field of image
   recognition in the recent past. However, traditional leaf recognition
   methods can be easily affected by environments and cannot realize
   multi-leaf recognition under a complex background in real time. In this
   work, we present a real-time leaf recognition method based on image
   segmentation and feature recognition. First, we denoise the input of a
   leaf image, performing a leaf segmentation with an improved FCN network
   model, and then optimize the contour edge with a CRF algorithm to get a
   leaf segmentation image. Second, we extract the content features of the
   segmented leaf image with an Inception-V2 network model to get a feature
   map of the leaf image. Third, we input the feature map into an RPN
   network to obtain a set of regional candidate frames and then integrate
   the feature map and the information of candidate frames in a RoI Pooling
   layer, which can extract the feature map of a candidate frame area and
   scale it to a fixed-size feature map. Finally, we send the feature map
   to a fully connected layer to classify each preselection box content
   through the calculation of preselection feature maps, and then obtain
   the final accurate position of the prediction box by utilizing a
   bounding box regression. The experimental results show that the proposed
   method can achieve multi-leaf recognitions with high accuracy and fast
   speed under complex environments in real time.},
Publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
Address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
Type = {Article},
Language = {English},
Affiliation = {Cai, XQ; Sun, HY (Corresponding Author), North China Univ Technol, Sch Informat Sci \& Technol, Beijing 100144, Peoples R China.
   Cai, Xingquan; Huo, Yuqing; Chen, Yunbo; Xi, Mengyao; Tu, Yuxin; Sun, Chen; Sun, Haiyan, North China Univ Technol, Sch Informat Sci \& Technol, Beijing 100144, Peoples R China.},
DOI = {10.1142/S0218001421540331},
Article-Number = {2154033},
ISSN = {0218-0014},
EISSN = {1793-6381},
Keywords = {Leaf recognition; image segmentation; feature extraction; FCN network},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {xingquancai@126.com
   huoyuqing2019@126.com
   yunbochenVIP@163.com
   X1mengyao@126.com
   yuxingtu@126.com
   3562953797@163.com
   sunhaiyan80@hotmail.com},
Affiliations = {North China University of Technology},
ResearcherID-Numbers = {Tu, Yuxin/IAM-7611-2023
   Tu, Yuxin/IYJ-4614-2023
   xi, mengyao/AIE-6921-2022},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}61503005]; Beijing
   Social Science Foundation {[}19YTC043, 20YTB011]; North China University
   of Technology {[}YY19XN132]},
Funding-Text = {This work was supported by the Funding Project of National Natural
   Science Foundation of China (No. 61503005), the Funding Project of
   Beijing Social Science Foundation (No.19YTC043, No. 20YTB011) and the
   Funding Project of Yuyou Talent Program in North China University of
   Technology (No. YY19XN132). We would like to thank those who care of
   this paper and our projects. Also, we would like to thank everyone who
   spent time on reading early versions of this paper, including the
   anonymous reviewers.},
Cited-References = {Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615.
   {[}曹诗雨 Cao Shiyu], 2017, {[}中国图象图形学报, Journal of Image and Graphics], V22, P671.
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184.
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169.
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81.
   Hamid L.E., 2019, ELECT LETT COMPUTER, V18, P37.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Jiang Feng, 2017, Journal of Software, V28, P160, DOI 10.13328/j.cnki.jos.005136.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Liao J.W., 2018, MOD COMPUTER, V619.
   Liu XL, 2019, ARTIF INTELL REV, V52, P1089, DOI 10.1007/s10462-018-9641-3.
   Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.48550/ARXIV.1411.4038.
   {[}梅星宇 Mei Xingyu], 2019, {[}江苏农业学报, Jiangsu Journal of Agricultural Sciences], V35, P1334.
   Miao P., 2020, COMPUTER KNOWL TECHN, V16, P201.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Oikonomou VP, 2020, INT J PATTERN RECOGN, V34, DOI 10.1142/S0218001420570049.
   Peng CW., 2020, INT J PATTERN RECOGN, V34, P3340.
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Reyes A.K., 2015, WORKING NOTES CLEF, V1391, P1884.
   Song XY, 2020, LASER OPTOELECTRON P, V57, DOI 10.3788/LOP57.041016.
   Srivastava V., 2018, P 2018 INT C INV RES, P626.
   Sun WW, 2018, IEEE GEOSCI REMOTE S, V15, P474, DOI 10.1109/LGRS.2018.2795531.
   Thanikkal JG, 2017, 2017 RECENT DEVELOPMENTS IN CONTROL, AUTOMATION AND POWER ENGINEERING (RDCAPE), P404, DOI 10.1109/RDCAPE.2017.8358305.
   {[}王全东 Wang Quandong], 2018, {[}计算机辅助设计与图形学学报, Journal of Computer-Aided Design \& Computer Graphics], V30, P2278.
   {[}吴昀璞 Wu Yunpu], 2018, {[}系统仿真学报, Journal of System Simulation], V30, P4492.
   Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153.
   {[}许景辉 Xu Jinghui], 2020, {[}农业机械学报, Transactions of the Chinese Society for Agricultural Machinery], V51, P230.
   Yang F., 2020, INT J PATTERN RECOGN, V34, P1168.
   {[}叶继华 Ye Jihua], 2019, {[}系统仿真学报, Journal of System Simulation], V31, P1421.
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075.
   {[}张宁 Zhang Ning], 2013, {[}计算机应用, Journal of Computer Applications], V33, P2009.
   Zhang Ning, 2011, Application Research of Computers, V28, P4001, DOI 10.3969/j.issn.1001-3695.2011.11.001.
   {[}张帅 Zhang Shuai], 2016, {[}北京林业大学学报, Journal of Beijing Forestry University], V38, P108.
   Zhou F., 2017, CHIN J COMPUT, V40, P1229, DOI DOI 10.11897/SP.J.1016.2017.01229.},
Number-of-Cited-References = {36},
Times-Cited = {0},
Usage-Count-Last-180-days = {7},
Usage-Count-Since-2013 = {17},
Journal-ISO = {Int. J. Pattern Recognit. Artif. Intell.},
Doc-Delivery-Number = {ZL2IA},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000763502300009},
DA = {2023-08-12},
}

@article{ WOS:000810046000002,
Author = {Zhong, Xianping and Ban, Heng},
Title = {Pre-trained network-based transfer learning: A small-sample machine
   learning approach to nuclear power plant classification problem},
Journal = {ANNALS OF NUCLEAR ENERGY},
Year = {2022},
Volume = {175},
Month = {SEP 15},
Abstract = {Some research topics belonging to classification problems in the nuclear
   industry, such as fault diagnosis and accident identification, can be
   solved by feature extraction and subsequent application of statistical
   machine learning classifiers. Recently, deep neural network-based
   methods with automatic feature extraction and high accuracy have gained
   wide attention. They usually require large-scale training data, however,
   plant fault or accident data are scarce or difficult to obtain. This
   paper proposes a convolutional network (CNN)-based transfer learning
   method to solve this problem. The network's shallow layer is derived
   from a pre-trained CNN based on the ImageNet database to automatically
   extract features, and the deep layer is customized to match the
   classification problem. Data in non-image formats are converted to image
   formats and subsequently used to train the network. Case studies of
   rotating machines fault diagnosis show that the proposed method requires
   only limited training data to achieve high accuracy. (c) 2022 Elsevier
   Ltd. All rights reserved.},
Publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Zhong, XP (Corresponding Author), Univ Pittsburgh, Dept Mech Engn \& Mat Sci, 3700 OHara St, Pittsburgh, PA 15213 USA.
   Zhong, Xianping; Ban, Heng, Univ Pittsburgh, Dept Mech Engn \& Mat Sci, 3700 OHara St, Pittsburgh, PA 15213 USA.},
DOI = {10.1016/j.anucene.2022.109201},
EarlyAccessDate = {MAY 2022},
Article-Number = {109201},
ISSN = {0306-4549},
EISSN = {1873-2100},
Keywords = {Classification problem; Nuclear power plant; Transfer learning;
   Pre-trained convolutional network; ImageNet; Classification problem;
   Nuclear power plant; Transfer learning; Pre-trained convolutional
   network; ImageNet},
Keywords-Plus = {TIME FOURIER-TRANSFORM; FAULT-DIAGNOSIS; NEURAL-NETWORK; EXPERT-SYSTEM},
Research-Areas = {Nuclear Science \& Technology},
Web-of-Science-Categories  = {Nuclear Science \& Technology},
Author-Email = {xianping.zhong@pitt.edu},
Affiliations = {Pennsylvania Commonwealth System of Higher Education (PCSHE); University
   of Pittsburgh},
Funding-Acknowledgement = {Department of Energy {[}DE-NE0008909]; University of Pittsburgh Center},
Funding-Text = {This material is based upon work supported by the Department of Energy
   under Award Number DE-NE0008909. This research is supported in part by
   the University of Pittsburgh Center for Research Computing through the
   resources provided.},
Cited-References = {Albawi S, 2017, I C ENG TECHNOL.
   {[}Anonymous], MICROSIMULATION TECH.
   {[}Anonymous], BEARING DATA CTR CAS.
   Bakator Mihalj, 2018, Multimodal Technologies and Interaction, V2, DOI 10.3390/mti2030047.
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555.
   CHANG SH, 1995, NUCL TECHNOL, V112, P266, DOI 10.13182/NT95-A35179.
   Chen RX, 2019, COMPUT IND, V106, P48, DOI 10.1016/j.compind.2018.11.003.
   CHOI SS, 1995, IEEE T NUCL SCI, V42, P1406, DOI 10.1109/23.467727.
   Dietterich T, 1995, ACM COMPUT SURV, V27, P326, DOI 10.1145/212094.212114.
   Dong Z, 2017, ENERGIES, V10, DOI 10.3390/en10122125.
   Gautam S, 2019, ISA T, V92, P180, DOI 10.1016/j.isatra.2019.02.011.
   Han T, 2021, APPL SOFT COMPUT, V103, DOI 10.1016/j.asoc.2021.107150.
   Han T, 2019, MECH SYST SIGNAL PR, V117, P170, DOI 10.1016/j.ymssp.2018.07.048.
   Hashemian H M., 2011, NUCL POWER CONTROL R, P49, DOI 10.5772/18768.
   He C, 2021, ANN NUCL ENERGY, V159, DOI 10.1016/j.anucene.2021.108326.
   He K, 2015, C COMPUTER VISION PA.
   Hu G, 2021, FRONT ENERGY RES, V9, DOI 10.3389/fenrg.2021.663296.
   Hussain M, 2019, ADV INTELL SYST, V840, P191, DOI 10.1007/978-3-319-97982-3\_16.
   Ioffe S., 2015, 32 INT C MACHINE LEA.
   Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342.
   Kononenko I, 2001, ARTIF INTELL MED, V23, P89, DOI 10.1016/S0933-3657(01)00077-X.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   LEUNG H, 1991, IEEE T SIGNAL PROCES, V39, P2101, DOI 10.1109/78.134446.
   Li JK, 2022, ANN NUCL ENERGY, V165, DOI 10.1016/j.anucene.2021.108639.
   Li Q, 2001, OBSERVER BASED FAULT.
   Neyshabur B., 2020, ADV NEURAL INF PROCE, V33, P1.
   Omeiza Daniel, 2019, ARXIV190801224.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Peterson LE., 2009, SCHOLARPEDIA, V4, P1883, DOI {[}10.4249/scholarpedia.1883, DOI 10.4249/SCHOLARPEDIA.1883].
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74.
   Singla N, 2020, FORENSIC SCI INT, V309, DOI 10.1016/j.forsciint.2020.110187.
   Skillman G.R., 2021, ENCY NUCL ENERGY, P17, DOI {[}10.1016/B978-0-12-409548-9.12146-3, DOI 10.1016/B978-0-12-409548-9.12146-3].
   Sui Z, 2014, NUCL ENG DES, V271, P479, DOI 10.1016/j.nucengdes.2013.12.019.
   Taskiran M, 2020, DIGIT SIGNAL PROCESS, V106, DOI 10.1016/j.dsp.2020.102809.
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579.
   Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640.
   Wang LH, 2017, CHIN J MECH ENG-EN, V30, P1357, DOI 10.1007/s10033-017-0190-5.
   Wei DD, 2021, MECH SYST SIGNAL PR, V158, DOI 10.1016/j.ymssp.2021.107744.
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6.
   Xu P, 2017, ANN NUCL ENERGY, V99, P279, DOI 10.1016/j.anucene.2016.09.006.
   Yoshikawa Hidekazu, 2005, Nuclear Engineering and Technology, V37, P151.
   Yosinski J, 2014, ADV NEUR IN, V27.
   YU FTS, 1994, APPL OPTICS, V33, P5262, DOI 10.1364/AO.33.005262.
   Yu W., 2014, VISUALIZING COMP CON.
   Zhu XX, 2019, MEASUREMENT, V138, P526, DOI 10.1016/j.measurement.2019.02.022.
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555.},
Number-of-Cited-References = {47},
Times-Cited = {13},
Usage-Count-Last-180-days = {7},
Usage-Count-Since-2013 = {23},
Journal-ISO = {Ann. Nucl. Energy},
Doc-Delivery-Number = {2B2TT},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000810046000002},
DA = {2023-08-12},
}

@inproceedings{ WOS:000783765200003,
Author = {Ibrahim, Nehad M. Abdulrahman and Gabr, Dalia Goda and Emara,
   Abdel-Hamid M.},
Editor = {Saeed, F and Mohammed, F and Ghaleb, F},
Title = {A New Deep Learning System for Wild Plants Classification and Species
   Identification: Using Leaves and Fruits},
Booktitle = {ADVANCES ON INTELLIGENT INFORMATICS AND COMPUTING: HEALTH INFORMATICS,
   INTELLIGENT SYSTEMS, DATA SCIENCE AND SMART COMPUTING},
Series = {Lecture Notes on Data Engineering and Communications Technologies},
Year = {2022},
Volume = {127},
Pages = {26-37},
Note = {6th International Conference of Reliable Information and Communication
   Technology (IRICT), ELECTR NETWORK, DEC 22-23, 2021},
Organization = {Yemeni Scientists Res Grp; Univ Teknologi Malaysia, Behav Informat Res
   Grp; Charles Darwin Univ, Coll Engn, IT \& Environm},
Abstract = {Many studies are based on the study of plant classification and their
   identification using its leaves, and there are many studies to identify
   plants using its fruits. Most of these studies are based on the leaves
   of the plant in general as well as the fruits in general as well. In
   this research, we present a new tool using artificial intelligence to
   classify and identify wild plants through the leaves of these plants, or
   by using their fruits, or by using both leaves and fruits together. This
   tool has proven an excellent result compared to similar tools in the
   same field. More than one AI model was applied to three datasets, lower
   plants dataset (LPDS), upper plant dataset (UPDS), and fruit plant
   dataset (FPDS). The aim of this study is to use machine learning methods
   to serve in the plant taxonomy and identification. The wild plant's
   dataset was gathered in its natural habitat in Egypt. The developed
   convolution neural network model (AlexNet CNN), the Random Forest (RF),
   and the support vector machine (SVM) techniques were contrasted in the
   species classifications. The highest degree of accuracy achieved was
   98.2\% by using the developed CNN model.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Ibrahim, NMA (Corresponding Author), Imam Abdulrahman Bin Faisal Univ, Coll Comp Sci \& Informat Technol, Dept Comp Sci, Dammam, Saudi Arabia.
   Ibrahim, Nehad M. Abdulrahman, Imam Abdulrahman Bin Faisal Univ, Coll Comp Sci \& Informat Technol, Dept Comp Sci, Dammam, Saudi Arabia.
   Gabr, Dalia Goda, Al Azhar Univ, Fac Sci, Bot \& Microbiol Dept, Girls Branch, Cairo, Egypt.
   Emara, Abdel-Hamid M., Al Azhar Univ, Fac Engn, Dept Comp \& Syst Engn, Cairo 11884, Egypt.
   Emara, Abdel-Hamid M., Taibah Univ, Coll Comp Sci \& Engn, Medina 41477, Saudi Arabia.},
DOI = {10.1007/978-3-030-98741-1\_3},
ISSN = {2367-4512},
ISBN = {978-3-030-98741-1; 978-3-030-98740-4},
Keywords = {Plant taxonomy; Deep learning; Support vector machine; Random forest},
Research-Areas = {Computer Science; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Telecommunications},
Author-Email = {nmaibrahim@iau.edu.sa
   daliaGabr.e120@azhar.edu.eg},
Affiliations = {Imam Abdulrahman Bin Faisal University; Egyptian Knowledge Bank (EKB);
   Al Azhar University; Egyptian Knowledge Bank (EKB); Al Azhar University;
   Taibah University},
ResearcherID-Numbers = {Emara, Dr. Abdel Hamid/ACU-8000-2022
   Ibrahim, Nehad/AAQ-7940-2021
   Gabr, Dalia/GOP-2692-2022
   },
ORCID-Numbers = {Emara, Dr. Abdel Hamid/0000-0002-7730-9693
   Ibrahim, Nehad/0000-0002-7681-5594
   Gabr, Dalia/0000-0002-8603-6877},
Cited-References = {Choi S, 2015, CEUR WORKSHOP PROC, V1391, P2.
   Dileep MR, 2019, TENCON IEEE REGION, P321, DOI 10.1109/TENCON.2019.8929394.
   Gogul I, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION AND NETWORKING (ICSCN).
   Gyires-Toth BP, 2019, CYBERN INF TECHNOL, V19, P88, DOI 10.2478/cait-2019-0005.
   Haupt, 2018, CEUR WORKSHOP PROC, V2125, P1.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428.
   Jana B.K., 2012, INDIAN J FUNDAM APPL, V1, P54.
   Johannes A, 2017, COMPUT ELECTRON AGR, V138, P200, DOI 10.1016/j.compag.2017.04.013.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Lee JH, 2019, IEEE GLOBE WORK, DOI {[}10.1109/ICAIIC.2019.8669002, 10.1109/gcwkshps45667.2019.9024382].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Mahajan S, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13020356.
   Picon A, 2019, COMPUT ELECTRON AGR, V161, P280, DOI 10.1016/j.compag.2018.04.002.
   Rauf HT, 2019, DATA BRIEF, V26, DOI 10.1016/j.dib.2019.104340.
   Reyes A. K., FINE TUNING DEEP CON.
   Sun Y, 2017, J ELECTR COMPUT ENG, V2017, DOI 10.1155/2017/9240407.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Wang XF, 2021, COMPUT INTEL NEUROSC, V2021, DOI 10.1155/2021/5529905.
   Xiao J, 2020, J PHYS CONF SER, V1518, DOI 10.1088/1742-6596/1518/1/012041.
   Zhang SW, 2020, NEUROCOMPUTING, V408, P246, DOI 10.1016/j.neucom.2019.09.113.
   Zhang YC, 2018, ICMLC 2020: 2020 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING, P145, DOI 10.1145/3383972.3383975.},
Number-of-Cited-References = {23},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BS9RS},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000783765200003},
DA = {2023-08-12},
}

@inproceedings{ WOS:000811818000006,
Author = {Arshed, Muhammad Asad and Ghassan, Hadia and Hussain, Mubashar and
   Hassan, Muhammad and Kanwal, Ayesha and Fayyaz, Rimsha},
Book-Group-Author = {IEEE},
Title = {A Light Weight Deep Learning Model for Real World Plant Identification},
Booktitle = {2022 SECOND INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING AND HIGH
   PERFORMANCE COMPUTING (DCHPC)},
Year = {2022},
Pages = {40-45},
Note = {2nd International Conference on Distributed Computing and High
   Performance Computing (DCHPC), Qom, IRAN, MAR 02-03, 2022},
Organization = {Distributed Comp Syst Sci Grp; Qom Univ Technol; Informat Soc Iran; IEEE
   Iran Sect},
Abstract = {Automatic identification and classification of different plant leaf
   species have become a common trend among researchers and scientists. To
   obtain a result with better precision, they use various methods and
   techniques of deep learning to build a model. Convolutional neural
   networks are becoming the most common method used by scientists to
   classify plant leaves. However, the classification of plant leaves can
   be challenging with more rare species and complicated backgrounds, for
   which researchers build several models to achieve high-level accuracy.
   In the present study for the classification of leaves, we have created a
   model for plant leaf classification based on a dataset we collected.
   We've used the Resnet-50 model, a well-known CNN architecture, which
   provided an efficient method to organize and analyze a deep
   classification to reduce the complexity so that there will be fewer
   parameters for training and low time consumption as well. Using
   Resnet-50, we intended to develop a significant result in our
   classification model. The convolutional neural network is famous for its
   influential abilities in feature extraction and classification. And
   Resnet-50 being a residual network enabled us to train deep networks in
   our model. The average training accuracy reached 98.3\%, while the
   average testing accuracy reached 92.5\%. The key contribution of this
   study is effective accuracy as well as we have trained the model on our
   own prepared dataset that we have prepared from real world environment.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Arshed, MA (Corresponding Author), Univ Management \& Technol, Sch Syst \& Technol SST, Lahore, Pakistan.
   Arshed, Muhammad Asad; Hussain, Mubashar, Univ Management \& Technol, Sch Syst \& Technol SST, Lahore, Pakistan.
   Ghassan, Hadia, Minhaj Univ Lahore, Dept Comp Sci, Lahore, Pakistan.
   Hassan, Muhammad; Kanwal, Ayesha; Fayyaz, Rimsha, Univ Engn \& Technol, Dept Comp Sci, Lahore, Pakistan.},
DOI = {10.1109/DCHPC55044.2022.9731841},
ISBN = {978-1-6654-9672-8},
Keywords = {Deep Learning; Plant Classification; Convolutional Neural Network (CNN);
   Resnet-50},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Theory \&
   Methods},
Author-Email = {asad.arshed@umt.edu.pk
   hadiasheikh560@gmail.com
   mubashar.hussain@umt.edu.pk
   ranashb520@gmail.com
   ayeshakanwal2911@gmail.com
   mishafayyaz@gmail.com},
Affiliations = {University of Management \& Technology (UMT); Minhaj University;
   University of Engineering \& Technology Lahore},
ORCID-Numbers = {Hussain, Mubashar/0000-0002-0075-6444
   Arshed, Muhammad Asad/0000-0002-5583-1253},
Cited-References = {Abu M.A., 2019, INT J ENG RES TECHNO, V12, P563.
   Hu GS, 2021, COMPUT ELECTR ENG, V90, DOI 10.1016/j.compeleceng.2021.107023.
   IEEE Staff, 2018, NEURAL NETWORKS, DOI DOI 10.1016/j.neunet.2010.09.007.
   Kothari J. D, CASE STUDY IMAGE CLA.
   Peng Wu, 2021, Journal of Physics: Conference Series, V1820, DOI 10.1088/1742-6596/1820/1/012161.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Valarmathi G, 2021, MATER TODAY-PROC, V46, P3684, DOI 10.1016/j.matpr.2021.01.847.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Xie XY, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.00751.
   Xiong JB, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10010081.},
Number-of-Cited-References = {10},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BT2MU},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000811818000006},
DA = {2023-08-12},
}

@article{ WOS:000884922700008,
Author = {Villaruz, Jolitte A.},
Title = {Deep Convolutional Neural Network Feature Extraction for Berry Trees
   Classification},
Journal = {JOURNAL OF ADVANCES IN INFORMATION TECHNOLOGY},
Year = {2021},
Volume = {12},
Number = {3},
Pages = {226-233},
Month = {AUG},
Abstract = {To support biodiversity conservations, plant classification studies,
   particularly from images, are necessary. This study explores the use of
   the deep convolutional neural network as a feature extractor to a plant
   classification problem. An original dataset consisting of images of
   seedlings of the three most important berry trees belonging to the
   Philippine indigenous plants was used. The result shows that as the
   network layers are getting deeper, they are becoming better at
   extracting discriminative features, such that, irrespective of
   classifier used their prediction performance keeps on improving. When
   the different layers were individually visualized, the features
   extracted were far from random, uninterpretable patterns. Rather, they
   show relevant properties that are capable of sorting patterns
   progressively from low to higher level. Hence, for classification
   problems bounded with the limitation of data, time, and computational
   hardware, leveraging the representational power of the deep
   convolutional neural network is very useful.},
Publisher = {ENGINEERING \& TECHNOLOGY PUBLISHING},
Address = {2448 DESIRE AVE, ROWLAND HEIGHTS, CA 91748 USA},
Type = {Article},
Language = {English},
Affiliation = {Villaruz, JA (Corresponding Author), Aklan State Univ, Technol Dept, Kalibo Campus, Kalibo, Aklan, Philippines.
   Villaruz, Jolitte A., Aklan State Univ, Technol Dept, Kalibo Campus, Kalibo, Aklan, Philippines.},
ISSN = {1798-2340},
Keywords = {feature extraction; deep convolutional neural network; deep learning;
   AlexNet; plant classification; SVM},
Keywords-Plus = {SYSTEM},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science,
   Interdisciplinary Applications},
Author-Email = {jvillaruz@asu.edu.ph},
Funding-Acknowledgement = {Aklan State University through the Research and Development Services
   unit},
Funding-Text = {The author would like to thank the anonymous referees for their valuable
   comments and helpful suggestions. The support of Aklan State University
   through the Research and Development Services unit and the help extended
   by Dr. Rogelio L. Felizardo, Mr. Dennis M. Barrios II, and Prof. Julie
   Ann A. Salido is highly acknowledged. The field workers of the Clonal
   Nursery of Aklan State University and the efforts of Foresters Hanna Mae
   T. Ibuyan and Darlyn Joy I. Galicha in taking the photos of plant
   seedlings are particularly valued.},
Cited-References = {Abu-Mostafa Y, 2012, LEARNING DATA SHORT.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Bertrand S., 2018, P THE IAMPS INT WORK.
   Chatfield K, 2014, Arxiv, DOI arXiv:1405.3531.
   Collobert R, 2011, J MACH LEARN RES, V12, P2493.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cruz R. T. D., 2012, DAR RES DEV DIG, V14, P4.
   Ding L., 2018, INT ARCH PHOTOGRAMM, VXLII-3, P277, DOI {[}10.5194/isprs-archives-XLII-3-277-2018, DOI 10.5194/ISPRS-ARCHIVES-XLII-3-277-2018].
   Donahue J, 2014, PR MACH LEARN RES, V32.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Eleazar N. P., 2012, DAR RES DEV DIG, V14, P3.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597.
   Jean S., 2014, ARXIV.
   Koniusz P, 2017, IEEE T PATTERN ANAL, V39, P313, DOI 10.1109/TPAMI.2016.2545667.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   LeCun Y., 1989, ADV NEURAL INFORM PR, V2.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Mata-Montero E., 2016, PROC IFIP WORLD INFO, P26, DOI {[}10.1007/978-3-319-44447-5\_3, DOI 10.1007/978-3-319-44447-5\_3].
   Mikolov T., 2011, 2011 IEEE Workshop on Automatic Speech Recognition \& Understanding (ASRU), P196, DOI 10.1109/ASRU.2011.6163930.
   Murphy GEP, 2014, ECOL EVOL, V4, P91, DOI 10.1002/ece3.909.
   Ongsulee P, 2017, INT CONF ICT KNOWL, P92.
   Pimm SL, 2014, SCIENCE, V344, P987, DOI 10.1126/science.1246752.
   Prasad S., 2011, P 2011 INT C COMM CO, P343, DOI {[}10.1145/1947940.1948012, DOI 10.1145/1947940.1948012].
   Nguyen QK, 2013, PROC INT CONF ADV, P404, DOI 10.1109/ATC.2013.6698145.
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347.
   Salido Julie Ann A., 2018, International Journal of Machine Learning and Computing, V8, P61, DOI 10.18178/ijmlc.2018.8.1.664.
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Sulc M, 2015, LECT NOTES COMPUT SC, V8928, P185, DOI 10.1007/978-3-319-16220-1\_14.
   Sutskever I, 2014, ADV NEUR IN, V27.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Villaruz JA, 2018, I C HUMANOID NANOTEC, DOI 10.1109/HNICEM.2018.8666412.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, PLANT SPECIES IDENTI, V25.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Yu W., 2016, P 33 INT C MACH LEAR.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.},
Number-of-Cited-References = {46},
Times-Cited = {4},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {J. Adv. Inf. Technol.},
Doc-Delivery-Number = {6G7HM},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000884922700008},
DA = {2023-08-12},
}

@article{ WOS:000599232800006,
Author = {Latif, Ghazanfar and Alghazo, Jaafar and Maheswar, R. and Vijayakumar,
   V. and Butt, Mohsin},
Title = {Deep learning based intelligence cognitive vision drone for automatic
   plant diseases identification and spraying},
Journal = {JOURNAL OF INTELLIGENT \& FUZZY SYSTEMS},
Year = {2020},
Volume = {39},
Number = {6},
Pages = {8103-8114},
Abstract = {The agriculture industry is of great importance in many countries and
   plays a considerable role in the national budget. Also, there is an
   increased interest in plantation and its effect on the environment. With
   vast areas suitable for farming, countries are always encouraging
   farmers through various programs to increase national farming
   production. However, the vast areas and large farms make it difficult
   for farmers and workers to continually monitor these broad areas to
   protect the plants from diseases and various weather conditions. A new
   concept dubbed Precision Farming has recently surfaced in which the
   latest technologies play an integral role in the farming process. In
   this paper, we propose a SMART Drone system equipped with high precision
   cameras, high computing power with proposed image processing
   methodologies, and connectivity for precision farming. The SMART system
   will automatically monitor vast farming areas with precision, identify
   infected plants, decide on the chemical and exact amount to spray.
   Besides, the system is connected to the cloud server for sending the
   images so that the cloud system can generate reports, including
   prediction on crop yield. The system is equipped with a user-friendly
   Human Computer Interface (HCI) for communication with the farm base.
   This multidrone system can process vast areas of farmland daily. The
   Image processing technique proposed in this paper is a modified ResNet
   architecture. The system is compared with deep CNN architecture and
   other machine learning based systems. The ResNet architecture achieves
   the highest average accuracy of 99.78\% on a dataset consisting of
   70,295 leaf images for 26 different diseases of 14 plants. The results
   obtained were compared with the CNN results applied in this paper and
   other similar techniques in previous literature. The comparisons
   indicate that the proposed ResNet architecture performs better compared
   to other similar techniques.},
Publisher = {IOS PRESS},
Address = {NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Latif, G (Corresponding Author), Prince Mohammad bin Fahd Univ, Coll Comp Engn \& Sci, Dhahran, Eastern Provinc, Saudi Arabia.
   Latif, Ghazanfar; Alghazo, Jaafar, Prince Mohammad bin Fahd Univ, Coll Comp Engn \& Sci, Dhahran, Eastern Provinc, Saudi Arabia.
   Maheswar, R., VIT Bhopal Univ, Sch EEE, Bhopal, Madhya Pradesh, India.
   Vijayakumar, V., MIT Sq, Southampton, Hants, England.
   Butt, Mohsin, King Fahd Univ Petr \& Minerals, Coll Appl \& Supporting Studies, Dhahran, Saudi Arabia.},
DOI = {10.3233/JIFS-189132},
ISSN = {1064-1246},
EISSN = {1875-8967},
Keywords = {Automatic plant identification; residual networks; cognitive vision
   drone; deep learning; automatic spraying; Convolutional Neural Networks
   (CNN); smart devices; plant diseases},
Keywords-Plus = {CLASSIFICATION; SMART},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {glatif@pmu.edu.sa},
Affiliations = {Prince Mohammad Bin Fahd University; VIT Bhopal University; King Fahd
   University of Petroleum \& Minerals},
ResearcherID-Numbers = {BUTT, MUHAMMAD MOHSIN/D-1754-2017
   Rajagopal, Maheswar/I-7936-2019
   Alghazo, Jaafar/C-8250-2016},
ORCID-Numbers = {Rajagopal, Maheswar/0000-0001-7977-4751
   Alghazo, Jaafar/0000-0001-7518-4818},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Alghazo J., 2017, J TELECOMMUNICATION, V9, P33.
   Alghmgham DA., 2019, PROCEDIA COMPUT SCI, V163, P266, DOI {[}10.1016/j.procs.2019.12.108, DOI 10.1016/J.PROCS.2019.12.108].
   {[}Anonymous], 2013, INT J ENG SCI TECHNO.
   {[}Anonymous], 2019, ROUTLEDGE STUD MARK.
   Arun C. H., 2013, INT J COMPUTER APPL, V62, P1, DOI 10.5120/10129-4920.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chakrabarty A, 2020, STUD BIG DATA, V63, P229, DOI 10.1007/978-981-13-9177-4\_11.
   Deng LL, 2020, J INTELL FUZZY SYST, V38, P379, DOI 10.3233/JIFS-179413.
   Glorot X., 2010, AISTATS.
   Gopal A, 2012, 2012 INTERNATIONAL CONFERENCE ON MACHINE VISION AND IMAGE PROCESSING (MVIP), P5, DOI 10.1109/MVIP.2012.6428747.
   He K, 2015, IEEE C COMP VIS PATT, DOI {[}10.1109/CVPR.2015.7299173, DOI 10.1109/CVPR.2015.7299173].
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0\_38.
   Hewitt C., 2018, ARXIV PREPRINT ARXIV, V1811, P08398, DOI {[}10.48550/arXiv.1811.08398, DOI 10.48550/ARXIV.1811.08398].
   Iost FH, 2020, J ECON ENTOMOL, V113, P1, DOI 10.1093/jee/toz268.
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Khan T., 2020, INT BUSINESS TRADE I, P291.
   Latif G, 2020, WIREL NETW, V26, P2375, DOI 10.1007/s11276-019-02165-6.
   Latif G, 2019, IEEE ACCESS, V7, P9634, DOI 10.1109/ACCESS.2018.2888488.
   Latif G, 2018, CURR MED IMAGING, V14, P914, DOI 10.2174/1573405614666180402150218.
   Mallah C, 2013, SIGNAL PROCESS PATTE, P279.
   Mishra P. K., 2012, 2012 International Conference on Advances in Engineering, Science and Management (ICAESM), P68.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Nayak P, 2020, STUD BIG DATA, V63, P139, DOI 10.1007/978-981-13-9177-4\_7.
   Neto AJV, 2018, IEEE ACCESS, V6, P11101, DOI 10.1109/ACCESS.2018.2803439.
   Panda CK, 2020, NATURAL REMEDIES FOR PEST, DISEASE AND WEED CONTROL, P235, DOI 10.1016/B978-0-12-819304-4.00020-8.
   Pearline SA, 2019, J INTELL FUZZY SYST, V36, P1997, DOI 10.3233/JIFS-169911.
   Rashad M.Z., 2011, INT J COMPUT SCI INF, V3, P93, DOI DOI 10.5121/IJCSIT.2011.3407).
   Sasse F.C., 2018, THESIS.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Szegedy C., 2015, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2015.7298594.
   Valizadeh S., 2009, PIP 2009 INFR HIDD A, P580, DOI {[}10.1061/4106901o28360\% 2953, DOI 10.1061/4106901028360\%2953].
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {35},
Times-Cited = {8},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {33},
Journal-ISO = {J. Intell. Fuzzy Syst.},
Doc-Delivery-Number = {PF7MC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000599232800006},
DA = {2023-08-12},
}

@article{ WOS:000819010800001,
Author = {Kumar, Amit Krishan and Mai, Nguyen Ngoc and Kumar, Ashmit and Chand,
   Nividita V. and Assaf, Mansour H.},
Title = {Quantum classifier for recognition and identification of leaf profile
   features},
Journal = {EUROPEAN PHYSICAL JOURNAL D},
Year = {2022},
Volume = {76},
Number = {6},
Month = {JUN},
Abstract = {Quantum-based classifiers and architecture are gaining lots of attention
   in image representation and cryptography. The proposed algorithm applies
   a quantum classifier to a computer vision system for leaf recognition
   which can be applied to a quantum computer. Images from ten species of
   leaves which are categorised into two groups, namely simple and
   palmately, are recognised using a quantum classifier. The pixels of
   images are transformed to qubit states using quantum Fourier transform
   (QFT) and Hadamard gates. The profile and structural features are
   extracted by applying 1D-convolution and controlled not (CNOT) gates. A
   quantum nearest neighbour search classifier is used to find the closest
   matching leaf based on probability. The results for different levels of
   image processing are evaluated and compared with the nearest neighbour
   classifier. The recognition rate of the quantum classifier for the best
   level of image processing is 97.33\%. The recognition rate of the
   classifier is better than the nearest neighbour classifier and also has
   a low computation time.},
Publisher = {SPRINGER},
Address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
Type = {Article},
Language = {English},
Affiliation = {Kumar, AK (Corresponding Author), Beijing Inst Technol, Sch Automat, State Key Lab Intelligent Control \& Decis Complex, Beijing 100081, Peoples R China.
   Mai, NN (Corresponding Author), Nguyen Tat Thanh Univ, Dist 12, Ho Chi Minh City, Vietnam.
   Kumar, Amit Krishan, Beijing Inst Technol, Sch Automat, State Key Lab Intelligent Control \& Decis Complex, Beijing 100081, Peoples R China.
   Mai, Nguyen Ngoc, Nguyen Tat Thanh Univ, Dist 12, Ho Chi Minh City, Vietnam.
   Kumar, Ashmit; Chand, Nividita V., Fiji Natl Univ, Coll Agr Fisheries \& Forestry, Suva, Fiji.
   Assaf, Mansour H., Univ South Pacific, Sch Informat Technol Engn Math \& Phys, Suva, Fiji.},
DOI = {10.1140/epjd/s10053-022-00429-z},
Article-Number = {110},
ISSN = {1434-6060},
EISSN = {1434-6079},
Keywords-Plus = {IMAGE},
Research-Areas = {Optics; Physics},
Web-of-Science-Categories  = {Optics; Physics, Atomic, Molecular \& Chemical},
Author-Email = {fste\_11@yahoo.com
   nnmai@ntt.edu.vn},
Affiliations = {Beijing Institute of Technology; Nguyen Tat Thanh University (NTTU);
   Fiji National University (FNU); University of the South Pacific},
ResearcherID-Numbers = {Kumar, Amit Krishan/GRR-5642-2022},
ORCID-Numbers = {Kumar, Amit Krishan/0000-0002-0173-2081},
Cited-References = {Abdulmunem A., 2019, HUMAN ACTION RECOGNI, DOI {[}10.1063/1.5123119, DOI 10.1063/1.5123119].
   Abura'ed N, 2017, ACM COMPUT SURV, V49, DOI 10.1145/3009965.
   Aimeur E, 2006, LECT NOTES ARTIF INT, V4013, P431.
   Amin AHM, 2013, PROCEDIA COMPUT SCI, V24, P84, DOI 10.1016/j.procs.2013.10.030.
   Anant Bhardwaj, 2013, International Journal of Innovation and Applied Studies, V3, P237.
   Belis Vasilis, 2021, EPJ WEB C, V251.
   Blank C, 2020, NPJ QUANTUM INFORM, V6, DOI 10.1038/s41534-020-0272-6.
   Cai X., 2014, ENTANGLEMENT BASED Q.
   Cerezo M, 2021, NAT REV PHYS, V3, P625, DOI 10.1038/s42254-021-00348-9.
   Chaki J, 2020, J KING SAUD UNIV-COM, V32, P1158, DOI 10.1016/j.jksuci.2018.01.007.
   Chakraborty S, 2020, INT J THEOR PHYS, V59, P3348, DOI 10.1007/s10773-020-04590-2.
   Du J., 2005, SHAPE RECOGNITION BA.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Fatihah Sahidan N., 2019, INDONESIAN J ELECT E, V16, P737, DOI {[}10.11591/ijeecs.v16.i2.pp737-743, DOI 10.11591/IJEECS.V16.I2.PP737-743].
   Gandhi VS, 2013, IEEE IJCNN.
   Grant E, 2018, NPJ QUANTUM INFORM, V4, DOI 10.1038/s41534-018-0116-9.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Iliyasu AM, 2013, ENTROPY-SWITZ, V15, P2874, DOI 10.3390/e15082874.
   Jankowski N, 2011, LECT NOTES COMPUT SC, V7063, P238, DOI 10.1007/978-3-642-24958-7\_28.
   Kumar A, 2016, LECT NOTES COMPUT SC, V10036, P128, DOI 10.1007/978-3-319-51969-2\_11.
   Kumar A, 2015, IEEE IMTC P, P1326, DOI 10.1109/I2MTC.2015.7151465.
   Kumar AK, 2023, VISUAL COMPUT, V39, P2847, DOI 10.1007/s00371-022-02497-z.
   Kumar AK, 2020, IET IMAGE PROCESS, V14, P4606, DOI 10.1049/iet-ipr.2019.1458.
   Li HS, 2014, PROCEEDINGS OF 2014 IEEE INTERNATIONAL CONFERENCE ON PROGRESS IN INFORMATICS AND COMPUTING (PIC), P237, DOI 10.1109/PIC.2014.6972332.
   Li J., 2015, OPEN J APPL SCI, V5, P233, DOI DOI 10.4236/OJAPPS.2015.56024.
   Li PY, 2015, IEEE I CONF COMP VIS, P819, DOI 10.1109/ICCV.2015.100.
   Ma YL, 2020, IEEE ACCESS, V8, P210277, DOI 10.1109/ACCESS.2020.3038891.
   Maheshwari D, 2022, IEEE ACCESS, V10, P3705, DOI 10.1109/ACCESS.2021.3139323.
   Mora C, 2011, PLOS BIOL, V9, DOI 10.1371/journal.pbio.1000606.
   Mouine S., 2012, P 2 ACM INT C MULT R, P1.
   Otgonbaatar S, 2021, IEEE J-STARS, V14, P7057, DOI 10.1109/JSTARS.2021.3095377.
   Prasetyo E., 2018, TELKOMNIKA TELECOMMU, V16, P1826.
   Rashad M. Z., 2011, International Journal of Computer Science \& Information Technology, V3, P93, DOI 10.5121/ijcsit.2011.3407.
   Sang JZ, 2016, QUANTUM INF PROCESS, V15, P37, DOI 10.1007/s11128-015-1135-5.
   Schuld M, 2020, PHYS REV A, V101, DOI 10.1103/PhysRevA.101.032308.
   Schuld M, 2014, LECT NOTES ARTIF INT, V8862, P208, DOI 10.1007/978-3-319-13560-1\_17.
   Shu L., 2009, ACTA AGRONOMICA SINI.
   Sucithra B., 2020, International Journal of Computer Vision and Image Processing, V10, P15, DOI 10.4018/IJCVIP.2020040102.
   Sulc M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0265-4.
   Wang X, 2020, IEEE ACCESS, V8, P39175, DOI 10.1109/ACCESS.2020.2976117.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yuan SZ, 2019, INT J THEOR PHYS, V58, P2823, DOI 10.1007/s10773-019-04166-9.
   Zhang Y, 2013, QUANTUM INF PROCESS, V12, P2833, DOI 10.1007/s11128-013-0567-z.
   Zhong YH, 2012, J COMPUT, V7, P2312, DOI 10.4304/jcp.7.9.2312-2317.},
Number-of-Cited-References = {44},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Eur. Phys. J. D},
Doc-Delivery-Number = {2O4DL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000819010800001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000288361500149,
Author = {Bremananth, R. and Nithya, B. and Saipriya, R.},
Book-Group-Author = {IEEE},
Title = {Wood Species Recognition Using GLCM and Correlation},
Booktitle = {2009 INTERNATIONAL CONFERENCE ON ADVANCES IN RECENT TECHNOLOGIES IN
   COMMUNICATION AND COMPUTING (ARTCOM 2009)},
Year = {2009},
Pages = {615+},
Note = {International Conference on Advances in Recent Technologies in
   Communication and Computing, Kerala, INDIA, OCT 27-28, 2009},
Abstract = {The proposed system identifies the species of the wood using the
   textural features present in its barks. Each species of a wood has its
   own unique patterns in its bark, which enabled the proposed system to
   identify it accurately. Automatic wood recognition system has not yet
   been well established mainly due to lack of research in this area and
   the difficulty in obtaining the wood database. In our work, a wood
   recognition system has been designed based on pre-processing techniques,
   feature extraction and by correlating the features of those wood species
   for their classification. Texture classification is a problem that has
   been studied and tested using different methods due to its valuable
   usage in various pattern recognition problems, such as wood recognition,
   rock classification. The most popular technique used for the textural
   classification is Gray-level Co-occurrence Matrices (GLCM). The features
   from the enhanced images are thus extracted using the GLCM is
   correlated, which determines the classification between the various wood
   species. The result thus obtained shows a high rate of recognition
   accuracy proving that the techniques used in suitable to be implemented
   for commercial purposes.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Bremananth, R (Corresponding Author), Sri Ramakrishna Engn Coll, Dept Comp Applicat, Coimbatore, Tamil Nadu, India.
   Bremananth, R., Sri Ramakrishna Engn Coll, Dept Comp Applicat, Coimbatore, Tamil Nadu, India.
   Nithya, B.; Saipriya, R., Sri Ramakrishna Engn Coll, Dept Comp Technol, Coimbatore, Tamil Nadu, India.},
ISBN = {978-1-4244-5104-3},
Keywords = {Correlation; Grey Level Co-Occurrence Matrix; Probability Density
   Function; Wood Recognition},
Keywords-Plus = {CLASSIFICATION; IMAGES},
Research-Areas = {Engineering; Telecommunications},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Telecommunications},
Author-Email = {bremresearch@gmail.com
   nithi\_bathru@yahoo.co.in
   sai\_ramu@ymail.com},
Affiliations = {Sri Ramakrishna Engineering College; Sri Ramakrishna Engineering College},
ResearcherID-Numbers = {Ramachandran, Bremananth/X-8054-2018},
Cited-References = {Brandtberg T., FUZZY SETS SYSTEM, V132, P371.
   Jordan R, 1998, ULTRASONICS, V36, P219, DOI 10.1016/S0041-624X(97)00148-0.
   Kauppinen H., 1999, THESIS U OULU.
   Khalid M., 2008, DESIGN INTELLIGENT W, P9.
   Lampinen J, 1996, INT J PATTERN RECOGN, V10, P97, DOI 10.1142/S0218001496000098.
   Tou Jing Yi, 2002, GABOR FILTER GRAY LE.},
Number-of-Cited-References = {6},
Times-Cited = {22},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BTX39},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000288361500149},
DA = {2023-08-12},
}

@article{ WOS:000781068200001,
Author = {Onishi, Masanori and Watanabe, Shuntaro and Nakashima, Tadashi and Ise,
   Takeshi},
Title = {Practicality and Robustness of Tree Species Identification Using UAV RGB
   Image and Deep Learning in Temperate Forest in Japan},
Journal = {REMOTE SENSING},
Year = {2022},
Volume = {14},
Number = {7},
Month = {APR},
Abstract = {Identifying tree species from the air has long been desired for forest
   management. Recently, combination of UAV RGB image and deep learning has
   shown high performance for tree identification in limited conditions. In
   this study, we evaluated the practicality and robustness of the tree
   identification system using UAVs and deep learning. We sampled training
   and test data from three sites in temperate forests in Japan. The
   objective tree species ranged across 56 species, including dead trees
   and gaps. When we evaluated the model performance on the dataset
   obtained from the same time and same tree crowns as the training
   dataset, it yielded a Kappa score of 0.97, and 0.72, respectively, for
   the performance on the dataset obtained from the same time but with
   different tree crowns. When we evaluated the dataset obtained from
   different times and sites from the training dataset, which is the same
   condition as the practical one, the Kappa scores decreased to 0.47.
   Though coniferous trees and representative species of stands showed a
   certain stable performance regarding identification, some
   misclassifications occurred between: (1) trees that belong to
   phylogenetically close species, (2) tree species with similar leaf
   shapes, and (3) tree species that prefer the same environment.
   Furthermore, tree types such as coniferous and broadleaved or evergreen
   and deciduous do not always guarantee common features between the
   different trees belonging to the tree type. Our findings promote the
   practicalization of identification systems using UAV RGB images and deep
   learning.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Onishi, M (Corresponding Author), Kyoto Univ, Grad Sch Agr, Kyoto 6068502, Japan.
   Onishi, Masanori, Kyoto Univ, Grad Sch Agr, Kyoto 6068502, Japan.
   Watanabe, Shuntaro, Kagoshima Univ, Grad Sch Sci \& Engn, Kagoshima 8900065, Japan.
   Nakashima, Tadashi; Ise, Takeshi, Kyoto Univ, Field Sci Educ \& Res Ctr, Kyoto 6068502, Japan.},
DOI = {10.3390/rs14071710},
Article-Number = {1710},
EISSN = {2072-4292},
Keywords = {deep learning; UAV RGB image; tree species identification},
Keywords-Plus = {AIRBORNE LIDAR; INDIVIDUAL TREES; CLASSIFICATION; DISCRIMINATION;
   FUSION; LEAF},
Research-Areas = {Environmental Sciences \& Ecology; Geology; Remote Sensing; Imaging
   Science \& Photographic Technology},
Web-of-Science-Categories  = {Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science \& Photographic Technology},
Author-Email = {onishi.masanori.25e@kyoto-u.jp
   watanabe@sci.kagoshima-u.ac.jp
   nakashima.tadashi.p83@kyoto-u.jp
   ise.takeshi.5e@kyoto-u.acjp},
Affiliations = {Kyoto University; Kagoshima University; Kyoto University},
ORCID-Numbers = {Ise, Takeshi/0000-0003-4331-5144},
Funding-Acknowledgement = {JSPS KAKENHI {[}JP19J22591]; JST PRESTO {[}JPMJPR15O1]},
Funding-Text = {This research was funded by JSPS KAKENHI Grant Number JP19J22591 and JST
   PRESTO Grant Number JPMJPR15O1.},
Cited-References = {Asner GP, 1998, REMOTE SENS ENVIRON, V64, P234, DOI 10.1016/S0034-4257(98)00014-5.
   Baatz M., 2000, ANGEW GEOGRAPHISCHE.
   Ballanti L, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8060445.
   Bendig J, 2015, INT J APPL EARTH OBS, V39, P79, DOI 10.1016/j.jag.2015.02.012.
   Boschetti M, 2007, INT J REMOTE SENS, V28, P1251, DOI 10.1080/01431160600928542.
   Cao L, 2016, INT J APPL EARTH OBS, V49, P39, DOI 10.1016/j.jag.2016.01.007.
   Clark ML, 2012, REMOTE SENS-BASEL, V4, P1820, DOI 10.3390/rs4061820.
   Clark ML, 2005, REMOTE SENS ENVIRON, V96, P375, DOI 10.1016/j.rse.2005.03.009.
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104.
   Dalponte M, 2008, IEEE T GEOSCI REMOTE, V46, P1416, DOI 10.1109/TGRS.2008.916480.
   Dalponte M, 2019, PEERJ, V6, DOI 10.7717/peerj.6227.
   Dalponte M, 2013, IEEE T GEOSCI REMOTE, V51, P2632, DOI 10.1109/TGRS.2012.2216272.
   Dalponte M, 2012, REMOTE SENS ENVIRON, V123, P258, DOI 10.1016/j.rse.2012.03.013.
   Dalponte M, 2009, REMOTE SENS ENVIRON, V113, P2345, DOI 10.1016/j.rse.2009.06.013.
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013.
   Feret JB, 2013, IEEE T GEOSCI REMOTE, V51, P73, DOI 10.1109/TGRS.2012.2199323.
   Forestry Agency, 2020, ANN REP FOR FOR JAP.
   Gewali U.B., 2018, ARXIV180208701.
   Goodwin N, 2005, AUST J BOT, V53, P337, DOI 10.1071/BT04085.
   GRANT L, 1987, REMOTE SENS ENVIRON, V22, P309, DOI 10.1016/0034-4257(87)90064-2.
   Ishihara M., 2010, Japanese Journal of Ecology (Otsu), V60, P111.
   Jakubowski MK, 2013, REMOTE SENS-BASEL, V5, P4163, DOI 10.3390/rs5094163.
   Jansson G, 1999, LANDSCAPE ECOL, V14, P283, DOI 10.1023/A:1008085902053.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Ke YH, 2011, INT J REMOTE SENS, V32, P4725, DOI 10.1080/01431161.2010.494184.
   KNIPLING E B, 1970, Remote Sensing of Environment, V1, P155.
   Koch B, 2006, PHOTOGRAMM ENG REM S, V72, P357, DOI 10.14358/PERS.72.4.357.
   Machala M, 2014, EUR J REMOTE SENS, V47, P117, DOI 10.5721/EuJRS20144708.
   Maschler J, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081218.
   Mohan M, 2017, FORESTS, V8, DOI 10.3390/f8090340.
   Nakatake S., 2018, Journal of the Japanese Forest Society, V100, P149.
   Natesan S., 2019, INT ARCH PHOTOGRAMME, V4213, P475, DOI {[}DOI 10.5194/ISPRS-ARCHIVES-XLII-2-W13-475-2019, 10.5194/isprs-archives-XLII-2-W13-475-2019].
   Nevalainen O, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9030185.
   Onishi M, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-020-79653-9.
   Paszke A., 2017, NIPS AUT WORKSH.
   Picos J, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12050885.
   Chen Q, 2006, PHOTOGRAMM ENG REM S, V72, P923, DOI 10.14358/PERS.72.8.923.
   Rahman MT, 2015, URBAN FOR URBAN GREE, V14, P562, DOI 10.1016/j.ufug.2015.05.008.
   Safonova A, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11060643.
   Schreyer J, 2014, REMOTE SENS-BASEL, V6, P10636, DOI 10.3390/rs61110636.
   Shang X, 2014, IEEE J-STARS, V7, P2481, DOI 10.1109/JSTARS.2013.2282166.
   Shen X, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111180.
   Solberg S, 2006, PHOTOGRAMM ENG REM S, V72, P1369, DOI 10.14358/PERS.72.12.1369.
   Sothe C, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111338.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   TUCKER CJ, 1979, REMOTE SENS ENVIRON, V8, P127, DOI 10.1016/0034-4257(79)90013-0.
   Ustin SL, 2009, REMOTE SENS ENVIRON, V113, pS67, DOI 10.1016/j.rse.2008.10.019.
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967.
   Waser LT, 2014, REMOTE SENS-BASEL, V6, P4515, DOI 10.3390/rs6054515.
   Wulder M, 2000, REMOTE SENS ENVIRON, V73, P103, DOI 10.1016/S0034-4257(00)00101-2.
   Youngentob KN, 2011, REMOTE SENS ENVIRON, V115, P1115, DOI 10.1016/j.rse.2010.12.012.
   Zhang CY, 2012, PHOTOGRAMM ENG REM S, V78, P1079, DOI 10.14358/PERS.78.10.1079.
   Zhang ZY, 2016, FORESTS, V7, DOI 10.3390/f7060122.},
Number-of-Cited-References = {53},
Times-Cited = {3},
Usage-Count-Last-180-days = {12},
Usage-Count-Since-2013 = {51},
Journal-ISO = {Remote Sens.},
Doc-Delivery-Number = {0K8WA},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000781068200001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000843771300001,
Author = {Phyu Phyu Htun and Boschetti, Marco and Buriro, Attaullah and
   Confalonieri, Roberto and Sun, Boyuan and Htwe, Ah Nge and Tillo, Tammam},
Book-Group-Author = {IEEE},
Title = {A LIGHTWEIGHT APPROACH FOR WOOD HYPERSPECTRAL IMAGES CLASSIFICATION},
Booktitle = {2021 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA \& EXPO WORKSHOPS
   (ICMEW)},
Series = {IEEE International Conference on Multimedia and Expo Workshops},
Year = {2021},
Note = {IEEE International Conference on Multimedia and Expo (ICME), ELECTR
   NETWORK, JUL 05-09, 2021},
Organization = {IEEE},
Abstract = {This paper presents a Convolutional Neural Network (CNN)-based spatial
   classifier to classify hyperspectral images for wood recognition. The
   spatial classifier is built by adapting the input and output units of
   Cifar10Net, a conventional image classifier that accepts three-band
   images as input. Obtained results in terms of accuracy and training time
   show that the proposed classifier can be trained using few training
   data, and few computational resources.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Htun, PP (Corresponding Author), Univ Comp Studies, Yangon, Myanmar.
   Phyu Phyu Htun; Htwe, Ah Nge, Univ Comp Studies, Yangon, Myanmar.
   Boschetti, Marco, Microtec srl GmbH, Bolzano, Italy.
   Buriro, Attaullah; Confalonieri, Roberto; Sun, Boyuan, Free Univ Bolen Bolzano, Bolzano, Italy.
   Tillo, Tammam, Indraprastha Inst Informat Technol, Delhi, India.},
DOI = {10.1109/ICMEW53276.2021.9455943},
ISSN = {2330-7927},
ISBN = {978-1-6654-4989-2},
Keywords = {Hyperspectral images classification; computer vision; wood recognition},
Research-Areas = {Computer Science; Engineering; Imaging Science \& Photographic
   Technology},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Engineering, Electrical \& Electronic; Imaging
   Science \& Photographic Technology},
Author-Email = {phyuphyuhtun@ucsy.edu.mm
   marco.boschetti@microtec.eu
   attaullah.buriro@unibz.it
   roberto.confalonieri@unibz.it
   boyuan.sun@unibz.it
   ahngehtwe@ucsy.edu.mm
   tillo@iiitd.ac.in},
Affiliations = {Free University of Bozen-Bolzano; Indraprastha Institute of Information
   Technology Delhi},
Funding-Acknowledgement = {H2I project - EFRE-FESR {[}CUP: I56C19000100009]},
Funding-Text = {This work has been partly supported by the H2I project, funded by the
   EFRE-FESR programme 2014-2020 (CUP: I56C19000100009).},
Cited-References = {Faccini Simone, {*}{*}DATA OBJECT{*}{*}, P2021, DOI 10.17632/2sfw446fht.1.
   Hsieh TH, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20061734.
   Hu W, 2015, J SENSORS, V2015, DOI 10.1155/2015/258619.
   Ibrahim I, 2018, EUR J WOOD WOOD PROD, V76, P345, DOI 10.1007/s00107-017-1163-1.
   Snyder Michael, 2017, WHAT IS DIFFERENCE S.
   Wang H, 2019, 2019 4TH INTERNATIONAL CONFERENCE ON MECHANICAL, CONTROL AND COMPUTER ENGINEERING (ICMCCE 2019), P413, DOI 10.1109/ICMCCE48743.2019.00099.
   Ziadi A, 2007, CAN CON EL COMP EN, P417.},
Number-of-Cited-References = {7},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BT6SP},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000843771300001},
DA = {2023-08-12},
}

@article{ WOS:000846739000068,
Author = {Guender, Maurice and Yamati, Facundo R. Ispizua and Kierdorf, Jana and
   Roscher, Ribana and Mahlein, Anne-Katrin and Bauckhage, Christian},
Title = {Agricultural plant cataloging and establishment of a data framework from
   UAV-based crop images by computer vision},
Journal = {GIGASCIENCE},
Year = {2022},
Volume = {11},
Abstract = {Background Unmanned aerial vehicle (UAV)-based image retrieval in modern
   agriculture enables gathering large amounts of spatially referenced crop
   image data. In large-scale experiments, however, UAV images suffer from
   containing a multitudinous amount of crops in a complex canopy
   architecture. Especially for the observation of temporal effects, this
   complicates the recognition of individual plants over several images and
   the extraction of relevant information tremendously.
   Results In this work, we present a hands-on workflow for the automatized
   temporal and spatial identification and individualization of crop images
   from UAVs abbreviated as ``cataloging{''} based on comprehensible
   computer vision methods. We evaluate the workflow on 2 real-world
   datasets. One dataset is recorded for observation of Cercospora leaf
   spot-a fungal disease-in sugar beet over an entire growing cycle. The
   other one deals with harvest prediction of cauliflower plants. The plant
   catalog is utilized for the extraction of single plant images seen over
   multiple time points. This gathers a large-scale spatiotemporal image
   dataset that in turn can be applied to train further machine learning
   models including various data layers.
   Conclusion The presented approach improves analysis and interpretation
   of UAV data in agriculture significantly. By validation with some
   reference data, our method shows an accuracy that is similar to more
   complex deep learning-based recognition techniques. Our workflow is able
   to automatize plant cataloging and training image extraction, especially
   for large datasets.},
Publisher = {OXFORD UNIV PRESS},
Address = {GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Gunder, M (Corresponding Author), Schloss Birlinghoven, Fraunhofer IAIS, D-53757 St Augustin, Germany.
   Guender, Maurice; Bauckhage, Christian, Schloss Birlinghoven, Fraunhofer Inst Intelligent Anal \& Informat Syst, D-53757 St Augustin, Germany.
   Guender, Maurice; Bauckhage, Christian, Univ Bonn, Inst Comp Sci 3, Friedrich Hirzebruch Allee 5, D-53115 Bonn, Germany.
   Yamati, Facundo R. Ispizua; Mahlein, Anne-Katrin, Inst Sugar Beet Res IfZ, Holtenser Landstr 77, D-37079 Gottingen, Germany.
   Kierdorf, Jana; Roscher, Ribana, Univ Bonn, Inst Geodesy \& Geoinformat, Niebuhrstr 1a, D-53113 Bonn, Germany.
   Roscher, Ribana, Tech Univ Munich, Data Sci Earth Observat, Dept Aerosp \& Geodesy, Lise Meitner Str 9, D-85521 Ottobrunn, Germany.},
DOI = {10.1093/gigascience/giac054},
Article-Number = {giac054},
ISSN = {2047-217X},
Keywords = {UAV imaging; remote sensing; plant identification; plant
   individualization; precision agriculture},
Keywords-Plus = {CERCOSPORA LEAF-SPOT},
Research-Areas = {Life Sciences \& Biomedicine - Other Topics; Science \& Technology -
   Other Topics},
Web-of-Science-Categories  = {Biology; Multidisciplinary Sciences},
Author-Email = {mguender@uni-bonn.de},
Affiliations = {Fraunhofer Gesellschaft; University of Bonn; University of Bonn;
   Technical University of Munich},
ResearcherID-Numbers = {Ispizua Yamati, Facundo Ramón/ABQ-4266-2022
   Roscher, Ribana/N-2238-2014
   },
ORCID-Numbers = {Ispizua Yamati, Facundo Ramón/0000-0001-5775-3554
   Roscher, Ribana/0000-0003-0094-6210
   Gunder, Maurice/0000-0001-9308-8889
   Mahlein, Anne-Katrin/0000-0003-1091-3690
   Kierdorf, Jana/0000-0003-1145-1555},
Funding-Acknowledgement = {Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under
   Germany's Excellence Strategy {[}EXC 2070-390732324]; European
   Agriculture Fund for Rural Development; North-RhineWestphalia
   {[}17-02.12.01-10/16-EP-000461792519-001]; German Federal Ministry of
   Education and Research (BMBF) {[}01DD20001]},
Funding-Text = {This work has been funded by the Deutsche Forschungsgemeinschaft (DFG,
   German Research Foundation) under Germany's Excellence Strategy-EXC
   2070-390732324 and partially by the European Agriculture Fund for Rural
   Development with contribution from North-RhineWestphalia
   (17-02.12.01-10/16-EP-000461792519-001). The work is partly funded by
   the German Federal Ministry of Education and Research (BMBF) in the
   framework of the international future AI lab ``AI4EO-Artificial
   Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and
   Beyond{''} (grant number: 01DD20001).},
Cited-References = {{[}Anonymous], GOOGLE INC KEYHOLE M.
   {[}Anonymous], QGIS A FREE OPEN SOU.
   {[}Anonymous], QFIELD A FREE OPEN S.
   Bauckhage C, 2013, KUNSTL INTELL, V27, P313, DOI 10.1007/s13218-013-0273-0.
   Bock CH, 2022, TROP PLANT PATHOL, V47, P25, DOI 10.1007/s40858-021-00439-z.
   Bock CH, 2020, PHYTOPATHOL RES, V2, DOI 10.1186/s42483-020-00049-8.
   Drees L, 2021, COMPUT ELECTRON AGR, V190, DOI 10.1016/j.compag.2021.106415.
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242.
   Ester M., 1996, P 2 INT C KNOWL DISC, P226, DOI DOI 10.5555/3001460.3001507.
   Gorlich F, 2021, DRONES-BASEL, V5, DOI 10.3390/drones5020034.
   Grenzdorffer G. J., 2019, INT ARCH PHOTOGRAMME, V42, P329, DOI {[}10.5194/isprs-archives-XLII-2-W13-329-2019, DOI 10.5194/ISPRS-ARCHIVES-XLII-2-W13-329-2019].
   Gunder M., GIGASCIENCE DATABASE, P2022, DOI 10.5524/102225.
   Hamuda E, 2016, COMPUT ELECTRON AGR, V125, P184, DOI 10.1016/j.compag.2016.04.024.
   He KM, 2018, Arxiv, DOI {[}arXiv:1703.06870, DOI 10.48550/ARXIV.1703.06870].
   He KM, 2015, Arxiv, DOI {[}arXiv:1512.03385, DOI 10.48550/ARXIV.1512.03385].
   Huang CY, 2009, SENSORS-BASEL, V9, P4869, DOI 10.3390/s90604869.
   Huete AR, 1997, REMOTE SENS ENVIRON, V59, P440, DOI 10.1016/S0034-4257(96)00112-5.
   James K, 2020, METHODS ECOL EVOL, V11, P1509, DOI 10.1111/2041-210X.13473.
   Jay S, 2020, PLANT PHENOMICS, V2020, DOI 10.34133/2020/9452123.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Kierdorf J., 2022, ARXIV, DOI {[}10.48550/arXiv.2204.00294, DOI 10.48550/ARXIV.2204.00294].
   KWS SAAT SE \& Co. KGa, CERC TRAITS LEAFL GE.
   Lottes Philipp, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3024, DOI 10.1109/ICRA.2017.7989347.
   Louhaichi M, 2001, GEOCARTO INT, V16, P65, DOI {[}10.1080/10106040108542184, DOI 10.1080/10106040108542184].
   Maes WH, 2019, TRENDS PLANT SCI, V24, P152, DOI 10.1016/j.tplants.2018.11.007.
   Mahlein AK, 2018, ANNU REV PHYTOPATHOL, V56, P535, DOI 10.1146/annurev-phyto-080417-050100.
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46.
   Oniga V.E., 2018, PROCEEDINGS, V2, P352, DOI {[}DOI 10.3390/ECRS-2-05165, 10.3390/ecrs-2-05165].
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Rekha BU, 2018, PROCEEDINGS OF THE 2018 INTERNATIONAL CONFERENCE ON COMPUTATIONAL TECHNIQUES, ELECTRONICS AND MECHANICAL SYSTEMS (CTEMS), P193, DOI 10.1109/CTEMS.2018.8769124.
   Rondeaux G, 1996, REMOTE SENS ENVIRON, V55, P95, DOI 10.1016/0034-4257(95)00186-7.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Savian F, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12142194.
   SHANE WW, 1992, PLANT DIS, V76, P812, DOI 10.1094/PD-76-0812.
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0.
   TUCKER CJ, 1979, REMOTE SENS ENVIRON, V8, P127, DOI 10.1016/0034-4257(79)90013-0.
   Valente J, 2020, PRECIS AGRIC, V21, P1366, DOI 10.1007/s11119-020-09725-3.
   Xue JR, 2017, J SENSORS, V2017, DOI 10.1155/2017/1353691.
   Yamati F. R. I., 2022, Sugar Industry / Zuckerindustrie, V147, P79, DOI 10.39691/si28345.},
Number-of-Cited-References = {39},
Times-Cited = {3},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {2},
Journal-ISO = {GigaScience},
Doc-Delivery-Number = {4C9BL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000846739000068},
OA = {gold, Green Published, Green Submitted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000859804900030,
Author = {Kajihara, Alexandre Yuji and Bertolini, Diego and Schwerz, Andre Luis},
Book-Group-Author = {IEEE},
Title = {Identification of herbarium specimens: a case study with Piperaceae
   Giseke family},
Booktitle = {2022 29TH INTERNATIONAL CONFERENCE ON SYSTEMS, SIGNALS AND IMAGE
   PROCESSING (IWSSIP)},
Series = {International Conference on Systems Signals and Image Processing},
Year = {2022},
Note = {29th International Conference on Systems, Signals and Image Processing
   (IWSSIP), Sofia, BULGARIA, JUN 01-03, 2022},
Organization = {Tech Univ Sofia; IEEE Bulgaria Sect; IEEE CAS SSC Joint Bulgarian
   Chapter; Tech Univ Sofia, Res Dev Sector},
Abstract = {Although millions of herbarium specimens have been recently digitized,
   many of them have not yet been properly identified or reviewed. The main
   reason is that the classification process is manual, slow, and
   error-prone. Machine Learning techniques are promising alternatives for
   supporting herbarium plants identification. This paper evaluates feature
   extraction techniques and classification algorithms to identify
   herbarium specimens of the Piperaceae Giseke family at the genus level.
   For the evaluation, we extracted a balanced subset of pre-processed
   images from the five genera of the Piperaceae family (Manekia, Ottonia,
   Peperomia, Piper, and Pothomorphe) from the speciesLink repository. Our
   experiments point to potential support in identifying of herbarium
   images of the Piperaceae family, mainly for the genera Manekia,
   Peperomia and Ottonia. The best accuracy was 80.53\% achieved by
   combining MobileNet-V2 and the SVM classifier.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kajihara, AY (Corresponding Author), Univ Tecnol Fed Parana, Campo Mourao, Parana, Brazil.
   Kajihara, Alexandre Yuji; Bertolini, Diego; Schwerz, Andre Luis, Univ Tecnol Fed Parana, Campo Mourao, Parana, Brazil.},
DOI = {10.1109/IWSSIP55020.2022.9854444},
ISSN = {2157-8672},
ISBN = {978-1-6654-9578-3},
Keywords = {Machine Learning; identification support; herbarium specimens;
   Piperaceae},
Research-Areas = {Computer Science; Engineering; Imaging Science \& Photographic
   Technology},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Imaging Science \& Photographic Technology},
Author-Email = {kajihara@alunos.utfpr.edu.br
   diegobertolini@utfpr.edu.br
   andreluis@utfpr.edu.br},
Affiliations = {Universidade Tecnologica Federal do Parana},
Funding-Acknowledgement = {Federal University of Technology -Paran `a campus Campo Mourao},
Funding-Text = {We thank the Federal University of Technology -Paran ` a campus Campo
   Mourao for their financial support.},
Cited-References = {{[}Anonymous], 2009, BIOL SYSTEMATICS PRI.
   {[}Anonymous], 2015, P 3 INT C LEARNING R.
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
   Carranza-Rojas J, 2018, MULTIMED SYST APPL, P151, DOI 10.1007/978-3-319-76445-0\_9.
   Carranza-Rojas J, 2017, BMC EVOL BIOL, V17, P1, DOI 10.1186/s12862-017-1014-z.
   Christ Jheniffer Abeldt, 2016, Rodriguésia, V67, P1031, DOI 10.1590/2175-7860201667413.
   Clark J. Y., 2012, 2012 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB), P343, DOI 10.1109/CIBCB.2012.6217250.
   CRIA, 2022 PROJETOS SPECIE.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Howard A.G., 2017, ARXIV170404861 CS.
   Judd W.S., 2008, PLANT SYSTEMATICS PH, V3rd.
   Jye KS, 2017, FRONT LIFE SCI, V10, P98, DOI 10.1080/21553769.2017.1412361.
   Kittler J., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P897, DOI 10.1109/ICPR.1996.547205.
   Little DP, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11365.
   Mata-Montero E., 2016, PROC IFIP WORLD INFO, P26, DOI {[}10.1007/978-3-319-44447-5\_3, DOI 10.1007/978-3-319-44447-5\_3].
   Maurya R, 2021, COMPUT BIOL MED, V137, DOI 10.1016/j.compbiomed.2021.104862.
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
   Pryer KM, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11372.
   Unger J, 2016, BMC EVOL BIOL, V16, DOI 10.1186/s12862-016-0827-5.
   Younis S, 2018, BOT LETT, V165, P377, DOI 10.1080/23818107.2018.1446357.},
Number-of-Cited-References = {20},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BT9ED},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000859804900030},
DA = {2023-08-12},
}

@article{ WOS:000621776200007,
Author = {Isik, Sahin and Ozkan, Kemal},
Title = {Overview of handcrafted features and deep learning models for leaf
   recognition},
Journal = {JOURNAL OF ENGINEERING RESEARCH},
Year = {2021},
Volume = {9},
Number = {1},
Month = {MAR},
Abstract = {In this study, an automated system for classification of leaf species
   based on the global and local features is presented by concentrating on
   a smart and unorthodox decision system. The utilized global features
   consist of 11 features and are separated into two categories: gross
   shape features (7) and moment based features (4), respectively. In case
   of local features, only the curve points on Bezier curves are accepted
   as discriminative features. With the purpose of reducing the search
   space and improving the performance of the system, firstly, the class
   label of leaf object is determined by conducting the global features
   with respect to predefined threshold values. Once the target class is
   determined, the local features have been performed on in order to
   validate the label of leaf sample. After conducting experiments on the
   K-Nearest Neighbor (K-NN) with Hausdorff distance, this system provides
   valuable accuracy rate as achieving the 96.78\% performance on Flavia
   and the 94.66\% on Swedish dataset Moreover, by applying a deep learning
   model, namely, Inception-v3 architecture, the superior results were
   recorded as 99.11\% and 98.95\% when compared to state-of-the-art
   methods. It turns out that one can use our feature extraction and
   classification technique or Inception-v3 model by considering
   compromises and commutations about efficiency and effectiveness.},
Publisher = {ACADEMIC PUBLICATION COUNCIL},
Address = {PO BOX 17225, KHALDIYA 72453, KUWAIT},
Type = {Article},
Language = {English},
Affiliation = {Isik, S (Corresponding Author), Eskisehir Osmangazi Univ, Dept Comp Engn, Meselik Campus, TR-26480 Eskisehir, Turkey.
   Isik, Sahin; Ozkan, Kemal, Eskisehir Osmangazi Univ, Dept Comp Engn, Meselik Campus, TR-26480 Eskisehir, Turkey.},
DOI = {10.36909/jer.v9i1.7737},
ISSN = {2307-1877},
EISSN = {2307-1885},
Keywords = {Leaf Classification; Gross Shape Features; Moment Based Features; Bezier
   Curve Points; Hausdorff Distance; Deep Learning},
Keywords-Plus = {PLANT; CLASSIFICATION; IMAGES},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Multidisciplinary},
Author-Email = {sahini@ogu.edu.tr},
Affiliations = {Eskisehir Osmangazi University},
ResearcherID-Numbers = {Özkan, Kemal/GLU-8209-2022
   isik, sahin/H-5373-2018},
ORCID-Numbers = {Özkan, Kemal/0000-0003-2252-2128
   isik, sahin/0000-0003-1768-7104},
Cited-References = {Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Franz E., 1990, PAPER AM SOC AGR ENG.
   Gulmezoglu MB, 1999, IEEE T SPEECH AUDI P, V7, P620, DOI 10.1109/89.799687.
   Horng JH, 2001, PATTERN RECOGN LETT, V22, P183, DOI 10.1016/S0167-8655(00)00104-5.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073.
   Iglesias A., COMPUTER AIDED GEOME.
   Jobin A, 2012, PROC TECH, V1, P171, DOI 10.1016/j.protcy.2012.10.021.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kvam PH, 2007, NONPARAMETRIC STATISTICS WITH APPLICATIONS TO SCIENCE AND ENGINEERING, P1, DOI 10.1002/9780470168707.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Liu JC, 2018, CHIN AUTOM CONGR, P3165, DOI 10.1109/CAC.2018.8623427.
   Lopez Barrientos J., 2017, LEAF RECOGNITION DEE.
   Mizuta M, 1996, INFORMATION INTELLIGENCE AND SYSTEMS, VOLS 1-4, P516, DOI 10.1109/ICSMC.1996.569845.
   Pawara P, 2017, LECT NOTES COMPUT SC, V10617, P615, DOI 10.1007/978-3-319-70353-4\_52.
   Remagnino P., 2017, MACHINE LEARNING PLA, P57, DOI {[}10.1007/978-3-662-53745-9\_4, DOI 10.1007/978-3-662-53745-9\_4].
   Rhouma MB, 2017, COMPUT ELECTRON AGR, V142, P326, DOI 10.1016/j.compag.2017.08.029.
   Shipunov AB, 2005, BIOL J LINN SOC, V85, P1, DOI 10.1111/j.1095-8312.2005.00468.x.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Soderkvist O, 2001, COMPUTER VISION CLAS.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhang Jianjun, 2006, Conf Proc IEEE Eng Med Biol Soc, VSuppl, P6769.
   Zhang SW, 2017, CLUSTER COMPUT, V20, P1517, DOI 10.1007/s10586-017-0859-7.
   Zhang X, 2019, MULTIMED TOOLS APPL, V78, P27463, DOI 10.1007/s11042-019-07846-0.},
Number-of-Cited-References = {28},
Times-Cited = {3},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {10},
Journal-ISO = {J. Eng. Res.},
Doc-Delivery-Number = {QM4UQ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000621776200007},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000363980400024,
Author = {Tiay, Tanakorn and Benyaphaichit, Pipimphorn and Riyamongkol, Panomkhawn},
Editor = {Mitrpanont, JL},
Title = {Flower Recognition System Based on Image Processing},
Booktitle = {2014 THIRD ICT INTERNATIONAL STUDENT PROJECT CONFERENCE (ICT-ISPC)},
Year = {2014},
Pages = {99-102},
Note = {3rd ICT International Student Project Conference (lSPC), Mahidol Univ,
   Fac ICT, THAILAND, MAR 26-27, 2014},
Organization = {Symphony Commun Publ Co; Tangerine Co; Avalant Co; CPRAM Co; Green Spot
   Co; Mister Cup Co; MUlCT Alumni Assoc; TAT; Univ Teknologi Malaysia; ICT},
Abstract = {The flower recognition system based on image processing has been
   developed. This system uses edge and color characteristics of flower
   images to classify flowers. Hu's seven-moment algorithm is applied to
   acquire edge characteristics. Red, green, blue, hue, and saturation
   characteristics are derived from histograms. K-nearest neighbor is used
   to classify flowers. The accuracy of this system is more than 80\%.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Tiay, T (Corresponding Author), Naresuan Univ, Fac Engn, Dept Elect \& Comp Engn, Muang 65000, Phitsanulok, Thailand.
   Tiay, Tanakorn; Benyaphaichit, Pipimphorn; Riyamongkol, Panomkhawn, Naresuan Univ, Fac Engn, Dept Elect \& Comp Engn, Muang 65000, Phitsanulok, Thailand.},
ISBN = {978-1-4799-5573-2},
Keywords = {flower recognition; flower detection; image processing; K-nearest
   neighbor; Hu's seven moments; graph cut},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications; Engineering,
   Electrical \& Electronic},
Author-Email = {tanakornt53@email.nu.ac.th
   pipimphornb53@email.nu.ac.th
   panomkhawnr@nu.ac.th},
Affiliations = {Naresuan University},
Cited-References = {Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5.
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851.
   Chai Y., 2011, THESIS U OXFORD.
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964.
   Das M, 1999, IEEE INTELL SYST APP, V14, P24, DOI 10.1109/5254.796084.
   Gonzalez R.C., 2004, DIGITAL IMAGE PROCES.
   Gonzalez R.C., 2009, DIGITAL IMAGE PROCES, DOI {[}DOI 10.1117/1.3115362, 10.1117/1.3115362].
   Kasyauqi R., 2013, SMART COMPUTING REV, V3.
   Nilsback M.-E., 2006, P IEEE COMP SOC C CO, V2, P1447, DOI DOI 10.1109/CVPR.2006.42.
   Saitoh T., 2003, SYSTEM COMPUTER JAPA, V34.
   Sunpetchniyom T, 2012, 2012 6TH INTERNATIONAL CONFERENCE ON NEW TRENDS IN INFORMATION SCIENCE, SERVICE SCIENCE AND DATA MINING (ISSDM2012), P819.
   Vuarnoz V., 2010, FLOWER RECOGNITION.
   Zhihu Huang, 2010, 2010 2nd International Conference on Computer Engineering and Technology (ICCET), P476, DOI 10.1109/ICCET.2010.5485542.},
Number-of-Cited-References = {13},
Times-Cited = {10},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BD8JA},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000363980400024},
DA = {2023-08-12},
}

@article{ WOS:000881204100001,
Author = {Batchuluun, Ganbayar and Nam, Se Hyun and Park, Kang Ryoung},
Title = {Deep Learning-Based Plant Classification Using Nonaligned Thermal and
   Visible Light Images},
Journal = {MATHEMATICS},
Year = {2022},
Volume = {10},
Number = {21},
Month = {NOV},
Abstract = {There have been various studies conducted on plant images. Machine
   learning algorithms are usually used in visible light image-based
   studies, whereas, in thermal image-based studies, acquired thermal
   images tend to be analyzed with a naked eye visual examination. However,
   visible light cameras are sensitive to light, and cannot be used in
   environments with low illumination. Although thermal cameras are not
   susceptible to these drawbacks, they are sensitive to atmospheric
   temperature and humidity. Moreover, in previous thermal camera-based
   studies, time-consuming manual analyses were performed. Therefore, in
   this study, we conducted a novel study by simultaneously using thermal
   images and corresponding visible light images of plants to solve these
   problems. The proposed network extracted features from each thermal
   image and corresponding visible light image of plants through residual
   block-based branch networks, and combined the features to increase the
   accuracy of the multiclass classification. Additionally, a new database
   was built in this study by acquiring thermal images and corresponding
   visible light images of various plants.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Park, KR (Corresponding Author), Dongguk Univ, Div Elect \& Elect Engn, 30 Pildong Ro,1 Gil, Seoul 04620, South Korea.
   Batchuluun, Ganbayar; Nam, Se Hyun; Park, Kang Ryoung, Dongguk Univ, Div Elect \& Elect Engn, 30 Pildong Ro,1 Gil, Seoul 04620, South Korea.},
DOI = {10.3390/math10214053},
Article-Number = {4053},
EISSN = {2227-7390},
Keywords = {plant image; image classification; thermal image; visible light image;
   deep learning},
Research-Areas = {Mathematics},
Web-of-Science-Categories  = {Mathematics},
Author-Email = {parkgr@dongguk.edu},
Affiliations = {Dongguk University},
ORCID-Numbers = {NAM, SE HYUN/0000-0002-0181-8774},
Funding-Acknowledgement = {National Research Foundation of Korea (NRF) - Ministry of Science and
   ICT (MSIT) through the Basic Science Research Program
   {[}NRF-2022R1F1A1064291]; NRF - MSIT through the Basic Science Research
   Program {[}NRF-2020R1A2C1006179, 2021R1F1A1045587]; National Research
   Foundation of Korea {[}2021R1F1A1045587] Funding Source: Korea Institute
   of Science \& Technology Information (KISTI), National Science \&
   Technology Information Service (NTIS)},
Funding-Text = {This research was supported in part by the National Research Foundation
   of Korea (NRF) funded by the Ministry of Science and ICT (MSIT) through
   the Basic Science Research Program (NRF-2022R1F1A1064291), in part by
   the NRF funded by the MSIT through the Basic Science Research Program
   (NRF-2021R1F1A1045587), and in part by the NRF funded by the MSIT
   through the Basic Science Research Program (NRF-2020R1A2C1006179).},
Cited-References = {Abdul Hamid N.N.A., 2019, INDONES J ELECT ENG, V14, P333, DOI {[}10.11591/ijeecs.v14.i1.pp333-339, DOI 10.11591/IJEECS.V14.I1.PP333-339].
   Anasta N., 2021, IOP Conference Series: Earth and Environmental Science, V739, DOI 10.1088/1755-1315/739/1/012088.
   {[}Anonymous], FIDS30 DATABASE.
   {[}Anonymous], WILD PLANTS FREE MEA.
   {[}Anonymous], BOLIVIA PLANT PROCES.
   {[}Anonymous], ANAL VARIANCE.
   {[}Anonymous], LOGITECH C270 HD WEB.
   {[}Anonymous], PLANTVILLAGE DATASET.
   {[}Anonymous], CATEGORICAL CROSS EN.
   {[}Anonymous], PLANTCR THERVISDB.
   {[}Anonymous], NVIDIA GEFORCE GTX T.
   {[}Anonymous], PROCESSING LINES MED.
   Ashwinkumar S, 2022, MATER TODAY-PROC, V51, P480, DOI 10.1016/j.matpr.2021.05.584.
   Biswas B., 2019, COMPUTATIONAL INTELL, P105, DOI {[}10.1007/978-981-13-9042-5\_10, DOI 10.1007/978-981-13-9042-5\_10].
   Castellano Giovanna, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P588, DOI 10.1007/978-3-030-66823-5\_35.
   Chakraborty Aditya, 2021, Proceedings of 5th International Conference on Computing Methodologies and Communication (ICCMC 2021), P1624, DOI 10.1109/ICCMC51019.2021.9418042.
   Chompookham Thipwimon, 2021, ICIC Express Letters, V15, P553, DOI 10.24507/icicel.15.06.553.
   Dandekar M., 2021, P INT JOINT C NEURAL, P1.
   Derczynski L, 2016, LREC 2016 - TENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P261.
   Dey S, 2022, PLANT BIOSYST, V156, P411, DOI 10.1080/11263504.2020.1866093.
   Franczyk Bogdan, 2020, Procedia Computer Science, V176, P1211, DOI 10.1016/j.procs.2020.09.117.
   Ghosh S, 2020, PROCEEDINGS OF 2020 IEEE APPLIED SIGNAL PROCESSING CONFERENCE (ASPCON 2020), P163, DOI {[}10.1109/ASPCON49795.2020.9276669, 10.1109/aspcon49795.2020.9276669].
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hossain MS, 2019, IEEE T IND INFORM, V15, P1027, DOI 10.1109/TII.2018.2875149.
   Hussain I., 2020, INT J COMPUTER, V39, P88, DOI {[}10.1007/978-3-319-96133-0\_23, DOI 10.1007/978-3-319-96133-0\_23].
   Kader A., 2020, INT RES J ENG TECHNO, V7, P1.
   Keras Chollet F., US.
   Kingma D. P., 2014, P INT C LEAR REPR.
   Lydia M.S., 2019, P 4 INT C COMPUTING.
   Marrelli M, 2021, PLANTS-BASEL, V10, DOI 10.3390/plants10071355.
   Muhathir M., 2020, J INFORM TELECOMMUN, V4, P1, DOI {[}10.31289/jite.v4i1.3860, DOI 10.31289/JITE.V4I1.3860].
   OpenCV, US.
   Powers D. M., 2010, J MACH LEARN TECHNOL, P37.
   Python, US.
   Raza SEA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0123262.
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767.
   Rhamadiyanti Dzalfa Tsalsabila, 2021, 2021 International Seminar on Intelligent Technology and Its Applications (ISITIA), P226, DOI 10.1109/ISITIA52817.2021.9502258.
   Rudnik K, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9193971.
   Santos Thiago, 2019, Zenodo, DOI 10.5281/ZENODO.3361736.
   Shahi TB, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0264586.
   Siddiqi R., 2020, P 11 INT C ADV INFOR, P1, DOI {[}10.1145/3406601.3406619, DOI 10.1145/3406601.3406619].
   Siddiqi R, 2019, ICDLT 2019: 2019 3RD INTERNATIONAL CONFERENCE ON DEEP LEARNING TECHNOLOGIES, P91, DOI 10.1145/3342999.3343002.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Singh D, 2020, ACM INT CONF PR SER, P249, DOI 10.1145/3371158.3371196.
   Srivastava S., 2020, INT J ENG RES TECHNO, V9, P896.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243.
   Wang DF, 2021, COMPUT ELECTRON AGR, V190, DOI 10.1016/j.compag.2021.106468.
   Yilma G, 2021, TURK J ELECTR ENG CO, V29, P2869, DOI 10.3906/elk-2105-115.
   Zhu WJ, 2018, IFAC PAPERSONLINE, V51, P424, DOI 10.1016/j.ifacol.2018.08.184.},
Number-of-Cited-References = {50},
Times-Cited = {2},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {8},
Journal-ISO = {Mathematics},
Doc-Delivery-Number = {6B2YE},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000881204100001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000468730700296,
Author = {Raut, Swati P. and Bhalchandra, A. S.},
Book-Group-Author = {IEEE},
Title = {Plant Recognition System based on Leaf Image},
Booktitle = {PROCEEDINGS OF THE 2018 SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT
   COMPUTING AND CONTROL SYSTEMS (ICICCS)},
Year = {2018},
Pages = {1579-1581},
Note = {2nd International Conference on Intelligent Computing and Control
   Systems (ICICCS), Vaigai Coll Engn, Madurai, INDIA, JUN 14-15, 2018},
Organization = {IEEE; Electron Devices Soc},
Abstract = {Nature has huge members of plants identifying them and classifying them
   is important task for botanists. Usually plants are recognized by leaf
   and its characteristics like shape, texture, vein structure, color etc.
   A system is developed which recognizes plants automatically based on
   leaf structure using image processing. Moreover evolutionary changes are
   also taking place in plants and it has impact on identification and
   classification. Image data base and related information is stored on
   cloud. Shape measure include area, perimeter, ratio of MajorAxisLength
   and MinorAxisLength, vein structure indicating angle of sub veins with
   major vein, texture are the major feature used. Botanists visit remote
   places, jungles where the plants need to be identified and classified.
   So an attempt is made to develop an automatic identification system
   where in an image of leaf is captured by any smart phone, uploaded on
   cloud, where image available and complete data base is trained. Captured
   image is processed features will be extracted and gives to classifies
   the plant classification result will be transmitted back to smart phone
   and related information.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Raut, SP (Corresponding Author), Govt Coll Engn, Dept Elect, Aurangabad, Maharashtra, India.
   Raut, Swati P.; Bhalchandra, A. S., Govt Coll Engn, Dept Elect, Aurangabad, Maharashtra, India.},
ISBN = {978-1-5386-2842-3},
Keywords = {leaf pattern; machine learning; digital signature},
Research-Areas = {Automation \& Control Systems; Computer Science},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Artificial Intelligence},
Author-Email = {Swatiraut288@gmail.com
   asbhalchandra@gmail.com},
Affiliations = {Government College of Engineering, Aurangabad},
ResearcherID-Numbers = {Bhalchandra, Anjali/X-6029-2019},
ORCID-Numbers = {Bhalchandra, Anjali/0000-0001-9744-6663},
Cited-References = {Amlekar M. M, 2015, ICPC.
   {[}Anonymous], 2007, LEAF RECOGNITION ALG.
   Arun Priya C., 2012, PATTERN RECOGNITION.
   chaki Jyotismita, 2011, INT J ADV COMPUTER S.
   chemburkar Aamod, 2014, IJECS.
   Elhariri Esraa, 2014, PLANT CLASSIFICATION.
   Gonzalez wood, 2002, IMAGE PROCESSING.
   Gopal A., 2012, CLASSIFICATION SELEC.
   Kadir A., 2011, INT J COMPUTER TREND.
   Pankaja K., 2017, ICIMIA.
   Rao Ausha, 2017, ICIMIA.
   Singh Satnam, 2015, IOSE JECE.},
Number-of-Cited-References = {12},
Times-Cited = {2},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BM8AW},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000468730700296},
DA = {2023-08-12},
}

@article{ WOS:000659345500001,
Author = {Sun, Yongke and Lin, Qizhao and He, Xin and Zhao, Youjie and Dai, Fei
   and Qiu, Jian and Cao, Yong},
Title = {Wood Species Recognition with Small Data: A Deep Learning Approach},
Journal = {INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS},
Year = {2021},
Volume = {14},
Number = {1},
Pages = {1451-1460},
Abstract = {Wood species recognition is an important work in the wood trade and wood
   commercial activities. Although many recognition methods were presented
   in recent years, the existing wood species recognition methods mainly
   use shallow recognition models with low accuracy and are still
   unsatisfying for many real-world applications. Besides, their
   generalization ability is not strong. In this paper, a novel
   deep-learning-based wood species recognition method was proposed, which
   improved the accuracy and generalization greatly. The method uses 20X
   amplifying glass to acquire wood images, extracts the image features
   with ResNet50 neural network, refines the features with linear
   discriminant analysis (LDA), and recognizes the wood species with a KNN
   classifier. Our data was small, but we adopted transfer learning to
   improve our method. About 3000 wood images were used in our wood species
   recognition experiments and our method was executed in 25 rare wood
   species and the results showed our method had better generalization
   performance and accuracy. Compared with traditional deep learning our
   results were obtained from a small amount of data, which just confirmed
   the effectiveness of our method. (C) 2021 The Authors. Published by
   Atlantis Press B.V.},
Publisher = {SPRINGERNATURE},
Address = {CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Cao, Y (Corresponding Author), Southwest Forestry Univ, Coll Mat Sci \& Engn, Kunming 650224, Yunnan, Peoples R China.
   Sun, Yongke, Southwest Forestry Univ, Yunnan Prov Key Lab Wood Adhes \& Glued Prod, Kunming 650224, Yunnan, Peoples R China.
   Lin, Qizhao; He, Xin; Zhao, Youjie; Qiu, Jian; Cao, Yong, Southwest Forestry Univ, Coll Mat Sci \& Engn, Kunming 650224, Yunnan, Peoples R China.
   Dai, Fei, Southwest Forestry Univ, Coll Big Data \& Intelligent Engn, Kunming 650224, Yunnan, Peoples R China.},
DOI = {10.2991/ijcis.d.210423.001},
ISSN = {1875-6891},
EISSN = {1875-6883},
Keywords = {Wood recognition; Transfer learning; Generalization performance; Feature
   extraction; ResNet50; Linear discriminant analysis; KNN},
Keywords-Plus = {LINEAR DISCRIMINANT-ANALYSIS; IDENTIFICATION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications},
Author-Email = {cn\_caoyong@126.com},
Affiliations = {Southwest Forestry University - China; Southwest Forestry University -
   China; Southwest Forestry University - China},
ORCID-Numbers = {You-jie, Zhao/0000-0002-3198-0465},
Funding-Acknowledgement = {Project of National Natural Science Foundation {[}61962055, 31960142];
   Major Project of Science and Technology of Yunnan Province
   {[}202002AD080002, 2019ZE005]},
Funding-Text = {This work was supported by Project of National Natural Science
   Foundation (61962055 and 31960142), and the Major Project of Science and
   Technology of Yunnan Province (Grant No. 202002AD080002 and No.
   2019ZE005).},
Cited-References = {Abidin AZ, 2018, COMPUT BIOL MED, V95, P24, DOI 10.1016/j.compbiomed.2018.01.008.
   Barmpoutis P, 2018, COMPUT ELECTRON AGR, V144, P241, DOI 10.1016/j.compag.2017.12.011.
   Bhardwaj S, 2019, J TEST EVAL, V47, P4412, DOI 10.1520/JTE20180580.
   Cesar R.M., 1995, INTRO NEURAL NETWORK.
   Chicco D, 2020, BMC GENOMICS, V21, DOI 10.1186/s12864-019-6413-7.
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964.
   Fuentealba C, 2004, 2004 IEEE International Conference on Industrial Technology (ICIT), Vols. 1- 3, P763.
   Gani W, 2013, QUAL RELIAB ENG INT, V29, P841, DOI 10.1002/qre.1440.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hua-yu D., 2017, J NORTHWEST FOR UNIV, V32, P244.
   Huang YL, 2020, BIORESOURCES, V15, P130, DOI 10.15376/biores.15.1.130-141.
   Huh S, 2010, IEEE T NEURAL NETWOR, V21, P1990, DOI 10.1109/TNN.2010.2090047.
   Ibrahim I, 2018, EUR J WOOD WOOD PROD, V76, P345, DOI 10.1007/s00107-017-1163-1.
   Karalis G, 2020, ADV EXP MED BIOL, V1194, P239, DOI 10.1007/978-3-030-32622-7\_21.
   Kobayashi K, 2015, J WOOD SCI, V61, P630, DOI 10.1007/s10086-015-1507-6.
   Manik F.Y., 2020, J PHYS C SER, V1566.
   Martinez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974.
   Nisgoski S, 2018, MADERAS-CIENC TECNOL, V20, P199, DOI 10.4067/S0718-221X2018005002401.
   Peng Z, 2013, OPTIK, V124, P2833, DOI 10.1016/j.ijleo.2012.08.058.
   Ravindran P, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0292-9.
   Ruffinatto F, 2015, IAWA J, V36, P208, DOI 10.1163/22941932-00000096.
   Santosa S., 2019, IAENG INT J COMPUTER, V46, P149.
   Shang DY, 2020, J CHROMATOGR A, V1615, DOI 10.1016/j.chroma.2019.460775.
   Singh S, 2001, PATTERN RECOGN, V34, P1601, DOI 10.1016/S0031-3203(00)00099-6.
   Souza DV, 2020, WOOD SCI TECHNOL, V54, P1065, DOI 10.1007/s00226-020-01196-z.
   Sun Y, 2015, J COMPUTATIONAL THEO, V12, P5372, DOI {[}10.1166/jctn.2015.4529, DOI 10.1166/JCTN.2015.4529].
   Vinjamuri R, 2014, COMPUT INTEL NEUROSC, V2014, DOI 10.1155/2014/373957.
   Wang HJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0076101.
   Wang ZH, 2020, SENSOR ACTUAT B-CHEM, V309, DOI 10.1016/j.snb.2020.127767.
   Xia XL, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P783, DOI 10.1109/ICIVC.2017.7984661.
   Yang J., 2019, 2019 IEEE INT C COMP.
   Zapolska A, 2020, INT J REMOTE SENS, V41, P5388, DOI 10.1080/01431161.2020.1731931.
   Zhao P, 2014, OPTIK, V125, P1144, DOI 10.1016/j.ijleo.2013.07.124.},
Number-of-Cited-References = {33},
Times-Cited = {10},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {24},
Journal-ISO = {Int. J. Comput. Intell. Syst.},
Doc-Delivery-Number = {SP0EL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000659345500001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000320971900022,
Author = {Wang, Hang-jun and Qi, Heng-nian and Wang, Xiao-Feng},
Title = {A new Gabor based approach for wood recognition},
Journal = {NEUROCOMPUTING},
Year = {2013},
Volume = {116},
Number = {SI},
Pages = {192-200},
Month = {SEP 20},
Note = {7th International Conference on Intelligent Computing (ICIC), Zhengzhou,
   PEOPLES R CHINA, AUG 11-14, 2011},
Organization = {IEEE Computat Intelligence Soc; Int Neural Network Soc; Natl Sci Fdn
   China},
Abstract = {Correct wood recognition has an important meaning in the rational use of
   wood resources. To complete this task automatically, based on wood
   stereogram images, we propose a new Gabor based wood recognition
   approach in this paper, which has been successfully applied in many
   pattern recognition fields for its robustness against local distortions.
   However, only a few approaches can make full use of the information in
   Gabor patterns. To obtain more information of Gabor feature for wood
   recognition, we first use a set of 40 Gabor patterns to represent a wood
   image, which consist of important information at different orientation
   and scales. Then, we apply the block-based feature extraction with more
   statistical features besides mean and standard deviation on these Gabor
   patterns to enhance the discriminative ability of our approach. Finally,
   we reduce the dimensionality of the proposed feature descriptor by using
   feature selection. Only a few features are selected to achieve both high
   recognition performance and computational efficiency. We evaluate our
   approach on the wood database in Zhejiang A \& F University (ZAFU),
   which contains 24 wood species. Experimental results show that by
   adopting proper sub-block numbers and blocking schemes, our approach
   outperforms the most of state-of-the-art approaches. (C) 2012 Elsevier
   B.V. All rights reserved.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article; Proceedings Paper},
Language = {English},
Affiliation = {Qi, HN (Corresponding Author), Zhejiang A\&F Univ, Sch Informat Engn, Linan 311300, Peoples R China.
   Wang, Hang-jun; Wang, Xiao-Feng, Chinese Acad Sci, Hefei Inst Intelligent Machines, Hefei 230031, Peoples R China.
   Wang, Hang-jun, Univ Sci \& Technol China, Dept Automat, Hefei 230027, Peoples R China.
   Wang, Hang-jun; Qi, Heng-nian, Zhejiang A\&F Univ, Sch Informat Engn, Linan 311300, Peoples R China.
   Wang, Xiao-Feng, Hefei Univ, Dept Comp Sci \& Technol, Key Lab Network \& Intelligent Informat Proc, Hefei 230601, Peoples R China.},
DOI = {10.1016/j.neucom.2012.02.045},
ISSN = {0925-2312},
EISSN = {1872-8286},
Keywords = {Wood recognition; Gabor patterns; Wood stereogram images; Texture
   analysis; Feature selection},
Keywords-Plus = {PALMPRINT RECOGNITION; FACE RECOGNITION; FEATURES; TEXTURE;
   REPRESENTATION; HISTOGRAM; SELECTION; PATTERNS},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {hangjunw@email.ustc.edu.cn
   qihengnian@yahoo.com.cn
   xfwang@iim.ac.cn},
Affiliations = {Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Science \& Technology of China, CAS; Zhejiang A\&F University; Hefei
   University},
Cited-References = {Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244.
   Bianconi F, 2007, PATTERN RECOGN, V40, P3325, DOI 10.1016/j.patcog.2007.04.023.
   Caleanu C, 2007, PATTERN RECOGN LETT, V28, P950, DOI 10.1016/j.patrec.2006.12.013.
   Colak S, 2003, PROCEEDINGS OF THE IEEE 29TH ANNUAL NORTHEAST BIOENGINEERING CONFERENCE, P122.
   Cotter SF, 2001, SIGNAL PROCESS, V81, P1849, DOI 10.1016/S0165-1684(01)00064-0.
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160.
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676.
   Du YZ, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/936512.
   Gabor D., 1946, J I ELECT ENGR, V93, P429, DOI {[}DOI 10.1049/JI-3-2.1946.0074, 10.1049/ji-3-2.1946.0074].
   Gao XB, 2009, NEUROCOMPUTING, V72, P3174, DOI 10.1016/j.neucom.2009.03.003.
   Gheyas IA, 2010, PATTERN RECOGN, V43, P5, DOI 10.1016/j.patcog.2009.06.009.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Huang DS, 2008, PATTERN RECOGN, V41, P1316, DOI 10.1016/j.patcog.2007.08.016.
   Li B, 2008, PATTERN RECOGN, V41, P3813, DOI 10.1016/j.patcog.2008.05.027.
   Li B, 2008, PATTERN RECOGN, V41, P3287, DOI 10.1016/j.patcog.2008.05.014.
   Li XL, 2008, IEEE T SYST MAN CY C, V38, P145, DOI 10.1109/TSMCC.2007.913886.
   Pan X, 2008, NEUROCOMPUTING, V71, P3032, DOI 10.1016/j.neucom.2007.12.030.
   Pan X, 2009, NEUROCOMPUTING, V72, P2040, DOI 10.1016/j.neucom.2008.11.019.
   Perez CA, 2011, PATTERN RECOGN, V44, P951, DOI 10.1016/j.patcog.2010.10.017.
   Shang L, 2006, NEUROCOMPUTING, V69, P1782, DOI 10.1016/j.neucom.2005.11.004.
   Shen L, 2011, IET IMAGE PROCESS, V5, P394, DOI 10.1049/iet-ipr.2009.0251.
   Shen LL, 2006, PATTERN ANAL APPL, V9, P273, DOI 10.1007/s10044-006-0033-y.
   Tao DC, 2007, IEEE T PATTERN ANAL, V29, P1700, DOI 10.1109/TPAMI.2007.1096.
   Wang HJ, 2009, WORLD SUMMIT ON GENETIC AND EVOLUTIONARY COMPUTATION (GEC 09), P985.
   Wang HangJun, 2009, Journal of Zhejiang Forestry College, V26, P896.
   Wang HJ, 2009, ADV ATMOS SCI, V26, P613, DOI 10.1007/s00376-009-0613-z.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wang XF, 2009, IEEE T KNOWL DATA EN, V21, P1515, DOI 10.1109/TKDE.2009.21.
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235.
   Xie SF, 2010, IEEE T IMAGE PROCESS, V19, P1349, DOI 10.1109/TIP.2010.2041397.
   Xie X., 2008, J ELECTRON IMAGING, V17, P1.
   Zhang BH, 2007, IEEE T IMAGE PROCESS, V16, P57, DOI 10.1109/TIP.2006.884956.
   Zhang BC, 2010, NEURAL COMPUT APPL, V19, P617, DOI 10.1007/s00521-009-0311-x.
   Zhang W.C., 2006, P INT C PATT REC, P606.
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786.
   Zhao ZQ, 2007, NEUROCOMPUTING, V71, P448, DOI 10.1016/j.neucom.2007.07.010.},
Number-of-Cited-References = {36},
Times-Cited = {15},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {35},
Journal-ISO = {Neurocomputing},
Doc-Delivery-Number = {172AI},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000320971900022},
DA = {2023-08-12},
}

@inproceedings{ WOS:000786719700010,
Author = {Satake, Sara S. and Calvo, Rodrigo and Britto Jr, Alceu S. and Costa,
   Yandre M. G.},
Editor = {Rozinaj, G and Vargic, R},
Title = {Classification of Toxic Ornamental Plants for Domestic Animals Using CNN},
Booktitle = {SYSTEMS, SIGNALS AND IMAGE PROCESSING, IWSSIP 2021},
Series = {Communications in Computer and Information Science},
Year = {2022},
Volume = {1527},
Pages = {108-120},
Note = {28th International Conference on Systems, Signals and Image Processing
   (IWSSIP), ELECTR NETWORK, JUN 02-04, 2021},
Organization = {Slovak Univ Technol},
Abstract = {Veterinary medicine emphasizes accidents caused by toxic plants with
   domestic animals as an extremely important topic, as the right diagnosis
   can be crucial for the affected animal. In this work, we propose the
   classification of toxic ornamental plants, according to nine different
   categories, using five widely-known CNN architectures, namely: DenseNet,
   ResNet, VGG16, VGG19 and Xception. The rationale behind it is that the
   automatic identification of these types of plant can be a useful tool to
   help in the prevention of those accidents. The authors have carefully
   curated a database to support the development of this work, collecting
   images available on the Pinterest website, and also performing some
   important data pre-processing. This database was also made available as
   a contribution of this work. Transfer learning was employed by taking
   advantage of feature learned from the ImageNet dataset. We also analyzed
   the heat maps generated by the Layer-wise Relevant Propagation method,
   which allowed to observe the individual behavior of the best and worst
   architectures. The best performance was achieved using DenseNet, with an
   accuracy of 97.67\%. That model managed to generalize very well, even to
   deal with noisy images, which are frequent in photos of decorative
   environments.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Costa, YMG (Corresponding Author), State Univ Maringa UEM, Maringa, Parana, Brazil.
   Satake, Sara S.; Calvo, Rodrigo; Costa, Yandre M. G., State Univ Maringa UEM, Maringa, Parana, Brazil.
   Britto Jr, Alceu S., Pontifical Catholic Univ Parana PUCPR, Curitiba, Parana, Brazil.},
DOI = {10.1007/978-3-030-96878-6\_10},
ISSN = {1865-0929},
EISSN = {1865-0937},
ISBN = {978-3-030-96878-6; 978-3-030-96877-9},
Keywords = {Plant classification; Toxic ornamental plants for animals; Convolutional
   Neural Networks; Computer vision; Machine learning; Pattern recognition;
   Layer-wise Relevant Propagation},
Research-Areas = {Computer Science; Engineering; Imaging Science \& Photographic
   Technology},
Web-of-Science-Categories  = {Computer Science, Software Engineering; Engineering, Electrical \&
   Electronic; Imaging Science \& Photographic Technology},
Author-Email = {pg402922@uem.br
   rcalvo@uem.br
   alceu@ppgia.pucpr.br
   yandre@din.uem.br},
Affiliations = {Universidade Estadual de Maringa; Pontificia Universidade Catolica do
   Parana},
Funding-Acknowledgement = {Coordination of Superior Level Staff Improvement (CAPES); Brazilian
   National Research Council (CNPq)},
Funding-Text = {We thank the Coordination of Superior Level Staff Improvement (CAPES)
   and the Brazilian National Research Council (CNPq) for the financial
   support.},
Cited-References = {Barroso C.M., 2007, ORNAMENTAL HORTIC, V13.
   Botelho JD, 2014, CIENC RURAL, V44, P1810, DOI 10.1590/0103-8478cr20131036.
   Chen ZX, 2017, INTERSPEECH, P102, DOI 10.21437/Interspeech.2017-1085.
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   Conceicao J.L.S., 2015, REV UNINGA REV, V24, P59.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Joly, 2017, PLANT IDENTIFICATION.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Marmanis D, 2016, IEEE GEOSCI REMOTE S, V13, P105, DOI 10.1109/LGRS.2015.2499239.
   Martins D. B., 2013, Arquivos de Ciencias Veterinarias e Zoologia da UNIPAR, V16, P11.
   Nakahata GHS, 2020, IEEE INT SYM MULTIM, P104, DOI 10.1109/ISM.2020.00025.
   Ribeiro A.M., 2020, REV SISTEMAS COMPUTA, V10, P122.
   Sampat MP, 2009, IEEE T IMAGE PROCESS, V18, P2385, DOI 10.1109/TIP.2009.2025923.
   Santos CRO, 2013, MED VET-RECIFE, V7, P11.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sunderhauf Niko, 2014, P CLEF WORK NOT, P756.
   Tenedini V., 2016, ANAIS SALAO INT ENSI, V7.
   VASCONCELOS J., 2009, REV CIENTIFICA UFPA, V7.
   Xu Q, 2019, NEUROCOMPUTING, V328, P69, DOI 10.1016/j.neucom.2018.03.080.
   Yalcin H, 2016, INT CONF AGRO-GEOINF, P233.
   Yang YC, 2018, IEEE INT CONF HEALT, P152, DOI 10.1109/ICHI.2018.00025.
   Zeinsteger P. A., 2004, Revista Veterinaria, V15, P35.
   Zhai A, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P515, DOI 10.1145/3041021.3054201.
   Zhang JM, 2019, MATH BIOSCI ENG, V16, P3345, DOI 10.3934/mbe.2019167.},
Number-of-Cited-References = {25},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BT0EG},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000786719700010},
DA = {2023-08-12},
}

@inproceedings{ WOS:000543935900039,
Author = {Adam, Sukemi and Amir, Amirullah},
Book-Group-Author = {IEEE},
Title = {Fruit Plant Leaf Identification Feature Extraction Using Zernike Moment
   Invariant (ZMI) and Methods Backpropagation},
Booktitle = {2019 INTERNATIONAL CONFERENCE ON INFORMATICS, MULTIMEDIA, CYBER AND
   INFORMATION SYSTEM (ICIMCIS)},
Year = {2019},
Pages = {225-229},
Note = {1st International Conference on Informatics, Multimedia, Cyber and
   Information System (ICIMCIS), Jakarta, INDONESIA, OCT 24-25, 2019},
Abstract = {The concept of pattern recognition is often used to identify a wide
   range of objects. Due to the ability to recognize objects is needed by
   humans. One of them is for pattern recognition on the leaves as
   identification in determining the types of leaves. However, in the
   acquisition, very frequent disturbances called noise. Noise in the image
   is a region of pixel image intensity of unwanted or deemed to disturb
   the segmentation process until the introduction. The impact of noise can
   degrade the image quality when the segmentation process. Therefore, in
   this study, the researchers added a preprocessing stage to reduce noise
   modest invisible when the acquisition using the camera. Gaussian filter
   used as a technique to tackle the problem at last preprocessing. Aside
   from the noise, constraints at the time of feature extraction of natural
   researchers also because the study took shape characteristic based on
   the area of the image. So if the object changes the coordinates of the
   start pixel image was unrecognizable. Based on these problems do
   research to identify the leaves by using Zernike Moment invariant
   feature extraction (ZMI) and Backpropagation algorithm. Based on the
   testing that was done on 100 test data success rate Based on these
   problems do research to identify the leaves by using Zernike Moment
   invariant feature extraction (ZMI) and Backpropagation algorithm. Based
   on the testing that was done on 100 test data success rate78\%.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Adam, S (Corresponding Author), Univ Sriwijaya, Palembang, Indonesia.
   Adam, Sukemi; Amir, Amirullah, Univ Sriwijaya, Palembang, Indonesia.},
DOI = {10.1109/icimcis48181.2019.8985219},
ISBN = {978-1-7281-2930-3},
Keywords = {Feature Extraction; Gaussian filter; Zernike Moment invariant;
   Backpropagation; Leaf Recognition},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Theory \&
   Methods},
Author-Email = {sukemi@ilkom.unsri.ac.id
   09011281320015@students.ilkom.unsri.ac.id},
Affiliations = {Universitas Sriwijaya},
Cited-References = {Abdullah RF, 2017, PROS ANN RES SEMIN, V3, P65.
   {[}Anonymous], 2011, PRACTICAL IMAGE VIDE.
   Bellingradt D, 2017, NEW DIR BOOK HIST, P1, DOI 10.1007/978-3-319-53366-7\_1.
   Chen CY, 2010, J CLIN GERONTOL GERI, V1, P2, DOI 10.1016/j.jcgg.2010.10.002.
   Faarifah RY, 2017, J INF SCI TEKNOL, V2, P101.
   Jeon WS, 2017, INT J FUZZY LOG INTE, V17, P26, DOI 10.5391/IJFIS.2017.17.1.26.
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109.
   Kisi O, 2005, INDIAN J ENG MATER S, V12, P434.
   Kumar TP, 2016, P INT C COMP VIS IM, V460, P531.
   Liantoni, 2015, J SEMIN TEKNOL, V5, P9.
   Oluleye B., 2014, BRIT J MATH COMPUTER, V4, P2217, DOI DOI 10.9734/BJMCS/2014/10931.
   Rahman Ziaur, 2019, International Journal of Computers and Applications, V41, P207, DOI 10.1080/1206212X.2017.1422358.
   Rao P., 2013, INT J LATEST TRENDS, V2, P228.
   Rumelhart DE, 1986, BACKPROPAGATION BASI.
   Sardogan M, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING (UBMK), P382, DOI 10.1109/UBMK.2018.8566635.
   Zhang WC, 2017, PATTERN RECOGN, V63, P193, DOI 10.1016/j.patcog.2016.10.008.},
Number-of-Cited-References = {16},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BP2NB},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000543935900039},
DA = {2023-08-12},
}

@inproceedings{ WOS:000458872701030,
Author = {Carpentier, Mathieu and Giguere, Philippe and Gaudreault, Jonathan},
Editor = {Maciejewski, AA and Okamura, A and Bicchi, A and Stachniss, C and Song, DZ and Lee, DH and Chaumette, F and Ding, H and Li, JS and Wen, J and Roberts, J and Masamune, K and Chong, NY and Amato, N and Tsagwarakis, N and Rocco, P and Asfour, T and Chung, WK and Yasuyoshi, Y and Sun, Y and Maciekeski, T and Althoefer, K and AndradeCetto, J and Chung, WK and Demircan, E and Dias, J and Fraisse, P and Gross, R and Harada, H and Hasegawa, Y and Hayashibe, M and Kiguchi, K and Kim, K and Kroeger, T and Li, Y and Ma, S and Mochiyama, H and Monje, CA and Rekleitis, I and Roberts, R and Stulp, F and Tsai, CHD and Zollo, L},
Book-Author = {Kosecka, J},
Title = {Tree Species Identification from Bark Images Using Convolutional Neural
   Networks},
Booktitle = {2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)},
Series = {IEEE International Conference on Intelligent Robots and Systems},
Year = {2018},
Pages = {1075-1081},
Note = {25th IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS), Madrid, SPAIN, OCT 01-05, 2018},
Organization = {IEEE Robot \& Automat Soc; IEEE Ind Elect Soc; Robot Soc Japan; Soc
   Instrument \& Control Engineers; New Technol Fdn; IEEE; Adept
   MobileRobots; Willow Garage; Aldebaran Robot; Natl Instruments;
   Reflexxes GmbH; Schunk Intec S L U; Univ Carlos III Madrid; BOSCH; JD
   COM; Pal Robot; KUKA; Santander; Squirrel AI Learning; Baidu; Generat
   Robots; KINOVA Robot; Ouster; Univ Pablo Olavide Sevilla; Rapyuta Robot;
   SICK; TOYOTA; UP; Amazon; ARGO; Built Robot; Disney Res; Easy Mile;
   Hitachi; Robot; Khalifa Univ; Magazino; MathWorks; New Dexterity;
   Schunk; nuTonomy; PILZ; Prophesee; Rootnik; Saga Robot; Shadow; Soft
   Bank Robot; Anyverse; GalTech; Generat Robot; IEEE CAA Journal
   Automatica Sinica; Sci Robot, AAAS; TERAS},
Abstract = {Tree species identification using bark images is a challenging problem
   that could prove useful for many forestry related tasks. However, while
   the recent progress in deep learning showed impressive results on
   standard vision problems, a lack of datasets prevented its use on tree
   bark species classification. In this work, we present, and make publicly
   available, a novel dataset called BarkNet 1.0 containing more than
   23,000 high-resolution bark images from 23 different tree species over a
   wide range of tree diameters. With it, we demonstrate the feasibility of
   species recognition through bark images, using deep learning. More
   specifically, we obtain an accuracy of 93.88\% on single crop, and an
   accuracy of 97.81\% using a majority voting approach on all of the
   images of a tree. We also empirically demonstrate that, for a fixed
   number of images, it is better to maximize the number of tree
   individuals in the training database, thus directing future data
   collection efforts.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
ISSN = {2153-0858},
ISBN = {978-1-5386-8094-0},
Research-Areas = {Computer Science; Robotics},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Robotics},
ORCID-Numbers = {gaudreault, jonathan/0000-0001-5493-8836
   Giguere, Philippe/0000-0002-7520-8290},
Cited-References = {Adelson E., 2009, J VISUAL-JAPAN, V9, P784, DOI {[}10.1167/9.8.784, DOI 10.1167/9.8.784].
   Atanasov N, 2016, INT J ROBOT RES, V35, P73, DOI 10.1177/0278364915596589.
   Blaanco LJ, 2016, INT CONF CONTEMP, P248.
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS).
   Boudra S, 2015, LECT NOTES COMPUT SC, V9386, P764, DOI 10.1007/978-3-319-25903-1\_66.
   Bressane A, 2015, ENVIRON MONIT ASSESS, V187, DOI 10.1007/s10661-015-4400-2.
   Champ  J., 2015, CEUR WORKSHOP P, V1391.
   Chi ZR, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS \& SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P1035.
   CIMPOI M, 2015, PROC CVPR IEEE, P3828, DOI DOI 10.1109/CVPR.2015.7299007.
   Fiel S., 2011, P 16 COMP VIS WINT W, P67.
   Ghasemi Toudeshki  A., 2018, UAV VISUAL TEACH REP.
   Goeau  H., 2017, CLEF WORKING NOTES, V2017.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123.
   Hellstrom  T., 2009, INT J FOREST ENG, V20.
   Huang ZK, 2006, LECT NOTES COMPUT SC, V4113, P1121, DOI 10.1007/11816157\_138.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Marcos D, 2016, INT C PATT RECOG, P2012, DOI 10.1109/ICPR.2016.7899932.
   Mizoguchi  T., 2017, P SOC PHOTO-OPT INS.
   Ojala T, 2002, INT C PATT RECOG, P701, DOI 10.1109/ICPR.2002.1044854.
   Othmani AA, 2016, MACH VISION APPL, V27, P751, DOI 10.1007/s00138-015-0738-2.
   Ramos FT, 2007, IEEE INT CONF ROBOT, P2036, DOI 10.1109/ROBOT.2007.363621.
   Shen L, 2016, LECT NOTES COMPUT SC, V9911, P467, DOI 10.1007/978-3-319-46478-7\_29.
   Smolyanskiy N, 2017, IEEE INT C INT ROBOT, P4241, DOI 10.1109/IROS.2017.8206285.
   Stanislaw  J., 2018, ICLR WORKSH.
   Sulc  M., 2014, TREE IDENTIFICATION.
   Sulc  M., 2016, WORKING NOTES CLEF.
   Sulc M, 2013, INT CONF IMAG VIS, P82, DOI 10.1109/IVCNZ.2013.6726996.
   Sunderhauf Niko, 2014, P CLEF WORK NOT, P756.
   Svab M, 2014, COMPUTER VISION BASE.
   Trottier  L., 2017, MULTITASK LEARNING D.
   Zhang Y., 2017, SURVEY MULTITASK LEA.},
Number-of-Cited-References = {33},
Times-Cited = {27},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BM0LT},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000458872701030},
OA = {Green Submitted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000235659100016,
Author = {Wu, QF and Zhou, CL and Wang, CN},
Editor = {Shen, HT and Li, JB and Li, ML and Ni, J and Wang, W},
Title = {Feature extraction and XML representation of plant leaf for image
   retrieval},
Booktitle = {ADVANCED WEB AND NETWORK TECHNOLOGIES, AND APPLICATIONS, PROCEEDINGS},
Series = {Lecture Notes in Computer Science},
Year = {2006},
Volume = {3842},
Pages = {127-131},
Note = {8th Asia-Pacific Web Conference and Workshops (APWeb 2006), Harbin,
   PEOPLES R CHINA, JAN 16-18, 2006},
Organization = {Natl Nat Sci Fdn China; Australian Res Council Res Network EII; Harbin
   Inst Technol; Heilongjiang Univ; Hohai Univ; Yellow River Conservat
   Commiss},
Abstract = {Leaf recognition and retrieval plays an important role in plant
   recognition and retrieval and its key issue lies in whether selected
   features are stable and have good ability to discriminate different
   kinds of leaves. From the view of plant leaf morphology, domain-related
   visual features and semantic features of plant leaf are analyzed and
   extracted First. Then these features are translated into a hierarchy
   that is easily represented by XML. On such a basis, the leaf image
   retrieval system proposed in this paper provides two types of retrieval
   methods, which could give better precision and flexibility. Experiment
   results prove the effectiveness of selected features and performance
   superiority of the leaf image retrieval system.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Article; Proceedings Paper},
Language = {English},
Affiliation = {Wu, QF (Corresponding Author), Xiamen Univ, Dept Comp Sci, Inst Artificial Intelligence, Fujian 361005, Peoples R China.
   Xiamen Univ, Dept Comp Sci, Inst Artificial Intelligence, Fujian 361005, Peoples R China.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {3-540-31158-0},
Research-Areas = {Computer Science; Remote Sensing; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods; Remote Sensing; Telecommunications},
Author-Email = {qfwu@xmu.edu.cn},
Affiliations = {Xiamen University},
ResearcherID-Numbers = {Wu, QF/G-3372-2010
   Zhou, CL/G-4667-2010},
ORCID-Numbers = {Zhou, CL/0000-0002-6779-7670},
Cited-References = {{[}傅弘 Fu Hong], 2004, {[}植物学通报, Chinese Bulletin of Botany], V21, P429.
   Han JW, 2003, SIGNAL PROCESS-IMAGE, V18, P141, DOI 10.1016/S0923-5965(02)00116-9.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Mehrotra K, 1997, ELEMENTS ARTIFICIAL.
   NISHIDA IC, 1998, P IEEE INT CON PATT, P1171.
   {[}祁亨年 Qi Hengnian], 2003, {[}浙江林学院学报, Journal of Zhejiang Forestry College], V20, P281.
   Tam AM, 2001, J AM SOC INF SCI TEC, V52, P930, DOI 10.1002/asi.1151.
   Wang ZY, 2000, LECT NOTES COMPUT SC, V1929, P477.
   Zhang HR, 2002, INT J POWDER METALL, V38, P7.
   Zhou Z, 2001, SCHOOL PSYCHOL INT, V22, P5, DOI 10.1177/01430343010221001.},
Number-of-Cited-References = {10},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BDV63},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000235659100016},
DA = {2023-08-12},
}

@article{ WOS:000539678000001,
Author = {Wang, Zhaobin and Cui, Jing and Zhu, Ying},
Title = {Plant recognition based on Jaccard distance and BOW},
Journal = {MULTIMEDIA SYSTEMS},
Year = {2020},
Volume = {26},
Number = {5},
Pages = {495-508},
Month = {OCT},
Abstract = {Plant recognition is a meaningful research that has attracted many
   researchers. Due to the variety of plants, it is difficult for the
   existing identification methods to identify their species efficiently.
   We proposes a plant recognition method based on Jaccard distance and Bag
   of words (BOW). Firstly, Jaccard distance is employed to calculate the
   similarity between the test sample and part of the training samples of
   all species, C-1 species with the highest similarity are selected as
   candidate species of the test image, which not only reduce the amount of
   computation but also shorten the time consumption. Secondly, BOW is
   employed to extract features from texture image and contour image, and
   support vector machine is used for training and classification. In our
   method, the texture and contour features of leaf images are extracted by
   Laws texture measure and Sobel operators respectively. The local and
   global features of the leaf can be described well. Some representative
   datasets are used to evaluate the proposed method and obtain high
   accuracy. Comparison with existing methods proves that the proposed
   method not only has a high accuracy, but also has robustness in noise
   environment.},
Publisher = {SPRINGER},
Address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
Type = {Article},
Language = {English},
Affiliation = {Wang, ZB (Corresponding Author), Lanzhou Univ, Sch Infomat Sci \& Engn, Lanzhou 730000, Peoples R China.
   Wang, Zhaobin; Cui, Jing, Lanzhou Univ, Sch Infomat Sci \& Engn, Lanzhou 730000, Peoples R China.
   Zhu, Ying, Gansu Acad Sci, Inst Biol, Key Lab Microbial Resources Exploitat \& Applicat, Lanzhou, Peoples R China.},
DOI = {10.1007/s00530-020-00657-6},
EarlyAccessDate = {JUN 2020},
ISSN = {0942-4962},
EISSN = {1432-1882},
Keywords = {Plant recognition; Feature extraction; Bag of words; Jaccard distance},
Keywords-Plus = {LEAF RECOGNITION; SHAPE-FEATURES; SPARSE REPRESENTATION; TEXTURE;
   DESCRIPTORS; ALGORITHM},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Theory \&
   Methods},
Author-Email = {zhaobin\_wang@hotmail.com
   zhuying\_365@126.com},
Affiliations = {Lanzhou University},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}61201421]},
Funding-Text = {This work was funded by National Natural Science Foundation of China
   (Grant No. 61201421).},
Cited-References = {{[}Anonymous], 2006, IEEE COMPUTER SOC C.
   Bertrand S, 2018, ECOL INFORM, V46, P57, DOI 10.1016/j.ecoinf.2018.05.007.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Cantero SVAB, 2018, IEEE T CYBERNETICS, V20, P20.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chengzhuan Y., IEEE ACCESS.
   Demisse GG, 2018, IEEE T PATTERN ANAL, V20, P1.
   Di Ruberto C, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS (VISAPP), VOL 1, P601.
   Florindo JB, 2020, INF SCI, V20, P20.
   Florindo JB, 2017, INFORM SCIENCES, V415, P142, DOI 10.1016/j.ins.2017.06.022.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Hsiao JK, 2014, 2014 SCIENCE AND INFORMATION CONFERENCE (SAI), P389, DOI 10.1109/SAI.2014.6918216.
   Huang L, 2019, RESP RES, V20, DOI 10.1186/s12931-019-0983-4.
   Jiao MZ, 2008, 28TH INTERNATIONAL DISPLAY RESEARCH CONFERENCE, P11.
   Kolivand H, 2019, ARAB J SCI ENG, V44, P3315, DOI 10.1007/s13369-018-3504-8.
   Kumar P.S. V. V. S.R., 2017, INT C CONT COMP INF, P548.
   Larese MG, 2015, MACH VISION APPL, V27, P1.
   Laws K.I, USCCIP940.
   Liu NA, 2016, NEUROCOMPUTING, V216, P460, DOI 10.1016/j.neucom.2016.08.005.
   Pires RDL, 2016, COMPUT ELECTRON AGR, V125, P48, DOI 10.1016/j.compag.2016.04.032.
   Mittal P, 2018, 2ND INTERNATIONAL CONFERENCE ON INTELLIGENT CIRCUITS AND SYSTEMS (ICICS 2018), P184, DOI 10.1109/ICICS.2018.00046.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Pearline A, 2019, IET IMAGE PROCESS.
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   Salleh S. S., 2011, 2011 UkSim 13th International Conference on Computer Modelling and Simulation (UKSim 2011), P319, DOI 10.1109/UKSIM.2011.67.
   Seeland M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0175101.
   Soderkvist O, COMPUTER VISION CLAS.
   Sun Guo-dong, 2017, Optics and Precision Engineering, V25, P224, DOI 10.3788/OPE.20172501.0224.
   Suresha M, 2017, 2017 2ND INTERNATIONAL CONFERENCE FOR CONVERGENCE IN TECHNOLOGY (I2CT), P663, DOI 10.1109/I2CT.2017.8226213.
   Tang L, 2013, APPL MECH MATER, V321-324, P956, DOI 10.4028/www.scientific.net/AMM.321-324.956.
   Tsolakidis DG, 2014, ARTIF INTELL METHODS, V20, P20.
   Turkoglu M, 2019, APPL MATH COMPUT, V352, P1, DOI 10.1016/j.amc.2019.01.054.
   turkolu M., PHYS A.
   VijayaLakshmi B, 2016, COMPUT ELECTRON AGR, V125, P99, DOI 10.1016/j.compag.2016.04.033.
   Wang B, 2015, INFORM SCIENCES, V302, P132, DOI 10.1016/j.ins.2014.07.028.
   Wang Z, 2017, NEURAL PROCESS LETT, V47, P1.
   Wang ZB, 2016, NEURAL COMPUT APPL, V27, P899, DOI 10.1007/s00521-015-1904-1.
   Wang ZB, 2014, IEEE IJCNN, P975, DOI 10.1109/IJCNN.2014.6889656.
   Xu Y, 2013, INFORM SCIENCES, V238, P138, DOI 10.1016/j.ins.2013.02.051.
   Xuming Yu, 2016, 2016 IEEE International Workshop on Electromagnetics (iWEM): Applications and Student Innovation Competition, P1, DOI 10.1109/iWEM.2016.7505042.
   Yang C, 2019, SIGNAL PROCESS-IMAGE, V20, P20.
   Yousefi E, 2017, COMPUT ELECTRON AGR, V140, P70, DOI 10.1016/j.compag.2017.05.031.
   Zeng J, 2019, IEEE ACCESS, V20, P11.
   Zhang L, 2017, CLUSTER COMPUT, V20, P20.
   Zhang SW, 2017, CLUSTER COMPUT, V20, P1517, DOI 10.1007/s10586-017-0859-7.
   Zhang SW, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0178317.
   Zhang SY, 2018, MICROSCOPY-JPN, V67, DOI 10.1093/jmicro/dfx091.
   Zhang X, 2019, MULTIMED TOOLS APPL, V78, P27463, DOI 10.1007/s11042-019-07846-0.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zheng Y, 2019, IEEE T IMAGE PROCESS.},
Number-of-Cited-References = {52},
Times-Cited = {6},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {14},
Journal-ISO = {Multimedia Syst.},
Doc-Delivery-Number = {NA5XX},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000539678000001},
DA = {2023-08-12},
}

@article{ WOS:000809322800001,
Author = {Quach, Boi M. and Dinh, V. Cuong and Pham, Nhung and Huynh, Dang and
   Nguyen, Binh T.},
Title = {Leaf recognition using convolutional neural networks based features},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2023},
Volume = {82},
Number = {1},
Pages = {777-801},
Month = {JAN},
Abstract = {There is a warning light for the loss of plant habitats worldwide that
   entails concerted efforts to conserve plant biodiversity. Thus, plant
   species classification is crucial to address this environmental
   challenge. In recent years, there has been a considerable increase in
   studies related to plant taxonomy. While some researchers try to improve
   their recognition performance using novel approaches, others concentrate
   on computational optimization for their framework. In addition, a few
   studies are diving into feature extraction to gain significantly in
   terms of accuracy. This paper proposes an effective method for the leaf
   recognition problem. In our proposed approach, a leaf goes through some
   pre-processing to extract its refined color image, vein image,
   xy-projection histogram, handcrafted shape, texture features, and
   Fourier descriptors. These attributes are then transformed into a better
   representation by neural network-based encoders before a support vector
   machine (SVM) model is utilized to classify different leaves. Overall,
   our approach performs a state-of-the-art result on the Flavia leaf
   dataset, achieving the accuracy of 99.69\% on test sets under random
   10-fold cross-validation and bypassing the previous methods. Another
   important contribution is the trade-offs in classification performance
   while minimizing the feature categories used. In order to tackle this
   challenge, we designed several empirical experiments to analyze the
   performance of different combinations of feature sources and choose the
   best combination for features for the main problem. We also release our
   codes (Scripts are available at
   https://github.com/Tayerquach/flavia\_recognition) for contributing to
   the research community in the leaf classification problem.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Nguyen, BT (Corresponding Author), Univ Sci, Ho Chi Minh City, Vietnam.
   Nguyen, BT (Corresponding Author), Vietnam Natl Univ Ho Chi Minh City, Ho Chi Minh City, Vietnam.
   Nguyen, BT (Corresponding Author), AISIA Res Lab, Ho Chi Minh City, Vietnam.
   Quach, Boi M.; Dinh, V. Cuong, Dublin City Univ, Dublin, Ireland.
   Pham, Nhung; Nguyen, Binh T., Univ Sci, Ho Chi Minh City, Vietnam.
   Pham, Nhung; Nguyen, Binh T., Vietnam Natl Univ Ho Chi Minh City, Ho Chi Minh City, Vietnam.
   Huynh, Dang; Nguyen, Binh T., AISIA Res Lab, Ho Chi Minh City, Vietnam.
   Huynh, Dang, Hong Bang Int Univ, Ho Chi Minh City, Vietnam.},
DOI = {10.1007/s11042-022-13199-y},
EarlyAccessDate = {JUN 2022},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Leaf recognition; Deep learning; Support vector machines},
Keywords-Plus = {PLANT; SHAPE; IDENTIFICATION; SYSTEM},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {ngtbinh@hcmus.edu.vn},
Affiliations = {Dublin City University; Vietnam National University Hochiminh City; Hong
   Bang International University},
ORCID-Numbers = {Nguyen, Thanh Binh/0000-0001-5249-9702},
Funding-Acknowledgement = {University of Science, Vietnam National University in Ho Chi Minh City;
   AISIA Research Lab in Vietnam},
Funding-Text = {We want to thank the University of Science, Vietnam National University
   in Ho Chi Minh City, and AISIA Research Lab in Vietnam for supporting us
   throughout this paper.},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Ahmed N, 2016, SCI INT, V28, DOI DOI 10.9790/0661-17134853.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Benco M, 2007, RADIOENGINEERING, V16, P64.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Eid HF, 2015, 2015 4TH INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION TECHNOLOGY AND SENSOR APPLICATION (AITS), P76, DOI 10.1109/AITS.2015.28.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Jeon WS, 2017, INT J FUZZY LOG INTE, V17, P26, DOI 10.5391/IJFIS.2017.17.1.26.
   Khmag A, 2017, IEEE ST CONF RES DEV, P467, DOI 10.1109/SCORED.2017.8305438.
   Kumar PSVVSR, 2016, PROCEEDINGS OF THE 2016 2ND INTERNATIONAL CONFERENCE ON CONTEMPORARY COMPUTING AND INFORMATICS (IC3I), P548, DOI 10.1109/IC3I.2016.7918024.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Mittal P, 2018, 2ND INTERNATIONAL CONFERENCE ON INTELLIGENT CIRCUITS AND SYSTEMS (ICICS 2018), P184, DOI 10.1109/ICICS.2018.00046.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Salve P, 2022, J KING SAUD UNIV-COM, V34, P1361, DOI 10.1016/j.jksuci.2018.09.018.
   Salve P, 2016, ADV INTELL SYST, V379, P85, DOI 10.1007/978-81-322-2517-1\_10.
   Schrader J, 2021, ANN BOT-LONDON, V128, P395, DOI 10.1093/aob/mcab078.
   Shah MP, 2017, IEEE IMAGE PROC, P860, DOI 10.1109/ICIP.2017.8296403.
   Su JY, 2020, IEEE ACCESS, V8, P208753, DOI 10.1109/ACCESS.2020.3037649.
   Turkoglu M., 2019, 2019 INT ART INT DAT, P1, DOI {[}10.1109/IDAP.2019.8875911, DOI 10.1109/IDAP.2019.8875911].
   Turkoglu M, 2019, J FAC ENG ARCHIT GAZ, V34, P2097, DOI 10.17341/gazimmfd.423674.
   Turkoglu M, 2019, APPL MATH COMPUT, V352, P1, DOI 10.1016/j.amc.2019.01.054.
   Wang ZB, 2014, IEEE IJCNN, P975, DOI 10.1109/IJCNN.2014.6889656.
   Wen C.Y., 2004, FORENSIC SCI J, V3, P23.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yamashita R, 2018, INSIGHTS IMAGING, V9, P611, DOI 10.1007/s13244-018-0639-9.
   ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949.
   Zhang H, 2012, 2012 INTERNATIONAL CONFERENCE ON INDUSTRIAL CONTROL AND ELECTRONICS ENGINEERING (ICICEE), P2025, DOI 10.1109/ICICEE.2012.538.
   Zhiyu Liu, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9226, P115, DOI 10.1007/978-3-319-22186-1\_11.},
Number-of-Cited-References = {32},
Times-Cited = {1},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {7L1HB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000809322800001},
OA = {Green Submitted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000626242600031,
Author = {Heredia, Ignacio},
Book-Group-Author = {ACM},
Title = {Large-Scale Plant Classification with Deep Neural Networks},
Booktitle = {ACM INTERNATIONAL CONFERENCE ON COMPUTING FRONTIERS 2017},
Year = {2017},
Pages = {259-262},
Note = {14th ACM International Conference on Computing Frontiers, Siena, ITALY,
   MAY 15-17, 2017},
Organization = {Assoc Comp Machinery; E4 Comp Engn; IBM; NVIDIA; SECO; Intel; ACM
   SIGMICRO; Univ Siena},
Abstract = {This paper discusses the potential of applying deep learning techniques
   for plant classification and its usage for citizen science in
   large-scale biodiversity monitoring. We show that plant classification
   using near state-of-the-art convolutional network architectures like
   ResNet50 achieves significant improvements in accuracy compared to the
   most widespread plant classification application in test sets composed
   of thousands of different species labels. We find that the predictions
   can be confidently used as a baseline classification in citizen science
   communities like iNaturalist (or its Spanish fork, Natusfera) which in
   turn can share their data with biodiversity portals like GBIF.},
Publisher = {ASSOC COMPUTING MACHINERY},
Address = {1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Heredia, I (Corresponding Author), Inst Fis Cantabria CSIC UC, Adv Comp Dept, Av Castros S-N, Santander 39005, Cantabria, Spain.
   Heredia, Ignacio, Inst Fis Cantabria CSIC UC, Adv Comp Dept, Av Castros S-N, Santander 39005, Cantabria, Spain.},
DOI = {10.1145/3075564.3075590},
ISBN = {978-1-4503-4487-6},
Keywords = {deep learning; plant classification; citizen science; biodiversity
   monitoring},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Author-Email = {iheredia@ifca.unican.es},
Affiliations = {Consejo Superior de Investigaciones Cientificas (CSIC); Universidad de
   Cantabria; CSIC - Instituto de Fisica de Cantabria (IFCA)},
ORCID-Numbers = {Heredia, Ignacio/0000-0001-6317-7100},
Funding-Acknowledgement = {EGI-Engage Lifewatch Competence Centre; EU Youth Guarantee Initiative
   (Ministerio de Economia, Industria y Competitividad, Secretaria de
   Estado de Investigacion, Desarollo e Innovacion, through the Universidad
   de Cantabria)},
Funding-Text = {I want to thank Jesus Marco de Lucas and Fernando Aguilar for their
   helpful comments and supervision. I also wanted to thank PlantNet and
   Flora-on for their image's open access policy, and the EGI-Engage
   Lifewatch Competence Centre for their support. The author is funded with
   the EU Youth Guarantee Initiative (Ministerio de Economia, Industria y
   Competitividad, Secretaria de Estado de Investigacion, Desarollo e
   Innovacion, through the Universidad de Cantabria).},
Cited-References = {{[}Anonymous], 2010, P PYTH SCI COMP C SC.
   {[}Anonymous], 2012, DEEP LEARNING UNSUPE.
   {[}Anonymous], NATURE, DOI 10.1038/nature14539.
   {[}Anonymous], 2015, LASAGNE 1 RELEASE.
   {[}Anonymous], IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123.
   Bonnet P, 2016, MULTIMED TOOLS APPL, V75, P1647, DOI 10.1007/s11042-015-2607-4.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Joly A, 2016, LECT NOTES COMPUT SC, V9822, P286, DOI 10.1007/978-3-319-44564-9\_26.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lifewatch, E SCI TECHN EUR INFR.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Sociedade Portuguesa de Botanica, 2014, FLOR ON FLOR PORT IN.},
Number-of-Cited-References = {17},
Times-Cited = {10},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {16},
Doc-Delivery-Number = {BQ9NG},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000626242600031},
OA = {Green Submitted},
DA = {2023-08-12},
}

@article{ WOS:000984379700002,
Author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M. T.},
Title = {An Efficient Mobile Application for Identification of Immunity Boosting
   Medicinal Plants using Shape Descriptor Algorithm},
Journal = {WIRELESS PERSONAL COMMUNICATIONS},
Year = {2023},
Volume = {131},
Number = {2},
Pages = {1189-1205},
Month = {JUL},
Abstract = {In the Covid-19 pandemic situation, the world is looking for
   immunity-boosting techniques for fighting against coronavirus. Every
   plant is medicine in one or another way, but Ayurveda explains the uses
   of plant-based medicines and immunity boosters for specific requirements
   of the human body. To help Ayurveda, botanists are trying to identify
   more species of medicinal immunity-boosting plants by evaluating the
   characteristics of the leaf. For a normal person, detecting
   immunity-boosting plants is a difficult task. Deep learning networks
   provide highly accurate results in image processing. In the medicinal
   plant analysis, many leaves are like each other. So, the direct analysis
   of leaf images using the deep learning network causes many issues for
   medicinal plant identification. Hence, keeping the requirement of a
   method at large to help all human beings, the proposed leaf shape
   descriptor with the deep learning-based mobile application is developed
   for the identification of immunity-boosting medicinal plants using a
   smartphone. SDAMPI algorithm explained numerical descriptor generation
   for closed shapes. This mobile application achieved 96\%accuracy for the
   64 x 64 sized images.},
Publisher = {SPRINGER},
Address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
Type = {Article},
Language = {English},
Affiliation = {Dubey, AK (Corresponding Author), Amity Univ Uttar Pradesh, Amity Sch Engn \& Technol, Dept Elect \& Commun Engn, Noida 201313, UP, India.
   Thanikkal, Jibi G., Amity Univ Uttar Pradesh, Amity Sch Engn \& Technol, Dept Comp Sci \& Engn, Noida 201313, UP, India.
   Dubey, Ashwani Kumar, Amity Univ Uttar Pradesh, Amity Sch Engn \& Technol, Dept Elect \& Commun Engn, Noida 201313, UP, India.
   Thomas, M. T., St Thomas Coll, Dept Bot, Trichur, Kerala, India.},
DOI = {10.1007/s11277-023-10476-3},
EarlyAccessDate = {APR 2023},
ISSN = {0929-6212},
EISSN = {1572-834X},
Keywords = {Bigram; COVID-19; Deep learning; Medicinal plants; Mobile app; Shape
   descriptor},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Telecommunications},
Web-of-Science-Categories  = {Telecommunications},
Author-Email = {jibimary@gmail.com
   dubey1ak@gmail.com
   thomastbgri@gmail.com},
Affiliations = {Amity University Noida; Amity University Noida},
Cited-References = {Acharya Y., 1992, CHARAKA SAMHITA.
   Akiyama T., 2019, P 8 GLOBAL C CONSUME, P324, DOI {[}10.1109/GCCE46687.2019.9015298, DOI 10.1109/GCCE46687.2019.9015298].
   Arora G, 2022, NEURAL COMPUT APPL, V34, P8385, DOI 10.1007/s00521-020-05212-y.
   Cai SZ, 2020, IEEE T INSTRUM MEAS, V69, P3538, DOI 10.1109/TIM.2019.2932649.
   Christaki EV, 2010, J FOOD AGRIC ENVIRON, V8, P245.
   Cohen Marc Maurice, 2014, J Ayurveda Integr Med, V5, P251, DOI 10.4103/0975-9476.146554.
   Dahanukar SA., 1999, IMMUNOMODULATORY AGE, P289, DOI {[}10.1007/978-3-0348-8763-2\_12, DOI 10.1007/978-3-0348-8763-2\_12].
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
   Deepa MA, 2004, FITOTERAPIA, V75, P581, DOI 10.1016/j.fitote.2004.04.008.
   Freeman H., 1961, IRE T ELECTRON COMPU, V10, P260, DOI DOI 10.1109/TEC.1961.5219197.
   Gao M, 2017, P IEEE INT C E-SCI, P29, DOI 10.1109/eScience.2017.15.
   Goeau H., 2013, PL NTNET MOBILE APP, P423, DOI {[}10.1145/2502081.2502251, DOI 10.1145/2502081.2502251].
   Golechha M, 2020, BRAIN BEHAV IMMUN, V87, P130, DOI 10.1016/j.bbi.2020.05.003.
   Google, US.
   Hashmat Imam, 2012, International Research Journal of Biological Sciences, V1, P76.
   Heberling JM, 2018, APPL PLANT SCI, V6, DOI 10.1002/aps3.1193.
   Huang YQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3047190.
   Khanafer M, 2020, IEEE INSTRU MEAS MAG, V23, P10, DOI 10.1109/mim.2020.9200875.
   Khanal P, 2022, J AYURVEDA INTEGR ME, V13, DOI 10.1016/j.jaim.2020.11.004.
   Kumar D, 2012, J MICROBIOL IMMUNOL, V45, P165, DOI 10.1016/j.jmii.2011.09.030.
   Lay-Ekuakille A, 2020, IEEE INSTRU MEAS MAG, V23, P23, DOI 10.1109/mim.2020.9200877.
   Liu YK, 2020, IEEE T INSTRUM MEAS, V69, P4681, DOI 10.1109/TIM.2019.2957849.
   Lowe DG., 1999, P 7 IEEE INT C COMPU, V2, P1150, DOI {[}10.1109/iccv.1999.790410, DOI 10.1109/ICCV.1999.790410].
   Mahboubi M., 2019, Clinical Phytoscience, V5, DOI 10.1186/s40816-018-0097-4.
   Meena J, 2018, INT J PHARM SCI RES, V9, P1377, DOI 10.13040/IJPSR.0975-8232.9(4).1377-86.
   Nielsen AAK, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-05378-z.
   Nuzzi C, 2019, IEEE INSTRU MEAS MAG, V22, P44, DOI 10.1109/MIM.2019.8674634.
   Prajapati M S, 2010, Pharmacogn Rev, V4, P85, DOI 10.4103/0973-7847.65330.
   Priyankara HAC, 2015, 2015 MORATUWA ENGINEERING RESEARCH CONFERENCE (MERCON), P148, DOI 10.1109/MERCon.2015.7112336.
   Puttarak P, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-09823-9.
   Nguyen QK, 2013, PROC INT CONF ADV, P404, DOI 10.1109/ATC.2013.6698145.
   Rai K., 2017, INT J APPL SCI BIOTE, V5, P127, DOI {[}DOI 10.3126/IJASBT.V5I2.16952, 10.3126/ijasbt.v5i2.16952].
   Roodenrys S, 2002, NEUROPSYCHOPHARMACOL, V27, P279, DOI 10.1016/S0893-133X(01)00419-5.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Su JY, 2020, IEEE ACCESS, V8, P208753, DOI 10.1109/ACCESS.2020.3037649.
   Tarun Vij, 2015, Asian Pacific Journal of Tropical Disease, V5, P1.
   Thanikkal JG, 2020, IEEE SENS J, V20, P13103, DOI 10.1109/JSEN.2020.3002909.
   Tillu G, 2020, J AYURVEDA INTEGR ME, V11, P95, DOI 10.1016/j.jaim.2020.06.012.
   Vashista Siddharth, 2022, 2022 International Mobile and Embedded Technology Conference (MECON), P408, DOI 10.1109/MECON53876.2022.9752052.
   Vellingiri B, 2020, SCI TOTAL ENVIRON, V725, DOI 10.1016/j.scitotenv.2020.138277.
   Wang B, 2013, IEEE IMAGE PROC, P4417, DOI 10.1109/ICIP.2013.6738910.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xie JS, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3042315.
   Xie Q, 2020, IEEE T INSTRUM MEAS, V69, P5395, DOI 10.1109/TIM.2019.2958580.
   Yogesh, 2016, INT CONF RELI INFO, P590, DOI 10.1109/ICRITO.2016.7785023.},
Number-of-Cited-References = {45},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Journal-ISO = {Wirel. Pers. Commun.},
Doc-Delivery-Number = {L1KR7},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000984379700002},
OA = {Bronze},
DA = {2023-08-12},
}

@article{ WOS:000383527100041,
Author = {Grinblat, Guillermo L. and Uzal, Lucas C. and Larese, Monica G. and
   Granitto, Pablo M.},
Title = {Deep learning for plant identification using vein morphological patterns},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2016},
Volume = {127},
Pages = {418-424},
Month = {SEP},
Abstract = {We propose using a deep convolutional neural network (CNN) for the
   problem of plant identification from leaf vein patterns. In particular,
   we consider classifying three different legume species: white bean, red
   bean and soybean. The introduction of a CNN avoids the use of
   handcrafted feature extractors as it is standard in state of the art
   pipeline. Furthermore, this deep learning approach significantly
   improves the accuracy of the referred pipeline. We also show that the
   reported accuracy is reached by increasing the model depth. Finally, by
   analyzing the resulting models with a simple visualization technique, we
   are able to unveil relevant vein patterns. (C) 2016 Elsevier B.V. All
   rights reserved.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Uzal, LC (Corresponding Author), UNR CONICET, French Argentine Int Ctr Informat \& Syst Sci, CIFASIS, Rosario, Santa Fe, Argentina.
   Grinblat, Guillermo L.; Uzal, Lucas C.; Larese, Monica G.; Granitto, Pablo M., UNR CONICET, French Argentine Int Ctr Informat \& Syst Sci, CIFASIS, Rosario, Santa Fe, Argentina.},
DOI = {10.1016/j.compag.2016.07.003},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Deep learning; Machine vision; Automatic plant identification; Leaf vein
   image},
Keywords-Plus = {LEAF; VENATION; RECOGNITION; FEATURES},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {uzal@cifasis-conicet.gov.ar},
Affiliations = {Consejo Nacional de Investigaciones Cientificas y Tecnicas (CONICET);
   National University of Rosario},
ResearcherID-Numbers = {Granitto, Pablo M/A-3645-2013
   },
ORCID-Numbers = {Larese, Monica/0000-0003-4167-6077
   Granitto, Pablo/0000-0002-2473-8769},
Funding-Acknowledgement = {ANPCyT {[}PICT-2012-0181]},
Funding-Text = {The authors thank P. F. Verdes for valuable comments and acknowledge
   grant support from ANPCyT PICT-2012-0181.},
Cited-References = {Agarwal G, 2006, TAXON, V55, P597, DOI 10.2307/25065637.
   {[}Anonymous], 2013, ARXIV13084214.
   Chollet F., 2015, KERAS.
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5\_51.
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1.
   Gwo CY, 2013, COMPUT ELECTRON AGR, V91, P124, DOI 10.1016/j.compag.2012.12.005.
   Husin Z, 2012, COMPUT ELECTRON AGR, V89, P18, DOI 10.1016/j.compag.2012.07.009.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   Jia Y., 2014, ACM INT C MULT, DOI {[}DOI 10.1145/2647868.2654889, 10.1145/2647868.2654889].
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Larese MG, 2014, EXPERT SYST APPL, V41, P4638, DOI 10.1016/j.eswa.2014.01.029.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   LeCun Y., 1989, ADV NEURAL INFORM PR, V2.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Parekh R., 2012, INT J ADV ENG TECHNO, V2, P149.
   Price CA, 2011, PLANT PHYSIOL, V155, P236, DOI 10.1104/pp.110.162834.
   Pydipati R, 2006, COMPUT ELECTRON AGR, V52, P49, DOI 10.1016/j.compag.2006.01.004.
   Sack L, 2008, P NATL ACAD SCI USA, V105, P1567, DOI 10.1073/pnas.0709333105.
   Scoffoni C, 2011, PLANT PHYSIOL, V156, P832, DOI 10.1104/pp.111.173856.
   SOILLE P, 1999, MORPHOLOGICAL IMAGE.
   Sole-Casals Jordi, 2008, IWPACBB, P243.
   Sonka M., 2014, IMAGE PROCESSING ANA.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.},
Number-of-Cited-References = {29},
Times-Cited = {288},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {180},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {DW3FI},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000383527100041},
OA = {Green Published},
DA = {2023-08-12},
}

@inproceedings{ WOS:000361019800097,
Author = {Hsiao, Jou-Ken and Kang, Li-Wei and Chang, Ching-Long and Hsu, Chao-Yung
   and Chen, Chia-Yen},
Book-Group-Author = {IEEE},
Title = {Learning Sparse Representation for Leaf Image Recognition},
Booktitle = {2014 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS - TAIWAN
   (ICCE-TW)},
Year = {2014},
Note = {1st IEEE International Conference on Consumer Electronics - Taiwan
   (ICCE-TW), Taipei, TAIWAN, MAY 26-28, 2014},
Organization = {IEEE; IEEE Consumer Elect Soc; IEEE Consumer Elect Soc Taipei Chapter;
   Natl Taiwan Normal Univ; IEEE Taipei Sect; Minist Sci \& Technol; Bur
   Foreign Trade; Minist Educ; Taipei City Govt, Dept Informat \& Tourism;
   Natl Taiwan Normal Univ, Aim Top Univ Project; Natl Taiwan Normal Univ,
   Ctr Learning Technol Chinese; Natl Taiwan Normal Univ, Adv Ctr Study
   Learning Sci; Natl Taiwan Normal Univ, Off Res \& Dev; Taiwan Assoc Syst
   Sci \& Engn},
Abstract = {Automatic plant identification via computer vision techniques has been
   greatly important for a number of professionals, such as environmental
   protectors, land managers, and foresters. In this paper, a novel leaf
   image recognition technique via sparse representation is proposed for
   automatic plant identification. In order to model leaf images, we learn
   an overcomplete dictionary for sparsely representing the training images
   of each leaf species. Each dictionary is learned using a set of
   descriptors extracted from the training images in such a way that each
   descriptor is represented by linear combination of a small number of
   dictionary atoms. For each test leaf image, we calculate the correlation
   between the image and each learned dictionary of leaf species to achieve
   the identification of the leaf image. As a result, efficient leaf
   recognition can be achieved on public leaf dataset based on the proposed
   framework leading to a more compact and richer representation of leaf
   images compared to traditional clustering approaches. Moreover, our
   method is also adapted to newly added leaf species without retraining
   classifiers and suitable to be highly parallelized as well as integrated
   with any leaf image descriptors/features.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kang, LW (Corresponding Author), Natl Yunlin Univ Sci \& Technol, Dept Comp Sci \& Informat Engn, Yunlin, Taiwan.
   Hsiao, Jou-Ken; Kang, Li-Wei; Chang, Ching-Long, Natl Yunlin Univ Sci \& Technol, Dept Comp Sci \& Informat Engn, Yunlin, Taiwan.
   Kang, Li-Wei, Natl Yunlin Univ Sci \& Technol, Grad Sch Engn Sci \& Technol Doctoral Program, Yunlin, Taiwan.
   Hsu, Chao-Yung, Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   Chen, Chia-Yen, Natl Univ Kaohsiung, Dept Comp Sci \& Informat Engn, Kaohsiung, Taiwan.},
ISBN = {978-1-4799-4851-2},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {M10117006@yuntech.edu.tw
   lwkang@yuntech.edu.tw
   chang@yuntech.edu.tw
   cyhsu@iis.sinica.edu.tw
   ayen@nuk.edu.tw},
Affiliations = {National Yunlin University Science \& Technology; National Yunlin
   University Science \& Technology; Academia Sinica - Taiwan; National
   University Kaohsiung},
Cited-References = {Du J. X., 2007, APPL MATH COMPUTATIO, V185.
   Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253.
   Kebapci H., 2011, COMPUTER J, V54.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Mairal J, 2010, J MACH LEARN RES, V11, P19.
   Mouine S., 2012, P ICMR JUN.
   Mouine S., 2013, P ICMR APR.
   Mzoughi O, 2013, IEEE IMAGE PROC, P3967, DOI 10.1109/ICIP.2013.6738817.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {10},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BD4RC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000361019800097},
DA = {2023-08-12},
}

@inproceedings{ WOS:000851392300091,
Author = {Leonida, Althani Miguel G. and Caballero, Arlene R.},
Book-Group-Author = {IEEE},
Title = {Aleaf: An Android-Based Phytotherapy Leaf Recognition Using Custom
   Vision Machine Learning},
Booktitle = {2022 7TH INTERNATIONAL CONFERENCE ON BUSINESS AND INDUSTRIAL RESEARCH
   (ICBIR2022)},
Year = {2022},
Pages = {488-493},
Note = {7th International Conference on Business and Industrial Research
   (ICBIR), ELECTR NETWORK, MAY 19-20, 2022},
Organization = {Thal NIchl Inst Technol; Artificial Intelligence Assoc Thailand; IEEE;
   IEEE Comp Soc, Thailand Chapter; TNI; IEEE, Thailand Sect; IEEE Comp
   Soc, Thailand Chapter; Council IT Deans Thailand},
Abstract = {Phytotherapy is the area of medicine which deals with the practice of
   medicinal plants as remedies for illnesses or as therapeutic agents. Due
   to its budget-friendliness, high accessibility, and long account of
   effectivity, plant- based therapy as complementary and alternative
   medicine (CAM) is widespread in the first world while remains the
   primary health care for the third world. The more traditional use of
   plant-based therapy for medicinal purposes is to preserve the original
   properties of the plant; vitiated components are minimum. Even with its
   popularity, information regarding phytotherapy and the benefits of herbs
   and plants is nil. Even with its utilization among people, especially
   with the guidance of the elderlies, the knowledge is still lacking. This
   may be due to the fact that the practice of phytotherapy is usually
   based on experience and information is passed down through verbal
   transmission. The main objective of the study is to implement an Android
   application that acts as a leaf identification system, capable of
   detecting whether a plant has medicinal properties or not through the
   captured leaf image and its patterns. It displays their therapeutic
   benefits to the human body, its preparation, administration, dosage and
   frequency and duration of usage.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Leonida, AMG (Corresponding Author), Lyceum Philippines Univ, Coll Technol, Manila, Philippines.
   Leonida, Althani Miguel G.; Caballero, Arlene R., Lyceum Philippines Univ, Coll Technol, Manila, Philippines.},
DOI = {10.1109/ICBIR54589.2022.9786509},
ISBN = {978-1-6654-9475-5},
Keywords = {Phytotherapy; Machine Learning; Custom Vision; Leaf Recognition;
   Transfer Learning Algorithm},
Research-Areas = {Business \& Economics; Computer Science; Operations Research \&
   Management Science},
Web-of-Science-Categories  = {Business; Computer Science, Artificial Intelligence; Computer Science,
   Information Systems; Operations Research \& Management Science},
Author-Email = {althani.leonida@lpunetwork.edu.ph
   arlene.caballero@lpu.edu.ph},
Affiliations = {Lyceum of the Philippines University},
Cited-References = {Ahmad Sohail, 2014, ScientificWorldJournal, V2014, P829076, DOI 10.1155/2014/829076.
   Akinmoladun AC, 2007, SCI RES ESSAYS, V2, P191.
   Anitha B, 2018, J NAT AYURVEDIC MED, V2, P2.
   Baskaran C., 2012, ASIAN PAC J TROP DIS, V2, pS658, DOI 10.1016/S2222-1808(12)60239-4.
   Falzon CC, 2017, PRIMARY CARE, V44, P217, DOI 10.1016/j.pop.2017.02.001.
   Farooque A. M. D., 2012, International Journal of Research in Pharmacy and Chemistry, V2, P467.
   Furst R, 2015, PLANTA MED, V81, P962, DOI 10.1055/s-0035-1545948.
   Haneefa KPM, 2012, J PHARMA SCI RES, V4, P1642.
   Iwansyah AC, 2021, FOOD SCI TECH-BRAZIL, V41, P987, DOI 10.1590/fst.15420.
   Janakiraman D.., 2014, EVALUATION ANTIINFLA.
   Kagoro M. L. P., 2020, PHYTOCHEMICAL ANTIOX.
   Kim HS, 2005, J ETHNOPHARMACOL, V100, P37, DOI 10.1016/j.jep.2005.05.030.
   Kou XJ, 2018, NUTRIENTS, V10, DOI 10.3390/nu10030343.
   Kumar K. A., 2013, INT J PHARM PHARM SC, V3.
   Lakshmipriya Gopalakrishnan, 2016, Food Science and Human Wellness, V5, P49, DOI 10.1016/j.fshw.2016.04.001.
   Leite PM, 2021, EUR J INTEGR MED, V41, DOI 10.1016/j.eujim.2020.101270.
   Mahmood K.T., 2010, J PHARM SCI RES, V2, P775, DOI DOI 10.1371/JOURNAL.PONE.0095492.
   Musa S., 2019, SHORT HIST PLANTS ME.
   Newman DJ, 2020, J NAT PROD, V83, P770, DOI 10.1021/acs.jnatprod.9b01285.
   Pan SY, 2014, EVID-BASED COMPL ALT, V2014, DOI 10.1155/2014/525340.
   Pandey S, 2016, J IMMUNOTOXICOL, V13, P590, DOI 10.3109/1547691X.2016.1149528.
   Gutierrez RMP, 2008, J ETHNOPHARMACOL, V117, P1, DOI 10.1016/j.jep.2008.01.025.
   Pinoliad S. L., 2020, ONYXRAY MOBILE BASED, DOI {[}10.1145/3411681.3411698, DOI 10.1145/3411681.3411698].
   Bruning MCR, 2012, CIENC SAUDE COLETIVA, V17, P2675, DOI 10.1590/S1413-81232012001000017.
   Shanmugam S., 2012, Asian Pacific Journal of Tropical Biomedicine, V2, pS429.
   Shruti B., 2018, J PHARMACOGN PHYTOCH, V7.
   Souza CRF, 2008, BRAZ J CHEM ENG, V25, P59, DOI 10.1590/S0104-66322008000100008.
   Sushila R., 2010, PHARMACOLOGYONLINE, V3.
   Swanson C. P., 1953, Q REV BIOL, V28, DOI {[}10.1086/399378, DOI 10.1086/399378].
   Tan CQ, 2018, LECT NOTES COMPUT SC, V11141, P270, DOI 10.1007/978-3-030-01424-7\_27.
   Tripathi S., 2019, GLOBAL J ARCHAEOLOGY, V10, P87, DOI DOI 10.19080/GJAA.2019.10.555796.
   Vaisakh M. N., 2012, INT J PHARM SCI, V3.
   Vijaya Anand, 2016, Pharmacognosy Journal, V8, P314.
   Vijayaraghavan K, 2017, MOL MED REP, V15, P1007, DOI 10.3892/mmr.2017.6133.
   WHO-World Organization of Health, 2013, WHO TRADITIONAL MED.
   Yadav R. N. S., 2011, Journal of Phytology, V3, P10.
   Yasmin H., 2009, DHAKA U J PHARM SCI, V8, DOI {[}10.3329/dujps.v8i1.5337, DOI 10.3329/DUJPS.V8I1.5337].
   Zakaria Z. A., 2007, International Journal of Tropical Medicine, V2, P96.},
Number-of-Cited-References = {38},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BT7XW},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S); Conference Proceedings Citation Index- Social Science &amp; Humanities (CPCI-SSH)},
Unique-ID = {WOS:000851392300091},
DA = {2023-08-12},
}

@article{ WOS:000473379500066,
Author = {Tenhunen, Henri and Pahikkala, Tapio and Nevalainen, Olli and Teuhola,
   Jukka and Mattila, Heta and Tyystjarvi, Esa},
Title = {Automatic detection of cereal rows by means of pattern recognition
   techniques},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2019},
Volume = {162},
Pages = {677-688},
Month = {JUL},
Abstract = {Automatic locating of weeds from fields is an active research topic in
   precision agriculture. A reliable and practical plant identification
   technique would enable the reduction of herbicide amounts and lowering
   of production costs, along with reducing the damage to the ecosystem.
   When the seeds have been sown row-wise, most weeds may be located
   between the sowing rows. The present work describes a clustering-based
   method for recognition of plantlet rows from a set of aerial
   photographs, taken by a drone flying at approximately ten meters. The
   algorithm includes three phases: segmentation of green objects in the
   view, feature extraction, and clustering of plants into individual rows.
   Segmentation separates the plants from the background. The main feature
   to be extracted is the center of gravity of each plant segment. A
   tentative clustering is obtained piecewise by applying the 2D Fourier
   transform to image blocks to get information about the direction and the
   distance between the rows. The precise sowing line position is finally
   derived by principal component analysis. The method was able to find the
   rows from a set of photographs of size 1452 x 969 pixels approximately
   in 0.11 s, with the accuracy of 94 per cent.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Tyystjarvi, E (Corresponding Author), Univ Turku, Mol Plant Biol, Dept Biochem, Turku, Finland.
   Tenhunen, Henri; Pahikkala, Tapio; Nevalainen, Olli; Teuhola, Jukka, Univ Turku, Dept Future Technol, Comp Sci, Turku, Finland.
   Mattila, Heta; Tyystjarvi, Esa, Univ Turku, Mol Plant Biol, Dept Biochem, Turku, Finland.},
DOI = {10.1016/j.compag.2019.05.002},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Computer vision; Pattern recognition; Principal component analysis;
   Fourier transform; Precision agriculture},
Keywords-Plus = {CROP ROWS; TRANSFORM; IDENTIFICATION; ALGORITHM; POSITION; SYSTEM;
   ROBOT; WEED},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {esatyy@utu.fi},
Affiliations = {University of Turku; Finland National Institute for Health \& Welfare;
   University of Turku},
ResearcherID-Numbers = {Pahikkala, Tapio/H-9659-2012
   Tyystjarvi, Esa/B-2360-2015},
ORCID-Numbers = {Pahikkala, Tapio/0000-0003-4183-2455
   Mattila, Heta/0000-0002-5071-9721
   Tyystjarvi, Esa/0000-0001-6808-7470},
Funding-Acknowledgement = {European Agricultural Fund for Rural Development {[}30934]; Academy of
   Finland {[}307335]; University of Turku Graduate School; Turku
   University Foundation; Vaisala Fund},
Funding-Text = {This study was financially supported by European Agricultural Fund for
   Rural Development (grant nr. 30934), Academy of Finland (grant nr.
   307335), University of Turku Graduate School, Turku University
   Foundation, and Vaisala Fund.},
Cited-References = {Anter AM, 2019, EXPERT SYST APPL, V118, P340, DOI 10.1016/j.eswa.2018.10.009.
   Bai X., 2012, GRAPH BASED METHODS.
   Bakker T, 2008, COMPUT ELECTRON AGR, V60, P87, DOI 10.1016/j.compag.2007.07.006.
   Billingsley J, 1997, COMPUT ELECTRON AGR, V16, P147, DOI 10.1016/S0168-1699(96)00034-8.
   Bossu J, 2009, COMPUT ELECTRON AGR, V65, P133, DOI 10.1016/j.compag.2008.08.004.
   Burgos-Artizzu XP, 2011, COMPUT ELECTRON AGR, V75, P337, DOI 10.1016/j.compag.2010.12.011.
   Choi KH, 2015, COMPUT ELECTRON AGR, V113, P266, DOI 10.1016/j.compag.2015.02.014.
   Cormen Thomas H., 2001, INTRO ALGORITHMS, V2nd.
   Decarolis F, 2014, CENT ISS CONT ECON T, P1, DOI 10.1007/s11119-013-9304-y.
   Garcia-Santillan I, 2018, PRECIS AGRIC, V19, P18, DOI 10.1007/s11119-016-9494-1.
   Gonzalez R.C., 2002, DIGITAL IMAGE PROCES, V2nd ed..
   Han S, 2004, COMPUT ELECTRON AGR, V43, P179, DOI 10.1016/j.compag.2004.01.007.
   HUANG TS, 1979, IEEE T ACOUST SPEECH, V27, P13, DOI 10.1109/TASSP.1979.1163188.
   de Castro AI, 2012, PRECIS AGRIC, V13, P302, DOI 10.1007/s11119-011-9247-0.
   Jiang GQ, 2016, COMPUT ELECTRON AGR, V123, P211, DOI 10.1016/j.compag.2016.02.002.
   Jiang GQ, 2015, EXPERT SYST APPL, V42, P2429, DOI 10.1016/j.eswa.2014.10.033.
   Josso B, 2005, MECH SYST SIGNAL PR, V19, P1152, DOI 10.1016/j.ymssp.2004.07.005.
   Kaufman L., 1987, Statistical Data Analysis Based on the L1-Norm and Related Methods. First International Conference, P405.
   Kise M, 2005, BIOSYST ENG, V90, P357, DOI 10.1016/j.biosystemseng.2004.12.008.
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489.
   Maes WH, 2019, TRENDS PLANT SCI, V24, P152, DOI 10.1016/j.tplants.2018.11.007.
   Marchant J. A., 1995, Real-Time Imaging, V1, P363, DOI 10.1006/rtim.1995.1036.
   Motohka T, 2010, REMOTE SENS-BASEL, V2, P2369, DOI 10.3390/rs2102369.
   OLSEN HJ, 1995, COMPUT ELECTRON AGR, V12, P147, DOI 10.1016/0168-1699(94)00044-Q.
   Otsu N., 1975, AUTOMATICA, V11, P23, DOI {[}10.1109/TSMC.1979.4310076, DOI 10.1109/TSMC.1979.4310076].
   Pla F, 1997, IMAGE VISION COMPUT, V15, P465, DOI 10.1016/S0262-8856(96)01147-X.
   Reid J. F, 1987, SAE T, V96, P673.
   Sogaard HT, 2003, COMPUT ELECTRON AGR, V38, P141, DOI 10.1016/S0168-1699(02)00140-0.
   Southall B, 2002, INT J ROBOT RES, V21, P61, DOI 10.1177/027836402320556485.
   Tang L, 2005, AM SOC AGR BIOL ENG, V51, P1079.
   Vidovic I, 2016, PATTERN RECOGN, V55, P68, DOI 10.1016/j.patcog.2016.01.013.
   Winterhalter W, 2018, IEEE ROBOT AUTOM LET, V3, P3394, DOI 10.1109/LRA.2018.2852841.
   WOEBBECKE DM, 1995, T ASAE, V38, P259, DOI 10.13031/2013.27838.
   Zhang XY, 2018, COMPUT ELECTRON AGR, V154, P165, DOI 10.1016/j.compag.2018.09.014.},
Number-of-Cited-References = {34},
Times-Cited = {10},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {52},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {IF8YP},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000473379500066},
DA = {2023-08-12},
}

@article{ WOS:000753064700002,
Author = {Rajesh, K. V. N. and Bhaskari, D. Lalitha},
Title = {Multi-class classification using convolution neural networks for plant
   leaf recognition of Ayurvedic plants},
Journal = {INTERNATIONAL JOURNAL OF COMPUTATIONAL SCIENCE AND ENGINEERING},
Year = {2022},
Volume = {25},
Number = {1},
Pages = {11-21},
Abstract = {Ayurveda is the traditional medicine system of India. The ingredients
   from which Ayurvedic medicines are made are mostly herbal and mineral in
   nature. Also, there are many herbal home remedies in India for general
   ailments. This knowledge has been passed down from generation to
   generation in large joint families. This knowledge is slowly fading away
   in the current generation of nuclear families. The current generation is
   unable to identify even locally available plants. The authors have come
   up with the idea of using convolution neural networks for solving this
   problem. In this solution, the images of leaves are used to identify the
   plant. This problem is a case of multi-class classification. A leaf
   image database is created and a neural network model is built using
   convolutional neural network (CNN). Keras deep learning framework with
   tensorflow as backend, is used for this purpose. The work presented in
   this paper is a part of larger research work in this area. This paper
   explains the developed CNN model and presents the results corresponding
   to six Ayurvedic leaves commonly available in and around the City of
   Visakhapatnam in the State of Andhra Pradesh.},
Publisher = {INDERSCIENCE ENTERPRISES LTD},
Address = {WORLD TRADE CENTER BLDG, 29 ROUTE DE PRE-BOIS, CASE POSTALE 856, CH-1215
   GENEVA, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Rajesh, KVN (Corresponding Author), Andhra Univ, Coll Engn Autonomous, Dept Comp Sci \& Syst Engn, Visakhapatnam 530003, Andhra Pradesh, India.
   Rajesh, K. V. N.; Bhaskari, D. Lalitha, Andhra Univ, Coll Engn Autonomous, Dept Comp Sci \& Syst Engn, Visakhapatnam 530003, Andhra Pradesh, India.},
ISSN = {1742-7185},
EISSN = {1742-7193},
Keywords = {convolution neural networks; CNNs; multi-class classification; plant
   leaf recognition; PLR; leaf feature extraction},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications},
Author-Email = {kvn.rajesh@gmail.com
   lalithabhaskari@yahoo.co.in},
Affiliations = {Andhra University},
ResearcherID-Numbers = {D, Lalitha Bhaskari/AAI-4195-2021},
ORCID-Numbers = {D, Lalitha Bhaskari/0000-0002-7773-7567},
Cited-References = {Ahmed M, 2020, INT J COMPUT SCI ENG, V21, P137, DOI 10.1504/IJCSE.2020.105220.
   Ahmed N, 2016, SCI INT, V28, DOI DOI 10.9790/0661-17134853.
   Ambarwari A., 2020, TELKOMNIKA, V18, P726.
   Arun C, 2017, JURNAL ILMU KOMPUTER, V10, P19, DOI {[}10.21609/jiki.v10i1.405, DOI 10.21609/JIKI.V10I1.405].
   Begue A, 2017, INT J ADV COMPUT SC, V8, P166.
   Islam M. A., 2019, INT J COMPUTER IJC, V33, P26.
   Isnanto R., 2018, INT J ELECTR COMPUT, V8, P1920.
   Kaur S., 2019, J MULTIMEDIA INFORM, V6, P49, DOI DOI 10.33851/JMIS.2019.6.2.49.
   Mansur P., 2018, INT J ADV SIGNAL IMA, V4, P30, DOI {[}10.29284/IJASIS.4.1.2018.30-36, DOI 10.29284/IJASIS.4.1.2018.30-36].
   Pushpa BR, 2016, INT J APPL ENG RES, V11, P5142.
   Tan ZY, 2020, INT J COMPUT SCI ENG, V23, P214, DOI 10.1504/IJCSE.2020.111426.
   Too EC, 2020, INT J COMPUT SCI ENG, V21, P522.
   Vo Anh H., 2019, International Journal of Machine Learning and Computing, V9, P363, DOI 10.18178/ijmlc.2019.9.3.811.
   Wu ZH, 2019, CONNECT SCI, V31, P169, DOI 10.1080/09540091.2018.1510902.
   Liu Y, 2021, CONNECT SCI, V33, P719, DOI 10.1080/09540091.2021.1875987.
   Zhu XL, 2020, CONNECT SCI, V32, P1, DOI 10.1080/09540091.2019.1609420.},
Number-of-Cited-References = {16},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Int. J. Comput. Sci. Eng.},
Doc-Delivery-Number = {YV9TD},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000753064700002},
DA = {2023-08-12},
}

@article{ WOS:000243043000019,
Author = {Fu, H. and Chi, Z.},
Title = {Combined thresholding and neural network approach for vein pattern
   extraction from leaf images},
Journal = {IEE PROCEEDINGS-VISION IMAGE AND SIGNAL PROCESSING},
Year = {2006},
Volume = {153},
Number = {6},
Pages = {881-892},
Month = {DEC},
Abstract = {Living plant recognition based on images of leaf, flower and fruit is a
   very challenging task in the field of pattern recognition and computer
   vision. There has been little work reported on flower and fruit image
   processing and recognition. In recent years, several researchers have
   dedicated their work to leaf characterisation. As an inherent trait,
   leaf vein definitely contains the important information for plant
   species recognition despite its complex modality. A new approach that
   combines a thresholding method and an artificial neural network (ANN)
   classifier is proposed to extract leaf veins. A preliminary segmentation
   based on the intensity histogram of leaf images is first carried out to
   coarsely determine vein regions. This is followed by a fine segmentation
   using a trained ANN classifier with ten features extracted from a window
   centred on the object pixel as its inputs. Compared with other methods,
   experimental results show that this combined approach is capable of
   extracting more accurate venation modality of the leaf for the
   subsequent vein pattern classification. The approach can also reduce the
   computing time compared with a direct neural network approach.},
Publisher = {INST ENGINEERING TECHNOLOGY-IET},
Address = {MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Fu, H (Corresponding Author), Hong Kong Polytech Univ, Ctr Multimedia Signal Proc, Dept Elect \& Informat Engn, Hong Kong, Peoples R China.
   Hong Kong Polytech Univ, Ctr Multimedia Signal Proc, Dept Elect \& Informat Engn, Hong Kong, Peoples R China.},
DOI = {10.1049/ip-vis:20060061},
ISSN = {1350-245X},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {enhongfu@eie.polyu.edu.hk},
Affiliations = {Hong Kong Polytechnic University},
ORCID-Numbers = {fu, hong/0000-0003-2246-7552},
Cited-References = {ABBASI S, 1997, INT C SCAL SPAC THEO, P284.
   ASH A., 1999, MANUAL LEAF ARCHITEC.
   CHEN M, 1995, NEURAL NETWORK MODEL.
   Chi Z., 1996, FUZZY ALGORITHMS APP.
   Gouveia F, 1997, ISIE `97 - PROCEEDINGS OF THE IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS, VOLS 1-3, P757, DOI 10.1109/ISIE.1997.648634.
   Haykin S., 1994, NEURAL NETWORKS COMP.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   Pratt W, 2013, DIGITAL IMAGE PROCES.
   Sezgin M, 2004, J ELECTRON IMAGING, V13, P146, DOI 10.1117/1.1631315.
   Soille P, 2000, IMAGE VISION COMPUT, V18, P1025, DOI 10.1016/S0262-8856(00)00043-3.
   {*}U CT HOM GARD CTR, UC PLANT DAT TREES S.
   {*}U QUEENSL CTR PES, 2006, LUC.
   {*}UC BERK DIG LIB P, CALFLORA.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   WANG Z, 2000, P 2000 IEEE PAC RIM, P380.},
Number-of-Cited-References = {15},
Times-Cited = {39},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {15},
Journal-ISO = {IEE Proc.-Vis. Image Signal Process.},
Doc-Delivery-Number = {119VS},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000243043000019},
DA = {2023-08-12},
}

@article{ WOS:000868264700001,
Author = {Picek, Lukas and Sulc, Milan and Patel, Yash and Matas, Jiri},
Title = {Plant recognition by AI: Deep neural nets, transformers, and kNN in deep
   embeddings},
Journal = {FRONTIERS IN PLANT SCIENCE},
Year = {2022},
Volume = {13},
Month = {SEP 27},
Abstract = {The article reviews and benchmarks machine learning methods for
   automatic image-based plant species recognition and proposes a novel
   retrieval-based method for recognition by nearest neighbor
   classification in a deep embedding space. The image retrieval method
   relies on a model trained via the Recall@k surrogate loss.
   State-of-the-art approaches to image classification, based on
   Convolutional Neural Networks (CNN) and Vision Transformers (ViT), are
   benchmarked and compared with the proposed image retrieval-based method.
   The impact of performance-enhancing techniques, e.g., class prior
   adaptation, image augmentations, learning rate scheduling, and loss
   functions, is studied. The evaluation is carried out on the PlantCLEF
   2017, the ExpertLifeCLEF 2018, and the iNaturalist 2018 Datasets-the
   largest publicly available datasets for plant recognition. The
   evaluation of CNN and ViT classifiers shows a gradual improvement in
   classification accuracy. The current state-of-the-art Vision Transformer
   model, ViT-Large/16, achieves 91.15\% and 83.54\% accuracy on the
   PlantCLEF 2017 and ExpertLifeCLEF 2018 test sets, respectively; the best
   CNN model (ResNeSt-269e) error rate dropped by 22.91\% and 28.34\%.
   Apart from that, additional tricks increased the performance for the
   ViT-Base/32 by 3.72\% on ExpertLifeCLEF 2018 and by 4.67\% on PlantCLEF
   2017. The retrieval approach achieved superior performance in all
   measured scenarios with accuracy margins of 0.28\%, 4.13\%, and 10.25\%
   on ExpertLifeCLEF 2018, PlantCLEF 2017, and iNat2018-Plantae,
   respectively.},
Publisher = {FRONTIERS MEDIA SA},
Address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Picek, L (Corresponding Author), Univ West Bohemia, Fac Appl Sci, Dept Cybernet, Plzen, Czech Republic.
   Picek, Lukas, Univ West Bohemia, Fac Appl Sci, Dept Cybernet, Plzen, Czech Republic.
   Sulc, Milan; Patel, Yash; Matas, Jiri, Czech Tech Univ, Fac Elect Engn, Dept Cybernet, Visual Recognit Grp, Prague, Czech Republic.},
DOI = {10.3389/fpls.2022.787527},
Article-Number = {787527},
ISSN = {1664-462X},
Keywords = {plant; species; classification; recognition; machine learning; computer
   vision; species recognition; fine-grained},
Keywords-Plus = {IDENTIFICATION; FEATURES; SHAPE},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {picekl@kky.zcu.cz},
Affiliations = {University of West Bohemia Pilsen; Czech Technical University Prague},
ResearcherID-Numbers = {Niederle, Matěj/AAG-6723-2020
   Picek, Lukáš/HLX-8615-2023
   Picek, Lukas/GXG-4988-2022
   },
ORCID-Numbers = {Niederle, Matěj/0000-0002-0110-1749
   Picek, Lukáš/0000-0002-6041-9722
   Matas, Jiri/0000-0003-0863-4844
   Sulc, Milan/0000-0002-6321-0131},
Funding-Acknowledgement = {UWB {[}SGS2022-017]; Ministry of Environment of the Czech Republic
   {[}SS05010008]; Toyota Motor Europe; OP VVV
   {[}CZ.02.1.01/0.0/0.0/16\textbackslash{}\_019/0000765]; Grant Agency of
   the Czech Technical University in Prague {[}SGS20/171/OHK3/3T/13];
   Project StratDL in the realm of COMET K1 center Software Competence
   Center Hagenberg; Amazon Research Award},
Funding-Text = {LP was supported by the UWB project No. SGS2022-017. LP and JM were
   supported by the Ministry of Environment of the Czech Republic project
   No. SS05010008. MS and JM were supported by Toyota Motor Europe. JM and
   YP were supported by Research Center for Informatics (project
   CZ.02.1.01/0.0/0.0/16\textbackslash{}\_019/0000765 funded by OP VVV). YP
   was supported by the Grant Agency of the Czech Technical University in
   Prague, grant No. SGS20/171/OHK3/3T/13, by Project StratDL in the realm
   of COMET K1 center Software Competence Center Hagenberg and an Amazon
   Research Award.},
Cited-References = {Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Bonnet P, 2018, MULTIMED SYST APPL, P131, DOI 10.1007/978-3-319-76445-0\_8.
   Buslaev A, 2020, INFORMATION, V11, DOI 10.3390/info11020125.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432.
   Dosovitskiy A, 2021, INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929.
   Garcin C., 2021, NEURIPS 2021 35 C NE.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Goeau H., 2016, CLEF WORKING NOTES 2.
   Goeau H., 2018, CLEF WORKING NOTES 2.
   Goeau H., 2021, WORKING NOTES CLEF 2, V2936, P1422.
   Goeau H., 2019, CLEF 2019 C LABS EVA, P1.
   Goeau H., 2020, CLEF TASK OVERVIEW 2.
   Goeau H., 2017, CEUR WORKSHOP P.
   Goodfellow I., 2016, DEEP LEARNING, V1.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI {[}10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372].
   Joly Alexis, 2021, Experimental IR Meets Multilinguality, Multimodality, and Interaction: 12th International Conference of the CLEF Association, CLEF 2021, Proceedings. Lecture Notes in Computer Science, Information Systems and Applications, incl. Internet/Web, and HCI (12880), P371, DOI 10.1007/978-3-030-85251-1\_24.
   Joly Alexis, 2020, Experimental IR Meets Multilinguality, Multimodality, and Interaction. 11th International Conference of the CLEF Association, CLEF 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12260), P342, DOI 10.1007/978-3-030-58219-7\_23.
   Joly A, 2019, LECT NOTES COMPUT SC, V11696, P387, DOI 10.1007/978-3-030-28577-7\_29.
   Joly A, 2018, LECT NOTES COMPUT SC, V11018, P247, DOI 10.1007/978-3-319-98932-7\_24.
   Keaton MR, 2021, Arxiv, DOI {[}arXiv:2106.02141, DOI 10.48550/ARXIV.2106.02141, 10.48550/ARXIV.2106.02141].
   Khosla Prannay, 2020, ADV NEURAL INFORM PR, P18661, DOI DOI 10.48550/ARXIV.2004.11362.
   Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9\_19.
   Lasseck Mario, 2017, CLEF.
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826.
   Loshchilov I., 2018, P INT C LEARN REPR.
   Malik OA, 2021, 2021 IEEE ASIA-PACIFIC CONFERENCE ON COMPUTER SCIENCE AND DATA ENGINEERING (CSDE), DOI 10.1109/CSDE53843.2021.9718387.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Patel Y., 2021, P IEEECVF C COMPUTER, P7502.
   Picek L., 2019, CLEF WORKING NOTES.
   Picek L, 2022, IEEE WINT CONF APPL, P3281, DOI 10.1109/WACV51458.2022.00334.
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046.
   Prasad S., 2011, P 2011 INT C COMM CO, P343, DOI {[}10.1145/1947940.1948012, DOI 10.1145/1947940.1948012].
   Saerens M, 2002, NEURAL COMPUT, V14, P21, DOI 10.1162/089976602753284446.
   Sipka T, 2022, IEEE WINT CONF APPL, P2031, DOI 10.1109/WACV51458.2022.00209.
   Sulc M., 2018, CLEF WORKING NOTES.
   Sulc M., 2020, THESIS CZECH TU PRAG.
   Sulc M, 2019, IEEE INT CONF COMP V, P3220, DOI 10.1109/ICCVW.2019.00402.
   Sulc M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0265-4.
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Tan M., 2021, P INT C MACHINE LEAR, P10096.
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P854, DOI 10.1109/ICCV48922.2021.00091.
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914.
   Vaswani A, 2017, ADV NEUR IN, V30.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.
   Wah C, 2011, CALTECH UCSD BIRDS 2.
   Wightman R., 2019, PYTORCH IMAGE MODELS.
   Wu DZ, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/2015017.
   Wu Q., 2006, ADV ARTIFICIAL INTEL, V3, P5.
   Wu Stephen Gang, 2007, 2007 IEEE International Symposium on Signal Processing and Information Technology, P11, DOI 10.1109/ISSPIT.2007.4458016.
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634.
   Zhang H., 2020, P IEEECVF C COMPUTER, P2736.
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515.},
Number-of-Cited-References = {57},
Times-Cited = {1},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {9},
Journal-ISO = {Front. Plant Sci.},
Doc-Delivery-Number = {5I3MI},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000868264700001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000431913000012,
Author = {Zhang, Shanwen and Zhang, Chuanlei and Wang, Zhen and Kong, Weiwei},
Title = {Combining sparse representation and singular value decomposition for
   plant recognition},
Journal = {APPLIED SOFT COMPUTING},
Year = {2018},
Volume = {67},
Pages = {164-171},
Month = {JUN},
Abstract = {Plant recognition is one of important research areas of pattern
   recognition. As plant leaves are extremely irregular, complex and
   diverse, many existing plant classification and recognition methods
   cannot meet the requirements of the automatic plant recognition system.
   A plant recognition approach is proposed by combining singular value
   decomposition (SVD) and sparse representation (SR) in this paper. The
   difference from the traditional plant classification methods is that,
   instead of establishing a classification model by extracting the
   classification features, the proposed method directly reduces the image
   dimensionality and recognizes the test samples based on the sparse
   coefficients, and uses the class-specific dictionary learning for sparse
   modeling to reduce the recognition time. The proposed method is verified
   on two plant leaf datasets and is compared with other four existing
   plant recognition methods The overall recognition accuracy of the
   proposed approach for the 6 kinds of plant leaves is over 96\%, which is
   the best classification rate. The experimental results show the
   feasibility and effectiveness of the proposed method. (C) 2018 Published
   by Elsevier B.V.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Zhang, CL (Corresponding Author), Tianjin Univ Sci \& Technol, Sch Comp Sci \& Informat Engn, 1038 Da Gu Nan Lu, Tianjin 300222, Peoples R China.
   Zhang, Shanwen; Wang, Zhen; Kong, Weiwei, XiJing Univ, Dept Informat Engn, Xian 710123, Shaanxi, Peoples R China.
   Zhang, Chuanlei, Tianjin Univ Sci \& Technol, Sch Comp Sci \& Informat Engn, 1038 Da Gu Nan Lu, Tianjin 300222, Peoples R China.},
DOI = {10.1016/j.asoc.2018.02.052},
ISSN = {1568-4946},
EISSN = {1872-9681},
Keywords = {Plant leaf image; Plant species recognition; Sparse representation (SR);
   Singular value decomposition (SVD)},
Keywords-Plus = {SHAPE; TEXTURE; FEATURES; COLOR},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications},
Author-Email = {97313114@tust.edu.cn},
Affiliations = {Xijing University; Tianjin University Science \& Technology},
ResearcherID-Numbers = {Wang, Zhen/A-2309-2011},
ORCID-Numbers = {Wang, Zhen/0000-0002-9182-4421},
Funding-Acknowledgement = {China National Natural Science Foundation {[}61473237, 61402331];
   Tianjin Research Program of Application Foundation and Advanced
   Technology {[}14JCYBJC42500]; Tianjin science and technology
   correspondent project {[}16JCTPJC47300]; Tianjin Jinnan District science
   and technology project},
Funding-Text = {This work was partially supported by China National Natural Science
   Foundation under grant No. 61473237 and 61402331, Tianjin Research
   Program of Application Foundation and Advanced Technology No.
   14JCYBJC42500, Tianjin science and technology correspondent project No.
   16JCTPJC47300 and 2017 Tianjin Jinnan District science and technology
   project.},
Cited-References = {Bama B. S., 2011, IND J COMP SCI ENG, V2, P202.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Bin Gan, 2014, BIOMED RES, P1.
   Bruckstein AM, 2009, SIAM REV, V51, P34, DOI 10.1137/060657704.
   Caglavan G., 2013, IMAGE ANAL P LECT NO, V815, P161.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Du JX, 2007, NEUROCOMPUTING, V70, P896, DOI 10.1016/j.neucom.2006.10.026.
   Du JX, 2013, NEUROCOMPUTING, V116, P150, DOI 10.1016/j.neucom.2012.03.028.
   Gan B., 2013, ENG, V5.
   {[}高永丽 GAO Yong-li], 2010, {[}计算机工程与设计, Computer Engineering and Design], V31, P1573.
   Gkalelis N, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P168.
   Hang XY, 2008, IEEE INT C BIO BIO W, P174, DOI 10.1109/BIBMW.2008.4686232.
   Jiming L., 2012, ADV INF SCI SERVICE, V4, P116.
   Ke Huang, 2006, ADV NIPS, P1.
   Kebapci H, 2011, COMPUT J, V54, P1475, DOI 10.1093/comjnl/bxq037.
   Kim SJ, 2007, IEEE J-STSP, V1, P606, DOI 10.1109/JSTSP.2007.910971.
   Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282.
   STEWART GW, 1993, SIAM REV, V35, P551, DOI 10.1137/1035134.
   Valliammal N., 2012, INT J COMPUT COMMUN, V6, P152.
   Wagner A, 2012, IEEE T PATTERN ANAL, V34, P372, DOI 10.1109/TPAMI.2011.112.
   Wang R.Q., 2000, B BOT, V17, P155.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79.
   Xu Y, 2013, INFORM SCIENCES, V238, P138, DOI 10.1016/j.ins.2013.02.051.
   Zhang, 2017, MULTIMED TOOLS APPL, V77, P1.
   Zhang SW, 2011, NEUROCOMPUTING, V74, P3690, DOI 10.1016/j.neucom.2011.07.015.
   Zhang SW, 2011, NEUROCOMPUTING, V74, P2284, DOI 10.1016/j.neucom.2011.03.007.
   Zhang SW, 2011, KNOWL-BASED SYST, V24, P341, DOI 10.1016/j.knosys.2010.11.002.
   Zheng CH, 2011, IEEE ACM T COMPUT BI, V8, P1273, DOI 10.1109/TCBB.2011.20.
   Zuo Yuanyuan, 2011, Tsinghua Science and Technology, V16, P13, DOI 10.1016/S1007-0214(11)70003-7.},
Number-of-Cited-References = {31},
Times-Cited = {22},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {17},
Journal-ISO = {Appl. Soft. Comput.},
Doc-Delivery-Number = {GF4EC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000431913000012},
DA = {2023-08-12},
}

@article{ WOS:000806136800002,
Author = {d'Andrimont, Raphael and Yordanov, Momchil and Martinez-Sanchez, Laura
   and van der Velde, Marijn},
Title = {Monitoring crop phenology with street-level imagery using computer
   vision},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2022},
Volume = {196},
Month = {MAY},
Abstract = {Street-level imagery holds a significant potential to scale-up in-situ
   data collection. This is enabled by combining the use of cheap
   high-quality cameras with recent advances in deep learning compute
   solutions to derive relevant thematic information. We present a
   framework to collect and extract crop type and phenological information
   from street level imagery using computer vision. Monitoring crop
   phenology is critical to assess gross primary productivity and crop
   yield. During the 2018 growing season, high-definition pictures were
   captured with side looking action cameras in the Flevoland province of
   the Netherlands. Each month from March to October, a fixed 200-km route
   was surveyed collecting one picture per second resulting in a total of
   400,000 geo-tagged pictures. At 220 specific parcel locations, detailed
   on the spot crop phenology observations were recorded for 17 crop types
   (including bare soil, green manure, and tulips): bare soil, carrots,
   green manure, grassland, grass seeds, maize, onion, potato, summer
   barley, sugar beet, spring cereals, spring wheat, tulips, vegetables,
   winter barley, winter cereals and winter wheat. Furthermore, the time
   span included specific pre-emergence parcel stages, such as differently
   cultivated bare soil for spring and summer crops as well as post-harvest
   cultivation practices, e.g. green manuring and catch crops.
   Classification was done using TensorFlow with a well-known image
   recognition model, based on transfer learning with convolutional neural
   network (MobileNet). A hypertuning methodology was developed to obtain
   the best performing model among 160 models. This best model was applied
   on an independent inference set discriminating crop type with a Macro F1
   score of 88.1\% and main phenological stage at 86.9\% at the parcel
   level. Potential and caveats of the approach along with practical
   considerations for implementation and improvement are discussed. The
   proposed framework speeds up high quality in-situ data collection and
   suggests avenues for massive data collection via automated
   classification using computer vision.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {d'Andrimont, R (Corresponding Author), European Commiss, Joint Res Ctr JRC, Ispra, Italy.
   d'Andrimont, Raphael; Yordanov, Momchil; Martinez-Sanchez, Laura; van der Velde, Marijn, European Commiss, Joint Res Ctr JRC, Ispra, Italy.},
DOI = {10.1016/j.compag.2022.106866},
EarlyAccessDate = {APR 2022},
Article-Number = {106866},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Phenology; Plant recognition; Agriculture; Computer vision; Deep
   learning; Remote sensing; CNN; BBCH; Crop type; Street view imagery;
   Survey; In-situ; Earth observation; Parcel; In situ},
Keywords-Plus = {VIEW},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {raphael.dandrimont@ec.europa.eu},
Affiliations = {European Commission Joint Research Centre; EC JRC ISPRA Site},
ResearcherID-Numbers = {van der Velde, Marijn/B-3305-2009
   },
ORCID-Numbers = {van der Velde, Marijn/0000-0002-9103-7081
   d'Andrimont, Raphael/0000-0002-7326-7684},
Funding-Acknowledgement = { {[}31280]},
Funding-Text = {The authors thank Guido Lemoine for his support and agronomic inputs.
   The authors would like to thank TerraSphere Imaging for the high quality
   data collection, in particular Menno de Vries and Paul van der Voet. The
   authors would also like to thank Dominique Fasbender for his precious
   statistical advice and JRC colleagues from the Big DataAnalytics project
   for their support. The authors also thank the Explor-atory Research
   program of the Joint Research Centre for funding the Rural Refocus
   project (31280) .},
Cited-References = {Affouard A., 2017, P ICLR INT C LEARN R.
   Anami BS, 2020, ARTIF INTELL AGR, V4, P12, DOI 10.1016/j.aiia.2020.03.001.
   {[}Anonymous], 2001, BB MONOGRAPH.
   {[}Anonymous], 2018, BAS GEW BRP.
   {[}Anonymous], 2011, AMAZON WEB SERVICES.
   Barve V, 2014, ECOL INFORM, V24, P194, DOI 10.1016/j.ecoinf.2014.08.008.
   Branco P., 2015, ARXIV, V1505, P1.
   Cao MY, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13122331.
   Champ J, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11373.
   D'Andrimont R, 2020, SCI DATA, V7, DOI 10.1038/s41597-020-00675-z.
   d'Andrimont R, 2020, REMOTE SENS ENVIRON, V239, DOI 10.1016/j.rse.2020.111660.
   d'Andrimont R, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081300.
   Deng J., 2009, 2009 IEEE C COMP VIS, P248, DOI DOI 10.1109/CVPR.2009.5206848.
   Deus E, 2016, ENVIRON MONIT ASSESS, V188, DOI 10.1007/s10661-016-5555-1.
   ElQadi MM, 2017, ECOL INFORM, V39, P23, DOI 10.1016/j.ecoinf.2017.02.006.
   Gebru T, 2017, P NATL ACAD SCI USA, V114, P13108, DOI 10.1073/pnas.1700035114.
   Goeau H, 2018, BIODIVERS INF SCI ST, V2, P25637, DOI {[}10.3897/biss.2.25637, DOI 10.3897/BISS.2.25637].
   Howard A.G., 2017, ABS170404861 CORR.
   Huang YP, 2019, ADV NEUR IN, V32.
   Hufkens K, 2019, AGR FOREST METEOROL, V265, P327, DOI 10.1016/j.agrformet.2018.11.002.
   Iandola F.N., 2016, ARXIV.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Keenan TF, 2014, NAT CLIM CHANGE, V4, P598, DOI {[}10.1038/NCLIMATE2253, 10.1038/nclimate2253].
   Klosterman ST, 2014, BIOGEOSCIENCES, V11, P4305, DOI 10.5194/bg-11-4305-2014.
   Kussul N, 2014, INT GEOSCI REMOTE SE, P1497, DOI 10.1109/IGARSS.2014.6946721.
   LeCun Y., 1995, CONVOLUTIONAL NETWOR, P255, DOI {[}10.5555/303568.303704, DOI 10.5555/303568.303704].
   LeCun Yann, 2015, LENET 5 CONVOLUTIONA.
   Lemoine G., 2013, NAT HAZARD EARTH SYS, V1, P1445, DOI DOI 10.5194/NHESSD-1-1445-2013.
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI {[}10.1109/TPAMI.2017.2773081, 10.1109/TCC.2017.2764082].
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Namin ST, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0333-4.
   Nijland W, 2016, REMOTE SENS ENVIRON, V177, P13, DOI 10.1016/j.rse.2016.02.018.
   Olivas E. S., 2009, HDB RES MACHINE LEAR.
   Opitz J., 2019, ARXIV PREPRINT ARXIV, Patent No. {[}abs/1911.03347, 191103347].
   Paliyam M., 2021, TACKLING CLIMATE CHA.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Ringland J, 2019, COMPUT ELECTRON AGR, V158, P36, DOI 10.1016/j.compag.2019.01.014.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Sabottke CF, 2020, RADIOL-ARTIF INTELL, V2, DOI 10.1148/ryai.2019190015.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Schaap B.F., 2011, ADAPTING AGR 2050 FL.
   Seiferling I, 2017, LANDSCAPE URBAN PLAN, V165, P93, DOI 10.1016/j.landurbplan.2017.05.010.
   Soille P, 2018, FUTURE GENER COMP SY, V81, P30, DOI 10.1016/j.future.2017.11.007.
   Sonnentag O, 2012, AGR FOREST METEOROL, V152, P159, DOI 10.1016/j.agrformet.2011.09.009.
   Stafford R, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0014381.
   Sun YM, 2009, INT J PATTERN RECOGN, V23, P687, DOI 10.1142/S0218001409007326.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Waldner F, 2019, INT J APPL EARTH OBS, V80, P82, DOI 10.1016/j.jag.2019.01.002.
   Wu FM, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041165.
   Wu Y., 2019, DETECTRON 2.
   Yalcin Hulya, 2015, 2015 Fourth International Conference on Agro-Geoinformatics (Agro-Geoinformatics), P338, DOI 10.1109/Agro-Geoinformatics.2015.7248114.
   Yalcin H, 2017, INT CONF AGRO-GEOINF, P332.
   Yan YL, 2021, ISPRS J PHOTOGRAMM, V171, P278, DOI 10.1016/j.isprsjprs.2020.11.022.
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716.
   Zheng YY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19051058.
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907.},
Number-of-Cited-References = {56},
Times-Cited = {5},
Usage-Count-Last-180-days = {13},
Usage-Count-Since-2013 = {26},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {1V5NT},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000806136800002},
OA = {Green Submitted, hybrid},
DA = {2023-08-12},
}

@inproceedings{ WOS:000464411500088,
Author = {Khmag, Asem and Al-Haddad, S. A. R. and Kamarudin, Noraziahtulhidayu},
Book-Author = {Othman, ML
   Abdrahman, F
   Kamsani, NA
   Fauzi, MFA},
Title = {Recognition System for Leaf Images Based on Its Leaf Contour And
   Centroid},
Booktitle = {PROCEEDINGS OF THE 2017 IEEE 15TH STUDENT CONFERENCE ON RESEARCH AND
   DEVELOPMENT (SCORED)},
Series = {IEEE Student Conference on Research and Development SCOReD},
Year = {2017},
Pages = {467-472},
Note = {15th IEEE Student Conference on Research and Development (IEEE SCOReD),
   Putrajaya, MALAYSIA, DEC 13-14, 2017},
Organization = {IEEE; Univ Putra Malaysia; IEEE Malaysia Sect; Univ Putra Malaysia, Dept
   Elect \& Elect Engn; Univ Putra Malaysia, IEEE Student Branch},
Abstract = {The recognition of plants is directly associated to society's life.
   Leaves from plants are proved to be a feasible source of information
   used to identify plant species. The recognition system of leaves is
   accomplished automatically using the experts of human being.
   Unfortunately, it has their loopholes that are a time consuming
   processes and low-effectiveness progression. The leaves classification
   using predictable process is quite complicated, time complexity, and as
   a result of using very long-termed in botanical science for non-experts
   that make it more irritated operation. Thus, the prompt developments in
   digital images, computer vision and object detection and recognition
   systems encourage scientists to work towards plant species recognition
   according to image processing technology. In this study, an image
   processing algorithm in order to find out the shape structure of tested
   plants is presented. This technique exploits the variant to scaling
   shift, spin technique, scaling approach, and filtering processes. The
   leaf contours of the same plants are computed using Support Victor
   Machine (SVM) where the similar sequences of the same contours usually
   carry the same features while the different plants sequences have
   different contours. In this regard, SVM classifier is exploited to be
   applied as a classifier to the plant's leaf. In the Experiment part, the
   finding was taken from Flavia dataset and it demonstrated that the
   suggested technique has high recognition efficiency compared to state of
   the art methods and is shows better quality images especially in
   complicated features of digital images such as ridges, edges, lines,
   curves and complicated contours.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Khmag, A (Corresponding Author), Univ Zawia, Fac Engn, Comp Syst Engn, Az Zawia, Libya.
   Khmag, Asem, Univ Zawia, Fac Engn, Comp Syst Engn, Az Zawia, Libya.
   Al-Haddad, S. A. R.; Kamarudin, Noraziahtulhidayu, Univ Putra Malaysia, Comp \& Commun Engn, Serdang, Malaysia.},
ISSN = {2643-2439},
EISSN = {2643-2447},
ISBN = {978-1-5386-2126-4},
Keywords = {leaf recognition; support vector machine; image contours; feature
   extraction; image classification},
Keywords-Plus = {REPRESENTATION; CLASSIFICATION; RETRIEVAL},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Engineering, Multidisciplinary;
   Engineering, Electrical \& Electronic},
Author-Email = {Khmaj2002@gmail.com},
Affiliations = {Universiti Putra Malaysia},
ResearcherID-Numbers = {Al-Haddad, S. A. R./AAM-6449-2020
   kamarudin, noraziahtulhidayu/AAQ-8508-2021
   Khmag, Asem/AAH-1051-2019
   },
ORCID-Numbers = {Khmag, Asem/0000-0002-1360-5346
   Kamarudin, Noraziahtulhidayu/0000-0001-7467-4348},
Cited-References = {Adamek T, 2004, IEEE T CIRC SYST VID, V14, P742, DOI 10.1109/TCSVT.2004.826776.
   Alajlan N, 2007, PATTERN RECOGN, V40, P1911, DOI 10.1016/j.patcog.2006.12.005.
   Apriyanti D, 2013 INT C COMP CONT, P53.
   Bao P, 2005, IEEE T PATTERN ANAL, V27, P1485, DOI 10.1109/TPAMI.2005.173.
   Du JX, 2013, NEUROCOMPUTING, V116, P150, DOI 10.1016/j.neucom.2012.03.028.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Gonzalez R.C., 2004, DIGITAL IMAGE PROCES.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Joly A, 2014, INT WORKSH ENV MULT.
   Kadir A., 2011, International Journal of Computer Science \& Information Technology, V3, P256, DOI 10.5121/ijcsit.2011.3318.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Khmag A, 2017, VISUAL COMPUTER, P1.
   Khmag A, 2017, VISUAL COMPUT, V33, P1141, DOI 10.1007/s00371-016-1273-5.
   Khmag A, 2016, IEEJ T ELECTR ELECTR, V11, P339, DOI 10.1002/tee.22223.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Milan S., 2008, IMAGE PROCESSING ANA.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Nesaratnam R, 2015 INT C INN INF E, P1.
   Parekh R., 2012, INT J ADV ENG TECHNO, V2, P149.
   Rafael C.G., 2008, DIGITAL IMAGE PROCES.
   Waldchen J, 2016, LANDSCH NATURSCH THU, V53, P121.
   Wang B, 2015, INFORM SCIENCES, V302, P132, DOI 10.1016/j.ins.2014.07.028.
   Wang XF, 2005, LECT NOTES COMPUT SC, V3644, P87.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wang Z., 2016, INT C MACHINE LEARNI, P2939, DOI DOI 10.1007/S11831-016-9181-4.
   Wu S. G., 2007 IEEE INT S SIGN, P11.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xiaorong Li, 1988, Proceedings of the SPIE - The International Society for Optical Engineering, V938, P372.
   Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008.
   Zhang L, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 5, PROCEEDINGS, P90, DOI 10.1109/ICNC.2008.253.
   Zulkifli Z, 2011 11 INT C HYBR I, P430.},
Number-of-Cited-References = {31},
Times-Cited = {11},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BM4XN},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000464411500088},
OA = {Green Accepted},
DA = {2023-08-12},
}

@article{ WOS:000477040700001,
Author = {Rzanny, Michael and Maeder, Patrick and Deggelmann, Alice and Chen,
   Minqian and Waeldchen, Jana},
Title = {Flowers, leaves or both? How to obtain suitable images for automated
   plant identification},
Journal = {PLANT METHODS},
Year = {2019},
Volume = {15},
Month = {JUL 23},
Abstract = {BackgroundDeep learning algorithms for automated plant identification
   need large quantities of precisely labelled images in order to produce
   reliable classification results. Here, we explore what kind of
   perspectives and their combinations contain more characteristic
   information and therefore allow for higher identification
   accuracy.ResultsWe developed an image-capturing scheme to create
   observations of flowering plants. Each observation comprises five
   in-situ images of the same individual from predefined perspectives
   (entire plant, flower frontal- and lateral view, leaf top- and back side
   view). We collected a completely balanced dataset comprising 100
   observations for each of 101 species with an emphasis on groups of
   conspecific and visually similar species including twelve Poaceae
   species. We used this dataset to train convolutional neural networks and
   determine the prediction accuracy for each single perspective and their
   combinations via score level fusion. Top-1 accuracies ranged between
   77\% (entire plant) and 97\% (fusion of all perspectives) when averaged
   across species. Flower frontal view achieved the highest accuracy
   (88\%). Fusing flower frontal, flower lateral and leaf top views yields
   the most reasonable compromise with respect to acquisition effort and
   accuracy (96\%). The perspective achieving the highest accuracy was
   species dependent.ConclusionsWe argue that image databases of herbaceous
   plants would benefit from multi organ observations, comprising at least
   the front and lateral perspective of flowers and the leaf top view.},
Publisher = {BMC},
Address = {CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Rzanny, M (Corresponding Author), Max Planck Inst Biogeochem, Dept Biogeochem Integrat, Hans Knoll Str 10, Jena, Germany.
   Rzanny, Michael; Deggelmann, Alice; Waeldchen, Jana, Max Planck Inst Biogeochem, Dept Biogeochem Integrat, Hans Knoll Str 10, Jena, Germany.
   Maeder, Patrick; Chen, Minqian, Tech Univ Ilmenau, Software Engn Safety Crit Syst Grp, Ehrenberg Str 20, D-98693 Ilmenau, Germany.},
DOI = {10.1186/s13007-019-0462-4},
Article-Number = {77},
EISSN = {1746-4811},
Keywords = {Species identification; Object classification; Multi-organ plant
   classification; Convolutional networks; Deep learning; Plant images;
   Computer vision; Plant observation; Plant determination; Plant leaf;
   Flower; Poaceae},
Keywords-Plus = {DIVERSITY},
Research-Areas = {Biochemistry \& Molecular Biology; Plant Sciences},
Web-of-Science-Categories  = {Biochemical Research Methods; Plant Sciences},
Author-Email = {mrzanny@bgc-jena.mpg.de},
Affiliations = {Max Planck Society; Technische Universitat Ilmenau},
ResearcherID-Numbers = {Rzanny, Michael/GOH-1028-2022
   Mäder, Patrick/A-1848-2018},
ORCID-Numbers = {Rzanny, Michael/0000-0002-7232-5547
   Mäder, Patrick/0000-0001-6871-2707},
Funding-Acknowledgement = {German Ministry of Education and Research (BMBF) {[}01LC1319A,
   01LC1319B]; German Federal Ministry for the Environment, Nature
   Conservation, Building and Nuclear Safety (BMUB) {[}3514 685C19];
   Stiftung Naturschutz Thringen (SNT) {[}SNT-082-248-03/2014]},
Funding-Text = {We are funded by the German Ministry of Education and Research (BMBF)
   Grants: 01LC1319A and 01LC1319B; the German Federal Ministry for the
   Environment, Nature Conservation, Building and Nuclear Safety (BMUB)
   Grant: 3514 685C19; and the Stiftung Naturschutz Thringen (SNT) Grant:
   SNT-082-248-03/2014.},
Cited-References = {Allan E, 2013, OECOLOGIA, V173, P223, DOI 10.1007/s00442-012-2589-0.
   {[}Anonymous], 2019, FLOR CAPT.
   Champ J, 2015, CLEF C LABS EV FOR C, V1391.
   Ebeling A, 2018, OIKOS, V127, P208, DOI 10.1111/oik.04210.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Goeau H., 2016, WORKING NOTES CLEF 2, P428.
   Goeau H, 2018, BIODIVERS INF SCI ST, V2, P25637, DOI {[}10.3897/biss.2.25637, DOI 10.3897/BISS.2.25637].
   He AF, 2016, IEEE SYS MAN CYBERN, P2020, DOI 10.1109/SMC.2016.7844537.
   Huang J, 2016, ARXIV161110012 CORR.
   Jansen Florian, 2008, Tuexenia, P239.
   Joly A, 2016, MULTIMEDIA SYST, V22, P751, DOI 10.1007/s00530-015-0462-9.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Krause J, 2018, ICMR `18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P517, DOI 10.1145/3206025.3206089.
   Kumar N, 2012, 12 EUR C COMP VIS EC.
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Lee SH, 2017, IEEE IMAGE PROC, P4462, DOI 10.1109/ICIP.2017.8297126.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lee ST, 2018, 2018 IEEE SYMPOSIUM ON VLSI TECHNOLOGY, P169, DOI 10.1109/VLSIT.2018.8510667.
   Murphy GEP, 2014, ECOL EVOL, V4, P91, DOI 10.1002/ece3.909.
   Naeem S, 2012, SCIENCE, V336, P1401, DOI 10.1126/science.1215855.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Pimm SL, 2015, TRENDS ECOL EVOL, V30, P685, DOI 10.1016/j.tree.2015.08.008.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Rzanny M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0245-8.
   Rzanny M, 2012, J ANIM ECOL, V81, P614, DOI 10.1111/j.1365-2656.2012.01951.x.
   Seeland M, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-018-2474-x.
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Do TB, 2017, INT CONF KNOWL SYS, P191, DOI 10.1109/KSE.2017.8119457.
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1.
   W\_aldchen J., 2019, BIOL UNSERER Z, V49, P99, DOI {[}10.1002/biuz.201970211, DOI 10.1002/BIUZ.201970211].
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197.},
Number-of-Cited-References = {33},
Times-Cited = {31},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {16},
Journal-ISO = {Plant Methods},
Doc-Delivery-Number = {IL1EC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000477040700001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000626021406029,
Author = {Alamoudi, Shadi and Hong, Xia and Wei, Hong},
Book-Group-Author = {IEEE},
Title = {Plant Leaf Recognition Using Texture Features and Semi-Supervised
   Spherical K-means Clustering},
Booktitle = {2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)},
Series = {IEEE International Joint Conference on Neural Networks (IJCNN)},
Year = {2020},
Note = {International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI),
   ELECTR NETWORK, JUL 19-24, 2020},
Organization = {IEEE; IEEE Computat Intelligence Soc; Int Neural Network Soc},
Abstract = {Automatic plant leave recognition using digital images and machine
   learning techniques is an important task. The disadvantage of supervised
   learning techniques is that they are limited to learn from labelled
   datasets which are often expensive to obtain. In this paper, a novel
   decision fusion framework is proposed by combining semi-supervised
   clustering with the well known image features analysis methods in
   computer vision. Initially the leave image features are generated by
   applying the Grey Level Co-occurrence Matrix analysis to the processed
   leave images transformed by Gabor or Laplacian of Gaussian filters. Then
   an on-line spherical k-means clustering technique, guided by a minimum
   number of labelled leaves, is used to train the base classifiers. The
   final decision of classification is produced by selecting classifier
   which produces the max-cosine value amongst the baseline classifiers.
   Comparative experiments have been carried out to demonstrate that
   proposed approaches are suited for automatic leave type recognition.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Alamoudi, S (Corresponding Author), Univ Reading, Sch Math Phys \& Computat Sci, Dept Comp Sci, Reading RG6 6AY, Berks, England.
   Alamoudi, Shadi; Hong, Xia; Wei, Hong, Univ Reading, Sch Math Phys \& Computat Sci, Dept Comp Sci, Reading RG6 6AY, Berks, England.},
DOI = {10.1109/ijcnn48605.2020.9207386},
ISSN = {2161-4393},
ISBN = {978-1-7281-6926-2},
Keywords-Plus = {IMAGE RETRIEVAL; SHAPE; IDENTIFICATION; SEGMENTATION; FILTER},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Hardware \&
   Architecture},
Affiliations = {University of Reading},
Cited-References = {Aguirre-Ramos H, 2018, APPL MATH COMPUT, V339, P568, DOI 10.1016/j.amc.2018.07.057.
   Bair E, 2013, WIRES COMPUT STAT, V5, P349, DOI 10.1002/wics.1270.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   DAVIES ER, 2008, HDB TEXTURE ANAL, P3, DOI DOI 10.1109/IMVIP.2008.27.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Fu H., 2003, CHINESE B BOT, V21, P429.
   Guyer D., 1984, COMPUTER VISION IMAG.
   Han JW, 2003, SIGNAL PROCESS-IMAGE, V18, P141, DOI 10.1016/S0923-5965(02)00116-9.
   Hang ST, 2016, CLEF 2016 C.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   Haykin S., 2009, NEURAL NETWORKS LEAR.
   Hickey L., 1999, MANUAL LEAF ARCHITEC, P26.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   JAIN AK, 1990, 1990 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS, P14, DOI 10.1016/0031-3203(91)90143-S.
   Kong H, 2013, IEEE T CYBERNETICS, V43, P1719, DOI 10.1109/TSMCB.2012.2228639.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020.
   Mzoughi O, 2013, IEEE IMAGE PROC, P3967, DOI 10.1109/ICIP.2013.6738817.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Russell S.J., 2016, ARTIF INTELL.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sumaiya MN, 2017, OPTIK, V130, P114, DOI 10.1016/j.ijleo.2016.11.040.
   Wang ZY, 2000, LECT NOTES COMPUT SC, V1929, P477.
   Weier TE, 1974, BOT INTRO PLANT BIOL.
   Wu Q., 2006, ADV ARTIF INTELL, V20, P3.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yahiaoui I, 2006, LECT NOTES COMPUT SC, V4261, P357.
   Zhang F., 2002, COMPUTER ENG APPL, V7, P67.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhong S, 2005, IEEE IJCNN, P3180.},
Number-of-Cited-References = {34},
Times-Cited = {9},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BQ9MM},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000626021406029},
DA = {2023-08-12},
}

@inproceedings{ WOS:000461145900074,
Author = {Krause, Jonas and Sugita, Gavin and Baek, Kyungim and Lim, Lipyeow},
Book-Group-Author = {Assoc Comp Machinery},
Title = {WTPlant(What's That Plant?): a Deep Learning System for Identifying
   Plants in Natural Images},
Booktitle = {ICMR `18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON
   MULTIMEDIA RETRIEVAL},
Year = {2018},
Pages = {517-520},
Note = {8th ACM International Conference on Multimedia Retrieval (ACM ICMR),
   Yokohama, JAPAN, JUN 11-14, 2018},
Organization = {Assoc Comp Machinery; ACM SIGMM},
Abstract = {Despite the availability of dozens of plant identification mobile
   applications, identifying plants from a natural image remains a
   challenging problem - most of the existing applications do not address
   the complexity of natural images, the large number of plant species, and
   the multi-scale nature of natural images. In this technical
   demonstration, we present the WTPlant system for identifying plants in
   natural images. WTPlant is based on deep learning approaches.
   Specifically, it uses stacked Convolutional Neural Networks for image
   segmentation, a novel preprocessing stage for multi-scale analyses, and
   deep convolutional networks to extract the most discriminative features.
   WTPlant employs different classification architectures for plants and
   flowers, thus enabling plant identification throughout all the seasons.
   The user interface also shows, in an interactive way, the most
   representative areas in the image that are used to predict each plant
   species. The first version of WTPlant is trained to classify 100
   different plant species present in the campus of the University of
   Hawai'i at Manoa. First experiments support the hypothesis that an
   initial segmentation process helps guide the extraction of
   representative samples and, consequently, enables Convolutional Neural
   Networks to better recognize objects of different scales in natural
   images. Future versions aim to extend the recognizable species to cover
   the land-based flora of the Hawaiian Islands.},
Publisher = {ASSOC COMPUTING MACHINERY},
Address = {1515 BROADWAY, NEW YORK, NY 10036-9998 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Krause, J (Corresponding Author), Univ Hawaii Manoa, Dept Informat \& Comp Sci, 1680 East West Rd, Honolulu, HI 96822 USA.
   Krause, Jonas; Sugita, Gavin; Baek, Kyungim; Lim, Lipyeow, Univ Hawaii Manoa, Dept Informat \& Comp Sci, 1680 East West Rd, Honolulu, HI 96822 USA.},
DOI = {10.1145/3206025.3206089},
ISBN = {978-1-4503-5046-4},
Keywords = {Image Processing; Deep Learning; Convolutional Neural Network; Plant
   Taxonomy; Multi-Scale Analysis},
Keywords-Plus = {IDENTIFICATION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Theory \&
   Methods},
Author-Email = {krausej@hawaii.edu
   gsugita6@hawaii.edu
   kyungim@hawaii.edu
   lipyeow@hawaii.edu},
Affiliations = {University of Hawaii System; University of Hawaii Manoa},
Funding-Acknowledgement = {Brazilian National Council for Scientific and Technological Development
   (CNPq) {[}219438-14.5]},
Funding-Text = {This research was supported in part by the Brazilian National Council
   for Scientific and Technological Development (CNPq) under grant number
   219438-14.5.},
Cited-References = {Affouard A., 2017, P 5 INT C LEARN REPR.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Cerutti G, 2013, COMPUT VIS IMAGE UND, V117, P1482, DOI 10.1016/j.cviu.2013.07.003.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lasseck  Mario, 2017, WORKING NOTES CROSS.
   Lee Sue H., 2015, ARXIVORGABS150608425.
   Namin S T, 2017, BIORXIV, DOI {[}10.1101/134205, DOI 10.1101/134205].
   Pound M. P., 2017, DEEP LEARNING MULTIT.
   Pound MP, 2017, GIGASCIENCE, V6, DOI 10.1093/gigascience/gix083.
   Roerdink J. B. T. M., 2000, Fundamenta Informaticae, V41, P187.
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Ubbens JR, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01190.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Zhou B., 2017, PROC CVPR IEEE, P633, DOI DOI 10.1109/CVPR.2017.544.},
Number-of-Cited-References = {18},
Times-Cited = {7},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BM2KP},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000461145900074},
DA = {2023-08-12},
}

@inproceedings{ WOS:000351597601121,
Author = {Aptoula, E. and Yanikoglu, B.},
Book-Group-Author = {IEEE},
Title = {MORPHOLOGICAL FEATURES FOR LEAF BASED PLANT RECOGNITION},
Booktitle = {2013 20TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP 2013)},
Series = {IEEE International Conference on Image Processing ICIP},
Year = {2013},
Pages = {1496-1499},
Note = {20th IEEE International Conference on Image Processing (ICIP),
   Melbourne, AUSTRALIA, SEP 15-18, 2013},
Organization = {Inst Elect \& Elect Engineers; IEEE Signal Proc Soc},
Abstract = {Although plant recognition has become an increasingly popular research
   topic, it remains nonetheless a scientific and technical challenge.
   Besides all the difficulties of classic object recognition, such as
   illumination, viewpoint and scale variations, plants can additionally
   exhibit visual changes depending on their age and condition, thus
   demanding a specialized approach. In this paper, we present two
   descriptors based on mathematical morphology; the first consists of the
   computation of morphological covariance on the leaf contour profile and
   the second is an extension of the recently introduced circular
   covariance histogram, capturing leaf venation characteristics. The
   effectiveness of both descriptors has been validated with the ImageClef'
   12 plant identification dataset.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Aptoula, E (Corresponding Author), Okan Univ, Dept Comp Engn, TR-34959 Istanbul, Turkey.
   Aptoula, E., Okan Univ, Dept Comp Engn, TR-34959 Istanbul, Turkey.
   Yanikoglu, B., Sabanci Univ, Dept Comp Engn, TR-34956 Istanbul, Turkey.},
ISSN = {1522-4880},
ISBN = {978-1-4799-2341-0},
Keywords = {Plant recognition; feature extraction; mathematical morphology;
   morphological covariance; circular covariance histogram},
Keywords-Plus = {CLASSIFICATION; IMAGES},
Research-Areas = {Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Imaging Science \& Photographic Technology},
Affiliations = {Okan University; Sabanci University},
ResearcherID-Numbers = {Yanikoglu, Berrin/AAE-4843-2022
   Aptoula, Erchan/AAI-1070-2020},
ORCID-Numbers = {Yanikoglu, Berrin/0000-0001-7403-7592
   Aptoula, Erchan/0000-0001-6168-2883},
Cited-References = {Aptoula E, 2012, PATTERN RECOGN, V45, P4524, DOI 10.1016/j.patcog.2012.06.004.
   Aptoula E, 2011, ADV IMAG ELECT PHYS, V169, P1, DOI 10.1016/B978-0-12-385981-5.00001-X.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Goeau H., 2011, CLEF 2011 NOTES.
   Lin FY, 2008, COMM COM INF SC, V15, P432.
   Mokhtarian F, 2004, IEEE T IMAGE PROCESS, V13, P653, DOI 10.1109/TIP.2004.826126.
   Shu X, 2011, IMAGE VISION COMPUT, V29, P286, DOI 10.1016/j.imavis.2010.11.001.
   Villena-Roman J., 2011, CLEF NOTEBOOK PAPERS.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Yanikoglu B., 2012, CLEF NOTES.
   Yanikoglu B., 2011, CLEF NOTES.},
Number-of-Cited-References = {12},
Times-Cited = {18},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BC3FD},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000351597601121},
DA = {2023-08-12},
}

@inproceedings{ WOS:000374793800027,
Author = {Brilhador, Anderson and Serrarens, Daniel A. and Lopes, Fabricio M.},
Editor = {Pardo, A and Kittler, J},
Title = {A Computer Vision Approach for Automatic Measurement of the Inter-plant
   Spacing},
Booktitle = {PROGRESS IN PATTERN RECOGNITION, IMAGE ANALYSIS, COMPUTER VISION, AND
   APPLICATIONS, CIARP 2015},
Series = {Lecture Notes in Computer Science},
Year = {2015},
Volume = {9423},
Pages = {219-227},
Note = {20th Iberoamerican Congress on Pattern Recognition (CIARP), Montevideo,
   URUGUAY, NOV 09-12, 2015},
Organization = {IAPR; Uruguayan IAPR Chapter; Argentine Soc Pattern Recognit; Special
   Interest Grp Brazilian Comp Soc; Chilean Assoc Pattern Recognit; Cuban
   Assoc Pattern Recognit; Mexican Assoc Comp Vis, Neural Comp \& Robot;
   Spanish Assoc Pattern Recognit \& Image Anal; Portuguese Assoc Pattern
   Recognit},
Abstract = {Global food demand is increasing every year and it is needed to respond
   to this demand. In addition, some crops such as corn, which is the most
   produced grain in the world, is used as food, feed, bio-energy and other
   industrial purposes. Thus, it is needed the development of new
   technologies that make possible to produce more from less land. In
   particular, the corn crop is sensitive to its spatial arrangement and
   any variation in plant distribution pattern can lead to reduction in
   corn production. Nowadays, the uniformity of the plant spacing is
   checked manually by agronomists in order to predict possible production
   losses. In this context, this work proposes an automatic approach for
   measuring the spacing between corn plants in the early stages of growth.
   The proposed approach is based on computer vision techniques in order to
   evaluate the automatic inter-plant spacing measurement from images in a
   simple and efficient way, allowing its use on devices with low
   computational power such as smart phones and tablets. An image dataset
   was built as an additional contribution of this work containing 2186
   corn plants in two conditions: tillage after the application of
   herbicide (TH) with 1387 corn plants and conventional tillage (CT) with
   799 corn plants. The dataset is available at url:
   http://github.com/Brilhador/cornspacing. The experimental results
   achieve 90\% of precision and 92\% of sensitivity in corn plant
   identification. Regarding the automatic measurement of the interplant
   spacing, the results showed no significant differences from the same
   measurements taken manually, indicating the effectiveness of the
   proposed approach in two distinct types of planting.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Lopes, FM (Corresponding Author), Univ Tecnol Fed Parana, Cornelio Procopio, PR, Brazil.
   Brilhador, Anderson; Lopes, Fabricio M., Univ Tecnol Fed Parana, Cornelio Procopio, PR, Brazil.
   Serrarens, Daniel A., Belagr Comercio \& Representacao Prod Agr Ltd Para, Tamarama, Brazil.},
DOI = {10.1007/978-3-319-25751-8\_27},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-25751-8; 978-3-319-25750-1},
Keywords = {Computer vision; Inter-plant spacing; Pattern recognition; Precision
   agriculture; Image processing},
Keywords-Plus = {POPULATION; SHAPE},
Research-Areas = {Computer Science; Robotics},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory \& Methods;
   Robotics},
Author-Email = {andersonbrilhador@gmail.com
   daniel.serrarens@belagricola.com.br
   fabricio@utfpr.edu.br},
Affiliations = {Universidade Tecnologica Federal do Parana},
ResearcherID-Numbers = {Lopes, Fabrício Martins/F-6996-2010},
ORCID-Numbers = {Lopes, Fabrício Martins/0000-0002-8786-3313},
Cited-References = {Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120.
   {[}Anonymous], 1993, CORN PLANT DEV.
   {[}Anonymous], 2008, ANALISE IMAGENS DIGI.
   Brilhador A., 2013, LNCS, V8258.
   Costa LD, 2009, IMAGE PROCESS SER, P1.
   Council I.G., 2015, GRAIN MARK REP.
   Doerge T, 2002, CROP INSIGHTS, V12, P1.
   Freire F.M., 2001, MANEJO FERTILIDADE S.
   Gee C, 2008, COMPUT ELECTRON AGR, V60, P49, DOI 10.1016/j.compag.2007.06.003.
   Kataoka T, 2003, IEEE ASME INT C ADV, P1079, DOI 10.1109/aim.2003.1225492.
   Kazmi W, 2015, COMPUT ELECTRON AGR, V112, P10, DOI 10.1016/j.compag.2015.01.008.
   Nakarmi AD, 2012, COMPUT ELECTRON AGR, V82, P23, DOI 10.1016/j.compag.2011.12.011.
   Nakarmi AD, 2014, BIOSYST ENG, V125, P54, DOI 10.1016/j.biosystemseng.2014.07.001.
   Nielsen R. L., 2006, EFFECT PLANT SPACING.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Sangoi L, 2002, FIELD CROP RES, V79, P39, DOI 10.1016/S0378-4290(02)00124-7.
   Shrestha DS, 2005, APPL ENG AGRIC, V21, P295.
   Shrestha DS, 2004, BIOSYST ENG, V89, P119, DOI 10.1016/j.biosystemseng.2004.06.007.
   Tang L, 2008, T ASABE, V51, P2181, DOI 10.13031/2013.25381.
   Webb Andrew R, 2003, STAT PATTERN RECOGNI.
   WILLEY RW, 1981, EXP AGR, V17, P63, DOI 10.1017/S0014479700011248.
   Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008.},
Number-of-Cited-References = {22},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BE6UG},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000374793800027},
OA = {Bronze},
DA = {2023-08-12},
}

@article{ WOS:000433653700005,
Author = {Gao, Junfeng and Nuyttens, David and Lootens, Peter and He, Yong and
   Pieters, Jan G.},
Title = {Recognising weeds in a maize crop using a random forest machine-learning
   algorithm and near-infrared snapshot mosaic hyperspectral imagery},
Journal = {BIOSYSTEMS ENGINEERING},
Year = {2018},
Volume = {170},
Pages = {39-50},
Month = {JUN},
Abstract = {This study explores the potential of a novel hyperspectral snapshot
   mosaic camera for weed and maize classification. The image processing,
   feature engineering and machine learning techniques were discussed when
   developing an optimal classification model for the three kinds of weeds
   and maize. A total set of 185 spectral features including reflectance
   and vegetation index features was constructed. Subsequently, the
   principal component analysis was used to reduce the redundancy of the
   constructed features, and the first 5 principal components, explaining
   over 95\% variance ratio, were kept for further analysis. Furthermore,
   random forests as one of machine learning techniques were built for
   developing the classifier with three different combinations of features.
   Accuracy oriented feature reduction was performed when choosing the
   optimal number of features for building the classification model.
   Moreover, hyperparameter tuning was explored for the optimal selection
   of random forest model. The results showed that the optimal random
   forest model with 30 important spectral features can achieve a mean
   correct classification rate of 1.0, 0.789, 0.691 and 0.752 for Zea mays,
   Convolvulus arvensis, Rumex and Cirsium arvense, respectively. The
   McNemar test showed an overall better performance of the optimal random
   forest model at the 0.05 significance level compared to the k-nearest
   neighbours (KNN) model. (C) 2018 lAgrE. Published by Elsevier Ltd. All
   rights reserved.},
Publisher = {ACADEMIC PRESS INC ELSEVIER SCIENCE},
Address = {525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA},
Type = {Article},
Language = {English},
Affiliation = {He, Y (Corresponding Author), Zhejiang Univ, Coll Biosyst Engn \& Food Sci, Yuhangtang Rd 866, Hangzhou 310058, Zhejiang, Peoples R China.
   Gao, Junfeng; Pieters, Jan G., Univ Ghent, Dept Biosyst Engn, Coupure Links 653, B-9000 Ghent, Belgium.
   Nuyttens, David, ILVO, Technol \& Food Sci Unit, Burg Van Gansberghelaan 115, B-9820 Merelbeke, Belgium.
   Lootens, Peter, ILVO, Plant Sci Unit, Caritasstr 39, B-9090 Melle, Belgium.
   He, Yong, Zhejiang Univ, Coll Biosyst Engn \& Food Sci, Yuhangtang Rd 866, Hangzhou 310058, Zhejiang, Peoples R China.},
DOI = {10.1016/j.biosystemseng.2018.03.006},
ISSN = {1537-5110},
EISSN = {1537-5129},
Keywords = {Snapshot hyperspectral imaging; Machine learning; Plant classification;
   Hyperparameter tuning; Feature selection; Cross validation},
Keywords-Plus = {CLASSIFICATION; DISCRIMINATION; FUTURE; ILLUMINATION; REFLECTANCE;
   TECHNOLOGY; MANAGEMENT; SELECTION; SENSOR; L.},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agricultural Engineering; Agriculture, Multidisciplinary},
Author-Email = {yhe@zju.edu.cn},
Affiliations = {Ghent University; Institute For Agricultural \& Fisheries Research;
   Institute For Agricultural \& Fisheries Research; Zhejiang University},
ResearcherID-Numbers = {Lootens, Peter/AAT-8456-2020
   Pieters, Jan/S-8048-2019
   },
ORCID-Numbers = {Pieters, Jan/0000-0003-4034-9269
   Gao, Junfeng/0000-0002-5622-8210
   Nuyttens, David/0000-0002-4531-0526},
Funding-Acknowledgement = {Special Research Fund (BOF) of the Ghent University {[}01SC3616];
   Technology and Food Science Unit of the Institute of Agriculture and
   Fisheries Research (ILVO), Belgium},
Funding-Text = {The study had been made possible by the Special Research Fund (BOF) of
   the Ghent University (No. 01SC3616). This study was also supported by
   the Technology and Food Science Unit of the Institute of Agriculture and
   Fisheries Research (ILVO), Belgium. The authors wish to exp ress their
   gratitude to Koen Mertens, Bernard De Baets, and Wouter Saeys, also to
   Filip De Brouwer for his assistances in the experiment. Besides, we also
   thank Yingwang Gao for his critical reading in this paper.},
Cited-References = {ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209.
   Behmann J, 2015, PRECIS AGRIC, V16, P239, DOI 10.1007/s11119-014-9372-7.
   Belgiu M, 2016, ISPRS J PHOTOGRAMM, V114, P24, DOI 10.1016/j.isprsjprs.2016.01.011.
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324.
   Chamundeeswari VV, 2009, IEEE GEOSCI REMOTE S, V6, P214, DOI 10.1109/LGRS.2008.2009954.
   Choi YS, 2009, PATTERN RECOGN LETT, V30, P1236, DOI 10.1016/j.patrec.2009.05.007.
   Dietterich TG, 1998, NEURAL COMPUT, V10, P1895, DOI 10.1162/089976698300017197.
   Dogan M. Nedim, 2004, Turkish Journal of Agriculture and Forestry, V28, P349.
   Everitt B. S, 1992, MONOGRAPHS STAT APPL, V45, DOI {[}10.1002/bimj.4710350708, DOI 10.1002/BIMJ.4710350708].
   Gao JF, 2018, INT J APPL EARTH OBS, V67, P43, DOI 10.1016/j.jag.2017.12.012.
   Gao JF, 2013, COMPUT ELECTRON AGR, V99, P186, DOI 10.1016/j.compag.2013.09.011.
   Garcia-Ruiz FJ, 2015, BIOSYST ENG, V139, P1, DOI 10.1016/j.biosystemseng.2015.07.012.
   Ghosh A, 2014, INT J APPL EARTH OBS, V26, P49, DOI 10.1016/j.jag.2013.05.017.
   Harker KN, 2013, WEED TECHNOL, V27, P1, DOI 10.1614/WT-D-12-00109.1.
   Hsu C., 2008, BJU INT, V101, P1396, DOI {[}http://dx.doi.org/10.1177/02632760022050997, DOI 10.1177/02632760022050997, 10.1177/02632760022050997].
   Ishida T, 2018, COMPUT ELECTRON AGR, V144, P80, DOI 10.1016/j.compag.2017.11.027.
   Jones G, 2009, PRECIS AGRIC, V10, P1, DOI 10.1007/s11119-008-9086-9.
   Juel A, 2015, INT J APPL EARTH OBS, V42, P106, DOI 10.1016/j.jag.2015.05.008.
   Jurado-Exposito M, 2003, CROP PROT, V22, P1177, DOI 10.1016/S0261-2194(03)00159-5.
   Kazmi W, 2015, COMPUT ELECTRON AGR, V118, P290, DOI 10.1016/j.compag.2015.08.023.
   Kazmi W, 2015, COMPUT ELECTRON AGR, V112, P10, DOI 10.1016/j.compag.2015.01.008.
   Loosvelt L, 2012, IEEE T GEOSCI REMOTE, V50, P4185, DOI 10.1109/TGRS.2012.2189012.
   Lopez-Granados F, 2011, WEED RES, V51, P1, DOI 10.1111/j.1365-3180.2010.00829.x.
   Lopez-Granados F, 2016, PRECIS AGRIC, V17, P183, DOI 10.1007/s11119-015-9415-8.
   Macias FA, 2000, J AGR FOOD CHEM, V48, P2512, DOI 10.1021/jf9903051.
   Meissle M, 2010, J APPL ENTOMOL, V134, P357, DOI 10.1111/j.1439-0418.2009.01491.x.
   Nitze I, 2015, INT J APPL EARTH OBS, V34, P136, DOI 10.1016/j.jag.2014.08.001.
   Oerke EC, 2006, J AGR SCI-CAMBRIDGE, V144, P31, DOI 10.1017/S0021859605005708.
   Oerke E. C., 1996, GERMAN PHYTOMEDICAL, V6, P63.
   Okamoto H, 2007, WEED BIOL MANAG, V7, P31, DOI 10.1111/j.1445-6664.2006.00234.x.
   Ostry V, 2015, ACTA VET BRNO, V84, P47, DOI 10.2754/avb201585010047.
   Pantazi XE, 2016, BIOSYST ENG, V146, P193, DOI 10.1016/j.biosystemseng.2016.01.014.
   Pearson R. L., 1972, P 8 INT S REM SENS E, VVIII, P1355, DOI DOI 10.1177/002076409904500102.
   Penuelas J, 1998, TRENDS PLANT SCI, V3, P151, DOI 10.1016/S1360-1385(98)01213-8.
   Peters J, 2007, ECOL MODEL, V207, P304, DOI 10.1016/j.ecolmodel.2007.05.011.
   Ray T. W., 1993, ASTROPHYSICAL JETS, P149.
   Romeo J, 2013, SENSORS-BASEL, V13, P4348, DOI 10.3390/s130404348.
   Shaner DL, 2014, PEST MANAG SCI, V70, P1329, DOI 10.1002/ps.3706.
   Shaw DR, 2005, FRONT ECOL ENVIRON, V3, P526, DOI 10.2307/3868608.
   Slaughter DC, 2008, COMPUT ELECTRON AGR, V61, P63, DOI 10.1016/j.compag.2007.05.008.
   Smith AM, 2003, WEED TECHNOL, V17, P811, DOI 10.1614/WT02-179.
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002.
   Stehman SV, 1997, REMOTE SENS ENVIRON, V62, P77, DOI 10.1016/S0034-4257(97)00083-7.
   Tang JL, 2016, COMPUT ELECTRON AGR, V122, P103, DOI 10.1016/j.compag.2015.12.016.
   Tardieu F, 2013, FRONT PHYSIOL, V4, DOI 10.3389/fphys.2013.00017.
   Tellaeche A, 2011, APPL SOFT COMPUT, V11, P908, DOI 10.1016/j.asoc.2010.01.011.
   Thenkabail PS, 2013, IEEE J-STARS, V6, P427, DOI 10.1109/JSTARS.2013.2252601.
   THOMPSON JF, 1991, CROP PROT, V10, P254, DOI 10.1016/0261-2194(91)90002-9.
   Torres-Sanchez J, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0058210.
   Vina A, 2011, REMOTE SENS ENVIRON, V115, P3468, DOI 10.1016/j.rse.2011.08.010.
   Vrindts E., 2002, Precision Agriculture, V3, P63, DOI 10.1023/A:1013326304427.
   Wendel A, 2017, ISPRS J PHOTOGRAMM, V129, P162, DOI 10.1016/j.isprsjprs.2017.04.010.
   Wendel A, 2016, IEEE INT CONF ROBOT, P5128, DOI 10.1109/ICRA.2016.7487717.
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9.
   Zhang Y, 2012, CROP PROT, V41, P96, DOI 10.1016/j.cropro.2012.05.007.
   Zhang Ze Pu, 2003, Weed Biology and Management, V3, P197, DOI 10.1046/j.1444-6162.2003.00105.x.},
Number-of-Cited-References = {56},
Times-Cited = {78},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {51},
Journal-ISO = {Biosyst. Eng.},
Doc-Delivery-Number = {GH7SM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000433653700005},
DA = {2023-08-12},
}

@article{ WOS:001008548900001,
Author = {Yudaputra, Angga and Yuswandi, Ade Yusuf and Witono, Joko Ridho and
   Cropper, Wendell P. and Usmadi, Didi},
Title = {Tree species identification in ex situ conservation areas using
   WorldView-2 Satellite Data and Machine Learning Methods: a case study in
   the Bogor Botanic Garden},
Journal = {TROPICAL ECOLOGY},
Year = {2023},
Month = {2023 JUN 19},
Abstract = {Spatially-explicit data on the species composition of forest plants can
   be an important tool for forest management and conservation. One
   specific application of these data is for identifying tropical tree
   species through machine learning techniques to classify satellite remote
   sensing images. This study aims to examine the ability to use
   Worldview-2 high-resolution data with various machine learning methods
   to identify tree species in the Bogor Botanic Garden. Eighteen species
   from 11 families were selected as samples representing an ecologically
   and taxonomically diverse data set. Using aggregated image variables,
   each tree species was found to have different reflectance, texture, and
   spectral vegetation index variable values. Cluster analysis showed that
   the 18 tree species could be separated into three clusters that partly
   reflected taxonomic relationships. Four machine learning algorithms
   (Support Vector Machine (SVM), Random Forest (RF), K-nearest neighbor
   (KNN), and Bayesian) were used to predict the species identity of pixels
   in the image data. A multicollinearity test using a Variance Inflation
   Factor method reduced the predictor variables from 54 to 9. The highest
   accuracy (0.96) was observed using SVM, followed by RF (0.91), KNN
   (0.86), and Bayesian (0.74). The implementation of high-resolution
   satellite imagery and machine learning for species identification in
   tropical ex situ plant conservation areas, such as botanic gardens is
   reported here for the first time.},
Publisher = {SPRINGERNATURE},
Address = {CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND},
Type = {Article; Early Access},
Language = {English},
Affiliation = {Usmadi, D (Corresponding Author), Natl Res \& Innovat Agcy, Res Ctr Ecol \& Ethnobiol, KST Soekarno Jalan Raya Jakarta-Bogor Km 46, Cibinong 16911, West Java, Indonesia.
   Yudaputra, Angga, Natl Res \& Innovat Agcy, Bot Gardens \& Forestry, Res Ctr Plant Conservat, KST Soekarno Jalan Raya Jakarta-Bogor Km 46, Cibinong 16911, West Java, Indonesia.
   Yuswandi, Ade Yusuf, Natl Res \& Innovat Agcy, Directorate Sci Collect Management, KST Soekarno Jalan Raya Jakarta-Bogor Km 46, Cibinong 16911, West Java, Indonesia.
   Witono, Joko Ridho, Natl Res \& Innovat Agcy, Res Ctr Biosystemat \& Evolut, KST Soekarno Jalan Raya Jakarta-Bogor Km 46, Cibinong 16911, West Java, Indonesia.
   Cropper, Wendell P., Univ Florida, Sch Forest Fisheries \& Geomat Sci, Gainesville, FL 32611 USA.
   Usmadi, Didi, Natl Res \& Innovat Agcy, Res Ctr Ecol \& Ethnobiol, KST Soekarno Jalan Raya Jakarta-Bogor Km 46, Cibinong 16911, West Java, Indonesia.},
DOI = {10.1007/s42965-023-00308-7},
EarlyAccessDate = {JUN 2023},
ISSN = {0564-3295},
EISSN = {2661-8982},
Keywords = {Classification; Conservation; Machine learning algorithm; Modeling
   ecology; Remote sensing; Tropical tree; Urban forest},
Keywords-Plus = {RANDOM FOREST CLASSIFIER; IMAGE CLASSIFICATION; NEURAL-NETWORKS; REMOTE;
   VARIABLES; ALGORITHMS; FUSION; INDEX},
Research-Areas = {Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Ecology},
Author-Email = {didi020@brin.go.id},
Affiliations = {State University System of Florida; University of Florida},
ResearcherID-Numbers = {Cropper, Wendell/E-5952-2010},
ORCID-Numbers = {Cropper, Wendell/0000-0001-7851-7382},
Cited-References = {{[}Anonymous], 2015, MISC FUNCTIONS DEP S.
   Ariati SR., 2019, ALPHABETICAL LIST PL.
   BPS-Statistics Indonesia, 2020, BOG MUN FIG 2020.
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324.
   Breiman L, 2012, R PACKAGE VERSION, V2.
   BUNTINE W, 1992, MACH LEARN, V8, P75, DOI 10.1023/A:1022686419106.
   Chatterjee S., 2006, REGRESSION ANAL EXAM, DOI DOI 10.1002/0470055464.
   Cho MA, 2015, INT J APPL EARTH OBS, V38, P349, DOI 10.1016/j.jag.2015.01.015.
   Deur M, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12233926.
   Digitalglobe, 2010, BEN 8 SPECTR BANDS W.
   Dormann CF, 2013, ECOGRAPHY, V36, P27, DOI 10.1111/j.1600-0587.2012.07348.x.
   Du PJ, 2015, ISPRS J PHOTOGRAMM, V105, P38, DOI 10.1016/j.isprsjprs.2015.03.002.
   Duro DC, 2012, REMOTE SENS ENVIRON, V118, P259, DOI 10.1016/j.rse.2011.11.020.
   ESA-European Space Agency, 2020, SENT 2 TOOLB.
   Fasbender D, 2008, IEEE T GEOSCI REMOTE, V46, P1847, DOI 10.1109/TGRS.2008.917131.
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013.
   Foody GM, 2004, INT J REMOTE SENS, V25, P3091, DOI 10.1080/01431160310001648019.
   Gamon JA, 1999, NEW PHYTOL, V143, P105, DOI 10.1046/j.1469-8137.1999.00424.x.
   Gitelson AA, 2002, REMOTE SENS ENVIRON, V80, P76, DOI 10.1016/S0034-4257(01)00289-9.
   Gitelson AA, 1996, J PLANT PHYSIOL, V148, P494, DOI 10.1016/S0176-1617(96)80284-7.
   Haider N., 2018, Journal of Plant Science Research, V34, P277, DOI 10.32381/JPSR.2018.34.02.17.
   Hammer Oyvind, 2001, Palaeontologia Electronica, V4, pUnpaginated.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Hartling S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061284.
   HUETE A R, 1988, Remote Sensing of Environment, V25, P295.
   Immitzer M, 2012, REMOTE SENS-BASEL, V4, P2661, DOI 10.3390/rs4092661.
   Jaya INS., 2003, J MANAJ HUTAN TROPIK, V9, P1.
   Jaya INS., 2002, JURNAL MANAJERNEN HU, V8, P57.
   JORDAN CF, 1969, ECOLOGY, V50, P663, DOI 10.2307/1936256.
   Jye KS, 2017, FRONT LIFE SCI, V10, P98, DOI 10.1080/21553769.2017.1412361.
   Kaartinen H, 2012, REMOTE SENS-BASEL, V4, P950, DOI 10.3390/rs4040950.
   Karlson M, 2016, INT J APPL EARTH OBS, V50, P80, DOI 10.1016/j.jag.2016.03.004.
   Kayitakire F, 2006, REMOTE SENS ENVIRON, V102, P390, DOI 10.1016/j.rse.2006.02.022.
   Kimes DS, 1998, INT J REMOTE SENS, V19, P2639, DOI 10.1080/014311698214433.
   Li J, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0183250.
   Liu P, 2017, SOFT COMPUT, V21, P7053, DOI 10.1007/s00500-016-2247-2.
   Majka Michal, 2020, CRAN.
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865.
   Meng QM, 2007, GISCI REMOTE SENS, V44, P149, DOI 10.2747/1548-1603.44.2.149.
   Mohammed NN, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INTERNET OF THINGS (ITHINGS) AND IEEE GREEN COMPUTING AND COMMUNICATIONS (GREENCOM) AND IEEE CYBER, PHYSICAL AND SOCIAL COMPUTING (CPSCOM) AND IEEE SMART DATA (SMARTDATA), P1011, DOI 10.1109/iThings-GreenCom-CPSCom-SmartData.2017.155.
   Mohibullah M., 2015, INT J COMPUT SCI INF, V13, P61.
   Naimi B, 2014, ECOGRAPHY, V37, P191, DOI 10.1111/j.1600-0587.2013.00205.x.
   Nguyen QH, 2021, MATH PROBL ENG, V2021, DOI 10.1155/2021/4832864.
   Ollinger SV, 2011, NEW PHYTOL, V189, P375, DOI 10.1111/j.1469-8137.2010.03536.x.
   Pal M, 2005, INT J REMOTE SENS, V26, P217, DOI 10.1080/01431160412331269698.
   Pal M, 2005, INT J REMOTE SENS, V26, P1007, DOI 10.1080/01431160512331314083.
   Peerbhay KY, 2014, IEEE J-STARS, V7, P307, DOI 10.1109/JSTARS.2013.2262634.
   PENUELAS J, 1995, PHOTOSYNTHETICA, V31, P221.
   Ripley B. D., 1996, PATTERN RECOGNITION.
   Rodriguez-Galiano VF, 2012, ISPRS J PHOTOGRAMM, V67, P93, DOI 10.1016/j.isprsjprs.2011.11.002.
   Rouse J.W., 1974, P 3 EARTH RESOURCES.
   Russell S.J., 2016, ARTIF INTELL.
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE.
   Sinwar D., 2014, INT J RES APPL SCI E, V2, P270.
   Tarantino C., 2012, EARTH OBSERVATION.
   Tomppo E, 2004, REMOTE SENS ENVIRON, V92, P1, DOI 10.1016/j.rse.2004.04.003.
   Updike T., 2010, RADIOMETRIC USE WORL.
   Verlic A, 2014, SUMAR LIST, V138, P477.
   Viera AJ, 2005, FAM MED, V37, P360.
   Zhang HKK, 2015, REMOTE SENS-BASEL, V7, P6828, DOI 10.3390/rs70606828.
   Zhou JJ, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07951-w.},
Number-of-Cited-References = {61},
Times-Cited = {0},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Trop. Ecol.},
Doc-Delivery-Number = {J3HH4},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:001008548900001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000270543000067,
Author = {Du, Minggang and Zhang, Shanwen and Wang, Hong},
Editor = {Huang, DS and Jo, KH and Lee, HH and Bevilacqua, V and Kang, HJ},
Title = {Supervised Isomap for Plant Leaf Image Classification},
Booktitle = {EMERGING INTELLIGENT COMPUTING TECHNOLOGY AND APPLICATIONS: WITH ASPECTS
   OF ARTIFICIAL INTELLIGENCE},
Series = {Lecture Notes in Artificial Intelligence},
Year = {2009},
Volume = {5755},
Pages = {627+},
Note = {5th International Conference on Intelligent Computing, Ulsan, SOUTH
   KOREA, SEP 16-19, 2009},
Organization = {IEEE Computat Intelligence Soc; Int Neural Network Soc; Natl Nat Sci Fdn
   China},
Abstract = {Plant classification is. very important and necessary with respect to
   agricultural information, ecological protection and plant automatic
   classification system. Compared with other methods, Such as cell and
   molecule biology methods, classification based on leaf image is the
   first choice for plant classification. Plant recognition and
   classification is a complex and difficult problem, and is very important
   in Computer-Aided Plant Species Identification technology. The feature
   extraction is a key step to plant classification. This paper presents a
   method to extract discriminant features for plant leaf images by using
   Supervised Isomap. Experiments on the leaf image dataset have been
   performed. Experimental results show that the supervised Isomap is very
   effective and feasible.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Du, MG (Corresponding Author), Shanxi Normal Univ, Linfen 041004, Peoples R China.
   Du, Minggang; Wang, Hong, Shanxi Normal Univ, Linfen 041004, Peoples R China.
   Zhang, Shanwen, Zhongyuan Univ Technol, Fac Sci, Zhengzhou 450007, Peoples R China.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-642-04019-1},
Keywords = {Plant classification; Isomap; Supervised Isomap; Plant leaf image;
   K-nearest neighbor},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {wangh@sxnu.edu.cn},
Affiliations = {Shanxi Normal University; Zhongyuan University of Technology},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}60805021]; National
   Basic Research Program of China(973 Program) {[}2007CB311002]},
Funding-Text = {This work was supported by the grant of the National Natural Science
   Foundation of China, No. 60805021, the grant from the National Basic
   Research Program of China (973 Program), No.2007CB311002.},
Cited-References = {AGAZZI OE, 1993, PATTERN RECOGN, V26, P1813, DOI 10.1016/0031-3203(93)90178-Y.
   {[}Anonymous], 1996, PROC 1 INT WORKSHOP.
   {[}Anonymous], 1996, SYSTEMATIC THEORY NE.
   Bebis G, 1999, PATTERN RECOGN, V32, P1175, DOI 10.1016/S0031-3203(98)00159-9.
   DIETTERICH TG, 2002, ADV NEURAL INFORM PR, V14, P585.
   Fuh CS, 1998, IMAGE VISION COMPUT, V16, P677, DOI 10.1016/S0262-8856(98)00080-8.
   Gu X, 2005, LECT NOTES COMPUT SC, V3644, P253.
   HE Y, 1991, IEEE T PATTERN ANAL, V13, P1172, DOI 10.1109/34.103276.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   LANDRAUD AM, 1991, COMPUTER VISION GRAP, V3, P85.
   Loncaric S, 1998, PATTERN RECOGN, V31, P983, DOI 10.1016/S0031-2023(97)00122-2.
   MCCOLLUM AJ, 1988, COMPUTER VISION GRAP, V12, P337.
   Nishida H, 1998, PATTERN RECOGN, V31, P1557, DOI 10.1016/S0031-3203(98)00005-3.
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323.
   Saitoh T, 2000, INT C PATT RECOG, P507, DOI 10.1109/ICPR.2000.906123.
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319.
   Wang XF, 2005, LECT NOTES COMPUT SC, V3644, P87.
   Wang ZY, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOL 1 \& 2, P372, DOI 10.1109/FUZZ.2002.1005019.
   WU SG, 2007, ARXIV07074289.
   Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008.},
Number-of-Cited-References = {20},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BLM65},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000270543000067},
DA = {2023-08-12},
}

@article{ WOS:000781941600005,
Author = {Goyal, Neha and Kumar, Nitin and Kapil},
Title = {Leaf Bagging: A novel meta heuristic optimization based framework for
   leaf identification},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2022},
Volume = {81},
Number = {22},
Pages = {32243-32264},
Month = {SEP},
Abstract = {Automated plant recognition based on leaf images is a challenging task
   among the researchers from several fields. This task requires
   distinguishing features derived from leaf images for assigning class
   label to a leaf image. There are several methods in literature for
   extracting such distinguishing features. In this paper, we propose a
   novel automated framework for leaf identification. The proposed
   framework works in multiple phases i.e. pre-processing, feature
   extraction, classification using bagging approach. Initially, leaf
   images are pre-processed using image processing operations such as
   boundary extraction and cropping. In the feature extraction phase,
   popular nature inspired optimization algorithms viz. Spider Monkey
   Optimization (SMO), Particle Swarm Optimization (PSO) and Gray Wolf
   Optimization (GWO) have been exploited for reducing the dimensionality
   of features. In the last phase, a leaf image is classified by multiple
   classifiers and then output of these classifiers is combined using
   majority voting. The effectiveness of the proposed framework is
   established based on the experimental results obtained on three datasets
   i.e. Flavia, Swedish and self-collected leaf images. On all the
   datasets, it has been observed that the classification accuracy of the
   proposed method is better than the individual classifiers. Furthermore,
   the classification accuracy for the proposed approach is comparable to
   deep learning based method on the Flavia dataset.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Goyal, N (Corresponding Author), Natl Inst Technol, Kurukshetra, Haryana, India.
   Goyal, Neha; Kumar, Nitin; Kapil, Natl Inst Technol, Kurukshetra, Haryana, India.},
DOI = {10.1007/s11042-022-12825-z},
EarlyAccessDate = {APR 2022},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Leaf classification; Feature selection; Nature-based optimization;
   Ensemble approach; Bootstrap aggregating},
Keywords-Plus = {SHAPE-FEATURES; PLANT; CLASSIFICATION; TEXTURE; MACHINE},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {neha.goyal2309@gmail.com},
Affiliations = {National Institute of Technology (NIT System); National Institute of
   Technology Kurukshetra},
ResearcherID-Numbers = {Aggarwal, Kapil/ACA-2420-2022
   },
ORCID-Numbers = {Aggarwal, Kapil/0000-0001-7658-5058
   Goyal, Neha/0000-0002-7016-4663},
Funding-Acknowledgement = {University Grant Commission, India},
Funding-Text = {We acknowledge University Grant Commission, India for providing research
   fellowship to one of the author Ms. Neha Goyal},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Bansal JC, 2014, MEMET COMPUT, V6, P31, DOI 10.1007/s12293-013-0128-0.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Bodhwani Vinit, 2019, Procedia Computer Science, V152, P186, DOI 10.1016/j.procs.2019.05.042.
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Choudhary MK, 2021, P INT C COMM COMP TE, P657.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047\_33.
   Das Choudhury S, 2018, BIOSYST ENG, V170, P72, DOI 10.1016/j.biosystemseng.2018.04.001.
   Freund Y., 1996, P 13 INT C MACH LEAR, V96, P148, DOI DOI 10.5555/3091696.3091715.
   Gonzalez R. C., 2018, DIGITAL IMAGE PROCES.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Goyal N, 2021, SIVIP, P1.
   Goyal N, 2021, MULTIMED TOOLS APPL, V80, P4533, DOI 10.1007/s11042-020-09899-y.
   Goyal N, 2019, MULTIMED TOOLS APPL, V78, P27785, DOI 10.1007/s11042-019-7588-2.
   Hall M. A., 1999, Proceedings of the Twelfth International Florida AI Research Society Conference, P235.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Hsiao JK, 2014, 2014 SCIENCE AND INFORMATION CONFERENCE (SAI), P389, DOI 10.1109/SAI.2014.6918216.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Kennedy J., 1995, 1995 IEEE International Conference on Neural Networks Proceedings (Cat. No.95CH35828), P1942, DOI 10.1109/ICNN.1995.488968.
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968.
   Kumar S, 2020, SUSTAIN COMPUT-INFOR, V28.
   LAWS KI, 1979, P IM UND WORKSH, P47, DOI DOI 10.111141600-0846.2009.00354.X.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lin CH, 2012, IET IMAGE PROCESS, V6, P822, DOI 10.1049/iet-ipr.2011.0445.
   Lofstedt T, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0212110.
   Mirjalili S, 2014, ADV ENG SOFTW, V69, P46, DOI 10.1016/j.advengsoft.2013.12.007.
   Pankaja K., 2020, Journal of the Institution of Engineers (India): Series B (Electrical, Electronics \& Telecommunication and Computer Engineering), V101, P597, DOI 10.1007/s40031-020-00470-9.
   Patel Bharati, 2020, 2020 First International Conference on Power, Control and Computing Technologies (ICPC2T), P267, DOI 10.1109/ICPC2T48082.2020.9071511.
   Patel B, 2019, ADVANCES IN BIOMETRICS: MODERN METHODS AND IMPLEMENTATION STRATEGIES, P211, DOI 10.1007/978-3-030-30436-2\_10.
   Patil Sumedh, 2021, Proceedings of the 5th International Conference on Trends in Electronics and Informatics (ICOEI 2021), P1138, DOI 10.1109/ICOEI51242.2021.9453003.
   Pearline SA, 2019, J INTELL FUZZY SYST, V36, P1997, DOI 10.3233/JIFS-169911.
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   Schapire RE, 2012, ADAPT COMPUT MACH LE, P1.
   Sharaff A, 2021, DATA DRIVEN APPROACH, P49.
   SHEARER SA, 1990, T ASAE, V33, P2037.
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Sulc M, 2016, VERY DEEP RESIDUAL N.
   SUN Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI DOI 10.1155/2017/7361042.
   Tamura H., 1978, IEEE Transactions on Systems, Man and Cybernetics, VSMC-8, P460, DOI 10.1109/TSMC.1978.4309999.
   Too JW, 2021, EVOL INTELL, V14, P1691, DOI 10.1007/s12065-020-00441-5.
   Too J, 2018, COMPUTERS, V7, DOI 10.3390/computers7040058.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yang KL, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10111721.
   Zhao ZQ, 2015, NEUROCOMPUTING, V151, P1112, DOI 10.1016/j.neucom.2014.02.077.},
Number-of-Cited-References = {51},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {3X7GP},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000781941600005},
DA = {2023-08-12},
}

@inproceedings{ WOS:000628838900033,
Author = {Safar, Amna and Safar, Maytham},
Editor = {Bi, Y and Bhatia, R and Kapoor, S},
Title = {Intelligent Flower Detection System Using Machine Learning},
Booktitle = {INTELLIGENT SYSTEMS AND APPLICATIONS, VOL 2},
Series = {Advances in Intelligent Systems and Computing},
Year = {2020},
Volume = {1038},
Pages = {463-472},
Note = {Intelligent Systems Conference (IntelliSys), London, ENGLAND, SEP 05-06,
   2019},
Abstract = {It is a very hard and a challenging mission to identify different types
   of flowers as they are very similar. Even expert botanists and gardeners
   cannot identify some of them accurately. The idea of automating flowers
   recognition is bewildering as the flowers are not rigid objects and
   their images can be affected by many External influences. The proposed
   system use machine learning algorithms to fully automate and increase
   the accuracy of flower classification. Machine learning model will be
   used to extract flower's features automatically, process through
   different layers of the neural network and finally classify the flower
   class. The proposed work is based on ``Resnet{''} model, which is used
   for classification task. Resnet won the first place on ILSVRC 2015. Many
   enhancements have been made on Resnet model to improve the accuracy.
   Fine tuning, dropout ratio and class weight are some of the proposed
   model enhancements. The proposed model reaches 92\% accuracy, which is
   the highest percent till the moment.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Safar, A (Corresponding Author), Kuwait Univ, Kuwait, Kuwait.
   Safar, Amna; Safar, Maytham, Kuwait Univ, Kuwait, Kuwait.},
DOI = {10.1007/978-3-030-29513-4\_33},
ISSN = {2194-5357},
EISSN = {2194-5365},
ISBN = {978-3-030-29513-4; 978-3-030-29512-7},
Keywords = {Machine learning; Flower classification; ResNet model},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {eng.safar@gmail.com
   Maytham.safar@ku.edu.kw},
Affiliations = {Kuwait University},
ResearcherID-Numbers = {Safar, Maytham H/E-9238-2010},
ORCID-Numbers = {Safar, Maytham H/0000-0002-0007-3141},
Funding-Acknowledgement = {Kuwait Foundation for the Advancement of Sciences (KFAS); College of
   Graduate Studies, Kuwait University},
Funding-Text = {This research was supported by Kuwait Foundation for the Advancement of
   Sciences (KFAS) and College of Graduate Studies, Kuwait University.
   Special thanks to Prof. Maytham Safar who provided insight and expertise
   that greatly assisted the research. We would also like to thank the
   experts who were involved in the validation for this research project.},
Cited-References = {Albadarneh A, 2017, INT J COMPUT SCI NET, V17, P144.
   Angelova A, 2012, DEV DEPLOYMENT LARGE.
   Aurelien G., 2017, HANDS ON MACHINE LEA.
   Bergstra J, 2012, J MACH LEARN RES, V13, P281.
   Biradar B.V., 2015, INT J COMPUT SCI INF, V6, P2498.
   Crepet WL, 2000, P NATL ACAD SCI USA, V97, P12939, DOI 10.1073/pnas.97.24.12939.
   Gurnani A, 2017, ARXIV170803763.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Nagato T, 2017, FUJITSU SCI TECH J, V53, P52.
   Nilsback M.E., 2006, P 2006 IEEE COMPUTER, VVolume 2, P1447, DOI DOI 10.1109/CVPR.2006.42.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Saitoh T, 2004, INT C PATT RECOG, P27, DOI 10.1109/ICPR.2004.1333997.},
Number-of-Cited-References = {12},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BR0ER},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000628838900033},
DA = {2023-08-12},
}

@inproceedings{ WOS:000501609100009,
Author = {Selda, Jesse Dave S. and Ellera, Roi Martin R. and Cajayon, II, Leandro
   C. and Linsangan, Noel B.},
Book-Group-Author = {ACM},
Title = {Plant Identification by Image Processing of Leaf Veins},
Booktitle = {PROCEEDINGS OF 2017 INTERNATIONAL CONFERENCE ON IMAGING, SIGNAL
   PROCESSING AND COMMUNICATION},
Year = {2015},
Pages = {40-44},
Note = {International Conference on Imaging, Signal Processing and Communication
   (ICISPC), Penang, MALAYSIA, JUL 26-28, 2017},
Abstract = {This study focuses on building a portable device capable of plant
   identification by image processing of leaf veins using Raspberry pi. The
   devise that the study will develop can help professionals in the field
   of Botany and Biology. It can be used by Pathologist, plant breeders and
   Doctors that specializes on medicinal plants. This study will
   concentrate on identifying the plant using its leaf veins. Its
   concentration is on simple leaf type. Twenty different kinds of plant
   leaves will be tested with 10 trials per leaf. Each leaf image will have
   its own labeled folder in the database that is created automatically
   after registration of leaf image. Series of algorithms were used for the
   leaf recognition method. Scale-Invariant Feature Transform (SIFT)
   extraction will be used to describe and detect the local features of the
   recognized leaf vein image. With the use of Support Vectors Machines
   (SVM), the matrix produced by the SIFT will be used to classify the
   correct plant to be shown on the Liquid Crystal Display (LCD) screen as
   the output containing the plant name, description and image. Initial
   results showed accuracy rate of 84.29\% while the error rate was
   15.71\%.},
Publisher = {ASSOC COMPUTING MACHINERY},
Address = {1515 BROADWAY, NEW YORK, NY 10036-9998 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Linsangan, NB (Corresponding Author), Mapua Univ, Sch EECE, Manila, Philippines.
   Selda, Jesse Dave S.; Ellera, Roi Martin R.; Cajayon, Leandro C., II; Linsangan, Noel B., Mapua Univ, Sch EECE, Manila, Philippines.},
DOI = {10.1145/3132300.3132315},
ISBN = {978-1-4503-5289-5},
Keywords = {Leaf Veins; Python; Simple Leaf; SIFT; SVM},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \&
   Electronic},
Author-Email = {nblinsangan@mapua.edu.ph},
Affiliations = {Mapua University},
Cited-References = {{[}Anonymous], 2005, MONGABAY WEBSITE ENV.
   {[}Anonymous], 2008, CLEAN TECHNICA WEBSI.
   {[}Anonymous], 2004, DISTINCTIVE IMAGE FE.
   Chapelle Olivier, 1998, IEEE T NEURAL NETWOR.
   Cope J.S, 2011, PLANT SPECIES IDENTI.
   Lee K. B, 2013, IMPLEMENTATION LEAF.
   Pahalawatta K.K, 2008, THESIS U CANTERBURY.
   Scovanner Paul, 2007, 3 DIMENSIONAL SIFT D.
   Shilpashree K. S, 2015, IMPLEMENTATION IMAGE.
   VAPNIK V, 1995, SUPPORT VECTOR NETWO.},
Number-of-Cited-References = {10},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BO1OE},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000501609100009},
DA = {2023-08-12},
}

@article{ WOS:000431531900002,
Author = {Hu, Jing and Chen, Zhibo and Yang, Meng and Zhang, Rongguo and Cui, Yaji},
Title = {A Multiscale Fusion Convolutional Neural Network for Plant Leaf
   Recognition},
Journal = {IEEE SIGNAL PROCESSING LETTERS},
Year = {2018},
Volume = {25},
Number = {6},
Pages = {853-857},
Month = {JUN},
Abstract = {Plant leaf recognition is a computer vision task used to automatically
   recognize plant species. It is very challenging since rich plant leaf
   morphological variations, such as sizes, textures, shapes, venation, and
   so on. Most existing plant leaf methods typically normalize all plant
   leaf images to the same size and recognize them at one scale, resulting
   in unsatisfactory performances. In this letter, a multiscale fusion
   convolutional neural network (MSF-CNN) is proposed for plant leaf
   recognition at multiple scales. First, an input image is down-sampled
   into multiples low resolution images with a list of bilinear
   interpolation operations. Then, these input images with different scales
   are step-by-step fed into the MSF-CNN architecture to learn
   discriminative features at different depths. At this stage, the feature
   fusion between two different scales is realized by a concatenation
   operation, which concatenates feature maps learned on different scale
   images from a channel view. Along with the depth of the MSF-CNN,
   multiscale images are progressively handled and the corresponding
   features are fused. Third, the last layer of the MSF-CNN aggregates all
   discriminative information to obtain the final feature for predicting
   the plant species of the input image. Experiments show the proposed
   MSF-CNN method is superior to multiple state-of-the art plant leaf
   recognition methods on the MalayaKew Leaf dataset and the LeafSnap Plant
   Leaf dataset.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Chen, ZB (Corresponding Author), Beijing Forestry Univ, Sch Informat Sci \& Technol, Beijing 100083, Peoples R China.
   Hu, Jing; Chen, Zhibo; Yang, Meng, Beijing Forestry Univ, Sch Informat Sci \& Technol, Beijing 100083, Peoples R China.
   Hu, Jing; Zhang, Rongguo, Taiyuan Univ Sci \& Technol, Coll Comp Sci \& Technol, Taiyuan 030024, Shanxi, Peoples R China.
   Cui, Yaji, Beijing Univ Posts \& Telecommun, Int Sch, Beijing 100876, Peoples R China.},
DOI = {10.1109/LSP.2018.2809688},
ISSN = {1070-9908},
EISSN = {1558-2361},
Keywords = {Multiscale convolutional neural network (MSCNN); multiscale feature;
   plant leaf recognition},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {279641292@qq.com
   zhibo@bjfu.edu.cn
   yangmeng@bjfu.edu.cn
   rg-zh@163.com
   13683290326@163.com},
Affiliations = {Beijing Forestry University; Taiyuan University of Science \&
   Technology; Beijing University of Posts \& Telecommunications},
ORCID-Numbers = {Hu, Jing/0000-0003-0470-2352},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}61402038]},
Funding-Text = {This work was supported by the National Natural Science Foundation of
   China under the Grant 61402038. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof. Wei
   Li. (Corresponding author: Zhibo Chen.)},
Cited-References = {Charters J., 2014, 2014 IEEE INT C MULT, P1, DOI {[}10.1109/ICMEW.2014.6890557, DOI 10.1109/ICMEW.2014.6890557].
   Clarke J, 2006, LECT NOTES COMPUT SC, V4292, P427.
   Cope J. S., 2012, CLASSIFYING PLANT LE.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Du CB, 2017, IEEE ACCESS, V5, P15750, DOI 10.1109/ACCESS.2017.2735019.
   Gray D., 2007, P IEEE INT WORKSH PE, V3, P1.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Liu Z., 2015, HYBRID DEEP LEARNING.
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
   Mouine S., 2012, ICMR, P1, DOI DOI 10.1145/2324796.2324853.
   Nair V., 2010, ICML 10 PROC 27 INT.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Rasti R., IEEE T MED IMAG.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757.
   Zhu J., 2014, DEEP HYBRID SIMILARI.},
Number-of-Cited-References = {28},
Times-Cited = {61},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {66},
Journal-ISO = {IEEE Signal Process. Lett.},
Doc-Delivery-Number = {GE9ED},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000431531900002},
DA = {2023-08-12},
}

@inproceedings{ WOS:000391252300046,
Author = {Yalcin, Hulya and Razavi, Salar},
Book-Group-Author = {IEEE},
Title = {Plant Classification using Convolutional Neural Networks},
Booktitle = {2016 FIFTH INTERNATIONAL CONFERENCE ON AGRO-GEOINFORMATICS
   (AGRO-GEOINFORMATICS)},
Series = {International Conference on Agro-Geoinformatics},
Year = {2016},
Pages = {233-237},
Note = {5th International Conference on Agro-Geoinformatics
   (Agro-Geoinformatics), Inst Agr Resources \& Regional Planning, Chinese
   Acad Agr Sci, Tianjin, PEOPLES R CHINA, JUL 18-20, 2016},
Organization = {Tianjin Polytechn Univ; George Mason Univ, Ctr Spatial Informat Sci \&
   Syst; CSISS Fdn Inc; Open Geospatial Consortium; Turkish Minist Agr;
   TARBIL Agr Informat Appl Res Ctr; Istanbul Tech Univ; Chinese Acad Agr
   Sci; Chinese Soc Agr Resources \& Regional Planning; Chinese Assoc Agr
   Sci Soc; Wuhan Univ; IEEE Geoscience \& Remote Sensing Soc; State Adm
   Foreign Experts Affairs China; Minist Agr; Natl Nat Sci Fdn China},
Abstract = {Application of the benefits of modern computing technology to improve
   the efficiency of agricultural fields is inevitable with growing
   concerns about increasing world population and limited food resources.
   Computing technology is crucial not only to industries related to food
   production but also to environmentalists and other related authorities.
   It is expected to increase the productivity, contribute to a better
   understanding of the relationship between environmental factors and
   healthy crops, reduce the labor costs for farmers and increase the
   operation speed and accuracy. Implementing machine learning methods such
   as deep neural networks on agricultural data has gained immense
   attention in recent years. One of the most important problems is
   automatic classification of plant species based on their types.
   Automatic plant type identification process could offer a great help for
   application of pesticides, fertilization and harvesting of different
   species on-time in order to improve the production processes of food and
   drug industries. In this paper, we propose a Convolutional Neural
   Network (CNN) architecture to classify the type of plants from the image
   sequences collected from smart agro-stations. First challenges
   introduced by illumination changes and deblurring are eliminated with
   some preprocessing steps. Following the preprocessing step,
   Convolutional Neural Network architecture is employed to extract the
   features of images. The construction of the CNN architecture and the
   depth of CNN are crucial points that should be emphasized since they
   affect the recognition capability of the architecture of neural
   networks. In order to evaluate the performance of the approach proposed
   in this paper, the results obtained through CNN model are compared with
   those obtained by employing SVM classifier with different kernels, as
   well as feature descriptors such as LBP and GIST. The performance of the
   approach is tested on dataset collected through a government supported
   project, TARBIL, for which over 1200 agro-stations are placed throughout
   Turkey. The experimental results on TARBIL dataset confirm that the
   proposed method is quite effective.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Yalcin, H (Corresponding Author), Istanbul Tech Univ, Dept Elect \& Telecommun Engn, Visual Intelligence Lab, Istanbul, Turkey.
   Yalcin, Hulya; Razavi, Salar, Istanbul Tech Univ, Dept Elect \& Telecommun Engn, Visual Intelligence Lab, Istanbul, Turkey.},
ISSN = {2334-3168},
Keywords = {agriculture; plant classification; convolutional neural networks; deep
   learning; computer vision},
Keywords-Plus = {PHENOLOGY; MANAGEMENT},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Information Systems},
Author-Email = {hulyayalcin@itu.edu.tr
   razavi15@itu.edu.tr},
Affiliations = {Istanbul Technical University},
Cited-References = {{[}Anonymous], 2013, INT J ENG SCI TECHNO.
   Bagis S, 2012, INT C AGR, P1.
   Bodhe S., 2012, INT J COMPUT APPL, V52.
   Broich M, 2015, ENVIRON MODELL SOFTW, V64, P191, DOI 10.1016/j.envsoft.2014.11.017.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Cleland EE, 2007, TRENDS ECOL EVOL, V22, P357, DOI 10.1016/j.tree.2007.04.003.
   Coleman A. M., 2008, ADAPTIVE LANDSCAPE C.
   Dey D., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P329, DOI 10.1109/WACV.2012.6163017.
   Gaber T, 2015, ADV INTELL SYST COMP, V368, P375, DOI 10.1007/978-3-319-19719-7\_33.
   Jassmann T.J., 2015, P SOUTHEASTCON 2015, P1, DOI {[}10.1109/SECON.2015.7132978, DOI 10.1109/SECON.2015.7132978].
   Oberthur T, 2007, COMPUT ELECTRON AGR, V58, P60, DOI 10.1016/j.compag.2006.08.005.
   Razavi S., 2016, SIGN PROC COMM APPL, DOI {[}10.1109/SIU.2016.7496150, DOI 10.1109/SIU.2016.7496150].
   Richardson AD, 2013, AGR FOREST METEOROL, V169, P156, DOI 10.1016/j.agrformet.2012.09.012.
   Srbinovska M, 2015, J CLEAN PROD, V88, P297, DOI 10.1016/j.jclepro.2014.04.036.
   Sunderhauf Niko, 2014, P CLEF WORK NOT, P756.
   Weiss U, 2011, ROBOT AUTON SYST, V59, P265, DOI 10.1016/j.robot.2011.02.011.
   Xiao Xiao, 2012, Proceedings of the 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE), P373.
   Yalcin Hulya, 2015, 2015 Fourth International Conference on Agro-Geoinformatics (Agro-Geoinformatics), P338, DOI 10.1109/Agro-Geoinformatics.2015.7248114.
   Yalcin H., 2016, SIGN PROC COMM APPL, DOI {[}10.1109/SIU.2016.7495926, DOI 10.1109/SIU.2016.7495926].
   Zhiyu Liu, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9226, P115, DOI 10.1007/978-3-319-22186-1\_11.
   Zhou XD, 2009, INT J ORAL SCI, V1, P1.},
Number-of-Cited-References = {22},
Times-Cited = {28},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {10},
Doc-Delivery-Number = {BG7FE},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000391252300046},
DA = {2023-08-12},
}

@inproceedings{ WOS:000377404100117,
Author = {Schur, Amir and Tappert, Charles C.},
Editor = {Stephanidis, C},
Title = {Employing Mobile Applications in Human-Machine Interaction in Visual
   Pattern Recognition Research},
Booktitle = {HCI INTERNATIONAL 2015 - POSTERS' EXTENDED ABSTRACTS, PT I},
Series = {Communications in Computer and Information Science},
Year = {2015},
Volume = {528},
Pages = {696-699},
Note = {2nd International Conference on Learning and Collaboration Technologies
   / 17th International Conference on Human-Computer Interaction, Los
   Angeles, CA, AUG 02-07, 2015},
Abstract = {This study is part of the first author's continued dissertation research
   in human-machine interaction in visual pattern recognition. Previous
   research focused on evaluating human-machine interaction using a flower
   recognition tool. Initial research showed that human interaction in
   color recognition improved accuracy significantly. We then looked more
   deeply into various automated color recognition algorithms and ways of
   combining them with human feedback. Described here is the process of
   upgrading the initial system into a new mobile application using
   Appinventor. After data collection, models were built for various color
   spaces. Sharing this experience may help other researchers incorporating
   a human-computer interaction component into their work.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Schur, A (Corresponding Author), Pace Univ, Seidenberg Sch CSIS, 1 Martine Ave, White Plains, NY 10606 USA.
   Schur, Amir; Tappert, Charles C., Pace Univ, Seidenberg Sch CSIS, 1 Martine Ave, White Plains, NY 10606 USA.},
DOI = {10.1007/978-3-319-21380-4\_117},
ISSN = {1865-0929},
ISBN = {978-3-319-21380-4; 978-3-319-21379-8},
Keywords = {Human-computer interaction; Visual object recognition; Pattern
   classification; Feature extraction; Appinventor; Color space},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications; Computer Science,
   Theory \& Methods},
Author-Email = {amirschur@aol.com
   ctappert@pace.edu},
Affiliations = {Pace University},
Cited-References = {Schur A., 2014, CCIS 1, V434, P368.
   Ugarriza LG, 2009, IEEE T IMAGE PROCESS, V18, P2275, DOI 10.1109/TIP.2009.2025555.},
Number-of-Cited-References = {2},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BE9FM},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000377404100117},
OA = {Bronze},
DA = {2023-08-12},
}

@inproceedings{ WOS:000618443400122,
Author = {bin Mohd, Othman and Rahim, Mohd Rasyid bin A. and bin Khalik, Mohd
   Haffez},
Editor = {Rahman, AAA and Abid, MAAM and Mohamad, N and Juoi, JM and Fauadi, MHFM and AbdManaf, ME and Abdullah, Z and Sued, MK and Yee, CS and Rahim, TA and Suan, MSM and Anand, TJS},
Title = {Plant Species Identification Based on Otsu's and Edge Detection
   Techniques},
Booktitle = {PROCEEDINGS OF INNOVATIVE RESEARCH AND INDUSTRIAL DIALOGUE 2018
   (IRID'18)},
Year = {2019},
Pages = {244-245},
Note = {Conference on Innovative Research and Industrial Dialogue (IRID), Univ
   Teknikal Malaysia Melaka, Melaka, MALAYSIA, JUL 18, 2018},
Organization = {Minist Educ Malaysia; Fac Mfg Engn, Adv Mfg Ctr},
Abstract = {This study was conducted to obtain accurate confirmation of the leaves
   recognition. Pattern recognition of plants based on the leaves
   identification become a popular trend. Each leaf has its own identity
   that brings a lot of information that can be used to identify and
   classify the origin or type of plant. This paper proposes to determine
   the type of plants based on shape and veins by using Otsu and Edge
   Detection techniques. The main purpose of this paper is to ensure the
   chosen techniques are justified for plant classification. This study has
   tested six different types of leaf with five samples each, and the
   method yields accuracy greater than 90\% in circularity and aspect ratio
   fields.},
Publisher = {UNIV TEKNIKAL MALAYSIA MELAKA},
Address = {HANG TUAH JAYA, MELAKA, MELAKA 76100, MALAYSIA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {bin Mohd, O (Corresponding Author), Univ Teknikal Malaysia Melaka, Fac Informat \& Commun Technol, Durian Tunggal 76100, Melaka, Malaysia.
   bin Mohd, Othman, Univ Teknikal Malaysia Melaka, Fac Informat \& Commun Technol, Durian Tunggal 76100, Melaka, Malaysia.},
ISBN = {978-967-2145-62-2},
Keywords = {Otsu' method; image recognition; leaf image retrieval},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Industrial; Engineering, Manufacturing},
Author-Email = {mothman@utem.edu.my},
Affiliations = {Universiti Teknologi Malaysia; University Teknikal Malaysia Melaka},
Cited-References = {Kanjalkar H.P., 2013, INT J SCI ENG RES, V4, P1777.
   Nath SS, 2014, 2014 INTERNATIONAL CONFERENCE ON CONTROL, INSTRUMENTATION, COMMUNICATION AND COMPUTATIONAL TECHNOLOGIES (ICCICCT), P554, DOI 10.1109/ICCICCT.2014.6993023.
   Sannakki S. S., COMP DIFFERENT LEAF, V1, P15.
   Wu Stephen Gang, 2007, 2007 IEEE International Symposium on Signal Processing and Information Technology, P11, DOI 10.1109/ISSPIT.2007.4458016.},
Number-of-Cited-References = {4},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BQ7RJ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000618443400122},
DA = {2023-08-12},
}

@article{ WOS:000582382700005,
Author = {He, Tuo and Lu, Yang and Jiao, Lichao and Zhang, Yonggang and Jiang,
   Xiaomei and Yin, Yafang},
Title = {Developing deep learning models to automate rosewood tree species
   identification for CITES designation and implementation},
Journal = {HOLZFORSCHUNG},
Year = {2020},
Volume = {74},
Number = {12},
Pages = {1123-1133},
Month = {DEC},
Abstract = {The implementation of Convention on International Trade in Endangered
   Species of Wild Fauna and Flora (CITES) to combat illegal logging and
   associated trade necessitates accurate and efficient field screening of
   wood species. In this study, a total of 10,237 images of 15 Dalbergia
   and 11 Pterocarpus species were collected from the transverse surfaces
   of 417 wood specimens. Three deep learning models were then constructed,
   trained, and tested with these images to discriminate between timber
   species. The optimal parameters of the deep learning model were
   analyzed, and the representative wood anatomical features that were
   activated by the deep learning models were visualized. The results
   demonstrated that the overall accuracies of the 26-class, 15-class, and
   11-class models were 99.3, 93.7, and 88.4\%, respectively. It is
   suggested that at least 100 high-quality images per species with minimum
   patch sizes of 1000 x 1000 from more than 10 wood specimens were needed
   to train reliable and applicable deep learning models. The feature
   visualization indicated that the vessel groupings and axial parenchyma
   were the main wood anatomical features activated by the deep learning
   models. The combination of the state-of-the-art deep learning models,
   parameter configuration, and feature visualization provide a time- and
   cost-effective tool for the field screening of wood species to support
   effective CITES designation and implementation.},
Publisher = {WALTER DE GRUYTER GMBH},
Address = {GENTHINER STRASSE 13, D-10785 BERLIN, GERMANY},
Type = {Article},
Language = {English},
Affiliation = {Yin, YF (Corresponding Author), Chinese Acad Forestry, Chinese Res Inst Wood Ind, Dept Wood Anat \& Utilizat, Beijing 100091, Peoples R China.
   Yin, YF (Corresponding Author), Chinese Acad Forestry, Wood Collect WOODPEDIA, Beijing 100091, Peoples R China.
   He, Tuo; Lu, Yang; Jiao, Lichao; Zhang, Yonggang; Jiang, Xiaomei; Yin, Yafang, Chinese Acad Forestry, Chinese Res Inst Wood Ind, Dept Wood Anat \& Utilizat, Beijing 100091, Peoples R China.
   He, Tuo; Lu, Yang; Jiao, Lichao; Zhang, Yonggang; Jiang, Xiaomei; Yin, Yafang, Chinese Acad Forestry, Wood Collect WOODPEDIA, Beijing 100091, Peoples R China.},
DOI = {10.1515/hf-2020-0006},
ISSN = {0018-3830},
EISSN = {1437-434X},
Keywords = {CITES; Dalbergia; deep learning models; Pterocarpus; rosewood tree
   species; wood identification},
Keywords-Plus = {FORENSIC TIMBER IDENTIFICATION; WOOD IDENTIFICATION; TROPICAL FORESTS;
   TRADE; CLASSIFICATION; RECOGNITION},
Research-Areas = {Forestry; Materials Science},
Web-of-Science-Categories  = {Forestry; Materials Science, Paper \& Wood},
Author-Email = {yafang@caf.ac.cn},
Affiliations = {Chinese Academy of Forestry; Research Institute of Wood Industry, CAF;
   Chinese Academy of Forestry; Research Institute of Wood Industry, CAF},
ResearcherID-Numbers = {Yin, Yafang/C-1568-2017
   },
ORCID-Numbers = {Lu, Yang/0009-0009-9666-6424},
Funding-Acknowledgement = {Project of Chinese Academy of Forestry {[}CAFYBB2017ZE003]; National
   Special Support Plan of China {[}W02020331]},
Funding-Text = {This work was supported by the Project of Chinese Academy of Forestry
   (CAFYBB2017ZE003) and National Special Support Plan of China (no.
   W02020331).},
Cited-References = {{[}Anonymous], 2019, DEC MAD PROP AM APP.
   {[}Anonymous], 2009, MABBERLEYS PLANT BOO.
   {[}Anonymous], 2015, CHINAS HONGMU CONSUM.
   Barlow J, 2016, NATURE, V535, P144, DOI 10.1038/nature18326.
   Barrett MA, 2010, SCIENCE, V328, P1109, DOI 10.1126/science.1187740.
   Bogucki R, 2019, CONSERV BIOL, V33, P676, DOI 10.1111/cobi.13226.
   Brancalion PHS, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aat1192.
   Canteiro C, 2019, CONSERV BIOL, V33, P523, DOI 10.1111/cobi.13291.
   Ceballos G, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1400253.
   Dormontt EE, 2015, BIOL CONSERV, V191, P790, DOI 10.1016/j.biocon.2015.06.038.
   Dumenu WK, 2019, BIOL CONSERV, V236, P124, DOI 10.1016/j.biocon.2019.05.044.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Ellwood ER, 2019, CONSERV BIOL, V33, P498, DOI 10.1111/cobi.13287.
   Espinoza EO, 2015, IAWA J, V36, P311, DOI 10.1163/22941932-20150102.
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056.
   Figueroa-Mata G, 2018, INT WORK C BIOINSPIR, P1, DOI DOI 10.1109/IWOBI.2018.8464206.
   Gasson P, 2011, IAWA J, V32, P137, DOI 10.1163/22941932-90000049.
   Gasson P, 2010, ANN BOT-LONDON, V105, P45, DOI 10.1093/aob/mcp270.
   Han B, 2018, ADV NEUR IN, V31.
   Hartvig I, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0138231.
   He T, 2019, PLANTA, V249, P1617, DOI 10.1007/s00425-019-03116-3.
   Houghton RA, 2015, NAT CLIM CHANGE, V5, P1022, DOI 10.1038/nclimate2869.
   Hwang SW, 2018, J WOOD SCI, V64, P69, DOI 10.1007/s10086-017-1680-x.
   IAWA Committee, 2016, IND XYL 4 1.
   Irwin A, 2019, NATURE, V568, P19, DOI 10.1038/d41586-019-01035-7.
   Jiao LC, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-20381-6.
   Koch G, 2015, J FORENSIC RES, V6, DOI {[}10.4172/2157-7145.1000317, DOI 10.4172/2157-7145.1000317].
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Laurance WF, 2008, SCIENCE, V319, P1184, DOI 10.1126/science.319.5867.1184b.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lewis SL, 2015, SCIENCE, V349, P827, DOI 10.1126/science.aaa9932.
   Lim CL, 2017, CONSERV BIOL, V31, P1362, DOI 10.1111/cobi.12984.
   Lowe AJ, 2016, BIOSCIENCE, V66, P990, DOI 10.1093/biosci/biw129.
   Martins J, 2013, MACH VISION APPL, V24, P567, DOI 10.1007/s00138-012-0417-5.
   Ng KKS, 2016, FORENSIC SCI INT-GEN, V23, P197, DOI 10.1016/j.fsigen.2016.05.002.
   Nualart N, 2017, BOT REV, V83, P303, DOI 10.1007/s12229-017-9188-z.
   Paula PL, 2014, MACH VISION APPL, V25, P1019, DOI 10.1007/s00138-014-0592-7.
   Pavlovich MJ, 2018, MASS SPECTROM REV, V37, P171, DOI 10.1002/mas.21509.
   Ravindran P, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0292-9.
   Saatchi SS, 2011, P NATL ACAD SCI USA, V108, P9899, DOI 10.1073/pnas.1019576108.
   Silva DC, 2018, HOLZFORSCHUNG, V72, P521, DOI 10.1515/hf-2017-0160.
   Siriwat P, 2018, ENVIRON CONSERV, V45, P352, DOI {[}10.1017/s037689291800005x, 10.1017/S037689291800005X].
   Snel FA, 2018, WOOD SCI TECHNOL, V52, P1411, DOI 10.1007/s00226-018-1027-9.
   Ugochukwu AI, 2018, ECOL ECON, V146, P730, DOI 10.1016/j.ecolecon.2017.12.021.
   Ullman S, 2016, P NATL ACAD SCI USA, V113, P2744, DOI 10.1073/pnas.1513198113.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wiedenhoeft AC, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0219917.
   Yu M, 2017, PLANTA, V246, P1165, DOI 10.1007/s00425-017-2758-9.
   Zhang MM, 2019, IAWA J, V40, P58, DOI 10.1163/22941932-40190224.},
Number-of-Cited-References = {51},
Times-Cited = {11},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {47},
Journal-ISO = {Holzforschung},
Doc-Delivery-Number = {OH2EN},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000582382700005},
DA = {2023-08-12},
}

@inproceedings{ WOS:000585967404094,
Author = {Pacifico, Luciano D. S. and Macario, Valmir and Oliveira, Joao F. L.},
Book-Group-Author = {IEEE},
Title = {Plant Classification Using Artificial Neural Networks},
Booktitle = {2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)},
Series = {IEEE International Joint Conference on Neural Networks (IJCNN)},
Year = {2018},
Note = {International Joint Conference on Neural Networks (IJCNN), Rio de
   Janeiro, BRAZIL, JUL 08-13, 2018},
Abstract = {Automatic plant species identification is a difficulty challenge and an
   interesting area of research for both botanical taxonomy and computer
   science. From the past few years, some attempts towards the development
   of automatic plant recognition systems have been proposed, but the
   performance of such systems is not satisfactory in terms of accuracy,
   and these systems are also task dependent, since they are strongly
   influenced by the set of characteristics extracted from plant samples,
   leading to the problem known as data set bias. In this work, we use a
   Multi-Layer Perceptron (MLP) artificial neural network trained with
   Backpropagation algorithm to perform automatic plant classification. To
   avoid data set bias problem, some plant data sets which use different
   plant features obtained by different feature extraction processes are
   employed. We compare MLP algorithm with several supervised learning
   methods from plant recognition literature using a statistical hypothesis
   test of type Friedman/Nemenyi test. The obtained results show the
   potential of MLP algorithm to deal with plant classification in a
   unbiased fashion.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Pacifico, LDS (Corresponding Author), Univ Fed Rural Pernambuco UFRPE, Dept Comp DC, Recife, PE, Brazil.
   Pacifico, Luciano D. S.; Macario, Valmir, Univ Fed Rural Pernambuco UFRPE, Dept Comp DC, Recife, PE, Brazil.
   Oliveira, Joao F. L., Univ Pernambuco UPE, Fac Ciencias \& Tecnol Garanhuns FACETEG, Garanhuns, PE, Brazil.},
ISSN = {2161-4393},
ISBN = {978-1-5090-6014-6},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Hardware \&
   Architecture; Engineering, Electrical \& Electronic},
Author-Email = {luciano.pacifico@ufrpe.br
   valmir.macario@ufrpe.br
   jflo@cin.ufpe.br},
Affiliations = {Universidade Federal Rural de Pernambuco (UFRPE); Universidade de
   Pernambuco (UPE)},
ResearcherID-Numbers = {Oliveira, João/AAN-7214-2020
   },
ORCID-Numbers = {Oliveira, João/0000-0002-1150-4904
   PACIFICO, LUCIANO/0000-0001-6945-3612},
Cited-References = {Agarwal G, 2006, TAXON, V55, P597, DOI 10.2307/25065637.
   {[}Anonymous], 2013, SIGNAL PROCESS PATTE.
   {[}Anonymous], 1935, B AM IRIS SOC, DOI DOI 10.1007/978-1-4612-5098-2-2.
   Asuncion A., 2007, UCI MACHINE LEARNING.
   Buitinck L., 2013, ECML PKDD WORKSHOP L, P108, DOI DOI 10.48550/ARXIV.1309.0238.
   Charytanowicz M, 2010, ADV INTEL SOFT COMPU, V69, P15.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   De Stefano C, 2012, INT CONF FRONT HAND, P467, DOI 10.1109/ICFHR.2012.166.
   Demsar J, 2006, J MACH LEARN RES, V7, P1.
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x.
   Friedman M, 1937, J AM STAT ASSOC, V32, P675, DOI 10.2307/2279372.
   Friedman M, 1940, ANN MATH STAT, V11, P86, DOI 10.1214/aoms/1177731944.
   Haykin S., 2001, NEURAL NETWORKS COMP.
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Liu JC, 2016, 2016 10TH INTERNATIONAL CONFERENCE ON INNOVATIVE MOBILE AND INTERNET SERVICES IN UBIQUITOUS COMPUTING (IMIS), P5, DOI 10.1109/IMIS.2016.60.
   Mallah C.D., 2013, COMPUTATIONAL RES, V1, P1.
   Mitchell T.M., 1997, MACH LEARN, V1.
   NEMENYI P, 1962, BIOMETRICS, V18, P263.
   Rahmani M. E., 2015, ALLDATA 2015, V82.
   Rankothge W., 2013, ADV TECHN ENG ICATE, P1.
   Rumelhart D.E., 1985, TECH REP, DOI {[}10.1016/b978-1-4832-1446-7.50035-2, DOI 10.7551/MITPRESS/5236.001.0001].
   Sabu A, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON INVENTIVE COMMUNICATION AND COMPUTATIONAL TECHNOLOGIES (ICICCT), P145, DOI 10.1109/ICICCT.2017.7975176.
   Sahay A, 2016, INT CONF SOFTW ENG, P914, DOI 10.1109/ICSESS.2016.7883214.
   Sethulekshmi AV., 2014, J COMPUT SCI INF TEC, V5, P8061.
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347.
   Wang ZB, 2014, IEEE IJCNN, P975, DOI 10.1109/IJCNN.2014.6889656.
   Yang S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0116500.},
Number-of-Cited-References = {28},
Times-Cited = {12},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BQ3NO},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000585967404094},
DA = {2023-08-12},
}

@inproceedings{ WOS:000467273300184,
Author = {Villaruz, Jolitte A. and Salido, Julie Ann A. and Barrios, II, Dennis M.
   and Felizardo, Rogelio L.},
Book-Group-Author = {IEEE},
Title = {Philippine Indigenous Plant Seedlings Classification Using Deep Learning},
Booktitle = {2018 IEEE 10TH INTERNATIONAL CONFERENCE ON HUMANOID, NANOTECHNOLOGY,
   INFORMATION TECHNOLOGY, COMMUNICATION AND CONTROL, ENVIRONMENT AND
   MANAGEMENT (HNICEM)},
Series = {IEEE International Conference on Humanoid Nanotechnology Information
   Technology Communication and Control Environment and Management},
Year = {2018},
Note = {10th IEEE International Conference on Humanoid, Nanotechnology,
   Information Technology, Communication and Control, Environment and
   Management (HNICEM), Baguio City, PHILIPPINES, NOV 29-DEC 02, 2018},
Organization = {IEEE},
Abstract = {Plant taxonomists have specialized knowledge on a specific plant
   species. The shortage of these experts and their unbalanced distribution
   throughout the globe remains to be a serious problem worldwide. Deep
   learning technology can be used to create tools that will help them
   speed up the process of plant classification. This study implements
   three deep learning models to classify images of Philippine indigenous
   plant seedlings into five species. AlexNet, GoogLeNet, and ResNet50 were
   fine-tuned for this purpose. In the three pre-trained models, the weight
   and bias learning rate factors of the fully connected layers were both
   increased to 20 to speed up learning. Augmentation techniques such as
   rotation, random flip, and random horizontally and vertically
   translations were performed to training images to avoid the effects of
   overfitting. The resulting deep learning models can be implemented to
   classify Philippine indigenous plant seedlings as they all achieved
   above 90\% accuracy on the validation set, with ResNet50 exhibited the
   highest with 98.93\%.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Villaruz, JA (Corresponding Author), Aklan Sate Univ, Coll Ind Technol, Aklan, Philippines.
   Villaruz, Jolitte A.; Salido, Julie Ann A.; Barrios, Dennis M., II, Aklan Sate Univ, Coll Ind Technol, Aklan, Philippines.
   Felizardo, Rogelio L., Aklan State Univ, Coll Agr Forestry \& Environm Sci, Aklan, Philippines.},
ISSN = {2475-7152},
ISBN = {978-1-5386-7767-4},
Keywords = {Philippine indigenous plant; plant classification; deep neural networks;
   deep learning; transfer learning},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Multidisciplinary},
Author-Email = {jvillaruz@asu.ph
   jasalido@asu.edu.ph
   denbarrios07@gmail.com
   rogelio.felizardo@yahoo.com.ph},
ResearcherID-Numbers = {Alimboyong, Catherine/AFM-6879-2022
   SALIDO, JULIE ANN/M-4074-2016},
ORCID-Numbers = {SALIDO, JULIE ANN/0000-0001-9944-213X},
Funding-Acknowledgement = {Aklan State University through the Research and Development Services
   unit; Commission on Higher Education; De La Salle University (DLSU);
   Aklan State University},
Funding-Text = {The authors would also like to thank the anonymous referees for their
   valuable comments and helpful suggestions. The support of Aklan State
   University through the Research and Development Services unit is highly
   acknowledged. The help extended by the field workers of the Clonal
   Nursery of Aklan State University and the efforts of Foresters Hanna Mae
   T. Ibuyan and Darlyn Joy I. Galicha in taking the photos of plant
   seedlings are particularly valued. J. A. Salido acknowledges the
   Commission on Higher Education, in collaboration with the De La Salle
   University (DLSU) and Aklan State University, for the funding support
   through the Commission on Higher Education K-12 Transition (CHED K-12)
   Program.},
Cited-References = {{[}Anonymous], 2016, NAT METHODS, DOI {[}DOI 10.1038/nature14539, DOI 10.1038/nmeth.3707].
   Deng J., 2012, ILSVRC 2012.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Ghazi M. Mehdipour, 2016, NEUROCOMPUTING, V235.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   He K, 2015, C COMPUTER VISION PA.
   Jackel L. D. Le Cun, 1990, ADV NEURAL INF PROCE.
   Kim P., 2017, MATLAB DEEP LEARNING, P121, DOI {[}10.1007/978-1-4842-2845-6, DOI 10.1007/978-1-4842-2845-6].
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lee S. H., 2015, P INT C IM PROC ICIP, V2015.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lin M., 2013, J VOCAT REHABIL, V13, P5154.
   Mata-Montero E., 2016, AUTOMATED PLANT SPEC, P2636.
   Reyes A.K., 2015, C LABS EV FOR SEPT, V1391, P9.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sun Y., 2017, COMPUT INTELL NEUROS, V2017.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Wldchen J., 2018, PLANT SPECIES IDENTI, V25.
   Young T., 2017, RECENT TRENDS DEEP L, P130.},
Number-of-Cited-References = {21},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BM6QH},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000467273300184},
DA = {2023-08-12},
}

@article{ WOS:000759173000014,
Author = {Guldenring, Ronja and Nalpantidis, Lazaros},
Title = {Self-supervised contrastive learning on agricultural images},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2021},
Volume = {191},
Month = {DEC},
Abstract = {Agriculture emerges as a prominent application domain for advanced
   computer vision algorithms. As much as deep learning approaches can help
   solve problems such as plant detection, they rely on the availability of
   large amounts of annotated images for training. However, relevant
   agricultural datasets are scarce and at the same time, generic
   well-established image datasets such as ImageNet do not necessarily
   capture the characteristics of agricultural environments. This
   observation has motivated us to explore the applicability of
   self-supervised contrastive learning on agricultural images. Our
   approach considers numerous non-annotated agricultural images, which are
   easy to obtain, and uses them to pre-train deep neural networks. We then
   require only a limited number of annotated images to fine-tune those
   networks in a supervised training manner for relevant downstream tasks,
   such as plant classification or segmentation. To the best of our
   knowledge, contrastive self-supervised learning has not been explored
   before in the area of agricultural images. Our results reveal that it
   outperforms conventional deep learning approaches in classification
   downstream tasks, especially for small amounts of available annotated
   training images where up to 14\% increase of average top-1
   classification accuracy has been observed. Furthermore, the
   computational cost for generating data-specific pre-trained weights is
   fairly low, allowing one to generate easily new pre-trained weights for
   any custom model architecture or task.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Guldenring, R (Corresponding Author), Dept Elect Engn, DK-2800 Lyngby, Denmark.
   Guldenring, Ronja; Nalpantidis, Lazaros, Dept Elect Engn, DK-2800 Lyngby, Denmark.},
DOI = {10.1016/j.compag.2021.106510},
Article-Number = {106510},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Contrastive learning; Deep learning; Self-supervision; SwAV;
   Transfer-learning},
Keywords-Plus = {SUGAR-BEET; CLASSIFICATION},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {ronjag@elektro.dtu.dk},
ResearcherID-Numbers = {Nalpantidis, Lazaros/AAV-9414-2021},
ORCID-Numbers = {Nalpantidis, Lazaros/0000-0002-3620-4123},
Funding-Acknowledgement = {European Commission; European GNSS Agency
   {[}H2020-SPACE-EGNSS2019-870258]},
Funding-Text = {This work has been supported by the European Commission and European
   GNSS Agency through the project ``Galileo-assisted robot to tackle the
   weed Rumex obtusifolius and increase the profitability and
   sustainability of dairy farming (GALIRUMI){''},
   H2020-SPACE-EGNSS2019-870258.},
Cited-References = {{[}Anonymous], IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123.
   Argueso D, 2020, COMPUT ELECTRON AGR, V175, DOI 10.1016/j.compag.2020.105542.
   Azizi, 2021, ARXIV PREPRINT ARXIV.
   Bargoti Suchet, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3626, DOI 10.1109/ICRA.2017.7989417.
   Berman M, 2018, PROC CVPR IEEE, P4413, DOI 10.1109/CVPR.2018.00464.
   Bosilj P, 2020, J FIELD ROBOT, V37, P7, DOI 10.1002/rob.21869.
   Caron M., 2020, ADV NEURAL INFORM PR, P9912.
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9\_9.
   Chebrolu N, 2017, INT J ROBOT RES, V36, P1045, DOI 10.1177/0278364917720510.
   Chen T, 2020, PR MACH LEARN RES, V119.
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549.
   Chiu MT, 2020, IEEE COMPUT SOC CONF, P212, DOI 10.1109/CVPRW50498.2020.00032.
   Cortes C, 2012, J MACH LEARN RES, V13, P795.
   Cristianini N., 2002, NEURAL INFORM PROCES.
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292, DOI DOI 10.48550/ARXIV.1306.0895.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167.
   Espejo-Garcia B, 2020, COMPUT ELECTRON AGR, V175, DOI 10.1016/j.compag.2020.105593.
   Espejo-Garcia B, 2020, COMPUT ELECTRON AGR, V171, DOI 10.1016/j.compag.2020.105306.
   Ferreira AD, 2019, COMPUT ELECTRON AGR, V165, DOI 10.1016/j.compag.2019.104963.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Gidaris S., 2018, ARXIV180307728, P1.
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622.
   Grill J., 2020, ADV NEURAL INF PROCE.
   Guldenring R., 2021, 2021 IEEE RSJ INT C.
   Gutmann M., 2010, P 13 INT C ART INT S, P297, DOI DOI 10.1145/3292500.3330651.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065.
   Jaccard P., 1912, NEW PHYTOL, V11, P37, DOI DOI 10.1111/J.1469-8137.1912.TB05611.X.
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393.
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975.
   King DB, 2015, ACS SYM SER, V1214, P1.
   Kornblith S, 2019, PR MACH LEARN RES, V97.
   Kounalakis T, 2019, COMPUT ELECTRON AGR, V165, DOI {[}10.1016/j.compag.2019.104973, 10.1016/j.compag.201].
   Loshchilov I., 2017, P INT C LEARNING REP.
   Madsen SL, 2019, BIOSYST ENG, V187, P147, DOI 10.1016/j.biosystemseng.2019.09.005.
   Misra I., 2016, ARXIV160308561.
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4\_5.
   Olsen A, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-38343-3.
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278.
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4\_28.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Skovsen S, 2019, IEEE COMPUT SOC CONF, P2676, DOI 10.1109/CVPRW.2019.00325.
   Suh HK, 2018, BIOSYST ENG, V174, P50, DOI 10.1016/j.biosystemseng.2018.06.017.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   vanden Oord A, 2018, ARXIV, DOI DOI 10.48550/ARXIV.1807.03748.
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180.
   Xie Z., 2021, PROPAGATE YOURSELF E.
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556.
   Zapata P.A. Marin, 2020, SELFSUPERVISED FEATU.
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9\_40.
   Zhao ZC, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12203276.},
Number-of-Cited-References = {52},
Times-Cited = {14},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {21},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {ZE9BX},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000759173000014},
OA = {Green Published, hybrid},
DA = {2023-08-12},
}

@inproceedings{ WOS:000279663600021,
Author = {Abadi, Mohamed and Capelle-Laize, Anne-Sophie and Khoudeir, Majdi and
   Combes, Didier and Carre, Serge},
Editor = {Elmoataz, A and Lezoray, O and Nouboud, F and Mammass, D and Meunier, J},
Title = {Grassland Species Characterization for Plant Family Discrimination by
   Image Processing},
Booktitle = {IMAGE AND SIGNAL PROCESSING, PROCEEDINGS},
Series = {Lecture Notes in Computer Science},
Year = {2010},
Volume = {6134},
Pages = {173+},
Note = {4th International Conference on Image and Signal Processing, Trois
   Rivieres, CANADA, JUN 30-JUL 02, 2010},
Organization = {Int Assoc Pattern Recognit; European Assoc Signal Proc; Univ Quebec
   Trois Rivieres; Univ Caen Basse Normandie; Univ Ibn Zohr; Ctr Rech Math},
Abstract = {Pasture species belongirig to poaceae and fabaceae families constitute
   of essential elements to maintain natural and cultivated regions. Their
   balance and productivity are key factors for good functioning of the
   grassland ecosystems. The study is based on a process of image
   processing. First of all an individual signature is defined while
   considering geometric characteristics of each family. Then, this
   signature is used to discriminate between these families.
   Our approach focuses on the use of shape features in different
   situations. Specifically, the approach is based on cutting the
   representative leaves of each plant family. After cutting, we obtain
   leaves sections of different; sizes and random geometry. Then, the shape
   features are calculated. Principal component; analysis is used to select
   the most; discriminatory features.
   The results will be used to optimize the acquisition conditions. We have
   a discrimination rate of more than 90\% for the experiments carried out
   in a controlled environment. Experiments are being carried out to extend
   this study in natural environments.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Abadi, M (Corresponding Author), Univ Poitiers, XLIM SIC, BP 30179, F-86962 Futuroscope, France.
   Abadi, Mohamed; Capelle-Laize, Anne-Sophie; Khoudeir, Majdi, Univ Poitiers, XLIM SIC, BP 30179, F-86962 Futuroscope, France.
   Combes, Didier; Carre, Serge, INRA, Poitou Charentes, UR4 P3F, F-86600 Lusignan, France.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-642-13680-1},
Keywords = {shape features; plant classification; leaf recognition; pasture; poaceae
   and fabaceae family; image processing},
Keywords-Plus = {LEAF IMAGES; IDENTIFICATION; CLASSIFICATION; SEGMENTATION; RECOGNITION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic},
Author-Email = {abadi@sic.sp2mi.univ-poitiers.fr
   capelle@sic.sp2mi.univ-poitiers.fr
   khoudeir@sic.sp2mi.univ-poitiers.fr
   dcombes@lusignan.inra.fr
   scarre@lusignan.inra.fr},
Affiliations = {Universite de Poitiers; INRAE},
Funding-Acknowledgement = {Poitou-Charentes},
Funding-Text = {The authors would like to thank Poitou-Charentes region for financing
   this study.},
Cited-References = {{[}Anonymous], MATLAB TOOLBOX PATTE.
   {[}Anonymous], 2000, DIGITAL SIGNAL PROC.
   Cheng HD, 2001, PATTERN RECOGN, V34, P2259, DOI 10.1016/S0031-3203(00)00149-7.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Du JX, 2005, LECT NOTES COMPUT SC, V3644, P282.
   Gebhardt S, 2006, PRECIS AGRIC, V7, P165, DOI 10.1007/s11119-006-9006-9.
   Himstedt M, 2009, CROP SCI, V49, P1910, DOI 10.2135/cropsci2008.11.0636.
   HUYGUE C, 2005, PRAIRIES CULTURES 4.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   LECONTE D, 2002, PRAIR NORM C ROB.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Vandenbroucke N, 2003, COMPUT VIS IMAGE UND, V90, P190, DOI 10.1016/S1077-3142(03)00025-0.
   Wang XF, 2005, LECT NOTES COMPUT SC, V3644, P87.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wang ZY, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOL 1 \& 2, P372, DOI 10.1109/FUZZ.2002.1005019.
   WU S, 2007, COMPUTER SCI ARTIFIC.
   Yang Li, 2004, Chinese Journal of Computers, V27, P420.
   Zaller JG, 2004, WEED RES, V44, P414, DOI 10.1111/j.1365-3180.2004.00416.x.},
Number-of-Cited-References = {18},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {8},
Doc-Delivery-Number = {BPQ95},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000279663600021},
OA = {Bronze},
DA = {2023-08-12},
}

@article{ WOS:000626959500001,
Author = {Hussein, Burhan Rashid and Malik, Owais Ahmed and Ong, Wee-Hong and
   Slik, Johan Willem Frederik},
Title = {Reconstruction of damaged herbarium leaves using deep learning
   techniques for improving classification accuracy},
Journal = {ECOLOGICAL INFORMATICS},
Year = {2021},
Volume = {61},
Month = {MAR},
Abstract = {Leaf is one of the most commonly used organs for species identification.
   The traditional identification process involves a manual analysis of
   individual dried or fresh leaf's features by the botanists. Recent
   advancements in computer vision techniques have assisted in automating
   the plants families/species identification process based on the digital
   images of leaves. However, most of the existing studies have focused on
   using datasets for fresh and intact leaves. A huge amount of data for
   preserved plants in the form of digitized herbaria specimens have not
   been effectively utilized for the task of automated identification
   because of the presence of damaged leaves in specimens. In this study,
   deep learning techniques have been proposed as a tool for reconstructing
   the damaged herbarium leaves in order to maximize the usefulness of the
   digitized specimens for automated plant identification task by
   increasing the number of individual samples of leaves. The
   reconstruction results of two different families of convolution neural
   networks (CNNs) have been compared for data from ten different plant
   families namely Anacardiaceae, Annonaceae, Dipterocarpaceae, Ebenaceae,
   Euphorbiaceae, Malvaceae, Phyllanthaceae, Polygalaceae, Rubiaceae and
   Sapotaceae. The performance of automated identification task was
   improved by more than 20\% using the reconstructed leaves images as
   compared to using the original data (i.e. images of specimens with
   damaged leaves). This work evidently suggests that deep learning
   techniques can be utilized for reconstruction of damaged leaves even on
   a challenging herbarium leaves dataset.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Malik, OA (Corresponding Author), Univ Brunei Darussalam, Fac Sci, Digital Sci, Jalan Tungku Link, BE-1410 Gadong, Brunei.
   Hussein, Burhan Rashid; Malik, Owais Ahmed; Ong, Wee-Hong, Univ Brunei Darussalam, Fac Sci, Digital Sci, Jalan Tungku Link, BE-1410 Gadong, Brunei.
   Malik, Owais Ahmed; Ong, Wee-Hong, Univ Brunei Darussalam, Inst Appl Data Analyt, Jalan Tungku Link, BE-1410 Gadong, Brunei.
   Slik, Johan Willem Frederik, Univ Brunei Darussalam, Fac Sci, Environm \& Life Sci, Jalan Tungku Link, BE-1410 Gadong, Brunei.},
DOI = {10.1016/j.ecoinf.2021.101243},
EarlyAccessDate = {FEB 2021},
Article-Number = {101243},
ISSN = {1574-9541},
EISSN = {1878-0512},
Keywords = {Deep learning; Damaged leaves; Generative adversarial networks; Partial
   convolution; Plant species identification; Herbaria},
Keywords-Plus = {AUTOMATED IDENTIFICATION; SPECIMENS},
Research-Areas = {Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Ecology},
Author-Email = {owais.malik@ubd.edu.bn},
Affiliations = {University Brunei Darussalam; University Brunei Darussalam; University
   Brunei Darussalam},
ResearcherID-Numbers = {hussein, burhan rashid/AAG-6113-2021
   },
ORCID-Numbers = {hussein, burhan rashid/0000-0002-9683-8689
   Malik, Owais/0000-0002-4888-5448},
Funding-Acknowledgement = {Universiti Brunei Darussalam {[}UBD/RSCH/1.4/FICBF(b)/2018/011]},
Funding-Text = {Special thanks to Universiti Brunei Darussalam for sponsoring this
   research under the research grant UBD/RSCH/1.4/FICBF(b)/2018/011. We
   gratefully acknowledge the Institute of Applied Data Analytics,
   Universiti Brunei Darussalam for providing facilities and support.},
Cited-References = {Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Black S, 2020, IEEE WINT CONF APPL, P1049, DOI 10.1109/WACV45572.2020.9093362.
   Carranza-Rojas J., 2016, CLEI ELECT J, V19, P1, DOI {[}10.19153/ cleiej.19.1.7, DOI 10.19153/CLEIEJ.19.1.7].
   Carranza-Rojas J, 2018, MULTIMED SYST APPL, P151, DOI 10.1007/978-3-319-76445-0\_9.
   Carranza-Rojas J, 2017, BMC EVOL BIOL, V17, P1, DOI 10.1186/s12862-017-1014-z.
   Corney DPA, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0042112.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   Hussein BR, 2020, LECT NOTES ELECTR EN, V603, P85, DOI 10.1007/978-981-15-0058-9\_9.
   Hussein BR, 2020, LECT NOTES ELECTR EN, V603, P321, DOI 10.1007/978-981-15-0058-9\_31.
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6\_6.
   Mata-Montero E., 2016, PROC IFIP WORLD INFO, P26, DOI {[}10.1007/978-3-319-44447-5\_3, DOI 10.1007/978-3-319-44447-5\_3].
   Meineke EK, 2018, ECOL MONOGR, V88, P505, DOI 10.1002/ecm.1307.
   Mora-Fallas A., 2019, BIODIVERS INF SCI ST, V3, P4, DOI DOI 10.3897/BISS.3.37341.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Pan ZQ, 2019, IEEE ACCESS, V7, P36322, DOI 10.1109/ACCESS.2019.2905015.
   Po Ken Pang, 2019, IOP Conference Series: Materials Science and Engineering, V495, DOI 10.1088/1757-899X/495/1/012032.
   Polgar G, 2018, RAFFLES B ZOOL, V66, P320.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Sara U., 2019, J COMPUT COMMUN, V07, P8, DOI {[}DOI 10.4236/JCC.2019.73002, 10.4236/jcc.2019.73002].
   Simonyan K., 2014, 14091556V6 ARXIV, P1, DOI DOI 10.48550/ARXIV.1409.1556.
   Tomaszewski D, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0153071.
   Villacis-llobet J., 2019, COMMUN COMPUT PHYS, P438.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang L, 2020, IEEE ACCESS, V8, P63514, DOI 10.1109/ACCESS.2020.2982224.
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861.
   Yosinski J, 2014, ADV NEUR IN, V27.
   Younis S, 2018, BOT LETT, V165, P377, DOI 10.1080/23818107.2018.1446357.},
Number-of-Cited-References = {31},
Times-Cited = {5},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Ecol. Inform.},
Doc-Delivery-Number = {QU0EX},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000626959500001},
DA = {2023-08-12},
}

@article{ WOS:000502138500001,
Author = {Mustafa, M. S. and Husin, Z. and Tan, W. K. and Mavi, M. F. and Farook,
   R. S. M.},
Title = {Development of automated hybrid intelligent system for herbs plant
   classification and early herbs plant disease detection},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2020},
Volume = {32},
Number = {15},
Pages = {11419-11441},
Month = {AUG},
Abstract = {Plants such as herbs are widely used in the medical and cosmetic
   industry. Recognizing a species and detecting an early disease of a
   plant are quite challenging and difficult to implement as an automated
   device. The manual identification process is a lengthy process and
   requires a prior understanding about the plant itself, such as shape,
   odour, and texture. Thus, this research aimed to realize the
   computerized method to recognize the species and detect early disease of
   the herbs by referring to these characteristics. This research has been
   developed a system for recognizing the species and detecting the early
   disease of the herbs using computer vision and electronic nose, which
   focus on odour, shape, colour and texture extraction of herb leaves,
   together with a hybrid intelligent system that are involved fuzzy
   inference system, naive Bayes (NB), probabilistic neural network (PNN)
   and support vector machine (SVM) classifier. These techniques were used
   to perform a convenient and effective herb species recognition and early
   disease detection on ten different herb species samples. The species
   recognition accuracy rate among ten different species using computer
   vision and electronic nose is archived 97\% and 96\%, respectively, in
   SVM, 98\% and 98\%, respectively, in PNN and both 94\% in NB. In the
   early disease detection, the detection rate among ten different herb's
   species using computer vision and electronic nose are 98\% and 97\%,
   respectively, in SVM, both 98\% in PNN, 95\% and 94\%, respectively, in
   NB. Integrated three machine learning approaches have successfully
   achieved almost 99\% for recognition and detection rate.},
Publisher = {SPRINGER LONDON LTD},
Address = {236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Husin, Z (Corresponding Author), Univ Malaysia Perlis, Sch Comp \& Commun Engn, Arau, Malaysia.
   Mustafa, M. S.; Husin, Z.; Tan, W. K.; Mavi, M. F., Univ Malaysia Perlis, Sch Comp \& Commun Engn, Arau, Malaysia.
   Farook, R. S. M., Univ Malaysia Perlis, Fac Engn Technol, Arau, Malaysia.},
DOI = {10.1007/s00521-019-04634-7},
EarlyAccessDate = {DEC 2019},
ISSN = {0941-0643},
EISSN = {1433-3058},
Keywords = {Herb species recognition; Early herb disease detection; Computer vision;
   Electronic nose; Hybrid intelligent system},
Keywords-Plus = {ELECTRONIC NOSE; RECOGNITION; FEATURES; LEAVES; SHAPE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {zulhusin@unimap.edu.my},
Affiliations = {Universiti Malaysia Perlis; Universiti Malaysia Perlis},
ORCID-Numbers = {Tan, Wei Keong/0000-0002-5138-672X},
Funding-Acknowledgement = {Universiti Malaysia Perlis},
Funding-Text = {The work was supported by Universiti Malaysia Perlis.},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Ahmed Ishtiaq, 2014, International Journal of Machine Learning and Computing, V4, P183, DOI 10.7763/IJMLC.2014.V4.409.
   American Phytopathological S, 1979, IND COMM NAM PLANT D, V41.
   Azar AT, 2013, NEURAL COMPUT APPL, V23, P1019, DOI 10.1007/s00521-012-1026-y.
   Barbedo JGA, 2016, BIOSYST ENG, V147, P104, DOI 10.1016/j.biosystemseng.2016.03.012.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Govaerts R, 2001, TAXON, V50, P1085, DOI 10.2307/1224723.
   Grosan C, 2011, INTEL SYST REF LIBR, V17, P423.
   Husin Z, 2012, COMPUT ELECTRON AGR, V89, P18, DOI 10.1016/j.compag.2012.07.009.
   Husin Z, 2013, 8 INT C INF TECHN AP.
   Husin Z. B, 2012, 2012 3 INT C INT SYS.
   Jiang MY, 2018, NEURAL COMPUT APPL, V29, P61, DOI 10.1007/s00521-016-2401-x.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Madani A, 2018, NEURAL COMPUT APPL, V30, P2807, DOI 10.1007/s00521-017-2887-x.
   Mohamad Yusof U. K, 2015, AUST J BASIC APPL SC, V9, P360.
   Mora C, 2011, PLOS BIOL, V9, DOI 10.1371/journal.pbio.1000606.
   Niculescu-Mizil A., 2005, P 22 INT C MACH LEAR, P625, DOI {[}10.1145/1102351.1102430, DOI 10.1145/1102351.1102430].
   Omatu S, 2016, NEUROCOMPUTING, V172, P394, DOI 10.1016/j.neucom.2015.03.101.
   Patel AK, 2016, GEOSCI FRONT, V7, P53, DOI 10.1016/j.gsf.2014.10.005.
   Sabrol Hiteshwari, 2016, INDIAN J SCI TECHNOL, V9, P1, DOI {[}10.17485/ijst/2016/v9i44/92825, DOI 10.17485/ijst/2016/v9i44/92825].
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   Sankaran S, 2010, COMPUT ELECTRON AGR, V72, P1, DOI 10.1016/j.compag.2010.02.007.
   Sannakki S. S., 2011, International Journal of Machine Intelligence, V3, P36.
   Scotland RW, 2003, TAXON, V52, P101, DOI 10.2307/3647306.
   Sharma R, 2019, J AGR FOOD CHEM, V67, P7530, DOI 10.1021/acs.jafc.9b02500.
   Shree SRB, 2018, NEURAL COMPUT APPL, V29, P123, DOI 10.1007/s00521-016-2416-3.
   Soh AC, 2018, J ENG SCI TECHNOL, V13, P3043.
   Soh AC, 2014, INT J SMART SENS INT, V7, P584.
   Sun YB, 2019, ANN APPL BIOL, V174, P209, DOI 10.1111/aab.12485.
   Ucar A, 2017, CHEMOMETR INTELL LAB, V166, P69, DOI 10.1016/j.chemolab.2017.05.013.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Wang B, 2015, INFORM SCIENCES, V302, P132, DOI 10.1016/j.ins.2014.07.028.
   Wang X, 2014, DIGIT SIGNAL PROCESS, V34, P101, DOI 10.1016/j.dsp.2014.08.005.
   Wang ZB, 2016, NEURAL COMPUT APPL, V27, P899, DOI 10.1007/s00521-015-1904-1.
   Yigit E, 2019, COMPUT ELECTRON AGR, V156, P369, DOI 10.1016/j.compag.2018.11.036.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {36},
Times-Cited = {29},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {32},
Journal-ISO = {Neural Comput. Appl.},
Doc-Delivery-Number = {ML7NA},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000502138500001},
DA = {2023-08-12},
}

@article{ WOS:001036533500001,
Author = {Yordanov, Momchil and d'Andrimont, Raphael and Martinez-Sanchez, Laura
   and Lemoine, Guido and Fasbender, Dominique and van der Velde, Marijn},
Title = {Crop Identification Using Deep Learning on LUCAS Crop Cover Photos},
Journal = {SENSORS},
Year = {2023},
Volume = {23},
Number = {14},
Month = {JUL},
Abstract = {Massive and high-quality in situ data are essential for
   Earth-observation-based agricultural monitoring. However, field
   surveying requires considerable organizational effort and money. Using
   computer vision to recognize crop types on geo-tagged photos could be a
   game changer allowing for the provision of timely and accurate
   crop-specific information. This study presents the first use of the
   largest multi-year set of labelled close-up in situ photos
   systematically collected across the European Union from the Land Use
   Cover Area frame Survey (LUCAS). Benefiting from this unique in situ
   dataset, this study aims to benchmark and test computer vision models to
   recognize major crops on close-up photos statistically distributed
   spatially and through time between 2006 and 2018 in a practical
   agricultural policy relevant context. The methodology makes use of crop
   calendars from various sources to ascertain the mature stage of the
   crop, of an extensive paradigm for the hyper-parameterization of
   MobileNet from random parameter initialization, and of various
   techniques from information theory in order to carry out more accurate
   post-processing filtering on results. The work has produced a dataset of
   169,460 images of mature crops for the 12 classes, out of which 15,876
   were manually selected as representing a clean sample without any
   foreign objects or unfavorable conditions. The best-performing model
   achieved a macro F1 (M-F1) of 0.75 on an imbalanced test dataset of 8642
   photos. Using metrics from information theory, namely the equivalence
   reference probability, resulted in an increase of 6\%. The most
   unfavorable conditions for taking such images, across all crop classes,
   were found to be too early or late in the season. The proposed
   methodology shows the possibility of using minimal auxiliary data
   outside the images themselves in order to achieve an M-F1 of 0.82 for
   labelling between 12 major European crops.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Yordanov, M (Corresponding Author), SIDOR Consulting S L, Barcelona 08500, Spain.
   van der Velde, M (Corresponding Author), European Commiss, Joint Res Ctr JRC, I-21027 Ispra, Italy.
   Yordanov, Momchil, SIDOR Consulting S L, Barcelona 08500, Spain.
   d'Andrimont, Raphael; Martinez-Sanchez, Laura; Lemoine, Guido; Fasbender, Dominique; van der Velde, Marijn, European Commiss, Joint Res Ctr JRC, I-21027 Ispra, Italy.
   Fasbender, Dominique, Walloon Inst Evaluat Foresight \& Stat IWEPS, B-5001 Namur, Belgium.},
DOI = {10.3390/s23146298},
Article-Number = {6298},
EISSN = {1424-8220},
Keywords = {plant recognition; agriculture; computer vision; deep learning; data
   valorization; mapping from imagery; image classification algorithms},
Keywords-Plus = {LAND-COVER},
Research-Areas = {Chemistry; Engineering; Instruments \& Instrumentation},
Web-of-Science-Categories  = {Chemistry, Analytical; Engineering, Electrical \& Electronic;
   Instruments \& Instrumentation},
Author-Email = {momtchil.iordanov@ext.ec.europa.eu
   raphael.dandrimont@ec.europa.eu
   laura.martinez-sanchez@ec.europa.eu
   guido.lemoine@ec.europa.eu
   d.fasbender@iweps.be
   marijn.van-der-velde@ec.europa.eu},
Affiliations = {European Commission Joint Research Centre; EC JRC ISPRA Site},
Cited-References = {Affouard A., 2017, P ICLR INT C LEARN R.
   AGRI4CAST, 2018, AGRI4CAST B ARCH 201.
   Alzubaidi L, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00444-8.
   Bansal Subodh, 2020, IOP Conference Series: Materials Science and Engineering, V998, DOI 10.1088/1757-899X/998/1/012065.
   Bergstra J, 2012, J MACH LEARN RES, V13, P281.
   Bogaert P, 2017, STOCH ENV RES RISK A, V31, P2297, DOI 10.1007/s00477-016-1310-y.
   Bruha I., 2000, ACM SIGKDD EXPLOR NE, V2, P110, DOI {[}10.1145/380995.381059, DOI 10.1145/380995.381059].
   Columbus L., 2021, 10 WAYS AI HAS POTEN.
   d'Andrimont R., 2022, EARTH SYST SCI DATA, V2022, P1.
   d'Andrimont R, 2022, COMPUT ELECTRON AGR, V196, DOI 10.1016/j.compag.2022.106866.
   d'Andrimont R, 2021, EARTH SYST SCI DATA, V13, P1119, DOI 10.5194/essd-13-1119-2021.
   Devos W., JRC127678 EUR UN.
   Howard AG, 2017, Arxiv, DOI {[}arXiv:1704.04861, DOI 10.48550/ARXIV.1704.04861].
   Gao JM, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13112012.
   Gao X, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14020255.
   He H, 2013, IMBALANCED LEARNING: FOUNDATIONS, ALGORITHMS, AND APPLICATIONS, P1, DOI 10.1002/9781118646106.
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140.
   Kapach K., 2012, INT J COMPUTATIONAL, V3, P4, DOI DOI 10.1504/IJCVR.2012.046419.
   Koch E., 2005, P 17 INT C BIOM ICB, P554.
   Kollia I, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10111223.
   Lu YZ, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105760.
   Matvienko I, 2020, Arxiv, DOI arXiv:2004.03468.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Opitz J, 2021, Arxiv, DOI {[}arXiv:1911.03347, DOI 10.48550/ARXIV.1911.03347].
   Owusu M, 2021, COMPUT ENVIRON URBAN, V89, DOI 10.1016/j.compenvurbsys.2021.101681.
   Santoso I., 2021, IOP Conference Series : Earth and Environmental Science, V924, DOI 10.1088/1755-1315/924/1/012059.
   Schiller C., 2021, SCI REP-UK, V11, P1.
   Sima A., JRC120223 EUR UN.
   Soekhoe D, 2016, LECT NOTES COMPUT SC, V9897, P50, DOI 10.1007/978-3-319-46349-0\_5.
   Soille P, 2018, FUTURE GENER COMP SY, V81, P30, DOI 10.1016/j.future.2017.11.007.
   Sudars K, 2020, DATA BRIEF, V31, DOI 10.1016/j.dib.2020.105833.
   Tian HongKun, 2020, Information Processing in Agriculture, V7, P1, DOI 10.1016/j.inpa.2019.09.006.
   Tseng G., 2021, P 35 C NEUR INF PROC.
   van der Velde M, 2019, AGR SYST, V168, P56, DOI 10.1016/j.agsy.2018.10.003.
   Waldner F, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0181911.
   Wu ZN, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21113647.
   Zheng YY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19051058.
   Zhu LL, 2021, CURR RES FOOD SCI, V4, P233, DOI 10.1016/j.crfs.2021.03.009.},
Number-of-Cited-References = {38},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Sensors},
Doc-Delivery-Number = {N4DI9},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:001036533500001},
OA = {Green Submitted, gold},
DA = {2023-08-12},
}

@article{ WOS:000561307700008,
Author = {Almeida, Brianna K. and Garg, Manish and Kubat, Miroslav and Afkhami,
   Michelle E.},
Title = {Not that kind of tree: Assessing the potential for decision tree-based
   plant identification using trait databases},
Journal = {APPLICATIONS IN PLANT SCIENCES},
Year = {2020},
Volume = {8},
Number = {7},
Month = {JUL},
Abstract = {Premise Advancements in machine learning and the rise of accessible
   ``big data{''} provide an important opportunity to improve trait-based
   plant identification. Here, we applied decision-tree induction to a
   subset of data from the TRY plant trait database to (1) assess the
   potential of decision trees for plant identification and (2) determine
   informative traits for distinguishing taxa. Methods Decision trees were
   induced using 16 vegetative and floral traits (689 species, 20 genera).
   We assessed how well the algorithm classified species from test data and
   pinpointed those traits that were important for identification across
   diverse taxa. Results The unpruned tree correctly placed 98\% of the
   species in our data set into genera, indicating its promise for
   distinguishing among the species used to construct them. Furthermore, in
   the pruned tree, an average of 89\% of the species from the test data
   sets were properly classified into their genera, demonstrating the
   flexibility of decision trees to also classify new species into genera
   within the tree. Closer inspection revealed that seven of the 16 traits
   were sufficient for the classification, and these traits yielded
   approximately two times more initial information gain than those not
   included. Discussion Our findings demonstrate the potential for
   tree-based machine learning and big data in distinguishing among taxa
   and determining which traits are important for plant identification.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Almeida, BK (Corresponding Author), Univ Miami, Dept Biol, 1301 Mem Dr, Coral Gables, FL 33143 USA.
   Almeida, Brianna K.; Afkhami, Michelle E., Univ Miami, Dept Biol, 1301 Mem Dr, Coral Gables, FL 33143 USA.
   Garg, Manish; Kubat, Miroslav, Univ Miami, Dept Elect \& Comp Engn, 1251 Mem Dr, Coral Gables, FL 33143 USA.},
DOI = {10.1002/aps3.11379},
Article-Number = {e11379},
ISSN = {2168-0450},
Keywords = {decision tree; information gain; machine learning; plant identification;
   TRY plant trait database},
Keywords-Plus = {KEYS},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {b.almeida@miami.edu},
Affiliations = {University of Miami; University of Miami},
ORCID-Numbers = {Almeida, Brianna/0000-0002-9921-9971},
Cited-References = {Acevedo MA, 2009, ECOL INFORM, V4, P206, DOI 10.1016/j.ecoinf.2009.06.005.
   {[}Anonymous], 2008, EURASIA J MATH SCI T, DOI DOI 10.12973/EJMSTE/75344.
   Austen GE, 2016, SCI REP-UK, V6, DOI 10.1038/srep33634.
   Casaioli M, 2003, NONLINEAR PROC GEOPH, V10, P373, DOI 10.5194/npg-10-373-2003.
   Chaudhary VB, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.28.
   EDWARDS M, 1987, COMPUT APPL BIOSCI, V3, P1.
   Eibe F., 2016, DATA MINING PRACTICA, V4th ed.
   Elith J, 2006, ECOGRAPHY, V29, P129, DOI 10.1111/j.2006.0906-7590.04596.x.
   Fan JP, 2015, IEEE T IMAGE PROCESS, V24, P4172, DOI 10.1109/TIP.2015.2457337.
   Gallo T, 2011, BIOSCIENCE, V61, P459, DOI 10.1525/bio.2011.61.6.8.
   Garnier E, 2017, J ECOL, V105, P298, DOI 10.1111/1365-2745.12698.
   GAYLARD A, 1995, S AFR J WILDL RES, V25, P35.
   Hortal J, 2007, CONSERV BIOL, V21, P853, DOI 10.1111/j.1523-1739.2007.00686.x.
   Kattge J, 2011, GLOBAL CHANGE BIOL, V17, P2905, DOI 10.1111/j.1365-2486.2011.02451.x.
   Kubat M., 2017, INTRO MACHINE LEARNI.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Ott T, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11351.
   Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN.
   Roy HE, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0150794.
   Singh AK, 2018, TRENDS PLANT SCI, V23, P883, DOI 10.1016/j.tplants.2018.07.004.
   Stajich JE, 2012, NUCLEIC ACIDS RES, V40, pD675, DOI 10.1093/nar/gkr918.
   Thessen A., 2016, ONE ECOSYSTEM, V1, pe8621, DOI {[}10.3897/oneeco.1.e8621, DOI 10.3897/ONEECO.1.E8621].
   TILLING S, 1984, J BIOL EDUC, V18, P293, DOI 10.1080/00219266.1984.9654660.
   Vayssieres MP, 2000, J VEG SCI, V11, P679, DOI 10.2307/3236575.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Weaver WN, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11367.
   Will KW, 2004, CLADISTICS, V20, P47, DOI 10.1111/j.1096-0031.2003.00008.x.
   Wu XD, 2014, IEEE T KNOWL DATA EN, V26, P97, DOI 10.1109/TKDE.2013.109.
   Zanne AE, 2020, BIOL REV, V95, P409, DOI 10.1111/brv.12570.
   Zhang ZH, 2016, ANN TRANSL MED, V4, DOI 10.3978/j.issn.2305-5839.2015.12.38.
   Zhou ZH, 2002, ARTIF INTELL MED, V24, P25, DOI 10.1016/S0933-3657(01)00094-X.},
Number-of-Cited-References = {33},
Times-Cited = {6},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {11},
Journal-ISO = {Appl. Plant Sci.},
Doc-Delivery-Number = {NC6DR},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000561307700008},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000601310900040,
Author = {Beck, Michael A. and Liu, Chen-Yi and Bidinosti, Christopher P. and
   Henry, Christopher J. and Godee, Cara M. and Ajmani, Manisha},
Title = {An embedded system for the automated generation of labeled plant images
   to enable machine learning applications in agriculture},
Journal = {PLOS ONE},
Year = {2020},
Volume = {15},
Number = {12},
Month = {DEC 17},
Abstract = {A lack of sufficient training data, both in terms of variety and
   quantity, is often the bottleneck in the development of machine learning
   (ML) applications in any domain. For agricultural applications, ML-based
   models designed to perform tasks such as autonomous plant classification
   will typically be coupled to just one or perhaps a few plant species. As
   a consequence, each crop-specific task is very likely to require its own
   specialized training data, and the question of how to serve this need
   for data now often overshadows the more routine exercise of actually
   training such models. To tackle this problem, we have developed an
   embedded robotic system to automatically generate and label large
   datasets of plant images for ML applications in agriculture. The system
   can image plants from virtually any angle, thereby ensuring a wide
   variety of data; and with an imaging rate of up to one image per second,
   it can produce lableled datasets on the scale of thousands to tens of
   thousands of images per day. As such, this system offers an important
   alternative to time- and cost-intensive methods of manual generation and
   labeling. Furthermore, the use of a uniform background made of blue
   keying fabric enables additional image processing techniques such as
   background replacement and image segementation. It also helps in the
   training process, essentially forcing the model to focus on the plant
   features and eliminating random correlations. To demonstrate the
   capabilities of our system, we generated a dataset of over 34,000
   labeled images, with which we trained an ML-model to distinguish grasses
   from non-grasses in test data from a variety of sources. We now plan to
   generate much larger datasets of Canadian crop plants and weeds that
   will be made publicly available in the hope of further enabling ML
   applications in the agriculture sector.},
Publisher = {PUBLIC LIBRARY SCIENCE},
Address = {1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA},
Type = {Article},
Language = {English},
Affiliation = {Beck, MA (Corresponding Author), Univ Winnipeg, Dept Phys, Winnipeg, MB, Canada.
   Beck, MA (Corresponding Author), Univ Winnipeg, Dept Appl Comp Sci, Winnipeg, MB, Canada.
   Beck, Michael A.; Bidinosti, Christopher P.; Ajmani, Manisha, Univ Winnipeg, Dept Phys, Winnipeg, MB, Canada.
   Beck, Michael A.; Bidinosti, Christopher P.; Henry, Christopher J.; Ajmani, Manisha, Univ Winnipeg, Dept Appl Comp Sci, Winnipeg, MB, Canada.
   Liu, Chen-Yi; Bidinosti, Christopher P., Univ Manitoba, Dept Elect \& Comp Engn, Winnipeg, MB, Canada.
   Godee, Cara M., Univ Winnipeg, Dept Biol, Winnipeg, MB, Canada.},
DOI = {10.1371/journal.pone.0243923},
Article-Number = {e0243923},
ISSN = {1932-6203},
Keywords-Plus = {PRECISION AGRICULTURE; ARABIDOPSIS-THALIANA; COMPUTER VISION; FIELD;
   PLATFORM; CROP; CLASSIFICATION; OPERATIONS; ROBOTS},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {m.beck@uwinnipeg.ca},
Affiliations = {University of Winnipeg; University of Winnipeg; University of Manitoba;
   University of Winnipeg},
ResearcherID-Numbers = {Beck, Michael/HTM-1856-2023
   },
ORCID-Numbers = {Henry, Christopher/0000-0002-2624-1502},
Funding-Acknowledgement = {George Weston Limited -Seeding Food Innovation {[}SFI18-0276]; Western
   Economic Diversification Canada -Regional Innovation Ecosystems Program
   {[}15453]; Mitacs -Accelerate {[}IT14120]},
Funding-Text = {M.B., C.B., C.H., C.-Y., and C.G. received funding from: George Weston
   Limited -Seeding Food Innovation SFI18-0276, https://www.weston.ca;
   Mitacs -Accelerate IT14120, https://www.mitacs.ca; and Western Economic
   Diversification Canada -Regional Innovation Ecosystems Program 15453,
   https://www.wd-deo.gc.ca/eng/19775.asp The funders had no role in study
   design, data collection and analysis, decision to publish, or
   preparation of the manuscript.},
Cited-References = {a R-M, 2020, J CLIN MED, V9.
   {[}Anonymous], 2019, NAT ELECTRON, V2, P1, DOI 10.1038/s41928-019-0203-8.
   {[}Anonymous], 2015, Prescrire Int, V24, P93.
   {[}Anonymous], 2018, PLOS ONE, V13, P1, DOI DOI 10.3171/2018.1.JNS172645.
   Antonacci A, 2018, TRAC-TREND ANAL CHEM, V98, P95, DOI 10.1016/j.trac.2017.10.022.
   Antoniou A, 2018, 171104340 ARXIV.
   Barbedo JGA, 2013, SPRINGERPLUS, V2, DOI 10.1186/2193-1801-2-660.
   Bacco M., 2019, ARRAY, V3-4, P100009, DOI {[}10.1016/j.array.2019.100009, DOI 10.1016/J.ARRAY.2019.100009].
   Bah MD, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111690.
   Bai G, 2016, COMPUT ELECTRON AGR, V128, P181, DOI 10.1016/j.compag.2016.08.021.
   Barker J, 2016, COMPUT ELECTRON AGR, V122, P74, DOI 10.1016/j.compag.2016.01.017.
   Bechar A, 2017, BIOSYST ENG, V153, P110, DOI 10.1016/j.biosystemseng.2016.11.004.
   Bechar A, 2016, BIOSYST ENG, V149, P94, DOI 10.1016/j.biosystemseng.2016.06.014.
   Beck MB, 2020, EAGL 1.
   Binch A, 2017, COMPUT ELECTRON AGR, V140, P123, DOI 10.1016/j.compag.2017.05.018.
   Bojarski Mariusz, 2016, arXiv.
   Bosilj P, 2018, IEEE ROBOT AUTOM LET, V3, P2950, DOI 10.1109/LRA.2018.2848305.
   Bradski GAK, 2008, LEARNING OPENCV COMP.
   Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980.
   Chene Y, 2012, COMPUT ELECTRON AGR, V82, P122, DOI 10.1016/j.compag.2011.12.007.
   Crimmins MA, 2008, ENVIRON MANAGE, V41, P949, DOI 10.1007/s00267-008-9086-6.
   Cubuk ED, 2019, 180509501 ARXIV.
   Dobrescu A, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0247-6.
   Duckett T, 2020, 2018 INT ROB SHOWC U.
   Dutta Abhishek, 2019, MM `19: Proceedings of the 27th ACM International Conference on Multimedia, P2276, DOI 10.1145/3343031.3350535.
   Dyrmann M., 2017, ARXIV171105458.
   Gehan MA, 2017, PEERJ, V5, DOI 10.7717/peerj.4088.
   Gehan MA, 2017, AM J BOT, V104, P505, DOI 10.3732/ajb.1700044.
   Giuffrida MV, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0278-7.
   Granier C, 2006, NEW PHYTOL, V169, P623, DOI 10.1111/j.1469-8137.2005.01609.x.
   Han DM, 2018, EXPERT SYST APPL, V95, P43, DOI 10.1016/j.eswa.2017.11.028.
   Haug S, 2015, LECT NOTES COMPUT SC, V8928, P105, DOI {[}10.1007/978-3-319-16220-1-8, 10.1007/978-3-319-16220-1\_8].
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123.
   Henry CJ, 2019, INT J REMOTE SENS, V40, P4416, DOI 10.1080/01431161.2018.1563840.
   Iandola FN, 160207360 ARXIV.
   Jansen M, 2009, FUNCT PLANT BIOL, V36, P902, DOI 10.1071/FP09095.
   Jiang Y, 2016, COMPUT ELECTRON AGR, V130, P57, DOI 10.1016/j.compag.2016.09.017.
   Jimenez-Berni JA, 2018, FRONT PLANT SCI, V9, DOI 10.3389/fpls.2018.00237.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Khanna A, 2019, COMPUT ELECTRON AGR, V157, P218, DOI 10.1016/j.compag.2018.12.039.
   Lemley J, 2017, IEEE ACCESS, V5, P5858, DOI 10.1109/ACCESS.2017.2696121.
   Liakos KG, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18082674.
   Lobet G, 2017, TRENDS PLANT SCI, V22, P559, DOI 10.1016/j.tplants.2017.05.002.
   Lottes P, 2016, IEEE INT CONF ROBOT, P5157, DOI 10.1109/ICRA.2016.7487720.
   Manoj PDS, 2015, IEEE T COMPUT, V64, P3022, DOI 10.1109/TC.2015.2389827.
   Mikolajczyk Agnieszka, 2018, 2018 International Interdisciplinary PhD Workshop (IIPhDW), P117, DOI 10.1109/IIPHDW.2018.8388338.
   Minervini M, 2017, PLANT J, V90, P204, DOI 10.1111/tpj.13472.
   Mingxing T, 190511946 ARXIV.
   Oberti R, 2016, BIOSYST ENG, V146, P1, DOI 10.1016/j.biosystemseng.2016.05.010.
   Patricio DI, 2018, COMPUT ELECTRON AGR, V153, P69, DOI 10.1016/j.compag.2018.08.001.
   Rapson C. J., 2018, P 11 INT C MOB COMP, P1, DOI DOI 10.1109/IVCNZ.2018.8634750.
   Relf-Eckstein JE, 2019, NJAS-WAGEN J LIFE SC, V90-91, DOI 10.1016/j.njas.2019.100307.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8.
   Schenk E., 2009, WORKSH OP SOURC INN, V72, P3.
   Shakoor N, 2017, CURR OPIN PLANT BIOL, V38, P184, DOI 10.1016/j.pbi.2017.05.006.
   Shamshiri RR, 2018, INT J AGR BIOL ENG, V11, P1, DOI 10.25165/j.ijabe.20181104.4278.
   Singh A, 2016, TRENDS PLANT SCI, V21, P110, DOI 10.1016/j.tplants.2015.10.015.
   Sixt L, 2017, 161101331 ARXIV.
   Story D, 2015, MACH VISION APPL, V26, P495, DOI 10.1007/s00138-015-0670-5.
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97.
   Taigman Y., 2014, P IEEE COMP SOC C CO.
   Tardieu F, 2017, CURR BIOL, V27, pR770, DOI 10.1016/j.cub.2017.05.055.
   Tisne S, 2013, PLANT J, V74, P534, DOI 10.1111/tpj.12131.
   Ubbens J, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0273-z.
   Ubbens JR, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01190.
   Unal I, 2015, INT J ADV ROBOT SYST, V12, DOI 10.5772/62059.
   Vazquez-Arellano M, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16050618.
   Vinyals O., 2015, P IEEE COMP SOC C CO.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Xie S, 161105431 ARXIV.
   Narvaez FY, 2017, IEEE-ASME T MECH, V22, P2428, DOI 10.1109/TMECH.2017.2760866.},
Number-of-Cited-References = {75},
Times-Cited = {6},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {12},
Journal-ISO = {PLoS One},
Doc-Delivery-Number = {PI8BZ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000601310900040},
OA = {Green Submitted, Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000426116300015,
Author = {Ballado, Jr., Alejandro H. and Garcia, Ramon G. and Chichoco, Joanne Gem
   Z. and Domingo, Bianca Marie B. and Santuyo, Kimberly Joy M. and
   Sulmaca, Van Jay S. and Bentir, Sarah Alma P. and Sarte, Shydel M.},
Book-Group-Author = {IEEE},
Title = {Forest Mapping and Classification of Forest Type Using LiDAR Data and
   Tree Specie Identification Through Image Processing Based on Leaf
   Extraction Algorithms},
Booktitle = {2017 IEEE 9TH INTERNATIONAL CONFERENCE ON HUMANOID, NANOTECHNOLOGY,
   INFORMATION TECHNOLOGY, COMMUNICATION AND CONTROL, ENVIRONMENT AND
   MANAGEMENT (IEEE HNICEM)},
Series = {IEEE International Conference on Humanoid Nanotechnology Information
   Technology Communication and Control Environment and Management},
Year = {2017},
Note = {9th IEEE International Conference on Humanoid, Nanotechnology,
   Information Technology, Communication and Control, Environment and
   Management (IEEE HNICEM), Pasay, PHILIPPINES, NOV 29-DEC 03, 2017},
Organization = {IEEE; IEEE Philippines},
Abstract = {With the use of Light Detection and Ranging (LiDAR) Data, this study
   focuses on the processing of the LiDAR derived data through different
   software tools to generate a map that can classify forest types. A 20 x
   20 meter plot in the selected forest area was identified in this study
   for the field validation of the classified leaf type. Leaf recognition
   is performed using Neural Network in Matlab. The leaf statistics were
   measured through the prototype developed using leaf extraction
   algorithms T-test is used for the comparative measurement between the
   perimeter of the extracted data and the actual perimeter of a sample
   leaf. The result shows that for the specie, the actual perimeter is
   statistically the same with the perimeter measured by the developed
   prototype. The accuracy of classification was calculated as 91.25\%. The
   overall minimum and maximum precision of the prototype is computed to be
   90.40\% and 99.14\%, respectively.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Ballado, AH (Corresponding Author), Mapua Univ, Sch Elect Elect \& Comp Engn, Manila, Philippines.
   Ballado, Alejandro H., Jr.; Garcia, Ramon G.; Chichoco, Joanne Gem Z.; Domingo, Bianca Marie B.; Santuyo, Kimberly Joy M.; Sulmaca, Van Jay S.; Bentir, Sarah Alma P.; Sarte, Shydel M., Mapua Univ, Sch Elect Elect \& Comp Engn, Manila, Philippines.},
ISSN = {2475-7152},
ISBN = {978-1-5386-0912-5},
Keywords = {LiDAR; tree height; circumference at breast height; diameter at breast;
   leaf extraction},
Research-Areas = {Engineering; Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Engineering, Multidisciplinary; Engineering, Environmental; Nanoscience
   \& Nanotechnology},
Author-Email = {ahballado@mapua.edu.ph
   rggarcia@mapua.edu.ph
   jgzchichoco@mymail.mapua.edu.ph
   bmbdomingo@mymail.mapua.edu.ph
   kjmsantuyo@mymail.mapua.edu.ph
   vjssulmaca@mymail.mapua.edu.ph
   sapbentir@gmail.com
   shydelsarte@gmail.com},
Affiliations = {Mapua University},
ORCID-Numbers = {Ballado, Alejandro Jr./0000-0001-9929-5272
   Bentir, Sarah Alma/0009-0006-5414-2524
   garcia, ramon/0000-0003-3903-1499},
Funding-Acknowledgement = {Department of Science and Technology (DOST)},
Funding-Text = {This study is supported in part by the Department of Science and
   Technology (DOST) grant and with the assistance from the Mapua
   University and Philippine Council for Industry, Energy and Emerging
   Technology Research and Development (PCIEERD) under the Phil-LiDAR 2
   Program: National Resource Inventory of the Philippines Using LiDAR and
   other Remotely Sensed Data, headed by Dr. Ariel C. Blanco of UP-Diliman.},
Cited-References = {Bama B. S., 2011, IND J COMP SCI ENG, V2, P202.
   Blanco A. C., 2015, INT ARCH PHOTOGRAMME, VXL.
   Chaki J., 2011, INT J ADV COMPUTER S, V2.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   da Silva R., 2014, INT J ADV REMOTE SEN, V3, P506.
   Ibanez C. A. G., 2015, INT ARCH PHOTOGRAMME.
   Jabal MFAB, 2013, J COMPUT SCI-NETH, V9, P1295, DOI {[}DOI 10.3844/J.CSSP.2013.1295.1304, DOI 10.3844/JCSSP.2013.1295.1304].
   Knight D, 2010, AUTOMATIC PLANT LEAF.
   Kulkarni A.H., 2013, INT J ADV RES COMPUT, V2, P1.
   Lee K.-B., 2013, INT J BIOSCI BIOTECH, V5, P57, DOI {[}10.14257/ijbsbt.2013.5.5.06, DOI 10.14257/IJBSBT.2013.5.5.06, 10.3724/sp.j.1087.2009.01707, DOI 10.1007/978-94-007-5857-5\_12].
   PATIL S, 2013, INT J EMERGING TECHN, V3, P266.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {12},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BJ5OG},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000426116300015},
DA = {2023-08-12},
}

@article{ WOS:000613285700002,
Author = {Sindic, Caleb M. T. and Riday, Heathcliffe},
Title = {Using image object recognition to increase biomass in red clover
   (Trifolium pratense L.) breeding},
Journal = {CROP SCIENCE},
Year = {2020},
Volume = {60},
Number = {4},
Pages = {1770-1781},
Month = {JUL-AUG},
Abstract = {Phenotyping in forage legume breeding can be time consuming and resource
   intensive. With computation advances and computational power cost
   reductions, utilizing artificial intelligence in automated phenotyping
   is becoming feasible for even resource limited forage legume breeding
   programs. Here we report on the use of machine learning to train a
   neural network to identify and isolate red clover plants from digital
   images of space planted red clover nurseries. Challenges to red clover
   plant identification included: 1) plants were grown with a grass
   companion; and 2) plants were close enough to each other so that plants
   often overlapped each other in the digital images. To estimate biomass
   yield a second neural network was trained using machine learning to
   count leaves per plant among identified red clover plants from the plant
   classification neural network. The two neural networks were validated on
   six red clover digital image sets taken on red clover space plant
   nurseries. Average neural network plant classification success rates
   were measured at 94.6\% across the six digital image sets. Neural
   network red clover leaf counts were correlated with human visual biomass
   scores at r(2) = 0.528. We conclude that automated phenotyping based on
   digital image analysis of red clover breeding nurseries is currently
   feasible. We further conclude that additional phenotypic traits could be
   obtained on identified red clover plants from such images sets.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Riday, H (Corresponding Author), USDA ARS, US Dairy Forage Res Ctr, Madison, WI 53706 USA.
   Sindic, Caleb M. T.; Riday, Heathcliffe, USDA ARS, US Dairy Forage Res Ctr, Madison, WI 53706 USA.},
DOI = {10.1002/csc2.20028},
ISSN = {0011-183X},
EISSN = {1435-0653},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agronomy},
Author-Email = {Heathcliffe.Riday@ars.usda.gov},
Affiliations = {United States Department of Agriculture (USDA)},
Cited-References = {{[}Anonymous], 2016, SAS.
   ATKINS IM, 1969, AGRON J, V61, P88, DOI 10.2134/agronj1969.00021962006100010030x.
   Atkinson JA, 2018, ANN PLANT REV ONLINE, V1, P719, DOI 10.1002/9781119312994.apr0651.
   Campbell ZC, 2018, PHYTOCHEM REV, V17, P1329, DOI 10.1007/s11101-018-9585-x.
   Dias FM, 2004, ENG APPL ARTIF INTEL, V17, P945, DOI 10.1016/j.engappai.2004.08.011.
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI {[}10.1007/BF02478259, DOI 10.1007/BF02478259].
   Parloff R., 2016, FORTUNE.
   Riday H., 2010, INT J PLANT BREEDING, V4, P22.
   Riday H., 2013, BREEDING STRATEGIES, P21.
   Riday H, 2016, CROP SCI, V56, P2314, DOI 10.2135/cropsci2015.09.0573.
   Riday H, 2011, CROP SCI, V51, P631, DOI 10.2135/cropsci2010.07.0390.
   Riday H, 2009, EUPHYTICA, V170, P339, DOI 10.1007/s10681-009-9991-7.
   Rowley HA, 1998, PROC CVPR IEEE, P38, DOI 10.1109/CVPR.1998.698585.
   Sindic CMT, 2020, CROP SCI, V60, P1770, DOI 10.1002/csc2.20028.
   Taylor NL, 2008, CROP SCI, V48, P1, DOI 10.2135/cropsci2007.08.0446.
   Tzutalin, 2015, LABELIMG GIT CODE.},
Number-of-Cited-References = {16},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {8},
Journal-ISO = {Crop Sci.},
Doc-Delivery-Number = {QA2NS},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000613285700002},
DA = {2023-08-12},
}

@inproceedings{ WOS:000402055400198,
Author = {Sahay, Aparajita and Chen, Min},
Book-Group-Author = {IEEE},
Title = {Leaf Analysis for Plant Recognition},
Booktitle = {PROCEEDINGS OF 2016 IEEE 7TH INTERNATIONAL CONFERENCE ON SOFTWARE
   ENGINEERING AND SERVICE SCIENCE (ICSESS 2016)},
Series = {International Conference on Software Engineering and Service Science},
Year = {2016},
Pages = {914-917},
Note = {7th IEEE International Conference on Software Engineering and Service
   Science (ICSESS), China Hall Sci \& Technol, Beijing, PEOPLES R CHINA,
   AUG 26-28, 2016},
Organization = {Inst Elect \& Elect Engineers; IEEE Beijing Sect},
Abstract = {Plants are essential resources for nature and people's lives. Plant
   recognition provides valuable information for plant research and
   development, and has great impact on environmental protection and
   exploration. This paper presents a leaf analysis system for plant
   identification, which consists of three main components. First, given a
   leaf image, a preprocessing step is conducted for noise reduction.
   Second, the feature extraction component identifies representative
   features and computes scale invariant feature descriptors. Third, the
   matching plant species are identified and returned using a weighted
   K-nearest neighbor search algorithm. The system is implemented as a
   Windows phone app and is tested on the Leatsnapdataset{[}8], an
   electronic field guide developed by Columbia University and University
   of Maryland with different combinations of species at various
   orientations, scales and levels of brightness. The experimental results
   demonstrate the effectiveness of our proposed framework in plant
   recognition.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Sahay, A (Corresponding Author), Univ Washington Bothell, Comp \& Software Syst, Sch STEM, Bothell, WA 98011 USA.
   Sahay, Aparajita; Chen, Min, Univ Washington Bothell, Comp \& Software Syst, Sch STEM, Bothell, WA 98011 USA.},
ISSN = {2327-0594},
ISBN = {978-1-4673-9904-3},
Keywords = {plant recognition; leaf analysis; SIFT; KNN search},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Software Engineering},
Author-Email = {sahay87@uw.edu
   minchen2@uw.edu},
Affiliations = {University of Washington; University of Washington Bothell},
Cited-References = {{[}Anonymous], IJCAI 83.
   Aranda M. C., 2010, P ACM INT C IM VID R, P327, DOI {[}10.1145/1816041.1816089, DOI 10.1145/1816041.1816089].
   Arora A., 2012, CLEF ONL WORK NOT LA.
   Bama B. S., 2011, J COMPUTER SCI ENG, V2, P202.
   Gou J., 2012, J INFORM COMPUT SCI, V9, P1429.
   Grand-Brochier M., 2013, P ACM INT WORKSH VID.
   Gwo CY, 2013, APPL PLANT SCI, V1, DOI 10.3732/apps.1200005.
   Kadir A., 2015, GATE COMPUTER VISION, V1, P3, DOI DOI 10.15579/GTCVPR.0101.003007.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Li YF, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P885.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Manh AG, 2001, J AGR ENG RES, V80, P139, DOI 10.1006/jaer.2001.0725.
   Mouine S., 2012, P ACM INT C MULT RET.
   Prasad S., 2011, P 2011 INT C COMM CO, P343, DOI {[}10.1145/1947940.1948012, DOI 10.1145/1947940.1948012].
   Quan L, 2006, ACM T GRAPHIC, V25, P599, DOI 10.1145/1141911.1141929.
   Rui Y., 1996, P 1 INT WORKSH IM DA, P22.
   Shabanzade M., 2011, SIGNAL IMAGE PROCESS, V2, P23, DOI DOI 10.5121/sipij.2011.2303.
   Xia CL, 2013, BIOSYST ENG, V116, P23, DOI 10.1016/j.biosystemseng.2013.06.003.},
Number-of-Cited-References = {18},
Times-Cited = {7},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BH6RS},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000402055400198},
DA = {2023-08-12},
}

@article{ WOS:000770817300005,
Author = {Ghosh, Sukanta and Singh, Amar and Kavita and Jhanjhi, N. Z. and Masud,
   Mehedi and Aljahdali, Sultan},
Title = {SVM and KNN Based CNN Architectures for Plant Classification},
Journal = {CMC-COMPUTERS MATERIALS \& CONTINUA},
Year = {2022},
Volume = {71},
Number = {3},
Pages = {4257-4274},
Abstract = {Automatic plant classification through plant leaf is a classical problem
   in Computer Vision. Plants classification is challenging due to the
   introduction of new species with a similar pattern and look-a-like. Many
   efforts are made to automate plant classification using plant leaf,
   plant flower, bark, or stem. After much effort, it has been proven that
   leaf is the most reliable source for plant classification. But it is
   challenging to identify a plant with the help of leaf structure because
   plant leaf shows similarity in morphological variations, like sizes,
   textures, shapes, and venation. Therefore, it is required to normalize
   all plant leaves into the same size to get better performance.
   Convolutional Neural Networks (CNN) provides a fair amount of accuracy
   when leaves are classified using this approach. But the performance can
   be improved by classifying using the traditional approach after applying
   CNN. In this paper, two approaches, namely CNN + Support Vector Machine
   (SVM) and CNN + K-Nearest Neighbors (kNN) used on 3 datasets, namely
   LeafSnap dataset, Flavia Dataset, and MalayaKew Dataset. The datasets
   are augmented to take care all the possibilities. The assessments and
   correlations of the predetermined feature extractor models are given.
   CNN + kNN managed to reach maximum accuracy of 99.5\%, 97.4\%, and
   80.04\%, respectively, in the three datasets.},
Publisher = {TECH SCIENCE PRESS},
Address = {871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA},
Type = {Article},
Language = {English},
Affiliation = {Kavita (Corresponding Author), Chandigarh Univ, Dept Comp Sci \& Engn, Mohali, India.
   Ghosh, Sukanta; Singh, Amar, Lovely Profess Univ, Jalandhar 144005, Punjab, India.
   Kavita, Chandigarh Univ, Dept Comp Sci \& Engn, Mohali, India.
   Jhanjhi, N. Z., Tailors Univ, Sch Comp Sci \& Engn, Subang Jaya 47500, Malaysia.
   Masud, Mehedi; Aljahdali, Sultan, Taif Univ, Coll Comp \& Informat Technol, Dept Comp Sci, At Taif 21944, Saudi Arabia.},
DOI = {10.32604/cmc.2022.023414},
ISSN = {1546-2218},
EISSN = {1546-2226},
Keywords = {Plant leaf classification; artificial intelligence; SVM; kNN; deep
   learning; deep CNN; training epoch},
Research-Areas = {Computer Science; Materials Science},
Web-of-Science-Categories  = {Computer Science, Information Systems; Materials Science,
   Multidisciplinary},
Author-Email = {kavita@ieee.org},
Affiliations = {Lovely Professional University; Chandigarh University; Taif University},
ResearcherID-Numbers = {Jhanjhi, Prof Dr Noor Zaman/F-3051-2011
   Masud, Mehedi/AAZ-7022-2020
   ., Kavita/GPX-5065-2022
   Aljahdali, Sultan/AAH-9712-2019
   Singh, Amar/IAN-5693-2023
   ., Kavita/AAP-4316-2021
   ., Kavita/ACO-4421-2022
   },
ORCID-Numbers = {Jhanjhi, Prof Dr Noor Zaman/0000-0001-8116-4733
   Masud, Mehedi/0000-0001-6019-7245
   ., Kavita/0000-0001-5422-1659
   Aljahdali, Sultan/0000-0003-2615-8271
   Singh, Amar/0000-0002-5567-6521
   ., Kavita/0000-0001-5422-1659
   ., Kavita/0000-0001-5422-1659
   Ghosh, Sukanta/0000-0002-7715-7669},
Funding-Acknowledgement = {Taif University Researchers Supporting Project, Taif University, Taif,
   Saudi Arabia {[}TURSP-2020/10]; Taif University, Taif, Saudi Arabia},
Funding-Text = {The authors would like to thank for the support from Taif University
   Researchers Supporting Project number (TURSP-2020/10), Taif University,
   Taif, Saudi Arabia.},
Cited-References = {Abdullah, 2017, 2017 20TH INTERNATIONAL CONFERENCE OF COMPUTER AND INFORMATION TECHNOLOGY (ICCIT).
   Abouelnaga Y, 2016, 2016 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE \& COMPUTATIONAL INTELLIGENCE (CSCI), P1192, DOI {[}10.1109/CSCI.2016.0225, 10.1109/CSCI.2016.224].
   Ahlawat S, 2020, PROCEDIA COMPUT SCI, V167, P2554, DOI 10.1016/j.procs.2020.03.309.
   Amlekar M. M., 2019, ADV INTELLIGENT SYST, V839.
   Bharate AA, 2017, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT SUSTAINABLE SYSTEMS (ICISS 2017), P103, DOI 10.1109/ISS1.2017.8389326.
   Chouhan Siddharth Singh, 2019, 2019 4th International Conference on Information Systems and Computer Networks (ISCON), P700, DOI 10.1109/ISCON47742.2019.9036158.
   Gadekallu TR, 2021, J REAL-TIME IMAGE PR, V18, P1383, DOI 10.1007/s11554-020-00987-8.
   Ghosh S., 2020, J PHYS C SER, V1531.
   Ghosh S., 2021, MACHINE LEARNING DAT, V1.
   Haug S, 2014, IEEE WINT CONF APPL, P1142, DOI 10.1109/WACV.2014.6835733.
   Jiang HX, 2020, IEEE ACCESS, V8, P68828, DOI 10.1109/ACCESS.2020.2986946.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6.
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Marzougui Fatma, 2020, INT ARAB CONF INF TE.
   Papp D., 2016, P CROSS LANG EV FOR, P659.
   Sachar S., 2017, IOP C SERIES, P1022.
   Seeland M, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-018-2474-x.
   Singh A, 2019, J MULT-VALUED LOG S, V32, P407.
   Singh A, 2020, INT J AMBIENT ENERGY, DOI 10.1080/01430750.2020.1789741.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Tropea M., 2019, P 2019 IEEE ACM 23 I, P1, DOI DOI 10.1109/CCNC.2019.8651744.
   Wang B, 2019, IEEE ACCESS, V7, P151754, DOI 10.1109/ACCESS.2019.2947510.
   Xu B., 2021, PROC INT C INFORM AU, P795.
   Zhiyu Liu, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9226, P115, DOI 10.1007/978-3-319-22186-1\_11.},
Number-of-Cited-References = {25},
Times-Cited = {9},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {19},
Journal-ISO = {CMC-Comput. Mat. Contin.},
Doc-Delivery-Number = {ZV9AS},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000770817300005},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000719380900060,
Author = {Chavan, Shruti and Ford, John and Yu, Xinrui and Saniie, Jafar},
Book-Group-Author = {IEEE},
Title = {Plant Species Image Recognition using Artificial Intelligence on Jetson
   Nano Computational Platform},
Booktitle = {2021 IEEE INTERNATIONAL CONFERENCE ON ELECTRO INFORMATION TECHNOLOGY
   (EIT)},
Year = {2021},
Pages = {350-354},
Note = {IEEE International Conference on Electro Information Technology (EIT),
   Mount Pleasant, MI, MAY 14-15, 2021},
Organization = {IEEE},
Abstract = {The ongoing research for plant/animal species identification by computer
   vision engineers is exciting and vast. This paper describes a deep
   learning approach to identify plant species using image analysis. An
   efficient Artificial Intelligence System is designed and implemented
   with minimal components, including a camera and Jetson Nano
   (single-board embedded computing device). Convolutional Neural Networks
   are trained to capture the features from images and recognize the plant
   species. Thus, the experiment used, in particular, CNN
   architectures-AlexNet, ResNet50, and MobileNetv2, within Python's
   Tensorflow framework, to accomplish species identification. Of these,
   AlexNet provided the best results, with 72\% validation accuracy after
   15 epochs. A portion of the LeafSnap dataset, containing 15 plant
   species and 30 images per species, was used to compare the performance
   of architectures.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Chavan, S (Corresponding Author), IIT, Dept Elect \& Comp Engn, Embedded Comp \& Signal Proc Res Lab, Chicago, IL 60616 USA.
   Chavan, Shruti; Ford, John; Yu, Xinrui; Saniie, Jafar, IIT, Dept Elect \& Comp Engn, Embedded Comp \& Signal Proc Res Lab, Chicago, IL 60616 USA.},
DOI = {10.1109/EIT51626.2021.9491893},
ISBN = {978-1-6654-1846-1},
Keywords = {convolutional neural network; plant identification; Jetson Nano;
   LeafSnap dataset},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Affiliations = {Illinois Institute of Technology},
Cited-References = {{[}Anonymous], SWEDISH LEAF DATASET.
   {[}Anonymous], LEAFSNAP DATASET.
   {[}Anonymous], IMAGE CLASSIFICATION.
   {[}Anonymous], NVIDIA JETSON NANO.
   {[}Anonymous], FLAVIA DATASET.
   Sungbin Choi, 2015, CLEF WORKING NOTES.
   Varghese B. K., 2020, 2020 4 INT C COMP ME, P800.
   Xiao QG, 2018, ECOL INFORM, V48, P117, DOI 10.1016/j.ecoinf.2018.09.001.
   Yalcin H, 2016, INT CONF AGRO-GEOINF, P233.},
Number-of-Cited-References = {9},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {6},
Doc-Delivery-Number = {BS4HT},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000719380900060},
DA = {2023-08-12},
}

@article{ WOS:000175378800001,
Author = {Hemming, J and Rath, T},
Title = {Image processing for plant determination using the hough transform and
   clustering methods},
Journal = {GARTENBAUWISSENSCHAFT},
Year = {2002},
Volume = {67},
Number = {1},
Pages = {1-10},
Month = {JAN-FEB},
Abstract = {One important goal of future crop production is to reduce the usage of
   herbicides. In many circumstances automatic detection and determination
   of weeds and crops is necessary for feasible weed control. Previous
   approaches in the area of computer vision based plant classification in
   a natural environment have only limited success, especially under
   variable field conditions with overlapping plants.
   The objective of this study was to apply a technique to cluster objects
   classified by a computer vision system in order to recognise not only
   single leaves but also whole plants.
   Images of the crop were obtained using a device that blocked out the
   natural light to provide controlled artificial lighting conditions. The
   crop rows were calculated from the determined plant positions. Plants
   which were not located in the row were labelled as weeds. It has been
   investigated, whether information on the row position can be used to
   reduce the classification errors.
   With this approach plant classification under field conditions can be
   improved and a plant classification accuracy of over 90 \% for cabbage
   and 70 \% for carrots has been obtained. The results clearly depended on
   the growing stage of the crop, on specific parameters of the row
   identification process and on the question whether the type I error (not
   identified crops) or the type 2 error (weed classified as crop) should
   be minimised.},
Publisher = {EUGEN ULMER GMBH CO},
Address = {POSTFACH 700561 WOLLGRASWEG 41, D-70599 STUTTGART, GERMANY},
Type = {Article},
Language = {English},
Affiliation = {Hemming, J (Corresponding Author), IMAG, Inst Agr \& Environm Engn, POB 43, NL-6700 AA Wageningen, Netherlands.
   IMAG, Inst Agr \& Environm Engn, NL-6700 AA Wageningen, Netherlands.
   Leibniz Univ Hannover, Inst Hort \& Agr Engn, D-30419 Hannover, Germany.},
ISSN = {0016-478X},
Keywords-Plus = {GUIDANCE; WEEDS},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Horticulture},
Author-Email = {j.hemming@imag.wag-ur.nl},
Affiliations = {Leibniz University Hannover},
ResearcherID-Numbers = {Rath, Thomas/A-4708-2013},
Cited-References = {Billingsley J, 1997, COMPUT ELECTRON AGR, V16, P147, DOI 10.1016/S0168-1699(96)00034-8.
   CASTELMAN KR, 1996, DIGITAL IMAGE PROCES.
   HABERACKER P, 1987, DIGITALE BILDVERARBE.
   HAUSLER A, 1998, SONDERH, V16, P249.
   Hemming J, 2001, J AGR ENG RES, V78, P233, DOI 10.1006/jaer.2000.0639.
   HEMMING J, 2000, THESIS U HANOVER GER, V50.
   Hough P.V.C., 1962, U.S. Patent, Patent No. 3,069,654.
   Marchant J. A., 1995, Real-Time Imaging, V1, P363, DOI 10.1006/rtim.1995.1036.
   Meyer GE, 1998, T ASAE, V41, P1189, DOI 10.13031/2013.17244.
   {*}MVTEC, 1998, HALCON REF MAN.
   Perez AJ, 2000, COMPUT ELECTRON AGR, V25, P197, DOI 10.1016/S0168-1699(99)00068-X.
   PETRY W, 1989, J AGRON CROP SCI, V163, P345, DOI 10.1111/j.1439-037X.1989.tb00777.x.
   Pla F, 1997, IMAGE VISION COMPUT, V15, P465, DOI 10.1016/S0262-8856(96)01147-X.
   RATH T, 1997, METHODEN COMPUTERBIL, V42.
   Sanchiz JM, 1998, REAL-TIME IMAGING, V4, P55, DOI 10.1006/rtim.1996.0063.
   SOKEFELD M, 1996, AUTOMATISCHE ERKENNU.
   Sonka M., 1993, IMAGE PROCESSING ANA.
   Tian L., 1993, 933608 ASAE.
   Tillett ND, 1999, J AGR ENG RES, V74, P225, DOI 10.1006/jaer.1999.0458.
   WOEBBECKE DM, 1995, T ASAE, V38, P271, DOI 10.13031/2013.27839.},
Number-of-Cited-References = {20},
Times-Cited = {7},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Journal-ISO = {Gartenbauwissenschaft},
Doc-Delivery-Number = {548FW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000175378800001},
DA = {2023-08-12},
}

@article{ WOS:000875499900001,
Author = {Ayumi, Vina and Ermatita, Ermatita and Abdiansah, Abdiansah and
   Noprisson, Handrie and Jumaryadi, Yuwan and Purba, Mariana and Utami,
   Marissa and Putra, Erwin Dwika},
Title = {Transfer Learning for Medicinal Plant Leaves Recognition: A Comparison
   with and without a Fine-Tuning Strategy},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
Year = {2022},
Volume = {13},
Number = {9},
Pages = {138-144},
Month = {SEP},
Abstract = {leaves are another common source of information for determining plant
   species. According to the dataset that has been collected, we propose
   transfer learning models VGG16, VGG19, and MobileNetV2 to examine the
   distinguishing features to identify medicinal plant leaves. We also
   improved algorithm using fine-tuning strategy and analyzed a comparison
   with and without a fine-tuning strategy to transfer learning models
   performance. Several protocols or steps were used to conduct this study,
   including data collection, data preparation, feature extraction,
   classification, and evaluation. The distribution of training and
   validation data is 80\% for training data and 20\% for validation data,
   with 1500 images of thirty species. The testing data consisted of a
   total of 43 images of 30 species. Each species class consists of 1-3
   images. With a validation accuracy of 96.02 percent, MobileNetV2 with
   finetuning had the best validation accuracy. MobileNetV2 with finetuning
   also had the best testing accuracy of 81.82\%.},
Publisher = {SCIENCE \& INFORMATION SAI ORGANIZATION LTD},
Address = {19 BOLLING RD, BRADFORD, WEST YORKSHIRE, 00000, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Ayumi, V (Corresponding Author), Univ Sriwijaya, Engn, Palembang, Indonesia.
   Ayumi, Vina, Univ Sriwijaya, Engn, Palembang, Indonesia.
   Ermatita, Ermatita; Abdiansah, Abdiansah, Univ Sriwijaya, Fac Comp Sci, Palembang, Indonesia.
   Noprisson, Handrie; Jumaryadi, Yuwan, Univ Mercu Buana, Fac Comp Sci, Jakarta, Indonesia.
   Utami, Marissa, Univ Sjakhyakirti, Program Informat, Palembang, Indonesia.
   Utami, Marissa; Putra, Erwin Dwika, Univ Muhammadiyah Bengkulu, Fac Engn, Bengkulu, Indonesia.},
DOI = {10.17577/IJERTV11IS090062},
ISSN = {2158-107X},
EISSN = {2156-5570},
Keywords = {-Medicinal leaf plant; transfer learning; deep learning; phytomedicine},
Keywords-Plus = {CONVOLUTIONAL NEURAL-NETWORKS; CLASSIFICATION; IMAGES},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Affiliations = {Universitas Sriwijaya; Universitas Sriwijaya; Universitas Mercu Buana},
ResearcherID-Numbers = {Jumaryadi, Yuwan/AAH-9040-2019
   },
ORCID-Numbers = {Jumaryadi, Yuwan/0000-0002-5757-6321
   PURBA, MARIANA/0000-0001-8423-6034},
Funding-Acknowledgement = {Universitas Sriwijaya; Universitas Mercu Buana},
Funding-Text = {The authors would like to thank Universitas Sriwijaya and Universitas
   Mercu Buana that have been supported this research.},
Cited-References = {Ayumi Vina, 2021, 2021 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS), P40, DOI 10.1109/ICIMCIS53775.2021.9699363.
   Ayumi V., 2021, INT J SCI RES SCI EN, V7, P241.
   Ayumi V., 2020, INT J SCI RES SCI EN, V7, P204.
   Ayumi V., 2019, INT J SCI RES COMPUT, P48.
   Ayumi V, 2016, IEEE ST CONF RES DEV.
   Azlah MAF, 2019, COMPUTERS, V8, DOI 10.3390/computers8040077.
   Bhakta I., 2022, INT C COMPUTATIONAL, P345.
   Bharali P, 2019, COMM COM INF SC, V1025, P194, DOI 10.1007/978-981-15-1384-8\_16.
   Bhattacharya Shreyasi, 2020, Intelligence Enabled Research. DoSIER 2019. Advances in Intelligent Systems and Computing (AISC 1109), P61, DOI 10.1007/978-981-15-2021-1\_8.
   Bhuiyan Md Rafiuzzaman, 2021, Soft Computing Techniques and Applications. Proceedings of the International Conference on Computing and Communication (IC3 2020). Advances in Intelligent Systems and Computing (AISC 1248), P371, DOI 10.1007/978-981-15-7394-1\_35.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Defriani M., 2022, J COMPUT NETW ARCHIT, V4, P104.
   Gao LW, 2018, COMPUT ELECTRON AGR, V155, P426, DOI 10.1016/j.compag.2018.10.020.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Ghouse F., 2022, INT J INTELL ENG SYS, V15, P55.
   Hidayat E, 2016, IEEE ST CONF RES DEV.
   Jawad E. M., 2022, INT J INTELL ENG SYS, V15, P12.
   Jumaryadi Yuwan, 2020, 2020 2nd International Conference on Broadband Communications, Wireless Sensors and Powering (BCWSP), P59, DOI 10.1109/BCWSP50066.2020.9249463.
   Kalathingal MSH, 2020, J FOOD PROCESS ENG, V43, DOI 10.1111/jfpe.13128.
   Karadal CH, 2021, EXPERT SYST APPL, V185, DOI 10.1016/j.eswa.2021.115659.
   Karothia Rajeev, 2022, Proceedings of International Conference on Data Science and Applications: ICDSA 2021. Lecture Notes in Networks and Systems (288), P131, DOI 10.1007/978-981-16-5120-5\_11.
   Korot E, 2021, NAT MACH INTELL, V3, P288, DOI 10.1038/s42256-021-00305-2.
   Lu JZ, 2021, AGRICULTURE-BASEL, V11, DOI 10.3390/agriculture11080707.
   Maslej-Kresnakova V, 2021, MON NOT R ASTRON SOC, V505, P1464, DOI 10.1093/mnras/stab1400.
   Miaomiao Ji, 2020, Information Processing in Agriculture, V7, P418, DOI 10.1016/j.inpa.2019.10.003.
   Mishra Sushruta, 2021, Intelligent and Cloud Computing. Proceedings of ICICC 2019. Smart Innovation, Systems and Technologies (SIST 153), P375, DOI 10.1007/978-981-15-6202-0\_38.
   Mukherjee G, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON CONTROL, INSTRUMENTATION, ENERGY \& COMMUNICATION (CIEC), P98, DOI 10.1109/CIEC.2016.7513746.
   Naresh YG, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P524, DOI 10.1109/ACPR.2015.7486558.
   Noprisson H., 2016, 2016 8 INT C INF TEC.
   Pandian JA, 2019, INT CONF ADV COMPU, P199, DOI 10.1109/IACC48062.2019.8971580.
   Pechebovicz D, 2020, IEEE INT CONF INDUST, P674, DOI 10.1109/ICIT45562.2020.9067289.
   Huu PN, 2022, IEEE ACCESS, V10, P1839, DOI 10.1109/ACCESS.2021.3138778.
   Putri Yuanita A., 2021, Journal of Physics: Conference Series, V1845, DOI 10.1088/1742-6596/1845/1/012026.
   Ranggadara Indra, 2020, Journal of Physics: Conference Series, V1477, DOI 10.1088/1742-6596/1477/3/032019.
   RAO RM, 2004, ADV IMAGE PROCESSING, P51.
   Roopashree S., 2020, MENDELEY DATA, V1.
   Sensuse D. I., 2017, INT J ADV SCI ENG IN, V7.
   Shailendra R, 2022, NEURAL PROCESS LETT, V54, P4465, DOI 10.1007/s11063-022-10818-5.
   Sivaranjani C., 2019, 2019 INT C COMPUTATI, P1.
   Syarief M, 2020, TELKOMNIKA TELECOMMU, V18, P1376.
   Thangaraj R, 2021, J PLANT DIS PROTECT, V128, P73, DOI 10.1007/s41348-020-00403-0.
   Venkataraman D., 2017, 2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI), P793, DOI 10.1109/ICACCI.2017.8125939.
   Wagle SA, 2022, PLANTS-BASEL, V11, DOI 10.3390/plants11010024.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Widiyanto S, 2019, 2019 4 INT C INF COM, P1, DOI {[}10.1109/ICIC47613.2019.8985909, DOI 10.1109/ICIC47613.2019.8985909].
   Yang KL, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10111721.
   Yen HH, 2021, J MED BIOL ENG, V41, P504, DOI 10.1007/s40846-021-00608-0.},
Number-of-Cited-References = {47},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {4},
Journal-ISO = {Int. J. Adv. Comput. Sci. Appl.},
Doc-Delivery-Number = {5S9KR},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000875499900001},
DA = {2023-08-12},
}

@article{ WOS:000525545900139,
Author = {Wang, Xuan and Du, Weikang and Guo, Fangxia and Hu, Simin},
Title = {Leaf Recognition Based on Elliptical Half Gabor and Maximum Gap Local
   Line Direction Pattern},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {39175-39183},
Abstract = {Plant identification via leaf images is very meaningful to agricultural
   information. The existing methods were based on one or two kinds of the
   three distinct characteristics in leaf images including leaf contours,
   textures and veins. This limits their recognition performance and scope
   of application. This paper describes a novel counting-based leaf
   recognition method, which can directly and effectively combine all of
   the three kinds of significant characteristics in leaf images. In order
   to obtain the stable and independent local line responses from leaf
   contour, texture and vein, elliptical half Gabor is introduced and
   convoluted with the raw grayscale leaf images, and then maximum gap
   local line direction patterns are extracted from the local line
   responses and normalized in direction by cyclically right shifting these
   patterns until the most numerous bit plane with a value of 1 to the left
   bit. The histogram of the normalized patterns is calculated and regarded
   as the counting-based local structure descriptor, and support vector
   machine is utilized as the classifier. Experimental results on three
   frequently used leaf databases show that the proposed approach yields a
   better performance in terms of the classification accuracy,
   applicability and feasibility in comparison with the state of the art
   methods.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Wang, X (Corresponding Author), Shaanxi Normal Univ, Sch Phys \& Informat Technol, Xian 710062, Peoples R China.
   Wang, Xuan; Du, Weikang; Guo, Fangxia; Hu, Simin, Shaanxi Normal Univ, Sch Phys \& Informat Technol, Xian 710062, Peoples R China.},
DOI = {10.1109/ACCESS.2020.2976117},
ISSN = {2169-3536},
Keywords = {Shape; Feature extraction; Support vector machines; Veins; Image
   recognition; Histograms; Plants (biology); Counting-based descriptor;
   elliptical half-Gabor filters; maximum gap local line direction pattern;
   plant identification; support vector machine},
Keywords-Plus = {IDENTIFICATION; CLASSIFICATION; EXTRACTION; SCALE; FACE},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {wxuan@snnu.edu.cn},
Affiliations = {Shaanxi Normal University},
ORCID-Numbers = {wang, xuan/0000-0001-9084-532X},
Funding-Acknowledgement = {Major Program of National Natural Science Foundation of China
   {[}60633020]; National Natural Science Foundation of China {[}61703256];
   Natural Science Basic Research Plan in Shaanxi Province of China
   {[}2017JQ6070]},
Funding-Text = {This work was supported in part by the Major Program of National Natural
   Science Foundation of China under Grant 60633020, in part by the
   National Natural Science Foundation of China under Grant 61703256, and
   in part by the Natural Science Basic Research Plan in Shaanxi Province
   of China under Grant 2017JQ6070.},
Cited-References = {Ahmed F, 2012, ELECTRON LETT, V48, P1203, DOI 10.1049/el.2012.1841.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Casanova D, 2009, INT J IMAG SYST TECH, V19, P236, DOI 10.1002/ima.20201.
   Cerutti G, 2014, PATTERN RECOGN LETT, V49, P177, DOI 10.1016/j.patrec.2014.07.016.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Fei LK, 2016, PATTERN RECOGN LETT, V69, P35, DOI 10.1016/j.patrec.2015.10.003.
   Fu H, 2006, IEE P-VIS IMAGE SIGN, V153, P881, DOI 10.1049/ip-vis:20060061.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Jabid T, 2010, ETRI J, V32, P784, DOI 10.4218/etrij.10.1510.0132.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Liu JD, 2009, LECT NOTES COMPUT SC, V5754, P253.
   Luo YT, 2016, PATTERN RECOGN, V50, P26, DOI 10.1016/j.patcog.2015.08.025.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Park J, 2008, J SYST SOFTWARE, V81, P71, DOI 10.1016/j.jss.2007.05.001.
   Rivera AR, 2013, IEEE T IMAGE PROCESS, V22, P1740, DOI 10.1109/TIP.2012.2235848.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Vapnik V. N., 1995, CONSTRUCTING LEARNIN, P119, DOI {[}10.1007/978-1-4757-2440-0\_6, DOI 10.1007/978-1-4757-2440-0\_6].
   Wang X, 2019, PATTERN RECOGN LETT, V128, P244, DOI 10.1016/j.patrec.2019.09.008.
   Wang X, 2014, DIGIT SIGNAL PROCESS, V34, P101, DOI 10.1016/j.dsp.2014.08.005.
   Wang X, 2012, KNOWL-BASED SYST, V27, P451, DOI 10.1016/j.knosys.2011.10.008.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Wei Hong-Wei, 2011, Proceedings of the 2011 International Conference on Computer Science and Network Technology (ICCSNT), P560, DOI 10.1109/ICCSNT.2011.6182020.
   Yun C, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION MANAGEMENT, INNOVATION MANAGEMENT AND INDUSTRIAL ENGINEERING, VOL 3, PROCEEDINGS, P110, DOI 10.1109/ICIII.2009.336.
   Zhang SW, 2013, PATTERN RECOGN, V46, P1891, DOI 10.1016/j.patcog.2013.01.015.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhiyu Liu, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9226, P115, DOI 10.1007/978-3-319-22186-1\_11.
   Zhong FJ, 2013, NEUROCOMPUTING, V119, P375, DOI 10.1016/j.neucom.2013.03.020.},
Number-of-Cited-References = {34},
Times-Cited = {14},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {6},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {LC7XF},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000525545900139},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000528677800064,
Author = {Dileep, M. R. and Pournami, P. N.},
Book-Group-Author = {IEEE},
Title = {AyurLeaf: A Deep Learning Approach for Classification of Medicinal
   Plants},
Booktitle = {PROCEEDINGS OF THE 2019 IEEE REGION 10 CONFERENCE (TENCON 2019):
   TECHNOLOGY, KNOWLEDGE, AND SOCIETY},
Series = {TENCON IEEE Region 10 Conference Proceedings},
Year = {2019},
Pages = {321-325},
Note = {IEEE Region 10 Conference on Technology, Knowledge, and Society
   (TENCON), Kochi, INDIA, OCT 17-20, 2019},
Organization = {IEEE Kerala Sect; IEEE Humanitarian Activities Comm; Nissan Digital;
   Nest; Kerala State IT Miss; Cochin Shipyard Ltd; Terumo Penpol; Oracle
   Acad; InApp; Natl Instruments; Mentor; UST Global; I3; ICFOSS; Kerala
   Startup Miss; IEEE Reg 10},
Abstract = {Ayurvedic medicines have a vital role in preserving physical and mental
   health of human beings. Identification and classification of medicinal
   plants are essential for better treatment. Lack of experts in this field
   makes proper identification and classification of medicinal plants a
   tedious task. Hence, a fully automated system for medicinal plant
   classification is highly desirable. This work proposes AyurLeaf, a Deep
   Learning based Convolutional Neural Network (CNN) model, to classify
   medicinal plants using leaf features such as shape, size, color, texture
   etc. This research work also proposes a standard dataset for medicinal
   plants, commonly seen in various regions of Kerala, the state on
   southwestern coast of India. The proposed dataset contains leaf samples
   from 40 medicinal plants. A deep neural network inspired from Alexnet is
   utilised for the efficient feature extraction from the dataset. Finally,
   the classification is performed using Softmax and SVM classifiers. Our
   model, upon 5-cross validation, achieved a classification accuracy of
   96.76\% on AyurLeaf dataset. AyurLeaf helps us to preserve the
   traditional medicinal knowledge carried by our ancestors and provides an
   easy way to identify and classify medicinal plants.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Dileep, MR (Corresponding Author), Natl Inst Technol Calicut, Dept Comp Sci \& Engn, Kattangal, Kerala, India.
   Dileep, M. R.; Pournami, P. N., Natl Inst Technol Calicut, Dept Comp Sci \& Engn, Kattangal, Kerala, India.},
ISSN = {2159-3442},
ISBN = {978-1-7281-1895-6},
Keywords = {Deep Learning; Convolutional Neural Network; Classification; Medicinal
   plant; Leaf features},
Keywords-Plus = {IDENTIFICATION; EXTRACTION},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {dileeepmr@gmail.com
   pournamipn@nitc.ac.in},
Affiliations = {National Institute of Technology (NIT System); National Institute of
   Technology Calicut},
Cited-References = {Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Gopal A, 2012, 2012 INTERNATIONAL CONFERENCE ON MACHINE VISION AND IMAGE PROCESSING (MVIP), P5, DOI 10.1109/MVIP.2012.6428747.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Janani R., 2013, 2013 International Conference on Advanced Electronic Systems (ICAES), P238, DOI 10.1109/ICAES.2013.6659400.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Manojkumar P, 2017, 2017 THIRD IEEE INTERNATIONAL CONFERENCE ON RESEARCH IN COMPUTATIONAL INTELLIGENCE AND COMMUNICATION NETWORKS (ICRCICN), P231, DOI 10.1109/ICRCICN.2017.8234512.
   Pavaloiu IB, 2017, E-HEALTH BIOENG CONF, P599, DOI 10.1109/EHB.2017.7995495.
   Prasad S, 2017, TENCON IEEE REGION, P2722, DOI 10.1109/TENCON.2017.8228324.
   Sabu A, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P574.
   Salima A, 2015, INT C ADV COMP SCI I, P275, DOI 10.1109/ICACSIS.2015.7415152.
   Tan J. W., 2018, DEEP LEARNING PLANT, P1.
   Venkataraman D, 2016, IEEE I C COMP INT CO, P1000.},
Number-of-Cited-References = {12},
Times-Cited = {15},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BO8NC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000528677800064},
DA = {2023-08-12},
}

@article{ WOS:000479446700001,
Author = {Gai, Jingyao and Tang, Lie and Steward, Brian L.},
Title = {Automated crop plant detection based on the fusion of color and depth
   images for robotic weed control},
Journal = {JOURNAL OF FIELD ROBOTICS},
Year = {2020},
Volume = {37},
Number = {1},
Pages = {35-52},
Month = {JAN},
Abstract = {Robotic weeding enables weed control near or within crop rows
   automatically, precisely and effectively. A computer-vision system was
   developed for detecting crop plants at different growth stages for
   robotic weed control. Fusion of color images and depth images was
   investigated as a means of enhancing the detection accuracy of crop
   plants under conditions of high weed population. In-field images of
   broccoli and lettuce were acquired 3-27 days after transplanting with a
   Kinect v2 sensor. The image processing pipeline included data
   preprocessing, vegetation pixel segmentation, plant extraction, feature
   extraction, feature-based localization refinement, and crop plant
   classification. For the detection of broccoli and lettuce, the
   color-depth fusion algorithm produced high true-positive detection rates
   (91.7\% and 90.8\%, respectively) and low average false discovery rates
   (1.1\% and 4.0\%, respectively). Mean absolute localization errors of
   the crop plant stems were 26.8 and 7.4 mm for broccoli and lettuce,
   respectively. The fusion of color and depth was proved beneficial to the
   segmentation of crop plants from background, which improved the average
   segmentation success rates from 87.2\% (depth-based) and 76.4\%
   (color-based) to 96.6\% for broccoli, and from 74.2\% (depth-based) and
   81.2\% (color-based) to 92.4\% for lettuce, respectively. The
   fusion-based algorithm had reduced performance in detecting crop plants
   at early growth stages.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Tang, L (Corresponding Author), Iowa State Univ, Agr \& Biosyst Engn, Ames, IA 50011 USA.
   Gai, Jingyao; Tang, Lie; Steward, Brian L., Iowa State Univ, Agr \& Biosyst Engn, Ames, IA 50011 USA.},
DOI = {10.1002/rob.21897},
EarlyAccessDate = {JUL 2019},
ISSN = {1556-4959},
EISSN = {1556-4967},
Keywords = {computer vision; crop detection; robotic weeding; sensor fusion},
Keywords-Plus = {CLASSIFICATION; IDENTIFICATION; SEGMENTATION; ALGORITHM; SPACE},
Research-Areas = {Robotics},
Web-of-Science-Categories  = {Robotics},
Author-Email = {lietang@iastate.edu},
Affiliations = {Iowa State University},
ORCID-Numbers = {Gai, Jingyao/0000-0002-1330-0454},
Funding-Acknowledgement = {National Institute of Food and Agriculture {[}20136702121126]; Leopold
   Center for Sustainable Agriculture {[}M2009-23, M2012-24]},
Funding-Text = {National Institute of Food and Agriculture, Grant/Award Number:
   20136702121126; Leopold Center for Sustainable Agriculture, Grant/Award
   Numbers: M2009-23, M2012-24},
Cited-References = {Andujar D, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16070972.
   Andujar D, 2012, SENSORS-BASEL, V12, P17343, DOI 10.3390/s121217343.
   {[}Anonymous], 2009, PRECIS AGRIC, DOI {[}DOI 10.1016/S0065-7743(08)60047-0, DOI 10.3920/978-90-8686-664-9].
   {[}Anonymous], 2018, DEEP LEARNING CRITIC.
   Bawden O, 2017, J FIELD ROBOT, V34, P1179, DOI 10.1002/rob.21727.
   CHEN YS, 1988, PATTERN RECOGN LETT, V7, P99, DOI 10.1016/0167-8655(88)90124-9.
   Corti A, 2016, ROBOT AUTON SYST, V75, P584, DOI 10.1016/j.robot.2015.09.024.
   Di Girolamo S, 2015, PROCEEDINGS 2015 IEEE 23RD ANNUAL SYMPOSIUM ON HIGH-PERFORMANCE INTERCONNECTS - HOTI 2015, P26, DOI 10.1109/HOTI.2015.21.
   Dyrmann M., 2017, ADV ANIM BIOSCI, V8, P842, DOI {[}DOI 10.1017/S2040470017000206, 10.1017/S2040470017000206].
   Dyrmann M, 2018, J FIELD ROBOT, V35, P202, DOI 10.1002/rob.21734.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Fankhauser P, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P388, DOI 10.1109/ICAR.2015.7251485.
   Ferreira AD, 2017, COMPUT ELECTRON AGR, V143, P314, DOI 10.1016/j.compag.2017.10.027.
   Finlayson GD, 2002, LECT NOTES COMPUT SC, V2353, P823.
   Foglia MM, 2006, J FIELD ROBOT, V23, P363, DOI 10.1002/rob.20131.
   Gai J., 2016, THESIS.
   Gerhards R, 2003, WEED RES, V43, P385, DOI 10.1046/j.1365-3180.2003.00349.x.
   Hamuda E, 2017, COMPUT ELECTRON AGR, V133, P97, DOI 10.1016/j.compag.2016.11.021.
   Herrera CD, 2012, IEEE T PATTERN ANAL, V34, P2058, DOI 10.1109/TPAMI.2012.125.
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.
   Jin J, 2009, J FIELD ROBOT, V26, P591, DOI 10.1002/rob.20293.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Kise M, 2005, BIOSYST ENG, V90, P357, DOI 10.1016/j.biosystemseng.2004.12.008.
   Kusumam K, 2017, J FIELD ROBOT, V34, P1505, DOI 10.1002/rob.21726.
   Li J, 2018, J FIELD ROBOT, V35, P596, DOI 10.1002/rob.21763.
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
   Lottes P, 2017, J FIELD ROBOT, V34, P1160, DOI 10.1002/rob.21675.
   McCool C, 2017, IEEE ROBOT AUTOM LET, V2, P1344, DOI 10.1109/LRA.2017.2667039.
   Milioto A, 2017, ISPRS ANN PHOTO REM, V4-2, P41, DOI 10.5194/isprs-annals-IV-2-W3-41-2017.
   Pannacci E, 2017, CROP PROT, V96, P44, DOI 10.1016/j.cropro.2017.01.012.
   Philipp I, 2002, COMPUT ELECTRON AGR, V35, P1, DOI 10.1016/S0168-1699(02)00050-9.
   Piron A, 2011, PRECIS AGRIC, V12, P607, DOI 10.1007/s11119-010-9205-2.
   Potena C, 2017, ADV INTELL SYST COMP, V531, P105, DOI 10.1007/978-3-319-48036-7\_9.
   Sa I, 2017, IEEE ROBOT AUTOM LET, V2, P765, DOI 10.1109/LRA.2017.2651952.
   Shafarenko L, 1997, IEEE T IMAGE PROCESS, V6, P1530, DOI 10.1109/83.641413.
   Slaughter DC, 2008, COMPUT ELECTRON AGR, V61, P63, DOI 10.1016/j.compag.2007.05.008.
   Szegedy C., 2015, P IEEE COMPUTER SOC, DOI 10.1109/CVPR.2015.7298594.
   Szegedy C., RETHINKING INCEPTION.
   Tang L, 2008, T ASABE, V51, P2181, DOI 10.13031/2013.25381.
   Tang L, 2000, T ASAE, V43, P1019, DOI 10.13031/2013.2970.
   Tang L, 2003, T ASAE, V46, P1247, DOI 10.13031/2013.13944.
   Nguyen TT, 2016, BIOSYST ENG, V146, P33, DOI 10.1016/j.biosystemseng.2016.01.007.
   Tillett ND, 2008, BIOSYST ENG, V99, P171, DOI 10.1016/j.biosystemseng.2007.09.026.
   Tippetts B, 2016, J REAL-TIME IMAGE PR, V11, P5, DOI 10.1007/s11554-012-0313-2.
   USDA, 2016, 2015 CERT ORG SURV.
   USDA, 2017, 2016 CERT ORG SURV.
   van der Weide RY, 2008, WEED RES, V48, P215, DOI 10.1111/j.1365-3180.2008.00629.x.
   Weiss U., 2010, 2010 Ninth International Conference on Machine Learning and Applications (ICMLA 2010), P339, DOI 10.1109/ICMLA.2010.57.
   Weiss U, 2011, ROBOT AUTON SYST, V59, P265, DOI 10.1016/j.robot.2011.02.011.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xia CL, 2015, 2015 54TH ANNUAL CONFERENCE OF THE SOCIETY OF INSTRUMENT AND CONTROL ENGINEERS OF JAPAN (SICE), P1207, DOI 10.1109/SICE.2015.7285522.},
Number-of-Cited-References = {51},
Times-Cited = {33},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {50},
Journal-ISO = {J. Field Robot.},
Doc-Delivery-Number = {NT5UH},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000479446700001},
OA = {Bronze, Green Submitted},
DA = {2023-08-12},
}

@article{ WOS:000380415100019,
Author = {Nazarenko, D. V. and Kharyuk, P. V. and Oseledets, I. V. and Rodin, I.
   A. and Shpigun, O. A.},
Title = {Machine learning for LC-MS medicinal plants identification},
Journal = {CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS},
Year = {2016},
Volume = {156},
Pages = {174-180},
Month = {AUG 15},
Abstract = {Herbal medicines are vigorously marketed, but poorly regulated. Analysis
   methodology for this field is still forming. One particular analytical
   task is confirmation of plant species identity for medicinal plants used
   as ingredients. In this work, machine learning approach has been
   implemented for LC-MS plant species identification. Samples for 36 plant
   species have been analyzed. Peak data (m/z, abundance) from respective
   samples have been used for development of classification algorithms.
   Namely, logistic regression (LR), support vector machine (SVM) and
   random forest (RF) techniques were used. For most of used machine
   learning algorithms, classification accuracy of 95\% higher were
   obtained on cross-validation dataset. Now, massive training datasets are
   needed for full-scale application of this approach. (C) 2016 Elsevier
   B.V. All rights reserved.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Nazarenko, DV (Corresponding Author), Lomonosov Moscow State Univ, Dept Analyt Chem, Fac Chem, Moscow, Russia.
   Nazarenko, D. V.; Rodin, I. A.; Shpigun, O. A., Lomonosov Moscow State Univ, Dept Analyt Chem, Fac Chem, Moscow, Russia.
   Kharyuk, P. V., Lomonosov Moscow State Univ, Fac Computat Math \& Cybernet, Dept Computat Technol \& Modeling, Moscow, Russia.
   Oseledets, I. V., Skolkovo Inst Sci \& Technol, Skolkovo, Moscow Region, Russia.
   Oseledets, I. V., Russian Acad Sci, Inst Numer Math, Moscow, Russia.},
DOI = {10.1016/j.chemolab.2016.06.003},
ISSN = {0169-7439},
EISSN = {1873-3239},
Keywords = {Plant species identification; Liquid chromatography-mass spectrometry;
   Machine learning; Multiclass classification},
Keywords-Plus = {CHROMATOGRAPHIC FINGERPRINTS; HERBAL FINGERPRINTS; SIMILARITY ANALYSES;
   QUALITY EVALUATION; GC-MS; DIFFERENTIATION; DISCRIMINATION; SPECTROSCOPY},
Research-Areas = {Automation \& Control Systems; Chemistry; Computer Science; Instruments
   \& Instrumentation; Mathematics},
Web-of-Science-Categories  = {Automation \& Control Systems; Chemistry, Analytical; Computer Science,
   Artificial Intelligence; Instruments \& Instrumentation; Mathematics,
   Interdisciplinary Applications; Statistics \& Probability},
Author-Email = {dmitro.nazarenko@gmail.com},
Affiliations = {Lomonosov Moscow State University; Lomonosov Moscow State University;
   Skolkovo Institute of Science \& Technology; Russian Academy of Sciences},
ResearcherID-Numbers = {Oseledets, Ivan V/E-2146-2014
   Rodin, Igor A/M-3359-2016},
ORCID-Numbers = {Oseledets, Ivan V/0000-0003-2071-2163
   },
Funding-Acknowledgement = {Russian Science Foundation {[}14-11-00659]; Russian Science Foundation
   {[}14-11-00659] Funding Source: Russian Science Foundation},
Funding-Text = {Computational part of this work was supported by Russian Science
   Foundation Grant 14-11-00659.},
Cited-References = {Alaerts G, 2012, J CHROMATOGR B, V910, P61, DOI 10.1016/j.jchromb.2012.04.031.
   Analytics C., 2015, COMPUTER SOFTWARE VE.
   {[}Anonymous], 2013, USP36NF31.
   Azmir J, 2013, J FOOD ENG, V117, P426, DOI 10.1016/j.jfoodeng.2013.01.014.
   Bishop Ch. M., 2006, PATTERN RECOGN.
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324.
   Chang YX, 2015, J ETHNOPHARMACOL, V164, P210, DOI 10.1016/j.jep.2015.02.018.
   Chaudhury R.R., 2015, J REPROD HLTH MED, V1, P44, DOI {[}10.1016/j.jrhm.2015.01.004, DOI 10.1016/J.JRHM.2015.01.004].
   Cheng H, 2013, FOOD RES INT, V51, P813, DOI 10.1016/j.foodres.2013.01.053.
   Cordell GA, 2014, PHYTOCHEM LETT, V10, pXXVIII, DOI 10.1016/j.phytol.2014.06.002.
   Cordero C, 2015, J CHROMATOGR A, V1417, P79, DOI 10.1016/j.chroma.2015.09.027.
   dos Santos Grasel F., 2016, SPECTROCHIM ACTA A, V153, P94.
   Fan CL, 2013, J PHARMACEUT BIOMED, V84, P20, DOI 10.1016/j.jpba.2013.05.039.
   Fan QM, 2013, J MOL STRUCT, V1051, P66, DOI 10.1016/j.molstruc.2013.07.039.
   Farag MA, 2015, J PHARMACEUT BIOMED, V115, P383, DOI 10.1016/j.jpba.2015.08.003.
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1.
   Goodarzi M, 2013, ANAL CHIM ACTA, V804, P16, DOI 10.1016/j.aca.2013.09.017.
   Huang HL, 2015, SPECTROCHIM ACTA B, V105, P121, DOI 10.1016/j.sab.2014.10.005.
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260.
   Li BY, 2004, ANAL CHIM ACTA, V514, P69, DOI 10.1016/j.aca.2004.03.041.
   Liu W, 2015, INT J BIOL MACROMOL, V78, P230, DOI 10.1016/j.ijbiomac.2015.03.062.
   Mazina J, 2015, TALANTA, V139, P233, DOI 10.1016/j.talanta.2015.02.050.
   Ng A., 2015, MACHINE LEARNING STA.
   Riedl J, 2015, ANAL CHIM ACTA, V885, P17, DOI 10.1016/j.aca.2015.06.003.
   Sandasi M, 2013, BIOCHEM SYST ECOL, V51, P142, DOI 10.1016/j.bse.2013.08.028.
   Sheridan H, 2012, J ETHNOPHARMACOL, V140, P482, DOI 10.1016/j.jep.2012.01.050.
   Tarachiwin L, 2008, J PHARMACEUT BIOMED, V48, P42, DOI 10.1016/j.jpba.2008.04.025.
   Tistaert C, 2011, ANAL CHIM ACTA, V690, P148, DOI 10.1016/j.aca.2011.02.023.
   Viaene J, 2015, ANAL CHIM ACTA, V877, P41, DOI 10.1016/j.aca.2015.02.034.
   Wan JB, 2013, J PHARMACEUT BIOMED, V83, P34, DOI 10.1016/j.jpba.2013.04.019.
   Wang J, 2013, J PHARMACEUT BIOMED, V83, P57, DOI 10.1016/j.jpba.2013.04.035.
   Wong KH, 2013, J PHARMACEUT BIOMED, V84, P5, DOI 10.1016/j.jpba.2013.05.040.
   Yuan M, 2013, CHIN J NAT MEDICINES, V11, P699, DOI {[}10.1016/S1875-5364(13)60082-5, 10.3724/SP.J.1009.2013.00699].
   Yuval N., 2011, P NIPS WORKSH DEEP L.},
Number-of-Cited-References = {34},
Times-Cited = {12},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {43},
Journal-ISO = {Chemometrics Intell. Lab. Syst.},
Doc-Delivery-Number = {DS1WG},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000380415100019},
DA = {2023-08-12},
}

@inproceedings{ WOS:000371977800094,
Author = {Lee, Sue Han and Chan, Chee Seng and Wilkin, Paul and Remagnino, Paolo},
Book-Group-Author = {IEEE},
Title = {DEEP-PLANT: PLANT IDENTIFICATION WITH CONVOLUTIONAL NEURAL NETWORKS},
Booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
Series = {IEEE International Conference on Image Processing ICIP},
Year = {2015},
Pages = {452-456},
Note = {IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015},
Organization = {Inst Elect \& Elect Engineers; IEEE Signal Proc Soc},
Abstract = {This paper studies convolutional neural networks (CNN) to learn
   unsupervised feature representations for 44 different plant species,
   collected at the Royal Botanic Gardens, Kew, England. To gain intuition
   on the chosen features from the CNN model (opposed to a `black box'
   solution), a visualisation technique based on the deconvolutional
   networks (DN) is utilized. It is found that venations of different order
   have been chosen to uniquely represent each of the plant species.
   Experimental results using these CNN features with different classifiers
   show consistency and superiority compared to the state-of-the art
   solutions which rely on hand-crafted features.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Lee, SH (Corresponding Author), Univ Malaya, Fac Comp Sci \& Info Tech, Ctr Image \& Signal Proc, Kuala Lumpur, Malaysia.
   Lee, Sue Han; Chan, Chee Seng, Univ Malaya, Fac Comp Sci \& Info Tech, Ctr Image \& Signal Proc, Kuala Lumpur, Malaysia.
   Wilkin, Paul, Royal Bot Gardens, Dept Nat Capital \& Plant Hlth, Richmond, England.
   Remagnino, Paolo, Univ Kingston, Comp \& Info Syst, Kingston, Jamaica.},
ISSN = {1522-4880},
ISBN = {978-1-4799-8339-1},
Keywords = {plant classification; deep learning; feature visualisation},
Research-Areas = {Engineering; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Imaging Science \& Photographic
   Technology},
Author-Email = {leesuehan@siswa.um.edu.my
   cs.chan@um.edu.my
   p.wilkin@kew.org
   p.remagnino@kingston.ac.uk},
Affiliations = {Universiti Malaya; Royal Botanic Gardens, Kew},
ResearcherID-Numbers = {Chan, Chee Seng/B-9754-2011
   Wilkin, Paul/E-4045-2010
   Lee, Sue Han/AAM-6250-2021
   Remagnino, Paolo/K-1829-2012
   },
ORCID-Numbers = {Chan, Chee Seng/0000-0001-7677-2865
   Remagnino, Paolo/0000-0002-9168-7746
   Wilkin, Paul/0000-0003-4982-7175},
Cited-References = {{[}Anonymous], 2013, ARXIV14014447.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Candela H, 1999, DEV BIOL, V205, P205, DOI 10.1006/dbio.1998.9111.
   Casanova D, 2009, INT J IMAG SYST TECH, V19, P236, DOI 10.1002/ima.20201.
   Charters J., 2014, 2014 IEEE INT C MULT, P1, DOI {[}10.1109/ICMEW.2014.6890557, DOI 10.1109/ICMEW.2014.6890557].
   Clarke J, 2006, LECT NOTES COMPUT SC, V4292, P427.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6474, P135, DOI 10.1007/978-3-642-17688-3\_14.
   Donahue J., 2013, ARXIV13101531.
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2\_13.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, EXPERT SYST APPL, V41, P4638, DOI 10.1016/j.eswa.2014.01.029.
   Mullen RJ, 2008, LECT NOTES COMPUT SC, V5217, P251, DOI 10.1007/978-3-540-87527-7\_24.
   Roth-Nebelsick A, 2001, ANN BOT-LONDON, V87, P553, DOI 10.1006/anbo.2001.1391.
   Runions A, 2005, ACM T GRAPHIC, V24, P702, DOI 10.1145/1073204.1073251.
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347.
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474.},
Number-of-Cited-References = {22},
Times-Cited = {220},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {22},
Doc-Delivery-Number = {BE4NP},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000371977800094},
OA = {Green Submitted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000469479400004,
Author = {Lee, Jun Woo and Yoon, Yeo Chan},
Book-Group-Author = {IEEE},
Title = {Fine-Grained Plant Identification using wide and deep learning model},
Booktitle = {2019 INTERNATIONAL CONFERENCE ON PLATFORM TECHNOLOGY AND SERVICE
   (PLATCON)},
Year = {2019},
Pages = {17-21},
Note = {6th International Conference on Platform Technology and Service
   (PlatCon), Inst Creat Res Profess, Jeju, SOUTH KOREA, JAN 28-30, 2019},
Organization = {IEEE Kwangju Sect; IEEE; Blue Mango 7; Jeju Convent \& Visitors Bur;
   DAYLI Blockchain Financial Grp; D PARTNERS},
Abstract = {In recent years, with the evolution of deep learning technology, the
   performance of plant image recognition has improved remarkably. In this
   paper, we propose a model to address the fine-grained plant image
   classification task by using the wide and deep learning framework which
   combines a linear model and a deep learning model. Proposed method sums
   the result of the wide and deep learning model using a logistic function
   so that discrete features can be considered simultaneously with
   continuous image content. Our works used metadata such as the date of
   flowering and locational information for the wide model. Our experiment
   shows that the proposed method gives better performance than a baseline
   method.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Lee, JW (Corresponding Author), Elect \& Telecommun Res Inst, SW Content Res Lab, Daejeon, South Korea.
   Lee, Jun Woo; Yoon, Yeo Chan, Elect \& Telecommun Res Inst, SW Content Res Lab, Daejeon, South Korea.},
ISBN = {978-1-7281-1288-6},
Keywords = {plant image classification; deep neural network; fine-grained image
   classification},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Computer Science; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Telecommunications},
Author-Email = {leejw@etri.re.kr
   ycyoon@etri.re.kr},
Affiliations = {Electronics \& Telecommunications Research Institute - Korea (ETRI)},
Funding-Acknowledgement = {Institute for Information \& communications Technology Promotion(IITP) -
   Korea government(MSIT) {[}2016-0-00010-003]; Institute for Information
   \& Communication Technology Planning \& Evaluation (IITP), Republic of
   Korea {[}2016-0-00010-003] Funding Source: Korea Institute of Science \&
   Technology Information (KISTI), National Science \& Technology
   Information Service (NTIS)},
Funding-Text = {This work was supported by Institute for Information \& communications
   Technology Promotion(IITP) grant funded by the Korea
   government(MSIT)(No. 2016-0-00010-003, Digital Content In-House R\& D).},
Cited-References = {{[}Anonymous], 2015, P IEEE C COMP VIS PA.
   {[}Anonymous], 2017, AAAI.
   {[}Anonymous], 2014, P INT C LEARN REPR.
   Atito S, 2017, CLEF 2017 C.
   Boutell Matthew, 2004, PATT REC 2004 ICPR 2, V4.
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Cheng Heng-Tze, 2016, P 1 WORKSH DEEP LEAR.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Goeau Herve, 2017, CLEF 2017 C LABS EV.
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428.
   Hussein A. N., 2011, 2011 Proceedings of IEEE 7th International Colloquium on Signal Processing \& its Applications (CSPA 2011), P11, DOI 10.1109/CSPA.2011.5759833.
   Lasseck M., 2017, CLEF 2017 C.
   Lee CL, 2006, INT J IMAG SYST TECH, V16, P15, DOI 10.1002/ima.20063.
   Lee S. H, 2017, CLEF 2017 C.
   Li Y, 2006, IEEE SYS MAN CYBERN, P3890, DOI 10.1109/ICSMC.2006.384738.
   Liu XZ, 2005, PATTERN RECOGN, V38, P887, DOI 10.1016/j.patcog.2004.11.008.
   Ludwig A. R, 2017, CLEF 2017 C.
   Paterek Arkadiusz, 2007, P KDD CUP WORKSH, V2007.
   Sahebi Shaghayegh, 2013, INT C US MOD AD PERS.
   Sulc Milan, 2017, WORKING NOTES CLEF.
   Toma A, 2017, CLEF 2017 C.
   Wang C., 2011, P 17 ACM SIGKDD INT, DOI 10.1145/2020408.2020480.
   Wang J., 2014, P IEEE C COMP VIS PA.
   Wang Q, 2007, APPL ENVIRON MICROB, V73, P5261, DOI 10.1128/AEM.00062-07.
   Xiao T., 2015, P IEEE C COMP VIS PA.
   Xie Saining, 2015, P IEEE C COMP VIS PA.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.},
Number-of-Cited-References = {28},
Times-Cited = {2},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BM8NL},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000469479400004},
DA = {2023-08-12},
}

@article{ WOS:000965410200001,
Author = {Kamyshova, Galina and Osipov, Aleksey and Gataullin, Sergey and
   Korchagin, Sergey and Ignar, Stefan and Gataullin, Timur and Terekhova,
   Nadezhda and Suvorov, Stanislav},
Title = {Artificial Neural Networks and Computer Vision's-Based Phytoindication
   Systems for Variable Rate Irrigation Improving},
Journal = {IEEE ACCESS},
Year = {2022},
Volume = {10},
Pages = {8577-8589},
Abstract = {The article proposes a methodology for optimizing the process of
   irrigation of crops using a phytoindication system based on computer
   vision methods. We have proposed an algorithm and developed a system for
   obtaining a map of irrigation for maize in low latency mode. The system
   can be installed on a center pivot irrigation and consists of 8 IP
   cameras connected to a DVR connected to a laptop. The algorithm consists
   of three stages. Image preprocessing stage - applying an integrated
   excess green and excess red difference (ExGR) index. The classification
   stage is the application of the method that we choose depending on the
   system's operating conditions. At the final stage, a neural network
   trained using the Resilient Propagation method is used, which determines
   the rate of watering of plants in the current sector of the location of
   the sprinkler. The selected methods of pretreatment and classification
   made it possible to achieve an accuracy of plant identification up to
   93\%, growth stages - up to 92\% (with unconsolidated maize sowing and
   good lighting). System performance up to 100 plants in one second, which
   exceeds the performance of similar systems. The neural network showed an
   accuracy of 92\% on the training set and 87\% on the test set. Dynamic
   analysis of spatial and temporal variability leads to an increase in
   productivity and efficiency of water use. In addition, given the
   ubiquitous distribution of agribusiness management systems, this
   approach is quite simple to implement in the farm's conditions.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Kamyshova, G (Corresponding Author), Financial Univ, Dept Data Anal \& Machine Learning, Govt Russian Federat, Moscow 125993, Russia.
   Kamyshova, Galina; Osipov, Aleksey; Gataullin, Sergey; Korchagin, Sergey, Financial Univ, Dept Data Anal \& Machine Learning, Govt Russian Federat, Moscow 125993, Russia.
   Ignar, Stefan, Warsaw Univ Life Sci, Dept Hydraul Engn, PL-02776 Warsaw, Poland.
   Gataullin, Timur, State Univ Management, Dept Math Methods Econ \& Management, Moscow 109542, Russia.
   Terekhova, Nadezhda, Saratov State Agr Univ, Dept Math Mech \& Engn, Saratov 410012, Russia.
   Suvorov, Stanislav, Moscow Polytech Univ, Dept Appl Informat, Moscow 107023, Russia.},
DOI = {10.1109/ACCESS.2022.3143524},
ISSN = {2169-3536},
Keywords = {Artificial neural networks; computer vision; image classification;
   irrigation; machine learning},
Keywords-Plus = {IOT},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {gnkamyshova@fa.ru},
Affiliations = {Financial University Under the Government of Russian Federation; Warsaw
   University of Life Sciences; State University of Management; Saratov
   State Agrarian University; Moscow Polytechnic University},
ResearcherID-Numbers = {Suvorov, Stanislav/AFR-6953-2022
   Osipov, Aleksey/AAB-5151-2022
   Suvorov, Stanislav/AFR-2542-2022
   Gataullin, Timur/AAG-1004-2019
   Gataullin, Sergey/AAX-8389-2021
   },
ORCID-Numbers = {Osipov, Aleksey/0000-0002-1261-8559
   Suvorov, Stanislav/0000-0002-4931-1155
   Gataullin, Timur/0000-0002-9597-2894
   Gataullin, Sergey/0000-0002-0446-0552
   Kamyshova, Galina/0000-0002-8569-6259},
Cited-References = {Abouzahir S, 2021, BIOSYST ENG, V202, P179, DOI 10.1016/j.biosystemseng.2020.11.005.
   Aragon B, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10121867.
   Benos L, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21113758.
   Bonfante A, 2019, AGR SYST, V176, DOI 10.1016/j.agsy.2019.102646.
   Caceres G, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11091810.
   Campos NGS, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20010190.
   Chang CL, 2018, ROBOTICS, V7, DOI 10.3390/robotics7030038.
   Cravero A, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10050552.
   D'Emilio A, 2018, WATER-SUI, V10, DOI 10.3390/w10101431.
   Baptista VBD, 2019, WATER-SUI, V11, DOI 10.3390/w11102192.
   Diao WY, 2021, AGRICULTURE-BASEL, V11, DOI 10.3390/agriculture11080710.
   Diez FJ, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10010096.
   Garcia L, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041042.
   Gloria A, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093079.
   Gloria A, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20051402.
   Goap A, 2018, COMPUT ELECTRON AGR, V155, P41, DOI 10.1016/j.compag.2018.09.040.
   Gonzalez-de-Santos P, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10111638.
   Jha K, 2019, ARTIF INTELL AGR, V2, P1, DOI 10.1016/j.aiia.2019.05.004.
   Kamienski C, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020276.
   Kamyshova Galina, 2022, Agriculture Digitalization and Organic Production: Proceedings of the First International Conference, ADOP 2021. Smart Innovation, Systems and Technologies (245), P359, DOI 10.1007/978-981-16-3349-2\_30.
   Kamyshova G. N., 2021, ADV DYN SYST APPL, V16, P159.
   Kashyap PK, 2021, IEEE SENS J, V21, P17479, DOI 10.1109/JSEN.2021.3069266.
   Keswani B, 2019, NEURAL COMPUT APPL, V31, P277, DOI 10.1007/s00521-018-3737-1.
   Kujawa S, 2021, AGRICULTURE-BASEL, V11, DOI 10.3390/agriculture11060497.
   Kuznetsova A., 2021, STUDIES SYSTEMS DECI, P349, DOI {[}10.1007/978-3-030-66077-228, DOI 10.1007/978-3-030-66077-2\_28, DOI 10.1007/978-3-030].
   KuznetsovazT A, 2020, LECT NOTES COMPUT SC, V12249, P923, DOI 10.1007/978-3-030-58799-4\_66.
   Linaza MT, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11061227.
   Makarov M. A, 2019, 2019615941 IV STAT U.
   Martos V, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11135911.
   Mendes WR, 2019, EXPERT SYST APPL, V124, P13, DOI 10.1016/j.eswa.2019.01.043.
   Miletto M, 2020, UN WORLD WATER DEV R.
   Mohamed AZ, 2021, WATER-SUI, V13, DOI 10.3390/w13091167.
   Monteleone S, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20247091.
   Muangprathub J, 2019, COMPUT ELECTRON AGR, V156, P467, DOI 10.1016/j.compag.2018.12.011.
   Nawandar NK, 2019, COMPUT ELECTRON AGR, V162, P979, DOI 10.1016/j.compag.2019.05.027.
   Nikolaou G, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10081120.
   O'Shaughnessy SA, 2019, APPL ENG AGRIC, V35, P837, DOI 10.13031/aea.13128.
   Pang Y, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105766.
   Pereira LS, 2017, WATER RESOUR MANAG, V31, P2985, DOI 10.1007/s11269-017-1664-z.
   Prokopyeva KO, 2021, ARID ECOSYST, V11, P173, DOI 10.1134/S207909612102013X.
   Rad S, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8122601.
   Rovelo COR, 2019, WATER-SUI, V11, DOI 10.3390/w11081684.
   Saiz-Rubio V, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10020207.
   Serrano J, 2020, WATER-SUI, V12, DOI 10.3390/w12123427.
   Shafi U, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19173796.
   Shi X, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19132880.
   Soloviev D. A., 2021, 2021615359 SAR STAT.
   Sun AY, 2019, ENVIRON RES LETT, V14, DOI 10.1088/1748-9326/ab1b7d.
   Suntaranont B, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12051763.
   Svedin JD, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11071377.
   Torres-Sanchez R, 2020, WATER-SUI, V12, DOI 10.3390/w12020548.
   van Klompenburg T, 2020, COMPUT ELECTRON AGR, V177, DOI 10.1016/j.compag.2020.105709.
   Wang E, 2020, COMPUT ELECTRON AGR, V172, DOI 10.1016/j.compag.2020.105376.
   Wang JQ, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13020305.
   Yang W, 2021, COMPUT ELECTRON AGR, V184, DOI 10.1016/j.compag.2021.106092.
   Zhong LH, 2019, REMOTE SENS ENVIRON, V221, P430, DOI 10.1016/j.rse.2018.11.032.},
Number-of-Cited-References = {56},
Times-Cited = {9},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {3},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {C9ZH6},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000965410200001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000402375800061,
Author = {Shan, Zhi-lei and Zhao, Xi-lin and Chen, Zhuo and Mao, Man-man},
Book-Group-Author = {Destech Publicat Inc},
Title = {Plant Recognition Based on Multi-Feature and Locality Preserving
   Projections Fusion Algorithm},
Booktitle = {INTERNATIONAL CONFERENCE ON COMPUTER, MECHATRONICS AND ELECTRONIC
   ENGINEERING (CMEE 2016)},
Series = {DEStech Transactions on Computer Science and Engineering},
Year = {2016},
Note = {International Conference on Computer, Mechatronics and Electronic
   Engineering (CMEE), Beijing, PEOPLES R CHINA, NOV 20-21, 2016},
Organization = {Sci \& Engn Res Ctr},
Abstract = {For the demand of leaf recognition under rotation condition. This paper
   propose a method to extract the multi-feature extraction and Locally
   Preserving Projection (LPP) fusion algorithm. First, the texture feature
   of the leaf is extracted by Local Binary Pattern (LBP) algorithm based
   on leaf block. Then, in order to improve the speed of feature
   classification recognition, this paper adopts LPP algorithm for feature
   dimension reduction of the high dimensional of LBP. In addition,
   combining leaf texture feature LBP with shape feature (Hu Moment
   invariants) after dimension reduction, shaping into the comprehensive
   characteristics of plant leaves. Finally, Support Vector Machine (SVM)
   classifier is used to establish classifier for classification and
   identification of plant leaves, the results of experiment have verified
   the validity of the algorithm in this paper.},
Publisher = {DESTECH PUBLICATIONS, INC},
Address = {439 DUKE STREET, LANCASTER, PA 17602-4967 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zhao, XL (Corresponding Author), Hubei Univ Technol, Sch Elect \& Elect Engin, Wuhan 430068, Peoples R China.
   Shan, Zhi-lei; Zhao, Xi-lin; Chen, Zhuo; Mao, Man-man, Hubei Univ Technol, Sch Elect \& Elect Engin, Wuhan 430068, Peoples R China.},
ISSN = {2475-8841},
ISBN = {978-1-60595-406-6},
Keywords = {Local Binary Pattern (LBP); Locally Preserving Projection (LPP); Hu
   moment; SVM},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Engineering, Mechanical},
Affiliations = {Hubei University of Technology},
Cited-References = {He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55.
   {[}华斌 HUA Bin], 2011, {[}计算机工程与设计, Computer Engineering and Design], V32, P615.
   Ingrouille M J, 1986, LONDON NATURALIST, P6534.
   {[}刘晶 LIU Jing], 2006, {[}计算机应用研究, Application Research of Computers], V23, P181.
   Liu Nian, 2016, Journal of Beijing Forestry University, V38, P110.
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
   Wang LiJun, 2015, Journal of Beijing Forestry University, V37, P55.
   {[}王宪 Wang Xian], 2012, {[}光电工程, Opto-Electronic Engineering], V39, P109.
   Zhang Ning, 2011, COMPUTER SCI APPL, V28, P4001.
   Zhang S, 2010, ADV COMP INT IWACI 2, P53.
   Zhang ShanWen, 2015, Transactions of the Chinese Society of Agricultural Engineering, V31, P215.
   Zu Q, 2013, SPECTROSC SPECT ANAL, V33, P2745, DOI 10.3964/j.issn.1000-0593(2013)10-2745-06.},
Number-of-Cited-References = {12},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BH7BQ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000402375800061},
DA = {2023-08-12},
}

@inproceedings{ WOS:000380480700042,
Author = {Elhariri, Esraa and El-Bendary, Nashwa and Hassanien, Aboul Ella},
Editor = {Wahba, AM and EIKharashi, MW and EIDin, AMB and Taher, M and Zaki, AM},
Title = {Plant Classification System based on Leaf Features},
Booktitle = {2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING \& SYSTEMS
   (ICCES)},
Year = {2014},
Pages = {271-276},
Note = {9th International Conference on Computer Engineering \& Systems (ICCES),
   Cairo, EGYPT, DEC 22-23, 2014},
Organization = {Ain Shams Univ; IEEE Egypt Sect},
Abstract = {This paper presents a classification approach based on Random Forests
   (RF) and Linear Discriminant Analysis (LDA) algorithms for classifying
   the different types of plants. The proposed approach consists of three
   phases that are pre-processing, feature extraction, and classification
   phases. Since most types of plants have unique leaves, so the
   classification approach presented in this research depends on plants
   leave. Leaves are different from each other by characteristics such as
   the shape, color, texture and the margin. The used dataset for this
   experiments is a database of different plant species with total of only
   340 leaf images, was downloaded from UCI-Machine Learning Repository. It
   was used for both training and testing datasets with 10-fold
   cross-validation. Experimental results showed that LDA achieved
   classification accuracy of (92.65 \%) against the RF that achieved
   accuracy of (88.82 \%) with combination of shape, first order texture,
   Gray Level Co-occurrence Matrix (GLCM), HSV color moments, and vein
   features.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Elhariri, E (Corresponding Author), Fayoum Univ, Fac Comp Sci \& Informat, Al Fayyum, Egypt.
   Elhariri, Esraa, Fayoum Univ, Fac Comp Sci \& Informat, Al Fayyum, Egypt.
   El-Bendary, Nashwa, Arab Acad Sci Technol \& Maritime Transport, Cairo, Egypt.
   Hassanien, Aboul Ella, Cairo Univ, Fac Comp \& Informat, Cairo, Egypt.
   Elhariri, Esraa; El-Bendary, Nashwa; Hassanien, Aboul Ella, Sci Res Grp Egypt, Giza, Egypt.},
ISBN = {978-1-4799-6594-6},
Keywords = {image classification; features extraction; linear discriminant analysis
   (LDA); gray level co-occurrence matrix (GLCM); plants; leaves; random
   forests (RF)},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Author-Email = {eng.esraa.elhariri@gmail.com
   nashwa.elbendary@ieee.org
   aboitegypt@gmail.com},
Affiliations = {Egyptian Knowledge Bank (EKB); Fayoum University; Egyptian Knowledge
   Bank (EKB); Arab Academy for Science, Technology \& Maritime Transport;
   Egyptian Knowledge Bank (EKB); Cairo University},
ResearcherID-Numbers = {El-Bendary, Nashwa/AAI-5414-2020
   Elhariri, Esraa/AAZ-7031-2021
   Hassanien, Aboul ella/O-5672-2014},
ORCID-Numbers = {El-Bendary, Nashwa/0000-0001-6553-4159
   Elhariri, Esraa/0000-0003-2362-1574
   Hassanien, Aboul ella/0000-0002-9989-6681},
Cited-References = {Albregtsen F., 1995, IMAGE PROCESSING LAB.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863.
   Chaki J., 2012, INT J COMPUTER APPL, V56, DOI {[}10.5120/8927-3000, DOI 10.5120/8927-3000].
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x.
   Geethalakshmi S.N., 2013, INT J EMERGING TECHN, V4, P221.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   Jabal MFAB, 2013, J COMPUT SCI-NETH, V9, P1295, DOI {[}DOI 10.3844/J.CSSP.2013.1295.1304, DOI 10.3844/JCSSP.2013.1295.1304].
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Kadir A., 2011, INT J COMPUT APPL, V29, P15, DOI DOI 10.5120/3592-4981.
   Katyal Vini, 2012, INT J ADV RES COMPUT, V3.
   Kulkarni VY, 2013, LECT NOTES ENG COMP, P826.
   Kumar S., 2012, INDIAN J COMPUTER SC, V3, P436.
   LI T, 2006, INT J KNOWLEDGE INFO, V10, P453, DOI DOI 10.1007/S10115-006-0013-Y.
   Shahbahrami A., 2008, P 19 ANN WORKSH CIRC, P1.
   Silva PFB, 2013, LECT NOTES COMPUT SC, V7950, P197, DOI 10.1007/978-3-642-39094-4\_23.
   Soman S., 2012, P INT C COMP INT ICC, V9, P1.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {18},
Times-Cited = {29},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BF2MN},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000380480700042},
DA = {2023-08-12},
}

@article{ WOS:000292675000002,
Author = {Zhang, Shanwen and Lei, Ying-Ke},
Title = {Modified locally linear discriminant embedding for plant leaf
   recognition},
Journal = {NEUROCOMPUTING},
Year = {2011},
Volume = {74},
Number = {14-15},
Pages = {2284-2290},
Month = {JUL},
Abstract = {Based on locally linear embedding (LLE) and modified maximizing margin
   criterion (MMMC), a modified locally linear discriminant embedding
   (MLLDE) algorithm is proposed for plant leaf recognition in this paper.
   By MLLDE, the plant leaf images are mapped into a leaf subspace for
   analysis, which can detect the essential leaf manifold structure.
   Furthermore, the unwanted variations resulting from changes in period,
   location, and illumination can be eliminated or reduced. Different from
   principal component analysis (PCA) and linear discriminant analysis
   (LDA), which can only deal with flat Euclidean structures of plant leaf
   space, MLLDE not only inherits the advantages of locally linear
   embedding (LLE), but makes full use of class information to improve
   discriminant power by introducing translation and rescaling models. The
   experimental results on real plant leaf database show that the MLLDE is
   effective for plant leaf recognition. (C) 2011 Elsevier B.V. All rights
   reserved.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Lei, YK (Corresponding Author), Chinese Acad Sci, Inst Intelligent Machines, POB 1130, Hefei 230031, Anhui, Peoples R China.
   Zhang, Shanwen; Lei, Ying-Ke, Chinese Acad Sci, Inst Intelligent Machines, Hefei 230031, Anhui, Peoples R China.
   Lei, Ying-Ke, Univ Sci \& Technol China, Dept Automat, Hefei 230027, Anhui, Peoples R China.
   Lei, Ying-Ke, Inst Elect Engn, Hefei 230027, Anhui, Peoples R China.},
DOI = {10.1016/j.neucom.2011.03.007},
ISSN = {0925-2312},
EISSN = {1872-8286},
Keywords = {Plant leaf recognition; Locally linear embedding (LLE); Modified
   maximizing margin criterion (MM MC); Modified locally linear
   discriminant embedding (MLLDE); Manifold learning},
Keywords-Plus = {PROBABILISTIC NEURAL-NETWORKS; DIMENSIONALITY REDUCTION;
   FEATURE-EXTRACTION; TEXTURE FEATURES; FACE RECOGNITION; IMAGE RETRIEVAL;
   EIGENFACES; EIGENMAPS; EFFICIENT; MODEL},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {leiyingke@gmail.com},
Affiliations = {Chinese Academy of Sciences; Hefei Institutes of Physical Science, CAS;
   Chinese Academy of Sciences; University of Science \& Technology of
   China, CAS},
Funding-Acknowledgement = {National Science Foundation of China {[}60975005, 31071168, 61005010,
   60905023, 30900321, 60873012, 30700161, 60805021]; National Basic
   Research Program of China (973 Program) {[}2007CB311002]},
Funding-Text = {This work was supported by the grants of the National Science Foundation
   of China, Nos. 60975005, 31071168, 61005010, 60905023, 30900321,
   60873012, 30700161, and 60805021, and the National Basic Research
   Program of China (973 Program) under Grant no. 2007CB311002.},
Cited-References = {Abbasi S, 1999, MULTIMEDIA SYST, V7, P467, DOI 10.1007/s005300050147.
   {[}Anonymous], P IEEE PAC RIM C MUL.
   {[}Anonymous], 1996, SYSTEMATIC THEORY NE.
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228.
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317.
   de Ridder D, 2003, LECT NOTES COMPUT SC, V2714, P333.
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100.
   El-Ghazal A, 2007, LECT NOTES COMPUT SC, V4633, P650.
   Han J, 2007, IMAGE VISION COMPUT, V25, P1474, DOI 10.1016/j.imavis.2006.12.015.
   He XF, 2005, IEEE I CONF COMP VIS, P1208.
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55.
   Huang D, 2005, APPL MATH COMPUT, V162, P461, DOI 10.1016/j.amc.2003.12.105.
   Huang DS, 2008, IEEE T NEURAL NETWOR, V19, P2099, DOI 10.1109/TNN.2008.2004370.
   Huang Deshuang, 1996, Journal of Beijing Institute of Technology, V5, P184.
   Huang DS, 1998, IEEE T SYST MAN CY B, V28, P477, DOI 10.1109/3477.678658.
   Huang DS, 1999, INT J PATTERN RECOGN, V13, P945, DOI 10.1142/S0218001499000525.
   Huang DS, 1999, INT J PATTERN RECOGN, V13, P1083, DOI 10.1142/S0218001499000604.
   HUANG DS, 1999, J INTELLIGENT SYSTEM, V9, P1.
   Kim TK, 2005, IEEE T PATTERN ANAL, V27, P318, DOI 10.1109/TPAMI.2005.58.
   Li B, 2008, PATTERN RECOGN, V41, P3813, DOI 10.1016/j.patcog.2008.05.027.
   Li HF, 2006, IEEE T NEURAL NETWOR, V17, P157, DOI 10.1109/TNN.2005.860852.
   Liu CJ, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P270, DOI 10.1109/ICCV.2001.937635.
   Ma WY, 1996, PROC CVPR IEEE, P425, DOI 10.1109/CVPR.1996.517107.
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803.
   Park JS, 2004, LECT NOTES COMPUT SC, V3332, P146.
   Pillati M., 2005, P 29 ANN C GERM CLAS, P15.
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323.
   SAUL LK, 2003, J MACHINE LEARNING R, V4, P119.
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972.
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319.
   TICO M, 2000, P NORD SIGN PROC S N, P157.
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71.
   Venters C, 2000, REV CONTENT BASED IM.
   Weinberger KQ, 2004, PROC CVPR IEEE, P988.
   Xiang SM, 2009, IEEE T KNOWL DATA EN, V21, P1285, DOI 10.1109/TKDE.2008.204.
   Yang J, 2005, IEEE T PATTERN ANAL, V27, P230, DOI 10.1109/TPAMI.2005.33.
   Yang Y, 2010, AAAI CONF ARTIF INTE, P649.
   Zhang ZY, 2007, FOURTH INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY, VOL 3, PROCEEDINGS, P243, DOI 10.1109/FSKD.2007.459.
   Zhang ZY, 2004, SIAM J SCI COMPUT, V26, P313, DOI 10.1137/S1064827502419154.
   Zheng WM, 2005, NEUROCOMPUTING, V67, P357, DOI 10.1016/j.neucom.2004.12.008.},
Number-of-Cited-References = {40},
Times-Cited = {42},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {16},
Journal-ISO = {Neurocomputing},
Doc-Delivery-Number = {791OR},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000292675000002},
DA = {2023-08-12},
}

@inproceedings{ WOS:000692529100006,
Author = {Trung Nguyen Quoc and Vinh Truong Hoang},
Book-Group-Author = {IEEE},
Title = {Medicinal Plant identification in the wild by using CNN},
Booktitle = {11TH INTERNATIONAL CONFERENCE ON ICT CONVERGENCE: DATA, NETWORK, AND AI
   IN THE AGE OF UNTACT (ICTC 2020)},
Series = {International Conference on Information and Communication Technology
   Convergence},
Year = {2020},
Pages = {25-29},
Note = {11th International Conference on Information and Communication
   Technology Convergence (ICTC) - Data, Network, and AI in the age of
   Untact (ICTC), Jeju, SOUTH KOREA, OCT 21-23, 2020},
Organization = {Korean Inst Commun \& Informat Sci; IEEE Commun Soc; IEICE Commun Soc;
   Minist Sci \& ICT; Elect \& Telecommunicat Res Inst; Korean Federat Sci
   \& Technol Soc; Samsung Elect; LG Elect; SK Telecom; LGU+; KT; SOLiD;
   FRTek; Huawei; Ericsson LG; ICT Convergence Korea Forum; Soc Safety Syst
   Forum; 5G Based Smart Factory Standardizat Forum; Jeju Convent \&
   Visitors Bur; Korea Assoc Photon Ind Dev},
Abstract = {Plant identification based on deep learning received many attention and
   effort from the research community with many promising results. It
   becomes an active trend in the recent years. We apply Convolutional
   Neural Network (CNN) to recognize Vietnamese medicinal plant images in
   this paper. Different frameworks are evaluated such as: VGG16, Resnet50,
   InceptionV3, DenseNet121, Xception and MobileNet. The highest accuracy
   reached by Xception with 88.26\%. We might think this approach will
   greatly contribute to the discovery and conservation of valuable
   medicinal plants.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Quoc, TN (Corresponding Author), Ho Chi Minh Open Univ, Fac Informat Technol, Ho Chi Minh City, Vietnam.
   Trung Nguyen Quoc; Vinh Truong Hoang, Ho Chi Minh Open Univ, Fac Informat Technol, Ho Chi Minh City, Vietnam.},
ISSN = {2162-1233},
ISBN = {978-1-7281-6758-9},
Keywords = {VietNam medicinal plant; Deep learning; VGG16; Resnet50; InceptionV3;
   DenseNet121; Xception; MobileNet; VNPlant-200},
Keywords-Plus = {LEAF},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {trungnq.188i@ou.edu.vn
   vinh.th@ou.edu.vn},
ResearcherID-Numbers = {Hoang, Vinh Truong/E-6591-2017
   },
ORCID-Numbers = {Hoang, Vinh Truong/0000-0002-3464-3894
   Nguyen Quoc, Trung/0000-0002-5500-3554},
Cited-References = {Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Bertrand S, 2018, ECOL INFORM, V46, P57, DOI 10.1016/j.ecoinf.2018.05.007.
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   Fekri-Ershad S, 2020, EXPERT SYST APPL, V158, DOI 10.1016/j.eswa.2020.113509.
   Geetharamani G, 2019, COMPUT ELECTR ENG, V76, P323, DOI 10.1016/j.compeleceng.2019.04.011.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Howard A. G., 2017, EFFICIENT CONVOLUTIO.
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kheirkhah FM, 2019, IET COMPUT VIS, V13, P369, DOI 10.1049/iet-cvi.2018.5028.
   King DB, 2015, ACS SYM SER, V1214, P1.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larsson G, 2017, ICLR, P1.
   Le-Viet Tuan., 2019, SPIE, P39.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Olsen A, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-38343-3.
   Quoc T.N., 2020, INT C INT SCI, P406.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Soderkvist O., 2001, THESIS.
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xiao QG, 2018, ECOL INFORM, V48, P117, DOI 10.1016/j.ecoinf.2018.09.001.
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2.
   Zhang SW, 2018, COMPUT ELECTRON AGR, V155, P150, DOI 10.1016/j.compag.2018.10.018.
   Zhang SW, 2018, APPL SOFT COMPUT, V67, P164, DOI 10.1016/j.asoc.2018.02.052.},
Number-of-Cited-References = {29},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BS1LP},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000692529100006},
DA = {2023-08-12},
}

@article{ WOS:000782411600007,
Author = {Heidary-Sharifabad, Ahmad and Zarchi, Mohsen Sardari and Zarei,
   Gholamreza},
Title = {Padeep: A Patched Deep Learning Based Model for Plants Recognition on
   Small Size Dataset: Chenopodiaceae Case Study},
Journal = {INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS},
Year = {2022},
Volume = {21},
Number = {01},
Month = {MAR},
Abstract = {A large training sample is prerequisite for the successful training of
   each deep learning model for image classification. Collecting a large
   dataset is time-consuming and costly, especially for plants. When a
   large dataset is not available, the challenge is how to use a small or
   medium size dataset to train a deep model optimally. To overcome this
   challenge, a novel model is proposed to use the available small size
   plant dataset efficiently. This model focuses on data augmentation and
   aims to improve the learning accuracy by oversampling the dataset
   through representative image patches. To extract the relevant patches,
   ORB key points are detected in the training images and then image
   patches are extracted using an innovative algorithm. The extracted ORB
   image patches are used for dataset augmentation to avoid overfitting
   during the training phase. The proposed model is implemented using
   convolutional neural layers, where its structure is based on ResNet
   architecture. The proposed model is evaluated on a challenging ACHENY
   dataset. ACHENY is a Chenopodiaceae plant dataset, comprising 27030
   images from 30 classes. The experimental results show that the
   patch-based strategy outperforms the classification accuracy achieved by
   traditional deep models by 9\%.},
Publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
Address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
Type = {Article},
Language = {English},
Affiliation = {Zarchi, MS (Corresponding Author), Meybod Univ, Dept Comp Engn, Meybod, Iran.
   Heidary-Sharifabad, Ahmad, Islamic Azad Univ, Dept Comp Engn, Maybod Branch, Maybod, Iran.
   Zarchi, Mohsen Sardari, Meybod Univ, Dept Comp Engn, Meybod, Iran.
   Zarei, Gholamreza, Islamic Azad Univ, Dept Agron, Maybod Branch, Maybod, Iran.},
DOI = {10.1142/S1469026822500055},
Article-Number = {2250005},
ISSN = {1469-0268},
EISSN = {1757-5885},
Keywords = {Patch-based; ORB; deep learning; plant classification; Chenopodiaceae;
   ACHENY},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {ahmad.heidary@maybodiau.ac.ir
   sardari@meybod.ac.ir
   zareigholamreza@maybodiau.ac.ir},
Affiliations = {Islamic Azad University; Islamic Azad University},
Cited-References = {Abe T, 2018, J PLANT RES, V131, P1001, DOI 10.1007/s10265-018-1062-5.
   {[}Anonymous], 2008, COMPUT VIS IMAGE UND, DOI DOI 10.1016/j.cviu.2007.09.014.
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1\_56.
   Gessert N, 2020, METHODSX, V7, DOI 10.1016/j.mex.2020.100864.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Heidary-Sharifabad A., DATA BRIEF ACCEPTED.
   Heidary-Sharifabad A, 2021, INT J PATTERN RECOGN, V35, DOI 10.1142/S0218001421520157.
   Heidary-Sharifabad A, 2021, DATA BRIEF, V38, DOI 10.1016/j.dib.2021.107348.
   Heidary-Sharifabad A, 2021, BRIT FOOD J, V123, P3592, DOI 10.1108/BFJ-12-2020-1100.
   Hinz T, 2018, INT J COMPUT INTELL, V17, DOI 10.1142/S1469026818500086.
   Hou L, 2016, PROC CVPR IEEE, P2424, DOI 10.1109/CVPR.2016.266.
   Kadir A, 2013, NEURAL NETWORK APPL.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Lillian G. P. B., 2019, THE TREE.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009.
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023\_34.
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275.
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
   Schmid C, 2011, ENS INRIA VIS REC MA.
   Simon A., 2019, CLASSIFICATION TECHN, P109, DOI DOI 10.1016/B978-0-12-818004-4.00005-4.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Do TB, 2017, INT CONF KNOWL SYS, P191, DOI 10.1109/KSE.2017.8119457.
   Verma GK, 2018, INT J COMPUT INTELL, V17, DOI 10.1142/S1469026818500219.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Wang AC, 2019, COMPUT ELECTRON AGR, V158, P226, DOI 10.1016/j.compag.2019.02.005.
   Yang X, 2019, SCI HORTIC-AMSTERDAM, V256, DOI 10.1016/j.scienta.2019.05.051.
   Yu L, 2017, INT J COMPUT INTELL, V16, DOI 10.1142/S1469026817500018.
   Yuan X., 2011, IEEE INT C IM PROC.
   Zarchi MS, 2016, PATTERN RECOGN, V53, P174, DOI 10.1016/j.patcog.2015.11.010.
   Zhang DZ, 2019, INT J COMPUT INTELL, V18, DOI 10.1142/S1469026819500093.},
Number-of-Cited-References = {38},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Int. J. Comput. Intell. Appl.},
Doc-Delivery-Number = {0M8PX},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000782411600007},
DA = {2023-08-12},
}

@article{ WOS:000804524200004,
Author = {Keceli, Ali Seydi and Kaya, Aydin and Catal, Cagatay and Tekinerdogan,
   Bedir},
Title = {Deep learning-based multi-task prediction system for plant disease and
   species detection},
Journal = {ECOLOGICAL INFORMATICS},
Year = {2022},
Volume = {69},
Month = {JUL},
Abstract = {The manual prediction of plant species and plant diseases is expensive,
   time-consuming, and requires expertise that is not always available.
   Automated approaches, including machine learning and deep learning, are
   increasingly being applied to surmount these challenges. For this,
   accurate models are needed to provide reliable predictions and guide the
   decision-making process. So far, these two problems have been addressed
   separately, and likewise, separate models have been developed for each
   of these two problems, but considering that plant species and plant
   disease prediction are often related tasks, they can be considered
   together. We therefore propose and validate a novel approach based on
   the multi-task learning strategy, using shared representations between
   these related tasks, because they perform better than individual models.
   We apply a multi-input network that uses raw images and transferred deep
   features extracted from a pre-trained deep model to predict each plant's
   type and disease. We develop an end-to-end multi-task model that carries
   out more than one learning task at a time and combines the Convolutional
   Neural Network (CNN) features and transferred features. We then evaluate
   this model using public datasets. The results of our experiments
   demonstrated that this Multi Input Multi-Task Neural Network model
   increases efficiency and yields faster learning for similar detection
   tasks.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Keceli, AS (Corresponding Author), Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
   Keceli, Ali Seydi; Kaya, Aydin, Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
   Catal, Cagatay, Qatar Univ, Dept Comp Sci \& Engn, Doha, Qatar.
   Tekinerdogan, Bedir, Wageningen Univ \& Res, Informat Technol Grp, Wageningen, Netherlands.},
DOI = {10.1016/j.ecoinf.2022.101679},
EarlyAccessDate = {MAY 2022},
Article-Number = {101679},
ISSN = {1574-9541},
EISSN = {1878-0512},
Keywords = {Plant classification; Multi-task learning; Transfer learning; Deep
   neural networks; Convolutional neural networks},
Research-Areas = {Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Ecology},
Author-Email = {aliseydi@cs.hacettepe.edu.tr
   aydinkaya@cs.hacettepe.edu.tr
   ccatal@qu.edu.qa
   bedir.tekinerdogan@wur.nl},
Affiliations = {Hacettepe University; Qatar University; Wageningen University \&
   Research},
ResearcherID-Numbers = {Tekinerdogan, Bedir/K-3639-2019
   Keçeli, Ali Seydi/M-3158-2018},
ORCID-Numbers = {Tekinerdogan, Bedir/0000-0002-8538-7261
   Keçeli, Ali Seydi/0000-0001-6531-8464},
Cited-References = {Ahmed N, 2020, COMPUT INFORM, V39, P385, DOI 10.31577/cai\_2020\_3\_385.
   Albayrak A, 2021, ECOL INFORM, V66, DOI 10.1016/j.ecoinf.2021.101470.
   Argueso D, 2020, COMPUT ELECTRON AGR, V175, DOI 10.1016/j.compag.2020.105542.
   Arsenovic M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11070939.
   Bai XD, 2018, AGR FOREST METEOROL, V259, P260, DOI 10.1016/j.agrformet.2018.05.001.
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607.
   Brahimi M, 2017, APPL ARTIF INTELL, V31, P299, DOI 10.1080/08839514.2017.1315516.
   Chen JD, 2020, COMPUT ELECTRON AGR, V173, DOI 10.1016/j.compag.2020.105393.
   Chen JD, 2020, J SCI FOOD AGR, V100, P3246, DOI 10.1002/jsfa.10365.
   Ebrahimi MA, 2017, COMPUT ELECTRON AGR, V137, P52, DOI 10.1016/j.compag.2017.03.016.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Golhani Kamlesh, 2018, Information Processing in Agriculture, V5, P354, DOI 10.1016/j.inpa.2018.05.002.
   Govaerts R, 2001, TAXON, V50, P1085, DOI 10.2307/1224723.
   Hernandez S, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106597.
   Hughes D., 2015, ARXIV PREPRINT ARXIV.
   Kar S, 2021, COMPUT ELECTRON AGR, V182, DOI 10.1016/j.compag.2021.105992.
   Kawasaki Y, 2015, LECT NOTES COMPUT SC, V9475, P638, DOI 10.1007/978-3-319-27863-6\_59.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Keskar N. S., 2016, ARXIV.
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V12, P1097.
   Lai XC, 2021, APPL SOFT COMPUT, V98, DOI 10.1016/j.asoc.2020.106838.
   Li Y, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105803.
   Lu Y, 2017, NEUROCOMPUTING, V267, P378, DOI 10.1016/j.neucom.2017.06.023.
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433.
   Mora C, 2011, PLOS BIOL, V9, DOI 10.1371/journal.pbio.1001127.
   Parul Sharma, 2020, Information Processing in Agriculture, V7, P566, DOI 10.1016/j.inpa.2019.11.001.
   Ruder S, 2019, AAAI CONF ARTIF INTE, P4822.
   Scotland RW, 2003, TAXON, V52, P101, DOI 10.2307/3647306.
   Strezoski G, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P78, DOI 10.1145/3323873.3325009.
   Tiwari V, 2021, ECOL INFORM, V63, DOI 10.1016/j.ecoinf.2021.101289.
   Vandenhende Simon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P527, DOI 10.1007/978-3-030-58548-8\_31.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Yang WK, 2020, APPL SOFT COMPUT, V95, DOI 10.1016/j.asoc.2020.106533.
   Yang Z., SOFT COMPUT, P1.
   Yu H., 2022, MULTIMED TOOLS APPL, P1.
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068.
   Zhang Y., 2017, SURVEY MULTITASK LEA.
   Zhang Y, 2018, NATL SCI REV, V5, P30, DOI 10.1093/nsr/nwx105.
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4\_7.
   Zhao XY, 2018, LECT NOTES COMPUT SC, V11205, P415, DOI 10.1007/978-3-030-01246-5\_25.},
Number-of-Cited-References = {40},
Times-Cited = {4},
Usage-Count-Last-180-days = {8},
Usage-Count-Since-2013 = {22},
Journal-ISO = {Ecol. Inform.},
Doc-Delivery-Number = {1T1VC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000804524200004},
DA = {2023-08-12},
}

@article{ WOS:000872934300001,
Author = {Park, Geonha and Lee, Yun-Gyo and Yoon, Ye-Seul and Ahn, Ji-Young and
   Lee, Jei-Wan and Jang, Young-Pyo},
Title = {Machine Learning-Based Species Classification Methods Using DART-TOF-MS
   Data for Five Coniferous Wood Species},
Journal = {FORESTS},
Year = {2022},
Volume = {13},
Number = {10},
Month = {OCT},
Abstract = {Various problems worldwide are caused by illegal production and
   distribution of timber, such as deception about timber species and
   origin and illegal logging. Numerous studies on wood tracking are being
   conducted around the world to demonstrate the legitimacy of timber. Tree
   species identification is the most basic element of wood tracking
   research because the quality of wood varies greatly from species to
   species and is consistent with the botanical origin of commercially
   distributed wood. Although many recent studies have combined machine
   learning-based classification methods with various analytical methods to
   identify tree species, it is unclear which classification model is most
   effective. The purpose of this work is to examine and compare the
   performance of three supervised machine learning classification models,
   support vector machine (SVM), random forest (RF), and artificial neural
   network (ANN), in identifying five conifer species and propose an
   optimal model. Using direct analysis in real-time ionization combined
   with time-of-flight mass spectrometry (DART-TOF-MS), metabolic
   fingerprints of 250 individual specimens representing five species were
   collected three times. When the machine learning models were applied to
   classify the wood species, ANN outperformed SVM and RF. All three models
   showed 100\% prediction accuracy for genus classification. For species
   classification, the ANN model had the highest prediction accuracy of
   98.22\%. The RF model had an accuracy of 94.22\%, and the SVM had the
   lowest accuracy of 92.89\%. These findings demonstrate the practicality
   of authenticating wood species by combining DART-TOF-MS with machine
   learning, and they indicate that ANN is the best model for wood species
   identification.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Jang, YP (Corresponding Author), Kyung Hee Univ, Grad Sch, Dept Life \& Nanopharmaceut Sci, Seoul 02447, South Korea.
   Jang, YP (Corresponding Author), Kyung Hee Univ, Grad Sch, Dept Biomed \& Pharmaceut Sci, Seoul 02447, South Korea.
   Jang, YP (Corresponding Author), Kyung Hee Univ, Grad Sch, Dept Integrated Drug Dev \& Nat Prod, Seoul 02447, South Korea.
   Park, Geonha; Jang, Young-Pyo, Kyung Hee Univ, Grad Sch, Dept Life \& Nanopharmaceut Sci, Seoul 02447, South Korea.
   Lee, Yun-Gyo; Jang, Young-Pyo, Kyung Hee Univ, Grad Sch, Dept Biomed \& Pharmaceut Sci, Seoul 02447, South Korea.
   Yoon, Ye-Seul, Kyung Hee Univ, Coll Pharm, Dept Basic Pharmaceut Sci, Seoul 02447, South Korea.
   Ahn, Ji-Young; Lee, Jei-Wan, Natl Inst Forest Sci, Dept Forest Bioresources, Suwon 16631, South Korea.
   Jang, Young-Pyo, Kyung Hee Univ, Grad Sch, Dept Integrated Drug Dev \& Nat Prod, Seoul 02447, South Korea.},
DOI = {10.3390/f13101688},
Article-Number = {1688},
EISSN = {1999-4907},
Keywords = {DART-TOF-MS; wood species classification; machine learning;
   classification method; artificial neural network (ANN); random forest
   (RF); support vector machine (SVM)},
Keywords-Plus = {ARTIFICIAL NEURAL-NETWORK; REAL-TIME; DISCRIMINANT-ANALYSIS; WORLDVIEW-2
   IMAGERY; DUKUDUKU FOREST; IDENTIFICATION; METABOLOMICS; ORIGIN},
Research-Areas = {Forestry},
Web-of-Science-Categories  = {Forestry},
Author-Email = {ypjang@khu.ac.kr},
Affiliations = {Kyung Hee University; Kyung Hee University; Kyung Hee University; Korea
   Forest Research Institute (KFRI); National Institute of Forest Science
   (NIFOS), Republic of South Korea; Kyung Hee University},
ResearcherID-Numbers = {Lee, Jei-Wan/AET-4404-2022
   },
ORCID-Numbers = {Lee, Jei-Wan/0000-0003-4183-1487
   Lee, YunGyo/0000-0003-4243-3691
   Ahn, Ji Young/0000-0002-8598-2474
   Jang, Young Pyo/0000-0001-5865-9228},
Funding-Acknowledgement = {National Institute of Forest Science, Korea {[}FG0601-2019-02]},
Funding-Text = {This research was supported by a grant (Project No. FG0601-2019-02) from
   the National Institute of Forest Science, Korea in 2021. The finding and
   conclusions of this publication are solely the responsibility of the
   authors and do not necessarily represent the official views of the
   funding agency.},
Cited-References = {{[}Anonymous], 1988, PARALLEL DISTRIBUTED.
   Antunes ACN, 2019, J SCI FOOD AGR, V99, P6973, DOI 10.1002/jsfa.9986.
   Arora M, 2022, METABOLITES, V12, DOI 10.3390/metabo12030232.
   Bachle H, 2012, WOOD SCI TECHNOL, V46, P1181, DOI 10.1007/s00226-012-0481-z.
   Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295.
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324.
   Bylesjo M, 2006, J CHEMOMETR, V20, P341, DOI 10.1002/cem.1006.
   Cha J., 1994, ADV METHODS MARKETIN, V407, P52, DOI DOI 10.1002/0471667196.ESS1914.PUB2.
   Chen JY, 2020, INFRARED PHYS TECHN, V105, DOI 10.1016/j.infrared.2020.103225.
   Cho MA, 2015, INT J APPL EARTH OBS, V38, P349, DOI 10.1016/j.jag.2015.01.015.
   Cody RB, 2012, J ANAL APPL PYROL, V95, P134, DOI 10.1016/j.jaap.2012.01.018.
   Cord M, 2008, COGN TECHNOL, P1, DOI 10.1007/978-3-540-75171-7.
   Corsaro C, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12062824.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Deklerck V, 2019, WOOD SCI TECHNOL, V53, P953, DOI 10.1007/s00226-019-01111-1.
   Deklerck V, 2017, RAPID COMMUN MASS SP, V31, P1582, DOI 10.1002/rcm.7939.
   Dormontt EE, 2015, BIOL CONSERV, V191, P790, DOI 10.1016/j.biocon.2015.06.038.
   El Margae S., 2014, P 2014 9 INT C INTEL.
   Espinoza EO, 2014, RAPID COMMUN MASS SP, V28, P281, DOI 10.1002/rcm.6779.
   Esteban LG, 2017, WOOD SCI TECHNOL, V51, P1249, DOI 10.1007/s00226-017-0932-7.
   Evans PD, 2017, IAWA J, V38, P266, DOI 10.1163/22941932-20170171.
   Finch K, 2017, APPL PLANT SCI, V5, DOI 10.3732/apps.1600158.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Grandini Margherita, 2020, ARXIV.
   HANLEY JA, 1989, CRIT REV DIAGN IMAG, V29, P307.
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747.
   He T, 2019, HOLZFORSCHUNG, V73, P277, DOI 10.1515/hf-2018-0076.
   He T, 2019, PLANTA, V249, P1617, DOI 10.1007/s00425-019-03116-3.
   Holtken AM, 2012, HOLZFORSCHUNG, V66, P97, DOI 10.1515/HF.2011.142.
   Hong Jeong-Ki, 2014, KOREAN JOURNAL OF PLANT TAXONOMY, V44, P111, DOI 10.11110/kjpt.2014.44.2.111.
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427.
   Huang J, 2005, IEEE T KNOWL DATA EN, V17, P299, DOI 10.1109/TKDE.2005.50.
   Jozsa L. A., 1994, Special Publication - Forintek Canada Corp..
   Karatzoglou A., 2006, J STAT SOFTW, V15, P1, DOI {[}10.18637/jss.v015.i09, DOI 10.18637/JSS.V015.I09].
   Karlson M, 2016, INT J APPL EARTH OBS, V50, P80, DOI 10.1016/j.jag.2016.03.004.
   Kim HJ, 2015, METABOLOMICS, V11, P64, DOI 10.1007/s11306-014-0671-9.
   Kowsher M., 2021, EMERG SCI J, V5, P700, DOI 10.28991/esj-2021-01306.
   Lee JG, 2017, KOREAN J RADIOL, V18, P570, DOI 10.3348/kjr.2017.18.4.570.
   Lee LC, 2018, ANALYST, V143, P3526, DOI 10.1039/c8an00599k.
   Lee S, 2019, METABOLITES, V9, DOI 10.3390/metabo9090186.
   Liu SJ, 2022, WOOD SCI TECHNOL, V56, P1567, DOI 10.1007/s00226-022-01404-y.
   Lowe AJ, 2011, IAWA J, V32, P251, DOI 10.1163/22941932-90000055.
   Mahadevan S, 2008, ANAL CHEM, V80, P7562, DOI 10.1021/ac800954c.
   Nag A, 2021, INT J MOL SCI, V22, DOI 10.3390/ijms22084107.
   Nisgoski S, 2017, WOOD SCI TECHNOL, V51, P929, DOI 10.1007/s00226-017-0915-8.
   Okada T, 2010, CURR COMPUT-AID DRUG, V6, P179, DOI 10.2174/157340910791760055.
   Omer G, 2015, IEEE J-STARS, V8, P4825, DOI 10.1109/JSTARS.2015.2461136.
   Ozsahin S, 2018, EUR J WOOD WOOD PROD, V76, P563, DOI 10.1007/s00107-017-1219-2.
   Pal M, 2005, INT J REMOTE SENS, V26, P217, DOI 10.1080/01431160412331269698.
   Pavlovich MJ, 2016, RAPID COMMUN MASS SP, V30, P1123, DOI 10.1002/rcm.7536.
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720.
   Perez-Cova M, 2022, SEPARATIONS, V9, DOI 10.3390/separations9030079.
   Pierce CY, 2007, CHEM COMMUN, P807, DOI 10.1039/b613200f.
   Ranawana R, 2006, IEEE C EVOL COMPUTAT, P2239.
   Reboredo Fernando, 2013, Environment Systems \& Decisions, V33, P295, DOI 10.1007/s10669-013-9444-7.
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0.
   Salem H, 2022, DESALINATION, V522, DOI 10.1016/j.desal.2021.115411.
   Samuel AL., 1988, COMPUTER GAMES, VI, P400, DOI {[}10.1007/978-1-4613-8716-9\_15, DOI 10.1007/978-1-4613-8716-9\_15].
   Sathyanarayana S., 2014, INT J ELECT COMPUT S, V3, P435.
   Schmitz N., 2019, GLOB TIMBER TRACK NE, V43, P1.
   Schmitz N., 2020, GLOB TIMBER TRACK NE.
   Schweingruber FH, 2012, TREES WOOD DENDROCHR.
   Silvello GC, 2021, LWT-FOOD SCI TECHNOL, V140, DOI 10.1016/j.lwt.2020.110836.
   Sisco E, 2021, FORENSIC CHEM, V22, DOI 10.1016/j.forc.2020.100294.
   Sohn SI, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13204149.
   Svozil D, 1997, CHEMOMETR INTELL LAB, V39, P43, DOI 10.1016/S0169-7439(97)00061-0.
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9.
   Wongpoo T, 2022, LECT NOTE NETW SYST, V453, P77, DOI 10.1007/978-3-030-99948-3\_8.
   Wu FY, 2021, WOOD SCI TECHNOL, V55, P553, DOI 10.1007/s00226-021-01261-1.
   Xi Bowei, 2014, Methods Mol Biol, V1198, P333, DOI 10.1007/978-1-4939-1258-2\_22.
   Zhang MM, 2019, HOLZFORSCHUNG, V73, P975, DOI 10.1515/hf-2018-0304.
   Zhang MM, 2019, IAWA J, V40, P58, DOI 10.1163/22941932-40190224.
   Zobel B.J., 1989, WOOD VARIATION ITS C.},
Number-of-Cited-References = {73},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {12},
Journal-ISO = {Forests},
Doc-Delivery-Number = {5P1QQ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000872934300001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000426901200002,
Author = {Rao, Anusha and Kulkarni, S. B.},
Book-Group-Author = {IEEE},
Title = {An Improved Technique of Plant Leaf Classificaion Using Hybrid Feature
   Modeling},
Booktitle = {2017 INTERNATIONAL CONFERENCE ON INNOVATIVE MECHANISMS FOR INDUSTRY
   APPLICATIONS (ICIMIA)},
Year = {2017},
Pages = {5-9},
Note = {IEEE International Conference on Innovative Mechanisms for Industry
   Applications (ICIMIA, Bangalore, INDIA, FEB 21-23, 2017},
Organization = {IEEE; IEEE Bangalore Sect; Dayananda Sagar Coll Engn},
Abstract = {Various applications have been developed during recent years which are
   based on the computer vision system. In this field, plant species
   recognition is a challenging task for researchers due to environmental
   and image acquisition condition of image. Leaf classification
   application can be used for various purpose such as remote sensing
   imaging, botanical characteristically analysis etc. Now a day, amount of
   dataset is increasing rapidly in this field. Thus it motivates us to
   develop an efficient model for leaf classification in terms of
   computation time and accuracy. To address this issue, here in this work
   we propose a hybrid approach for feature extraction to improve the
   classification accuracy. Proposed model follows twofold working process
   whereas in first stage image pre-processing is applied to enhance the
   image quality of image by applying image enhancement with the help of
   auto-regressive model, second stage performs feature computation by
   combining morphological, shape and SIFT feature and finally Deep Neural
   Network is applied for classification performance evaluation. Proposed
   work is a combination of image enhancement, morphological feature, SIFT
   feature and classification technique. For efficient image enhancement,
   auto-regressive model is adopted here. A comparative study of
   classification performance is presented which shows the robustness of
   proposed model},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Rao, A (Corresponding Author), Dayananda Sagar Acad Technol \& Management, Dept ISE, Bengaluru, India.
   Rao, Anusha, Dayananda Sagar Acad Technol \& Management, Dept ISE, Bengaluru, India.
   Kulkarni, S. B., SDM Coll Engn \& Technol, Dept CSE, Dharwad, Karnataka, India.},
ISBN = {978-1-5090-5960-7},
Keywords = {leaf recognition; image enhancement; hybrid feature extraction model;
   Deep Neural Network},
Keywords-Plus = {IMAGE CLASSIFICATION},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Multidisciplinary},
Author-Email = {raoanusha1606@gmail.com
   sbkulkarni\_in@yahoo.com},
ORCID-Numbers = {Kulkarni, Shrinivas/0000-0001-5576-5076},
Cited-References = {{[}Anonymous], IEEE 7 INT S SIGN PR.
   Damodaran BB, 2014, IEEE J-STARS, V7, P2080, DOI 10.1109/JSTARS.2013.2294857.
   Du J., 2005, LNCS.
   Gu X., 2005, LNCS.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Krylov VA, 2011, IEEE J-STSP, V5, P554, DOI 10.1109/JSTSP.2010.2103925.
   Liwen Gao, 2010, Proceedings 2010 Sixth International Conference on Natural Computation (ICNC 2010), P1038, DOI 10.1109/ICNC.2010.5582971.
   Niu Y., IEEE T IMAGE PROCESS, P1.
   Risojevic V, 2016, IEEE J-STARS, V9, P1521, DOI 10.1109/JSTARS.2015.2513898.
   Salvador I., 2011, Proceedings of the 2011 22nd International Conference on Database and Expert Systems Applications (DEXA 2011), P364, DOI 10.1109/DEXA.2011.23.
   Schwartzkopf WC, 2005, IEEE T MED IMAGING, V24, P1593, DOI 10.1109/TMI.2005.859207.
   Shen L, 2014, IEEE GEOSCI REMOTE S, V11, P863, DOI 10.1109/LGRS.2013.2280298.
   Sun ZN, 2014, IEEE T PATTERN ANAL, V36, P1120, DOI 10.1109/TPAMI.2013.234.
   WANG X, 2005, LNCS.
   Xu CY, 2014, IEEE T NEUR NET LEAR, V25, P728, DOI 10.1109/TNNLS.2013.2280752.
   Ye Y., 2004, P 2004 INT S INT MUL.},
Number-of-Cited-References = {16},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BJ6OZ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000426901200002},
DA = {2023-08-12},
}

@inproceedings{ WOS:000364694000028,
Author = {Oncevay-Marcos, Arturo and Juarez-Chambi, Ronald and Khlebnikov-Nunez,
   Sofia and Beltran-Castanon, Cesar},
Editor = {Azzopardi, G and Petkov, N},
Title = {Leaf-Based Plant Identification Through Morphological Characterization
   in Digital Images},
Booktitle = {COMPUTER ANALYSIS OF IMAGES AND PATTERNS, CAIP 2015, PT II},
Series = {Lecture Notes in Computer Science},
Year = {2015},
Volume = {9257},
Pages = {326-335},
Note = {16th International Conference on Computer Analysis of Images and
   Patterns (CAIP), Valletta, MALTA, SEP 02-04, 2015},
Organization = {Maltese Minist Finance; Malta Council Sci \& Technol; Malta Tourism
   Author; Springer; Julich Supercomputing Ctr},
Abstract = {The plant species identification is a manual process performed mainly by
   botanical scientists based on their experience. In order to improve this
   task, several plant classification processes has been proposed applying
   pattern recognition. In this work, we propose a method combining three
   visual attributes of leaves: boundary shape, texture and color. Complex
   networks and multi-scale fractal dimension techniques were used to
   characterize the leaf boundary shape, the Haralick's descriptors for
   texture were extracted, and color moments were calculated. Experiments
   were performed on the ImageCLEF 2012 train dataset, scan pictures only.
   We reached up to 90.41\% of accuracy regarding the leaf-based plant
   identification problem for 115 species.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Oncevay-Marcos, A (Corresponding Author), Pontificia Univ Catolica Peru, Res Grp Pattern Recognit \& Appl Artificial Intell, Dept Engn, Lima, Peru.
   Oncevay-Marcos, Arturo; Juarez-Chambi, Ronald; Khlebnikov-Nunez, Sofia; Beltran-Castanon, Cesar, Pontificia Univ Catolica Peru, Res Grp Pattern Recognit \& Appl Artificial Intell, Dept Engn, Lima, Peru.},
DOI = {10.1007/978-3-319-23117-4\_28},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-23117-4; 978-3-319-23116-7},
Keywords = {Leaf-based plant identification; Complex networks; Multiscale fractal
   dimension; Haralick's descriptors; Color moments},
Keywords-Plus = {CLASSIFICATION; RECOGNITION; TRANSFORM},
Research-Areas = {Computer Science; Robotics},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory \& Methods; Robotics},
Author-Email = {foncevay@pucp.pe
   ronald.juarez@pucp.pe
   sjlebn@pucp.edu.pe
   cbeltran@pucp.pe},
Affiliations = {Pontificia Universidad Catolica del Peru},
ORCID-Numbers = {Oncevay, Felix/0000-0001-7675-6208
   Beltran Castanon, Cesar/0000-0002-0173-4140},
Cited-References = {{[}Anonymous], 2013, ARXIV14014447.
   {[}Anonymous], 2018, FORMULA UNIVERSAL LA.
   {[}Anonymous], 1995, STORAGE RETRIEVAL IM, DOI {[}10.1117/12.205308, DOI 10.1117/12.205308].
   Arora A., 2012, P CLEF 2012 LABS WOR.
   Backes A.R., 2012, ABS12013153 CORR.
   Backes AR, 2009, PATTERN RECOGN, V42, P54, DOI 10.1016/j.patcog.2008.07.006.
   Backes AR, 2010, PATTERN RECOGN LETT, V31, P44, DOI 10.1016/j.patrec.2009.08.007.
   Castanon CAB, 2014, LECT NOTES COMPUT SC, V8827, P580, DOI 10.1007/978-3-319-12568-8\_71.
   Brilhador A., 2013, LNCS, V8258.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Casanova D., 2012, P CLEF 2012 LABS WOR.
   EHSANIRAD A., 2010, INT J COMPUTER SCI I, V8, P78.
   Goeau H., 2012, IMAGECLEF 2012 WORKI.
   Gonzalez R. C., 2006, DIGITAL IMAGE PROCES.
   Gu X, 2005, LECT NOTES COMPUT SC, V3644, P253.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Jau-Ling Shih, 2002, Recent Advances in Visual Information Systems. 5th International Conference VISUAL 2002. Proceedings (Lecture Notes in Computer Science Vol.2314), P88.
   Kadir A., 2013, ARXIV13115829.
   Kebapci H., 2010, COMPUT J.
   Lee CL, 2006, INT J IMAG SYST TECH, V16, P15, DOI 10.1002/ima.20063.
   Lin FY, 2008, COMM COM INF SC, V15, P432.
   Man QK, 2008, COMM COM INF SC, V15, P192.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Pauwels EJ, 2009, ENG APPL ARTIF INTEL, V22, P26, DOI 10.1016/j.engappai.2008.04.017.
   Plotze RD, 2005, CAN J BOT, V83, P287, DOI {[}10.1139/b05-002, 10.1139/B05-002].
   POREBSKI A, 2008, P 4 EUR C COL GRAPH, P316.
   Ryszard S, 2007, ELE COM ENG, V1, P6.
   Werbos P., 1974, REGRESSION NEW TOOLS.
   Zhang XH, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 2, PROCEEDINGS, P773, DOI 10.1109/CISP.2008.88.},
Number-of-Cited-References = {29},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BD9HS},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000364694000028},
DA = {2023-08-12},
}

@article{ WOS:000814639400021,
Author = {Sharma, Aviral and Nigam, Saumya},
Title = {Parametric Model for Flora Detection in Middle Himalayas},
Journal = {INTERNATIONAL JOURNAL OF DECISION SUPPORT SYSTEM TECHNOLOGY},
Year = {2022},
Volume = {14},
Number = {1},
Abstract = {Plant detection forms an integral part of the life of the forest guards,
   researchers, and students in the field of botany and for common people
   also who are curious about knowing a plant. But detecting plants suffer
   a major drawback that the true identifier is only the flower, and in
   certain species, flowering occurs at major time period gaps spanning
   from few months to over 100 years (in certain types of bamboos). Machine
   learning-based systems could be used in developing models where the
   experience of researchers in the field of plant sciences can be
   incorporated into the model. In this paper, the authors present a
   machine learning-based approach based upon other quantifiable parameters
   for the detection of the plant presented. The system takes plant
   parameters as the inputs and will detect the plant family as the output.},
Publisher = {IGI GLOBAL},
Address = {701 E CHOCOLATE AVE, STE 200, HERSHEY, PA 17033-1240 USA},
Type = {Article},
Language = {English},
Affiliation = {Sharma, A (Corresponding Author), Univ Petr \& Energy Studies, Sch Comp Sci, Dept Informat, Dehra Dun, Uttarakhand, India.
   Sharma, Aviral, Univ Petr \& Energy Studies, Sch Comp Sci, Dept Informat, Dehra Dun, Uttarakhand, India.
   Nigam, Saumya, Univ Petr \& Energy Studies, Dehra Dun, Uttarakhand, India.},
DOI = {10.4018/IJDSST.286698},
ISSN = {1941-6296},
EISSN = {1941-630X},
Keywords = {Machine Learning; Plant Recognition},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems},
Affiliations = {University of Petroleum \& Energy Studies (UPES); University of
   Petroleum \& Energy Studies (UPES)},
Cited-References = {{[}Anonymous], 2003, PRACTICAL GUIDE SUPP.
   {[}Anonymous], 2018, NEURAL NETWORKS DEEP, DOI DOI 10.1007/978-3-319-94463-0.
   Crisci C, 2012, ECOL MODEL, V240, P113, DOI 10.1016/j.ecolmodel.2012.03.001.
   Freund Y, 1999, MACHINE LEARNING, PROCEEDINGS, P124.
   Karlik B, 2011, INT J ARTIF INTELL E, V4, P111.
   Nielsen M.A., 2015, NEURAL NETWORKS DEEP, V25.
   Osama K., 2015, PLANTOMICS OMICS PLA, P731, DOI DOI 10.1007/978-81-322-2172-2\_26.
   Ramcharan A, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.00272.
   Singh A, 2016, PROCEEDINGS OF THE 10TH INDIACOM - 2016 3RD INTERNATIONAL CONFERENCE ON COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT, P1310.
   Zhang ML, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON GRANULAR COMPUTING, VOLS 1 AND 2, P718.},
Number-of-Cited-References = {10},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Int. AJ. Decis. Support Syst. Technol.},
Doc-Delivery-Number = {2H9XI},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000814639400021},
DA = {2023-08-12},
}

@article{ WOS:001036231700001,
Author = {Cui, Zhelin and Li, Xinran and Li, Tao and Li, Mingyang},
Title = {Improvement and Assessment of Convolutional Neural Network for Tree
   Species Identification Based on Bark Characteristics},
Journal = {FORESTS},
Year = {2023},
Volume = {14},
Number = {7},
Month = {JUL},
Abstract = {Efficient tree species identification is of great importance in forest
   inventory and management. As the textural properties of tree barks vary
   less notably as a result of seasonal change than other tree organs, they
   are more suitable for the identification of tree species using deep
   learning models. In this study, we adopted the ConvNeXt convolutional
   neural network to identify 33 tree species using the BarkNetV2 dataset,
   compared the classification accuracy values of different tree species,
   and performed visual analysis of the network's visual features. The
   results show the following trends: (1) the pre-trained network weights
   exhibit up to 97.61\% classification accuracy for the test set,
   indicating that the network has high accuracy; (2) the classification
   accuracy values of more than half of the tree species can reach 98\%,
   while the confidence level of correct identification (probability ratio
   of true labels) of tree species images is relatively high; and (3) there
   is a strong correlation between the network's visual attractiveness and
   the tree bark's biological characteristics, which share similarities
   with humans' organization of tree species. The method suggested in this
   study has the potential to increase the efficiency of tree species
   identification in forest resources surveys and is of considerable value
   in forest management.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Li, MY (Corresponding Author), Nanjing Forestry Univ, Coinnovat Ctr Sustainable Forestry Southern China, Nanjing 210037, Peoples R China.
   Cui, Zhelin; Li, Xinran; Li, Tao; Li, Mingyang, Nanjing Forestry Univ, Coinnovat Ctr Sustainable Forestry Southern China, Nanjing 210037, Peoples R China.},
DOI = {10.3390/f14071292},
Article-Number = {1292},
EISSN = {1999-4907},
Keywords = {tree species identification; convolutional neural network; bark image;
   visual attractiveness},
Research-Areas = {Forestry},
Web-of-Science-Categories  = {Forestry},
Author-Email = {3210100096@njfu.edu.cn
   xinran0212@njfu.edu.cn
   litao3014@njfu.edu.cn
   lmy196727@njfu.edu.cn},
Affiliations = {Nanjing Forestry University},
Cited-References = {Alzubaidi L, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00444-8.
   Bertrand S, 2018, ECOL INFORM, V46, P57, DOI 10.1016/j.ecoinf.2018.05.007.
   Blaanco LJ, 2016, INT CONF CONTEMP, P248.
   Boudra S., 2018, P INT C CONT BAS MUL, P1, DOI {[}10.1109/CBMI.2018.8516536, DOI 10.1109/CBMI.2018.8516536].
   Brahme A., 2014, COMPREHENSIVE BIOMED.
   Carpentier M, 2018, IEEE INT C INT ROBOT, P1075, DOI 10.1109/IROS.2018.8593514.
   Chi ZR, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS \& SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P1035.
   Choi S., 2015, P C LAB EV FOR TOUL.
   Chui K.T., 2022, P INT C CYB SEC PRIV, VVolume 599, P248, DOI {[}10.1007/978-3-031-22018-0\_23, DOI 10.1007/978-3-031-22018-0\_23].
   Collins E, 2018, Arxiv, DOI {[}arXiv:1806.10206, 10.1007/978-3-030-01264-9\_21 1806.10206, DOI 10.1007/978-3-030-01264-9\_211806.10206].
   Faizal S., 2022, INT J RES APPL SCI E, V10, P1384, DOI {[}10.22214/ijraset.2022.46846, DOI 10.22214/IJRASET.2022.46846].
   Fekri-Ershad S, 2020, EXPERT SYST APPL, V158, DOI 10.1016/j.eswa.2020.113509.
   Fiel S., 2010, P 16 COMP VIS WINT W, P67.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Kapishnikov A, 2021, PROC CVPR IEEE, P5048, DOI 10.1109/CVPR46437.2021.00501.
   Kim TH, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-06562-4.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986.
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167.
   Misra Debaleena, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12540), P197, DOI 10.1007/978-3-030-65414-6\_15.
   Omeiza D., 2019, INTELL SYST C.
   Paszke A, 2019, Arxiv, DOI {[}arXiv:1912.01703, DOI 10.48550/ARXIV.1912.01703].
   Puri M, 2016, ARTIFICIAL NEURAL NETWORK FOR DRUG DESIGN, DELIVERY AND DISPOSITION, P1.
   Ratajczak R, 2019, VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4, P240, DOI 10.5220/0007361902400248.
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI {[}10.1162/NECO\_a\_00990, 10.1162/neco\_a\_00990].
   Remes V, 2019, PATTERN RECOGN LETT, V125, P612, DOI 10.1016/j.patrec.2019.06.027.
   Robert M, 2020, 2020 17TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2020), P25, DOI 10.1109/CRV50864.2020.00012.
   Ruan D., 2021, P IEEE CVF C COMP VI, P13967, DOI {[}10.48550/arXiv.2104.05160, DOI 10.48550/ARXIV.2104.05160].
   Sebastian R., 2022, MACHINE LEARNING PYT.
   Selvaraju R., 2016, ARXIV.
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74.
   Seon-Jong Kim, 2011, SICE 2011 - 50th Annual Conference of the Society of Instrument and Control Engineers of Japan, P1147.
   Smilkov D, 2017, Arxiv, DOI {[}arXiv:1706.03825, DOI 10.48550/ARXIV.1706.03825].
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Sundararajan M, 2017, PR MACH LEARN RES, V70.
   Wang Y, 2021, WOOD SCI TECHNOL, V55, P1171, DOI 10.1007/s00226-021-01309-2.
   Xin MY, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-019-0417-8.
   Xu ZQJ, 2019, LECT NOTES COMPUT SC, V11953, P264, DOI 10.1007/978-3-030-36708-4\_22.},
Number-of-Cited-References = {39},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Forests},
Doc-Delivery-Number = {N3RT8},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:001036231700001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000892628600043,
Author = {Sapna, R. and Sheshappa, S. N.},
Editor = {Chen, JIZ and Tavares, JMRS and Shi, F},
Title = {An Extensive Study on Machine Learning Paradigms Towards Medicinal Plant
   Classification on Potential of Medicinal Properties},
Booktitle = {THIRD INTERNATIONAL CONFERENCE ON IMAGE PROCESSING AND CAPSULE NETWORKS
   (ICIPCN 2022)},
Series = {Lecture Notes in Networks and Systems},
Year = {2022},
Volume = {514},
Pages = {541-555},
Note = {3rd International Conference on Image Processing and Capsule Networks
   (ICIPCN), ELECTR NETWORK, MAY 20-21, 2022},
Abstract = {The automatic classification of medicinal plants requires more
   exploration as it is considered as major issue for conservation,
   authentication, and manufacturing of medicines. Generally, medicinal
   plants have been classified by features of the leaf with respect to
   color, shape and texture. Leaf is a main parameter on analyzing its
   plant nutrition, plant contentions, plant soil-water association, plant
   preservation measures, crop ecosystems, plant respiration rate, plant
   transpiration rate and plant photosynthesis. Classification of the plant
   species is a primary and highly essential procedure for plant
   conservation. An object recognition system is required to classify the
   various species of the plant species and to protect them from various
   diseases. In this article, a detailed survey on machine learning models
   has been carried out to identify and classify medicinal plants by
   considering the texture and shape features of a plant leaf using linear
   and non linear feature descriptors. However the extracted features from
   the plant leaf image will be huge containing high redundancy
   information's. On employment of feature selection techniques through
   weighted average strategies through metaheuristic techniques, those
   techniques reduces the redundancy on feature extracted and minimizes the
   equal error rate to obtain the optimum weighted features. Further
   numerous classification techniques on supervised and unsupervised types
   has been employed to classify the optimal feature on various dataset has
   been experimented and validated using cross fold validation using
   confusion matrix. It is vital and essential task for providing detailed
   insight on that classification model for medicinal plant with respect to
   its medicinal properties. The efficacy of each model has been
   demonstrated on single plant and multiple plants on basis of classifier
   and dataset employed. Finally outline of the proposed methodology as
   framework to classify the medicinal plant has been provided. Evaluation
   of models has been carried out on the processing of the dataset.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Sapna, R (Corresponding Author), Visveshwaraya Technol Univ, Sir M Visvesvaraya Inst Technol, Bengaluru, India.
   Sapna, R (Corresponding Author), Presidency Univ, Dept Comp Sci \& Engn, Bengaluru, India.
   Sapna, R.; Sheshappa, S. N., Visveshwaraya Technol Univ, Sir M Visvesvaraya Inst Technol, Bengaluru, India.
   Sapna, R.; Sheshappa, S. N., Presidency Univ, Dept Comp Sci \& Engn, Bengaluru, India.},
DOI = {10.1007/978-3-031-12413-6\_43},
ISSN = {2367-3370},
EISSN = {2367-3389},
ISBN = {978-3-031-12413-6; 978-3-031-12412-9},
Keywords = {Medicinal plant classification; Machine learning; Feature extraction;
   Feature selection; Feature normalization},
Research-Areas = {Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Imaging Science \& Photographic Technology},
Author-Email = {sapna.aradhya@gmail.com
   sheshappa\_is@sirmvit.in},
Affiliations = {Visvesvaraya Technological University; Presidency University, Bangalore},
Cited-References = {Abbas Z, 2019, PROCEEDINGS 2019 AMITY INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AICAI), P317, DOI 10.1109/AICAI.2019.8701374.
   Bantan RAR, 2020, CHAOS, V30, DOI 10.1063/5.0024017.
   Cardinali F, 2021, HERITAGE-BASEL, V4, P220, DOI 10.3390/heritage4010013.
   Bloice MD, 2017, Arxiv, DOI {[}arXiv:1708.04680, 10.21105/joss.00432, DOI 10.21105/JOSS.00432].
   Dahigaonkar T.D., 2018, INT RES J ENG TECHNO, V5, P351.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Gopal A, 2012, 2012 INTERNATIONAL CONFERENCE ON MACHINE VISION AND IMAGE PROCESSING (MVIP), P5, DOI 10.1109/MVIP.2012.6428747.
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957.
   Kumar J. P., 2019, Information Processing in Agriculture, V6, P233.
   Manoj Kumar P., 2017, ID AYURV MED PLANTS.
   Moshou D, 2005, REAL-TIME IMAGING, V11, P75, DOI 10.1016/j.rti.2005.03.003.
   Nikbakhsh N, 2019, COMPUT ELECTRON AGR, V162, P440, DOI 10.1016/j.compag.2019.04.038.
   Pavithra N, 2016, INT J COMPUT SCI MOB, V5, P519.
   Sapna R., 2019, P 2019 IEEE INT C EL, P1.
   Turkoglu M, 2019, PHYSICA A, V527, DOI 10.1016/j.physa.2019.121297.
   Turkoglu M, 2019, APPL MATH COMPUT, V352, P1, DOI 10.1016/j.amc.2019.01.054.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {17},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BU3QG},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000892628600043},
DA = {2023-08-12},
}

@article{ WOS:000454884300001,
Author = {Seeland, Marco and Rzanny, Michael and Boho, David and Waeldchen, Jana
   and Maeder, Patrick},
Title = {Image-based classification of plant genus and family for trained and
   untrained plant species},
Journal = {BMC BIOINFORMATICS},
Year = {2019},
Volume = {20},
Month = {JAN 3},
Abstract = {BackgroundModern plant taxonomy reflects phylogenetic relationships
   among taxa based on proposed morphological and genetic similarities.
   However, taxonomical relation is not necessarily reflected by close
   overall resemblance, but rather by commonality of very specific
   morphological characters or similarity on the molecular level. It is an
   open research question to which extent phylogenetic relations within
   higher taxonomic levels such as genera and families are reflected by
   shared visual characters of the constituting species. As a consequence,
   it is even more questionable whether the taxonomy of plants at these
   levels can be identified from images using machine learning
   techniques.ResultsWhereas previous studies on automated plant
   identification from images focused on the species level, we investigated
   classification at higher taxonomic levels such as genera and families.
   We used images of 1000 plant species that are representative for the
   flora of Western Europe. We tested how accurate a visual representation
   of genera and families can be learned from images of their species in
   order to identify the taxonomy of species included in and excluded from
   learning. Using natural images with random content, roughly 500 images
   per species are required for accurate classification. The classification
   accuracy for 1000 species amounts to 82.2\% and increases to 85.9\% and
   88.4\% on genus and family level. Classifying species excluded from
   training, the accuracy significantly reduces to 38.3\% and 38.7\% on
   genus and family level. Excluded species of well represented genera and
   families can be classified with 67.8\% and 52.8\% accuracy.ConclusionOur
   results show that shared visual characters are indeed present at higher
   taxonomic levels. Most dominantly they are preserved in flowers and
   leaves, and enable state-of-the-art classification algorithms to learn
   accurate visual representations of plant genera and families. Given a
   sufficient amount and composition of training data, we show that this
   allows for high classification accuracy increasing with the taxonomic
   level and even facilitating the taxonomic identification of species
   excluded from the training process.},
Publisher = {BMC},
Address = {CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Seeland, M (Corresponding Author), Tech Univ Ilmenau, Inst Comp \& Syst Engn, Helmholtzpl 5, D-98693 Ilmenau, Germany.
   Seeland, Marco; Boho, David; Maeder, Patrick, Tech Univ Ilmenau, Inst Comp \& Syst Engn, Helmholtzpl 5, D-98693 Ilmenau, Germany.
   Rzanny, Michael; Waeldchen, Jana, Max Planck Inst Biogeochem, Dept Biogeochem Integrat, Hans Knoll Str 10, D-07745 Jena, Germany.},
DOI = {10.1186/s12859-018-2474-x},
Article-Number = {4},
ISSN = {1471-2105},
Keywords = {Plant identification; Deep learning; Zero-shot classification; Computer
   vision; Taxonomy},
Research-Areas = {Biochemistry \& Molecular Biology; Biotechnology \& Applied
   Microbiology; Mathematical \& Computational Biology},
Web-of-Science-Categories  = {Biochemical Research Methods; Biotechnology \& Applied Microbiology;
   Mathematical \& Computational Biology},
Author-Email = {marco.seeland@tu-ilmenau.de},
Affiliations = {Technische Universitat Ilmenau; Max Planck Society},
ResearcherID-Numbers = {Rzanny, Michael/GOH-1028-2022
   Seeland, Marco/H-1028-2011
   Mäder, Patrick/A-1848-2018},
ORCID-Numbers = {Rzanny, Michael/0000-0002-7232-5547
   Seeland, Marco/0000-0001-7204-3972
   Mäder, Patrick/0000-0001-6871-2707},
Funding-Acknowledgement = {German Ministry of Education and Research (BMBF) {[}01LC1319A,
   01LC1319B]; German Federal Ministry for the Environment, Nature
   Conservation, Building and Nuclear Safety (BMUB) {[}3514 685C19];
   Stiftung Naturschutz Thuringen (SNT) {[}SNT-082-248-03/2014]},
Funding-Text = {We are funded by the German Ministry of Education and Research (BMBF)
   grants: 01LC1319A and 01LC1319B; the German Federal Ministry for the
   Environment, Nature Conservation, Building and Nuclear Safety (BMUB)
   grant: 3514 685C19; and the Stiftung Naturschutz Thuringen (SNT) grant:
   SNT-082-248-03/2014. Funding played no role in the design of the study
   or collection, analysis, or interpretation of data.},
Cited-References = {Abadi Martin, 2016, arXiv.
   {[}Anonymous], 2009, BIOL SYSTEMATICS PRI.
   {[}Anonymous], 2016, PLANTCLEF.
   Bremer K, 1998, ANN MO BOT GARD, V85, P531, DOI 10.2307/2992015.
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011.
   CHASE MW, 1993, ANN MO BOT GARD, V80, P528, DOI 10.2307/2399846.
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097.
   Donahue J, 2014, PR MACH LEARN RES, V32.
   Entwisle TJ, 2005, AUST SYST BOT, V18, P1, DOI 10.1071/SB04013.
   Goeau H., 2016, WORKING NOTES CLEF 2, P428.
   Haston E, 2009, BOT J LINN SOC, V161, P128, DOI {[}10.1111/j.1095-8339.2009.01000.x, 10.1111/j.1095-8339.2009.00996.x].
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hofmann M, 2019, INT J COMPUT VISION, V127, P207, DOI 10.1007/s11263-018-1093-3.
   Rouhan G, 2014, METHODS MOL BIOL, V1115, P1, DOI 10.1007/978-1-62703-767-9\_1.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Rzanny M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0245-8.
   Seeland M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170629.
   Simpson MG, 2010, PLANT SYSTEMATICS, 2ND EDITION, P1.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113.
   Wittich HC, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2201-7.},
Number-of-Cited-References = {23},
Times-Cited = {23},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {16},
Journal-ISO = {BMC Bioinformatics},
Doc-Delivery-Number = {HG3OP},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000454884300001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000882350300002,
Author = {Chulif, Sophia and Lee, Sue Han and Chang, Yang Loong and Chai, Kok Chin},
Title = {A machine learning approach for cross-domain plant identification using
   herbarium specimens},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2023},
Volume = {35},
Number = {8, SI},
Pages = {5963-5985},
Month = {MAR},
Abstract = {The preservation of plant specimens in herbaria has been carried out for
   centuries in efforts to study and confirm plant taxa. With the
   increasing collection of herbaria made available digitally, it is
   practical to use herbarium specimens for the automation of plant
   identification. They are also substantially more accessible and less
   expensive to obtain compared to field images. In fact, in remote and
   inaccessible habitats, field images of rare plant species are still
   immensely lacking. As a result, rare plant species identification is
   challenging due to the deficiency of training data. To address this
   problem, we investigate a cross-domain adaptation approach that allows
   knowledge transfer from a model learned from herbarium specimens to
   field images. We propose a model called Herbarium-Field Triplet Loss
   Network (HFTL network) to learn the mapping between herbarium and field
   domains. Specifically, the model is trained to maximize the embedding
   distance of different plant species and minimize the embedding distance
   of the same plant species given herbarium-field pairs. This paper
   presents the implementation and performance of the HFTL network to
   assess the herbarium-field similarity of plants. It corresponds to the
   cross-domain plant identification challenge in PlantCLEF 2020 and
   PlantCLEF 2021. Despite the lack of field images, our results show that
   the network can generalize and identify rare species. Our proposed HFTL
   network achieved a mean reciprocal rank score of 0.108 and 0.158 on the
   test set related to the species with few training field photographs in
   PlantCLEF 2020 and PlantCLEF 2021, respectively.},
Publisher = {SPRINGER LONDON LTD},
Address = {236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Chulif, S (Corresponding Author), Swinburne Univ Technol, Fac Engn Comp \& Sci, Sarawak Campus, Kuching, Malaysia.
   Chulif, S (Corresponding Author), NEUON AI SDN BHD, Dept Artificial Intelligence, Kota Samarahan, Malaysia.
   Chulif, Sophia; Lee, Sue Han, Swinburne Univ Technol, Fac Engn Comp \& Sci, Sarawak Campus, Kuching, Malaysia.
   Chulif, Sophia; Chang, Yang Loong; Chai, Kok Chin, NEUON AI SDN BHD, Dept Artificial Intelligence, Kota Samarahan, Malaysia.
   Chang, Yang Loong, Univ Malaya, Fac Comp Sci \& Informat Technol, Kuala Lumpur, Malaysia.},
DOI = {10.1007/s00521-022-07951-6},
EarlyAccessDate = {NOV 2022},
ISSN = {0941-0643},
EISSN = {1433-3058},
Keywords = {Plant identification; Herbarium; Triplet loss; Convolutional neural
   networks; Computer vision},
Keywords-Plus = {SHAPE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {schulif@swinburne.edu.my
   shlee@swinburne.edu.my
   yangloong@neuon.ai
   kc@neuon.ai},
Affiliations = {Swinburne University of Technology; Swinburne University of Technology
   Sarawak; Universiti Malaya},
Funding-Acknowledgement = {Fundamental Research Grant Scheme (FRGS) MoHE Grant from the Ministry of
   Higher Education Malaysia {[}FRGS/1/2021/ICT02/SWIN/03/2]; NEUON AI SDN.
   BHD.},
Funding-Text = {This research was supported by the Fundamental Research Grant Scheme
   (FRGS) MoHE Grant No. (Ref: FRGS/1/2021/ICT02/SWIN/03/2), from the
   Ministry of Higher Education Malaysia and NEUON AI SDN. BHD. We thank
   Prof. Dr. Chan Chee Seng for providing professional guidance and
   comments that greatly improved this manuscript. We would also like to
   thank our anonymous reviewers for their time and effort in giving us
   valuable comments and suggestions for improving the quality of this
   manuscript.},
Cited-References = {Abadi M., 2016, TENSORFLOW LARGE SCA, DOI {[}DOI 10.1038/NN.3331, DOI 10.5555/3026877.3026899].
   {[}Anonymous], 2010, INT J COMP APPL.
   Antonelli A., 2020, STATE WORLDS PLANTS.
   Bar-On YM, 2018, P NATL ACAD SCI USA, V115, P6506, DOI 10.1073/pnas.1711842115.
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
   Bebber DP, 2010, P NATL ACAD SCI USA, V107, P22169, DOI 10.1073/pnas.1011841108.
   Buitinck L., 2013, ECML PKDD WORKSHOP L, P108, DOI DOI 10.48550/ARXIV.1309.0238.
   Carranza-Rojas J, 2018, MULTIMED SYST APPL, P151, DOI 10.1007/978-3-319-76445-0\_9.
   Carranza-Rojas J, 2017, BMC EVOL BIOL, V17, P1, DOI 10.1186/s12862-017-1014-z.
   Chen Q, 2014, CLEF WORKING NOTES, P693.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964.
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
   Fiel S., 2010, AUTOMATED IDENTIFICA.
   Goeau H., 2019, CLEF 2019 C LABS EVA.
   Goeau H., 2021, WORKING NOTES CLEF 2, V2936, P1422.
   Goeau H., 2013, PL NTNET MOBILE APP, P423, DOI {[}10.1145/2502081.2502251, DOI 10.1145/2502081.2502251].
   Goeau Herve, 2020, CLEF WORKING NOTES 2.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Guadarrama Sergio, 2016, TENSORFLOW SLIM LIGH.
   Guo YH, 2021, APPL SOFT COMPUT, V108, DOI 10.1016/j.asoc.2021.107523.
   Guru DS., 2010, INT J COMPUTERS APPL, V1, P21.
   Heberling JM, 2018, APPL PLANT SCI, V6, DOI 10.1002/aps3.1193.
   Hernandez S, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106597.
   Horiguchi S, 2020, IEEE T PATTERN ANAL, V42, P1279, DOI 10.1109/TPAMI.2019.2911075.
   Jackson DW., 2003, BOT GARD CONSERV NEW, V3, P42.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Jones HG, 2020, AOB PLANTS, V12, DOI 10.1093/aobpla/plaa052.
   Karthik R, 2020, APPL SOFT COMPUT, V86, DOI 10.1016/j.asoc.2019.105933.
   Kasinathan T, 2021, NEURAL COMPUT APPL, V33, P7491, DOI 10.1007/s00521-020-05497-z.
   Kebapci H, 2011, COMPUT J, V54, P1475, DOI 10.1093/comjnl/bxq037.
   Kew RBG, 2016, STATE WORLDS PLANTS.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Krishna NH, 2020, CLEF WORKING NOTES.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Little DP, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11365.
   Lowe DG., 1999, P 7 IEEE INT C COMPU, V2, P1150, DOI {[}10.1109/iccv.1999.790410, DOI 10.1109/ICCV.1999.790410].
   Mader P, 2021, METHODS ECOL EVOL, V12, P1335, DOI 10.1111/2041-210X.13611.
   O'Mahony N, 2020, ADV INTELL SYST COMP, V943, P128, DOI 10.1007/978-3-030-17795-9\_10.
   Pearline SA, 2019, J INTELL FUZZY SYST, V36, P1997, DOI 10.3233/JIFS-169911.
   PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P170, DOI 10.1109/TSMC.1977.4309681.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Saeed F, 2021, APPL SOFT COMPUT, V103, DOI 10.1016/j.asoc.2021.107164.
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682.
   Siripibal N, 2019, P 2019 INT C MANAGEM, P209, DOI {[}10.1145/3335550.3335584, DOI 10.1145/3335550.3335584].
   SUN Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI DOI 10.1155/2017/7361042.
   Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1609/AAAI.V31I1.11231.
   Tellaeche A, 2011, APPL SOFT COMPUT, V11, P908, DOI 10.1016/j.asoc.2010.01.011.
   Thiers BM., INDEX HERBARIORUM.
   Uguz S, 2021, NEURAL COMPUT APPL, V33, P4133, DOI 10.1007/s00521-020-05235-5.
   Unger J, 2016, BMC EVOL BIOL, V16, DOI 10.1186/s12862-016-0827-5.
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579.
   Villacis J, 2020, CLEF WORKING NOTES.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Willis, 2017, STATE WORLDS PLANTS.
   Zhang Y, 2021, WORKING NOTES CLEF.
   Zhang Yuhao, 2020, ARXIV.
   ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319.},
Number-of-Cited-References = {61},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Neural Comput. Appl.},
Doc-Delivery-Number = {9L2RV},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000882350300002},
OA = {hybrid},
DA = {2023-08-12},
}

@article{ WOS:000782408300001,
Author = {Gajjar, Viraj K. and Nambisan, Anand K. and Kosbar, Kurt L.},
Title = {Plant Identification in a Combined-Imbalanced Leaf Dataset},
Journal = {IEEE ACCESS},
Year = {2022},
Volume = {10},
Pages = {37882-37891},
Abstract = {Plant identification has applications in ethnopharmacology and
   agriculture. Since leaves are one of a distinguishable feature of a
   plant, they are routinely used for identification. Recent developments
   in deep learning have made it possible to accurately identify the
   majority of samples in five publicly available leaf datasets. However,
   each dataset captures the images in a highly controlled environment.
   This paper evaluates the performance of EfficientNet and several other
   convolutional neural network (CNN) architectures when applied to a
   combination of the LeafSnap, Middle European Woody Plants 2014, Flavia,
   Swedish, and Folio datasets. To normalize the impact of imbalance
   resulting from combining the original datasets, we used oversampling,
   undersampling, and transfer learning techniques to construct an
   end-to-end CNN classifier. We placed greater emphasis on metrics
   appropriate for a diverse-imbalanced dataset rather than stressing high
   performance on any one of the original datasets. A model from
   EfficientNet's family of CNN models achieved a highly accurate F-score
   of 0.9861 on the combined dataset.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Gajjar, VK (Corresponding Author), Missouri Univ Sci \& Technol, Dept Elect \& Comp Engn, Rolla, MO 65409 USA.
   Gajjar, Viraj K.; Nambisan, Anand K.; Kosbar, Kurt L., Missouri Univ Sci \& Technol, Dept Elect \& Comp Engn, Rolla, MO 65409 USA.},
DOI = {10.1109/ACCESS.2022.3165583},
ISSN = {2169-3536},
Keywords = {Feature extraction; Convolutional neural networks; Plants (biology);
   Support vector machines; Measurement; Transfer learning; Training data;
   Leaf dataset; imbalanced dataset; convolutional neural networks;
   transfer learning; plant identification},
Keywords-Plus = {RECOGNITION; NOMENCLATURE; NAME},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {vgf4c@mst.edu},
Affiliations = {University of Missouri System; Missouri University of Science \&
   Technology},
ORCID-Numbers = {Nambisan, Anand/0000-0003-4565-4609
   Gajjar, Viraj/0000-0002-5998-0571},
Cited-References = {Abadi M., 2015, TENSORFLOW LARGE SCA, P285.
   Araujo VM, 2022, NEUROCOMPUTING, V467, P427, DOI 10.1016/j.neucom.2021.10.015.
   Babatunde O., 2015, Journal of Agricultural Informatics, V6, P61.
   Batista GEAPA, 2004, ACM SIGKDD EXPLORATI, V6, P20, DOI {[}10.1145/1007730.1007735, DOI 10.1145/1007730.1007735].
   Bennett BC, 2014, J ETHNOPHARMACOL, V152, P387, DOI 10.1016/j.jep.2013.11.042.
   Bennett Bradley C, 2008, J Soc Integr Oncol, V6, P150.
   Bussmann R. W., 2015, EVIDENCE BASED VALID, P87.
   Chaudhury A, 2020, IEEE ACM T COMPUT BI, V17, P1042, DOI 10.1109/TCBB.2018.2873611.
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195.
   Chollet F., 2015, KERAS.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Fabricant DS, 2001, ENVIRON HEALTH PERSP, V109, P69, DOI 10.2307/3434847.
   Fernandez A, 2018, J ARTIF INTELL RES, V61, P863, DOI 10.1613/jair.1.11192.
   Gajjar V., 2018, P INT TEL C GLEND AZ, V54, P569.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Huang G., 2017, P 2017 IEEE C COMP V, P4700, DOI {[}DOI 10.1109/CVPR.2017.243, 10.1109/CVPR.2017.243].
   Jiang HX, 2020, IEEE ACCESS, V8, P68828, DOI 10.1109/ACCESS.2020.2986946.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar M, 2019, IEEE ACCESS, V7, P163912, DOI 10.1109/ACCESS.2019.2952176.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Li ZB, 2020, COMPUT ELECTRON AGR, V176, DOI 10.1016/j.compag.2020.105672.
   Ling C. X., 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P73.
   McKinney W., 2010, P 9 PYTH SCI C, P56, DOI DOI 10.25080/MAJORA-92BF1922-00A.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Osuna E, 1997, NEURAL NETWORKS FOR SIGNAL PROCESSING VII, P276, DOI 10.1109/NNSP.1997.622408.
   Pankaja K, 2017, 2017 INTERNATIONAL CONFERENCE ON INNOVATIVE MECHANISMS FOR INDUSTRY APPLICATIONS (ICIMIA), P442, DOI 10.1109/ICIMIA.2017.7975654.
   Pawara P, 2017, LECT NOTES COMPUT SC, V10617, P615, DOI 10.1007/978-3-319-70353-4\_52.
   Real E, 2019, AAAI CONF ARTIF INTE, P4780.
   Reul C., 2016, INT J ARTIF INTELL A, V7, P1, DOI DOI 10.5121/IJAIA.2016.7201.
   Rivera D, 2014, J ETHNOPHARMACOL, V152, P393, DOI 10.1016/j.jep.2013.12.022.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI {[}10.1109/ICCV.2017.74, 10.1007/s11263-019-01228-7].
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Song YP, 2019, IEEE ACCESS, V7, P163277, DOI 10.1109/ACCESS.2019.2951607.
   Raj APSS, 2019, IET IMAGE PROCESS, V13, P2176, DOI 10.1049/iet-ipr.2019.0346.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Tan CQ, 2018, LECT NOTES COMPUT SC, V11141, P270, DOI 10.1007/978-3-030-01424-7\_27.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Turkoglu M, 2019, APPL MATH COMPUT, V352, P1, DOI 10.1016/j.amc.2019.01.054.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang X, 2020, IEEE ACCESS, V8, P39175, DOI 10.1109/ACCESS.2020.2976117.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Wu ZN, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21113647.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zeng XC, 2000, J EXP THEOR ARTIF IN, V12, P1, DOI 10.1080/095281300146272.},
Number-of-Cited-References = {50},
Times-Cited = {2},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {8},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {0M8OR},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000782408300001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000561117201046,
Author = {Kala, Jules R. and Shabat, Aboubeda and Adetiba, Emmanuel and Akanle,
   Mathews B.},
Editor = {Soliman, KS},
Title = {Hybrid Leaf Recognition Model for Plant Classification using
   Convolutional Neural Network Crafted Features},
Booktitle = {VISION 2025: EDUCATION EXCELLENCE AND MANAGEMENT OF INNOVATIONS THROUGH
   SUSTAINABLE ECONOMIC COMPETITIVE ADVANTAGE},
Year = {2019},
Pages = {8162-8169},
Note = {34th International-Business-Information-Management-Association (IBIMA)
   Conference, Madrid, SPAIN, NOV 13-14, 2019},
Organization = {Int Business Informat Management Assoc},
Abstract = {The recognition of plants species based on leaf characterization is one
   of the first approaches used for the design of plants taxonomy. The
   shape is the most used characteristic of plant leaf and it is considered
   to be a stable feature when used for the recognition of plant species,
   using leaf image. Convolutional Neural Network(CNN) design is inspired
   from the human brain, it has been used for object recognition with
   success. This paper explores the application of CNN to improve the
   recognition process of plant species using leaf images. To achieve this,
   a model based on the combination of various implementations of CNN with
   other classifiers such as logistic regression classifier is proposed.
   The CNN in the proposed model is used for features extraction and the
   selected classifier is used for recognition. In order to evaluate this
   model, two experiments were conducted using FLAVIA and LeafSnap leaf
   image datasets. The proposed model achieves 95\% with LeafSnap and 99\%
   with FLAVIA using Vgg16+LogisticRegression.},
Publisher = {INT BUSINESS INFORMATION MANAGEMENT ASSOC-IBIMA},
Address = {34 E GERMANTOWN PIKE, NO. 327, NORRISTOWN, PA 19401 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kala, JR (Corresponding Author), Univ KwaZulu Natal, Dept Comp Sci, Durban, South Africa.
   Kala, Jules R., Univ KwaZulu Natal, Dept Comp Sci, Durban, South Africa.
   Shabat, Aboubeda, Durban Univ Technol, Dept Informat Technol, Durban, South Africa.
   Adetiba, Emmanuel; Akanle, Mathews B., Covenant Univ, Dept Elect \& Informat Engn, Canaanland, Ota, Nigeria.
   Adetiba, Emmanuel, Durban Univ Technol, Inst Syst Sci, HRA, POB 1334, Durban, South Africa.},
ISBN = {978-0-9998551-3-3},
Keywords = {Convolutional Neural Network; Features Extraction; Shape; SVM; Vgg16},
Keywords-Plus = {IDENTIFICATION},
Research-Areas = {Business \& Economics},
Web-of-Science-Categories  = {Business; Economics; Management},
Author-Email = {213569414@stu.ukzn.ac.z
   ashaba@dut.ac.za
   emmanuel.adetiba@covenantuniversity.edu.ng
   bola.akanle@covenantuniversity.edu.ng},
Affiliations = {University of Kwazulu Natal; Durban University of Technology; Covenant
   University; Durban University of Technology},
ResearcherID-Numbers = {Adetiba, Emmanuel/AAC-4129-2022},
Cited-References = {Adetiba E., 2015, SCI WORLD J.
   Adetiba E, 2016, ADV INTELL SYST, V419, P281, DOI 10.1007/978-3-319-27400-3\_25.
   Adetiba E, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0143542.
   Aickelin U, 2003, LECT NOTES COMPUT SC, V2787, P147.
   Annant B, 2013, INT J INNOVATION APP, P375.
   {[}Anonymous], 2013, APPL LOGISTIC REGRES, DOI DOI 10.1002/9781118548387.
   {[}Anonymous], 2016, CLEF WORKING NOTES.
   Babu M, 2007, INDIAK ISAN NET EXPE.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Bendiab E., 2011, INT J DIGITAL INFORM, V1, P284.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Hussein A. N., 2011, 2011 Proceedings of IEEE 7th International Colloquium on Signal Processing \& its Applications (CSPA 2011), P11, DOI 10.1109/CSPA.2011.5759833.
   Jassmann T.J., 2015, P SOUTHEASTCON 2015, P1, DOI {[}10.1109/SECON.2015.7132978, DOI 10.1109/SECON.2015.7132978].
   Kulkarin A. H, 2013, INT J ADV RES COMPUT.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Linnus C, 1735, LUGDUNI BATAVORUM HA.
   Stephen G, 2007, COMPUT SCI ARTIF INT, P11.
   Tzionas P., 2005, 5 INT C ON TECHN AUT, P365.},
Number-of-Cited-References = {21},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BP7AN},
Web-of-Science-Index = {Conference Proceedings Citation Index- Social Science &amp; Humanities (CPCI-SSH)},
Unique-ID = {WOS:000561117201046},
DA = {2023-08-12},
}

@article{ WOS:000923040200009,
Author = {Sapna, R. and Sheshappa, S. N. and Vijayakarthik, P. and Raj, S.
   Pravinth},
Title = {Global Pattern Feedforward Neural Network Structure with Bacterial
   Foraging Optimization towards Medicinal Plant Leaf Identification and
   Classification},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
Year = {2022},
Volume = {13},
Number = {12},
Pages = {63-70},
Month = {DEC},
Abstract = {Medicinal Plant species help to cure various diseases across the world.
   The automated identification of medicinal plant species to treat disease
   based on their structure is much required in pharmaceutical
   laboratories. Plant Species with a complex background in the field will
   make the detection and classification more difficult. In this paper,
   optimization of bacterial foraging technique has been employed towards
   medicinal plant prediction and classification architecture based on
   feed-forward neural network. It is capable of identifying both complex
   structures of medicinal plants. Feed-forward Neural Networks are
   considered to have good recognition accuracy compared to other machine
   learning approaches. Further bacterial foraging has been implemented to
   minimize the feature search space to the classifier and provides optimal
   features for the plant classification. The experimental outcomes of the
   proposed approach has been analysed by employing the medley dataset and
   evaluating the performance of the proposed approach with respect to dice
   similarity coefficient, Specificity and sensitivity towards medicinal
   plant classification. The findings are very positive, and further
   research will focus on using a large dataset and increased computing
   resources to examine how well deep-learning neural networks function in
   identifying medicinal plants for use in health care.},
Publisher = {SCIENCE \& INFORMATION SAI ORGANIZATION LTD},
Address = {19 BOLLING RD, BRADFORD, WEST YORKSHIRE, 00000, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Sapna, R (Corresponding Author), VTU, Sir MVIT, Belgaum, India.
   Sapna, R., VTU, Sir MVIT, Belgaum, India.
   Sheshappa, S. N.; Vijayakarthik, P., Sir MVIT, Dept Informat Sci \& Engn, Bangalore, Karnataka, India.
   Raj, S. Pravinth, Presidency Univ, Dept Comp Sci \& Engn, Bangalore, Karnataka, India.},
ISSN = {2158-107X},
EISSN = {2156-5570},
Keywords = {Medicinal plant; feed-forward neural networks; linear discriminant
   analysis; bacterial forging},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Affiliations = {Visvesvaraya Technological University; Presidency University, Bangalore},
Cited-References = {Arai K., 2013, INT JOURNALOF ADV RE, V2, P60, DOI 10.14569/ijarai.2013.020309.
   Aravind KR, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON INVENTIVE SYSTEMS AND CONTROL (ICISC 2018), P1191, DOI 10.1109/ICISC.2018.8398993.
   Berger DK, 2017, S AFR J BOT, V109, P327, DOI 10.1016/j.sajb.2017.01.028.
   DeChant C, 2017, PHYTOPATHOLOGY, V107, P1426, DOI 10.1094/PHYTO-11-16-0417-R.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Du MG, 2009, LECT NOTES ARTIF INT, V5755, P627.
   Dudi Bhanuprakash, 2019, INT J ADV TRENDS COM, V8.
   EASON G, 1955, PHILOS TR R SOC S-A, V247, P529, DOI 10.1098/rsta.1955.0005.
   Fitzgerald M, 2020, FRONT PHARMACOL, V10, DOI 10.3389/fphar.2019.01480.
   Gao W, 2012, HUM BRAIN MAPP, V33, P192, DOI 10.1002/hbm.21204.
   Gavhale Nayana G., 2020, INT RES J ENG TECHNO, V07.
   Herdiyeni Y, 2012, 2012 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER SCIENCE AND INFORMATION SYSTEMS, P301.
   Hossain J., 2010, Proceedings of the 13th International Conference on Computer and Information Technology (ICCIT 2010), P458, DOI 10.1109/ICCITECHN.2010.5723901.
   Katiyar S, 2020, INT J ADV COMPUT SC, V11, P793.
   Le TL, 2014, 5 S INF COMM TECHN H, P146.
   Lv MJ, 2020, IEEE ACCESS, V8, P57952, DOI 10.1109/ACCESS.2020.2982443.
   mendeley, MED LEAF DATASET.
   Mohanraj K., 2018, IMPPAT CURATED DATAB.
   Pallavi M, 2022, INT J ADV COMPUTER S, V13, DOI {[}10.14569/IJACSA.2022.0131070, DOI 10.14569/IJACSA.2022.0131070].
   Prasvita D.S., 2013, INT J ADV SCI ENG IN, V3, P5, DOI {[}10.18517/ijaseit.3.2.287, DOI 10.18517/IJASEIT.3.2.287].
   Praveena KN, 2022, INT J ADV COMPUT SC, V13, P362.
   Pukhrambam Banita, 2022, INT J ADV COMPUTER S, V13, DOI {[}10.14569/IJACSA.2022.0130614, DOI 10.14569/IJACSA.2022.0130614].
   Rani KSS, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/5872401.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Renukaradhya S., ARTIF INTELL, P15.
   Sapna R, 2022, LECT NOTE NETW SYST, V514, P541, DOI 10.1007/978-3-031-12413-6\_43.
   Sapna R., 2019, P 2019 IEEE INT C EL, P1.
   Singh K., 2021, INT J ADV COMPUT SC, V12.
   Talwar Shivangi, 2020, Curr Pharmacol Rep, V6, P354, DOI 10.1007/s40495-020-00245-2.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xiao P, 2007, 2007 IEEE SWARM INTELLIGENCE SYMPOSIUM, P9, DOI 10.1109/SIS.2007.368020.
   Zhang X., 2016, COMMUN COMPUT PHYS, V624.},
Number-of-Cited-References = {32},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Int. J. Adv. Comput. Sci. Appl.},
Doc-Delivery-Number = {8K3XZ},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000923040200009},
DA = {2023-08-12},
}

@article{ WOS:000516850900027,
Author = {Vilasini, M. and Ramamoorthy, P.},
Title = {CNN Approaches for Classification of Indian Leaf Species Using
   Smartphones},
Journal = {CMC-COMPUTERS MATERIALS \& CONTINUA},
Year = {2020},
Volume = {62},
Number = {3},
Pages = {1445-1472},
Abstract = {Leaf species identification leads to multitude of societal applications.
   There is enormous research in the lines of plant identification using
   pattern recognition. With the help of robust algorithms for leaf
   identification, rural medicine has the potential to reappear as like the
   previous decades. This paper discusses CNN based approaches for Indian
   leaf species identification from white background using smartphones.
   Variations of CNN models over the features like traditional shape,
   texture, color and venation apart from the other miniature features of
   uniformity of edge patterns, leaf tip, margin and other statistical
   features are explored for efficient leaf classification.},
Publisher = {TECH SCIENCE PRESS},
Address = {871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA},
Type = {Article},
Language = {English},
Affiliation = {Vilasini, M (Corresponding Author), Kra Inst Engn \& Technol, Coimbatore 641407, Tamil Nadu, India.
   Vilasini, M., Kra Inst Engn \& Technol, Coimbatore 641407, Tamil Nadu, India.
   Ramamoorthy, P., Adithiya Inst Engn \& Technol, Coimbatore 641107, Tamil Nadu, India.},
DOI = {10.32604/cmc.2020.08857},
ISSN = {1546-2218},
EISSN = {1546-2226},
Keywords = {Deep learning; CNN; classification; transfer learning; prewitt edge
   detection},
Keywords-Plus = {SUPPORT VECTOR MACHINE; IMAGE TEXTURE; EDGE-DETECTION; RECOGNITION;
   SEGMENTATION; IDENTIFICATION; VEGETATION; FEATURES; PATTERN; SYSTEM},
Research-Areas = {Computer Science; Materials Science},
Web-of-Science-Categories  = {Computer Science, Information Systems; Materials Science,
   Multidisciplinary},
Author-Email = {vilasiniaddress@gmail.com},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Ahmad J, 2018, COMPUT IND, V98, P23, DOI 10.1016/j.compind.2018.02.005.
   Ahonen T, 2007, P FINN SIGN PROC S F, V1, P1.
   An N, 2016, COMPUT ELECTRON AGR, V127, P376, DOI 10.1016/j.compag.2016.04.002.
   {[}Anonymous], 2015, P EUR C COMP VIS BOS.
   {[}Anonymous], INT J COMP APP.
   {[}Anonymous], 2015, ENVIRON SCI-TOKYO, DOI DOI 10.5244/C.29.CVPPP.3.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Bai XD, 2014, BIOSYST ENG, V125, P80, DOI 10.1016/j.biosystemseng.2014.06.015.
   Bakhshipour A, 2018, COMPUT ELECTRON AGR, V145, P153, DOI 10.1016/j.compag.2017.12.032.
   Bakhshipour A, 2017, BIOSYST ENG, V157, P1, DOI 10.1016/j.biosystemseng.2017.02.002.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Bell J., 2019, ARXIV190403124.
   Borah S, 2007, J FOOD ENG, V79, P629, DOI 10.1016/j.jfoodeng.2006.02.022.
   Borah S, 2003, INSIGHT, V45, P55, DOI 10.1784/insi.45.1.55.52593.
   Burgos-Artizzu XP, 2011, COMPUT ELECTRON AGR, V75, P337, DOI 10.1016/j.compag.2010.12.011.
   Cerutti Guillaume, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P202.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Chen Q, 2008, T ASABE, V51, P623, DOI 10.13031/2013.24363.
   Chen QS, 2007, SPECTROCHIM ACTA A, V66, P568, DOI 10.1016/j.saa.2006.03.038.
   Dobrescu A, 2017, IEEE INT CONF COMP V, P2072, DOI 10.1109/ICCVW.2017.243.
   Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715.
   Du CB, 2017, IEEE ACCESS, V5, P15750, DOI 10.1109/ACCESS.2017.2735019.
   Fern BM, 2014, ADV SCI LETT, V20, P209, DOI 10.1166/asl.2014.5300.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Gill GS, 2013, BIOSYST ENG, V116, P198, DOI 10.1016/j.biosystemseng.2013.08.002.
   Gopal A, 2012, 2012 INTERNATIONAL CONFERENCE ON MACHINE VISION AND IMAGE PROCESSING (MVIP), P5, DOI 10.1109/MVIP.2012.6428747.
   Gouveia F, 1997, ISIE `97 - PROCEEDINGS OF THE IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS, VOLS 1-3, P757, DOI 10.1109/ISIE.1997.648634.
   Goyal Neha, 2018, 2018 International Conference on Computing, Power and Communication Technologies (GUCON), P405, DOI 10.1109/GUCON.2018.8675114.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Guerrero JM, 2012, EXPERT SYST APPL, V39, P11149, DOI 10.1016/j.eswa.2012.03.040.
   Guijarro M, 2011, COMPUT ELECTRON AGR, V75, P75, DOI 10.1016/j.compag.2010.09.013.
   Guo W, 2013, COMPUT ELECTRON AGR, V96, P58, DOI 10.1016/j.compag.2013.04.010.
   He K., 2016, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2016.90.
   Heikkila M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.
   Iandola F.N., 2016, ARXIV.
   Jabal MFAB, 2013, J COMPUT SCI-NETH, V9, P1295, DOI {[}DOI 10.3844/J.CSSP.2013.1295.1304, DOI 10.3844/JCSSP.2013.1295.1304].
   Jelinkova H, 2014, TREES-STRUCT FUNCT, V28, P389, DOI 10.1007/s00468-013-0957-y.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Kataoka T, 2003, IEEE ASME INT C ADV, P1079, DOI 10.1109/aim.2003.1225492.
   Kirk K, 2009, BIOSYST ENG, V104, P308, DOI 10.1016/j.biosystemseng.2009.07.001.
   Konishi S, 2003, IEEE T PATTERN ANAL, V25, P57, DOI 10.1109/TPAMI.2003.1159946.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Laddi A, 2013, J FOOD ENG, V115, P226, DOI 10.1016/j.jfoodeng.2012.10.018.
   Larese MG, 2014, EXPERT SYST APPL, V41, P4638, DOI 10.1016/j.eswa.2014.01.029.
   Larsson G., 2016, ARXIV160507648, P1.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Li ST, 2003, PATTERN RECOGN, V36, P2883, DOI 10.1016/S0031-3203(03)00219-X.
   Li XL, 2011, EXPERT SYST APPL, V38, P11149, DOI 10.1016/j.eswa.2011.02.160.
   Liao S, 2009, IEEE T IMAGE PROCESS, V18, P1107, DOI 10.1109/TIP.2009.2015682.
   Liu HP, 2014, INFORM SCIENCES, V266, P75, DOI 10.1016/j.ins.2014.01.010.
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
   Meyer GE, 2008, COMPUT ELECTRON AGR, V63, P282, DOI 10.1016/j.compag.2008.03.009.
   Mishra AK, 2011, IEEE T PATTERN ANAL, V33, P310, DOI 10.1109/TPAMI.2010.83.
   Morris DD, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P238, DOI 10.1109/CRV.2018.00041.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Mouine S., 2012, P 2 ACM INT C MULT R, P49.
   Mouine S, 2013, P 3 ACM C INT C MULT, P309.
   Mouine S, 2013, LECT NOTES COMPUT SC, V7950, P205, DOI 10.1007/978-3-642-39094-4\_24.
   Mzoughi O, 2012, IEEE IMAGE PROC, P1033, DOI 10.1109/ICIP.2012.6467039.
   Narayan V., 2014, RATIO, V13, P885.
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8\_29.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Pahalawatta K.K, 2008, THESIS U CANTERBURY.
   Palacios-Morillo A, 2013, SPECTROCHIM ACTA A, V103, P79, DOI 10.1016/j.saa.2012.10.052.
   Pape JM, 2015, LECT NOTES COMPUT SC, V8928, P61, DOI 10.1007/978-3-319-16220-1\_5.
   Petchsri S, 2012, SCIENCEASIA, V38, P1, DOI 10.2306/scienceasia1513-1874.2012.38.001.
   Pornpanomchai C., 2011, INT J ENG TECHNOLOGY, V3, P347, DOI DOI 10.7763/IJET.2011.V3.251.
   Pornpanomchai C, 2011, THAI HERB LEAF IMAGE.
   Pound MP, 2017, IEEE INT CONF COMP V, P2055, DOI 10.1109/ICCVW.2017.241.
   Pound MP, 2017, GIGASCIENCE, V6, DOI 10.1093/gigascience/gix083.
   Prasvita D.S., 2013, INT J ADV SCI ENG IN, V3, P5, DOI {[}10.18517/ijaseit.3.2.287, DOI 10.18517/IJASEIT.3.2.287].
   Rabatel G., 2001, Visual Form 2001. 4th International Workshop on Visual Form IWVF4. Proceedings (Lecture Notes in Computer Science Vol.2059), P249.
   Rasmussen J, 2007, WEED RES, V47, P299, DOI 10.1111/j.1365-3180.2007.00565.x.
   Rasti R, 2018, IEEE T MED IMAGING, V37, P1024, DOI 10.1109/TMI.2017.2780115.
   Ren MY, 2017, PROC CVPR IEEE, P293, DOI 10.1109/CVPR.2017.39.
   Scharr H, 2016, MACH VISION APPL, V27, P585, DOI 10.1007/s00138-015-0737-3.
   Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024.
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136.
   Soares JVB, 2013, MACH VISION APPL, V24, P1623, DOI 10.1007/s00138-013-0530-0.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Vukadinovic D, 2015, NETH C COMP VIS, P1.
   Wang L., 2007, INT C COMP COMP TECH, P713.
   Wang ZB, 2016, NEURAL COMPUT APPL, V27, P899, DOI 10.1007/s00521-015-1904-1.
   Wu D, 2008, J FOOD ENG, V88, P474, DOI 10.1016/j.jfoodeng.2008.03.005.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xiao-Feng Wang, 2012, Intelligent Computing Technology. Proceedings 8th International Conference, ICIC 2012, P466, DOI 10.1007/978-3-642-31588-6\_60.
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI {[}10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z].
   Yahiaoui I., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P254, DOI 10.1109/ICME.2012.130.
   Ye MN, 2015, COMPUT ELECTRON AGR, V114, P247, DOI 10.1016/j.compag.2015.04.010.
   Yin X, 2014, IEEE IMAGE PROC, P408, DOI 10.1109/ICIP.2014.7025081.
   Yu ZH, 2013, AGR FOREST METEOROL, V174, P65, DOI 10.1016/j.agrformet.2013.02.011.
   Zhao Y, 2013, NEUROCOMPUTING, V106, P68, DOI 10.1016/j.neucom.2012.10.017.
   Zheng LY, 2010, PATTERN RECOGN LETT, V31, P920, DOI 10.1016/j.patrec.2010.01.016.
   Zheng LY, 2009, COMPUT ELECTRON AGR, V65, P93, DOI 10.1016/j.compag.2008.08.002.
   Zhiyu Liu, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9226, P115, DOI 10.1007/978-3-319-22186-1\_11.
   Zhou L, 2015, INTERNATIONAL CONFERENCE ON ECOLOGICAL ENVIRONMENT, ENERGY SAVING AND EDUCATION MANAGEMENT (EES 2015), P48.},
Number-of-Cited-References = {104},
Times-Cited = {10},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {25},
Journal-ISO = {CMC-Comput. Mat. Contin.},
Doc-Delivery-Number = {KQ3UC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000516850900027},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000458037300007,
Author = {Duong-Trung, Nghia and Quach, Luyl-Da and Nguyen, Chi-Ngon},
Title = {Learning Deep Transferability for Several Agricultural Classification
   Problems},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
Year = {2019},
Volume = {10},
Number = {1},
Pages = {58-67},
Month = {JAN},
Abstract = {This paper addresses several critical agricultural classification
   problems, e.g. grain discoloration and medicinal plants identification
   and classification, in Vietnam via combining the idea of knowledge
   transferability and state-of-the-art deep convolutional neural networks.
   Grain discoloration disease of rice is an emerging threat to rice
   harvest in Vietnam as well as all over the world and it acquires
   specific attention as it results in qualitative loss of harvested crop.
   Medicinal plants are an important element of indigenous medical systems.
   These resources are usually regarded as a part of culture's traditional
   knowledge. Accurate classification is preliminary to any kind of
   intervention and recommendation of services. Hence, leveraging
   technology in automatic classification of these problems has become
   essential. Unfortunately, building and training a machine learning model
   from scratch is next to impossible due to the lack of hardware
   infrastructure and finance support. It painfully restricts the
   requirements of rapid solutions to deal with the demand. For this
   purpose, the authors have exploited the idea of transfer learning which
   is the improvement of learning in a new prediction task through the
   transferability of knowledge from a related prediction task that has
   already been learned. By utilizing state-of-the-art deep networks
   re-trained upon our collected data, our extensive experiments show that
   the proposed combination performs perfectly and achieves the
   classification accuracy of 98:7\% and 98:5\% on our collected datasets
   within the acceptable training time on a normal laptop. A mobile
   application is also deployed to facilitate further integrated
   recommendation and services.},
Publisher = {SCIENCE \& INFORMATION SAI ORGANIZATION LTD},
Address = {19 BOLLING RD, BRADFORD, WEST YORKSHIRE, 00000, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Duong-Trung, N (Corresponding Author), FPT Univ, Can Tho City, Vietnam.
   Duong-Trung, Nghia, FPT Univ, Can Tho City, Vietnam.
   Quach, Luyl-Da, Tay Do Univ, Can Tho City, Vietnam.
   Nguyen, Chi-Ngon, Can Tho Univ, Can Tho City, Vietnam.},
ISSN = {2158-107X},
EISSN = {2156-5570},
Keywords = {Medicinal Plant Classification; Grain Discoloration Classification;
   Transfer Learning; Deep Learning},
Keywords-Plus = {CONVOLUTIONAL NEURAL-NETWORKS; MEDICINE; PLANTS},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Affiliations = {FPT University; Can Tho University},
ResearcherID-Numbers = {Duong-Trung, Nghia/N-5525-2019
   Duong-Trung, Nghia/K-6960-2017},
ORCID-Numbers = {Duong-Trung, Nghia/0000-0002-7402-4166
   Duong-Trung, Nghia/0000-0002-7402-4166},
Cited-References = {Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265.
   {[}Anonymous], 2014, NATL TECHNICAL REGUL.
   {[}Anonymous], 2016, 3780 VIETNAMESE MED.
   {[}Anonymous], 2012, COMPETITION IMAGENET.
   Ashfaq M, 2017, JAPS J ANIMAL PLANT, V27.
   Barker R., 2014, RICE EC ASIA.
   Chollet Franc, 2016, CVPR.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Do T.L., 2004, NHUNG CAY THUOC VA V.
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5.
   Farnsworth N.R., 1991, CONSERVATION MED PLA, P25.
   Forbes, 2018, FORBES.
   Griffin G., 2007, CALTECH 256 OBJECT C.
   Hamilton AC, 2004, BIODIVERS CONSERV, V13, P1477, DOI 10.1023/B:BIOC.0000021333.23413.42.
   Howard A.G., 2017, ABS170404861 CORR.
   Ioffe S., 2015, INT C MACHINE LEARNI, DOI DOI 10.1007/S13398-014-0173-7.2.
   Islam Waqar, 2017, INT J SCI RES AGR SC, V4, P30.
   Kanjanamaneesathian M., 2017, New Zealand Plant Protection, V70, P196, DOI 10.30843/nzpp.2017.70.49.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Krizhevsky A., 2010, CONVOLUTIONAL DEEP B.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   Lopez-Sanchez Daniel, 2017, INT S DISTR COMP ART.
   Luong Van Vien, 2014, XAC DINH NAM GAY BEN.
   Maggiori E, 2017, IEEE T GEOSCI REMOTE, V55, P645, DOI 10.1109/TGRS.2016.2612821.
   Mamedov N., 2012, MED AROMAT PLANTS, V1, P1000, DOI {[}10.4172/2167-0412.1000e133, DOI 10.4172/2167-0412.1000E133].
   Ministry of Health Viet Nam, 2014, QUYET DINH VE VIEC B.
   Muthayya S, 2014, ANN NY ACAD SCI, V1324, P7, DOI 10.1111/nyas.12540.
   Ogle BM, 2003, ECON BOT, V57, P103, DOI 10.1663/0013-0001(2003)057{[}0103:FFOMTM]2.0.CO;2.
   Okwu D. E., 2001, Global Journal of Pure and Applied Sciences, V7, P455.
   Okwu D. E, AFRI J ROOT TUBER CR, V3, P18.
   Okwu DE, 2004, J SUSTAIN AGR ENV, V6, P30.
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222.
   Padulosi S., 2002, Journal of Herbs, Spices \& Medicinal Plants, V9, P243, DOI 10.1300/J044v09n04\_01.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Russakovsky Olga, 2013, P IEEE INT C COMP VI.
   Search Engine Land, 2016, GOOGL SHIFT MOB 1 MO.
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162.
   Sifre L., 2014, THESIS.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1609/AAAI.V31I1.11231.
   Szegedy C., 2015, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2015.7298594.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633.
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128.
   Torrey L., 2010, HDB RES MACHINE LEAR, P242, DOI DOI 10.4018/978-1-60566-766-9.CH011.
   Torrey L, 2010, STUD COMPUT INTELL, V262, P147.
   Trang Tran Hoai Thao, 2016, Journal of ISSAAS (International Society for Southeast Asian Agricultural Sciences), V22, P50.
   Van Duong Nguyen, 1993, MED PLANTS VIETNAM C.
   Vernin Gaston, 2016, CHEM OF GINGER, P107.
   Wahlberg A, 2006, HEALTH-LONDON, V10, P123, DOI 10.1177/1363459306061784.
   World's Top Exports, 2018, RIC EXP COUNTR.
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197.
   Zoph Barret, 2017, ARXIV17070701226.},
Number-of-Cited-References = {54},
Times-Cited = {9},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Int. J. Adv. Comput. Sci. Appl.},
Doc-Delivery-Number = {HK5VT},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000458037300007},
DA = {2023-08-12},
}

@inproceedings{ WOS:000946662000112,
Author = {Lemikhova, Liliya and Nesteruk, Sergey and Somov, Andrey},
Book-Group-Author = {IEEE},
Title = {Transfer Learning for Few-Shot Plants Recognition: Antarctic Station
   Greenhouse Use-Case},
Booktitle = {2022 IEEE 31ST INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS (ISIE)},
Year = {2022},
Pages = {715-720},
Note = {IEEE 31st International Symposium on Industrial Electronics (ISIE),
   Anchorage, AK, JUN 01-03, 2022},
Organization = {IEEE},
Abstract = {In this paper, we apply computer vision for plant recognition at the
   Antarctic station greenhouse, a training facility for future space
   colonization missions. Our experiments rely on transfer learning and
   explore the importance of the pre-training data domain. We show that a
   common approach of using models pre-trained on the Imagenet dataset can
   be further improved using publicly available domain-specific datasets.
   The classification results of 17 plant varieties with the ResNet50 model
   increase the F-score from 75\% to 82\% using only 3 training images. We
   also achieve 78\% top-3 accuracy without any training data.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Lemikhova, L (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Lemikhova, Liliya; Nesteruk, Sergey; Somov, Andrey, Skolkovo Inst Sci \& Technol, Moscow, Russia.},
DOI = {10.1109/ISIE51582.2022.9831723},
ISBN = {978-1-6654-8240-0},
Keywords = {computer vision; few-shot learning; plant phenotyping; transfer
   learning; zero-shot learning},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Industrial; Engineering, Electrical \& Electronic},
Author-Email = {liliya.lemikhova@skoltech.ru
   sergei.nesteruk@skoltech.ru
   a.somov@skoltech.ru},
Affiliations = {Skolkovo Institute of Science \& Technology},
Cited-References = {Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789.
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   DeVries T, 2017, Arxiv, DOI {[}arXiv:1708.04552, DOI 10.48550/ARXIV.1708.04552].
   Dosovitskiy A, 2021, Arxiv, DOI {[}arXiv:2010.11929, DOI 10.48550/ARXIV.2010.11929].
   Dupont C, 2015, AD HOC NETW, V25, P505, DOI 10.1016/j.adhoc.2014.11.003.
   Gotmare A, 2018, Arxiv, DOI arXiv:1810.13243.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hossin M., 2015, INT J DATA MINING KN, V5, P1, DOI {[}10.5121/ijdkp.2015.5201, DOI 10.5121/IJDKP.2015.5201].
   Illarionova S, 2021, IEEE INT CONF COMP V, P1659, DOI 10.1109/ICCVW54120.2021.00191.
   Illarionova Svetlana, REMOTE SENS-BASEL, V13.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277.
   Li ZB, 2020, COMPUT ELECTRON AGR, V176, DOI 10.1016/j.compag.2020.105672.
   Lindner Lars, 2017, IND ROBOT.
   Liu B, 2018, PROC CVPR IEEE, P9090, DOI 10.1109/CVPR.2018.00947.
   Lu YZ, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105760.
   Luo ZL, 2017, Arxiv, DOI arXiv:1712.00123.
   Nesteruk S, 2022, IEEE ACCESS, V10, P24010, DOI 10.1109/ACCESS.2022.3154709.
   Nesteruk S, 2020, PROC IEEE INT SYMP, P411, DOI 10.1109/ISIE45063.2020.9152399.
   Nesteruk Sergey, 2021, IEEE SENS J.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Hughes DP, 2016, Arxiv, DOI arXiv:1511.08060.
   Pahuja R, 2013, IEEE PERVAS COMPUT, V12, P49, DOI 10.1109/MPRV.2013.26.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Radford A., 2021, ARXIV.
   Sergiyenko O, 2018, J SENSORS, V2018, DOI 10.1155/2018/3202761.
   Sergiyenko OY, 2021, IEEE SENS J, V21, P11262, DOI 10.1109/JSEN.2020.3007856.
   Somov Andrey, 2009, 2009 IEEE C EMERGING, P1.
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131.
   Tsipras D., 2020, PROC 37 INT C MACH L.
   Van Horn G, 2021, PROC CVPR IEEE, P12879, DOI 10.1109/CVPR46437.2021.01269.
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914.
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658.
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252.
   Welinder P., 2010, CALTECH UCSD BIRDS 2.
   Yu Z., 2020, P IEEECVF C COMPUTER, P12856.
   Zhao W, 2020, IEEE ACCESS, V8, P221975, DOI 10.1109/ACCESS.2020.3043662.},
Number-of-Cited-References = {38},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BU8EW},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000946662000112},
DA = {2023-08-12},
}

@article{ WOS:000611806200001,
Author = {Shi, Yun and Ma, Donghui and Lv, Jie and Li, Jie},
Title = {ACTL: Asymmetric Convolutional Transfer Learning for Tree Species
   Identification Based on Deep Neural Network},
Journal = {IEEE ACCESS},
Year = {2021},
Volume = {9},
Pages = {13643-13654},
Abstract = {The identification of tree species is of great significance to the
   sustainable management and utilization of forest ecosystems.
   Hyperspectral data provide sufficient spectral and spatial information
   to classify tree species. Convolutional neural networks (CNN) have
   achieved great success in hyperspectral image (HSI) classification. The
   outstanding performance of CNN in HSI classification relies on
   sufficient training samples. However, it's expensive and time consuming
   to acquire labeled training samples. In this article, a novel asymmetric
   convolutional transfer learning model for HSI classification is
   proposed. First, the tree species identification dataset is built from
   Goddard's LiDAR, Hyperspectral \& Thermal (G-LiHT) data. Then, the
   asymmetric convolutional transfer learning model and weights trained on
   ImageNet dataset are used to initialize the weights of the HSI
   classification model. Finally, a well fine-tuned neural network on tree
   species dataset is used to perform the HSI classification task. The
   experimental results reveal that the proposed model with asymmetric
   convolutional blocks effectively improves the accuracy of Howland forest
   tree species identification and provides a new idea for the
   classification of hyperspectral remote sensing images.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Ma, DH (Corresponding Author), Xian Univ Sci \& Technol, Collage Geomat, Xian 710054, Peoples R China.
   Ma, DH (Corresponding Author), Minist Nat Resources, Key Lab Coal Resources Explorat \& Comprehens Util, Xian 710021, Peoples R China.
   Shi, Yun; Ma, Donghui; Lv, Jie; Li, Jie, Xian Univ Sci \& Technol, Collage Geomat, Xian 710054, Peoples R China.
   Shi, Yun; Ma, Donghui, Minist Nat Resources, Key Lab Coal Resources Explorat \& Comprehens Util, Xian 710021, Peoples R China.},
DOI = {10.1109/ACCESS.2021.3051015},
ISSN = {2169-3536},
Keywords = {Vegetation; Forestry; Hyperspectral imaging; Neural networks; Feature
   extraction; Convolution; Data models; Deep neural networks; G-LiHT;
   hyperspectral image; asymmetric convolution block; transfer learning},
Keywords-Plus = {FEATURE-EXTRACTION; HYPERSPECTRAL DATA; CLASSIFICATION; LIDAR; CNN},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {donghm1106@gmail.com},
Affiliations = {Xi'an University of Science \& Technology; Ministry of Natural Resources
   of the People's Republic of China},
ORCID-Numbers = {shi, yun/0000-0001-7370-0663
   ma, donghui/0000-0003-2918-5531},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}41674013, 41874012]},
Funding-Text = {This work was supported in part by the National Natural Science
   Foundation of China under Grant 41674013 and Grant 41874012.},
Cited-References = {Alonzo M, 2014, REMOTE SENS ENVIRON, V148, P70, DOI 10.1016/j.rse.2014.03.018.
   Arshad A, 2019, IEEE ACCESS, V7, P28100, DOI 10.1109/ACCESS.2019.2901860.
   Chen JZ, 2019, IEEE ACCESS, V7, P81407, DOI 10.1109/ACCESS.2019.2923776.
   Chen T, 2019, IEEE T IMAGE PROCESS, V28, P2389, DOI 10.1109/TIP.2018.2886758.
   Chen YS, 2016, IEEE T GEOSCI REMOTE, V54, P6232, DOI 10.1109/TGRS.2016.2584107.
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330.
   Cook BD, 2013, REMOTE SENS-BASEL, V5, P4045, DOI 10.3390/rs5084045.
   Cui Y., 2019, IEEE ACCESS, V7.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200.
   Dong YN, 2021, IEEE T CYBERNETICS, V51, P3185, DOI 10.1109/TCYB.2020.3004263.
   Fang LY, 2018, IEEE T GEOSCI REMOTE, V56, P3534, DOI 10.1109/TGRS.2018.2801387.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Guo B., 2019, IEEE ACCESS, V7.
   Guo HN, 2021, IEEE T GEOSCI REMOTE, V59, P4287, DOI 10.1109/TGRS.2020.3014312.
   He X, 2021, IEEE GEOSCI REMOTE S, V18, P876, DOI 10.1109/LGRS.2020.2988494.
   He X, 2020, IEEE T GEOSCI REMOTE, V58, P3246, DOI 10.1109/TGRS.2019.2951445.
   Hong DF, 2021, IEEE T GEOSCI REMOTE, V59, P5966, DOI 10.1109/TGRS.2020.3015157.
   Hu MY, 2018, CHIN AUTOM CONGR, P2135, DOI 10.1109/CAC.2018.8623484.
   Jamil A, 2018, IEEE J-STARS, V11, P89, DOI 10.1109/JSTARS.2017.2756864.
   Jun G., 2008, P INT GEOSC REM SENS, pI.
   Knauer U, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11232788.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kukkonen M, 2019, IEEE T GEOSCI REMOTE, V57, P3462, DOI 10.1109/TGRS.2018.2885057.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   Li W, 2017, IEEE T GEOSCI REMOTE, V55, P844, DOI 10.1109/TGRS.2016.2616355.
   Lin JZ, 2017, IEEE GEOSCI REMOTE S, V14, P1656, DOI 10.1109/LGRS.2017.2723763.
   Liu B, 2018, IEEE T GEOSCI REMOTE, V56, P1909, DOI 10.1109/TGRS.2017.2769673.
   Liu LX, 2017, REMOTE SENS ENVIRON, V200, P170, DOI 10.1016/j.rse.2017.08.010.
   Liu SJ, 2021, IEEE T GEOSCI REMOTE, V59, P5085, DOI 10.1109/TGRS.2020.3018879.
   Marmanis D, 2016, IEEE GEOSCI REMOTE S, V13, P105, DOI 10.1109/LGRS.2015.2499239.
   Marrs J, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11070819.
   Mei SH, 2019, IEEE T GEOSCI REMOTE, V57, P6808, DOI 10.1109/TGRS.2019.2908756.
   Mu CH, 2020, IEEE ACCESS, V8, P6768, DOI 10.1109/ACCESS.2019.2963624.
   Persello C, 2016, IEEE T GEOSCI REMOTE, V54, P2615, DOI 10.1109/TGRS.2015.2503885.
   Polonen I., 2018, P 2018 9 WORKSH HYP, P1, DOI DOI 10.1109/WHISPERS.2018.8747253.
   Richter R, 2016, INT J APPL EARTH OBS, V52, P464, DOI 10.1016/j.jag.2016.07.018.
   Romero A, 2016, IEEE T GEOSCI REMOTE, V54, P1349, DOI 10.1109/TGRS.2015.2478379.
   Rostami M, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111374.
   Sellars P, 2020, IEEE T GEOSCI REMOTE, V58, P4180, DOI 10.1109/TGRS.2019.2961599.
   Shi Q, 2020, IEEE GEOSCI REMOTE S, V17, P1430, DOI 10.1109/LGRS.2019.2947473.
   Shi Y., INT J APPL EARTH OBS, V73, P2018.
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Vinayaraj P, 2020, IEEE J-STARS, V13, P6352, DOI 10.1109/JSTARS.2020.3031020.
   Wang ZM, 2020, IEEE T NEUR NET LEAR, V31, P2387, DOI 10.1109/TNNLS.2019.2935608.
   Wu H, 2018, IEEE T IMAGE PROCESS, V27, P1259, DOI 10.1109/TIP.2017.2772836.
   Wu WJ, 2019, IEEE GEOSCI REMOTE S, V16, P977, DOI 10.1109/LGRS.2018.2886559.
   Xia JS, 2015, IEEE T GEOSCI REMOTE, V53, P2532, DOI 10.1109/TGRS.2014.2361618.
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197.
   Zhang HK, 2019, IEEE T GEOSCI REMOTE, V57, P5813, DOI 10.1109/TGRS.2019.2902568.
   Zhang W, 2019, IEEE ACCESS, V7, P152412, DOI 10.1109/ACCESS.2019.2948404.
   Zhao B, 2017, IEEE GEOSCI REMOTE S, V14, P1436, DOI 10.1109/LGRS.2017.2691013.},
Number-of-Cited-References = {55},
Times-Cited = {5},
Usage-Count-Last-180-days = {11},
Usage-Count-Since-2013 = {40},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {PY1KB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000611806200001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000360308600093,
Author = {Munisami, Trishen and Ramsurn, Mahess and Kishnah, Somveer and Pudaruth,
   Sameerchand},
Editor = {James, AP and AlJumeily, D and Thampi, SM},
Title = {Plant leaf recognition using shape features and colour histogram with
   k-nearest neighbour classifiers},
Booktitle = {SECOND INTERNATIONAL SYMPOSIUM ON COMPUTER VISION AND THE INTERNET
   (VISIONNET'15)},
Series = {Procedia Computer Science},
Year = {2015},
Volume = {58},
Pages = {740-741},
Note = {2nd International Symposium on Computer Vision and the Internet
   (VisionNet), INDIA, AUG 10-13, 2015},
Abstract = {Automated systems for plant recognition can be used to classify plants
   into appropriate taxonomies. Such information can be useful for
   botanists, industrialists, food engineers and physicians. In this work,
   a recognition system capable of identifying plants by using the images
   of their leaves has been developed. A mobile application was also
   developed to allow a user to take pictures of leaves and upload them to
   a server. The server runs pre-processing and feature extraction
   techniques on the image before a pattern matcher compares the
   information from this image with the ones in the database in order to
   get potential matches. The different features that are extracted are the
   length and width of the leaf, the area of the leaf, the perimeter of the
   leaf, the hull area, the hull perimeter, a distance map along the
   vertical and horizontal axes, a colour histogram and a centroid-based
   radial distance map. A k-Nearest Neighbour classifier was implemented
   and tested on 640 leaves belonging to 32 different species of plants. An
   accuracy of 83.5\% was obtained. The system was further enhanced by
   using information obtained from a colour histogram which increased the
   recognition accuracy to 87.3\%. Furthermore, our system is simple to
   use, fast and highly scalable. 2015 The Authors. Published by Elsevier
   B.V. This is an open access article under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
Publisher = {ELSEVIER SCIENCE BV},
Address = {SARA BURGERHARTSTRAAT 25, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Pudaruth, S (Corresponding Author), Univ Mauritius, Dept Ocean Engn \& ICT, Fac Ocean Studies, Moka, Mauritius.
   Munisami, Trishen; Ramsurn, Mahess; Kishnah, Somveer, Univ Mauritius, Dept Comp Sci \& Engn, Fac Engn, Moka, Mauritius.
   Pudaruth, Sameerchand, Univ Mauritius, Dept Ocean Engn \& ICT, Fac Ocean Studies, Moka, Mauritius.},
DOI = {10.1016/j.procs.2015.08.095},
ISSN = {1877-0509},
Keywords = {Leaf; pattern recognition; shape features; colour histogram; k-nearest
   neighbour},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Author-Email = {s.pudaruth@uom.ac.mu},
Affiliations = {University of Mauritius; University of Mauritius},
ResearcherID-Numbers = {Pudaruth, Sameerchand/AAA-3463-2019
   Pudaruth, Sameerchand/AAV-5534-2021},
Cited-References = {Aparna K., 2013, RETRIEVAL DIGITAL IM.
   Arevalillo-Herraez Miguel, 2013, INTERACTIVE EVOLUTIO.
   Baaziz Nadia, TEXTURE FEATURE EXTR.
   Chandrika L, 2014, IMPLEMENTATION IMAGE.
   Chang Ni-Bin, 2014, COMP DATA FUSION GEN.
   Diaz Joey Mark, 2013, LUNG CANC CLASSIFICA.
   Gopalakrishnan S., 2014, RETRIEVAL IMAGES BAS.
   Hamza R.M., 2012, GENETIC ALGORITHM FI.
   Janani MS. R., 2014, IMPROVED CBIR METHOD.
   Kalaiyarasi K., 2014, IMAGE RETREIVAL BASE.
   Kishore Kumar P., 2014, USING GENETIC ALGORI.
   Kolte Mahesh.T, 2014, IMPROVEMENT PERFORMA.
   Ligade Anita Nanasaheb, 2013, OPTIMIZED CONTENT BA.
   Manickam Sapthagiri. k, 2013, EFFICIENT IMAGE RETR.
   Patil Chandrashekhar G., 2013, FUSION AT FEATURES L.
   Ramesh babu durai C., 2012, IMPROVED CONTENT BAS.
   Renukadevi Ms.N.T., 2014, PERFORMANCE ANAL OPT.
   Sabri Itedal, 2013, USING ARTIFICAL NEUR.
   Samala Ashok, 2009, SEARCHING SATELLITE.
   Shafimirza J.Apparao, 2012, RETRIEVAL DIGITAL IM.
   Song X.N., 2007, INT J REMOTE SENS.
   Sreenivas Rao S., 2013, TEXTURE BASED IMAGE.
   Venkat Dass M., 2014, IMAGE RETRIEVAL USIN.
   Zhang Dengsheng, CONTENT BASED IMAGE.
   Zhou Ji, 2014, LAND SURFACE TEMPERA.},
Number-of-Cited-References = {25},
Times-Cited = {85},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {6},
Doc-Delivery-Number = {BD3YG},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000360308600093},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000399207000008,
Author = {Zhang Chuanlei and Zhang Shanwen and Yang Jucheng and Shi Yancui and
   Chen Jia},
Title = {Apple leaf disease identification using genetic algorithm and
   correlation based feature selection method},
Journal = {INTERNATIONAL JOURNAL OF AGRICULTURAL AND BIOLOGICAL ENGINEERING},
Year = {2017},
Volume = {10},
Number = {2},
Pages = {74-83},
Month = {MAR},
Abstract = {Apple leaf disease is one of the main factors to constrain the apple
   production and quality. It takes a long time to detect the diseases by
   using the traditional diagnostic approach, thus farmers often miss the
   best time to prevent and treat the diseases. Apple leaf disease
   recognition based on leaf image is an essential research topic in the
   field of computer vision, where the key task is to find an effective way
   to represent the diseased leaf images. In this research, based on image
   processing techniques and pattern recognition methods, an apple leaf
   disease recognition method was proposed. A color transformation
   structure for the input RGB( Red, Green and Blue) image was designed
   firstly and then RGB model was converted to HSI( Hue, Saturation and
   Intensity), YUV and gray models. The background was removed based on a
   specific threshold value, and then the disease spot image was segmented
   with region growing algorithm( RGA). Thirty-eight classifying features
   of color, texture and shape were extracted from each spot image. To
   reduce the dimensionality of the feature space and improve the accuracy
   of the apple leaf disease identification, the most valuable features
   were selected by combining genetic algorithm( GA) and correlation based
   feature selection( CFS). Finally, the diseases were recognized by SVM
   classifier. In the proposed method, the selected feature subset was
   globally optimum. The experimental results of more than 90\% correct
   identification rate on the apple diseased leaf image database which
   contains 90 disease images for there kinds of apple leaf diseases,
   powdery mildew, mosaic and rust, demonstrate that the proposed method is
   feasible and effective.},
Publisher = {CHINESE ACAD AGRICULTURAL ENGINEERING},
Address = {RM 506, NO 41, MAIZIDIAN ST, CHAOYANG DISTRICT, BEIJING, 100125, PEOPLES
   R CHINA},
Type = {Article},
Language = {English},
Affiliation = {Yang, JC (Corresponding Author), Tianjin Univ Sci \& Technol, Coll Comp Sci \& Informat Engn, 1038 Dagu Nanlu, Tianjin 300222, Peoples R China.
   Zhang Chuanlei; Yang Jucheng; Shi Yancui; Chen Jia, Tianjin Univ Sci \& Technol, Coll Comp Sci \& Informat Engn, 1038 Dagu Nanlu, Tianjin 300222, Peoples R China.
   Zhang Shanwen, Xijing Univ, Xian 710123, Peoples R China.},
DOI = {10.3965/j.ijabe.20171002.2166},
ISSN = {1934-6344},
EISSN = {1934-6352},
Keywords = {apple leaf disease; diseased leaf recognition; region growing algorithm
   (RGA); genetic algorithm and correlation based feature selection
   (GA-CFS)},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agricultural Engineering},
Author-Email = {97313114@tust.edu.cn
   wjdw716@163.com
   jcyang@tust.edu.cn
   syc@tust.edu.cn
   410158566@qq.com},
Affiliations = {Tianjin University Science \& Technology; Xijing University},
Funding-Acknowledgement = {Natural Science Foundation of China {[}61473237, 61202170, 61402331];
   Shaanxi Provincial Natural Science Foundation Research Project
   {[}2014JM2-6096]; Tianjin Research Program of Application Foundation and
   Advanced Technology {[}14JCYBJC42500]; Tianjin science and technology
   correspondent project {[}16JCTPJC47300]; key projects of Tianjin science
   and technology support program {[}15ZCZDGX00200]; Fund of Tianjin Food
   Safety \& Low Carbon Manufacturing Collaborative Innovation Center},
Funding-Text = {The authors would like to thank the anonymous reviewers for their
   critical and constructive comments and suggestions. This work was
   partially supported by Natural Science Foundation of China (grant Nos.
   61473237, 61202170, and 61402331). It is also supported by the Shaanxi
   Provincial Natural Science Foundation Research Project (2014JM2-6096),
   Tianjin Research Program of Application Foundation and Advanced
   Technology (14JCYBJC42500), Tianjin science and technology correspondent
   project (16JCTPJC47300), the 2015 key projects of Tianjin science and
   technology support program (No. 15ZCZDGX00200) and the Fund of Tianjin
   Food Safety \& Low Carbon Manufacturing Collaborative Innovation Center.},
Cited-References = {ADAMS R, 1994, IEEE T PATTERN ANAL, V16, P641, DOI 10.1109/34.295913.
   Akbari R., 2011, INT J IND ENG COMP, V2, P419, DOI DOI 10.5267/J.IJIEC.2010.03.002.
   Al Bashish Dheeb, 2011, Information Technology Journal, V10, P267, DOI 10.3923/itj.2011.267.275.
   Al-Hiary H., 2011, INT J COMPUT APPL, V17, P31, DOI DOI 10.5120/2183-2754.
   {[}Anonymous], 2013, OPEN J APPL SCI.
   {[}Anonymous], 2013, INT J ADV RES ELECT.
   Arivazhagan S., 2013, Agricultural Engineering International: CIGR Journal, V15, P211.
   Babatunde O. H., 2014, INT J ELECT COMMUNIC, V5, P2278.
   Chaudhary Piyush, 2012, INT J COMPUTER SCI T, V3, P65.
   Delalieux S, 2007, EUR J AGRON, V27, P130, DOI 10.1016/j.eja.2007.02.005.
   Forman G., 2003, Journal of Machine Learning Research, V3, P1289, DOI 10.1162/153244303322753670.
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797.
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427.
   Kadir A., 2011, INT J COMPUT APPL, V29, P15, DOI DOI 10.5120/3592-4981.
   Kamdi S, 2013, INT J COMPUTER TECHN, V2, P103.
   Li G.H., 2011, PRELIMINARY STUDY AU, P1, DOI DOI 10.1109/ICEMS.2011.6073664.
   Patil J K, 2011, INT J ENG TRENDS TEC, V2, P73.
   Saeys Y, 2007, BIOINFORMATICS, V23, P2507, DOI 10.1093/bioinformatics/btm344.
   Song Kai, 2007, Transactions of the Chinese Society of Agricultural Engineering, V23, P155.
   Tiwari R., 2010, INT J COMPUTER APPL, V4, P28.
   Valliammal N., 2012, INT J COMPUT APPL, V44, P10, DOI DOI 10.5120/6322-8669.
   Vishnu S., 2015, INT J INNOV SCI ENG, V2, P774.
   Wang K., 2005, DIAGNOSIS CROP DIS I.
   Wang Na, 2009, Scientia Agricultura Sinica, V42, P3836.
   Wang ShouZhi, 2009, Nongye Jixie Xuebao = Transactions of the Chinese Society for Agricultural Machinery, V40, P152.
   Yang WK, 2011, NEURAL PROCESS LETT, V33, P99, DOI 10.1007/s11063-010-9167-4.
   Zhang S, 2011, China Patent, No, Patent No. {[}201020204409. 2, 2010202044092].
   Zhi-Chun Huang, 2010, 2010 International Conference on Machine Learning and Cybernetics (ICMLC 2010), P719, DOI 10.1109/ICMLC.2010.5580566.},
Number-of-Cited-References = {28},
Times-Cited = {71},
Usage-Count-Last-180-days = {11},
Usage-Count-Since-2013 = {48},
Journal-ISO = {Int. J. Agric. Biol. Eng.},
Doc-Delivery-Number = {ES0HK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000399207000008},
DA = {2023-08-12},
}

@article{ WOS:000977728900001,
Author = {Lin, Haoran and Liu, Xiaoyang and Han, Zemin and Cui, Hongxia and Dian,
   Yuanyong},
Title = {Identification of Tree Species in Forest Communities at Different
   Altitudes Based on Multi-Source Aerial Remote Sensing Data},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2023},
Volume = {13},
Number = {8},
Month = {APR},
Abstract = {The accurate identification of forest tree species is important for
   forest resource management and investigation. Using single remote
   sensing data for tree species identification cannot quantify both
   vertical and horizontal structural characteristics of tree species, so
   the classification accuracy is limited. Therefore, this study explores
   the application value of combining airborne high-resolution
   multispectral imagery and LiDAR data to classify tree species in study
   areas of different altitudes. Three study areas with different altitudes
   in Muyu Town, Shennongjia Forest Area were selected. Based on the
   object-oriented method for image segmentation, multi-source remote
   sensing feature extraction was performed. The recursive feature
   elimination algorithm was used to filter out the feature variables that
   were optimal for classifying tree species in each altitude study area.
   Four machine learning algorithms, SVM, KNN, RF, and XGBoost, were
   combined to classify tree species at each altitude and evaluate the
   accuracy. The results show that the diversity of tree layers decreased
   with the altitude in the different study areas. The texture features and
   height features extracted from LiDAR data responded better to the forest
   community structure in the different study areas. Coniferous species
   showed better classification than broad-leaved species within the same
   study areas. The XGBoost classification algorithm showed the highest
   accuracy of 87.63\% (kappa coefficient of 0.85), 88.24\% (kappa
   coefficient of 0.86), and 84.03\% (kappa coefficient of 0.81) for the
   three altitude study areas, respectively. The combination of
   multi-source remote sensing numbers with the feature filtering algorithm
   and the XGBoost algorithm enabled accurate forest tree species
   classification.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Dian, Y (Corresponding Author), Huazhong Agr Univ, Coll Hort \& Forestry Sci, Wuhan 430070, Peoples R China.
   Dian, Y (Corresponding Author), Huazhong Agr Univ, Hubei Engn Technol Res Ctr Forestry Informat, Wuhan 430070, Peoples R China.
   Dian, Y (Corresponding Author), Minist Agr, Key Lab Urban Agr Cent China, Wuhan 430070, Peoples R China.
   Lin, Haoran; Han, Zemin; Dian, Yuanyong, Huazhong Agr Univ, Coll Hort \& Forestry Sci, Wuhan 430070, Peoples R China.
   Liu, Xiaoyang, Hubei Forestry Invest \& Planning Inst, Wuhan 430079, Peoples R China.
   Cui, Hongxia, Hubei Acad Forestry, Wuhan 430075, Peoples R China.
   Dian, Yuanyong, Huazhong Agr Univ, Hubei Engn Technol Res Ctr Forestry Informat, Wuhan 430070, Peoples R China.
   Dian, Yuanyong, Minist Agr, Key Lab Urban Agr Cent China, Wuhan 430070, Peoples R China.},
DOI = {10.3390/app13084911},
Article-Number = {4911},
EISSN = {2076-3417},
Keywords = {different altitudes; multispectral image; LiDAR; machine learning; tree
   species classification},
Keywords-Plus = {LIDAR; CLASSIFICATION; COMBINATION; DIVERSITY; FEATURES},
Research-Areas = {Chemistry; Engineering; Materials Science; Physics},
Web-of-Science-Categories  = {Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied},
Author-Email = {linhaoran@webmail.hzau.edu.cn
   liuxiaoyang917@163.com
   hzm@webmail.hzau.edu.cn
   chxlky@163.com
   dianyuanyong@mail.hzau.edu.cn},
Affiliations = {Huazhong Agricultural University; Huazhong Agricultural University;
   Ministry of Agriculture \& Rural Affairs},
ORCID-Numbers = {dian, yuanyong/0000-0002-4957-3681},
Funding-Acknowledgement = {Open Research Fund of Key Laboratory of Digital Earth Science, Aerospace
   Information Research Institute Chinese Academy of Sciences, Chinese
   Academy of Sciences {[}2022LDE006]},
Funding-Text = {This research was funded by the Open Research Fund of Key Laboratory of
   Digital Earth Science, Aerospace Information Research Institute Chinese
   Academy of Sciences, Chinese Academy of Sciences (No. 2022LDE006).},
Cited-References = {Alvarez-Taboada F, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090913.
   Brilli L, 2013, INT J APPL EARTH OBS, V23, P29, DOI 10.1016/j.jag.2012.11.006.
   Cao JJ, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010089.
   Cateanu M, 2021, FORESTS, V12, DOI 10.3390/f12030265.
   Chen L, 2018, GLOBAL CHANGE BIOL, V24, P3969, DOI 10.1111/gcb.14288.
   Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785.
   Chen X., 2019, CURR CONTENTS, V56, P203.
   Chi DK, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12152435.
   Dabiri Z, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7120488.
   Dai PQ, 2020, LASER OPTOELECTRON P, V57, DOI 10.3788/LOP57.101001.
   De Frenne P, 2021, GLOBAL CHANGE BIOL, V27, P2279, DOI 10.1111/gcb.15569.
   Deepak M, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11242884.
   Dian YY, 2015, J INDIAN SOC REMOTE, V43, P101, DOI 10.1007/s12524-014-0392-6.
   Dong Y., 2020, IEEE GEOSCI REMOTE S, V42, P11.
   Felipe-Lucia MR, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07082-4.
   Ferreira MP, 2016, REMOTE SENS ENVIRON, V179, P66, DOI 10.1016/j.rse.2016.03.021.
   Gao S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21238162.
   {[}皋厦 Gao Sha], 2018, {[}遥感技术与应用, Remote Sensing Technology and Application], V33, P1073.
   Guo S., 2020, ANHUI FOR SCI TECHNO, V46, P47.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Hartling S, 2021, GISCI REMOTE SENS, V58, P1250, DOI 10.1080/15481603.2021.1974275.
   Hovi A, 2016, REMOTE SENS ENVIRON, V173, P224, DOI 10.1016/j.rse.2015.08.019.
   Hsu PH, 2007, ISPRS J PHOTOGRAMM, V62, P78, DOI 10.1016/j.isprsjprs.2006.12.004.
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604.
   Jiaxin K., 2019, BIODIVERS SCI, V27, P796, DOI {[}10.17520/biods.2019197, DOI 10.17520/BIODS.2019197].
   Kaasalainen S, 2015, FORESTS, V6, P252, DOI 10.3390/f6010252.
   Kandare K, 2017, INT J APPL EARTH OBS, V60, P72, DOI 10.1016/j.jag.2017.04.008.
   Li XuSheng, 2020, Scientia Silvae Sinicae, V56, P93, DOI 10.11707/j.1001-7488.20201010.
   Lopez-Angulo J, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0200216.
   Ma S, 2021, FOREST ECOL MANAG, V482, DOI 10.1016/j.foreco.2020.118856.
   Man QX, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12172725.
   Morris EK, 2014, ECOL EVOL, V4, P3514, DOI 10.1002/ece3.1155.
   Peng X, 2022, J INDIAN SOC REMOTE, V50, P25, DOI 10.1007/s12524-021-01453-z.
   Peng X, 2021, FORESTS, V12, DOI 10.3390/f12030328.
   Noi PT, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010018.
   PLOS ONE Staff, 2017, PLoS One, V12, pe0176866, DOI 10.1371/journal.pone.0176866.
   Pu RL, 2020, URBAN FOR URBAN GREE, V53, DOI 10.1016/j.ufug.2020.126675.
   Rodriguez-Galiano VF, 2012, ISPRS J PHOTOGRAMM, V67, P93, DOI 10.1016/j.isprsjprs.2011.11.002.
   Shen X, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111180.
   Shi YF, 2020, INT J APPL EARTH OBS, V84, DOI 10.1016/j.jag.2019.101970.
   {[}王康康 Wang Kangkang], 2021, {[}测绘地理信息, Journal of Geomatics], V46, P78.
   Wang M., 2016, J ANHUI AGR, V44, P264, DOI {[}10.13989/j.cnki.0517-6611.2016.11.090, DOI 10.13989/J.CNKI.0517-6611.2016.11.090].
   Wang YT, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13020216.
   Wu YanShuang, 2020, Journal of Beijing Forestry University, V42, P91, DOI 10.1217l/j.1000-1522.20190155.
   {[}徐逸 Xu Yi], 2021, {[}遥感学报, Journal of Remote Sensing], V25, P737.
   Xu Z, 2020, INT J APPL EARTH OBS, V92, DOI 10.1016/j.jag.2020.102173.
   Yan SJ, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13030479.
   Yin DM, 2019, REMOTE SENS ENVIRON, V223, P34, DOI 10.1016/j.rse.2018.12.034.
   Zhang KQ, 2003, IEEE T GEOSCI REMOTE, V41, P872, DOI 10.1109/TGRS.2003.810682.
   Zhang Ya-hao, 2021, Shengtaixue Zazhi, V40, P2357, DOI 10.13292/j.1000-4890.202108.015.
   Zhao Feng, 2009, Scientia Silvae Sinicae, V45, P81.
   Zhao KG, 2018, REMOTE SENS ENVIRON, V204, P883, DOI 10.1016/j.rse.2017.09.007.},
Number-of-Cited-References = {52},
Times-Cited = {0},
Usage-Count-Last-180-days = {10},
Usage-Count-Since-2013 = {10},
Journal-ISO = {Appl. Sci.-Basel},
Doc-Delivery-Number = {E8BJ1},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000977728900001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000703313800004,
Author = {Trinh Tan Dat and Pham Cung Le Thien Vu and Nguyen Nhat Truong and Le
   Tran Anh Dang and Vu Ngoc Thanh Sang and Pham The Bao},
Title = {Leaf Recognition Based on Joint Learning Multiloss of Multimodel
   Convolutional Neural Networks: A Testing for Vietnamese Herb},
Journal = {COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE},
Year = {2021},
Volume = {2021},
Month = {SEP 24},
Abstract = {A new modification of multi-CNN ensemble training is investigated by
   combining multiloss functions from state-of-the-art deep CNN
   architectures for leaf image recognition. We first apply the U-Net model
   to segment leaf images from the background to improve the performance of
   the recognition system. Then, we introduce a multimodel approach based
   on a combination of loss functions from the EfficientNet and MobileNet
   (called as multimodel CNN (MMCNN)) to generalize a multiloss function.
   The joint learning multiloss model designed for leaf recognition allows
   each network to perform its task and cooperate with the others
   simultaneously, where knowledge from various trained deep networks is
   shared. This cooperation-proposed multimodel is forced to deal with more
   complicated problems rather than a simple classification. Therefore, the
   network can learn much rich information and improve its generalization
   capability. Furthermore, a multiloss trade-off strategy between two deep
   learning models can reduce the effect of redundancy problems in ensemble
   classifiers. The performance of our approach is evaluated by our custom
   Vietnamese herbal leaf species dataset, and public datasets such as
   Flavia, Leafsnap, and Folio are used to build test cases. The results
   confirm that our approach enhances the leaf recognition performance and
   outperforms the current standard single networks while having less low
   computation cost.},
Publisher = {HINDAWI LTD},
Address = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Bao, PT (Corresponding Author), Sai Gon Univ, Informat Sci Fac, Ho Chi Minh City, Vietnam.
   Trinh Tan Dat; Pham Cung Le Thien Vu; Nguyen Nhat Truong; Le Tran Anh Dang; Vu Ngoc Thanh Sang; Pham The Bao, Sai Gon Univ, Informat Sci Fac, Ho Chi Minh City, Vietnam.},
DOI = {10.1155/2021/5032359},
Article-Number = {5032359},
ISSN = {1687-5265},
EISSN = {1687-5273},
Keywords-Plus = {CLASSIFICATION; FEATURES},
Research-Areas = {Mathematical \& Computational Biology; Neurosciences \& Neurology},
Web-of-Science-Categories  = {Mathematical \& Computational Biology; Neurosciences},
Author-Email = {ptbao@sgu.edu.vn},
Affiliations = {Saigon University},
ORCID-Numbers = {TRINH, TAN DAT/0000-0003-0443-612X
   Bao, Pham The/0000-0002-4847-4366
   Le Tran Anh, Dang/0000-0001-5860-4294
   Nguyen, Truong/0000-0003-3426-1009
   Pham Cung Le Thien, Vu/0000-0003-2307-6713
   Ngoc Thanh Sang, Vu/0000-0001-6973-0499},
Funding-Acknowledgement = {Vingroup Innovation Foundation (VINIF) {[}VINIF.2019.13]},
Funding-Text = {The research was supported by the Vingroup Innovation Foundation (VINIF)
   annual research grant program in project code VINIF.2019.13.},
Cited-References = {Abdul Wasay, 2020, P 3 MLSYS C MARCH AU.
   Amin AHM, 2013, PROCEDIA COMPUT SCI, V24, P84, DOI 10.1016/j.procs.2013.10.030.
   {[}Anonymous], 2019, OPENCV REFERENCE MAN.
   Arai K., 2013, INT JOURNALOF ADV RE, V2, P60, DOI 10.14569/ijarai.2013.020309.
   Azlah MAF, 2019, COMPUTERS, V8, DOI 10.3390/computers8040077.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Begue A, 2017, INT J ADV COMPUT SC, V8, P166.
   Chaki J, 2016, SMART INNOV SYST TEC, V43, P37, DOI 10.1007/978-81-322-2538-6\_5.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Du C, 2020, PATTERN RECOGN LETT, V129, P108, DOI 10.1016/j.patrec.2019.11.015.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Du JX, 2013, NEUROCOMPUTING, V116, P150, DOI 10.1016/j.neucom.2012.03.028.
   Du L, 2019, ISPRS J PHOTOGRAMM, V158, P63, DOI 10.1016/j.isprsjprs.2019.09.018.
   Ekor M, 2014, FRONT PHARMACOL, V4, DOI 10.3389/fphar.2013.00177.
   He K., 2016, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2016.90.
   Howard Andrew G., 2017, MOBILENETS EFFICIENT.
   Jeon WS, 2017, INT J FUZZY LOG INTE, V17, P26, DOI 10.5391/IJFIS.2017.17.1.26.
   Kowsari K, 2018, 2ND INTERNATIONAL CONFERENCE ON INFORMATION SYSTEM AND DATA MINING (ICISDM 2018), P19, DOI 10.1145/3206098.3206111.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Vasic MK, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9091459.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Pang M, 2019, IEEE T CIRC SYST VID, V29, P3184, DOI 10.1109/TCSVT.2018.2879833.
   Pang M, 2019, PATTERN RECOGN, V89, P91, DOI 10.1016/j.patcog.2019.01.005.
   Radu Valentin, 2017, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V1, DOI 10.1145/3161174.
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4\_28.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683.
   Shen ZQ, 2019, AAAI CONF ARTIF INTE, P4886, DOI 10.1609/aaai.v33i01.33014886.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Tsolakidis DG, 2014, LECT NOTES ARTIF INT, V8445, P406, DOI 10.1007/978-3-319-07064-3\_33.
   Verma S, 2008, VET WORLD, V1, P347.
   Vilasini M, 2020, CMC-COMPUT MATER CON, V62, P1445, DOI 10.32604/cmc.2020.08857.
   Wu Stephen Gang, 2007, 2007 IEEE International Symposium on Signal Processing and Information Technology, P11, DOI 10.1109/ISSPIT.2007.4458016.
   Xia XL, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P783, DOI 10.1109/ICIVC.2017.7984661.},
Number-of-Cited-References = {39},
Times-Cited = {3},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {11},
Journal-ISO = {Comput. Intell. Neurosci.},
Doc-Delivery-Number = {WB1BD},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000703313800004},
OA = {gold, Green Published},
DA = {2023-08-12},
}

@article{ WOS:000856543300004,
Author = {Roopashree, S. and Anitha, J. and Mahesh, T. R. and Kumar, V. Vinoth and
   Viriyasitavat, Wattana and Kaur, Amandeep},
Title = {An IoT based authentication system for therapeutic herbs measured by
   local descriptors using machine learning approach},
Journal = {MEASUREMENT},
Year = {2022},
Volume = {200},
Month = {AUG 15},
Abstract = {The work aims to develop an automatic recognition model to classify
   medicinal plants using machine learning techniques to enrich the
   traditional medical system of India. Though many countries have accepted
   conventional medicine as the best alternative to synthetic drugs, there
   exists limitations such as lack of awareness among general public and
   unavailability of easy access to its source evidences that has led to
   its limited acceptance and usability. Herein, an intelligent system is
   proposed to use Raspberry Pi 3 Model B+ (RPi) and the RPi camera to
   capture the leaf images of Indian medicinal herbs and reveal their
   medical properties. Five types of models implemented to identify the
   medicinal plants. One of the models proposed as Herbmodel extracts a
   feature map from a captured medicinal leaf by combining three different
   feature extraction techniques, namely, Scale Invariant Feature Transform
   (SIFT), Oriented FAST and Rotated BRIEF (ORB) and histogram of oriented
   gradients on support vector machine as a classifier, predicts an average
   accuracy of 96.22\% over a custom medicinal leaf dataset of 40 different
   species containing 2515 samples. Generate Bag of Visual Words (BoVW) by
   applying K-Means clustering on both SIFT and ORB descriptors to reduce
   the dimensionality. The combined feature vector is further analysed
   using random forest and k-nearest neighbor classifier. The efficacy of
   the proposed approach is benchmarked using Flavia dataset and artificial
   neural network (ANN) as a classifier. Our findings prove that the
   combination of local descriptors is an efficient measurement approach
   that benefits automatic recognition of plants based on leaf images.
   Also, a reliable source of medicinal leaf datasets with good quality
   leaf images is necessary to establish a machine learning model for
   medicinal plants.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Kumar, VV (Corresponding Author), Jain Deemed Univ, Dept Comp Sci \& Engn, Bangalore, India.
   Roopashree, S.; Mahesh, T. R.; Kumar, V. Vinoth, Jain Deemed Univ, Dept Comp Sci \& Engn, Bangalore, India.
   Anitha, J., RV Inst Technol \& Management, Dept Comp Sci \& Engn, Bengaluru, India.
   Viriyasitavat, Wattana, Chulalongkorn Univ, Fac Commerce \& Accountancy, Dept Stat, Business Informat Technol Div, Bangkok, Thailand.
   Kaur, Amandeep, Chandigarh Univ, Univ Ctr Res \& Dev, Dept Comp Sci \& Engn, Gharuan, Mohali, India.},
DOI = {10.1016/j.measurement.2022.111484},
EarlyAccessDate = {JUL 2022},
Article-Number = {111484},
ISSN = {0263-2241},
EISSN = {1873-412X},
Keywords = {Artificial Neural Network; Histogram of oriented gradients; Machine
   learning; Medicinal plant classification; Medicinal leaf dataset;
   oriented FAST and rotated BRIEF; Raspberry Pi; Scale invariant feature
   transform; Support vector machine},
Keywords-Plus = {MEDICINAL-PLANTS; CLASSIFICATION; IDENTIFICATION},
Research-Areas = {Engineering; Instruments \& Instrumentation},
Web-of-Science-Categories  = {Engineering, Multidisciplinary; Instruments \& Instrumentation},
Author-Email = {roopashaily@gmail.com
   anitha.jayapalan@gmail.com
   t.mahesh@jainuniversity.ac.in
   pvkumar243@gmail.com
   hardgolf@gmail.com
   er.aman68@yahoo.com},
Affiliations = {Jain University; Chulalongkorn University; Chandigarh University},
ResearcherID-Numbers = {Viriyasitavat, Wattana/Y-6952-2019
   Kaur, Amandeep/IYJ-2622-2023
   Shailendra, Roopashree/GRJ-7607-2022
   },
ORCID-Numbers = {Viriyasitavat, Wattana/0000-0001-7247-4596
   Kaur, Amandeep/0000-0002-9825-4951
   Shailendra, Roopashree/0000-0003-1327-1267
   Kaur, Dr. Amandeep/0000-0002-0168-1704
   TR, MAHESH/0000-0002-5589-8992},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Agarwal G, 2006, TAXON, V55, P597, DOI 10.2307/25065637.
   Altemimi A, 2017, PLANTS-BASEL, V6, DOI 10.3390/plants6040042.
   {[}Anonymous], 2005, COMPUTER VISION PATT, V1, P886, DOI DOI 10.1109/CVPR.2005.177.
   {[}Anonymous], 2013, SIGNAL PROCESS PATTE.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Begue A, 2017, INT J ADV COMPUT SC, V8, P166.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Dhiman G, 2019, ENG APPL ARTIF INTEL, V82, P148, DOI 10.1016/j.engappai.2019.03.021.
   Gordan M, 2022, MEASUREMENT, V193, DOI 10.1016/j.measurement.2022.110939.
   Grabowski K, 2022, MEASUREMENT, V189, DOI 10.1016/j.measurement.2021.110575.
   Gupta S, 2019, MULTIMED TOOLS APPL, V78, P34157, DOI 10.1007/s11042-019-08232-6.
   Gupta V.K., 2022, INT J MODERN RES, V2, P1, DOI DOI 10.1109/INDISCON53343.2021.9582222.
   Herdiyeni Y., 2013, INT J ADV SCI ENG IN, V3, P23, DOI 10.18517/ijaseit.3.1.270.
   Herdiyeni Y, 2012, 2012 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER SCIENCE AND INFORMATION SYSTEMS, P255.
   Herdiyeni Y, 2012, 2012 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER SCIENCE AND INFORMATION SYSTEMS, P301.
   Huynh HX, 2020, VIETNAM J COMPUT SCI, V7, P197, DOI 10.1142/S2196888820500116.
   Janani R., 2013, 2013 International Conference on Advanced Electronic Systems (ICAES), P238, DOI 10.1109/ICAES.2013.6659400.
   Jin TS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0139482.
   Kadir Abdul, 2013, Arxiv, DOI arXiv:1401.4447.
   Kan H. X., 2017, Pattern Recognition and Image Analysis, V27, P581, DOI 10.1134/S105466181703018X.
   Karami N., 2017, CANC PRESS, V3, P22, DOI {[}10.15562/tcp.41, DOI 10.15562/TCP.41].
   Kaur S, 2020, ENG APPL ARTIF INTEL, V90, DOI 10.1016/j.engappai.2020.103541.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Kumar R., 2021, INT J MODERN RES, V1, P1, DOI DOI 10.1109/ICMLC.2007.4370325.
   Le TL, 2014, 5 S INF COMM TECHN H, P146.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Pacifico Luciano D. S., 2019, 2019 8th Brazilian Conference on Intelligent Systems (BRACIS). Proceedings, P741, DOI 10.1109/BRACIS.2019.00133.
   Pacifico LDS, 2018, IEEE IJCNN.
   Pearline SA, 2019, J INTELL FUZZY SYST, V36, P1997, DOI 10.3233/JIFS-169911.
   Po Ken Pang, 2019, IOP Conference Series: Materials Science and Engineering, V495, DOI 10.1088/1757-899X/495/1/012032.
   Pornpanomchai C, 2011, THAI HERB LEAF IMAGE.
   Prasvita D.S., 2013, INT J ADV SCI ENG IN, V3, P5, DOI {[}10.18517/ijaseit.3.2.287, DOI 10.18517/IJASEIT.3.2.287].
   Priyankara HAC, 2015, 2015 MORATUWA ENGINEERING RESEARCH CONFERENCE (MERCON), P148, DOI 10.1109/MERCon.2015.7112336.
   Qilong Li, 2018, 2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS). Proceedings, P762, DOI 10.1109/ICIS.2018.8466432.
   Rahmani ME., 2015, ALLDATA, V2015, P82.
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
   Sabu A, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P574.
   Sabu A, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON INVENTIVE COMMUNICATION AND COMPUTATIONAL TECHNOLOGIES (ICICCT), P145, DOI 10.1109/ICICCT.2017.7975176.
   Sahay A, 2016, INT CONF SOFTW ENG, P914, DOI 10.1109/ICSESS.2016.7883214.
   Sainin MS, 2014, MALAYSIAN MED PLANT.
   Sanderson M, 2019, IMAGECLEF THE CLEF C.
   Sethulekshmi AV., 2014, J COMPUT SCI INF TEC, V5, P8061.
   Sharma T., 2022, INT J MODERN RES, V2, P8, DOI DOI 10.31234/OSF.IO/W9RB2.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Srunitha K., 2016, 2016 International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES), P411, DOI 10.1109/SCOPES.2016.7955863.
   Tin Kam Ho, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P278, DOI 10.1109/ICDAR.1995.598994.
   Tyagi R., 2016, J CRIT REV, V3, P69.
   Venkataraman D, 2016, IEEE I C COMP INT CO, P1000.
   Vo Anh H., 2019, International Journal of Machine Learning and Computing, V9, P363, DOI 10.18178/ijmlc.2019.9.3.811.
   Wang B, 2017, PROC CVPR IEEE, P2047, DOI 10.1109/CVPR.2017.221.
   WHO, 1999, WHO MON SEL MED PLAN, V1.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xue JR, 2019, INT J AGR BIOL ENG, V12, P123, DOI 10.25165/j.ijabe.20191202.4637.},
Number-of-Cited-References = {56},
Times-Cited = {8},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Measurement},
Doc-Delivery-Number = {4R1PX},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000856543300004},
DA = {2023-08-12},
}

@inproceedings{ WOS:000722429500012,
Author = {Balestra, Mattia and Chiappini, Stefano and Malinverni, Eva Savina and
   Galli, Andrea and Marcheggiani, Ernesto},
Editor = {Gervasi, O and Murgante, B and Misra, S and Garau, C and Blecic, I and Taniar, D and Apduhan, BO and Rocha, AMAC and Tarantino, E and Torre, CM},
Title = {A Machine Learning Approach for Mapping Forest Categories: An
   Application of Google Earth Engine for the Case Study of Monte
   Sant'Angelo, Central Italy},
Booktitle = {COMPUTATIONAL SCIENCE AND ITS APPLICATIONS, ICCSA 2021, PT VII},
Series = {Lecture Notes in Computer Science},
Year = {2021},
Volume = {12955},
Pages = {155-168},
Note = {21st International Conference on Computational Science and Its
   Applications (ICCSA), Cagliari, ITALY, SEP 13-16, 2021},
Organization = {IEEE Italy Sect; IEEE Geoscience \& Remote Sensing Soc; IEEE Comp Soc;
   Studium Gen Civitatis Perusii; Univ Stvdiorvm Caralitana; Univ Degli
   Studi Bascilicata; Monash Univ; Kyushu Sangyo Univ; Univ Minho, Escola
   Engn; Springer; MDPI Computers; Assoc Sci Infrastrutture Trasporto;
   Regione Autonoma Sardigna},
Abstract = {Remote Sensing plays a critical role in forest tree species
   identification. Regarding the current debate on earth observation and
   monitoring, many see this valuable technology useful for a wide range of
   purposes, including forest conservation and management. This paper
   focuses on a workflow for mapping forest tree species from satellite
   images, by statistic algorithms and machine learning. Among the world
   satellite platforms, the Sentinel-2 program was selected to investigate
   the mixed forest area of Monte Sant'Angelo in Central Italy. A list of
   monthly images from 2018 to 2020 have been processed using the Google
   Earth Engine geospatial processing service. The process includes the
   computation of vegetation indexes like the Normalized Difference
   Vegetation Index (NDVI), Transformed Difference Vegetation Index (TDVI),
   the Enhanced Vegetation Index (EVI) and the Green Normalized Difference
   Vegetation Index (GNDVI). A forest class time series was generated for
   each index. The Artificial Intelligence algorithm models (Machine
   Learning) were trained identifying accurate ground truths. Principal
   Component Analysis (PCA) was performed to reduce variables redundancy in
   Random Forest classifications. Four forest categories have been
   identified: holm oak woodlands, conifer reforestation, Ostryo Carpinion
   alliance and mixed hardwood forest. Due to the phenological differences
   among species, the classification global accuracy ranges from 70\% to
   80\%.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Chiappini, S (Corresponding Author), Univ Politecn Marche, Dipartimento Sci Agr Alimentari \& Ambientali D3A, Via Brecce Bianche 10, I-60131 Ancona, Italy.
   Balestra, Mattia; Chiappini, Stefano; Galli, Andrea; Marcheggiani, Ernesto, Univ Politecn Marche, Dipartimento Sci Agr Alimentari \& Ambientali D3A, Via Brecce Bianche 10, I-60131 Ancona, Italy.
   Malinverni, Eva Savina, Univ Politecn Marche, Dipartimento Ingn Civile Edile \& Architettura DIC, Via Brecce Bianche 12, I-60131 Ancona, Italy.
   Marcheggiani, Ernesto, Katholieke Univ Leuven, Dept Earth \& Environm Sci, B-3001 Leuven, Belgium.},
DOI = {10.1007/978-3-030-87007-2\_12},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-030-87007-2; 978-3-030-87006-5},
Keywords = {Google Earth Engine; PCA; Random forest; Sentinel-2; Time-series; Tree
   species classification},
Research-Areas = {Computer Science; Mathematics},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications; Computer Science,
   Theory \& Methods; Mathematics, Applied},
Author-Email = {m.balestra@pm.univpm.it
   s.chiappini@pm.univpm.it
   e.s.malinverni@univpm.it
   a.galli@univpm.it
   e.marcheggiani@univpm.it},
Affiliations = {Marche Polytechnic University; Marche Polytechnic University; KU Leuven},
ResearcherID-Numbers = {Balestra, Mattia/IQS-8533-2023
   galli, andrea/HTP-5003-2023
   Marcheggiani, Ernesto/AGM-7101-2022
   Frontoni, Emanuele/D-9838-2013
   },
ORCID-Numbers = {Balestra, Mattia/0000-0002-3741-3621
   Marcheggiani, Ernesto/0000-0002-6879-9942
   Frontoni, Emanuele/0000-0002-8893-9244
   GALLI, Andrea/0000-0003-2683-2094},
Cited-References = {Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324.
   Choubin B, 2019, EFFECTS DROUGHT VEGE, V2, P213.
   Choudhury A.M, 2020, THE FOREST, V11, P22, DOI {[}10.3390/f11111226, DOI 10.3390/F11111226].
   Choudhury MAM, 2021, FORESTS, V12, DOI 10.3390/f12060692.
   Chung LCH, 2021, BUILD ENVIRON, V199, DOI 10.1016/j.buildenv.2021.107879.
   Copernicus E, EUROPEAN SPACE AGENC.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Di Fazio S, 2011, LECT NOTES COMPUT SC, V6782, P284.
   Ghimire BR, 2017, FORESTS, V8, DOI 10.3390/f8100384.
   Goldblatt R, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8080634.
   Gorelick N, 2017, REMOTE SENS ENVIRON, V202, P18, DOI 10.1016/j.rse.2017.06.031.
   Gulinck H, 2018, SUSTAINABILITY-BASEL, V10, DOI 10.3390/su10072143.
   Hanes JM, 2014, SPRING REMOTE SENS P, P1, DOI 10.1007/978-3-642-25047-7.
   Isip MF, 2020, SPAT INF RES, V28, P369, DOI 10.1007/s41324-019-00297-7.
   Jin Li, 2020, 2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics), P498, DOI 10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics50389.2020.00091.
   Karasiak N, 2017, 2017 9TH INTERNATIONAL WORKSHOP ON THE ANALYSIS OF MULTITEMPORAL REMOTE SENSING IMAGES (MULTITEMP).
   Kumar L, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10101509.
   Macintyre P, 2020, INT J APPL EARTH OBS, V85, DOI 10.1016/j.jag.2019.101980.
   Modica G, 2015, FOREST SYST, V24, DOI 10.5424/fs/2015243-07855.
   Mondal P, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11242928.
   Parente L, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11232881.
   Pesaresi S., 2011, RECOGNITION CHARACTE, V2, P1, DOI {[}10.3390/d12080313, DOI 10.3390/D12080313].
   Pesaresi S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12071132.
   PRAAGMAN J, 1985, EUR J OPER RES, V19, P144, DOI 10.1016/0377-2217(85)90321-2.
   Pratico S, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13040586.
   Regan A., 2010, P 4S S SMALL SAT SYS, P1.
   Solano F, 2019, SMART INNOV SYST TEC, V100, P173, DOI 10.1007/978-3-319-92099-3\_21.
   Stehman SV, 2019, REMOTE SENS ENVIRON, V231, DOI 10.1016/j.rse.2019.05.018.
   Tassi A, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12223776.
   Tsafack N, 2020, INSECTS, V11, DOI 10.3390/insects11040249.
   Vogelmann JE, 2012, REMOTE SENS ENVIRON, V122, P92, DOI 10.1016/j.rse.2011.06.027.
   Wan HM, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13010144.
   Wessel M, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10091419.
   Xie YC, 2008, J PLANT ECOL, V1, P9, DOI 10.1093/jpe/rtm005.
   Xu KJ, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12101554.
   Zhang ZH, 2016, ANN TRANSL MED, V4, DOI 10.21037/atm.2016.03.37.},
Number-of-Cited-References = {36},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {12},
Doc-Delivery-Number = {BS4SW},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000722429500012},
DA = {2023-08-12},
}

@inproceedings{ WOS:000316990800156,
Author = {Kundu, S. and Hazra, A. and Deb, K. and Hazra, P.},
Editor = {Bose, C and Chowdhury, AS and Venkateswaran, P and Sarkar, SK},
Title = {. Dimensionality Reduction of Morphological features of Tomato Leaves
   and Fruiting Habits},
Booktitle = {PROCEEDINGS OF THE 2012 INTERNATIONAL CONFERENCE ON COMMUNICATIONS,
   DEVICES AND INTELLIGENT SYSTEMS (CODLS)},
Year = {2012},
Pages = {608-611},
Note = {International Conference on Communications, Devices and Intelligent
   Systems (COOlS), Jadavpur Univ, Dept Elect \& Tele-Communicat Engn,
   Kolkata, INDIA, DEC 28-29, 2012},
Organization = {IEEE; Jadavpur Univ; IEEE Commun Soc (COMSOC), Kolkata chapter},
Abstract = {Tomato (Solanum lycopersicum L) belongs to the family Solanaceae, which
   is extensively grown around the world. In Agriculture and Horticulture,
   the genetic purity of cultivars is critical to farmers, plant breeders,
   seed producers and as well as regulatory agencies. The genetic and
   morphological shape based features are used to classify different tomato
   cultivars and species. However, the large variations present in the
   shapes of tomato leaves and fruits make it complex enough to classify.
   In this paper we have applied image processing techniques on tomato
   leaves and fruits to obtain enhanced binarized images which precede the
   most crucial part of shape based feature extraction. Different
   morphological features are obtained and analyzed for tomato leaf
   recognition prototyping model. Finally, features are transformed for
   dimension reduction to get a better visibility of the features through
   Principal Component Analysis.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kundu, S (Corresponding Author), Jadavpur Univ, Dept Comp Sci \& Engn, Kolkata, India.
   Kundu, S.; Hazra, A.; Deb, K., Jadavpur Univ, Dept Comp Sci \& Engn, Kolkata, India.
   Hazra, P., Bidhan Chandra Krishi Viswavidyalaya, Fac Horticulture, Dept Vegetable, Kalyani, West Bengal, India.},
ISBN = {978-1-4673-4698-6; 978-1-4673-4699-3},
Keywords = {Tomato leaf; Tomato Fruit; Morphology; Leaf Classification; Feature
   Transformation; Dimensionality Reduction; Principal Component Analysis},
Keywords-Plus = {MACHINE VISION; IDENTIFICATION},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic; Telecommunications},
Affiliations = {Jadavpur University; Bidhan Chandra Agricultural University},
ResearcherID-Numbers = {Hazra, Abhisek/HLP-9177-2023},
ORCID-Numbers = {Hazra, Abhisek/0000-0003-0141-4339},
Cited-References = {Chakrabarti A. K., 2010, International Journal of Vegetable Science, V16, P326, DOI 10.1080/19315260.2010.482579.
   Chakrabarti A.K., 2009, INT C SOFT COMP PATT.
   Cooke RJ, 1999, POTATO RES, V42, P529, DOI 10.1007/BF02358169.
   Granitto PM, 2005, COMPUT ELECTRON AGR, V47, P15, DOI 10.1016/j.compag.2004.10.003.
   Hong F., 2004, P 8 INT C CONTR AUT.
   Hossain J., P 13 INT C COMP INF.
   Lucheng H., 2010, P 12 INT C COMP MOD.
   Sanyal P, 2008, IMAGING SCI J, V56, P319, DOI 10.1179/174313108X319397.
   Sanyal P., 2007, 10 INT C INF TECHN I.
   Tian L, 1997, T ASAE, V40, P1761.
   Warren D., 1997, 1 EUR C INF TECHN AG.},
Number-of-Cited-References = {11},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BEJ74},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000316990800156},
DA = {2023-08-12},
}

@article{ WOS:000792911200002,
Author = {Ganguly, Shreyan and Bhowal, Pratik and Oliva, Diego and Sarkar, Ram},
Title = {BLeafNet: A Bonferroni mean operator based fusion of CNN models for
   plant identification using leaf image classification},
Journal = {ECOLOGICAL INFORMATICS},
Year = {2022},
Volume = {69},
Month = {JUL},
Abstract = {Plants, the only natural source of oxygen, are the most important
   resources for every species in the world. A proper identification of
   plants is important for different fields. The observation of leaf
   characteristics is a popular method as leaves are easily available for
   examination. Researchers are increasingly applying image processing
   techniques for the identification of plants based on leaf images. In
   this paper, we have proposed a leaf image classification model, called
   BLeafNet, for plant identification, where the concept of deep learning
   is combined with Bonferroni fusion learning. Initially, we have designed
   five classification models, using ResNet-50 architecture, where five
   different inputs are separately used in the models. The inputs are the
   five variants of the leaf grayscale images, RGB, and three individual
   channels of RGB -red, green, and blue. For fusion of the five ResNet50
   outputs, we have used the Bonferroni mean operator as it expresses
   better connectivity among the confidence scores, and it also obtains
   better results than the individual models. We have also proposed a
   two-tier training method for properly training the end-to-end model. To
   evaluate the proposed model, we have used the Malayakew dataset,
   collected at the Royal Botanic Gardens in New England, which is a very
   challenging dataset as many leaves from different species have a very
   similar appearance. Besides, the proposed method is evaluated using the
   Leafsnap and the Flavia datasets. The obtained results on both the
   datasets confirm the superiority of the model as it outperforms the
   results achieved by many state-of-the-art models.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Oliva, D (Corresponding Author), Univ Guadalajara, CUCEI, Dept Innovat Basada Informat \& Conocimiento, Guadalajara, Mexico.
   Oliva, D (Corresponding Author), Tomsk Polytech Univ, Sch Comp Sci \& Robot, Tomsk, Russia.
   Ganguly, Shreyan, Jadavpur Univ, Dept Construct Engn, Jadavpur, India.
   Bhowal, Pratik, Jadavpur Univ, Dept Instrumentat \& Elect Engn, Jadavpur, India.
   Oliva, Diego, Univ Guadalajara, CUCEI, Dept Innovat Basada Informat \& Conocimiento, Guadalajara, Mexico.
   Sarkar, Ram, Jadavpur Univ, Dept Comp Sci \& Engn, Jadavpur, India.
   Oliva, Diego, Tomsk Polytech Univ, Sch Comp Sci \& Robot, Tomsk, Russia.},
DOI = {10.1016/j.ecoinf.2022.101585},
EarlyAccessDate = {FEB 2022},
Article-Number = {101585},
ISSN = {1574-9541},
EISSN = {1878-0512},
Keywords = {Plant identification; Leaf image; Deep learning; Ensemble learning;
   Bonferroni operator},
Research-Areas = {Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Ecology},
Author-Email = {diego.oliva@cucei.udg.mx},
Affiliations = {Jadavpur University; Jadavpur University; Universidad de Guadalajara;
   Jadavpur University; Tomsk Polytechnic University},
ResearcherID-Numbers = {Oliva, Diego/A-3271-2016
   Sarkar, Ram/AAX-3822-2020},
ORCID-Numbers = {Oliva, Diego/0000-0001-8781-7993
   Sarkar, Ram/0000-0001-8813-4086},
Funding-Acknowledgement = {Center for Microprocessor Applications for Training Education and
   Research (CMATER) research laboratory of the Computer Science and
   Engineering Department, Jadavpur Univer-sity, Kolkata, India},
Funding-Text = {\& nbsp;We would like to thank the Center for Microprocessor
   Applications for Training Education and Research (CMATER) research
   laboratory of the Computer Science and Engineering Department, Jadavpur
   Univer-sity, Kolkata, India for providing us the infrastructural
   support.},
Cited-References = {Abdul Kadir, 2014, Research Journal of Pharmaceutical, Biological and Chemical Sciences, V5, P1.
   Akter R., 2020, P 2020 EM TECHN COMP, P1.
   {[}Anonymous], 2013, INT J ENG SCI TECHNO.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Banerjee A, 2021, IEEE T CIRC SYST VID, V31, P2206, DOI 10.1109/TCSVT.2020.3019293.
   Beikmohammadi A., 2019, COMPUT ELECTRON AGR, V10.
   Beikmohammadi A, 2018, 2018 4TH IRANIAN CONFERENCE ON SIGNAL PROCESSING AND INTELLIGENT SYSTEMS (ICSPIS), P21, DOI 10.1109/ICSPIS.2018.8700547.
   Bhowal P, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03491-4.
   Carneiro Gabriel, 2021, 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, P7055, DOI 10.1109/IGARSS47720.2021.9555141.
   Chaudhury A, 2020, IEEE ACM T COMPUT BI, V17, P1042, DOI 10.1109/TCBB.2018.2873611.
   Christenhusz MJM, 2016, PHYTOTAXA, V261, P201, DOI 10.11646/phytotaxa.261.3.1.
   Du CB, 2017, IEEE ACCESS, V5, P15750, DOI 10.1109/ACCESS.2017.2735019.
   Esgario JGM, 2020, COMPUT ELECTRON AGR, V169, DOI 10.1016/j.compag.2019.105162.
   Figueroa-Mata G, 2020, BIOMIMETICS-BASEL, V5, DOI 10.3390/biomimetics5010008.
   Gwo CY, 2013, APPL PLANT SCI, V1, DOI 10.3732/apps.1200005.
   He K, 2015, C COMPUTER VISION PA.
   Herdiyeni Y, 2013, INT C ADV COMP SCI I, P353, DOI 10.1109/ICACSIS.2013.6761601.
   Hsiao J.-K., 2014, COMP STUDY LEAF IMAG, P389.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Arribas JI, 2011, COMPUT ELECTRON AGR, V78, P9, DOI 10.1016/j.compag.2011.05.007.
   Kan H. X., 2017, Pattern Recognition and Image Analysis, V27, P581, DOI 10.1134/S105466181703018X.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee S.H., 2015, DEEP PLANT PLANT IDE.
   Liu JC, 2018, CHIN AUTOM CONGR, P3165, DOI 10.1109/CAC.2018.8623427.
   Matey Palash Sushil, 2014, LEAF RECOGNITION USI.
   Mettripun N, 2020, 2020 59TH ANNUAL CONFERENCE OF THE SOCIETY OF INSTRUMENT AND CONTROL ENGINEERS OF JAPAN (SICE), P372.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Ray B, 2021, APPL SOFT COMPUT, V98, DOI 10.1016/j.asoc.2020.106935.
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   Sanyal R., 2021, IEEE ACM T COMPUT BI, V2021.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Song YP, 2019, IEEE ACCESS, V7, P163277, DOI 10.1109/ACCESS.2019.2951607.
   Sujith A, 2020, 2020 4 INT C COMPUTI, P220, DOI {[}10.1109/ICCMC48092.2020.ICCMC-00042, DOI 10.1109/ICCMC48092.2020.ICCMC-00042].
   Tsolakidis DG, 2014, LECT NOTES ARTIF INT, V8445, P406, DOI 10.1007/978-3-319-07064-3\_33.
   Wang JL, 2013, COMPUT ELECTRON AGR, V96, P23, DOI 10.1016/j.compag.2013.04.014.
   Wu S.G., 2007, CORR ABS 07074289.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Zhang JP, 2019, MED IMAGE ANAL, V54, P10, DOI 10.1016/j.media.2019.02.010.
   Zhao A., 2017, GRAPH BASED EXTRACTI, P663.},
Number-of-Cited-References = {39},
Times-Cited = {15},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {7},
Journal-ISO = {Ecol. Inform.},
Doc-Delivery-Number = {1C1TW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000792911200002},
DA = {2023-08-12},
}

@article{ WOS:000595518600003,
Author = {Natesan, Sowmya and Armenakis, Costas and Vepakomma, Udayalakshmi},
Title = {Individual tree species identification using Dense Convolutional Network
   (DenseNet) on multitemporal RGB images from UAV},
Journal = {JOURNAL OF UNMANNED VEHICLE SYSTEMS},
Year = {2020},
Volume = {8},
Number = {4},
Pages = {310-333},
Month = {DEC},
Abstract = {Tree species identification at the individual tree level is crucial for
   forest operations and management, yet its automated mapping remains
   challenging. Emerging technology, such as the high-resolution imagery
   from unmanned aerial vehicles (UAV) that is now becoming part of every
   forester's surveillance kit, can potentially provide a solution to
   better characterize the tree canopy. To address this need, we have
   developed an approach based on a deep Convolutional Neural Network (CNN)
   to classify forest tree species at the individual tree-level that uses
   high-resolution RGB images acquired from a consumer-grade camera mounted
   on a UAV platform. This work explores the ability of the Dense
   Convolutional Network (DenseNet) to classify commonly available economic
   coniferous tree species in eastern Canada. The network was trained using
   multitemporal images captured under varying acquisition parameters to
   include seasonal, temporal, illumination, and angular variability.
   Validation of this model using distinct images over a mixed-wood forest
   in Ontario, Canada, showed over 84\% classification accuracy in
   distinguishing five predominant species of coniferous trees. The model
   remains highly robust even when using images taken during different
   seasons and times, and with varying illumination and angles.},
Publisher = {CANADIAN SCIENCE PUBLISHING},
Address = {65 AURIGA DR, SUITE 203, OTTAWA, ON K2E 7W6, CANADA},
Type = {Article},
Language = {English},
Affiliation = {Armenakis, C (Corresponding Author), York Univ, Lassonde Sch Engn, Dept Earth \& Space Sci \& Engn, Geomat Engn, Toronto, ON M3J 1P3, Canada.
   Natesan, Sowmya; Armenakis, Costas, York Univ, Lassonde Sch Engn, Dept Earth \& Space Sci \& Engn, Geomat Engn, Toronto, ON M3J 1P3, Canada.
   Vepakomma, Udayalakshmi, FPInnovations, Pointe Claire, PQ H9R 3J9, Canada.},
DOI = {10.1139/juvs-2020-0014},
ISSN = {2291-3467},
Keywords = {UAV RGB images; tree species identification; DenseNet CNN; coniferous
   species; deep learning},
Keywords-Plus = {UNMANNED AERIAL VEHICLES; BARK BEETLE DAMAGE; CLASSIFICATION; NORWAY;
   AREA},
Research-Areas = {Remote Sensing},
Web-of-Science-Categories  = {Remote Sensing},
Author-Email = {armenc@yorku.ca},
Affiliations = {York University - Canada},
Funding-Acknowledgement = {Natural Sciences and Engineering Research Council of Canada (NSERC);
   York University},
Funding-Text = {The authors would like to thank Petawawa Reserve Forest (Canadian Forest
   Service) and Petawawa Army Base for making the area available for our
   trial and for providing background data. This work is financially
   supported by the Natural Sciences and Engineering Research Council of
   Canada (NSERC) and York University. Support for data acquisition was
   made possible by Natural Resources Canada under the Transformative
   Technologies contribution agreement with FPInnovations.},
Cited-References = {Abadi M., 2016, TENSORFLOW LARGE SCA, DOI {[}DOI 10.1038/NN.3331, DOI 10.5555/3026877.3026899].
   {[}Anonymous], 2018, AG PHOT.
   Assmann JJ, 2019, J UNMANNED VEH SYST, V7, P54, DOI 10.1139/juvs-2018-0018.
   Baena S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0188714.
   Berra EF, 2017, IEEE T GEOSCI REMOTE, V55, P4878, DOI 10.1109/TGRS.2017.2655365.
   Bowyer J.L., 2001, ENCY MAT SCI TECHNOL, P9637, DOI {[}10.1016/B0-08-043152-6/01746-0., DOI 10.1016/B0-08-043152-6/01746-0].
   Brovkina O, 2018, GEO-SPAT INF SCI, V21, P12, DOI 10.1080/10095020.2017.1416994.
   Budei BC, 2018, REMOTE SENS ENVIRON, V204, P632, DOI {[}10.1016/j.rse.2017.09.037, 10.1016].
   Carpentier M, 2018, IEEE INT C INT ROBOT, P1075, DOI 10.1109/IROS.2018.8593514.
   Chollet F., 2015, KERAS.
   Cown D., 2004, New Zealand Journal of Forestry, V49, P10.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013.
   Franklin SE, 2018, J UNMANNED VEH SYST, V6, P195, DOI 10.1139/juvs-2017-0022.
   Franklin SE, 2018, INT J REMOTE SENS, V39, P5236, DOI 10.1080/01431161.2017.1363442.
   Fricker GA, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11192326.
   Gini R, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7080315.
   Guo YH, 2019, SUSTAINABILITY-BASEL, V11, DOI 10.3390/su11040978.
   Hafemann LG, 2014, INT C PATT RECOG, P1103, DOI 10.1109/ICPR.2014.199.
   Hartling S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061284.
   Huang G., 2017, P 2017 IEEE C COMP V, P4700, DOI {[}DOI 10.1109/CVPR.2017.243, 10.1109/CVPR.2017.243].
   Imangholiloo M, 2019, FORESTS, V10, DOI 10.3390/f10050415.
   Ioffe S., 2015, INT C MACHINE LEARNI, DOI DOI 10.1007/S13398-014-0173-7.2.
   Jaskierniak D, 2016, J PLANT ECOL, V9, P272, DOI 10.1093/jpe/rtv056.
   Ji SP, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010075.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Kattenborn T, 2020, REMOTE SENS ECOL CON, V6, P472, DOI 10.1002/rse2.146.
   Kattenborn T, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-53797-9.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Ko C., 2018, ISPRS ANNA PHOTOGRAM, VIV-2, P153, DOI DOI 10.5194/ISPRS-ANNALS-IV-2-153-2018.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kuzmin A, 2016, EUR J REMOTE SENS, V49, P239, DOI 10.5721/EuJRS20164914.
   Li GD, 2020, REMOTE SENS LETT, V11, P195, DOI 10.1080/2150704X.2019.1697001.
   Li Y, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9010067.
   Liang XL, 2019, FOR ECOSYST, V6, DOI 10.1186/s40663-019-0173-3.
   Liu LX, 2017, REMOTE SENS ENVIRON, V200, P170, DOI 10.1016/j.rse.2017.08.010.
   Torres DL, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020563.
   Lopatin J, 2019, REMOTE SENS ECOL CON, V5, P302, DOI 10.1002/rse2.109.
   Luo YA, 2018, 2018 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), P464, DOI 10.1109/ICALIP.2018.8455251.
   Fernandez-Guisuraga JM, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020586.
   Mizoguchi T., 2017, INT SOC OPTICS PHOTO, V10332.
   Nasi R, 2018, URBAN FOR URBAN GREE, V30, P72, DOI 10.1016/j.ufug.2018.01.010.
   Nasi R, 2015, REMOTE SENS-BASEL, V7, P15467, DOI 10.3390/rs71115467.
   Natesan S., 2019, INT ARCH PHOTOGRAMME, V4213, P475, DOI {[}DOI 10.5194/ISPRS-ARCHIVES-XLII-2-W13-475-2019, 10.5194/isprs-archives-XLII-2-W13-475-2019].
   Natural Resources Canada, 2019, FOR FACT BOOK 2018 2.
   Nevalainen O, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9030185.
   Onishi M., 2018, ARXIV180410390.
   Padua L, 2017, INT J REMOTE SENS, V38, P2349, DOI 10.1080/01431161.2017.1297548.
   Puliti S, 2018, FORESTS, V9, DOI 10.3390/f9030102.
   Rudnicki M., 2017, FPLGTR248 USDA FOR S.
   Saarinen N, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10020338.
   Salami E, 2014, REMOTE SENS-BASEL, V6, P11051, DOI 10.3390/rs61111051.
   Samiappan S, 2017, DRONES-BASEL, V1, DOI 10.3390/drones1010004.
   Scott GJ, 2017, IEEE GEOSCI REMOTE S, V14, P549, DOI 10.1109/LGRS.2017.2657778.
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002.
   Sothe C, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111338.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Talbot B, 2018, SCAND J FOREST RES, V33, P387, DOI 10.1080/02827581.2017.1418421.
   Torresan C, 2017, INT J REMOTE SENS, V38, P2427, DOI 10.1080/01431161.2016.1252477.
   Trier OD, 2018, EUR J REMOTE SENS, V51, P336, DOI 10.1080/22797254.2018.1434424.
   Tuominen S, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10050714.
   Valente J., 2019, ISPRS ANN PHOTOGRAMM, VIV-2/W5, P179, DOI {[}DOI 10.5194/ISPRS-ANNALS-IV-2-W5-179-2019, 10.5194/isprs-annals-IV-2-W5-179-2019].
   Vepakomma U., 2019, INT ARCH PHOTOGRAMME, P643, DOI {[}10.5194/isprs-archives-XLII-2-W13-643-2019.XLII-2/W13, DOI 10.5194/ISPRS-ARCHIVES-XLII-2-W13-643-2019.XLII-2/W13].
   Vepakomma U, 2018, FORESTS, V9, DOI 10.3390/f9090540.
   Weinstein B, 2020, ECOL INFORM, V56, DOI 10.1016/j.ecoinf.2020.101061.
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6.
   White JC, 2016, CAN J REMOTE SENS, V42, P619, DOI 10.1080/07038992.2016.1207484.
   Xie YC, 2008, J PLANT ECOL, V1, P9, DOI 10.1093/jpe/rtm005.
   Xu YH, 2020, IEEE T BIG DATA, V6, P492, DOI 10.1109/TBDATA.2019.2923243.
   Yang GF, 2018, INT GEOSCI REMOTE SE, P2595, DOI 10.1109/IGARSS.2018.8517520.
   Yosinski J, 2014, ADV NEUR IN, V27.
   Zhang CJ, 2019, J APPL REMOTE SENS, V13, DOI 10.1117/1.JRS.13.016519.
   Zhang JM, 2019, MATH BIOSCI ENG, V16, P3345, DOI 10.3934/mbe.2019167.},
Number-of-Cited-References = {76},
Times-Cited = {20},
Usage-Count-Last-180-days = {10},
Usage-Count-Since-2013 = {37},
Journal-ISO = {J. Unmanned Veh. Syst.},
Doc-Delivery-Number = {PA3FF},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000595518600003},
OA = {Bronze},
DA = {2023-08-12},
}

@inproceedings{ WOS:000384639900049,
Author = {Chakkaravarthy, S. Sibi and Sajeevan, G. and Kamalanaban, E. and Kumar,
   K. A. Varun},
Editor = {Thampi, SM and Bandyopadhyay, S and Krishnan, S and Li, KC and Mosin, S and Ma, M},
Title = {Automatic Leaf Vein Feature Extraction for First Degree Veins},
Booktitle = {ADVANCES IN SIGNAL PROCESSING AND INTELLIGENT RECOGNITION SYSTEMS
   (SIRS-2015)},
Series = {Advances in Intelligent Systems and Computing},
Year = {2016},
Volume = {425},
Pages = {581-592},
Note = {2nd International Symposium on Signal Processing and Intelligent
   Recognition Systems (SIRS), Trivandrum, INDIA, DEC 16-19, 2015},
Organization = {Indian Inst Informat Technol \& Management; ACM, Trivandrum Chapter; Int
   Neural Networks Soc India, Reg Chapter; TECHNOPARK; IUPRAI},
Abstract = {Leaf vein is one of the most important and complex feature of the leaf
   used in automatic plant identification system for automatic
   classification and identification of plant species. Leaves of different
   species have different characteristic features which help in
   classification of specific plant species. These features help the
   botanists in identifying the key species of the plants from its leaf
   images more accurately. Vein feature is one of the most important
   complex features of leaf in plant species. In this paper we proposed a
   new feature extraction model, to extract the vein features from the leaf
   images. The proposed system using Hough lines stems the extraction of
   vein feature from the leaf images by plotting the lines over the first
   degree veins. Angle of lines from the primary vein to the secondary vein
   is considered as the input parameter for processing the extracted vein
   features. The centroid vein angle is considered to be the primary
   feature. The vein feature was given as the input to the neural network
   for efficient classification and the results were tested with 15 species
   of plants taken from ``leafilia{''} data sets.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Chakkaravarthy, SS (Corresponding Author), VelTech Rangarajan Dr Sagunthala R\&D Inst Sci \& T, Dept Comp Sci \& Engn, Madras, Tamil Nadu, India.
   Chakkaravarthy, S. Sibi; Kamalanaban, E.; Kumar, K. A. Varun, VelTech Rangarajan Dr Sagunthala R\&D Inst Sci \& T, Dept Comp Sci \& Engn, Madras, Tamil Nadu, India.
   Sajeevan, G., Ctr Dev Adv Comp C DAC, Dept Comp Sci \& Engn, SS \& DM Grp, Pune, Maharashtra, India.},
DOI = {10.1007/978-3-319-28658-7\_49},
ISSN = {2194-5357},
EISSN = {2194-5365},
ISBN = {978-3-319-28658-7; 978-3-319-28656-3},
Keywords = {Leaf vein; Venation; Leaf recognition system; Leaf identification
   system; Leaf classification; Leafilia},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {sb.sibi@gmail.com
   sajeevan@cdac.in
   kamalanaban2009@gmail.com
   varun.kumar300@gmail.com},
Affiliations = {Vel Tech Rangarajan Dr Sagunthala R\&D Institute of Science \&
   Technology; Centre for Development of Advanced Computing (C-DAC)},
ResearcherID-Numbers = {S, Sibi Chakkaravarthy/W-7245-2019
   K A, Varun Kumar/ABE-6527-2021
   Chakkaravarthy, Sibi S/D-3735-2019
   },
ORCID-Numbers = {S, Sibi Chakkaravarthy/0000-0001-7778-0453
   K A, Varun Kumar/0000-0001-9281-2273
   Kamalanaban, Ethala/0009-0007-1052-8502},
Cited-References = {Ash A., MANUAL LEAF ARCHITEC.
   Chaki J., 2011, IEEE INT C 1 JACSA 2.
   Correa E., 2014, PROTOCOL LEAF IMAGE.
   EHSANIRAD A., 2010, INT J COMPUTER SCI I, V8, P78.
   Fu H., 2003, IEEE INT C NEUR NETW.
   Fu H., 2006, IEEE P VIS IMAGE SIG, V153.
   Hati S., 2013, INT J COMPUTER APPL, V62, p0975 .
   Hossain J., 2010, ICCIT.
   Huai Y., 2009, IEEE INT C ICISE 200.
   Janani R., 2013, IEEE INT C ICAES 201.
   Li Y., 2005, LEAF VEIN EXTRACTION.
   Li Y, 2006, IEEE SYS MAN CYBERN, P3890, DOI 10.1109/ICSMC.2006.384738.
   Mishra P.K., 2012, IEEE INT C ICAESM 20.
   Mzoughi O., 2012, IEEE ICIP 2012.
   Nguyen Q.-K., 2013, IEEE INT C ATC 2013.
   Pan S., 2009, INT C ICISE.
   Pham N.-H., 2013, IEEE INT C COMP MAN.
   Prasad S., 2011, IEEE INT C ICCCT 201.
   Revathi P., IEEEI INT C EM TREND.
   Sun Z., 2011, INT C VIRT REAL VIS.
   Valliammal N., 2011, IEEE INT C PROC AUT.
   Wang L., 2010, IEEE P 29 CHIN CONTR.
   Wang X., 2010, IEEE INT C ICCASM 20.
   Wang Z., IEEE P VIS IM SIGN P.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yahiaoui I., 2012, IEEE INT C ICME.
   Yahiaoui I., 2006, LECT NOTES COMPUTER.
   Zheng X., 2009, IEEE INT C INF ENG C.
   Zheng X., 2010, IEEE 2 INT C INF ENG.
   Zulkifli Z., 2009, PLANT LEAF IDENTIFIC.},
Number-of-Cited-References = {30},
Times-Cited = {3},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {8},
Doc-Delivery-Number = {BF7ZC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000384639900049},
DA = {2023-08-12},
}

@inproceedings{ WOS:000465006200064,
Author = {Malik, Muzamil and Ikram, Amna and Batool, Syeda Naila and Aslam, Waqar},
Editor = {Bajwa, IS and Kamareddine, F and Costa, A},
Title = {A Performance Assessment of Rose Plant Classification Using Machine
   Learning},
Booktitle = {INTELLIGENT TECHNOLOGIES AND APPLICATIONS, INTAP 2018},
Series = {Communications in Computer and Information Science},
Year = {2019},
Volume = {932},
Pages = {745-756},
Note = {1st International Conference on Intelligent Technologies and
   Applications (INTAP), Islamia Univ Bahawalpur, Bahawalpur, PAKISTAN, OCT
   23-25, 2018},
Organization = {Artificial Intelligence Res Grp; Sir Sadiq Assoc Comp; Higher Educ
   Commiss},
Abstract = {Machine learning enriches the field of artificial intelligence that aims
   to make computers powerful by providing them information extracted from
   data. Flowers identification is highly significant and relevant for
   Plant Scientists. Carrying it out manually is not only a tedious task
   but also prone to errors due to a large number of flower types. Using
   machine learning algorithms to identify flowers is appealing. To this
   aim, two observations on flower leaves are relevant and leverage flower
   identification: one, flower plants have key knowledge in their leaves,
   thus enable distinctiveness; two, leaves have a much longer life on
   plants than flowers and fruits. In this paper, we have proposed a
   machine learning approach based on k Nearest Neighbor (k-NN) to identify
   rose types. Following steps are carried out during the identification
   process. First, rose plant images are taken using 23MP camera, ensuring
   temperature uniformity during the experiment. Second, texture and
   histogram features are extracted from the captured images. Third, k-NN
   algorithm is applied to these features with k taking values between 1
   and 10. Our research brings to limelight the usefulness of selected
   features for rose type identification with histogram and texture
   features achieving maximum accuracies of 65\% and 45.50\% respectively.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Malik, M (Corresponding Author), Islamia Univ Bahawalpur, Dept CS \& IT, Bahawalpur, Pakistan.
   Malik, Muzamil; Aslam, Waqar, Islamia Univ Bahawalpur, Dept CS \& IT, Bahawalpur, Pakistan.
   Ikram, Amna; Batool, Syeda Naila, Govt Sadiq Coll Women Univ, Dept CS \& IT, Bahawalpur, Pakistan.},
DOI = {10.1007/978-981-13-6052-7\_64},
ISSN = {1865-0929},
EISSN = {1865-0937},
ISBN = {978-981-13-6052-7},
Keywords = {Machine learning; Flower classification; Artificial intelligence;
   Nearest Neighbor},
Keywords-Plus = {NEAREST-NEIGHBOR; SYSTEM; IMAGES},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {muzmalik2013@gmail.com},
Affiliations = {Islamia University of Bahawalpur},
ResearcherID-Numbers = {Malik, Muzamil/ABB-4325-2020
   },
ORCID-Numbers = {Malik, Muzamil/0000-0003-4765-483X},
Cited-References = {Anxiang H, 2003, P 2003 IEEE INT C AC.
   Ashish D, 2009, INT J REMOTE SENS, V30, P1989, DOI 10.1080/01431160802549187.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Batista G.E., 2010, P 9 INT C MACH LEARN, DOI {[}10.1109/ICMLA.2010.142, DOI 10.1109/ICMLA.2010.142].
   Bharathi S, 2013, TENCON IEEE REGION.
   Blachnik M, 2008, LECT NOTES COMPUT SC, V5163, P827, DOI 10.1007/978-3-540-87536-9\_85.
   Boland M. V, 1999, HARALICK TEXTURE FEA.
   Cheng KY, 2014, NEUROCOMPUTING, V145, P416, DOI 10.1016/j.neucom.2014.05.011.
   El-Bendary N, 2015, EXPERT SYST APPL, V42, P1892, DOI 10.1016/j.eswa.2014.09.057.
   Garcia-Pedrajas N, 2017, IEEE T NEUR NET LEAR, V28, P470, DOI 10.1109/TNNLS.2015.2506821.
   Gou JP, 2011, COMM COM INF SC, V159, P118.
   Krishnaveni S, 2017, ICT EXPRESS, V3, P148, DOI 10.1016/j.icte.2017.04.006.
   Lakhvir Kaur L, 2016, INT J ENG COMPUT SCI, V5, P2319.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Miao ZJ, 2006, IMAGE VISION COMPUT, V24, P1115, DOI 10.1016/j.imavis.2006.04.004.
   Mittal Kavita, 2019, International Journal of Information Technology, V11, P535, DOI 10.1007/s41870-018-0233-x.
   Mohamad FS, 2011, PROCEDIA COMPUT SCI, V4, P1296, DOI 10.1016/j.procs.2011.04.140.
   Mohanaiah P., 2013, INT J SCI RES PUBLIC, V3, P1.
   Pang C, 2018, MULTIMED TOOLS APPL, V77, P7851, DOI 10.1007/s11042-017-4679-9.
   Pinto L. S, 2016, IEEE INT C REC TREND.
   Rahmani ME, 2016, INT J AGRIC ENVIRON, V7, P17, DOI 10.4018/IJAEIS.2016100102.
   Ramli Suzaimah, 2008, American Journal of Environmental Sciences, V4, P583, DOI 10.3844/ajessp.2008.583.588.
   Shiliang Sun, 2010, Proceedings of the 2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2010), P91, DOI 10.1109/FSKD.2010.5569740.
   Singh C, 2016, J VIS COMMUN IMAGE R, V41, P225, DOI 10.1016/j.jvcir.2016.10.002.
   Siraj F, 2010, 2010 2 INT C COMP IN.
   Umbaugh S. E., 2005, COMPUTER IMAGING DIG, P292.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhang YD, 2014, J FOOD ENG, V143, P167, DOI 10.1016/j.jfoodeng.2014.07.001.},
Number-of-Cited-References = {30},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {6},
Doc-Delivery-Number = {BM5IC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000465006200064},
DA = {2023-08-12},
}

@article{ WOS:000861438200004,
Author = {Campos, Juan and Yee, Arturo and Vega, Ines F.},
Title = {Simplifying VGG-16 for Plant Species Identification},
Journal = {IEEE LATIN AMERICA TRANSACTIONS},
Year = {2022},
Volume = {20},
Number = {11},
Pages = {2330-2338},
Month = {NOV},
Abstract = {Plant species identification represents an extraordinary challenge for
   machine learning due to visual interspecies similarities and large
   intraspecies variations. Furthermore, research literature reports that
   plant species identification usually lacks sufficiently large datasets
   for training classification models. In this paper, we address this
   problem with a model that simplifies the VGG-16 architecture, the N-VGG
   model. The idea behind N-VGG is to reduce experimentally observed
   overfitting on VGG-16 by using as few trainable parameters as possible.
   To do this, we substitute the flattening layer on the VGG architecture
   with a global average pooling layer. This reduces the size of the
   feature vector. In addition, we eliminate one of the two fully-connected
   layers and use a new hyper-parameter, N, to indicate the number of nodes
   on the remaining layer. To show the robustness of the N-VGG model, we
   conducted extensive experimentation. We trained N-VGG on five datasets
   for plant species identification. Four of these datasets are publicly
   available and have been widely used as benchmarks for plant
   identification models. For all datasets, we compare the accuracy of
   N-VGG to that of the VGG-16, Inception-v4, and EfficienNet-B3 models.
   The experimental results show that the N-VGG model achieved the best
   classification performance for all but one datasets, whereas all the
   models showed a remarkable performance for the remaining dataset. This
   evidence supports our initial idea that, for plant species
   classification, some accuracy might be lost due to overfitting and that
   having fewer trainable parameters helps in producing a more robust
   model.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Campos, J (Corresponding Author), Univ Tonoma Sinaloa, Ciencias Informac, Culiacan, Sinaloa, Mexico.
   Campos, Juan, Univ Tonoma Sinaloa, Ciencias Informac, Culiacan, Sinaloa, Mexico.
   Yee, Arturo, Univ Autonoma Sinaloa, Fac Informat Culiacan, Culiacan, Sinaloa, Mexico.
   Vega, Ines F., Univ Autonoma Sinaloa, Parque Innovac Tecnol, Culiacan, Sinaloa, Mexico.},
DOI = {10.1109/TLA.2022.9904757},
ISSN = {1548-0992},
Keywords = {Computer architecture; Feature extraction; Tensors; Deep learning;
   Transfer learning; Training; Computational modeling; Convolutional
   Neural Network; Deep Learning; Fine-grained Classification; Plant
   Species Identification; VGG-16},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic},
Author-Email = {juan.campos@uas.edu.mx
   arturo.yee@uas.edu.mx
   ifvega@uas.edu.mx},
Affiliations = {Universidad Autonoma de Sinaloa; Universidad Autonoma de Sinaloa},
ORCID-Numbers = {Campos-Leal, Juan/0000-0002-1337-1885
   Yee, Arturo/0000-0002-9052-6588},
Funding-Acknowledgement = {Mexican Council of Science and Technology (CONACYT) {[}291772];
   CONACYT-INEGI fund},
Funding-Text = {The authors wish to thank the Mexican Council of Science and Technology
   (CONACYT) for the scholarship awarded to the first author. They would
   also like to thank for the financial support provided by the research
   grant 291772 from the CONACYT-INEGI fund.},
Cited-References = {Aravind KR, 2019, SPAN J AGRIC RES, V17, DOI 10.5424/sjar/2019173-14762.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Bisen D, 2021, MULTIMED TOOLS APPL, V80, P6443, DOI 10.1007/s11042-020-10038-w.
   Chollet F., 2017, DEEP LEARNING PYTHON.
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Dourado LA, 2021, ECOL INFORM, V65, DOI 10.1016/j.ecoinf.2021.101411.
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   github, GITHUB.
   Goeau H., 2017, P C LABS EVALUATION.
   Goeau H, 2013, P 2 ACM INT WORKSH M, P23, DOI DOI 10.1145/2509896.2509902.
   Goeau H., 2018, P C LABS EVALUATION.
   Goeau H., 2016, WORKING NOTES CLEF 2, P428.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hughes David P., 2015, ARXIV PREPR ARXIV151.
   Johnson JM, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0192-5.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lee JH, 2019, IEEE GLOBE WORK, DOI {[}10.1109/ICAIIC.2019.8669002, 10.1109/gcwkshps45667.2019.9024382].
   Lee SH, 2020, COMPUT ELECTRON AGR, V170, DOI 10.1016/j.compag.2020.105220.
   Li AX, 2019, INT J AUTOM COMPUT, V16, DOI 10.1007/s11633-019-1177-8.
   Li Y, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105803.
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1\_48.
   Lu Y, 2017, NEUROCOMPUTING, V267, P378, DOI 10.1016/j.neucom.2017.06.023.
   Naturalista, 2021, COM NAC CON US BIOD.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Saedi SI, 2020, EXPERT SYST APPL, V159, DOI 10.1016/j.eswa.2020.113594.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Sulc M., 2018, P C LABS EVALUATION.
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Too EC, 2019, COMPUT ELECTRON AGR, V161, P272, DOI 10.1016/j.compag.2018.03.032.
   Vizcarra G, 2021, ECOL INFORM, V62, DOI 10.1016/j.ecoinf.2021.101268.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319.},
Number-of-Cited-References = {44},
Times-Cited = {1},
Usage-Count-Last-180-days = {22},
Usage-Count-Since-2013 = {30},
Journal-ISO = {IEEE Latin Am. Trans.},
Doc-Delivery-Number = {4Y3OS},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000861438200004},
DA = {2023-08-12},
}

@article{ WOS:000772049600001,
Author = {Huang, Yanbo and Zhao, Xiaohu and Pan, Zeng and Reddy, Krishna N. and
   Zhang, Jingcheng},
Title = {Hyperspectral plant sensing for differentiating glyphosate-resistant and
   glyphosate-susceptible johnsongrass through machine learning algorithms},
Journal = {PEST MANAGEMENT SCIENCE},
Year = {2022},
Volume = {78},
Number = {6},
Pages = {2370-2377},
Month = {JUN},
Abstract = {BACKGROUND Johnsongrass (Sorghum halepense) is one of the weeds that
   evolves resistance to glyphosate {[}N-(phosphonomethyl)-glycine], the
   most widely used herbicide, and the weed may cause agronomic troublesome
   in the southern USA. This paper reports a study on developing a
   hyperspectral plant sensing approach to explore the spectral features of
   glyphosate-resistant (GR) and glyphosate-sensitive (GS) plants to
   evaluate this approach using machine learning algorithms to
   differentiate between GR and GS plants. RESULTS On average, GR plants
   have higher spectral reflectance compared with GS plants. The sensitive
   spectral bands were optimally selected using the successive projections
   algorithm respectively wrapped with the machine learning algorithms of
   k-nearest neighbors (KNN), random forest (RF), and support vector
   machine (SVM) with Fisher linear discriminant analysis (FLDA) to
   classify between GS and GS plants. At 3 weeks after transplanting (WAT)
   KNN and SVM could not acceptably classify the GR and GS plants but they
   improved significantly with the stages to have their overall accuracies
   reaching 73\% and 77\%, respectively, at 5 WAT. RF and FLDA had a better
   ability to classify the plants at 3 WAT but RF was low in accuracy at 2
   WAT while FLDA dropped accuracy to 50\% at 4 WAT from 57\% at 3 WAT and
   raised it to 73\% at 5 WAT. CONCLUSIONS Previous studies were conducted
   developing the hyperspectral imaging approach to differentiate GR Palmer
   amaranth from GS Palmer amaranth and GR Italian ryegrass from GS Italian
   ryegrass with classification accuracies of 90\% and 80\%, respectively.
   This study demonstrated that the hyperspectral plant sensing approach
   could be developed to differentiate GR johnsongrass from
   glyphosate-sensitive GS johnsongrass with the highest classification
   accuracy of 77\%. The comparison with our previous studies indicated
   that the similar hyperspectral approach could be used and transferred
   from classification across different GR and GS weed biotypes, such as
   Palmer amaranth, Italian ryegrass and johnsongrass, so it is highly
   possible for classification of more other GR and GS weed biotypes as
   well. On the basis of classic pattern recognition approaches the process
   of plant classification can be enhanced by modeling using machine
   learning algorithms.},
Publisher = {JOHN WILEY \& SONS LTD},
Address = {THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Huang, YB (Corresponding Author), ARS, USDA, Genet \& Sustainable Agr Res Unit, Mississippi State, MS 39762 USA.
   Huang, Yanbo, ARS, USDA, Genet \& Sustainable Agr Res Unit, Mississippi State, MS 39762 USA.
   Zhao, Xiaohu; Pan, Zeng; Zhang, Jingcheng, Hangzhou Dianzi Univ, Hangzhou, Peoples R China.
   Reddy, Krishna N., ARS, USDA, Crop Prod Syst Res Unit, Stoneville, MS USA.},
DOI = {10.1002/ps.6864},
EarlyAccessDate = {MAR 2022},
ISSN = {1526-498X},
EISSN = {1526-4998},
Keywords = {glyphosate-resistant weed; johnsongrass; hyperspectral plant sensing;
   machine learning},
Keywords-Plus = {VARIABLE SELECTION; DISEASE; INDEXES},
Research-Areas = {Agriculture; Entomology},
Web-of-Science-Categories  = {Agronomy; Entomology},
Author-Email = {yanbo.huang@usda.gov},
Affiliations = {Mississippi State University; United States Department of Agriculture
   (USDA); Hangzhou Dianzi University; United States Department of
   Agriculture (USDA)},
Funding-Acknowledgement = {Mississippi Soybean Promotion Board},
Funding-Text = {The authors thank Ryan Poe and Howard Brand for their technical
   assistance in spectral measurement of the plants, and the Mississippi
   Soybean Promotion Board for funding.},
Cited-References = {Alimjan G, 2018, INT J PATTERN RECOGN, V32, DOI 10.1142/S0218001418590127.
   Araujo MCU, 2001, CHEMOMETR INTELL LAB, V57, P65, DOI 10.1016/S0169-7439(01)00119-8.
   Ashourloo D, 2014, REMOTE SENS-BASEL, V6, P4723, DOI 10.3390/rs6064723.
   Belgiu M, 2016, ISPRS J PHOTOGRAMM, V114, P24, DOI 10.1016/j.isprsjprs.2016.01.011.
   Benbrook CM, 2016, ENVIRON SCI EUR, V28, DOI 10.1186/s12302-016-0070-0.
   Blackburn GA, 2007, J EXP BOT, V58, P855, DOI 10.1093/jxb/erl123.
   Calderon R, 2015, REMOTE SENS-BASEL, V7, P5584, DOI 10.3390/rs70505584.
   Cheng T, 2011, REMOTE SENS ENVIRON, V115, P659, DOI 10.1016/j.rse.2010.11.001.
   Chrysostomou K., 2009, ENCY DATA WAREHOUSIN, V2nd ed., P2103.
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964.
   Duke SO, 2012, J AGR FOOD CHEM, V60, P10375, DOI 10.1021/jf302436u.
   Ferreira AD, 2017, COMPUT ELECTRON AGR, V143, P314, DOI 10.1016/j.compag.2017.10.027.
   Freeman C, 2013, IEEE T CYBERNETICS, V43, P1990, DOI 10.1109/TSMCB.2012.2237394.
   Golhani Kamlesh, 2018, Information Processing in Agriculture, V5, P354, DOI 10.1016/j.inpa.2018.05.002.
   Gregorutti B, 2017, STAT COMPUT, V27, P659, DOI 10.1007/s11222-016-9646-1.
   Heap I, 2021, HERBICIDE RESISTANT.
   Huang YB, 2020, INT J AGR BIOL ENG, V13, P1, DOI 10.25165/j.ijabe.20201304.5501.
   Huang YanBo, 2018, American Journal of Plant Sciences, V9, P1467, DOI 10.4236/ajps.2018.97107.
   Huang YB, 2016, BIOSYST ENG, V149, P51, DOI 10.1016/j.biosystemseng.2016.06.013.
   Huang Y, 2009, ALGORITHMS, V2, P973, DOI 10.3390/algor2030973.
   Huang YB, 2010, COMPUT ELECTRON AGR, V71, P107, DOI 10.1016/j.compag.2010.01.001.
   Koger CH, 2005, WEED SCI, V53, P84, DOI 10.1614/WS-04-102R.
   le Maire G, 2004, REMOTE SENS ENVIRON, V89, P1, DOI 10.1016/j.rse.2003.09.004.
   Liaghat S, 2014, INT J REMOTE SENS, V35, P3427, DOI 10.1080/01431161.2014.903353.
   Liu F, 2009, ANAL CHIM ACTA, V635, P45, DOI 10.1016/j.aca.2009.01.017.
   Liu QS, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P197, DOI 10.1109/AFGR.2002.1004157.
   Liu Y, 2005, IEEE IJCNN, P849.
   Lowe A, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0233-z.
   Mahlein AK, 2012, PLANT METHODS, V8, DOI 10.1186/1746-4811-8-3.
   Ongsulee P., 2017, 2017 15 INT C ICT KN, P1, DOI DOI 10.1109/ICTKE.2017.8259629.
   Paiva HM, 2012, CHEMOMETR INTELL LAB, V118, P260, DOI 10.1016/j.chemolab.2012.05.014.
   Reddy KN, 2014, PEST MANAG SCI, V70, P1910, DOI 10.1002/ps.3755.
   Shaner DL, 2005, WEED SCI, V53, P769, DOI 10.1614/WS-05-009R.1.
   Silva F. B., 2014, American Journal of Plant Sciences, V5, P2509, DOI 10.4236/ajps.2014.516265.
   Strobl C, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-307.
   Milanez KDTM, 2017, ANAL CHIM ACTA, V984, P76, DOI 10.1016/j.aca.2017.07.037.
   Thenkabail PS, 2014, PHOTOGRAMM ENG REM S, V80, P697.
   Wang SP, 2018, IEEE T SYST MAN CY-S, V48, P329, DOI 10.1109/TSMC.2016.2605132.
   Xie C, 2015, SCI REP-UK, V5, DOI 10.1038/srep08914.
   Xin Y, 2018, IEEE ACCESS, V6, P35365, DOI 10.1109/ACCESS.2018.2836950.
   Yu JL, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.01422.
   Zhang JC, 2019, PEST MANAG SCI, V75, P3260, DOI 10.1002/ps.5448.
   Zhang L, 2017, DRUG DISCOV TODAY, V22, P1680, DOI 10.1016/j.drudis.2017.08.010.},
Number-of-Cited-References = {43},
Times-Cited = {2},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {12},
Journal-ISO = {Pest Manag. Sci.},
Doc-Delivery-Number = {1I3LL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000772049600001},
DA = {2023-08-12},
}

@article{ WOS:000905095000004,
Author = {Xiao, Qi and Zhou, Zhenzeng and Shen, Zijie and Chen, Jiandan and Gu,
   Chunchuan and Li, Lihua and Chen, Fengnong and Liu, Hongying},
Title = {Electrochemical fingerprinting combined with machine learning algorithm
   for closely related medicinal plant identification},
Journal = {SENSORS AND ACTUATORS B-CHEMICAL},
Year = {2023},
Volume = {375},
Month = {JAN 15},
Abstract = {Medicinal plants have been widely used in the treatment of various
   diseases for human health. We developed a novel method for the
   identification of closely related medicinal plants using a machine
   learning (ML)-based electrochemical fingerprinting platform. Firstly,
   the system featured a bare glassy carbon electrode capable of recording
   the voltammetric response of active components in medicinal plants as
   electrochemical fingerprints. Subsequently, different algorithms and
   various datasets were employed to analyze the correlation between the
   above electrochemical fingerprint data and the medicinal plant species.
   As a proof-of-concept, 6 species of Anoectochilus roxburghii (A.
   roxburghii) were selected as the verification samples. The
   electrochemical fingerprints of the samples were measured by
   differential pulse voltammetry in two buffer solutions. Thereafter, four
   powerful ML algorithms were utilized for the identification of A.
   roxburghii with different datasets. The results showed that the accuracy
   of identifying species reached 94.4 \% by the nonlinear support vector
   machines based on the slope data of electrochemical responses in two
   buffer solutions, evidencing the successful discrimination of closely
   related medical plants by this method. Additionally, ML combined with
   electrochemical fingerprinting approaches had the advantages of being
   rapid, affordable, and straightforward, which provided potential
   applications in pharmaceutical research and plant taxonomy.},
Publisher = {ELSEVIER SCIENCE SA},
Address = {PO BOX 564, 1001 LAUSANNE, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Li, LH; Chen, FN; Liu, HY (Corresponding Author), Hangzhou Dianzi Univ, Coll Automat, Hangzhou 310018, Peoples R China.
   Xiao, Qi; Zhou, Zhenzeng; Shen, Zijie; Chen, Jiandan; Li, Lihua; Chen, Fengnong; Liu, Hongying, Hangzhou Dianzi Univ, Coll Automat, Hangzhou 310018, Peoples R China.
   Gu, Chunchuan, Zhejiang Shuren Univ, Shulan Hangzhou Hosp, Shulan Int Med Coll, Hangzhou 310000, Peoples R China.},
DOI = {10.1016/j.snb.2022.132922},
EarlyAccessDate = {NOV 2022},
Article-Number = {132922},
EISSN = {0925-4005},
Keywords = {Electrochemical fingerprinting; Machine learning; Medicinal plant
   identification; Anoectochilus roxburghii},
Research-Areas = {Chemistry; Electrochemistry; Instruments \& Instrumentation},
Web-of-Science-Categories  = {Chemistry, Analytical; Electrochemistry; Instruments \& Instrumentation},
Author-Email = {lilh@hdu.edu.cn
   fnchen@hdu.edu.cn
   liuhongying@hdu.edu.cn},
Affiliations = {Hangzhou Dianzi University; Zhejiang Shuren University},
Funding-Acknowledgement = {Science and Technology Program of Zhejiang Province of China; Na- tional
   Key R \& D Program of China;  {[}LGF22H200012];  {[}2021YFE0203700]},
Funding-Text = {This work was financially supported by the Science and Technology
   Program of Zhejiang Province of China (LGF22H200012) and the Na- tional
   Key R \& D Program of China (2021YFE0203700) . We sincerely express our
   gratitude to them for their financial support. There is no conflict of
   interest, finance or otherwise.},
Cited-References = {Banik K, 2019, PHARMACOL RES, V144, P192, DOI 10.1016/j.phrs.2019.04.004.
   Bernalte E, 2020, SENSOR ACTUAT B-CHEM, V307, DOI 10.1016/j.snb.2019.127620.
   Bonetti JL, 2022, ANAL CHEM, V94, P5029, DOI 10.1021/acs.analchem.1c04985.
   Bouwmeester R, 2019, ANAL CHEM, V91, P3694, DOI 10.1021/acs.analchem.8b05820.
   Chai QQ, 2021, J PHARMACEUT BIOMED, V199, DOI 10.1016/j.jpba.2021.114035.
   Chen Q, 2022, FRONT PLANT SCI, V13, DOI 10.3389/fpls.2022.764255.
   Du L, 2022, ACS EST ENG, V2, P92, DOI 10.1021/acsestengg.1c00287.
   El-Saber Batiha G, 2020, NUTRIENTS, V12, DOI 10.3390/nu12030872.
   Eswaran M, 2021, J HAZARD MATER, V418, DOI 10.1016/j.jhazmat.2021.126267.
   Fabijanic I, 2019, CARBOHYD POLYM, V216, P36, DOI 10.1016/j.carbpol.2019.03.102.
   Frezza C, 2020, PLANTS-BASEL, V9, DOI 10.3390/plants9070888.
   Fu L, 2021, J HERB MED, V30, DOI 10.1016/j.hermed.2021.100512.
   Fu L, 2020, BIOSENS BIOELECTRON, V159, DOI 10.1016/j.bios.2020.112212.
   Fu L, 2018, BIOSENS BIOELECTRON, V120, P102, DOI 10.1016/j.bios.2018.08.052.
   Fukada K, 2022, ANAL CHEM, V94, P7060, DOI 10.1021/acs.analchem.2c00378.
   Huang L, 2018, NUCLEIC ACIDS RES, V46, pD1117, DOI 10.1093/nar/gkx1028.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Kennedy GF, 2019, ANAL CHEM, V91, P12220, DOI 10.1021/acs.analchem.9b01891.
   Khanna K, 2021, PHYTOMEDICINE, V85, DOI 10.1016/j.phymed.2020.153361.
   Koltai H, 2020, TRENDS PLANT SCI, V25, P976, DOI 10.1016/j.tplants.2020.04.007.
   Liu MX, 2021, ANAL CHEM, V93, P9002, DOI 10.1021/acs.analchem.1c02010.
   Mistry A, 2021, ACS ENERGY LETT, V6, P1422, DOI 10.1021/acsenergylett.1c00194.
   Mori CC, 2018, PHYTOCHEMISTRY, V151, P9, DOI 10.1016/j.phytochem.2018.03.011.
   Partel J, 2021, AOB PLANTS, V13, DOI 10.1093/aobpla/plab050.
   Walsh I, 2021, NAT METHODS, V18, P1122, DOI 10.1038/s41592-021-01205-4.
   Wei ZB, 2022, FOOD CHEM, V372, DOI 10.1016/j.foodchem.2021.131158.
   Willcox ML, 2021, FRONT PHARMACOL, V12, DOI 10.3389/fphar.2021.777561.
   Wu N, 2021, ACS NANO, V15, P19522, DOI 10.1021/acsnano.1c06429.
   Wu T, 2021, J FUNCT FOODS, V87, DOI 10.1016/j.jff.2021.104815.
   Xu CH, 2022, CHEM ENG J, V436, DOI 10.1016/j.cej.2022.135203.
   Xu MJ, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01474.
   Xu YT, 2020, BIOELECTROCHEMISTRY, V133, DOI 10.1016/j.bioelechem.2020.107455.
   Ye SY, 2017, J ETHNOPHARMACOL, V209, P184, DOI 10.1016/j.jep.2017.07.032.
   Yu J, 2021, ECOTOX ENVIRON SAFE, V208, DOI 10.1016/j.ecoenv.2020.111691.
   Yue JQ, 2021, FRONT PLANT SCI, V12, DOI 10.3389/fpls.2021.752863.
   Yue JQ, 2021, CRIT REV ANAL CHEM, V51, P373, DOI 10.1080/10408347.2020.1736506.
   Yue XY, 2019, FOOD CHEM, V289, P84, DOI 10.1016/j.foodchem.2019.03.044.
   Zhang YH, 2021, FOOD CHEM, V346, DOI 10.1016/j.foodchem.2020.128895.
   Zhao YL, 2021, BIOSENS BIOELECTRON, V186, DOI 10.1016/j.bios.2021.113291.
   Zhao ZT, 2021, SENSOR ACTUAT B-CHEM, V326, DOI 10.1016/j.snb.2020.128811.
   Zheng YJ, 2020, FRONT PHARMACOL, V11, DOI 10.3389/fphar.2020.00349.},
Number-of-Cited-References = {41},
Times-Cited = {3},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {8},
Journal-ISO = {Sens. Actuator B-Chem.},
Doc-Delivery-Number = {7K2DI},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000905095000004},
DA = {2023-08-12},
}

@article{ WOS:000824451400001,
Author = {Li, Yingbo and Chai, Guoqi and Wang, Yueting and Lei, Lingting and
   Zhang, Xiaoli},
Title = {ACE R-CNN: An Attention Complementary and Edge Detection-Based Instance
   Segmentation Algorithm for Individual Tree Species Identification Using
   UAV RGB Images and LiDAR Data},
Journal = {REMOTE SENSING},
Year = {2022},
Volume = {14},
Number = {13},
Month = {JUL},
Abstract = {Accurate and automatic identification of tree species information at the
   individual tree scale is of great significance for fine-scale
   investigation and management of forest resources and scientific
   assessment of forest ecosystems. Despite the fact that numerous studies
   have been conducted on the delineation of individual tree crown and
   species classification using drone high-resolution red, green and blue
   (RGB) images, and Light Detection and Ranging (LiDAR) data, performing
   the above tasks simultaneously has rarely been explored, especially in
   complex forest environments. In this study, we improve upon the state of
   the Mask region-based convolution neural network (Mask R-CNN) with our
   proposed attention complementary network (ACNet) and edge detection
   R-CNN (ACE R-CNN) for individual tree species identification in
   high-density and complex forest environments. First, we propose ACNet as
   the feature extraction backbone network to fuse the weighted features
   extracted from RGB images and canopy height model (CHM) data through an
   attention complementary module, which is able to selectively fuse
   weighted features extracted from RGB and CHM data at different scales,
   and enables the network to focus on more effective information. Second,
   edge loss is added to the loss function to improve the edge accuracy of
   the segmentation, which is calculated through the edge detection filter
   introduced in the Mask branch of Mask R-CNN. We demonstrate the
   performance of ACE R-CNN for individual tree species identification in
   three experimental areas of different tree species in southern China
   with precision (P), recall (R), F1-score, and average precision (AP)
   above 0.9. Our proposed ACNet-the backbone network for feature
   extraction-has better performance in individual tree species
   identification compared with the ResNet50-FPN (feature pyramid network).
   The addition of the edge loss obtained by the Sobel filter further
   improves the identification accuracy of individual tree species and
   accelerates the convergence speed of the model training. This work
   demonstrates the improved performance of ACE R-CNN for individual tree
   species identification and provides a new solution for tree-level
   species identification in complex forest environments, which can support
   carbon stock estimation and biodiversity assessment.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Zhang, XL (Corresponding Author), Beijing Forestry Univ, Coll Forestry, Beijing Key Lab Precis Forestry, Beijing 100083, Peoples R China.
   Zhang, XL (Corresponding Author), Beijing Forestry Univ, Key Lab Forest Cultivat \& Protect, Minist Educ, Beijing 100083, Peoples R China.
   Li, Yingbo; Chai, Guoqi; Wang, Yueting; Lei, Lingting; Zhang, Xiaoli, Beijing Forestry Univ, Coll Forestry, Beijing Key Lab Precis Forestry, Beijing 100083, Peoples R China.
   Li, Yingbo; Chai, Guoqi; Wang, Yueting; Lei, Lingting; Zhang, Xiaoli, Beijing Forestry Univ, Key Lab Forest Cultivat \& Protect, Minist Educ, Beijing 100083, Peoples R China.},
DOI = {10.3390/rs14133035},
Article-Number = {3035},
EISSN = {2072-4292},
Keywords = {individual tree species identification; ACE R-CNN; Mask R-CNN; attention
   complementary module; edge detection; UAV RGB and CHM data},
Keywords-Plus = {CROWN DELINEATION; CLASSIFICATION; EFFICIENT},
Research-Areas = {Environmental Sciences \& Ecology; Geology; Remote Sensing; Imaging
   Science \& Photographic Technology},
Web-of-Science-Categories  = {Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science \& Photographic Technology},
Author-Email = {liyingbo7@bjfu.edu.cn
   chaigq@bjfu.edu.cn
   wangyueting@bjfu.edu.cn
   leilt@bjfu.edu.cn
   zhangxl@bjfu.edu.cn},
Affiliations = {Beijing Forestry University; Beijing Forestry University},
ORCID-Numbers = {Zhang, Xiaoli/0000-0001-7443-1557},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}32171779]; National Key
   Research and Development Program of China {[}2017YFD0600900]; DRAGON 5
   COOPERATION {[}59257]},
Funding-Text = {This research was funded by the National Natural Science Foundation of
   China (grant number: 32171779), National Key Research and Development
   Program of China (grant number: 2017YFD0600900) and DRAGON 5 COOPERATION
   {[}ID: 59257].},
Cited-References = {Abbas S, 2021, ISPRS J PHOTOGRAMM, V177, P204, DOI 10.1016/j.isprsjprs.2021.05.003.
   {[}Anonymous], 2020, IEEE T PATTERN ANAL, DOI {[}DOI 10.1109/TPAMI.2018.2844175, 10.1109/TPAMI.2018.2844175].
   Briechle S, 2021, INT J APPL EARTH OBS, V98, DOI 10.1016/j.jag.2020.102292.
   Bruggisser M, 2017, REMOTE SENS ENVIRON, V196, P28, DOI 10.1016/j.rse.2017.04.025.
   Budei BC, 2018, REMOTE SENS ENVIRON, V204, P632, DOI {[}10.1016/j.rse.2017.09.037, 10.1016].
   Cao JJ, 2021, INT J APPL EARTH OBS, V102, DOI 10.1016/j.jag.2021.102414.
   Cao XM, 2021, COMPUT METH PROG BIO, V207, DOI 10.1016/j.cmpb.2021.106174.
   Chu PY, 2021, PATTERN RECOGN LETT, V147, P206, DOI 10.1016/j.patrec.2021.04.022.
   Dalponte M, 2019, PEERJ, V6, DOI 10.7717/peerj.6227.
   Dalponte M, 2014, REMOTE SENS ENVIRON, V140, P306, DOI 10.1016/j.rse.2013.09.006.
   Dalponte M, 2012, REMOTE SENS ENVIRON, V123, P258, DOI 10.1016/j.rse.2012.03.013.
   Duncanson LI, 2014, REMOTE SENS ENVIRON, V154, P378, DOI 10.1016/j.rse.2013.07.044.
   Duncanson L, 2022, REMOTE SENS ENVIRON, V270, DOI 10.1016/j.rse.2021.112845.
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013.
   Goldbergs G, 2018, REMOTE SENS ENVIRON, V205, P141, DOI 10.1016/j.rse.2017.11.010.
   Hamraz H, 2019, ISPRS J PHOTOGRAMM, V158, P219, DOI 10.1016/j.isprsjprs.2019.10.011.
   Hao ZB, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14071561.
   Hartling S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061284.
   Hu B., 2021, ISPRS OPEN J PHOTOGR, V1, P100002, DOI {[}10.1016/j.ophoto.2021.100002, DOI 10.1016/J.OPHOTO.2021.100002].
   Hu GS, 2020, BIOSYST ENG, V194, P138, DOI 10.1016/j.biosystemseng.2020.03.021.
   Jaskierniak D, 2021, ISPRS J PHOTOGRAMM, V171, P171, DOI 10.1016/j.isprsjprs.2020.10.016.
   Lei LT, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14030504.
   Liu HP, 2022, COMPUT ELECTRON AGR, V194, DOI 10.1016/j.compag.2022.106794.
   Liu L, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104871.
   Liu MH, 2021, MEASUREMENT, V177, DOI 10.1016/j.measurement.2021.109301.
   Loh D, 2021, COMPUT MED IMAG GRAP, V88, DOI 10.1016/j.compmedimag.2020.101845.
   Lu XC, 2014, ISPRS J PHOTOGRAMM, V94, P1, DOI 10.1016/j.isprsjprs.2014.03.014.
   Luo J, 2022, MULTIMED TOOLS APPL, V81, P34295, DOI 10.1007/s11042-021-11248-6.
   Mayra J, 2021, REMOTE SENS ENVIRON, V256, DOI 10.1016/j.rse.2021.112322.
   Modzelewska A, 2020, INT J APPL EARTH OBS, V84, DOI 10.1016/j.jag.2019.101960.
   Mongus D, 2015, ISPRS J PHOTOGRAMM, V108, P219, DOI 10.1016/j.isprsjprs.2015.08.004.
   Natesan S, 2020, J UNMANNED VEH SYST, V8, P310, DOI 10.1139/juvs-2020-0014.
   Plesoianu AI, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12152426.
   Rana P, 2022, ISPRS J PHOTOGRAMM, V184, P189, DOI 10.1016/j.isprsjprs.2022.01.003.
   Safonova A, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21051617.
   Schiefer F, 2020, ISPRS J PHOTOGRAMM, V170, P205, DOI 10.1016/j.isprsjprs.2020.10.015.
   Shi YF, 2018, INT J APPL EARTH OBS, V73, P207, DOI 10.1016/j.jag.2018.06.018.
   Sothe C, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111338.
   Terryn L, 2022, REMOTE SENS ENVIRON, V271, DOI 10.1016/j.rse.2022.112912.
   Torabzadeh H, 2019, AGR FOREST METEOROL, V279, DOI 10.1016/j.agrformet.2019.107744.
   Trier OD, 2018, EUR J REMOTE SENS, V51, P336, DOI 10.1080/22797254.2018.1434424.
   Wang L, 2019, REMOTE SENS ENVIRON, V231, DOI 10.1016/j.rse.2019.111223.
   Wang SJ, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23091160.
   Wu JT, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105504.
   Wu XW, 2020, NEUROCOMPUTING, V396, P39, DOI 10.1016/j.neucom.2020.01.085.
   Yang JT, 2020, IEEE J-STARS, V13, P1055, DOI 10.1109/JSTARS.2020.2979369.
   Yang RC, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14061524.
   Yang XB, 2019, NEURAL COMPUT APPL, V31, P8681, DOI 10.1007/s00521-019-04457-6.
   Zhang B, 2020, REMOTE SENS ENVIRON, V247, DOI 10.1016/j.rse.2020.111938.
   Zhang C, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14040874.
   Zhao HW, 2022, ISPRS J PHOTOGRAMM, V187, P328, DOI 10.1016/j.isprsjprs.2022.03.005.
   Zhu QK, 2020, IEEE T MED IMAGING, V39, P753, DOI 10.1109/TMI.2019.2935018.
   Zimmermann RS, 2019, COMPUT VIS IMAGE UND, V188, DOI 10.1016/j.cviu.2019.102795.},
Number-of-Cited-References = {53},
Times-Cited = {9},
Usage-Count-Last-180-days = {26},
Usage-Count-Since-2013 = {45},
Journal-ISO = {Remote Sens.},
Doc-Delivery-Number = {2W3TX},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000824451400001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000325699700003,
Author = {Yusof, Rubiyah and Khalid, Marzuki and Khairuddin, Anis Salwa Mohd},
Title = {Fuzzy logic-based pre-classifier for tropical wood species recognition
   system},
Journal = {MACHINE VISION AND APPLICATIONS},
Year = {2013},
Volume = {24},
Number = {8},
Pages = {1589-1604},
Month = {NOV},
Abstract = {Classifying tropical wood species poses a considerable economic
   challenge and failure to classify the wood species accurately can have
   significant effects on timber industries. The problem of wood
   recognition is compounded with the nonlinearities of the features among
   the similar wood species. Besides that, large wood databases presented a
   problem of large processing time especially for online wood recognition
   system. In view of these problems, we propose the use of fuzzy
   logic-based pre-classifier as a means of treating uncertainty to improve
   the classification accuracy of tropical wood recognition system. The
   pre-classifier serve as a clustering mechanism for the large database
   simplifying the classification process making it more efficient. The use
   of the fuzzy logic-based pre-classifier has managed to increase the
   accuracy of the wood recognition system by 4 \% and reduce the
   processing time for training and testing by more than 75 \% and 26 \%
   respectively.},
Publisher = {SPRINGER},
Address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
Type = {Article},
Language = {English},
Affiliation = {Yusof, R (Corresponding Author), Univ Teknol Malaysia, Ctr Artificial Intelligence \& Robot, Kuala Lumpur, Malaysia.
   Yusof, Rubiyah; Khalid, Marzuki, Univ Teknol Malaysia, Ctr Artificial Intelligence \& Robot, Kuala Lumpur, Malaysia.
   Khairuddin, Anis Salwa Mohd, Univ Malaya, Dept Elect Engn, Fac Engn, Kuala Lumpur, Malaysia.},
DOI = {10.1007/s00138-013-0526-9},
ISSN = {0932-8092},
EISSN = {1432-1769},
Keywords = {Wood species recognition system; Pattern recognition; Fuzzy logic; Wood
   pores; Texture},
Keywords-Plus = {NEURAL-NETWORK; FEATURES; DESIGN},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Engineering, Electrical \& Electronic},
Author-Email = {rubiyah@ic.utm.my
   anissalwa@um.edu.my},
Affiliations = {Universiti Teknologi Malaysia; Universiti Malaya},
ResearcherID-Numbers = {KHAIRUDDIN, ANIS SALWA MOHD/B-5340-2010
   yusof, rubiyah/AAV-9212-2020},
ORCID-Numbers = {KHAIRUDDIN, ANIS SALWA MOHD/0000-0002-9873-4779
   },
Funding-Acknowledgement = {Malaysian Ministry of Science, Technology and Innovation (MOSTI)
   {[}TF0106C213]},
Funding-Text = {The authors would like to thank Malaysian Ministry of Science,
   Technology and Innovation (MOSTI) for funding this research through
   Technofund research grant (TF0106C213). The authors also would like to
   thank Forest Research Institute of Malaysia (FRIM) for providing us with
   the wood samples.},
Cited-References = {Aitkenhead MJ, 2003, ENG APPL ARTIF INTEL, V16, P167, DOI 10.1016/S0952-1976(03)00042-3.
   Amornraksa T, 2006, ELECTRON LETT, V42, P522, DOI 10.1049/el:20064330.
   {[}Anonymous], 1998, HDB PATTERN RECOGNIT.
   Beritelli F, 2000, 2000 5TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS I-III, P1483, DOI 10.1109/ICOSP.2000.893381.
   Bombardier V, 2010, ENG APPL ARTIF INTEL, V23, P978, DOI 10.1016/j.engappai.2010.05.001.
   Buch N., 2008, 5th International Conference on Visual Information Engineering, VIE 2008, P182, DOI 10.1049/cp:20080305.
   Castellani M, 2009, ENG APPL ARTIF INTEL, V22, P732, DOI 10.1016/j.engappai.2009.01.013.
   Daschiel H, 2005, IEEE T GEOSCI REMOTE, V43, P188, DOI 10.1109/TGRS.2004.838374.
   Foo SY, 2002, ENG APPL ARTIF INTEL, V15, P253, DOI 10.1016/S0952-1976(02)00041-6.
   Golipour L, 2009, INT CONF ACOUST SPEE, P1341, DOI 10.1109/ICASSP.2009.4959840.
   Hu Ng, 2010, 2010 International Conference on Computer Applications and Industrial Electronics (ICCAIE), P463, DOI 10.1109/ICCAIE.2010.5735124.
   Huang CL, 1998, MACH VISION APPL, V10, P292, DOI 10.1007/s001380050080.
   Ishibuchi H, 1999, IEEE T SYST MAN CY B, V29, P601, DOI 10.1109/3477.790443.
   Jian H., 1010, 6 INT C SEM KNOWL GR, P426.
   Jordan R, 1998, ULTRASONICS, V36, P219, DOI 10.1016/S0041-624X(97)00148-0.
   Khairuddin U., 2011, ICIC EXPRESS LETT B, V2, P441.
   Khalid M, 2008, INT J SIMUL SYST SCI, V9, P9.
   Labiod S, 2005, FUZZY SET SYST, V151, P59, DOI 10.1016/j.fss.2004.10.009.
   Lepisto L, 2006, IEE P-VIS IMAGE SIGN, V153, P475, DOI 10.1049/ip-vis:20050315.
   Lin L, 2012, PATTERN RECOGN, V45, P231, DOI 10.1016/j.patcog.2011.06.011.
   Lin L, 2009, PATTERN RECOGN, V42, P1297, DOI 10.1016/j.patcog.2008.10.033.
   Martins J., 2012, MACH VIS APPL.
   Menon P. K. B., 1993, STRUCTURE IDENTIFICA.
   Miao ZJ, 2006, ENG APPL ARTIF INTEL, V19, P79, DOI 10.1016/j.engappai.2005.05.009.
   Nguema MBB, 2000, ENG APPL ARTIF INTEL, V13, P279, DOI 10.1016/S0952-1976(99)00061-5.
   Piuri V, 2010, IEEE T SYST MAN CY C, V40, P358, DOI 10.1109/TSMCC.2009.2039479.
   Qin XJ, 2004, PROC CVPR IEEE, P326.
   Qin XJ, 2007, IEEE T VIS COMPUT GR, V13, P379, DOI 10.1109/TVCG.2007.31.
   Schaefer G, 2009, PATTERN RECOGN, V42, P1133, DOI 10.1016/j.patcog.2008.08.007.
   Su T.-L., 2009, INT C WAV AN PATT RE.
   Sun YL, 2004, IEEE T FUZZY SYST, V12, P755, DOI 10.1109/TFUZZ.2004.836097.
   Wang JP, 2011, MACH VISION APPL, V22, P87, DOI 10.1007/s00138-009-0197-8.
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X.
   Zhang GQP, 2000, IEEE T SYST MAN CY C, V30, P451, DOI 10.1109/5326.897072.},
Number-of-Cited-References = {34},
Times-Cited = {22},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {29},
Journal-ISO = {Mach. Vis. Appl.},
Doc-Delivery-Number = {235ER},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000325699700003},
DA = {2023-08-12},
}

@article{ WOS:000858264300001,
Author = {He, Jie and Sun, Yongke and Yu, Chunjiang and Cao, Yong and Zhao, Youjie
   and Du, Guanben},
Title = {An Improved Wood Recognition Method Based on the One-Class Algorithm},
Journal = {FORESTS},
Year = {2022},
Volume = {13},
Number = {9},
Month = {SEP},
Abstract = {Wood recognition is necessary for work in the wood trade activities. The
   advantage of the one-class wood classification method is more
   generalization, and it only needs positive samples and does not need
   negative samples in the training phase, so it is suitable for rare wood
   species inspection. This paper proposed an improved method based on the
   one-class support vector machine (OCSVM) for wood species recognition.
   It uses cross-section images acquired with a magnifying glass, which
   uses a pre-trained VGG16 model for feature extraction, a normal
   distribution test for key features filtering, and OCSVM to determine the
   wood species. The results showed that the approach achieved a mean
   recall of 0.842 for both positive and negative samples, which indicates
   this method has good performance for wood recognition. In a negative
   public dataset, the negative recall reached as high as 0.989, which
   showed that this method has good generalization.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Sun, YK (Corresponding Author), Southwest Forestry Univ, Sch Big Data \& Intelligent Engn, Kunming 650224, Yunnan, Peoples R China.
   Cao, Y (Corresponding Author), Int Engn \& Technol Inst, Hongkong 999077, Peoples R China.
   He, Jie; Sun, Yongke; Yu, Chunjiang; Zhao, Youjie, Southwest Forestry Univ, Sch Big Data \& Intelligent Engn, Kunming 650224, Yunnan, Peoples R China.
   Cao, Yong, Int Engn \& Technol Inst, Hongkong 999077, Peoples R China.
   Du, Guanben, Southwest Forestry Univ, Yunnan Prov Key Lab Wood Adhes \& Glued Prod, Kunming 650224, Yunnan, Peoples R China.},
DOI = {10.3390/f13091350},
Article-Number = {1350},
EISSN = {1999-4907},
Keywords = {wood recognition; transfer learning; one-class classification},
Keywords-Plus = {ANOMALY DETECTION; CLASSIFICATION},
Research-Areas = {Forestry},
Web-of-Science-Categories  = {Forestry},
Author-Email = {sunyongke@swfu.edu.cn
   cn\_caoyong@126.com},
Affiliations = {Southwest Forestry University - China; Southwest Forestry University -
   China},
ORCID-Numbers = {You-jie, Zhao/0000-0002-3198-0465
   He, Jie/0000-0003-2018-7776},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}31960142, 32071688,
   61962055]; Major Project of Science and Technology of Yunnan Province
   {[}202002AD080002]},
Funding-Text = {This research was funded by National Natural Science Foundation of China
   (31960142, 32071688 and 61962055) and the Major Project of Science and
   Technology of Yunnan Province under Grant number 202002AD080002.},
Cited-References = {Almuhtaram H, 2021, WATER RES, V197, DOI 10.1016/j.watres.2021.117073.
   Amin J, 2020, PATTERN RECOGN LETT, V131, P63, DOI 10.1016/j.patrec.2019.11.042.
   Bergmann P, 2020, PROC CVPR IEEE, P4182, DOI 10.1109/CVPR42600.2020.00424.
   Berriri M, 2021, EDUC SCI, V11, DOI 10.3390/educsci11030092.
   Binder A, 2018, P 35 INT C MACHINE L, V80, P4393.
   Breunig MM, 2000, SIGMOD REC, V29, P93, DOI 10.1145/335191.335388.
   Chang K, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21113880.
   Cofer RH, 2003, P SOC PHOTO-OPT INS, V5108, P287, DOI 10.1117/12.487029.
   Comaniciu D, 1997, PROC CVPR IEEE, P750, DOI 10.1109/CVPR.1997.609410.
   Cui XT, 2021, PLASMA SCI TECHNOL, V23, DOI 10.1088/2058-6272/abf1ac.
   DAGOSTINO RB, 1971, BIOMETRIKA, V58, P341, DOI 10.1093/biomet/58.2.341.
   de Andrade BG, 2020, IAWA J, V41, P681, DOI 10.1163/22941932-bja10001.
   de Geus AR, 2021, WOOD SCI TECHNOL, V55, P857, DOI 10.1007/s00226-021-01282-w.
   Fabijanska A, 2021, COMPUT ELECTRON AGR, V181, DOI 10.1016/j.compag.2020.105941.
   Grimwood A, 2021, PHYS IMAG RADIAT ONC, V18, P68, DOI 10.1016/j.phro.2021.05.003.
   Guo W, 2021, ENG APPL ARTIF INTEL, V102, DOI 10.1016/j.engappai.2021.104254.
   Hu W., 2020, C NEURAL INFORM PROC, V33, P19111.
   Huan EY, 2019, COMPUT MATH METHOD M, V2019, DOI 10.1155/2019/1258782.
   Liu FT, 2008, IEEE DATA MINING, P413, DOI 10.1109/ICDM.2008.17.
   Liu SH, 2020, IEEE T INSTRUM MEAS, V69, P8725, DOI 10.1109/TIM.2020.3001370.
   Maboudou-Tchao EM, 2020, QUAL TECHNOL QUANT M, V17, P609, DOI 10.1080/16843703.2019.1711302.
   Mohamad M., 2021, P 6 INT C ELECT CONT, P575.
   Mokji MM, 2007, I C COMP GRAPH IM VI, P273, DOI 10.1109/CGIV.2007.45.
   Perera P., 2021, ARXIV.
   Ravindran P, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0292-9.
   Sangeetha M, 2020, SOFT COMPUT, V24, P13369, DOI 10.1007/s00500-020-04755-5.
   Scholkopf B, 2000, ADV NEUR IN, V12, P582.
   Sethy PK, 2020, COMPUT ELECTRON AGR, V175, DOI 10.1016/j.compag.2020.105527.
   Setiawan W, 2020, TELKOMNIKA, V18, P1382, DOI {[}10.12928/telkomnika.v18i3.14868, DOI 10.12928/TELKOMNIKA.V18I3.14868].
   Shah A, 2021, INFORM SCIENCES, V569, P650, DOI 10.1016/j.ins.2021.05.021.
   Shustrov D, 2019, LECT NOTES COMPUT SC, V11482, P67, DOI 10.1007/978-3-030-20205-7\_6.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Soojin Kim, 2013, 2013 IEEE Third International Conference on Consumer Electronics - Berlin (ICCE-Berlin). Proceedings, P207, DOI 10.1109/ICCE-Berlin.2013.6698033.
   Souza DV, 2020, WOOD SCI TECHNOL, V54, P1065, DOI 10.1007/s00226-020-01196-z.
   Tan YH, 2020, OCEAN ENG, V201, DOI 10.1016/j.oceaneng.2020.107174.
   Tang XJ, 2018, 2018 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INTELLIGENT SYSTEMS (CIIS 2018), P37, DOI 10.1145/3293475.3293493.
   Thi K.M, 2019, INT J TREND SCI RES, V3, P1139.
   Todkar SS, 2021, J APPL GEOPHYS, V192, DOI 10.1016/j.jappgeo.2021.104392.
   Velazquez-Pupo R, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020374.
   Wang HJ, 2013, NEUROCOMPUTING, V116, P192, DOI 10.1016/j.neucom.2012.02.045.
   Wang SQ, 2018, PATTERN RECOGN, V74, P198, DOI 10.1016/j.patcog.2017.09.012.
   Wong TT, 2020, IEEE T KNOWL DATA EN, V32, P1586, DOI 10.1109/TKDE.2019.2912815.
   Xing HJ, 2020, PATTERN RECOGN LETT, V138, P571, DOI 10.1016/j.patrec.2020.09.005.
   Xu JL, 2020, J CHEMOMETR, V34, DOI 10.1002/cem.3132.
   Zhao ZY, 2021, BIORESOURCES, V16, P4986, DOI 10.15376/biores.16.3.4986-4999.},
Number-of-Cited-References = {45},
Times-Cited = {2},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {4},
Journal-ISO = {Forests},
Doc-Delivery-Number = {4T7AE},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000858264300001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000325582300024,
Author = {Rankothge, W. H. and Dissanayake, D. M. S. B. and Gunathilaka, U. V. K.
   T. and Gunarathna, S. A. C. M. and Mudalige, C. M. and Thilakumara, R.
   P.},
Book-Group-Author = {IEEE},
Title = {Plant Recognition System Based on Neural Networks},
Booktitle = {2013 INTERNATIONAL CONFERENCE ON ADVANCES IN TECHNOLOGY AND ENGINEERING
   (ICATE)},
Year = {2013},
Note = {International Conference on Advances in Technology and Engineering
   (ICATE), Mumbai, INDIA, JAN 23-25, 2013},
Abstract = {With the evolution of technologies, people have adopted their day today
   lives to utilize the benefits of highly advanced technologies.
   Artificial Intelligence and Neural Networks are playing major roles in
   this process and they have been involved in fields of medicine,
   automobiles, aeronautical science, military and many more. Unfortunately
   very little concern is devoted to the botanical science field,
   especially in taxonomic researches of plants. Even today, identification
   and classification of unknown plant species are performed manually by
   expert personnel who are very few in number. It takes a long time and
   the results are not very accurate.
   Advanced Plant Identification System (APIS) is an intelligent system
   which has the ability to identify tree species from photographs of their
   leaves and it provides more accurate results within less time.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Rankothge, WH (Corresponding Author), Sri Lanka Inst Informat Technol, Malabe, Sri Lanka.
   Rankothge, W. H.; Dissanayake, D. M. S. B.; Gunathilaka, U. V. K. T.; Gunarathna, S. A. C. M.; Mudalige, C. M.; Thilakumara, R. P., Sri Lanka Inst Informat Technol, Malabe, Sri Lanka.},
ISBN = {978-1-4673-5618-3; 978-1-4673-5617-6},
Keywords = {Plant Identification; Artificial Intelligence; Neural Networks; Pattern
   Recognition},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {windhya.r@sliit.lk
   sachitha4444@gmail.com
   kanchana1987922@gmail.com
   chamarams@yahoo.com
   mudaligechamitha@gmail.com
   rohana.t@sliit.lk},
Affiliations = {Sri Lanka Institute of Information Technology (SLIIT)},
Cited-References = {Abdulrahaman A., 2010, J HORTICULTURE FORES, V2, P112.
   Belhumeur P.N., 2008, SEARCHING WORLDS HER.
   Hossain J., 2010, Proceedings of the 13th International Conference on Computer and Information Technology (ICCIT 2010), P458, DOI 10.1109/ICCITECHN.2010.5723901.
   Rashad M. Z., 2011, International Journal of Computer Science \& Information Technology, V3, P93, DOI 10.5121/ijcsit.2011.3407.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhang L, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 5, PROCEEDINGS, P90, DOI 10.1109/ICNC.2008.253.},
Number-of-Cited-References = {6},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BHJ12},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000325582300024},
DA = {2023-08-12},
}

@article{ WOS:000487728800047,
Author = {Zhang, Jingcheng and He, Yuhang and Yuan, Lin and Liu, Peng and Zhou,
   Xianfeng and Huang, Yanbo},
Title = {Machine Learning-Based Spectral Library for Crop Classification and
   Status Monitoring},
Journal = {AGRONOMY-BASEL},
Year = {2019},
Volume = {9},
Number = {9},
Month = {SEP},
Abstract = {The establishment and application of a spectral library is a critical
   step in the standardization and automation of remote sensing
   interpretation and mapping. Currently, most spectral libraries are
   designed to support the classification of land cover types, whereas few
   are dedicated to agricultural remote sensing monitoring. Here, we
   gathered spectral observation data on plants in multiple experimental
   scenarios into a spectral database to investigate methods for crop
   classification (16 crop species) and status monitoring (tea plant and
   rice growth). We proposed a set of screening methods for spectral
   features related to plant classification and status monitoring (band
   reflectance, vegetation index, spectral differentiation, spectral
   continuum characteristics) that are based on ISODATA and JM distance.
   Next, we investigated the performance of different machine learning
   classifiers in the spectral library application, including K-nearest
   neighbor (KNN), Random Forest (RF), and a genetic algorithm coupled with
   a support vector machine (GA-SVM). The optimal combination of spectral
   features and the classifier with the highest classification accuracy
   were selected for crop classification and status monitoring scenarios.
   The GA-SVM classifier performed the best, which produced an accuracy of
   OAA = 0.94, Kappa = 0.93 for crop classification in a complex scenario
   (crops mixed with 71 non-crop plant species), and promising accuracies
   for tea plant growth monitoring (OAA = 0.98, Kappa = 0.97) and rice
   growth stage monitoring (OAA = 0.92, Kappa = 0.90). Therefore, the
   establishment of a plant spectral library combined with relevant feature
   extraction and a classification algorithm effectively supports
   agricultural monitoring by remote sensing.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Zhou, XF (Corresponding Author), Hangzhou Dianzi Univ, Coll Artificial Intelligence, Hangzhou 310018, Zhejiang, Peoples R China.
   Huang, YB (Corresponding Author), ARS, Crop Prod Syst Res Unit, USDA, POB 350, Stoneville, MS 38776 USA.
   Zhang, Jingcheng; He, Yuhang; Liu, Peng; Zhou, Xianfeng, Hangzhou Dianzi Univ, Coll Artificial Intelligence, Hangzhou 310018, Zhejiang, Peoples R China.
   Yuan, Lin, Zhejiang Univ Water Resources \& Elect Power, Sch Informat Engn \& Art \& Design, Hangzhou 310018, Zhejiang, Peoples R China.
   Huang, Yanbo, ARS, Crop Prod Syst Res Unit, USDA, POB 350, Stoneville, MS 38776 USA.},
DOI = {10.3390/agronomy9090496},
Article-Number = {496},
EISSN = {2073-4395},
Keywords = {hyperspectral; spectral library; machine learning; crop classification;
   status monitoring},
Keywords-Plus = {HYPERSPECTRAL DATA; RANDOM FOREST; WATER INDEX; REFLECTANCE; LEAF;
   VEGETATION; CANOPY; PLANTS; NITROGEN},
Research-Areas = {Agriculture; Plant Sciences},
Web-of-Science-Categories  = {Agronomy; Plant Sciences},
Author-Email = {zhouxianfeng@hdu.edu.cn
   Yanbo.Huang@ARS.USDA.GOV},
Affiliations = {Hangzhou Dianzi University; Zhejiang University of Water Resources and
   Electric Power; United States Department of Agriculture (USDA)},
ResearcherID-Numbers = {he, yh/HJA-5454-2022},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}41671415]; Zhejiang
   public welfare programme of agriculture technology {[}LGN19D010001];
   13th Five-year informatization Plan of Chinese Academy of Sciences
   {[}XXH13505-03-104]},
Funding-Text = {The research presented in this paper was supported by National Natural
   Science Foundation of China (Project No. 41671415), Zhejiang public
   welfare programme of agriculture technology (Project No. LGN19D010001),
   and 13th Five-year informatization Plan of Chinese Academy of Sciences
   (Project No. XXH13505-03-104).},
Cited-References = {Adao T, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111110.
   Backhaus A., 2011, 2011 3 WORKSH HYP IM, P1.
   Baldridge AM, 2009, REMOTE SENS ENVIRON, V113, P711, DOI 10.1016/j.rse.2008.11.007.
   Ball G.H., 1965, ISODATA NOVEL METHOD.
   Baret F., 1991, Remote Sensing of Environment, V35, P161, DOI 10.1016/0034-4257(91)90009-U.
   Belgiu M, 2016, ISPRS J PHOTOGRAMM, V114, P24, DOI 10.1016/j.isprsjprs.2016.01.011.
   Blackburn GA, 1998, REMOTE SENS ENVIRON, V66, P273, DOI 10.1016/S0034-4257(98)00059-5.
   Campos-Taberner M, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8030202.
   Chi MM, 2008, ADV SPACE RES, V41, P1793, DOI 10.1016/j.asr.2008.02.012.
   CONGALTON RG, 1983, PHOTOGRAMM ENG REM S, V49, P69.
   CURRAN PJ, 1989, REMOTE SENS ENVIRON, V30, P271, DOI 10.1016/0034-4257(89)90069-2.
   Datt B, 1999, J PLANT PHYSIOL, V154, P30, DOI 10.1016/S0176-1617(99)80314-9.
   Daughtry CST, 2000, REMOTE SENS ENVIRON, V74, P229, DOI 10.1016/S0034-4257(00)00113-9.
   Delalieux S, 2008, REMOTE SENS ENVIRON, V112, P3762, DOI 10.1016/j.rse.2008.05.003.
   Du MM, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9030289.
   Duarte-Carvajalino JM, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10101513.
   Fensholt R, 2003, REMOTE SENS ENVIRON, V87, P111, DOI 10.1016/j.rse.2003.07.002.
   Galvao LS, 2005, REMOTE SENS ENVIRON, V94, P523, DOI 10.1016/j.rse.2004.11.012.
   GAMON JA, 1992, REMOTE SENS ENVIRON, V41, P35, DOI 10.1016/0034-4257(92)90059-S.
   Gao BC, 1996, REMOTE SENS ENVIRON, V58, P257, DOI 10.1016/S0034-4257(96)00067-3.
   Gitelson AA, 2002, REMOTE SENS ENVIRON, V80, P76, DOI 10.1016/S0034-4257(01)00289-9.
   Gitelson AA, 2005, GEOPHYS RES LETT, V32, DOI 10.1029/2005GL022688.
   Gitelson AA, 2001, PHOTOCHEM PHOTOBIOL, V74, P38, DOI 10.1562/0031-8655(2001)074\&lt;0038:OPANEO\&gt;2.0.CO;2.
   Gitelson AA, 1996, J PLANT PHYSIOL, V148, P494, DOI 10.1016/S0176-1617(96)80284-7.
   HARDISKY MA, 1983, PHOTOGRAMM ENG REM S, V49, P77.
   Hastie T., 2009, ELEMENTS STAT LEARNI, P459, DOI DOI 10.1007/B94608\_13.
   Huang J, 2017, INT J REMOTE SENS, V38, P2273, DOI 10.1080/01431161.2016.1219076.
   Jin XL, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8120972.
   Koppe W, 2010, PHOTOGRAMM FERNERKUN, P167, DOI 10.1127/1432-8364/2010/0047.
   Nagler PL, 2000, REMOTE SENS ENVIRON, V71, P207, DOI 10.1016/S0034-4257(99)00082-6.
   Nidamanuri RR, 2011, BIOSYST ENG, V110, P231, DOI 10.1016/j.biosystemseng.2011.07.002.
   Pal M, 2005, INT J REMOTE SENS, V26, P217, DOI 10.1080/01431160412331269698.
   Penuelas J, 1997, INT J REMOTE SENS, V18, P2869, DOI 10.1080/014311697217396.
   PENUELAS J, 1994, REMOTE SENS ENVIRON, V48, P135, DOI 10.1016/0034-4257(94)90136-8.
   Pu R, 2017, T\&F SER REMOTE SENS, P1, DOI 10.1201/9781315120607.
   Pu R, 2003, INT J REMOTE SENS, V24, P1799, DOI 10.1080/01431160210155965.
   Pu RL, 2011, INT J REMOTE SENS, V32, P2207, DOI 10.1080/01431161003692040.
   QI J, 1994, REMOTE SENS ENVIRON, V48, P119, DOI 10.1016/0034-4257(94)90134-1.
   Rouse J.W., 1973, NASA GODDARD SPACE F.
   Schlerf M, 2005, REMOTE SENS ENVIRON, V95, P177, DOI 10.1016/j.rse.2004.12.016.
   Senthilnath J, 2013, IEEE J-STARS, V6, P861, DOI 10.1109/JSTARS.2012.2217941.
   Serrano L, 2002, REMOTE SENS ENVIRON, V81, P355, DOI 10.1016/S0034-4257(02)00011-1.
   Swain P.H., 1978, IEEE T PATTERN ANAL, V3, P713, DOI {[}10.1109/TPAMI.1981.4767177, DOI 10.1109/TPAMI.1981.4767177].
   Underwood E, 2003, REMOTE SENS ENVIRON, V86, P150, DOI 10.1016/S0034-4257(03)00096-8.
   Underwood EC, 2007, ENVIRON MANAGE, V39, P63, DOI 10.1007/s00267-005-0228-9.
   Vincini M., 2006, 4 ESA CHRIS PROBA WO, P19.
   Wolpert DH, 1996, NEURAL COMPUT, V8, P1341, DOI 10.1162/neco.1996.8.7.1341.
   Yuan HH, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9040309.
   Zarco-Tejada PJ, 2005, REMOTE SENS ENVIRON, V99, P271, DOI 10.1016/j.rse.2005.09.002.},
Number-of-Cited-References = {49},
Times-Cited = {21},
Usage-Count-Last-180-days = {7},
Usage-Count-Since-2013 = {47},
Journal-ISO = {Agronomy-Basel},
Doc-Delivery-Number = {JA3NF},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000487728800047},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000271604900001,
Author = {Feng, Youqian and Zhang, Shanwen},
Editor = {Huang, DS and Jo, KH and Lee, HH and Kang, HJ and Bevilacqua, V},
Title = {Supervised Locally Linear Embedding for Plant Leaf Image Feature
   Extraction},
Booktitle = {EMERGING INTELLIGENT COMPUTING TECHNOLOGY AND APPLICATIONS, PROCEEDINGS},
Series = {Lecture Notes in Computer Science},
Year = {2009},
Volume = {5754},
Pages = {1+},
Note = {5th International Conference on Intelligent Computing, Ulsan, SOUTH
   KOREA, SEP 16-19, 2009},
Organization = {IEEE Computat Intelligence Soc; Int Neural Network Soc; Natl Nat Sci Fdn
   China},
Abstract = {The objects of traditional plant identification were too broad and the
   classification features of it were usually not synthetic and the
   recognition rate was always slightly low. This paper gives one
   recognition approach based on supervised locally linear embedding (LLE)
   and K-nearest neighbors. The recognition results for thirty kinds of
   broad-leaved trees were realized and the average correct recognition
   rate reached 98.3\%. Comparison with other recognition method
   demonstrated the proposed method is effective in advancing the
   recognition rate.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Feng, YQ (Corresponding Author), AF Engn Univ, Inst Sci, Xian 710051, Peoples R China.
   Feng, Youqian, AF Engn Univ, Inst Sci, Xian 710051, Peoples R China.
   Zhang, Shanwen, Chinese Acad Sci, Hefei Inst Intelligent Machines, Intelligent Computat Lab, Hefei 230031, Anhui, Peoples R China.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-642-04069-6},
Keywords = {Plant leaf image; Manifold learning; Supervised locally linear
   embedding; Plant classification},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory \& Methods},
Author-Email = {fengyouqian123@163.com
   zhangshanwen1965@163.com},
Affiliations = {Air Force Engineering University; Chinese Academy of Sciences},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}60805021]},
Funding-Text = {This work was supported by the grant of the National Natural Science
   Foundation of China, No. 60805021.},
Cited-References = {ABBASI S, 1997, P SCAL SPAC 97 C UTR, P284.
   BRENDEL T, 1995, P SPIE, V2345.
   Christianini N., 2000, INTRO SUPPORT VECTOR.
   de Ridder D, 2003, LECT NOTES COMPUT SC, V2714, P333.
   de Ridder D., 2002, PH200201 DELFT U TEC.
   Du JX, 2007, NEUROCOMPUTING, V70, P896, DOI 10.1016/j.neucom.2006.10.026.
   Du JX, 2006, NEUROCOMPUTING, V70, P592, DOI 10.1016/j.neucom.2006.05.003.
   Du JX, 2005, LECT NOTES COMPUT SC, V3497, P281.
   DU JX, 2006, T I MEASUREMENT CONT, V28.
   GOUVEIA F, 1997, P IEEE INT S IND EL.
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797.
   HEYMANS BC, 1991, P IEEE INT JOINT C N.
   Li YF, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P885.
   Mokhtarian F, 2004, IEEE T IMAGE PROCESS, V13, P653, DOI 10.1109/TIP.2004.826126.
   SAITOH TKT, 2000, P 15 INT C PATT REC, V2.
   Vapnik V., 1998, STAT LEARNING THEORY.
   Wan YY, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P482, DOI 10.1109/ISIMP.2004.1434106.
   Wang SL, 2006, LECT NOTES ARTIF INT, V4093, P864.
   Yonekawa S, 1996, T ASAE, V39, P1525, DOI 10.13031/2013.27647.},
Number-of-Cited-References = {19},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BLZ67},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000271604900001},
DA = {2023-08-12},
}

@article{ WOS:000450854400022,
Author = {Zhu, Xiaolong and Zhu, Meng and Ren, Honge},
Title = {Method of plant leaf recognition based on improved deep convolutional
   neural network},
Journal = {COGNITIVE SYSTEMS RESEARCH},
Year = {2018},
Volume = {52},
Pages = {223-233},
Month = {DEC},
Abstract = {The identification of plant species mainly depends on the recognition of
   plant leaf characteristics. However, most recognition systems show the
   weak performance on detecting small objects like plant leaves in the
   complicated background. In order to improve the recognition ability of
   plant leaves in the complex environment, this paper proposes an improved
   deep convolutional neural network, which takes advantage of the
   Inception V2 with batch normalization (BN) instead of convolutional
   neural layers in the faster region convolutional neural network (Faster
   RCNN) offering multiscale image features to the region proposal network
   (RPN). In addition, the original images first are cut into the specified
   size according to the numerical order, and the segmented images are
   loaded into the proposed network sequentially. After the precise
   classification through softmax and bounding box regressor, the segmented
   images with identification labels are spliced together as final output
   images. The experimental results show that the proposed approach has
   higher recognition accuracy than Faster RCNN in recognizing leaf species
   in the complex background. (C) 2018 Elsevier B.V. All rights reserved.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Ren, HE (Corresponding Author), Northeast Forestry Univ, Coll Informat \& Comp Engn, Harbin 150040, Heilongjiang, Peoples R China.
   Ren, Honge, Northeast Forestry Univ, Coll Informat \& Comp Engn, Harbin 150040, Heilongjiang, Peoples R China.
   Forestry Intelligent Equipment Engn Res Ctr, Harbin 150040, Heilongjiang, Peoples R China.},
DOI = {10.1016/j.cogsys.2018.06.008},
ISSN = {2214-4366},
EISSN = {1389-0417},
Keywords = {Deep learning; Convolutional neural network; Leaf recognition; Complex
   background},
Keywords-Plus = {IDENTIFICATION; CLASSIFICATION; FEATURES},
Research-Areas = {Computer Science; Neurosciences \& Neurology; Psychology},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Neurosciences; Psychology,
   Experimental},
Author-Email = {zhuxiaolonglong@sina.cn
   nefu\_rhe@163.com},
Affiliations = {Northeast Forestry University - China},
ORCID-Numbers = {Ren, honge/0000-0002-5334-7636
   Zhu, Xiaolong/0000-0003-3085-5854},
Funding-Acknowledgement = {Fundamental Research Funds for the Central Universities {[}2572017PZ10]},
Funding-Text = {This research was supported by Fundamental Research Funds for the
   Central Universities {[}2572017PZ10]. All the works were conducted at
   Forestry Intelligent Equipment Engineering Research Center.},
Cited-References = {Backes AR, 2009, PATTERN RECOGN, V42, P54, DOI 10.1016/j.patcog.2008.07.006.
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Filonenko A, 2017, C HUM SYST INTERACT, P64, DOI 10.1109/HSI.2017.8004998.
   Girshick R., 2014, PROC IEEE C COMPUT V, P580, DOI DOI 10.1109/CVPR.2014.81.
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI {[}arXiv:1406.4729, 10.1007/978-3-319-10578-9\_23].
   Hegelich S, 2017, COGN SYST RES, V45, P59, DOI 10.1016/j.cogsys.2017.02.006.
   Ioffe S., 2015, INT C MACHINE LEARNI, DOI DOI 10.1007/S13398-014-0173-7.2.
   Jiang HZ, 2017, IEEE INT CONF AUTOMA, P650, DOI {[}10.1109/MWSYM.2017.8058653, 10.1109/FG.2017.82].
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Le THN, 2016, IEEE COMPUT SOC CONF, P46, DOI 10.1109/CVPRW.2016.13.
   LI HX, 2015, PROC CVPR IEEE, P5325, DOI DOI 10.1109/CVPR.2015.7299170.
   Liu Nian, 2016, Journal of Beijing Forestry University, V38, P110.
   Liu Y, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9060522.
   Lu Guan-ming, 2014, Journal of Nanjing University of Posts and Telecommunications, V34, P1.
   Lu Guan-ming, 2013, Journal of Nanjing University of Posts and Telecommunications, V33, P1.
   Mallah C., 2013, ACTA PRESS, V3842, P107.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Niu XX, 2012, PATTERN RECOGN, V45, P1318, DOI 10.1016/j.patcog.2011.09.021.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Roh MC, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P514, DOI 10.23919/MVA.2017.7986913.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Tang JH, 2015, IEEE T MULTIMEDIA, V17, P1899, DOI 10.1109/TMM.2015.2476660.
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5.
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655.
   Wang J, 2017, OPTIK, V149, P469, DOI 10.1016/j.ijleo.2017.09.064.
   Wang LiJun, 2015, Journal of Beijing Forestry University, V37, P55.
   Wang Y, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P346, DOI 10.1109/BigMM.2017.61.
   Wang ZB, 2016, NEURAL COMPUT APPL, V27, P899, DOI 10.1007/s00521-015-1904-1.
   Waris MA, 2017, NEUROCOMPUTING, V266, P631, DOI 10.1016/j.neucom.2017.05.071.
   Xu G, 2017, ENVIRON MODELL SOFTW, V91, P127, DOI 10.1016/j.envsoft.2017.02.004.
   {[}杨天天 Yang Tiantian], 2014, {[}东北林业大学学报, Journal of North-East Forestry University], V42, P75.
   Zaman FK, 2016, INT J AUTOM COMPUT, V13, P319, DOI 10.1007/s11633-016-0974-6.
   {[}张宁 Zhang Ning], 2013, {[}计算机应用, Journal of Computer Applications], V33, P2009.
   Zhao X., 2017, VEH TECHN C, P1.
   {[}郑一力 Zheng Yili], 2017, {[}农业机械学报, Transactions of the Chinese Society for Agricultural Machinery], V48, P30.
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1\_26.},
Number-of-Cited-References = {43},
Times-Cited = {42},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {86},
Journal-ISO = {Cogn. Syst. Res.},
Doc-Delivery-Number = {HB2GA},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000450854400022},
DA = {2023-08-12},
}

@article{ WOS:000649703400004,
Author = {Liu, Maohua and Han, Ziwei and Chen, Yiming and Liu, Zhengjun and Han,
   Yanshun},
Title = {Tree species classification of LiDAR data based on 3D deep learning},
Journal = {MEASUREMENT},
Year = {2021},
Volume = {177},
Month = {JUN},
Abstract = {Accurate tree species identification is essential for ecological
   evaluation and other forest applications. In this paper, we proposed a
   point-based deep neural network called LayerNet. For light detection and
   ranging (LiDAR) data in forest regions, the network can divide multiple
   overlapping layers in Euclidean space to obtain the local
   three-dimensional (3D) structural features of the tree. The features of
   all layers are aggregated, and the global feature is obtained by
   convolution to classify the tree species. To validate the proposed
   framework, multiple experiments, including airborne and ground-based
   LiDAR datasets, are conducted and compared with several existing tree
   species classification algorithms. The test results show that LayerNet
   can directly use 3D data to accurately classify tree species, with the
   highest classification accuracy of 92.5\%. Also, the results of
   comparative experiments demonstrate that the proposed framework has
   obvious advantages in classification accuracy and provides an effective
   solution for tree species classification tasks.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Chen, YM (Corresponding Author), Chinese Acad Surveying \& Mapping, Beijing 100089, Peoples R China.
   Liu, Maohua; Han, Ziwei, Shenyang Jianzhu Univ, Sch Transportat Engn, Shenyang 110168, Peoples R China.
   Chen, Yiming; Liu, Zhengjun; Han, Yanshun, Chinese Acad Surveying \& Mapping, Beijing 100089, Peoples R China.},
DOI = {10.1016/j.measurement.2021.109301},
EarlyAccessDate = {APR 2021},
Article-Number = {109301},
ISSN = {0263-2241},
EISSN = {1873-412X},
Keywords = {LiDAR; Point cloud; 3D deep learning; Tree species classification},
Keywords-Plus = {PROGRESSIVE TIN DENSIFICATION; INDIVIDUAL TREES; CANOPY STRUCTURE;
   AIRBORNE},
Research-Areas = {Engineering; Instruments \& Instrumentation},
Web-of-Science-Categories  = {Engineering, Multidisciplinary; Instruments \& Instrumentation},
Author-Email = {sjzulmh@163.com},
Affiliations = {Shenyang Jianzhu University; Chinese Academy of Surveying \& Mapping},
Funding-Acknowledgement = {National Key Research and Development Program of China
   {[}2018YFB0504504]; National Natural Science Foundation of China
   {[}41730107, 41671414]; Chinese Academy of Surveying and Mapping
   {[}AR1920]},
Funding-Text = {National Key Research and Development Program of China (2018YFB0504504);
   National Natural Science Foundation of China (41730107 and 41671414);
   Funded Project of Fundamental Scientific Research Business Expenses of
   Chinese Academy of Surveying and Mapping (AR1920).},
Cited-References = {{[}Anonymous], MACH LEARN.
   Cao L, 2016, INT J APPL EARTH OBS, V49, P39, DOI 10.1016/j.jag.2016.01.007.
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104.
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964.
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693.
   Guan HY, 2015, REMOTE SENS LETT, V6, P864, DOI 10.1080/2150704X.2015.1088668.
   Hamraz H, 2019, ISPRS J PHOTOGRAMM, V158, P219, DOI 10.1016/j.isprsjprs.2019.10.011.
   Jawak S. D., 2013, ADV REMOTE SENS, V2, P297, DOI DOI 10.4236/ARS.2013.24033.
   Jayachitra S, 2021, J CIRCUIT SYST COMP, V30, DOI 10.1142/S0218126621501784.
   Jones TG, 2010, REMOTE SENS ENVIRON, V114, P2841, DOI 10.1016/j.rse.2010.07.002.
   Kirby KR, 2007, FOREST ECOL MANAG, V246, P208, DOI 10.1016/j.foreco.2007.03.072.
   Lefsky MA, 1999, REMOTE SENS ENVIRON, V70, P339, DOI 10.1016/S0034-4257(99)00052-8.
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979.
   Li WK, 2012, PHOTOGRAMM ENG REM S, V78, P75, DOI 10.14358/PERS.78.1.75.
   Li Y., ARXIV180107791V5.
   Li YY, 2016, ADV NEUR IN, V29.
   Lin Y, 2016, INT J APPL EARTH OBS, V46, P45, DOI 10.1016/j.jag.2015.11.010.
   Lovell JL, 2003, CAN J REMOTE SENS, V29, P607, DOI 10.5589/m03-026.
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481.
   Moudry V, 2020, MEASUREMENT, V150, DOI 10.1016/j.measurement.2019.107047.
   Nie S, 2017, MEASUREMENT, V104, P70, DOI 10.1016/j.measurement.2017.03.007.
   Plourde LC, 2007, PHOTOGRAMM ENG REM S, V73, P829, DOI 10.14358/PERS.73.7.829.
   Polat N, 2015, MEASUREMENT, V63, P61, DOI 10.1016/j.measurement.2014.12.017.
   Qi CR, 2017, ADV NEUR IN, V30.
   Chen Q, 2006, PHOTOGRAMM ENG REM S, V72, P923, DOI 10.14358/PERS.72.8.923.
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0.
   Sanloglu I., 2018, POINT CLOUD FILTERIN.
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802.
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114.
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608.
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801.
   Zhao XQ, 2016, ISPRS J PHOTOGRAMM, V117, P79, DOI 10.1016/j.isprsjprs.2016.03.016.
   Zou XH, 2017, IEEE GEOSCI REMOTE S, V14, P2360, DOI 10.1109/LGRS.2017.2764938.},
Number-of-Cited-References = {33},
Times-Cited = {18},
Usage-Count-Last-180-days = {20},
Usage-Count-Since-2013 = {80},
Journal-ISO = {Measurement},
Doc-Delivery-Number = {SB0OU},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000649703400004},
DA = {2023-08-12},
}

@article{ WOS:A1986G375200001,
Author = {GUYER, DE and MILES, GE and SCHREIBER, MM and MITCHELL, OR and
   VANDERBILT, VC},
Title = {MACHINE VISION AND IMAGE-PROCESSING FOR PLANT-IDENTIFICATION},
Journal = {TRANSACTIONS OF THE ASAE},
Year = {1986},
Volume = {29},
Number = {6},
Pages = {1500-1507},
Month = {NOV-DEC},
Publisher = {AMER SOC AGRICULTURAL ENGINEERS},
Address = {2950 NILES RD, ST JOSEPH, MI 49085-9659},
Type = {Article},
Language = {English},
Affiliation = {GUYER, DE (Corresponding Author), PURDUE UNIV,DEPT AGR ENGN,W LAFAYETTE,IN 47907, USA.
   PURDUE UNIV,DEPT BOT \& PLANT PATHOL,W LAFAYETTE,IN 47907.
   PURDUE UNIV,SCH ELECT ENGN,W LAFAYETTE,IN 47907.
   PURDUE UNIV,APPLICAT REMOTE SENSING LAB,W LAFAYETTE,IN 47907.},
ISSN = {0001-2351},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agricultural Engineering},
Affiliations = {Purdue University System; Purdue University; Purdue University West
   Lafayette Campus; Purdue University System; Purdue University; Purdue
   University West Lafayette Campus; Purdue University System; Purdue
   University; Purdue University West Lafayette Campus},
Cited-References = {{[}Anonymous], 1978, REMOTE SENSING QUANT.
   FUKANANGA K, 1972, INTRO STATISTICAL PA.
   GUYER DE, 1984, THESIS PURDUE U LIBR.
   GUYER DE, 1984, ASAE841632 PAP.
   GUYER DE, 1985, PUBLICATION ASAE, V185, P156.
   ROSENFELD A, 1982, DIGITAL PICTURE PROC, V2.
   WHITTAKER AD, 1984, ASAE845510 PAP.},
Number-of-Cited-References = {7},
Times-Cited = {105},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {9},
Journal-ISO = {Trans. ASAE},
Doc-Delivery-Number = {G3752},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:A1986G375200001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000443112000006,
Author = {Tao, Liu and Cheng, Cai},
Editor = {Aiello, M and Yang, Y and Zou, Y and Zhang, LJ},
Title = {Plant Identification Based on Image Set Analysis},
Booktitle = {ARTIFICIAL INTELLIGENCE AND MOBILE SERVICES - AIMS 2018},
Series = {Lecture Notes in Computer Science},
Year = {2018},
Volume = {10970},
Pages = {69-82},
Note = {7th International Conference on Artificial Intelligence and Mobile
   Services (AIMS) Held as Part of the Services Conference Federation
   (SCF), Seattle, WA, JUN 25-30, 2018},
Abstract = {Plant identification is crucial in plant protection, crop breeding and
   agriculture research area. With the development of information
   technology, computer vision-based plant identification is an effective
   and efficient solution. Many tradition plant identification algorithms
   utilize only one type of plant images, such as images from leaves,
   flowers and stems etc., which may bring misclassifications. In order to
   recognize plant accurately, we propose a new plant identification scheme
   based on image set analysis. In this method, uses image set as a
   taxonomic unit, each image set consists of multiple images (whole plant,
   fruits, leaves, flowers, stems, branches, leaves scan), the
   characteristics of plants are fused together as a basis for plant
   identification. Compared to the traditional single feature, this study
   has more characteristics and provides more information in the
   classification process. A face recognition algorithm based on image
   collection is used in our research, for example: AHISD/CHISD
   (Affine/Convex Hull based Image Set Distance), PLRC (Pairwise Linear
   Regression Classification), SSDML (Set-to-Set Distance Metric Learning),
   SANP (Sparse Approximated Nearest Point), RNP (Regularized Nearest
   Points). Data set contains 64,150 plant images, it can be divided into
   369 training set (each training set contains 50 pictures) and 914 test
   sets (each test set contains 50 picture). The results show that CHISD
   has the highest recognition rate, at 77.02\%, which is more suitable for
   requiring higher accuracy. RNP identifies a plant class took an average
   of 0.75 s, the algorithm is more suitable for the occasion with higher
   time requirement. Therefore, the plant identification based on image set
   is feasible and low cost, which can be extended to agricultural
   production.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Cheng, C (Corresponding Author), Northwest A\&F Univ, Coll Engn Informat Engn, Yangling 712100, Shaanxi, Peoples R China.
   Tao, Liu; Cheng, Cai, Northwest A\&F Univ, Coll Engn Informat Engn, Yangling 712100, Shaanxi, Peoples R China.},
DOI = {10.1007/978-3-319-94361-9\_6},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-94361-9; 978-3-319-94360-2},
Keywords = {Plant identification; Image set; Feature extraction},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods; Telecommunications},
Author-Email = {chengcai@nwsuaf.edu.cn},
Affiliations = {Northwest A\&F University - China},
Cited-References = {Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244.
   Carranza-Rojas Jose, 2016, CLEIej, V19, P7.
   CEVIKALP H, 2010, PROC CVPR IEEE, P2567, DOI DOI 10.1109/CVPR.2010.5539965.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   CHEN L, 2014, PROC CVPR IEEE, P2673, DOI DOI 10.1109/CVPR.2014.342.
   Chi ZR, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS \& SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P1035.
   Clarke G., 2016, STATE WORLDS PLANTS.
   Feng QX, 2016, PROC CVPR IEEE, P4865, DOI 10.1109/CVPR.2016.526.
   Goeau H, 2015, LIFECLEF PLANT IDENT.
   Gu X, 2005, LECT NOTES COMPUT SC, V3644, P253.
   Harandi MT, 2014, LECT NOTES COMPUT SC, V8690, P17, DOI 10.1007/978-3-319-10605-2\_2.
   Hayat M, 2015, IEEE T PATTERN ANAL, V37, P713, DOI 10.1109/TPAMI.2014.2353635.
   Herdiyeni Y, 2013, INT C ADV COMP SCI I, P353, DOI 10.1109/ICACSIS.2013.6761601.
   Hu Y, 2011, PROC CVPR IEEE, P121, DOI 10.1109/CVPR.2011.5995500.
   Lu JW, 2015, PROC CVPR IEEE, P1137, DOI 10.1109/CVPR.2015.7298717.
   LUNAU K, 1991, ETHOLOGY, V88, P203, DOI 10.1111/j.1439-0310.1991.tb00275.x.
   Nguyen Q. K., 2014, INT C ADV TECHN COMM, P404.
   Saitoh T, 2000, INT C PATT RECOG, P507, DOI 10.1109/ICPR.2000.906123.
   Seng WC, 2009, 2009 INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING AND INFORMATICS, VOLS 1 AND 2, P125, DOI 10.1109/ICEEI.2009.5254804.
   Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965.
   Wang W, 2015, PROC CVPR IEEE, P2048, DOI 10.1109/CVPR.2015.7298816.
   Wu S. G., 2008, IEEE INT S SIGN PROC, P11.
   Yang P., 2013, P IEEE INT C AUT FAC, P1, DOI DOI 10.1109/FG.2013.6553727.
   Zhang ZP, 2016, LECT NOTES COMPUT SC, V9907, P236, DOI 10.1007/978-3-319-46487-9\_15.
   Zhu PF, 2013, IEEE I CONF COMP VIS, P2664, DOI 10.1109/ICCV.2013.331.},
Number-of-Cited-References = {25},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BK8MD},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000443112000006},
DA = {2023-08-12},
}

@inproceedings{ WOS:000371977801052,
Author = {Zhao, Zhong-Qiu and Hong, Yan and Zheng, Peng and Wu, Xindong},
Book-Group-Author = {IEEE},
Title = {PLANT IDENTIFICATION USING TRIANGULAR REPRESENTATION BASED ON SALIENT
   POINTS AND MARGIN POINTS},
Booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
Series = {IEEE International Conference on Image Processing ICIP},
Year = {2015},
Pages = {1145-1149},
Note = {IEEE International Conference on Image Processing (ICIP), Quebec City,
   CANADA, SEP 27-30, 2015},
Organization = {Inst Elect \& Elect Engineers; IEEE Signal Proc Soc},
Abstract = {Leaf classification is an important component of living plant
   identification. A leaf contains important information for plant species
   identification in spite of its complexity. This paper introduces a
   method of recognizing leaf images based on triangular representations. A
   leaf is represented by local descriptors associated with margin sample
   points and salient sample points. We introduce three new triangular
   representations - salient triangle area representation (STAR), salient
   triangle side lengths representation (STSL), and salient triangle area,
   side lengths and two angles representation (STASLA), and then we combine
   two local descriptors - one provides a triangular representation of the
   leaf margin while the other represents the spatial correlation between
   salient points of the leaf and leaf margin. Experiments on the
   Image-CLEF 2011 leaf datasets show the effectiveness and the efficiency
   of the proposed method.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zhao, ZQ (Corresponding Author), Hefei Univ Technol, Coll Comp Sci \& Informat Engn, Hefei, Peoples R China.
   Zhao, Zhong-Qiu; Hong, Yan; Zheng, Peng; Wu, Xindong, Hefei Univ Technol, Coll Comp Sci \& Informat Engn, Hefei, Peoples R China.
   Wu, Xindong, Univ Vermont, Dept Comp Sci, Burlington, VT 05405 USA.},
ISSN = {1522-4880},
ISBN = {978-1-4799-8339-1},
Keywords = {Plant recognition; Feature extraction; Triangular representations; Local
   descriptors},
Research-Areas = {Engineering; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Imaging Science \& Photographic
   Technology},
Affiliations = {Hefei University of Technology; University of Vermont},
ResearcherID-Numbers = {Wu, Xindong/AAB-6713-2022},
ORCID-Numbers = {Wu, Xindong/0000-0003-2396-1704},
Cited-References = {Alajlan N, 2008, IEEE T PATTERN ANAL, V30, P1003, DOI 10.1109/TPAMI.2008.37.
   Alajlan N, 2007, PATTERN RECOGN, V40, P1911, DOI 10.1016/j.patcog.2006.12.005.
   {[}Anonymous], 2009, ICCV.
   Aranda M. C., 2010, P ACM INT C IM VID R, P327, DOI {[}10.1145/1816041.1816089, DOI 10.1145/1816041.1816089].
   Backes AR, 2009, PATTERN RECOGN, V42, P54, DOI 10.1016/j.patcog.2008.07.006.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Cerutti Guillaume, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P202.
   Davis J. V., 2007, ICML.
   Goeau H., 2011, P 19 INT C MULT SCOT, DOI {[}10.1145/2072298.2072472, DOI 10.1145/2072298.2072472].
   Goeau H., 2011, CLEF NOTEBOOK PAPERS.
   Joly A., 2008, 16 ACM INT C MULT.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Mingqiang Y., 2008, TECHNOLOGY APPL, P978.
   Mokhtarian F, 2004, IEEE T IMAGE PROCESS, V13, P653, DOI 10.1109/TIP.2004.826126.
   Mouine S., 2013, ICMR.
   Mouine S., 2012, ICMR, P1, DOI DOI 10.1145/2324796.2324853.
   Mouine S, 2013, IEEE IMAGE PROC, P1466, DOI 10.1109/ICIP.2013.6738301.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Pauleve L, 2010, PATTERN RECOGN LETT, V31, P1348, DOI 10.1016/j.patrec.2010.04.004.
   Wang XY, 2010, COMPUT ELECTR ENG, V36, P31, DOI 10.1016/j.compeleceng.2009.04.005.
   Zulkifli Z, 2011, HYBRID INTELLIGENT S.},
Number-of-Cited-References = {21},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BE4NP},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000371977801052},
DA = {2023-08-12},
}

@inproceedings{ WOS:000380502000129,
Author = {Hu, Shuaiqi and Li, Ke and Bao, Xudong},
Editor = {Liang, J and Zeng, B and Wang, L and Shao, L and Hui, X and Tao, Z and Lin, S},
Title = {Wood Species Recognition Based on SIFT Keypoint Histogram},
Booktitle = {2015 8TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP)},
Year = {2015},
Pages = {702-706},
Note = {Proceedings 2015 8 International Congress on Image and Signal Processing
   (CISP), Shenyang, PEOPLES R CHINA, OCT 14-16, 2015},
Abstract = {Traditionally, only experts who are equipped with professional knowledge
   and rich experience are able to recognize different species of wood.
   Applying image processing techniques for wood species recognition can
   not only reduce the expense to train qualified identifiers, but also
   increase the recognition accuracy. In this paper, a wood species
   recognition technique base on Scale Invariant Feature Transformation
   (SIFT) keypoint histogram is proposed. We use first the SIFT algorithm
   to extract keypoints from wood cross section images, and then k-means
   and k-means++ algorithms are used for clustering. Using the clustering
   results, an SIFT keypoints histogram is calculated for each wood image.
   Furthermore, several classification models, including Artificial Neural
   Networks (ANN), Support Vector Machine (SVM) and K-Nearest Neighbor
   (KNN) are used to verify the performance of the method. Finally, through
   comparing with other prevalent wood recognition methods such as GLCM and
   LBP, results show that our scheme achieves higher accuracy.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Hu, SQ (Corresponding Author), Southeast Univ, Sch Informat Sci \& Engn, Nanjing, Jiangsu, Peoples R China.
   Hu, Shuaiqi; Li, Ke, Southeast Univ, Sch Informat Sci \& Engn, Nanjing, Jiangsu, Peoples R China.
   Bao, Xudong, Southeast Univ, Lab Image Sci \& Technol, Nanjing, Jiangsu, Peoples R China.},
ISBN = {978-1-4673-9098-9},
Keywords = {SIFT keypoint histogram; wood; pattern recognition},
Keywords-Plus = {KERNEL},
Research-Areas = {Engineering; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Imaging Science \& Photographic
   Technology},
Affiliations = {Southeast University - China; Southeast University - China},
Cited-References = {ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209.
   {[}Anonymous], 2010, 2010 2 INT C FUTURE.
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027.
   AURENHAMMER F, 1991, COMPUT SURV, V23, P345, DOI 10.1145/116873.116880.
   Azlin Ahmad, 2013, 2 INT C ADV COMP SCI.
   Foo SY, 2002, ENG APPL ARTIF INTEL, V15, P253, DOI 10.1016/S0952-1976(02)00041-6.
   Hasan A.F., 2013, INT J SCI ENG RES, V4, P50.
   Haugen David E., 2014, MICHIGAN TIMBER IND, P1.
   Huang CL, 1998, MACH VISION APPL, V10, P292, DOI 10.1007/s001380050080.
   Jing Yi Tou, 2009, Proceedings of the 2009 Fifth International Conference on Natural Computation (ICNC 2009), P8, DOI 10.1109/ICNC.2009.594.
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489.
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   MacKay D. J., 2003, OTHERS INFORM THEORY.
   Nasirzadeh M, 2010, 2010 SECOND INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE, COMMUNICATION SYSTEMS AND NETWORKS (CICSYN), P308, DOI 10.1109/CICSyN.2010.27.
   Pontil M, 1998, IEEE T PATTERN ANAL, V20, P637, DOI 10.1109/34.683777.
   Roe B, 2014, FOREST CHRON, V90, P651, DOI 10.5558/tfc2014-130.
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742.
   Wan E A, 1990, IEEE Trans Neural Netw, V1, P303, DOI 10.1109/72.80269.
   Wang JP, 2011, MACH VISION APPL, V22, P87, DOI 10.1007/s00138-009-0197-8.
   Waring CA, 2005, IEEE T SYST MAN CY B, V35, P467, DOI 10.1109/TSMCB.2005.846655.
   YUSOF R, 2013, SIGN IM TECHN INT BA, P737.
   Yusof R, 2013, COMPUT ELECTRON AGR, V93, P68, DOI 10.1016/j.compag.2013.01.007.},
Number-of-Cited-References = {23},
Times-Cited = {13},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {9},
Doc-Delivery-Number = {BF2US},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000380502000129},
OA = {Green Submitted},
DA = {2023-08-12},
}

@article{ WOS:000452103500001,
Author = {Franklin, Steven E.},
Title = {Pixel- and object-based multispectral classification of forest tree
   species from small unmanned aerial vehicles},
Journal = {JOURNAL OF UNMANNED VEHICLE SYSTEMS},
Year = {2018},
Volume = {6},
Number = {4},
Pages = {195-211},
Month = {DEC},
Abstract = {Forest inventory, monitoring, and assessment requires accurate tree
   species identification and mapping. Recent experiences with
   multispectral data from small fixed-wing and rotary blade unmanned
   aerial vehicles (UAVs) suggest a role for this technology in the
   emerging paradigm of enhanced forest inventory (EFI). In this paper,
   pixel-based and object-based image analysis (OBIA) methods were compared
   in UAV-based tree species classification of nine commercial tree species
   in mature eastern Ontario mixedwood forests. Unsupervised clustering and
   supervised classification of tree crown pixels yielded approximately
   50\%-60\% classification accuracy overall; OBIA with image segmentation
   to delineate tree crowns and machine learning yielded up to 80\%
   classification accuracy overall. Spectral response patterns and tree
   crown shape and geometric differences were interpreted in context of
   their ability to separate tree species of interest with these
   classification methods. Accuracy assessment was based on field-based
   forest inventory tree species identification. The paper provides a brief
   summary of future research issues that will influence the growth of this
   geomatics innovation in forest tree species classification and forest
   inventory.},
Publisher = {CANADIAN SCIENCE PUBLISHING},
Address = {65 AURIGA DR, SUITE 203, OTTAWA, ON K2E 7W6, CANADA},
Type = {Article},
Language = {English},
Affiliation = {Franklin, SE (Corresponding Author), Trent Univ, Sch Environm, Peterborough, ON K9J 7B8, Canada.
   Franklin, Steven E., Trent Univ, Sch Environm, Peterborough, ON K9J 7B8, Canada.},
DOI = {10.1139/juvs-2017-0022},
ISSN = {2291-3467},
Keywords = {forest inventory; tree species classification; multispectral imagery},
Keywords-Plus = {AIRCRAFT SYSTEMS UASS; UAV; PHOTOGRAMMETRY; COLOR; PHOTOGRAPHY;
   ENVIRONMENT; PHENOLOGY; IMAGERY; DRONES; COVER},
Research-Areas = {Remote Sensing},
Web-of-Science-Categories  = {Remote Sensing},
Author-Email = {sfranklin@trentu.ca},
Affiliations = {Trent University},
Funding-Acknowledgement = {Natural Sciences and Engineering Research Council of Canada},
Funding-Text = {Comments by anonymous reviews and the Associate Editor were helpful in
   improving the manuscript. I thank O. Ahmed and G. Williams for technical
   assistance, and ING Robotics and droneMetrics for image acquisition
   services. Financial support was provided by the Natural Sciences and
   Engineering Research Council of Canada.},
Cited-References = {Ahmed OS, 2017, INT J REMOTE SENS, V38, P2037, DOI 10.1080/01431161.2017.1294781.
   Akkaynak D, 2014, J OPT SOC AM A, V31, P312, DOI 10.1364/JOSAA.31.000312.
   ALDRED AH, 1975, FOREST CHRON, V51, P9, DOI 10.5558/tfc51009-1.
   Anderson K, 2016, PROG PHYS GEOG, V40, P187, DOI 10.1177/0309133316639175.
   {[}Anonymous], CANADIAN J REMOTE SE.
   {[}Anonymous], 2016, ENV APPL REMOTE SENS, DOI DOI 10.5772/62122.
   Avery T.E., 1969, AGR HDB, V308.
   Baillie S, 2015, P ASPRS TECHN S DEM, P1.
   Belgiu M, 2016, ISPRS J PHOTOGRAMM, V114, P24, DOI 10.1016/j.isprsjprs.2016.01.011.
   Blaschke T, 2010, ISPRS J PHOTOGRAMM, V65, P2, DOI 10.1016/j.isprsjprs.2009.06.004.
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324.
   Morellato LPC, 2016, BIOL CONSERV, V195, P60, DOI 10.1016/j.biocon.2015.12.033.
   Chianucci F, 2016, INT J APPL EARTH OBS, V47, P60, DOI 10.1016/j.jag.2015.12.005.
   Colomina I, 2014, ISPRS J PHOTOGRAMM, V92, P79, DOI 10.1016/j.isprsjprs.2014.02.013.
   Congalton R. G., 2019, ASSESSING ACCURACY R.
   Dandois JP, 2015, REMOTE SENS-BASEL, V7, P13895, DOI 10.3390/rs71013895.
   Ecological Stratification Working Group, 1996, NATL ECOLOGICAL FRAM.
   Elarab M, 2015, INT J APPL EARTH OBS, V43, P32, DOI 10.1016/j.jag.2015.03.017.
   Franklin S. E, 1991, CAN J REMOTE SENS, V17, P314, DOI {[}10.1080/07038992.1991.10855300, DOI 10.1080/07038992.1991.10855300.].
   Franklin SE, 2001, PHOTOGRAMM ENG REM S, V67, P849.
   Franklin SE, 2018, INT J REMOTE SENS, V39, P5236, DOI 10.1080/01431161.2017.1363442.
   Franklin SE, 2017, PHOTOGRAMM ENG REM S, V83, P501, DOI 10.14358/PERS.83.7.501.
   Freemantle J. R., 1992, P 15 CAN S REM SENS, P452.
   Gini R, 2014, EUR J REMOTE SENS, V47, P251, DOI 10.5721/EuJRS20144716.
   Gomez C, 2015, UNPUB.
   Goodbody TRH, 2017, INT J REMOTE SENS, V38, P2938, DOI 10.1080/01431161.2016.1219425.
   Goodbody TRH, 2017, FOREST CHRON, V93, P71, DOI 10.5558/tfc2017-012.
   Greatwood C, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17061189.
   Hall RJ, 2003, REMOTE SENSING OF FOREST ENVIRONMENTS: CONCEPTS AND CASE STUDIES, P47.
   Heller R. C, 1964, AGR HDB, V261.
   Hershey R., 1995, GEN TECHNICAL REPORT.
   Hunt E. R, 2014, IEEE J-STARS, V7, P4566, DOI {[}10.1109/ISTARS.2014.2317876, DOI 10.1109/ISTARS.2014.2317876].
   Husson E, 2014, WATER AIR SOIL POLL, V225, DOI 10.1007/s11270-014-1957-2.
   Kuzmin A, 2016, EUR J REMOTE SENS, V49, P239, DOI 10.5721/EuJRS20164914.
   Leboeuf A., 2015, PHOTOGRAPHIC INTERPR.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lee H., 2009, SO ONTARIO AIR PHOTO.
   Lisein J, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0141006.
   Loelkes G. L, 1983, USGS B, V1600.
   Luo C, 2014, SCIENCE, V344, P1351, DOI 10.1126/science.344.6190.1351-b.
   Ma L, 2015, ISPRS J PHOTOGRAMM, V102, P14, DOI 10.1016/j.isprsjprs.2014.12.026.
   McGwire KC, 2013, INT J REMOTE SENS, V34, P1615, DOI 10.1080/01431161.2012.723836.
   Michez A, 2016, ENVIRON MONIT ASSESS, V188, DOI 10.1007/s10661-015-4996-2.
   Milton E. J., 1997, CANADIAN J REMOTE SE, V23, P252.
   Morgan JL, 2010, BIOSCIENCE, V60, P47, DOI 10.1525/bio.2010.60.1.9.
   Mucherino A, 2009, SPRINGER SER OPTIM A, V34, P83, DOI 10.1007/978-0-387-88615-2\_4.
   Nasi R, 2015, REMOTE SENS-BASEL, V7, P15467, DOI 10.3390/rs71115467.
   Nevalainen O, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9030185.
   Nijland W, 2014, AGR FOREST METEOROL, V184, P98, DOI 10.1016/j.agrformet.2013.09.007.
   Ontario Ministry of Natural Resources and Forests (OMNRF), 2009, FOR RES INV TECHN SP.
   Pajares G, 2015, PHOTOGRAMM ENG REM S, V81, P281, DOI 10.14358/PERS.81.4.281.
   Paneque-Galvez J, 2014, FORESTS, V5, P1481, DOI 10.3390/f5061481.
   PARRY JT, 1969, PHOTOGRAMM ENG, V35, P669.
   Sayn-Wittgenstein L, 1978, FMRX118 FOR MAN I CA.
   Sayn-Wittgenstein L., 1961, PHOTOGRAMMETRIC ENG, V27, P792.
   Shahbazi M, 2014, GISCI REMOTE SENS, V51, P339, DOI 10.1080/15481603.2014.926650.
   Singh M, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121558.
   Smith GM, 1999, INT J REMOTE SENS, V20, P2653, DOI 10.1080/014311699211994.
   Smith MW, 2016, PROG PHYS GEOG, V40, P247, DOI 10.1177/0309133315615805.
   St-Onge B, 2015, FORESTS, V6, P3899, DOI 10.3390/f6113899.
   Tang LN, 2015, J FORESTRY RES, V26, P791, DOI 10.1007/s11676-015-0088-y.
   Thomas OH, 2017, PHOTOGRAMM ENG REM S, V83, P581, DOI {[}10.14358/PERS.83.8.581, 10.14358/pers.83.8.581].
   Thompson ID, 2007, FOREST ECOL MANAG, V252, P208, DOI 10.1016/j.foreco.2007.06.033.
   Thorley G. W, 1968, MANUAL COLOR AERIAL, P393.
   Torresan C, 2017, INT J REMOTE SENS, V38, P2427, DOI 10.1080/01431161.2016.1252477.
   Von Blyenburgh P, 2015, REMOTELY PILOTED AIR.
   von Bueren SK, 2015, BIOGEOSCIENCES, V12, P163, DOI 10.5194/bg-12-163-2015.
   Wegner JD, 2016, PROC CVPR IEEE, P6014, DOI 10.1109/CVPR.2016.647.
   White JC, 2016, CAN J REMOTE SENS, V42, P619, DOI 10.1080/07038992.2016.1207484.
   Whitehead K, 2014, J UNMANNED VEH SYST, V2, P86, DOI 10.1139/juvs-2014-0007.
   Whitehead K, 2014, J UNMANNED VEH SYST, V2, P69, DOI 10.1139/juvs-2014-0006.
   Wulder M. A, 1996, 2 INT AIRB REM SENS, VIII, P53.
   Zahawi RA, 2015, BIOL CONSERV, V186, P287, DOI 10.1016/j.biocon.2015.03.031.
   Zarco-Tejada PJ, 2014, EUR J AGRON, V55, P89, DOI 10.1016/j.eja.2014.01.004.
   Zhang J, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8030257.
   Zsilinszky V., 1964, PHOTOGRAMMETRIA, V5, P1.
   Zsilinszky V., 1966, PHOTOGRAPHIC INTERPR.},
Number-of-Cited-References = {77},
Times-Cited = {31},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {60},
Journal-ISO = {J. Unmanned Veh. Syst.},
Doc-Delivery-Number = {HC9CR},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000452103500001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000303357300058,
Author = {Nasirzadeh, M. and Khazael, A. Arab and bin Khalid, Marzuki},
Editor = {AlDabass, D and Pantelous, A and Orsoni, A and Abraham, A},
Title = {Woods Recognition System Based on Local Binary Pattern},
Booktitle = {2010 SECOND INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE,
   COMMUNICATION SYSTEMS AND NETWORKS (CICSYN)},
Year = {2010},
Pages = {308-313},
Note = {2nd International Conference on Computational Intelligence Communication
   Systems and Networks (CICSyN), Liverpool, ENGLAND, JUL 28-30, 2010},
Organization = {IEEE; IEEE Comp Soc; UKSim; Univ Liverpool; Asia Modelling \& Simulat
   Soc (AMSS); UK Simulat Soc; IEEE UK \& RI Comp Chapter; European Simulat
   Council (EUROSIM); European Council Modelling \& Simulat (ECMS); Univ
   Technol Malaysia (UTM); Univ Sci Malaysia (USM); Univ Malaysia Sabah
   (UMS); Univ Technol Mara (UiTM); Machine Intelligence Res Labs (MIR
   Labs); Norway Univ Sci \& Technol; Nottingham Trent Univ},
Abstract = {Malaysia is the largest exporter of tropical woods in the world,
   accounting for 70 percent of the world's supply of raw-logs. Sabah and
   Sarawak, the two Malaysian states on the island of Borneo, occupies some
   of the oldest and the most diverse rain forest in the world. Malaysia
   has a rich variety of tree species, and the wood produced from each of
   these has unique structure, physical and mechanical properties. The
   differences in woods structure and properties allow for the manufacture
   of woods based products with many different appearances and uses. In
   order to use this precious material efficiently, proper species must be
   used in the appropriate places. Intelligent Woods species recognition is
   a new application studied in the Computer Vision field to help prevent
   misclassifying of woods species in woods industries. Woods recognition
   is an implementation on identifying the different species of woods
   provided with the images captured for the woods samples or the
   characteristics observed. In this study, the features from the enhanced
   woods images are extracted using the LBP histogram, which determines the
   classification between the various woods species. The recognition is
   performed using a nearest neighbor classifier in the computed feature
   space with Chi square as a dissimilarity measure. The intelligent woods
   recognition system is designed to explore the possibility of developing
   a system which is able to perform automated woods recognition based on
   woods anatomy. The result thus obtained shows a high rate of recognition
   accuracy proving that the techniques due to its rotation invariance and
   robustness to gray-scale variations are very promising for practical
   applications.},
Publisher = {IEEE COMPUTER SOC},
Address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Nasirzadeh, M (Corresponding Author), Univ Technol Malaysia, Dept Elect Engn, Skudai, Malaysia.
   Nasirzadeh, M.; Khazael, A. Arab; bin Khalid, Marzuki, Univ Technol Malaysia, Dept Elect Engn, Skudai, Malaysia.},
DOI = {10.1109/CICSyN.2010.27},
ISBN = {978-0-7695-4158-7},
Keywords = {image processing; local binary pattern; woods recognition systems;
   nearest neighborhood; Histogram Fourier transforms},
Keywords-Plus = {INVARIANT TEXTURE CLASSIFICATION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Engineering, Electrical \& Electronic},
Author-Email = {maryamnasirzade@yahoo.com
   ali\_khazaell@yahoo.com
   marzuki@utm.my},
Affiliations = {Universiti Teknologi Malaysia},
Cited-References = {Ahonen T., ROTATION INVARIANT I.
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244.
   CROSS GR, 1983, IEEE T PATTERN ANAL, V5, P25, DOI 10.1109/TPAMI.1983.4767341.
   Guo Z., 2009, ROTATION INVARIANT T.
   He L.H., P 2005 IEEE ENG.
   Jafari-Khouzani K, 2005, IEEE T PATTERN ANAL, V27, P1004, DOI 10.1109/TPAMI.2005.126.
   Khalid M, 2008, INT J SIMUL SYST SCI, V9, P9.
   Lahdenoja O, 2005, IEEE IMAGE PROC, P1405.
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Pietikainen M, 2000, PATTERN RECOGN, V33, P43, DOI 10.1016/S0031-3203(99)00032-1.
   Samal A, 2006, INFORM SCIENCES, V176, P565, DOI 10.1016/j.ins.2004.09.017.
   Shih FY, 2004, INFORM SCIENCES, V158, P117, DOI 10.1016/j.ins.2003.03.002.
   Zhou H, 2008, INFORM SCIENCES, V178, P4314, DOI 10.1016/j.ins.2008.07.015.},
Number-of-Cited-References = {14},
Times-Cited = {23},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {11},
Doc-Delivery-Number = {BZY28},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000303357300058},
DA = {2023-08-12},
}

@inproceedings{ WOS:000374793800007,
Author = {Backes, Andre R. and de Mesquita Sa Junior, Jarbas Joaci and Kolb,
   Rosana Marta},
Editor = {Pardo, A and Kittler, J},
Title = {Color Fractal Descriptors for Adaxial Epidermis Texture Classification},
Booktitle = {PROGRESS IN PATTERN RECOGNITION, IMAGE ANALYSIS, COMPUTER VISION, AND
   APPLICATIONS, CIARP 2015},
Series = {Lecture Notes in Computer Science},
Year = {2015},
Volume = {9423},
Pages = {51-58},
Note = {20th Iberoamerican Congress on Pattern Recognition (CIARP), Montevideo,
   URUGUAY, NOV 09-12, 2015},
Organization = {IAPR; Uruguayan IAPR Chapter; Argentine Soc Pattern Recognit; Special
   Interest Grp Brazilian Comp Soc; Chilean Assoc Pattern Recognit; Cuban
   Assoc Pattern Recognit; Mexican Assoc Comp Vis, Neural Comp \& Robot;
   Spanish Assoc Pattern Recognit \& Image Anal; Portuguese Assoc Pattern
   Recognit},
Abstract = {The leaves are an important plant organ and source of information for
   the traditional plant taxonomy. This study proposes a plant
   classification approach using the adaxial epidermis tissue, a specific
   cell layer that covers the leaf. To accomplish this task, we apply a
   high discriminative color texture analysis method based on the
   Bouligand-Minkowski fractal dimension. In an experimental comparison,
   the success rate obtained by our proposed approach (96.66\%) was the
   highest among all the methods used, demonstrating that the
   Bouligand-Minkowski method is very suitable to extract discriminant
   features from the adaxial epidermis. Thus, this research can
   significantly contribute with other studies on plant classification by
   using computer vision.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Backes, AR (Corresponding Author), Univ Fed Uberlandia, Fac Comp, Av Joao Naves de Avila 2121, BR-38408100 Uberlandia, MG, Brazil.
   Backes, Andre R., Univ Fed Uberlandia, Fac Comp, Av Joao Naves de Avila 2121, BR-38408100 Uberlandia, MG, Brazil.
   de Mesquita Sa Junior, Jarbas Joaci, Univ Fed Ceara, Dept Engn Comp, BR-62010560 Sobral, Ceara, Brazil.
   Kolb, Rosana Marta, Univ Estadual Paulista, UNESP, Fac Ciencias \& Letras, Dept Ciencias Biol, BR-19806900 Assis, SP, Brazil.},
DOI = {10.1007/978-3-319-25751-8\_7},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-25751-8; 978-3-319-25750-1},
Keywords = {Adaxial epidermis tissue; Texture analysis; Color; Fractal dimension;
   Bouligand-Minkowski method},
Keywords-Plus = {COMPUTER VISION; DIMENSION; FEATURES},
Research-Areas = {Computer Science; Robotics},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory \& Methods;
   Robotics},
Author-Email = {arbackes@yahoo.com.br
   jarbas\_joaci@yahoo.com.br
   rosanakolb@hotmail.com},
Affiliations = {Universidade Federal de Uberlandia; Universidade Federal do Ceara;
   Universidade Estadual Paulista},
ResearcherID-Numbers = {Kolb, Rosana M./D-3592-2012
   },
ORCID-Numbers = {Kolb, Rosana M./0000-0003-3841-5597
   Backes, Andre/0000-0002-7486-4253},
Cited-References = {Backes AR, 2009, LECT NOTES COMPUT SC, V5702, P680, DOI 10.1007/978-3-642-03767-2\_83.
   Backes AR, 2012, PATTERN RECOGN, V45, P1984, DOI 10.1016/j.patcog.2011.11.009.
   Backes AR, 2010, PATTERN RECOGN, V43, P685, DOI 10.1016/j.patcog.2009.07.017.
   Backes AR, 2009, INT J PATTERN RECOGN, V23, P1145, DOI 10.1142/S0218001409007508.
   Bianconi F, 2009, PATTERN RECOGN LETT, V30, P765, DOI 10.1016/j.patrec.2009.02.006.
   Casanova D, 2009, INT J IMAG SYST TECH, V19, P236, DOI 10.1002/ima.20201.
   Sa JJD, 2013, PATTERN RECOGN LETT, V34, P1314, DOI 10.1016/j.patrec.2013.04.013.
   Sa JJD, 2013, ECOL INFORM, V15, P34, DOI 10.1016/j.ecoinf.2013.02.007.
   Sa JJD, 2012, PATTERN RECOGN, V45, P732, DOI 10.1016/j.patcog.2011.07.023.
   Drimbarean A, 2001, PATTERN RECOGN LETT, V22, P1161, DOI 10.1016/S0167-8655(01)00058-7.
   Everitt B.S., 2001, APPL MULTIVARIATE AN.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   Hoang MA, 2005, SIGNAL PROCESS, V85, P265, DOI 10.1016/j.sigpro.2004.10.009.
   LAINE A, 1993, IEEE T PATTERN ANAL, V15, P1186, DOI 10.1109/34.244679.
   Liu GH, 2011, PATTERN RECOGN, V44, P2123, DOI 10.1016/j.patcog.2011.02.003.
   Mandelbrot B.B., 2000, FRACTAL GEOMETRY NAT, V19th.
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Palm C, 2004, PATTERN RECOGN, V37, P965, DOI 10.1016/j.patcog.2003.09.010.
   Paschos G, 2003, PATTERN RECOGN LETT, V24, P309, DOI 10.1016/S0167-8655(02)00244-1.
   Plotze RD, 2005, CAN J BOT, V83, P287, DOI {[}10.1139/b05-002, 10.1139/B05-002].
   Porebski A., 2008, 2008 1 WORKSHOPS IMA, P1, DOI DOI 10.1109/IPTA.2008.4743780.
   Richards J.A., 2006, REMOTE SENSING DIGIT.
   Sa JJD, 2011, BOTANY, V89, P467, DOI {[}10.1139/B11-038, 10.1139/b11-038].
   Tricot C., 1995, CURVES FRACTAL DIMEN.},
Number-of-Cited-References = {25},
Times-Cited = {34},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BE6UG},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000374793800007},
OA = {Bronze},
DA = {2023-08-12},
}

@inproceedings{ WOS:000427105300041,
Author = {Blumenthal, Julie and Megherbi, Dalila B. and Lussier, Robert},
Book-Group-Author = {IEEE},
Title = {Supervised Machine Learning Via Hidden Markov Models for Accurate
   Classification of Plant Stress Levels \& Types Based on Imaged
   Chlorophyll Fluorescence Profiles \& Their Rate of Change in Time},
Booktitle = {2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND
   VIRTUAL ENVIRONMENTS FOR MEASUREMENT SYSTEMS AND APPLICATIONS (CIVEMSA)},
Series = {IEEE International Conference on Computational Intelligence and Virtual
   Environments for Measurement Systems and Applications},
Year = {2017},
Pages = {211-216},
Note = {IEEE International Conference on Computational Intelligence and Virtual
   Environments for Measurement Systems and Applications (CIVEMSA), Annecy,
   FRANCE, JUN 26-28, 2017},
Organization = {IEEE; IEEE Computat Intelligence Soc; IEEE Instrumentat \& Measurement
   Soc},
Abstract = {It has long been known that Chlorophyll fluorescence (ChlF), a plant
   response to stressors in time, is a useful tool in detecting plant
   stress. Accurate and early plant stress detection is imperative in
   enabling appropriate and timely intervention. One of the major
   limitations of prior work in ChIF-based plant classification is that, in
   general, only a few key inflection points of a localized selection of a
   chlorophyll fluorescence signal are used to calculate single index
   values for classification. These values yield limited insight into
   stress level and especially into stressor type. In this paper, we
   introduce and present a new method for plant stress classification that
   uses supervised learning, via Hidden Markov Models (HMMs), to build
   accurate class profiles using global (versus local) ChlF time-varying
   signal data acquired via video imaging. We show how creating
   increased-state supervised models can in particular, classify specific
   stressor types as well as achieve more granularity in stressor level
   classification. Experimental results are presented to show the value and
   potential of the proposed supervised method to enable more accurate and
   specific classification of plant stressor types and stressor levels.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Blumenthal, J (Corresponding Author), Univ Massachusetts, CMINDS Res Ctr, Lowell, MA 01854 USA.
   Blumenthal, Julie; Megherbi, Dalila B., Univ Massachusetts, CMINDS Res Ctr, Lowell, MA 01854 USA.
   Lussier, Robert, GrowTech Inc, Waltham, MA USA.},
ISSN = {2377-9314},
ISBN = {978-1-5090-4253-1},
Keywords = {Computational Intelligence; Machine Learning; Classification; Supervised
   Learning; Chlorophyll Fluorescence Imaging; Digital Image Processing;
   Pattern Representation and Classification/Recognition; Plant Stress;
   Hidden Markov Models; OJIP Transient; PSMT Transient},
Keywords-Plus = {SPEECH RECOGNITION; TRANSIENT; LEAVES; OJIP},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems},
Affiliations = {University of Massachusetts System; University of Massachusetts Lowell},
Cited-References = {Adamski J.M., 2001, J PLANT PHYSL, V168, P2056.
   {[}Anonymous], P 6 WSEAS INT C NEUR.
   {[}Anonymous], POSTER SESSION ABSTR.
   {[}Anonymous], LIGHT EMISSION PLANT.
   {[}Anonymous], 2013, J SIGNAL INF PROCESS.
   {[}Anonymous], 1940, SMITHSONIAN I MISCEL.
   Blumenthal J., UNSUPERVISED M UNPUB.
   Buschmann C, 2007, PHOTOSYNTH RES, V92, P261, DOI 10.1007/s11120-007-9187-8.
   Durbin R., 2001, BIOL SEQUENCE ANAL P, P46.
   Fink GA, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6308-4.
   Gomes MTG, 2012, SCI HORTIC-AMSTERDAM, V142, P49, DOI 10.1016/j.scienta.2012.04.026.
   GOVINDJEE, 1995, AUST J PLANT PHYSIOL, V22, P131, DOI 10.1071/PP9950131.
   Govindjee, 1986, LIGHT EMISSION PLANT, P497.
   Hatou K., 2011, IFAC P, V44, P1785, DOI {[}10.3182/20110828-6-IT-1002.01490, DOI 10.3182/20110828-6-IT-1002.01490].
   Hsiao SC, 2010, COMPUT ELECTRON AGR, V72, P127, DOI 10.1016/j.compag.2010.03.005.
   Jiang HX, 2008, TREE PHYSIOL, V28, P1863, DOI 10.1093/treephys/28.12.1863.
   Kautsky H., 1931, NATURWISSENSCHAFTEN, V19, P964, DOI DOI 10.1007/BF01516164.
   Kuckenberg J, 2009, BIOSYST ENG, V103, P121, DOI 10.1016/j.biosystemseng.2008.09.018.
   Lichtenthaler HK, 2005, PHOTOSYNTHETICA, V43, P379, DOI 10.1007/s11099-005-0062-6.
   Lichtenthaler HK, 1997, TRENDS PLANT SCI, V2, P316, DOI 10.1016/S1360-1385(97)89954-2.
   MAKHOUL J, 1995, P NATL ACAD SCI USA, V92, P9956, DOI 10.1073/pnas.92.22.9956.
   PAPAGEORGIOU G, 1968, BIOPHYS J, V8, P1299, DOI 10.1016/S0006-3495(68)86557-9.
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626.
   Redillas MCFR, 2011, PLANT BIOTECHNOL REP, V5, P169, DOI 10.1007/s11816-011-0170-7.
   Renger G., 1986, Light emission by plants and bacteria, P587.
   Rinderle U., 1988, CRC CRITICAL REV S1, V19, P829.
   Schansker G, 2005, BBA-BIOENERGETICS, V1706, P250, DOI 10.1016/j.bbabio.2004.11.006.
   Stirbet A, 2012, PHOTOSYNTH RES, V113, P15, DOI 10.1007/s11120-012-9754-5.
   Stirbet A, 2011, J PHOTOCH PHOTOBIO B, V104, P236, DOI 10.1016/j.jphotobiol.2010.12.010.
   Strasser BJ, 1995, PHOTOSYNTHESIS: FROM LIGHT TO BIOSPHERE, VOL 5, P977.
   Strasser R., 2000, PROBING PHOTOSYNTHES, V1st, P443.
   Strasser RJ., 1978, CHLOROPLAST DEV, P513.
   Toth SZ, 2007, PHOTOSYNTH RES, V93, P193, DOI 10.1007/s11120-007-9179-8.
   Willits DH, 2001, J AM SOC HORTIC SCI, V126, P188, DOI 10.21273/JASHS.126.2.188.
   Zaharieva Ivelina, 2001, Bulgarian Journal of Plant Physiology, V27, P3.
   Zhou CY, 2016, CHIN CONT DECIS CONF, P2085, DOI 10.1109/CCDC.2016.7531328.},
Number-of-Cited-References = {36},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BJ7DM},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000427105300041},
DA = {2023-08-12},
}

@article{ WOS:000673055300001,
Author = {Goyal, Neha and Kumar, Nitin and Gupta, Kapil},
Title = {Lower-dimensional intrinsic structural representation of leaf images and
   plant recognition},
Journal = {SIGNAL IMAGE AND VIDEO PROCESSING},
Year = {2022},
Volume = {16},
Number = {1},
Pages = {203-210},
Month = {FEB},
Abstract = {This paper proposes a statistical representation called
   ``Eigenleaves{''} for leaf images dependent on their natural structure.
   The proposed presentation possesses several improvements over
   traditional image representation methods as a feature vector, such as
   being simple and fully automatic. We aim toward representing an
   application of the eigenvector that finds the geometrical structure and
   correlated information. The method automates feature extraction without
   explicitly specifying the most dominating attribute and deals with
   issues raised when working with high-dimensional images. Our finding
   confirms that the proposed representation is efficient and significant.
   It is comparable to deep learning approaches that require lots of
   computational costs and a high number of training samples to find the
   hidden structure of leaves that need to be defined at an earlier stage.
   Encoded leaf images are stored for futuristic applications with lesser
   dimensionality and essential information.},
Publisher = {SPRINGER LONDON LTD},
Address = {236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Goyal, N (Corresponding Author), NIT Kurukshetra, Kurukshetra, Haryana, India.
   Goyal, Neha; Gupta, Kapil, NIT Kurukshetra, Kurukshetra, Haryana, India.
   Kumar, Nitin, NIT Kurukshetra, Kurukshetra, Uttarakhand, India.},
DOI = {10.1007/s11760-021-01983-6},
EarlyAccessDate = {JUL 2021},
ISSN = {1863-1703},
EISSN = {1863-1711},
Keywords = {Biodiversity conservation; Plant taxonomy; Principal component;
   Eigenleaves; Basis vectors; Image reconstruction},
Keywords-Plus = {SHAPE-FEATURES; EIGENFACES; EFFICIENT},
Research-Areas = {Engineering; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Imaging Science \& Photographic
   Technology},
Author-Email = {neha.goyal2309@gmail.com},
Affiliations = {National Institute of Technology (NIT System); National Institute of
   Technology Kurukshetra; National Institute of Technology (NIT System);
   National Institute of Technology Kurukshetra},
ResearcherID-Numbers = {Goyal, Neha/AFH-8800-2022
   Goyal, Neha/AAF-3497-2022
   },
ORCID-Numbers = {Goyal, Neha/0000-0002-7016-4663},
Funding-Acknowledgement = {University Grant Commission, India},
Funding-Text = {We acknowledge University Grant Commission, India, to provide financial
   assistance to one of the authors Ms. Neha Goyal under the scheme
   NET-JRF.},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   {[}Anonymous], 2013, ARXIV14014447.
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Goeau H., 2013, PL NTNET MOBILE APP, P423, DOI {[}10.1145/2502081.2502251, DOI 10.1145/2502081.2502251].
   Goyal Neha, 2018, 2018 International Conference on Computing, Power and Communication Technologies (GUCON), P405, DOI 10.1109/GUCON.2018.8675114.
   Goyal N, 2019, MULTIMED TOOLS APPL, V78, P27785, DOI 10.1007/s11042-019-7588-2.
   Hsiao JK, 2014, 2014 SCIENCE AND INFORMATION CONFERENCE (SAI), P389, DOI 10.1109/SAI.2014.6918216.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Li HF, 2004, ADV NEUR IN, V16, P97.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Pathak MR, 2014, J EXP BIOL AGRIC SCI, V2.
   Pearline SA, 2019, J INTELL FUZZY SYST, V36, P1997, DOI 10.3233/JIFS-169911.
   Nguyen QK, 2013, PROC INT CONF ADV, P404, DOI 10.1109/ATC.2013.6698145.
   Robinson BS, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0156572.
   SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519.
   Soderkvist O., 2001, THESIS.
   Tan J. W., 2018, IEEE ACM T COMPUTATI.
   Tscharntke T, 2012, LAND USE INTENSIFICATION: EFFECTS ON AGRICULTURE, BIODIVERSITY AND ECOLOGICAL PROCESSES, P7.
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097.
   Zhao ZQ, 2015, NEUROCOMPUTING, V151, P1112, DOI 10.1016/j.neucom.2014.02.077.},
Number-of-Cited-References = {27},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Signal Image Video Process.},
Doc-Delivery-Number = {YF8IL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000673055300001},
DA = {2023-08-12},
}

@article{ WOS:000614132700005,
Author = {Dorai, Kavita},
Title = {NMR Spectroscopy and the Plant Metabolome},
Journal = {EMAGRES},
Year = {2021},
Volume = {9},
Number = {4},
Pages = {333-347},
Abstract = {Nuclear magnetic resonance (NMR)-based metabolomics has led to several
   pathbreaking developments in the characterization of the plant
   metabolome. New techniques for data processing and analysis of NMR
   spectra have improved the identification and quantitation of primary and
   secondary plant metabolites. The exciting possibility of using machine
   learning algorithms and artificial-intelligence tools such as deep
   learning and neural networks for multivariate statistical analysis of
   NMR data has opened up new avenues of research in integrating `Big Data'
   methods with plant metabolomics. Quantitative metabolite fingerprinting
   using two-dimensional (2D) ultrafast and high-resolution magic angle
   spinning (HR-MAS) NMR experiments has provided unique perspectives on
   the interactions of plant metabolic networks and their responses to
   external environment stresses. Plant NMR metabolomic studies have
   contributed significantly to the understanding of plant classification
   and taxonomy, the interaction of plant metabolic networks, bioactivity
   and mechanism of action of significant metabolites in medicinally
   important plants, genetically modified plants and their ecological
   implications, plant-organism interactions, plant defense against
   herbivore and pathogen attacks, and plant metabolome response to abiotic
   stress.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Dorai, K (Corresponding Author), Indian Inst Sci Educ \& Res Mohali, Manauli, Punjab, India.
   Dorai, Kavita, Indian Inst Sci Educ \& Res Mohali, Manauli, Punjab, India.},
DOI = {10.1002/9780470034590.emrstm1629},
ISSN = {2055-6101},
Keywords = {plant NMR metabolomics; NMR data processing; big data and statistical
   analysis; 2D ultrafast and HR-MAS NMR methods; plant-organism
   interactions; plant stress response},
Research-Areas = {Spectroscopy},
Web-of-Science-Categories  = {Spectroscopy},
Affiliations = {Indian Institute of Science Education \& Research (IISER) - Mohali},
Funding-Acknowledgement = {MHRD-STARS India {[}STARS/2019/BS/524/FS]},
Funding-Text = {Funding from MHRD-STARS India under grant number STARS/2019/BS/524/FS is
   gratefully acknowledged.},
Cited-References = {Ali S, 2020, MOLECULES, V25, DOI 10.3390/molecules25163647.
   Alonso A, 2015, FRONT BIOENG BIOTECH, V3, DOI 10.3389/fbioe.2015.00023.
   Augustijn D, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0163258.
   Augustijn D., 2019, PLOS ONE, V13, P1.
   Augustijn D, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0218219.
   Bakiri A, 2018, J CHEM INF MODEL, V58, P262, DOI 10.1021/acs.jcim.7b00653.
   Beirnaert C, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006018.
   Bingol K, 2017, CURR OPIN BIOTECH, V43, P17, DOI 10.1016/j.copbio.2016.07.006.
   Bingol K, 2014, ANAL CHEM, V86, P47, DOI 10.1021/ac403520j.
   Blondel C, 2016, ENVIRON POLLUT, V214, P539, DOI 10.1016/j.envpol.2016.04.057.
   Bruguiere A, 2020, ANAL CHEM, V92, P8793, DOI 10.1021/acs.analchem.0c00193.
   Cerulli A, 2018, J PHARMACEUT BIOMED, V160, P168, DOI 10.1016/j.jpba.2018.07.046.
   Charris-Molina A, 2020, J PROTEOME RES, V19, P2977, DOI 10.1021/acs.jproteome.9b00872.
   Charris-Molina A, 2019, J PROTEOME RES, V18, P2241, DOI 10.1021/acs.jproteome.9b00093.
   Chin E, 2017, ARTHROPOD-PLANT INTE, V11, P901, DOI 10.1007/s11829-017-9546-0.
   Colquhoun IJ, 2007, J PESTIC SCI, V32, P200, DOI 10.1584/jpestics.R07-03.
   Coutinho ID, 2017, PHYTOCHEM ANALYSIS, V28, P529, DOI 10.1002/pca.2702.
   de Oliveira CS, 2014, MAGN RESON CHEM, V52, P422, DOI 10.1002/mrc.4082.
   Deborde C, 2019, METABOLOMICS, V15, DOI 10.1007/s11306-019-1488-3.
   Dona AC, 2016, COMPUT STRUCT BIOTEC, V14, P135, DOI 10.1016/j.csbj.2016.02.005.
   Eghbalnia HR, 2017, CURR OPIN BIOTECH, V43, P56, DOI 10.1016/j.copbio.2016.08.005.
   Feizi N, 2020, ANAL BIOCHEM, V611, DOI 10.1016/j.ab.2020.113945.
   Feng Z, 2020, FOOD CHEM, V310, DOI 10.1016/j.foodchem.2019.125914.
   Feraud B, 2020, METABOLOMICS, V16, DOI 10.1007/s11306-020-01662-6.
   Fernandez O, 2016, METABOLOMICS, V12, DOI 10.1007/s11306-016-1099-1.
   Ferry-Dumazet H, 2011, BMC PLANT BIOL, V11, DOI 10.1186/1471-2229-11-104.
   Frederich M, 2016, J MED CHEM, V59, P8649, DOI 10.1021/acs.jmedchem.5b01335.
   Garcia-Perez I, 2020, NAT PROTOC, V15, P2538, DOI 10.1038/s41596-020-0343-3.
   Giraudeau P, 2020, ANALYST, V145, P2457, DOI 10.1039/d0an00142b.
   Gogna N, 2015, J PHARMACEUT BIOMED, V115, P74, DOI 10.1016/j.jpba.2015.06.035.
   Gogna N, 2015, CURR SCI INDIA, V108, P1694.
   Gowda GAN, 2017, ANAL CHEM, V89, P490, DOI 10.1021/acs.analchem.6b04420.
   Harrington PD, 2017, J ANAL TEST, V1, DOI 10.1007/s41664-017-0003-y.
   Hdiji H., 2010, ECOTOXICOL ENV SAF, V73.
   Hellal K, 2020, MOLECULES, V25, DOI 10.3390/molecules25051247.
   Holmes E, 2002, ANALYST, V127, P1549, DOI 10.1039/b208254n.
   Hong X, 2020, PHYTOCHEM ANALYSIS, V31, P15, DOI 10.1002/pca.2861.
   Huberty M, 2020, J CHEM ECOL, V46, P745, DOI 10.1007/s10886-020-01156-8.
   Idle JR, 2007, CELL METAB, V6, P348, DOI 10.1016/j.cmet.2007.10.005.
   Izquierdo-Garcia JL, 2011, PROG NUCL MAG RES SP, V59, P263, DOI 10.1016/j.pnmrs.2011.02.001.
   Jacob D, 2017, METABOLOMICS, V13, DOI 10.1007/s11306-017-1178-y.
   Jezequel T, 2015, METABOLOMICS, V11, P1231, DOI 10.1007/s11306-015-0780-0.
   Jiang L, 2020, ANAL BIOCHEM, V597, DOI 10.1016/j.ab.2020.113692.
   Jo S, 2020, INT J FOOD PROP, V23, P241, DOI 10.1080/10942912.2020.1722160.
   Kaiser KA, 2009, MAGN RESON CHEM, V47, pS147, DOI 10.1002/mrc.2457.
   Kikuchi J, 2018, PROG NUCL MAG RES SP, V104, P56, DOI 10.1016/j.pnmrs.2017.11.003.
   Kim HK, 2011, TRENDS BIOTECHNOL, V29, P267, DOI 10.1016/j.tibtech.2011.02.001.
   Kim HK, 2010, NAT PROTOC, V5, P536, DOI 10.1038/nprot.2009.237.
   Kopriva I, 2019, ANAL CHIM ACTA, V1080, P55, DOI 10.1016/j.aca.2019.07.004.
   Krishnan P, 2005, J EXP BOT, V56, P255, DOI 10.1093/jxb/eri010.
   Krov K., 2012, ECOL CHEM ENG S, V19, P133.
   Kruk J, 2017, APPL MAGN RESON, V48, P1, DOI 10.1007/s00723-016-0846-9.
   Kuhlisch C, 2015, NAT PROD REP, V32, P937, DOI 10.1039/c5np00003c.
   Kumar D, 2016, CRIT REV ANAL CHEM, V46, P400, DOI 10.1080/10408347.2015.1106932.
   Larive CK, 2015, ANAL CHEM, V87, P133, DOI 10.1021/ac504075g.
   Le Guennec A, 2014, ANAL CHEM, V86, P5946, DOI 10.1021/ac500966e.
   Lee W, 2020, APPL BIOL CHEM, V63, DOI 10.1186/s13765-020-00548-4.
   Leenders Justine, 2015, Drug Discov Today Technol, V13, P39, DOI 10.1016/j.ddtec.2015.06.005.
   Li XK, 2019, ECOTOX ENVIRON SAFE, V184, DOI 10.1016/j.ecoenv.2019.109602.
   Li ZY, 2013, J PHARMACEUT BIOMED, V75, P158, DOI 10.1016/j.jpba.2012.11.023.
   Liu CX, 2010, J PROTEOME RES, V9, P6774, DOI 10.1021/pr100970q.
   Lopez JM, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-43374-5.
   Lu JX, 2019, PHYTOMEDICINE, V58, DOI 10.1016/j.phymed.2019.152826.
   Ludwig C, 2010, PHYTOCHEM ANALYSIS, V21, P22, DOI 10.1002/pca.1186.
   Macel M, 2010, MOL ECOL RESOUR, V10, P583, DOI 10.1111/j.1755-0998.2010.02854.x.
   Mahrous EA, 2015, J ADV RES, V6, P3, DOI 10.1016/j.jare.2014.10.003.
   Maraschin M, 2016, J NAT PROD, V79, P13, DOI 10.1021/acs.jnatprod.5b00315.
   Marchand J, 2017, CURR OPIN BIOTECH, V43, P49, DOI 10.1016/j.copbio.2016.08.004.
   Marcheafave GG, 2020, SCI TOTAL ENVIRON, V749, DOI 10.1016/j.scitotenv.2020.142350.
   Markley JL, 2017, CURR OPIN BIOTECH, V43, P34, DOI 10.1016/j.copbio.2016.08.001.
   Marshall DD, 2017, PROG NUCL MAG RES SP, V100, P1, DOI 10.1016/j.pnmrs.2017.01.001.
   Martin M, 2018, ANAL CHIM ACTA, V1019, P1, DOI 10.1016/j.aca.2018.02.067.
   Martineau E, 2020, MAGN RESON CHEM, V58, P390, DOI 10.1002/mrc.4899.
   Mazzei P, 2017, CHEM BIOL TECHNOL AG, V4, DOI 10.1186/s40538-017-0093-9.
   Mazzei P, 2018, J AGR FOOD CHEM, V66, P2580, DOI 10.1021/acs.jafc.7b04340.
   Mazzei P, 2016, J AGR FOOD CHEM, V64, P3538, DOI 10.1021/acs.jafc.6b00801.
   Mishra S, 2022, NAT PROD RES, V36, P390, DOI 10.1080/14786419.2020.1762190.
   Mishra S, 2019, ENVIRON EXP BOT, V164, P58, DOI 10.1016/j.envexpbot.2019.04.019.
   Mucci A, 2013, FOOD CHEM, V141, P3167, DOI 10.1016/j.foodchem.2013.05.151.
   Mustroph A, 2014, PLANT CELL ENVIRON, V37, P2366, DOI 10.1111/pce.12282.
   Nakabayashi R, 2015, CURR OPIN PLANT BIOL, V24, P10, DOI 10.1016/j.pbi.2015.01.003.
   Nkomo MM, 2014, BMC COMPLEM ALTERN M, V14, DOI 10.1186/1472-6882-14-99.
   Obata T, 2012, CELL MOL LIFE SCI, V69, P3225, DOI 10.1007/s00018-012-1091-5.
   Okazaki Y, 2012, PLANT BIOTECHNOL REP, V6, P1, DOI 10.1007/s11816-011-0191-2.
   Pagter M, 2017, J AGR FOOD CHEM, V65, P10123, DOI 10.1021/acs.jafc.7b03788.
   Peters K, 2018, INT J MOL SCI, V19, DOI 10.3390/ijms19051385.
   Plischke A, 2012, J AGR FOOD CHEM, V60, P1488, DOI 10.1021/jf204864y.
   Pontes JGM, 2017, ANAL METHODS-UK, V9, P1078, DOI {[}10.1039/c6ay03102a, 10.1039/C6AY03102A].
   Putri SP, 2013, J BIOSCI BIOENG, V115, P579, DOI 10.1016/j.jbiosc.2012.12.007.
   Ritota M, 2012, FOOD CHEM, V135, P684, DOI 10.1016/j.foodchem.2012.05.032.
   Rivas-Ubach A, 2013, METHODS ECOL EVOL, V4, P464, DOI 10.1111/2041-210X.12028.
   Sajitha TP, 2019, CHEMOECOLOGY, V29, P135, DOI 10.1007/s00049-019-00283-3.
   Sajitha TP, 2018, J CHEM ECOL, V44, P611, DOI 10.1007/s10886-018-0960-2.
   Salem MA, 2020, METABOLITES, V10, DOI 10.3390/metabo10010037.
   Sampaio BL, 2016, SCI REP-UK, V6, DOI 10.1038/srep29265.
   Schripsema J, 2010, PHYTOCHEM ANALYSIS, V21, P14, DOI 10.1002/pca.1185.
   Sekiyama Y, 2010, ANAL CHEM, V82, P1643, DOI 10.1021/ac9019076.
   Sharma R, 2017, RSC ADV, V7, P29860, DOI 10.1039/c7ra04032f.
   Shiokawa Y, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-20121-w.
   Shulaev V, 2008, PHYSIOL PLANTARUM, V132, P199, DOI 10.1111/j.1399-3054.2007.01025.x.
   Sidhu OP, 2010, PLANTA, V232, P85, DOI 10.1007/s00425-010-1159-0.
   Smolinska A, 2012, ANAL CHIM ACTA, V750, P82, DOI 10.1016/j.aca.2012.05.049.
   Song EH, 2016, J AGR FOOD CHEM, V64, P3009, DOI 10.1021/acs.jafc.5b05667.
   Spicer R, 2017, METABOLOMICS, V13, DOI 10.1007/s11306-017-1242-7.
   Sumner LW, 2015, NAT PROD REP, V32, P212, DOI 10.1039/c4np00072b.
   Sun J, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108821.
   Taglienti A, 2020, J SCI FOOD AGR, V100, P3418, DOI 10.1002/jsfa.10376.
   Vu T, 2019, J PROTEOME RES, V18, P3282, DOI 10.1021/acs.jproteome.9b00227.
   Tucker DJ, 2010, J CHEM ECOL, V36, P727, DOI 10.1007/s10886-010-9803-5.
   Urano K, 2010, CURR OPIN PLANT BIOL, V13, P132, DOI 10.1016/j.pbi.2009.12.006.
   Urumarudappa SKJ, 2016, INT J LEGAL MED, V130, P1457, DOI 10.1007/s00414-016-1436-y.
   Valentino G, 2020, MOLECULES, V25, DOI 10.3390/molecules25061444.
   van der Kooy F, 2009, PLANTA MED, V75, P763, DOI 10.1055/s-0029-1185450.
   Verhoeven A, 2018, ANAL CHIM ACTA, V1044, P66, DOI 10.1016/j.aca.2018.07.070.
   Veselkov KA, 2009, ANAL CHEM, V81, P56, DOI 10.1021/ac8011544.
   Vignoli A, 2019, ANGEW CHEM INT EDIT, V58, P968, DOI 10.1002/anie.201804736.
   Vinci G, 2018, PLANT SOIL, V429, P437, DOI 10.1007/s11104-018-3701-y.
   Wang K, 2015, ANAL METHODS-UK, V7, P9673, DOI 10.1039/c5ay01079a.
   Wang M, 2017, CHEM-BIOL INTERACT, V273, P133, DOI 10.1016/j.cbi.2017.06.011.
   Ward JL, 2007, FEBS J, V274, P1126, DOI 10.1111/j.1742-4658.2007.05675.x.
   Wedeking R., 2018, PLOS ONE, P13.
   Wishart DS, 2019, J MAGN RESON, V306, P155, DOI 10.1016/j.jmr.2019.07.013.
   Wolfender JL, 2013, CURR MED CHEM, V20, P1056.
   Wu XY, 2014, ANALYST, V139, P1769, DOI 10.1039/c3an02100a.
   Yun YH, 2020, CHEMOMETR INTELL LAB, V197, DOI 10.1016/j.chemolab.2019.103920.
   Zhang B, 2020, METABOLITES, V10, DOI 10.3390/metabo10050203.
   Zhao LJ, 2016, ENVIRON SCI TECHNOL, V50, P2000, DOI 10.1021/acs.est.5b05011.
   Zhi HJ, 2012, PHYTOCHEM ANALYSIS, V23, P492, DOI 10.1002/pca.2346.},
Number-of-Cited-References = {128},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {42},
Journal-ISO = {eMagRes},
Doc-Delivery-Number = {QB4TE},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000614132700005},
DA = {2023-08-12},
}

@article{ WOS:000343702400007,
Author = {Qi, Xianbiao and Xiao, Rong and Li, Chun-Guang and Qiao, Yu and Guo, Jun
   and Tang, Xiaoou},
Title = {Pairwise Rotation Invariant Co-Occurrence Local Binary Pattern},
Journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
Year = {2014},
Volume = {36},
Number = {11},
Pages = {2199-2213},
Month = {NOV},
Abstract = {Designing effective features is a fundamental problem in computer
   vision. However, it is usually difficult to achieve a great tradeoff
   between discriminative power and robustness. Previous works shown that
   spatial co-occurrence can boost the discriminative power of features.
   However the current existing co-occurrence features are taking few
   considerations to the robustness and hence suffering from sensitivity to
   geometric and photometric variations. In this work, we study the
   Transform Invariance (TI) of co-occurrence features. Concretely we
   formally introduce a Pairwise Transform Invariance (PTI) principle, and
   then propose a novel Pairwise Rotation Invariant Co-occurrence Local
   Binary Pattern (PRICoLBP) feature, and further extend it to incorporate
   multi-scale, multi-orientation, and multi-channel information. Different
   from other LBP variants, PRICoLBP can not only capture the spatial
   context co-occurrence information effectively, but also possess rotation
   invariance. We evaluate PRICoLBP comprehensively on nine benchmark data
   sets from five different perspectives, e.g., encoding strategy, rotation
   invariance, the number of templates, speed, and discriminative power
   compared to other LBP variants. Furthermore we apply PRICoLBP to six
   different but related applications-texture, material, flower, leaf,
   food, and scene classification, and demonstrate that PRICoLBP is
   efficient, effective, and of a well-balanced tradeoff between the
   discriminative power and robustness.},
Publisher = {IEEE COMPUTER SOC},
Address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
Type = {Article},
Language = {English},
Affiliation = {Qi, XB (Corresponding Author), Beijing Univ Posts \& Telecommun, Sch Informat \& Commun Engn, Beijing 100876, Peoples R China.
   Qi, Xianbiao; Li, Chun-Guang; Guo, Jun, Beijing Univ Posts \& Telecommun, Sch Informat \& Commun Engn, Beijing 100876, Peoples R China.
   Xiao, Rong, Microsoft Corp, Redmond, WA 98052 USA.
   Qiao, Yu; Tang, Xiaoou, Chinese Acad Sci, Shenzhen Key Lab Comp Vis \& Pattern Recognit, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.},
DOI = {10.1109/TPAMI.2014.2316826},
ISSN = {0162-8828},
EISSN = {1939-3539},
Keywords = {Co-occurrence LBPs; rotation invariance; texture classification;
   material recognition; flower recognition; leaf recognition; food
   recognition; scene recognition},
Keywords-Plus = {TEXTURE CLASSIFICATION; FACE RECOGNITION; REPRESENTATION; EXTRACTION;
   FEATURES; SCENE; SHAPE},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic},
Author-Email = {qixianbiao@gmail.com
   rxiao@microsoft.com
   lichunguang@bupt.edu.cn
   yu.qiao@siat.ac.cn
   guojun@bupt.edu.cn
   xtang@ie.cuhk.edu.hk},
Affiliations = {Beijing University of Posts \& Telecommunications; Microsoft; Chinese
   Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS},
ResearcherID-Numbers = {Yu, Qiao/IAP-6999-2023
   Qiao, Yu/ABD-5787-2021},
Funding-Acknowledgement = {Natural Science Foundation of China (NSFC) {[}61175011, 61273217,
   61171193]; 111 Project {[}B08004]; NSFC {[}91320101]; Shenzhen Basic
   Research Program {[}JC201005270350A, JCYJ20120903092050890,
   JCYJ20120617114614438]; CAS},
Funding-Text = {The authors would like to thank the anonymous reviewers for constructive
   comments. We also want to thank Dr. Guoying Zhao for giving some useful
   comments. X. Qi, C.-G. Li, and J. Guo are supported by the Natural
   Science Foundation of China (NSFC) under Grant nos. 61175011, 61273217,
   and 61171193, and the 111 Project under Grant no. B08004. Y. Qiao and X.
   Tang are supported by NSFC (91320101), Shenzhen Basic Research Program
   (JC201005270350A, JCYJ20120903092050890, JCYJ20120617114614438), 100
   Talents Programme of CAS.},
Cited-References = {Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244.
   Ahonen T, 2009, LECT NOTES COMPUT SC, V5575, P61, DOI 10.1007/978-3-642-02230-2\_7.
   {[}Anonymous], 2010, PROC ACM INT C MULTI, DOI DOI 10.1145/1873951.1874249.
   {[}Anonymous], 2006, IEEE COMPUTER SOC C.
   {[}Anonymous], 2007, 2007 IEEE C COMP VIS, DOI {[}10.1109/CVPR.2007.383018, DOI 10.1109/CVPR.2007.383018].
   {[}Anonymous], 1999, TEXTURES PHOTOGRAPHI.
   {[}Anonymous], 2010, ADV NEURAL PROCESSIN.
   Bo L., 2010, ADV NEURAL INFORM PR, P244.
   Bo L., 2009, P ADV NEUR INF PROC, V1730, P1731.
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598.
   Caputo B, 2010, IMAGE VISION COMPUT, V28, P150, DOI 10.1016/j.imavis.2009.05.005.
   Chai Y., 2011, THESIS SWISS FEDERAL.
   Chai YN, 2012, LECT NOTES COMPUT SC, V7572, P794, DOI 10.1007/978-3-642-33718-5\_57.
   Chai YN, 2011, IEEE I CONF COMP VIS, P2579, DOI 10.1109/ICCV.2011.6126546.
   Chang P., 1999, P IEEE C COMP VIS PA.
   Chen M, 2009, IEEE IMAGE PROC, P289, DOI 10.1109/ICIP.2009.5413511.
   Chum O, 2010, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2010.5539997.
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778.
   Fei-Fei L, 2005, PROC CVPR IEEE, P524.
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957.
   Guo ZH, 2010, PATTERN RECOGN, V43, P706, DOI 10.1016/j.patcog.2009.08.017.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Hayman E, 2004, LECT NOTES COMPUT SC, V2034, P253.
   Hobson P., 2013, P IEEE 20 INT C IM P.
   Hu DN, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.48.
   Huu-Giao Nguyen, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2945, DOI 10.1109/CVPR.2011.5995340.
   Ito S, 2010, LECT NOTES COMPUT SC, V6312, P209, DOI 10.1007/978-3-642-15552-9\_16.
   Kanan C, 2010, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2010.5539947.
   Kwitt Roland, 2012, Computer Vision - ECCV 2012. Proceedings of the 12th European Conference on Computer Vision, P359, DOI 10.1007/978-3-642-33765-9\_26.
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151.
   Li WB, 2012, LECT NOTES COMPUT SC, V7575, P345, DOI 10.1007/978-3-642-33765-9\_25.
   Liao ZC, 2013, PROC CVPR IEEE, P963, DOI 10.1109/CVPR.2013.129.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Liu C, 2010, PROC CVPR IEEE, P239, DOI {[}10.1109/CVPR.2010.5540207, 10.1109/ICCET.2010.5485248].
   Liu L, 2012, IEEE T PATTERN ANAL, V34, P574, DOI 10.1109/TPAMI.2011.145.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Ni BB, 2012, PROC CVPR IEEE, P3514, DOI 10.1109/CVPR.2012.6248094.
   Nilsback M.-E., 2009, THESIS U OXFORD OXFO.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Nosaka R, 2011, LECT NOTES COMPUT SC, V7088, P82, DOI 10.1007/978-3-642-25346-1\_8.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724.
   Orban GA, 2008, PHYSIOL REV, V88, P59, DOI 10.1152/physrev.00008.2007.
   Pandey M, 2011, IEEE I CONF COMP VIS, P1307, DOI 10.1109/ICCV.2011.6126383.
   Parizi SN, 2012, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2012.6248001.
   Qi XB, 2012, LECT NOTES COMPUT SC, V7577, P158, DOI 10.1007/978-3-642-33783-3\_12.
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537.
   Rasiwasia N, 2009, PROC CVPR IEEE, P1889, DOI 10.1109/CVPRW.2009.5206826.
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720.
   Sadeghi F, 2012, LECT NOTES COMPUT SC, V7576, P228, DOI 10.1007/978-3-642-33715-4\_17.
   Sharan L., 2009, J VISUAL-JAPAN, V9, P784, DOI {[}10.1167/9.8.784, DOI 10.1167/9.8.784].
   Sharan L, 2013, INT J COMPUT VISION, V103, P348, DOI 10.1007/s11263-013-0609-0.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Tan XY, 2007, LECT NOTES COMPUT SC, V4778, P168.
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132.
   Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4.
   Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182.
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153.
   Vu N.-S., 2011, P INT JOINT C BIOM, P1.
   Vu NS, 2010, LECT NOTES COMPUT SC, V6311, P313.
   Wang LW, 2012, PROC CVPR IEEE, P2767, DOI 10.1109/CVPR.2012.6248000.
   Wang ZL, 2013, IEEE T IMAGE PROCESS, V22, P537, DOI 10.1109/TIP.2012.2218826.
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224.
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970.
   Yang S, 2010, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2010.5539907.
   Yang Y, 2011, IEEE I CONF COMP VIS, P1465, DOI 10.1109/ICCV.2011.6126403.
   Yuan JS, 2011, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2011.5995476.
   Yuan XT, 2010, PROC CVPR IEEE, P3493, DOI 10.1109/CVPR.2010.5539967.
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4.
   Zhang SW, 2013, PATTERN RECOGN, V46, P1891, DOI 10.1016/j.patcog.2013.01.015.
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528.
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110.
   Zhao GY, 2012, IEEE T IMAGE PROCESS, V21, P1465, DOI 10.1109/TIP.2011.2175739.
   Zhu J., 2010, ADV NEURAL INFORM PR, P2586.},
Number-of-Cited-References = {74},
Times-Cited = {204},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {76},
Journal-ISO = {IEEE Trans. Pattern Anal. Mach. Intell.},
Doc-Delivery-Number = {AR6OI},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000343702400007},
DA = {2023-08-12},
}

@article{ WOS:000424540800007,
Author = {Jye, Kho Soon and Manickam, Sugumaran and Malek, Sorayya and Mosleh,
   Mogeeb and Dhillon, Sarinder Kaur},
Title = {Automated plant identification using artificial neural network and
   support vector machine},
Journal = {FRONTIERS IN LIFE SCIENCE},
Year = {2017},
Volume = {10},
Number = {1},
Pages = {98-107},
Abstract = {Ficus is one of the largest genera in plant kingdom reaching to about
   1000 species worldwide. While taxonomic keys are available for
   identifying most species of Ficus, it is very difficult and time
   consuming for interpretation by a nonprofessional thus requires highly
   trained taxonomists. The purpose of the current study is to develop an
   efficient baseline automated system, using image processing with pattern
   recognition approach, to identify three species of Ficus, which have
   similar leaf morphology. Leaf images from three different Ficus species
   namely F. benjamina, F. pellucidopunctata and F. sumatrana were
   selected. A total of 54 leaf image samples were used in this study.
   Three main steps that are image pre-processing, feature extraction and
   recognition were carried out to develop the proposed system. Artificial
   neural network (ANN) and support vector machine (SVM) were the
   implemented recognition models. Evaluation results showed the ability of
   the proposed system to recognize leaf images with an accuracy of 83.3\%.
   However, the ANN model performed slightly better using the AUC
   evaluation criteria. The system developed in the current study is able
   to classify the selected Ficus species with acceptable accuracy.},
Publisher = {TAYLOR \& FRANCIS LTD},
Address = {2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Dhillon, SK (Corresponding Author), Univ Malaya, Inst Biol Sci, Data Sci \& Bioinformat Lab, Fac Sci, Kuala Lumpur 50603, Malaysia.
   Jye, Kho Soon; Malek, Sorayya; Dhillon, Sarinder Kaur, Univ Malaya, Inst Biol Sci, Data Sci \& Bioinformat Lab, Fac Sci, Kuala Lumpur 50603, Malaysia.
   Manickam, Sugumaran, Univ Malaya, Inst Biol Sci, Fac Sci, Rimba Ilmu Bot Garden, Kuala Lumpur, Malaysia.
   Mosleh, Mogeeb, Taiz Univ, Software Engn Dept, Fac Engn \& Informat Technol, Taizi, Yemen.},
DOI = {10.1080/21553769.2017.1412361},
ISSN = {2155-3769},
EISSN = {2155-3777},
Keywords = {Life data technology; automated species identification; artificial
   neural network; support vector machine; Ficus},
Keywords-Plus = {CLASSIFICATION; IMAGE},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {sarinder@um.edu.my},
Affiliations = {Universiti Malaya; Universiti Malaya; Taiz University},
ResearcherID-Numbers = {Malek, Sorayya/IVV-5900-2023
   Dhillon, Sarinder Kaur/B-5320-2009
   Manickam, Sugumaran/B-5268-2010
   Mosleh, Mogeeb A. A./G-6217-2011
   Malek, Sorayya/B-9830-2010
   Mosleh, Mogeeb A. A./H-3735-2016
   },
ORCID-Numbers = {Malek, Sorayya/0000-0001-6450-6404
   Dhillon, Sarinder Kaur/0000-0003-1922-2044
   Manickam, Sugumaran/0000-0003-3125-767X
   Mosleh, Mogeeb A. A./0000-0001-5094-5561
   Malek, Sorayya/0000-0001-6450-6404
   Mosleh, Mogeeb A. A./0000-0001-5094-5561
   Kho, Soon Jye/0000-0002-1210-9316},
Funding-Acknowledgement = {University of Malaya {[}Living Lab LL023-16SUS, Living Lab
   (LLO20-16SUS)]},
Funding-Text = {This work was supported by the University of Malaya {[}grant number
   Living Lab LL023-16SUS], {[}grant number Living Lab (LLO20-16SUS)].},
Cited-References = {Abu A, 2013, SYST BIODIVERS, V11, P19, DOI 10.1080/14772000.2012.761655.
   Abu A, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-48.
   Berg C. C., 2006, FLORA MALESIANA SERI, V17, P1.
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401.
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851.
   Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646.
   Clark JY, 2012, CIBCB 2012 P IEEE S, DOI {[}10.1109/CIBCB.2012, DOI 10.1109/CIBCB.2012].
   Coltelli P, 2014, ENVIRON SCI-PROC IMP, V16, P2656, DOI {[}10.1039/c4em00451e, 10.1039/C4EM00451E].
   Cook JM, 2003, TRENDS ECOL EVOL, V18, P241, DOI 10.1016/S0169-5347(03)00062-4.
   Gutierrez S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0143197.
   Hearn DJ, 2009, TAXON, V58, P934, DOI 10.1002/tax.583021.
   Hu J, 2012, COMPUT ELECTRON AGR, V88, P133, DOI 10.1016/j.compag.2012.07.008.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Junior OL, 2009, P INT TRANSP SYST 20.
   Kalafi EY, 2016, BMC BIOINFORMATICS, V17, DOI 10.1186/s12859-016-1376-z.
   Kalafi EY, 2017, FOLIA MORPHOL, DOI {[}10.5603/FM.a2017, DOI 10.5603/FM.A2017].
   Karatzoglou A., 2004, J STAT SOFTW, V11, P1, DOI {[}DOI 10.18637/JSS.V011.I09, 10.18637/jss.v011.i09].
   Kebapci H, 2010, COMPUT J ADV ACCESS, V54, P1475.
   Kiranyaz S, 2010, P P INT WORKSH ADV I.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   LAMBERT FR, 1991, J ECOL, V79, P793, DOI 10.2307/2260668.
   Leow LK, 2015, BMC BIOINFORMATICS, V16, DOI 10.1186/1471-2105-16-S18-S4.
   Mao RF, 2006, ISDA 2006: SIXTH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS DESIGN AND APPLICATIONS, VOL 1, P17.
   Markovic M, 1997, P 13 INT C DIG SIGN.
   Math Works, 2001, IM PROC TOOLB.
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5.
   Mosleh MAA, 2012, BMC BIOINFORMATICS, V13, DOI 10.1186/1471-2105-13-S17-S25.
   Ramos E, 2009, ECOL INFORM, V4, P177, DOI 10.1016/j.ecoinf.2009.06.003.
   Salimi N, 2016, PEERJ, V4, DOI 10.7717/peerj.1664.
   Sarimveis H, 2006, ADV ENG SOFTW, V37, P218, DOI 10.1016/j.advengsoft.2005.07.005.
   Schneider CA, 2012, NAT METHODS, V9, P671, DOI 10.1038/nmeth.2089.
   See M, 2016, SAINS MALAYS, V45, P735.
   Shanahan M, 2001, BIOL REV, V76, P529, DOI 10.1017/S1464793101005760.
   Sunil K, 2008, ACTA ECOL SIN, V28, P4253, DOI DOI 10.1016/S1872-2032(08)60080-3.
   Wang JN, 2012, KNOWL-BASED SYST, V33, P102, DOI 10.1016/j.knosys.2012.03.014.
   Wong JY, 2016, J FISH BIOL, V89, P1324, DOI 10.1111/jfb.13039.
   Zhang H, 2012, 2012 INTERNATIONAL CONFERENCE ON INDUSTRIAL CONTROL AND ELECTRONICS ENGINEERING (ICICEE), P2025, DOI 10.1109/ICICEE.2012.538.},
Number-of-Cited-References = {37},
Times-Cited = {18},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Journal-ISO = {Front. Life Sci.},
Doc-Delivery-Number = {FV4JZ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000424540800007},
OA = {Bronze},
DA = {2023-08-12},
}

@article{ WOS:000774644100011,
Author = {Ibrahim, Nehad M. and Gabr, Dalia Goda Ibrahim and Rahman, Atta-ur and
   Dash, Sujata and Nayyar, Anand},
Title = {A deep learning approach to intelligent fruit identification and family
   classification},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2022},
Volume = {81},
Number = {19},
Pages = {27783-27798},
Month = {AUG},
Abstract = {The deep learning techniques have been playing an important role in the
   identification and classification problems such as diseases in medical
   science, marketing in the industry, manufacturing in engineering, and
   identification in plant taxonomy science. Fruit identification and its
   family classification is among one of the areas that needs more emphasis
   for the sake of automation. With this inspiration, fruit images for 52
   species belonging to four different families (Apiaceae, Brassicaceae,
   Asteraceae, and Apocynaceae) have been used in this study to build a
   deep learning analysis dataset. Further, the dataset has been augmented
   to 3800 images, divided to 2660 images for training and 1440 for
   testing, and different 14 fruit images belonging to the same families
   have been used for prediction of the testing module. A novel Convolution
   Neural Network (CNN) model architecture has been proposed to extract the
   fruit features, classify each image with its family, and use the trained
   model to predict that the new fruits belong to the same four families.
   The maximum accuracy obtained for the training and testing module was
   99.82\%. The prediction for this module succeeded by 93\% since all
   fruits' success predicted was attained except one from the family number
   2 (Brassicaceae). The same dataset was applied to two different models
   to evaluate our proposed model, the Deep learning model, aka Residual
   Neural Network, 20 layers (ResNet-20), and Support Vector Machine (SVM).
   The proposed CNN model achieved higher accuracy and efficiency than the
   ResNet-20 and SVM.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Nayyar, A (Corresponding Author), Duy Tan Univ, Fac Informat Technol, Grad Sch, Da Nang 550000, Vietnam.
   Ibrahim, Nehad M.; Rahman, Atta-ur, Imam Abdulrahman Bin Faisal Univ IAU, Coll Comp Sci \& Informat Technol CCSIT, Dept Comp Sci CS, POB 1982, Dammam 31441, Saudi Arabia.
   Gabr, Dalia Goda Ibrahim, Imam Abdulrahman Bin Faisal Univ IAU, Coll Sci, Dept Biol, POB 1982, Dammam 31441, Saudi Arabia.
   Dash, Sujata, North Orissa Univ, Dept Comp Applicat, Baripada 757003, Odisha, India.
   Nayyar, Anand, Duy Tan Univ, Fac Informat Technol, Grad Sch, Da Nang 550000, Vietnam.},
DOI = {10.1007/s11042-022-12942-9},
EarlyAccessDate = {MAR 2022},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Fruits identification; Plants classification; Deep learning; Convolution
   neural network; Image analysis; fruit and plant families},
Keywords-Plus = {PLANT CLASSIFICATION; RECOGNITION; SIMULATION; FRAMEWORK; SYSTEM},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {nmaibrahim@iau.edu.sa
   dggabr@iau.edu.sa
   aaurrahman@iau.edu.sa
   sujata238dash@gmail.com
   anandnayyar@duytan.edu.vn},
Affiliations = {Imam Abdulrahman Bin Faisal University; Imam Abdulrahman Bin Faisal
   University; Duy Tan University},
ResearcherID-Numbers = {Gabr, Dalia/GOP-2692-2022
   Dash, Sujata/I-8220-2019
   Nayyar, Anand/F-3732-2015
   Rahman, Atta ur/AAD-6541-2019
   Ibrahim, Nehad/AAQ-7940-2021
   },
ORCID-Numbers = {Nayyar, Anand/0000-0002-9821-6146
   Rahman, Atta ur/0000-0001-6696-277X
   Ibrahim, Nehad/0000-0002-7681-5594
   Gabr, Dalia/0000-0002-8603-6877},
Cited-References = {Ahmad M, 2020, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-01943-x.
   Alhaidari F, 2020, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-02448-3.
   Alotaibi SM, 2021, CMC-COMPUT MATER CON, V68, P149, DOI 10.32604/cmc.2021.015976.
   Atta-ur-Rahman, 2021, CMC-COMPUT MATER CON, V69, P21, DOI 10.32604/cmc.2021.013453.
   Atta-ur-Rahman, 2019, J CLOUD COMPUT-ADV S, V8, DOI 10.1186/s13677-019-0144-9.
   Atta-ur-Rahman, 2018, COMPUT MATH METHOD M, V2018, DOI 10.1155/2018/3461382.
   Behera R., 2016, INT J HYBRID INTELL, V13, P77, DOI DOI 10.3233/HIS-160226.
   Biswas S, 2019, COMM COM INF SC, V955, P540, DOI 10.1007/978-981-13-3140-4\_49.
   Boulent J, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.00941.
   Dash S., 2016, INT J KNOWLEDGE DISC, V6, P1, DOI DOI 10.4018/IJKDB.2016010101.
   Dash S, 2020, INT J DISTRIB SENS N, V16, DOI 10.1177/1550147719895210.
   Dash S, 2019, INT J SWARM INTELL R, V10, P1, DOI 10.4018/IJSIR.2019040101.
   Dash S, 2019, SCALABLE COMPUT-PRAC, V20, P191, DOI 10.12694/scpe.v20i2.1504.
   Dileep MR, 2019, TENCON IEEE REGION, P321, DOI 10.1109/TENCON.2019.8929394.
   Grillo O, 2017, COMPUT ELECTRON AGR, V141, P223, DOI 10.1016/j.compag.2017.07.024.
   Gyires-Toth BP, 2019, CYBERN INF TECHNOL, V19, P88, DOI 10.2478/cait-2019-0005.
   Haupt, 2018, CEUR WORKSHOP PROC, V2125, P1.
   He K., 2016, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2016.90.
   Hossain MS, 2019, IEEE T IND INFORM, V15, P1027, DOI 10.1109/TII.2018.2875149.
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.
   Jana BK., 2012, INDIAN J FUNDAMENTAL, V2, P51.
   Johannes A, 2017, COMPUT ELECTRON AGR, V138, P200, DOI 10.1016/j.compag.2017.04.013.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Khan MA, 2020, CMC-COMPUT MATER CON, V65, P139, DOI 10.32604/cmc.2020.011416.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Le, 2015, P WORK NOT CLEF 2015.
   Lee JH, 2019, IEEE GLOBE WORK, DOI {[}10.1109/ICAIIC.2019.8669002, 10.1109/gcwkshps45667.2019.9024382].
   Mahmud M, 2020, OPT LASER TECHNOL, V121, DOI 10.1016/j.optlastec.2019.105818.
   Naseem MT, 2020, NEURAL NETW WORLD, V30, P177, DOI 10.14311/NNW.2020.30.013.
   Panda M, 2019, CCIS, V955, P1, DOI {[}10.1007/978-981-13-3140-4, DOI 10.1007/978-981-13-3140-4].
   Patra BN., 2016, INT J LATEST TRENDS, V7, P8, DOI {[}10.21172/1.72.502, DOI 10.21172/1.72.502].
   Picon A, 2019, COMPUT ELECTRON AGR, V161, P280, DOI 10.1016/j.compag.2018.04.002.
   RAHMAN A, 2020, J AMB INTEL HUM COMP.
   Rahman A., 2017, IEEE T IND INFORM, V10, P95.
   Rahman A, 2019, J INTELL FUZZY SYST, V37, P553, DOI 10.3233/JIFS-162405.
   Rahman A, 2019, J INTELL FUZZY SYST, V37, P1545, DOI 10.3233/JIFS-18579.
   Rahman AU, 2018, J HEALTHC ENG, V2018, DOI 10.1155/2018/8137436.
   Rahman AU, 2021, TELECOMMUN SYST, V76, P49, DOI 10.1007/s11235-020-00700-x.
   Rehman A, 2020, J AMB INTEL SMART EN, V12, P125, DOI 10.3233/AIS-200554.
   Reyes AK, 2015, P WORK NOT CLEF.
   Sabzi Sajad, 2018, Information Processing in Agriculture, V5, P162, DOI 10.1016/j.inpa.2017.09.002.
   SHI Y, IEEE T MULTIMED.
   Singh R, 2020, EMERGING TECHNOLOGIE.
   Sungbin C, 2015, P WORK NOT CLEF.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Xiong ZT, 2021, IEEE T IMAGE PROCESS, V30, P2722, DOI 10.1109/TIP.2021.3053459.
   Zagrouba R, 2021, CMC-COMPUT MATER CON, V66, P2397, DOI 10.32604/cmc.2021.014042.
   Zaman G, 2021, IEEE ACCESS, V9, P42111, DOI 10.1109/ACCESS.2021.3063181.
   Zhang C, 2021, NEUROCOMPUTING, V449, P189, DOI 10.1016/j.neucom.2021.03.103.
   Zhang HX, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P25, DOI 10.1109/MIPR.2018.00013.
   Zheng Y, 2017, COMPUT ELECTRON AGR, V141, P215, DOI 10.1016/j.compag.2017.07.028.},
Number-of-Cited-References = {53},
Times-Cited = {13},
Usage-Count-Last-180-days = {10},
Usage-Count-Since-2013 = {18},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {3B2OW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000774644100011},
DA = {2023-08-12},
}

@article{ WOS:000675811100028,
Author = {Valarmathi, G. and Suganthi, S. U. and Subashini, V. and Janaki, R. and
   Sivasankari, R. and Dhanasekar, S.},
Title = {CNN algorithm for plant classification in deep learning},
Journal = {MATERIALS TODAY-PROCEEDINGS},
Year = {2021},
Volume = {46},
Number = {9},
Pages = {3684-3689},
Note = {International Conference on Materials, Manufacturing and Mechanical
   Engineering for Sustainable Developments (ICMSD), Sri Sairam Inst
   Technol, Chennai, INDIA, NOV 19-20, 2020},
Abstract = {The prior methodology of characterizing the plants for dependent on
   surface based order and another strategy depends on KNN classifier. This
   paper presents qualities examination of plants utilizing picture
   preparing methods for robotized vision framework utilized at
   horticultural field. In farming examination, the programmed plant
   attributes recognition is fundamental one in observing huge field. The
   proposed dynamic framework uses picture content portrayal and regulated
   classifier sort of neural organization. This will naturally distinguish
   the plant species when we import its picture as info. Picture preparing
   strategies for this sort of choice investigation includes pre-processing
   and characterization stage. At Processing, an info picture will be
   resized and commotion expulsion procedure is applied. At definite stage
   the neural organization orders the pictures as farming plant, harmful
   plant and therapeutic plant separately. At that point it will show the
   attributes of each plant. (c) 2020 Elsevier Ltd. All rights reserved.
   Selection and peer-review under responsibility of the scientific
   committee of the International Conference on Materials, Manufacturing
   and Mechanical Engineering for Sustainable Developments-2020.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Valarmathi, G (Corresponding Author), Sri Sairam Inst Technol, ECE, Chennai 600044, Tamil Nadu, India.
   Valarmathi, G.; Suganthi, S. U.; Subashini, V.; Janaki, R.; Sivasankari, R., Sri Sairam Inst Technol, ECE, Chennai 600044, Tamil Nadu, India.
   Dhanasekar, S., Sri Eshwar Coll Engn, ECE, Coimbatore 641202, Tamil Nadu, India.},
DOI = {10.1016/j.matpr.2021.01.847},
EarlyAccessDate = {JUL 2021},
ISSN = {2214-7853},
Keywords = {Plant classification; kNN algorithm; SVM classifier; Feature extraction;
   CNN algorithm},
Research-Areas = {Materials Science},
Web-of-Science-Categories  = {Materials Science, Multidisciplinary},
Author-Email = {Valarmathi.ece@sairamit.edu.in},
ResearcherID-Numbers = {Su, Suganthi/ABH-9157-2020
   V, Subashini/AAX-5080-2021
   G, Valarmathi/AAE-3877-2022
   SUBRAMANIYAM, DHANASEKAR/F-1339-2018
   R, JANAKI/AAE-3847-2022},
ORCID-Numbers = {Su, Suganthi/0000-0002-7585-8883
   V, Subashini/0000-0001-7042-4018
   G, Valarmathi/0000-0003-2844-0553
   SUBRAMANIYAM, DHANASEKAR/0000-0002-7660-2265
   R, JANAKI/0000-0002-9182-8078},
Cited-References = {Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Huang W., 2014, IEEE J SELECTED TOP, V7.
   Husin Zulkifli Bin, 2012 3 INT C INT SYS 2012 3 INT C INT SYS.
   Jhuria M, 2013, 2013 IEEE SECOND INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P521, DOI 10.1109/ICIIP.2013.6707647.
   Ramakrishnan M, IEEE ICCSP 2015 C.
   Saxena L., 2014, SURVEY IMAGE PROCESS.
   Thangadurai K., 2014 WORLD C COMP CO 2014 WORLD C COMP CO.},
Number-of-Cited-References = {7},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Journal-ISO = {Mater. Today-Proc.},
Doc-Delivery-Number = {TM8QV},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000675811100028},
DA = {2023-08-12},
}

@article{ WOS:000876301000028,
Author = {Sukumar, G. and Priya, W. Deva},
Title = {Classification of Flower Species based on Flower Texture to Improve
   Accuracy of Classifier using Linear Regression and Comparing with SVM
   algorithm},
Journal = {JOURNAL OF PHARMACEUTICAL NEGATIVE RESULTS},
Year = {2022},
Volume = {13},
Number = {SI},
Pages = {612-618},
Abstract = {Aim: Classification of flower species based on innovative flower texture
   to improve accuracy of classifier using linear regression and comparing
   with SVM algorithm. Methods and Materials: Flower species recognition is
   performed using Linear Regression (N=10) over SVM (N=10) with the split
   size of training and testing dataset 70\% and 30\% respectively.
   Calculation of samples is done by using G power of 80\% which contains
   two different groups, alpha (0.05), power (80\%) and environment ratio
   1. Results: Linear Regression has significantly better accuracy (95.9\%)
   compared to SVM (93.3\%) and attained significance value of p = 0.01.
   Conclusion: Linear regression achieved significantly better flower
   recognition than SVM for identifying the different types of flower
   species.},
Publisher = {RESEARCHTRENTZ ACAD PUBL EDUCATION SERVICES},
Address = {240 Elm Street, 2nd \& 3rd Floors, Somerville, MA, UNITED STATES},
Type = {Article},
Language = {English},
Affiliation = {Sukumar, G (Corresponding Author), Saveetha Univ, Saveetha Inst Med \& Tech Sci, Saveetha Sch Engn, Dept Comp Sci \& Engn, Chennai 602105, Tamil Nadu, India.
   Sukumar, G.; Priya, W. Deva, Saveetha Univ, Saveetha Inst Med \& Tech Sci, Saveetha Sch Engn, Dept Comp Sci \& Engn, Chennai 602105, Tamil Nadu, India.},
DOI = {10.47750/pnr.2022.13.S04.068},
ISSN = {0976-9234},
EISSN = {2229-7723},
Keywords = {Machine Learning; Linear Regression; Innovative Flower Texture; SVM;
   Flower Species; Flower Recognition},
Research-Areas = {Pharmacology \& Pharmacy},
Web-of-Science-Categories  = {Pharmacology \& Pharmacy},
Affiliations = {Saveetha Institute of Medical \& Technical Science; Saveetha School of
   Engineering},
Funding-Acknowledgement = {TRIX.edu, Hyderabad; Saveetha University; Saveetha Institute of Medical
   and Technical Sciences; Saveetha School of Engineering},
Funding-Text = {We thank the following organizations for providing financial support
   that enabled us to complete the study. 1. TRIX.edu, Hyderabad. 2.
   Saveetha University 3. Saveetha Institute of Medical and Technical
   Sciences. 4. Saveetha School of Engineering},
Cited-References = {Adhinarayanan R, 2020, ENERG SOURCE PART A, DOI 10.1080/15567036.2020.1773967.
   Arya V, 2014, PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON RELIABILTY, OPTIMIZATION, \& INFORMATION TECHNOLOGY (ICROIT 2014), P161, DOI 10.1109/ICROIT.2014.6798304.
   Aurtherson PB, 2023, BIOMASS CONVERS BIOR, V13, P6249, DOI 10.1007/s13399-021-01551-5.
   Bhansali KJ, 2021, FUEL, V304, DOI 10.1016/j.fuel.2021.121490.
   Bishop Ch. M., 2006, PATTERN RECOGN.
   Bonnet P., 2018, MULTIMEDIA TOOLS APP.
   Cervantes J, 2020, NEUROCOMPUTING, V408, P189, DOI 10.1016/j.neucom.2019.10.118.
   Deepanraj B, 2022, BIOMASS CONVERS BIOR, V12, P1715, DOI 10.1007/s13399-021-01312-4.
   Garcia JE, 2021, PLANT BIOLOGY, V23, P905, DOI 10.1111/plb.13326.
   Gogul I, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION AND NETWORKING (ICSCN).
   Hudson IL, 2010, PHENOLOGICAL RESEARCH: METHODS FOR ENVIRONMENTAL AND CLIMATE CHANGE ANALYSIS, P361, DOI 10.1007/978-90-481-3335-2\_17.
   Jayanth BV, 2021, ENERG SOURCE PART A, DOI 10.1080/15567036.2021.1924313.
   Joly A, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P551, DOI 10.1145/3123266.3129312.
   Kamath MS, 2020, PROCESS BIOCHEM, V99, P36, DOI 10.1016/j.procbio.2020.08.015.
   Mader Patrick, 2021, AUTHOR RESPONSETHE F, DOI {[}10.1111/2041-210x.13611/v2/response1, DOI 10.1111/2041-210X.13611/V2/RESPONSE1].
   Mouine S., 2012, P 2 ACM INT C MULT R, P1.
   Poojitha V, 2016, 2016 6TH INTERNATIONAL CONFERENCE - CLOUD SYSTEM AND BIG DATA ENGINEERING (CONFLUENCE), P53, DOI 10.1109/CONFLUENCE.2016.7508047.
   Prakash VRA, 2022, BIOMASS CONVERS BIOR, V12, P5451, DOI 10.1007/s13399-020-00938-0.
   Rajasekaran S, 2020, FUEL, V277, DOI 10.1016/j.fuel.2020.118166.
   Rajesh A, 2020, FUEL, V278, DOI 10.1016/j.fuel.2020.118315.
   Raju P, 2023, BIOMASS CONVERS BIOR, V13, P1143, DOI 10.1007/s13399-021-01427-8.
   Sathiyamoorthi R, 2021, ENVIRON PROG SUSTAIN, V40, DOI 10.1002/ep.13696.
   Seeland M, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-018-2474-x.
   Seeland M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170629.
   Seidemann J., 2005, WORLD SPICE PLANTS E, V1st.
   Shanmugam R, 2021, ENERG SOURCE PART A, V43, P3064, DOI 10.1080/15567036.2020.1833112.
   Sudhakar MP, 2021, FUEL, V306, DOI 10.1016/j.fuel.2021.121680.
   Zawbaa HM, 2014, 2014 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P895, DOI 10.1109/ICACCI.2014.6968612.},
Number-of-Cited-References = {28},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {3},
Journal-ISO = {J. Pharm. Negat. Results},
Doc-Delivery-Number = {5U1GI},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000876301000028},
OA = {Bronze},
DA = {2023-08-12},
}

@inproceedings{ WOS:000413240500053,
Author = {Wagenaar, Martijn and Okafor, Emmanuel and Frencken, Wouter and Wiering,
   Marco A.},
Editor = {DeMarsico, M and DiBaja, GS and Fred, A},
Title = {Using Deep Convolutional Neural Networks to Predict Goal-scoring
   Opportunities in Soccer},
Booktitle = {ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN
   RECOGNITION APPLICATIONS AND METHODS},
Year = {2017},
Pages = {448-455},
Note = {6th International Conference on Pattern Recognition Applications and
   Methods (ICPRAM), Porto, PORTUGAL, FEB 24-26, 2017},
Abstract = {Deep learning approaches have successfully been applied to several image
   recognition tasks, such as face, object, animal and plant
   classification. However, almost no research has examined on how to use
   the field of machine learning to predict goal-scoring opportunities in
   soccer from position data. In this paper, we propose the use of deep
   convolutional neural networks (DCNNs) for the above stated problem. This
   aim is actualized using the following steps: 1) development of novel
   algorithms for finding goal-scoring opportunities and ball possession
   which are used to obtain positive and negative examples. The dataset
   consists of position data from 29 matches played by a German Bundlesliga
   team. 2) These examples are used to create original and enhanced images
   (which contain object trails of soccer positions) with a resolution size
   of 256x256 pixels. 3) Both the original and enhanced images are fed
   independently as input to two DCNN methods: instances of both GoogLeNet
   and a 3-layered CNN architecture. A K-nearest neighbor classifier was
   trained and evaluated on ball positions as a baseline experiment. The
   results show that the GoogLeNet architecture outperforms all other
   methods with an accuracy of 67.1\%.},
Publisher = {SCITEPRESS},
Address = {AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Wagenaar, M (Corresponding Author), Univ Groningen, Inst Artificial Intelligence \& Cognit Engn ALICE, Groningen, Netherlands.
   Wagenaar, Martijn; Okafor, Emmanuel; Wiering, Marco A., Univ Groningen, Inst Artificial Intelligence \& Cognit Engn ALICE, Groningen, Netherlands.
   Frencken, Wouter, Football Club Groningen, Groningen, Netherlands.
   Frencken, Wouter, Univ Groningen, Ctr Human Movement Sci, Groningen, Netherlands.},
DOI = {10.5220/0006194804480455},
ISBN = {978-989-758-222-6},
Keywords = {Convolutional Neural Networks; Goal-scoring Opportunities in Soccer;
   Image Recognition},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {m.wagenaar.2@student.rug.nl
   e.okafor@rug.nl
   wouterfrencken@fcgroningen.nl
   m.a.wiering@rug.nl},
Affiliations = {University of Groningen; University of Groningen},
ResearcherID-Numbers = {Wiering, Marco/C-5909-2012
   Okafor, Emmanuel/AAB-5120-2019
   },
ORCID-Numbers = {Wiering, Marco/0000-0003-4331-7537
   Okafor, Emmanuel/0000-0001-6929-6880},
Cited-References = {{[}Anonymous], 2013, INT C MACHINE LEARNI.
   {[}Anonymous], 1995, P ADV NEUR INF PROC.
   {[}Anonymous], 1983, SOV MATH DOKL.
   Barris S, 2008, SPORTS MED, V38, P1025, DOI 10.2165/00007256-200838120-00006.
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181.
   Brooks J, 2016, STAT ANAL DATA MIN, V9, P338, DOI 10.1002/sam.11318.
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Grunz A, 2012, HUM MOVEMENT SCI, V31, P334, DOI 10.1016/j.humov.2011.02.008.
   JACOBS RA, 1988, NEURAL NETWORKS, V1, P295, DOI 10.1016/0893-6080(88)90003-2.
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Knauf K, 2016, MACH LEARN, V102, P247, DOI 10.1007/s10994-015-5520-1.
   Kohonen T, 1998, NEUROCOMPUTING, V21, P19, DOI 10.1016/S0925-2312(98)00031-9.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   LeCun Y., 1989, ADV NEURAL INF PROCE, V2, P568.
   Memmert D, 2017, SPORTS MED, V47, P1, DOI 10.1007/s40279-016-0562-5.
   Nair V., 2010, ICML 10 PROC 27 INT.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.},
Number-of-Cited-References = {19},
Times-Cited = {8},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {9},
Doc-Delivery-Number = {BI6GD},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000413240500053},
OA = {hybrid, Green Submitted},
DA = {2023-08-12},
}

@article{ WOS:000629347400009,
Author = {Pronozin, A. Yu and Paulish, A. A. and Zavarzin, E. A. and Prikhodko, A.
   Yu and Prokhoshin, N. M. and Kruchinina, V, Yu and Goncharov, N. P. and
   Komyshev, E. G. and Genaev, M. A.},
Title = {Automatic morphology phenotyping of tetra- and hexaploid wheat spike
   using computer vision methods},
Journal = {VAVILOVSKII ZHURNAL GENETIKI I SELEKTSII},
Year = {2021},
Volume = {25},
Number = {1},
Pages = {71-81},
Abstract = {Intraspecific classification of cultivated plants is necessary for the
   conservation of biological diversity, study of their origin and their
   phylogeny. The modern cultivated wheat species originated from three
   wild diploid ancestors as a result of several rounds of genome doubling
   and are represented by di-, tetra- and hexaploid species. The
   identification of wheat ploidy level is one of the main stages of their
   taxonomy. Such classification is possible based on visual analysis of
   the wheat spike traits. The aim of this study is to investigate the
   morphological characteristics of spikes for hexa- and tetraploid wheat
   species based on the method of high-performance phenotyping. Phenotyping
   of the quantitative characteristics of the spike of 17 wheat species
   (595 plants, 3348 images), including eight tetraploids (Triticum
   aethiopicum, T. dicoccoides, T. dicoccum, T. durum, T. militinae, T.
   polonicum, T. timopheevii, and T. turgidum) and nine hexaploids (T.
   compactum, T. aestivum, i:ANK-23 (near-isogenic line of T. aestivum cv.
   Novosibirskaya 67), T. antiquorum, T. spelta (including cv. Rother
   Sommer Kolben), T. petropavlovskyi, T. yunnanense, T. macha, T.
   sphaerococcum, and T. vavilovii), was performed. Wheat spike morphology
   was described on the basis of nine quantitative traits including shape,
   size and awns area of the spike. The traits were obtained as a result of
   image analysis using the WERecognizer program. A cluster analysis of
   plants according to the characteristics of the spike shape and
   comparison of their distributions in tetraploid and hexaploid species
   showed a higher variability of traits in hexaploid species compared to
   tetraploid ones. At the same time, the species themselves form two
   clusters in the visual characteristics of the spike. One type is
   predominantly hexaploid species (with the exception of one tetraploid,
   T. dicoccoides). The other group includes tetraploid ones (with the
   exception of three hexaploid ones, T. compactum, T. antiquorum, T.
   sphaerococcum, and i:ANK-23). Thus, it has been shown that the
   morphological characteristics of spikes for hexaploid and tetraploid
   wheat species, obtained on the basis of computer analysis of images,
   include differences, which are further used to develop methods for plant
   classifications by ploidy level and their species in an automatic mode.},
Publisher = {RUSSIAN ACAD SCI, INST CYTOLOGY GENETICS},
Address = {PR LAVRENTYEVA 10, NOVOSIBIRSK, 630090, RUSSIA},
Type = {Article},
Language = {English},
Affiliation = {Genaev, MA (Corresponding Author), Russian Acad Sci, Siberian Branch, Inst Cytol \& Genet, Novosibirsk, Russia.
   Genaev, MA (Corresponding Author), Novosibirsk State Univ, Novosibirsk, Russia.
   Genaev, MA (Corresponding Author), Russian Acad Sci, Siberian Branch, Inst Cytol \& Genet, Kurchatov Genom Ctr, Novosibirsk, Russia.
   Pronozin, A. Yu; Kruchinina, Yu, V; Goncharov, N. P.; Komyshev, E. G.; Genaev, M. A., Russian Acad Sci, Siberian Branch, Inst Cytol \& Genet, Novosibirsk, Russia.
   Paulish, A. A.; Zavarzin, E. A.; Prikhodko, A. Yu; Prokhoshin, N. M.; Komyshev, E. G.; Genaev, M. A., Novosibirsk State Univ, Novosibirsk, Russia.
   Kruchinina, Yu, V; Komyshev, E. G.; Genaev, M. A., Russian Acad Sci, Siberian Branch, Inst Cytol \& Genet, Kurchatov Genom Ctr, Novosibirsk, Russia.
   Goncharov, N. P., Novosibirsk State Agr Univ, Novosibirsk, Russia.},
DOI = {10.18699/VJ21.009},
ISSN = {2500-0462},
EISSN = {2500-3259},
Keywords = {wheat spike morphology; wheat; phenomics; image processing; computer
   vision; machine learning; biotechnology},
Keywords-Plus = {TRITICUM L.; PHENOMICS},
Research-Areas = {Agriculture; Genetics \& Heredity},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Genetics \& Heredity},
Author-Email = {mag@bionet.nsc.ru},
Affiliations = {Russian Academy of Sciences; Institute of Cytology \& Genetics ICG SB
   RAS; Novosibirsk State University; Russian Academy of Sciences;
   Institute of Cytology \& Genetics ICG SB RAS; Novosibirsk State Agrarian
   University},
ResearcherID-Numbers = {Kruchinina, Yulia/AFE-6628-2022
   Goncharov, Nikolay P./HPD-8088-2023
   Prikhodko, Alexey/AAC-8816-2022
   },
ORCID-Numbers = {Prikhodko, Alexey/0000-0002-8366-6240
   Kruchinina, Yulia/0000-0002-1084-9521
   Pronozin, Artem/0000-0002-3011-6288},
Funding-Acknowledgement = {Kurchatov Genome Center of the Institute of Cytology and Genetics of
   Siberian Branch of the Russian Academy of Sciences; Ministry of
   Education and Science of the Russian Federation {[}075-15-2019-1662];
   Russian Science Foundation {[}16-16-10021]; Ministry of Science and
   Higher Education of the Russian Federation {[}075-15-2019-1675]},
Funding-Text = {Preparation of spike samples, phenotyping, development of algorithms for
   shape analysis and classification were funded by the Kurchatov Genome
   Center of the Institute of Cytology and Genetics of Siberian Branch of
   the Russian Academy of Sciences, agreement with the Ministry of
   Education and Science of the Russian Federation No. 075-15-2019-1662.
   Cultivation of experimental plants and de visu determination of their
   species belonging by the traits determining the architec-tonics of the
   spike were supported by grant of Russian Science Foundation 16-16-10021.
   The data were processed using the resources of the Bioinformatics Center
   supported by the budget project No. 0259-2021-0009. The authors A.A.P.,
   E.A.Z., A.Yu.P., and N.M.P. were supported by the Mathematical Center in
   Akademgorodok, agreement with the Ministry of Science and Higher
   Education of the Russian Federation No. 075-15-2019-1675. The authors
   are grateful to D.A. Afonnikov for comments and recommendations during
   the work and I.G. Chukhina (VIR, St. Petersburg) for providing photos of
   herbarium specimens of T. petropavlovskyi.},
Cited-References = {Afonnikov DA, 2016, RUSS J GENET+, V52, P688, DOI 10.1134/S1022795416070024.
   {[}Anonymous], 2011, COMPUT SCI.
   {[}Anonymous], 1979, FLORA CULTIVATED PLA, DOI DOI 10.1007/s10745-011-9405-z.
   {[}Anonymous], 1983, STAT METHODS.
   Bannikova SV, 2004, J GENET BREED, V58, P273.
   Boguslavskii R. L., 1982, Byulleten' Vsesoyuznogo Ordena Lenina i Ordena Druzhby Narodov Instituta Rastenievodstva Imeni N. I. Vavilova, P73.
   Chen ZJ, 2007, ANNU REV PLANT BIOL, V58, P377, DOI 10.1146/annurev.arplant.58.032806.103835.
   Comai L, 2005, NAT REV GENET, V6, P836, DOI 10.1038/nrg1711.
   Demidchik VV, 2020, RUSS J PLANT PHYSL+, V67, P397, DOI 10.1134/S1021443720030061.
   Dorofeev V.F., 1985, DOKLADY VASKHNIL REP, V9, P1.
   Dorofeev V.F., 1984, INT COMECON LIST DES.
   Finigan P., 2012, POLYPLOIDY GENOME EV, P57, DOI 10.1007/978-3-642-31442-1\_4.
   Genaev MA, 2018, VAVILOVSKII ZH GENET, V22, P132, DOI 10.18699/VJ18.340.
   Genaev MA, 2019, AGRONOMY-BASEL, V9, DOI 10.3390/agronomy9070390.
   Golovnina KA, 2009, RUSS J GENET+, V45, P1360, DOI 10.1134/S1022795409110106.
   Goncharov N. P., 2005, Czech Journal of Genetics and Plant Breeding, V41, P52.
   {[}Гончаров Н.П. Goncharov N.P.], 2008, {[}Вавиловский журнал генетики и селекции, Vavilovskii zhurnal genetiki i selektsii], V12, P159.
   Goncharov N.P., 2008, GENET SEL, V12, P509.
   Goncharov N.P., 2009, MANUAL BOOK COMMON H.
   Goncharov NP, 2011, PLANT SYST EVOL, V295, P1, DOI 10.1007/s00606-011-0480-9.
   Hammer K, 2011, GENET RESOUR CROP EV, V58, P3, DOI 10.1007/s10722-010-9590-4.
   JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588.
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260.
   Koval SF, 1997, RUSS J GENET, V33, P995.
   Liu WT, 2018, PLANT CELL TISS ORG, V134, P183, DOI 10.1007/s11240-018-1411-z.
   Palmova E.F., 1935, INTRO WHEAT ECOLOGY.
   Robinson DO, 2018, PLANT CELL, V30, P2308, DOI 10.1105/tpc.18.00344.
   Rodionov A. V, 2020, Botanicheskii Zhurnal (St. Petersburg), V105, P835, DOI 10.31857/S0006813620070091.
   Rodionov AV, 2019, RUSS J GENET+, V55, P278, DOI {[}10.1134/S1022795419030141, 10.1134/S0016675819030159].
   Romanov B.V., 2018, PHENOMOGENOMICS PROD.
   Sinskaya E.N., 1969, HIST GEOGRAPHY CULTU.
   Tan FQ, 2015, BMC PLANT BIOL, V15, DOI 10.1186/s12870-015-0450-4.
   Udachin R.A., 1970, VESTNIK SELSKOKHOZYA, V9, P20.
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579.
   van Slageren M, 2013, KEW BULL, V68, P477, DOI 10.1007/s12225-013-9459-8.
   Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2.
   Williams JH, 2020, ANN BOT-LONDON, V125, P925, DOI 10.1093/aob/mcaa007.
   Yakubtsiner M.M., 1959, BOT ZH, V44, P1425.
   Yang WN, 2020, MOL PLANT, V13, P187, DOI 10.1016/j.molp.2020.01.008.
   Zatybekov A, 2020, VAVILOVSKII ZH GENET, V24, P605, DOI 10.18699/VJ20.654.
   Zhao CJ, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.00714.
   Zuev E.V., 2019, ATLAS DIVERSITY SOFT.},
Number-of-Cited-References = {42},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {3},
Journal-ISO = {Vavilovskii Zhurnal Genet. Sel.},
Doc-Delivery-Number = {QX4WB},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000629347400009},
OA = {Green Published},
DA = {2023-08-12},
}

@article{ WOS:000708158900008,
Author = {Karahan, Tolgahan and Nabiyev, Vasif},
Title = {Plant identification with convolutional neural networks and transfer
   learning},
Journal = {PAMUKKALE UNIVERSITY JOURNAL OF ENGINEERING SCIENCES-PAMUKKALE
   UNIVERSITESI MUHENDISLIK BILIMLERI DERGISI},
Year = {2021},
Volume = {27},
Number = {5},
Pages = {638-645},
Abstract = {Nature is rich with a vast amount of plant and flower species and
   because of their great diversity; identification of these species
   requires expertise in the field. Development of an automatic plant
   identification system can ease this process. In this work, deep
   Convolutional Neural Networks and Transfer Learning have been utilized
   in order to develop such an identification system. Images in the
   database have been collected from other databases and the web and in
   total it consists of 5,345 flowers and plant images belong to 76
   species. 65 of the species are various flower species and 11 of them are
   other plant species. Data augmentation techniques has been applied in
   order to increase the number of images in the database and to improve
   the generalization capacity of the model. For data augmentation, random
   rotation at four angles, random brightness change in the range of
   {[}-0.2, 0.2] and horizontal flip have been applied. Also preprocessing
   techniques such as center cropping and normalizing have been applied to
   images before input them to the model. In automatic plant recognition,
   0.9971 accuracy achieved on the training set and 0.9897 accuracy
   achieved on the test set.},
Publisher = {PAMUKKALE UNIV},
Address = {CAMPUS INCILIPINAR, DENIZLI, 20020, TURKEY},
Type = {Article},
Language = {English},
Affiliation = {Nabiyev, V (Corresponding Author), Karadeniz Tech Univ, Engn Fac, Comp Engn Dept, Trabzon, Turkey.
   Karahan, Tolgahan; Nabiyev, Vasif, Karadeniz Tech Univ, Engn Fac, Comp Engn Dept, Trabzon, Turkey.},
DOI = {10.5505/pajes.2020.84042},
ISSN = {1300-7009},
EISSN = {2147-5881},
Keywords = {Artificial intelligence; Deep learning; Convolutional neural networks;
   Identification; Classification},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Multidisciplinary},
Author-Email = {karahantlg@gmail.com
   vasif@ktu.edu.tr},
Affiliations = {Karadeniz Technical University},
Cited-References = {{[}Anonymous], INTRO CONVOLUTIONAL.
   Ataturk Orman Ciftligi, 2020, ATATURK ORMAN CIFTLI.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Github, 2020, TOLGA KARAHAN MOBILE.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Howard A. G., 2017, EFFICIENT CONVOLUTIO.
   ImageNet, 2020, IMAGENET LARGE SCALE.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lasseck Mario, 2017, CLEF.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Machine Think, 2020, MOB VERS 2.
   Mellon M, 2015, PRECISION RECALL F1, P413.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0.
   Sulc M., 2018, CLEF WORKING NOTES.
   Tan CQ, 2018, LECT NOTES COMPUT SC, V11141, P270, DOI 10.1007/978-3-030-01424-7\_27.
   University of Oxford, 102 CATEGORY FLOWER.
   Wang J., EFFECTIVENESS DATA A.
   Wikipedia Commons, 2020, FILE TYPICAL CNNPNG.
   Yabani Cicekler, 2020, YABAN CICEKLER.
   Yosinski J, 2014, ADV NEUR IN, V27.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zidek J, 2012, STRATIFIED SAMPLING, P73.},
Number-of-Cited-References = {28},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Journal-ISO = {Pamukkale Univ. J. Eng. Sci.},
Doc-Delivery-Number = {WI1WP},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000708158900008},
OA = {Green Submitted, gold},
DA = {2023-08-12},
}

@article{ WOS:000678776100001,
Author = {Scholl, Victoria M. and McGlinchy, Joseph and Price-Broncucia, Teo and
   Balch, Jennifer K. and Joseph, Maxwell B.},
Title = {Fusion neural networks for plant classification: learning to combine
   RGB, hyperspectral, and lidar data},
Journal = {PEERJ},
Year = {2021},
Volume = {9},
Month = {JUL 29},
Abstract = {Airborne remote sensing offers unprecedented opportunities to
   efficiently monitor vegetation, but methods to delineate and classify
   individual plant species using the collected data are still actively
   being developed and improved. The Integrating Data science with Trees
   and Remote Sensing (IDTReeS) plant identification competition openly
   invited scientists to create and compare individual tree mapping
   methods. Participants were tasked with training taxon identification
   algorithms based on two sites, to then transfer their methods to a third
   unseen site, using field-based plant observations in combination with
   airborne remote sensing image data products from the National Ecological
   Observatory Network (NEON). These data were captured by a high
   resolution digital camera sensitive to red, green, blue (RGB) light,
   hyperspectral imaging spectrometer spanning the visible to shortwave
   infrared wavelengths, and lidar systems to capture the spectral and
   structural properties of vegetation. As participants in the IDTReeS
   competition, we developed a two-stage deep learning approach to
   integrate NEON remote sensing data from all three sensors and classify
   individual plant species and genera. The first stage was a convolutional
   neural network that generates taxon probabilities from RGB images, and
   the second stage was a fusion neural network that ``learns{''} how to
   combine these probabilities with hyperspectral and lidar data. Our
   two-stage approach leverages the ability of neural networks to flexibly
   and automatically extract descriptive features from complex image data
   with high dimensionality. Our method achieved an overall classification
   accuracy of 0.51 based on the training set, and 0.32 based on the test
   set which contained data from an unseen site with unknown taxa classes.
   Although transferability of classification algorithms to unseen sites
   with unknown species and genus classes proved to be a challenging task,
   developing methods with openly available NEON data that will be
   collected in a standardized format for 30 years allows for continual
   improvements and major gains for members of the computational ecology
   community. We outline promising directions related to data preparation
   and processing techniques for further investigation, and provide our
   code to contribute to open reproducible science efforts.},
Publisher = {PEERJ INC},
Address = {341-345 OLD ST, THIRD FLR, LONDON, EC1V 9LL, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Scholl, VM (Corresponding Author), Univ Colorado, Cooperat Inst Res Environm Sci, Earth Lab, Boulder, CO 80309 USA.
   Scholl, VM (Corresponding Author), Univ Colorado, Dept Geog, Boulder, CO 80309 USA.
   Scholl, Victoria M.; McGlinchy, Joseph; Balch, Jennifer K.; Joseph, Maxwell B., Univ Colorado, Cooperat Inst Res Environm Sci, Earth Lab, Boulder, CO 80309 USA.
   Scholl, Victoria M.; Balch, Jennifer K., Univ Colorado, Dept Geog, Boulder, CO 80309 USA.
   Price-Broncucia, Teo, Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.},
DOI = {10.7717/peerj.11790},
Article-Number = {e11790},
ISSN = {2167-8359},
Keywords = {Machine learning; Deep learning; Species classification; Remote sensing;
   Airborne remote sensing; National Ecological Observatory Network; Data
   science competition; Neural networks; Open science},
Keywords-Plus = {TREE SPECIES CLASSIFICATION; WAVE-FORM LIDAR; FOREST; ECOSYSTEMS;
   PARAMETERS; IMAGERY},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {Victoria.Scholl@colorado.edu},
Affiliations = {University of Colorado System; University of Colorado Boulder;
   University of Colorado System; University of Colorado Boulder;
   University of Colorado System; University of Colorado Boulder},
ORCID-Numbers = {Joseph, Maxwell/0000-0002-7745-9990
   Scholl, Victoria/0000-0002-2085-1449
   McGlinchy, Joseph/0000-0003-2135-0168},
Funding-Acknowledgement = {Earth Lab, through the University of Colorado at Boulder (CU Boulder)
   Grand Challenge Initiative; Cooperative Institute for Research in
   Environmental Sciences (CIRES) at CU Boulder},
Funding-Text = {Funding for this work was provided by Earth Lab, through the University
   of Colorado at Boulder (CU Boulder) Grand Challenge Initiative, and the
   Cooperative Institute for Research in Environmental Sciences (CIRES) at
   CU Boulder. The funders had no role in study design, data collection and
   analysis, decision to publish, or preparation of the manuscript.},
Cited-References = {Abdollahnejad A, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12223722.
   Anderson JE, 2008, REMOTE SENS ENVIRON, V112, P1856, DOI 10.1016/j.rse.2007.09.009.
   {[}Anonymous], 2020, PROD.
   Asner GP, 2008, REMOTE SENS ENVIRON, V112, P1912, DOI 10.1016/j.rse.2007.02.043.
   Asner GP, 2012, REMOTE SENS ENVIRON, V124, P454, DOI 10.1016/j.rse.2012.06.012.
   Ballanti L, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8060445.
   Bisong E., 2019, BUILDING MACHINE LEA, P59, DOI DOI 10.1007/978-1-4842-4470-8\_7.
   Brodrick PG, 2019, TRENDS ECOL EVOL, V34, P734, DOI 10.1016/j.tree.2019.03.006.
   Carpenter J, 2011, SCIENCE, V331, P698, DOI 10.1126/science.331.6018.698.
   Dalponte M, 2013, IEEE T GEOSCI REMOTE, V51, P2632, DOI 10.1109/TGRS.2012.2216272.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Diaz J, 2020, P INF SYST CRIS RESP.
   Dubayah RO, 2000, J FOREST, V98, P44.
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013.
   Fricker GA, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11192326.
   Gallery W, 2015, NEON AOP DIGITAL CAM.
   Garcia FD, 2009, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2009.6.
   GDAL/OGR contributors, 2020, GDAL OGR GEOSP DAT A.
   Gini R, 2018, ISPRS INT J GEO-INF, V7, DOI 10.3390/ijgi7080315.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Goulden T, 2019, NETWORK.
   Graves S, 2020, ZENODO.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   He K., 2016, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2016.90.
   He KS, 2011, DIVERS DISTRIB, V17, P381, DOI 10.1111/j.1472-4642.2011.00761.x.
   Heinzel J, 2011, INT J APPL EARTH OBS, V13, P152, DOI 10.1016/j.jag.2010.09.010.
   HOWARD J, 2020, INFORM INT INTERDISC, V11, DOI DOI 10.3390/INF011020108.
   Johnson BR, 2010, INT GEOSCI REMOTE SE, P2079, DOI 10.1109/IGARSS.2010.5654121.
   Joseph M, 2020, R PACKAGE VERSION 0.
   Jucker T, 2017, GLOBAL CHANGE BIOL, V23, P177, DOI 10.1111/gcb.13388.
   Kampe TU, 2010, J APPL REMOTE SENS, V4, DOI 10.1117/1.3361375.
   Kampe TU, 2010, INT SOC OPTICS PHOTO, V7809.
   Karpowicz B, 2015, NETWORK.
   Keller M, 2008, FRONT ECOL ENVIRON, V6, P282, DOI 10.1890/1540-9295(2008)6{[}282:ACSFTN]2.0.CO;2.
   Kerr JT, 2003, TRENDS ECOL EVOL, V18, P299, DOI 10.1016/S0169-5347(03)00071-5.
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260.
   Koenig K, 2016, FORESTS, V7, DOI 10.3390/f7090198.
   Korpela I, 2010, SILVA FENN, V44, P319, DOI 10.14214/sf.156.
   Krause K, 2015, NETWORK.
   Kulakowski D, 2003, J BIOGEOGR, V30, P1445, DOI 10.1046/j.1365-2699.2003.00912.x.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lefsky MA, 2002, BIOSCIENCE, V52, P19, DOI 10.1641/0006-3568(2002)052{[}0019:LRSFES]2.0.CO;2.
   Lucash MS, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2293.
   Ma L, 2019, ISPRS J PHOTOGRAMM, V152, P166, DOI 10.1016/j.isprsjprs.2019.04.015.
   Marconi S, 2019, PEERJ, V7, DOI 10.7717/peerj.5843.
   Maschler J, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081218.
   Moon JB, 2017, ECOSPHERE, V8, DOI 10.1002/ecs2.1974.
   Mostafa Y, 2017, CAN J REMOTE SENS, V43, P545, DOI 10.1080/07038992.2017.1384310.
   Muss JD, 2011, REMOTE SENS ENVIRON, V115, P824, DOI 10.1016/j.rse.2010.11.008.
   Nagendra H, 2001, INT J REMOTE SENS, V22, P2377, DOI 10.1080/01431160117096.
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI 10.5555/3104482.3104569.
   Onishi M, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-020-79653-9.
   R Core Team, 2020, R LANG ENV STAT COMP.
   Rodarmel C., 2002, SURVEYING LAND INFOR, V62, P115.
   Schimel D, 2015, GLOBAL CHANGE BIOL, V21, P1762, DOI 10.1111/gcb.12822.
   Scholl VM, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091414.
   Senf C, 2017, INT J APPL EARTH OBS, V60, P49, DOI 10.1016/j.jag.2017.04.004.
   Smith LN, 2018, ARXIV.
   Thorpe AS, 2016, ECOSPHERE, V7, DOI 10.1002/ecs2.1627.
   Torabzadeh H, 2014, ISPRS J PHOTOGRAMM, V97, P25, DOI 10.1016/j.isprsjprs.2014.08.001.
   Tusa E, 2020, DATA HANDL SCI TECHN, V32, P281, DOI 10.1016/B978-0-444-63977-6.00013-4.
   Wang K, 2010, SENSORS-BASEL, V10, P9647, DOI 10.3390/s101109647.
   Wang YS, 2016, NEUROCOMPUTING, V184, P232, DOI 10.1016/j.neucom.2015.08.104.
   White JC, 2016, CAN J REMOTE SENS, V42, P619, DOI 10.1080/07038992.2016.1207484.
   Wu J, 2006, SPRINGER.
   Zhang C, 2021, J FORESTRY RES, V32, P1879, DOI 10.1007/s11676-020-01245-0.
   Zhu XX, 2017, IEEE GEOSC REM SEN M, V5, P8, DOI 10.1109/MGRS.2017.2762307.},
Number-of-Cited-References = {67},
Times-Cited = {6},
Usage-Count-Last-180-days = {9},
Usage-Count-Since-2013 = {42},
Journal-ISO = {PeerJ},
Doc-Delivery-Number = {TR2BP},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000678776100001},
OA = {Green Published, Green Submitted, gold},
DA = {2023-08-12},
}

@article{ WOS:000894559200001,
Author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M. T.},
Title = {A novel edge detection method for medicinal plant's leaf features
   extraction},
Journal = {INTERNATIONAL JOURNAL OF SYSTEM ASSURANCE ENGINEERING AND MANAGEMENT},
Year = {2023},
Volume = {14},
Number = {1},
Pages = {448-458},
Month = {FEB},
Abstract = {Morphological features-based leaf identification algorithms provide
   highly accurate results. But it is required a single-lined edge
   extraction algorithm for morphological feature generation. Existing edge
   extraction algorithms have heavy calculations and higher iteration steps
   to extract edges. The simplicity of the edge detection algorithm helps
   to reduce the complexity of the image feature extraction process. In
   this paper, a fast and straightforward novel edge detection algorithm is
   introduced in the spatial domain. In a single iteration over all the
   pixels of the image, our algorithm can achieve a better result than
   existing edge detection techniques. Also, this paper provides a novel
   algorithm for leaf shape, vein, apex, and base feature extraction
   techniques using the edge detection algorithm that can be utilized
   further for the classification and identification of medicinal plant
   species or any other plant species too. The performance measure of the
   proposed edge detection algorithm for leaf features is better as
   compared to the existing edge detection algorithms. This edge detection
   algorithm achieved 92\% of accuracy and a PSNR rate of 10.88 dB with the
   time complexity of O(n{*}m), where n is the height and m is the width of
   the given image. The importance of medicinal plant identification and
   existing leaf identification techniques are also discussed in this
   paper.},
Publisher = {SPRINGER INDIA},
Address = {7TH FLOOR, VIJAYA BUILDING, 17, BARAKHAMBA ROAD, NEW DELHI, 110 001,
   INDIA},
Type = {Article},
Language = {English},
Affiliation = {Dubey, AK (Corresponding Author), Amity Univ Uttar Pradesh, Amity Sch Engn \& Technol, Dept Elect \& Commun Engn, Noida 201313, Uttar Pradesh, India.
   Thanikkal, Jibi G., Amity Univ Uttar Pradesh, Amity Sch Engn \& Technol, Dept Comp Sci \& Engn, Noida 201313, Uttar Pradesh, India.
   Dubey, Ashwani Kumar, Amity Univ Uttar Pradesh, Amity Sch Engn \& Technol, Dept Elect \& Commun Engn, Noida 201313, Uttar Pradesh, India.
   Thomas, M. T., St Thomas Coll, Dept Bot, Trichur, Kerala, India.},
DOI = {10.1007/s13198-022-01814-y},
EarlyAccessDate = {DEC 2022},
ISSN = {0975-6809},
EISSN = {0976-4348},
Keywords = {Image processing; Edge detection; Leaves classification; Morphological
   features; Plant identification},
Keywords-Plus = {ALGORITHM},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Multidisciplinary},
Author-Email = {jibimary@gmail.com
   dubey1ak@gmail.com
   thomastbgri@gmail.com},
Affiliations = {Amity University Noida; Amity University Noida},
ResearcherID-Numbers = {Dubey, Ashwani Kumar/ABI-1337-2020},
ORCID-Numbers = {Dubey, Ashwani Kumar/0000-0003-0778-9262},
Cited-References = {Abdel-Gawad AH, 2020, IEEE ACCESS, V8, P136243, DOI 10.1109/ACCESS.2020.3009898.
   Ajij M, 2019, INT CONF IMAG VIS, DOI 10.1109/ivcnz48456.2019.8961036.
   Balasubramanian K, 2021, J AMB INTEL HUM COMP, V12, P3559, DOI 10.1007/s12652-019-01559-w.
   Banerjee A, 2020, IEEE T IMAGE PROCESS, V29, P4898, DOI 10.1109/TIP.2020.2975717.
   Beikmohammadi A, 2018, 2018 4TH IRANIAN CONFERENCE ON SIGNAL PROCESSING AND INTELLIGENT SYSTEMS (ICSPIS), P21, DOI 10.1109/ICSPIS.2018.8700547.
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851.
   Chaouch M, 2009, GRAPH MODELS, V71, P63, DOI 10.1016/j.gmod.2008.12.006.
   Demisse GG, 2018, IEEE T PATTERN ANAL, V40, P1338, DOI 10.1109/TPAMI.2017.2711607.
   Ghahremani M, 2021, IEEE T IMAGE PROCESS, V30, P1153, DOI 10.1109/TIP.2020.3042057.
   Goyal Neha, 2018, 2018 International Conference on Computing, Power and Communication Technologies (GUCON), P405, DOI 10.1109/GUCON.2018.8675114.
   Hirasen D., 2020, 2020 2 INT MULTIDISC, P1, DOI DOI 10.1109/IMITEC50163.2020.9334091.
   Jasitha P., 2019, 2019 4th International Conference on Recent Trends on Electronics, Information, Communication \& Technology (RTEICT), P715, DOI 10.1109/RTEICT46194.2019.9016966.
   Jerripothula KR, 2021, IEEE T IMAGE PROCESS, V30, P2784, DOI 10.1109/TIP.2021.3054464.
   Kumar A, 2021, WIRELESS PERS COMMUN, V121, P2989, DOI 10.1007/s11277-021-08860-y.
   Kumar M, 2019, IEEE ACCESS, V7, P163912, DOI 10.1109/ACCESS.2019.2952176.
   Lee CY, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03261-2.
   Li RZ, 2020, IEEE ACCESS, V8, P221539, DOI 10.1109/ACCESS.2020.3043817.
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P5206, DOI 10.1109/TIP.2020.2980170.
   Mabrouk S, 2017, IRBM, V38, P167, DOI 10.1016/j.irbm.2017.04.004.
   MacLeod N., 2005, SHAPE MODELS BASIS M, P219.
   Mathappan Nivaashini, 2020, International Journal of Medical Engineering and Informatics, V12, P529, DOI 10.1504/IJMEI.2020.111027.
   Mzoughi O., 2012, IMAGE ANAL RECOGNITI.
   Pankaja K, 2018, P 2018 INT C INV RES, P1190, DOI {[}10.1109/ICIRCA.2018.8597184, DOI 10.1109/ICIRCA.2018.8597184].
   Raja R, 2020, WIRELESS PERS COMMUN, V112, P169, DOI 10.1007/s11277-019-07021-6.
   Rajavel R, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03157-1.
   Ramachandran Saravana Kumar, 2021, International Journal of Medical Engineering and Informatics, V13, P54, DOI 10.1504/IJMEI.2021.111864.
   Roy S, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03308-4.
   Singh R, 2019, INT J SYST ASSUR ENG, V10, P91, DOI 10.1007/s13198-019-00768-y.
   Sommana B, 2018, 2018 THIRTEENTH INTERNATIONAL CONFERENCE ON KNOWLEDGE, INFORMATION AND CREATIVITY SUPPORT SYSTEMS (KICSS), P138.
   Su JY, 2020, IEEE ACCESS, V8, P208753, DOI 10.1109/ACCESS.2020.3037649.
   Sujith A, 2020, 2020 4 INT C COMPUTI, P220, DOI {[}10.1109/ICCMC48092.2020.ICCMC-00042, DOI 10.1109/ICCMC48092.2020.ICCMC-00042].
   Sung TL, 2020, INT J SYST ASSUR ENG, V11, P812, DOI 10.1007/s13198-019-00881-y.
   Thanikkal JG, 2020, IEEE SENS J, V20, P13103, DOI 10.1109/JSEN.2020.3002909.
   Thanikkal JG, 2018, INT CONF RELI INFO, P518, DOI 10.1109/ICRITO.2018.8748587.
   Thanikkal JG, 2017, 2017 RECENT DEVELOPMENTS IN CONTROL, AUTOMATION AND POWER ENGINEERING (RDCAPE), P404, DOI 10.1109/RDCAPE.2017.8358305.
   Wang B, 2019, IEEE ACCESS, V7, P151754, DOI 10.1109/ACCESS.2019.2947510.
   Wang J, 2022, J AMB INTEL HUM COMP, V13, P1293, DOI 10.1007/s12652-020-02574-y.
   Wang ZB, 2015, CAN CON EL COMP EN, P1430, DOI 10.1109/CCECE.2015.7129490.
   Wei Liu, 2020, 2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom), P1461, DOI 10.1109/TrustCom50675.2020.00197.
   Wu FS, 2022, INT J SYST ASSUR ENG, V13, P72, DOI 10.1007/s13198-021-01262-0.
   Xie X, 2020, J AMB INTEL HUM COMP, V11, P2061, DOI 10.1007/s12652-019-01232-2.
   Yang CZ, 2019, IEEE ACCESS, V7, P178108, DOI 10.1109/ACCESS.2019.2958416.
   Yu JG, 2020, IEEE T IMAGE PROCESS, V29, P389, DOI 10.1109/TIP.2019.2923571.
   Zhu YL, 2022, WIRELESS PERS COMMUN, V127, P561, DOI 10.1007/s11277-021-08347-w.},
Number-of-Cited-References = {44},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Journal-ISO = {Int. J. Syst. Assur. Eng. Manag.},
Doc-Delivery-Number = {8O6NL},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000894559200001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000380504200027,
Author = {Priyankara, H. A. Chathura and Withanage, D. K.},
Book-Group-Author = {IEEE},
Title = {Computer Assisted Plant Identification System for Android},
Booktitle = {2015 MORATUWA ENGINEERING RESEARCH CONFERENCE (MERCON)},
Year = {2015},
Pages = {148-153},
Note = {Moratuwa Engineering Research Conference, Moratuwa, SRI LANKA, APR
   07-08, 2015},
Organization = {University Moratuwa; Engn Res Unit; IEEE; Sri Lanka},
Abstract = {Plant leaves provide sufficient features to distinguish them among other
   species. Identification of plants using leaf images is a classic problem
   in digital image processing. Usually those image processing systems use
   shape based digital morphological features for leaf identification task.
   Even there are number of studies on leaf based plant identification,
   very few of them are for mobiles. In this paper we describe a leaf image
   based plant identification system using SIFT features combining with Bag
   Of Word (BOW) model and Support Vector Machine (SVM) classifier. The
   system is trained to classify 20 species and obtained 96.48\% accuracy
   level. Based on the results, we developed an Android application
   communicates with the server and gives users the ability to identify
   plant species using photographs taken of plant leaves using the smart
   phone.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Priyankara, HAC (Corresponding Author), Univ Moratuwa, Fac Informat Technol, Moratuwa, Sri Lanka.
   Priyankara, H. A. Chathura; Withanage, D. K., Univ Moratuwa, Fac Informat Technol, Moratuwa, Sri Lanka.},
ISBN = {978-1-4799-1740-2},
Keywords = {plant identification; image processing; leaf features},
Keywords-Plus = {LEAF; FEATURES},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Multidisciplinary},
Author-Email = {priyankarahac@gmail.com
   dkwithanage@uom.lk},
Affiliations = {University Moratuwa},
Cited-References = {{[}Anonymous], WORD.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   Herdiyeni Y, 2012, 2012 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER SCIENCE AND INFORMATION SYSTEMS, P255.
   Knight D, 2010, AUTOMATIC PLANT LEAF.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee K.-B., 2013, INT J BIOSCI BIOTECH, V5, P57, DOI {[}10.14257/ijbsbt.2013.5.5.06, DOI 10.14257/IJBSBT.2013.5.5.06, 10.3724/sp.j.1087.2009.01707, DOI 10.1007/978-94-007-5857-5\_12].
   Li Y, 2006, IEEE SYS MAN CYBERN, P3890, DOI 10.1109/ICSMC.2006.384738.
   Li-Wei Yang, 2012, Intelligent Computing Technology. Proceedings 8th International Conference, ICIC 2012, P393, DOI 10.1007/978-3-642-31588-6\_51.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Mouine S., 2013, ICIP 2013.
   Pham NH, 2013, 2013 INTERNATIONAL CONFERENCE ON COMPUTING, MANAGEMENT AND TELECOMMUNICATIONS (COMMANTEL), P134, DOI 10.1109/ComManTel.2013.6482379.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Park JK, 2006, LECT NOTES COMPUT SC, V4182, P416.
   Park J, 2008, J SYST SOFTWARE, V81, P71, DOI 10.1016/j.jss.2007.05.001.
   PETRY W, 1989, J AGRON CROP SCI, V163, P345, DOI 10.1111/j.1439-037X.1989.tb00777.x.
   Nguyen QK, 2013, PROC INT CONF ADV, P404, DOI 10.1109/ATC.2013.6698145.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Wu Q., 2006, ADV ARTIFICIAL INTEL, V3.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xia Q, 2014, LECT NOTES ARTIF INT, V8589, P369, DOI 10.1007/978-3-319-09339-0\_38.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Ye YH, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P723, DOI 10.1109/ISIMP.2004.1434166.},
Number-of-Cited-References = {23},
Times-Cited = {15},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BF2WI},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000380504200027},
DA = {2023-08-12},
}

@article{ WOS:000576302300072,
Author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M. T.},
Title = {Unique Shape Descriptor Algorithm for Medicinal Plant Identification
   (SDAMPI) With Abridged Image Database},
Journal = {IEEE SENSORS JOURNAL},
Year = {2020},
Volume = {20},
Number = {21},
Pages = {13103-13109},
Month = {NOV 1},
Abstract = {In image processing, leaf shape recognition requires a huge database of
   similar images. Normal leaf image database construction requires more
   time and space. On the other hand, mobile offline applications are not
   able to contain huge image database with high pixel ratio. To solve this
   problem, mainly in medicinal plant identification, small leaf
   descriptors are necessary to completely provide the required plant
   information. Presently, the Botanists use shape reference table to
   recognize the following shapes of the leaf: ovate, cordate, elliptical,
   oblong, lanceolate and linear. As the shape numbers are invariant under
   scale, rotation and translation, which is highly desirable property for
   object recognition and chain code techniques preserve data by allowing
   large data reduction. Hence, in the proposed Shape Descriptor Algorithm
   for Medicinal Plant Identification (SDAMPI), we developed a descriptor
   to resolve the pixel selection issue of Freeman chain code and generate
   a unique leaf shape number. This leaf shape digital descriptor will act
   as a reference table for medicinal plant leaf shape identification. The
   performance of the proposed descriptor is evaluated through Jaccard
   similarity index graph and Levenshtein distance (LD) graph. From the
   results, it is confirming that, SDAMPI descriptor can detect Medicinal
   plant leaf shapes more accurately than existing methods.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Dubey, AK (Corresponding Author), Amity Univ Uttar Pradesh, Dept Elect \& Commun Engn, Amity Sch Engn \& Technol, Noida 201313, India.
   Thanikkal, Jibi G., Amity Univ Uttar Pradesh, Amity Sch Engn \& Technol, Noida 201313, India.
   Dubey, Ashwani Kumar, Amity Univ Uttar Pradesh, Dept Elect \& Commun Engn, Amity Sch Engn \& Technol, Noida 201313, India.
   Thomas, M. T., St Thomas Coll Thrissur, Dept Bot, Trichur 680001, India.},
DOI = {10.1109/JSEN.2020.3002909},
ISSN = {1530-437X},
EISSN = {1558-1748},
Keywords = {Shape; Image databases; Biomedical imaging; Image recognition; Feature
   extraction; Chain code; image processing; plant leaves; shape features;
   shape number},
Keywords-Plus = {LEAF; SCALE},
Research-Areas = {Engineering; Instruments \& Instrumentation; Physics},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Instruments \& Instrumentation;
   Physics, Applied},
Author-Email = {jibimary@gmail.com
   dubey1ak@gmail.com
   thomastbgri@gmail.com},
Affiliations = {Amity University Noida; Amity University Noida},
ResearcherID-Numbers = {Dubey, Ashwani Kumar/ABI-1337-2020
   Thanikkal, Jibi/ABG-9262-2021
   },
ORCID-Numbers = {Dubey, Ashwani Kumar/0000-0003-0778-9262
   Thanikkal, Jibi/0000-0002-5577-1158
   M T, Thomas/0000-0001-5952-5125},
Cited-References = {{[}Anonymous], 2013, ARXIV14014447.
   {[}Anonymous], 2016, INDIAN J SCI TECHNOL.
   Barthelemy D., 2009, P TECH REP 13 WORLD.
   Cao J, 2016, INFORM SCIENCES, V374, P51, DOI 10.1016/j.ins.2016.09.023.
   Chaki J., 2018, J KING SAUD U COMP I, DOI {[}10.1016/ j.jksuci.2018.01.007, DOI 10.1016/J.JKSUCI.2018.01.007].
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   de Souza MMS, 2016, EXPERT SYST APPL, V63, P375, DOI 10.1016/j.eswa.2016.07.016.
   Fan JP, 2015, IEEE T IMAGE PROCESS, V24, P4172, DOI 10.1109/TIP.2015.2457337.
   Freeman H., 1974, Computing Surveys, V6, P57, DOI 10.1145/356625.356627.
   Freeman H., 1961, IRE T ELECTRON COMPU, V10, P260, DOI DOI 10.1109/TEC.1961.5219197.
   Gonzalez R.C., 2004, DIGITAL IMAGE PROCES.
   HAMACHER VC, 1971, IEEE T COMPUT, VC 20, P321, DOI 10.1109/T-C.1971.223238.
   Herdiyeni Y, 2015, INT CONF SOFT COMPUT, P218, DOI 10.1109/SOCPAR.2015.7492810.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Jibi G. T., 2018, P INT C REL INF TECH, P513.
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Pedro F. B., LEAF DATASET.
   Prasad S, 2017, MULTIMED TOOLS APPL, V76, P6915, DOI 10.1007/s11042-016-3309-2.
   Rachid EA, 2016, IEEE C ANTENNA MEAS.
   Rinartha K, 2018, ELECTR POWER ELECTR, P399, DOI 10.1109/EECCIS.2018.8692996.
   Salve P, 2016, ADV INTELL SYST, V379, P85, DOI 10.1007/978-81-322-2517-1\_10.
   Schneider C.K., 1912, HDB LAUBHOLZ KUNDE.
   Soderkvist O.J.O., 2001, THESIS, P1.
   Thanikkal JG, 2017, 2017 RECENT DEVELOPMENTS IN CONTROL, AUTOMATION AND POWER ENGINEERING (RDCAPE), P404, DOI 10.1109/RDCAPE.2017.8358305.
   Wang B, 2014, IEEE T IMAGE PROCESS, V23, P4101, DOI 10.1109/TIP.2014.2343457.
   Wang XG, 2014, PATTERN RECOGN, V47, P2116, DOI 10.1016/j.patcog.2013.12.008.
   Wilson GR, 1997, IEE P-VIS IMAGE SIGN, V144, P145, DOI 10.1049/ip-vis:19971159.
   Zhao TY, 2018, IEEE T IMAGE PROCESS, V27, P4740, DOI 10.1109/TIP.2018.2845118.
   Zhao ZQ, 2015, NEUROCOMPUTING, V151, P1112, DOI 10.1016/j.neucom.2014.02.077.},
Number-of-Cited-References = {29},
Times-Cited = {5},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {11},
Journal-ISO = {IEEE Sens. J.},
Doc-Delivery-Number = {NY3OC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000576302300072},
DA = {2023-08-12},
}

@inproceedings{ WOS:000380483700020,
Author = {Panwar, Rachana and Goyal, Kusha and Pandey, Nilay and Khanna, Nitin},
Book-Group-Author = {IEEE},
Title = {Imaging System for Classification of Local Flora of Uttarakhand Region},
Booktitle = {2014 INTERNATIONAL CONFERENCE ON POWER, CONTROL AND EMBEDDED SYSTEMS
   (ICPCES)},
Year = {2014},
Note = {3rd International Conference Control and Embedded Systems (ICPCES),
   Motilal Nehru Natl I Tech, Allahabad, INDIA, DEC 26-28, 2014},
Organization = {IEEE; TEQIP-II},
Abstract = {Many plants are facing the risk of extinction due to unplanned
   urbanization and over growth of population. Digital databases of plants
   should be maintained for proper tracking of local flora and making
   data-driven policies/decisions for their preservation. Plant
   identification is important for medical as well as educational purposes
   but maintaining an exhaustive digital database is a challenging task due
   to the presence of large number of plant species. This paper proposes a
   system for building a digital database of local flora and recognizing
   different plants using their leaf images. The system proposed in this
   paper involves four steps: 1) image acquisition, 2) image preprocessing,
   3) feature extraction, and 4) classification. Images are acquired using
   commonly available general purpose desktop scanner with white paper as a
   background. In the image-preprocessing module, the system applies
   several image-processing techniques to prepare a leaf image for the
   feature extraction process. Then twelve leaf-shape based features are
   estimated and IB1 classifier is used to classify the plant species. The
   proposed system was used to build a dataset of local flora of
   Uttarakhand region, consisting of 1684 images of thirty-two different
   plant species. The database contains around fifty leaves of each plant
   species. The proposed system gives promising results with an average
   classification accuracy of 79\% for these thirty species of plants.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Panwar, R (Corresponding Author), Graph Era Univ, Dept Elect \& Commun Engn, Dehra Dun, Uttarakhand, India.
   Panwar, Rachana; Goyal, Kusha; Pandey, Nilay; Khanna, Nitin, Graph Era Univ, Dept Elect \& Commun Engn, Dehra Dun, Uttarakhand, India.},
ISBN = {978-1-4799-5912-9},
Keywords = {Feature Extraction; IB1; Image Pre-processing; Plant Identification;
   Shape-based features},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Affiliations = {Graphic Era University},
ResearcherID-Numbers = {Pandey, Nilay/X-6131-2019
   Khanna, Nitin/A-2068-2013},
ORCID-Numbers = {Pandey, Nilay/0000-0002-3212-9787
   Khanna, Nitin/0000-0001-7571-9130},
Cited-References = {Aranda M. C., 2010, P ACM INT C IM VID R, P327, DOI {[}10.1145/1816041.1816089, DOI 10.1145/1816041.1816089].
   Clark JY, 2004, PROCEEDINGS OF THE 2004 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN BIOINFORMATICS AND COMPUTATIONAL BIOLOGY, P87.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Jabal MFAB, 2013, J COMPUT SCI-NETH, V9, P1295, DOI {[}DOI 10.3844/J.CSSP.2013.1295.1304, DOI 10.3844/JCSSP.2013.1295.1304].
   Kala C. P, 2010, MED PLANTS UTTARAKHA.
   MZOUGHI O, 2012, IEEE INT C MULT EXP, P254.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Pornpanomchai C., 2011, INT J ENG TECHNOLOGY, V3, P347, DOI DOI 10.7763/IJET.2011.V3.251.
   Rahmadhani M, 2010, AFITA INT C QU INF C, P306.
   Rawat R., 2011, INT J PHARMACOGN PHY, V3, P64.
   Valliammal N., 2012, INT J COMPUT COMMUN, V6, P152.
   Witten IH, 2011, MOR KAUF D, P1.
   WU GS, 2007, IEEE INT S SIGN PROC, P11.
   Yahiaoui I., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P254, DOI 10.1109/ICME.2012.130.},
Number-of-Cited-References = {14},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BF2NS},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000380483700020},
DA = {2023-08-12},
}

@article{ WOS:000753705600008,
Author = {Laxmi, Scindhiya and Gupta, S. K.},
Title = {Multi-category intuitionistic fuzzy twin support vector machines with an
   application to plant leaf recognition},
Journal = {ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE},
Year = {2022},
Volume = {110},
Month = {APR},
Abstract = {The intuitionistic fuzzy twin support vector machine for
   multi-categorization is developed in this study, which incorporates both
   structural and empirical risk concepts. In this method, each training
   pattern is first aggregated with the appropriate membership and
   non-membership degrees, which describe the position of a pattern in
   relation to its class centre and surrounding circumstances in input or
   feature space, and then the separating hyperplane is constructed using
   the kernel function and convex quadratic programming. Empirical findings
   on an artificial and thirteen UCI standard datasets show that it
   outperforms well-known existing methods including improved support
   vector machines, K-nearest neighbour, logistic regression, decision
   trees, random forests, and multilayer perceptrons. Furthermore, the
   suggested classifier with linear, polynomial, and Gaussian kernels has
   been used to identify the leaves of various plants, where the shape,
   texture, and margin data are extracted from the leaf in order to
   categorize the plant species. The method's generalization capacity is
   demonstrated by the classification results on two leaf datasets of
   thirty and one hundred species, respectively. To compare the suggested
   method's prediction capacity with others, statistical analysis is
   performed using two non-parametric tests, Friedman and Wilcoxon, with a
   5\% threshold of significance. The results show that the proposed method
   yields better performance for both linear and non-linear kernels.},
Publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Gupta, SK (Corresponding Author), Indian Inst Technol Roorkee, Dept Math, Roorkee 247667, Uttar Pradesh, India.
   Laxmi, Scindhiya; Gupta, S. K., Indian Inst Technol Roorkee, Dept Math, Roorkee 247667, Uttar Pradesh, India.},
DOI = {10.1016/j.engappai.2022.104687},
EarlyAccessDate = {FEB 2022},
Article-Number = {104687},
ISSN = {0952-1976},
EISSN = {1873-6769},
Keywords = {Machine learning; Fuzzy set; Kernel function; Plant identification;
   Support vector machines},
Keywords-Plus = {CLASSIFICATION; SELECTION},
Research-Areas = {Automation \& Control Systems; Computer Science; Engineering},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Artificial
   Intelligence; Engineering, Multidisciplinary; Engineering, Electrical \&
   Electronic},
Author-Email = {scindhiya2527adm@gmail.com
   s.gupta@ma.iitr.ac.in},
Affiliations = {Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee},
Funding-Acknowledgement = {Ministry of Human Resource Development, India},
Funding-Text = {The authors sincerely thank the reviewers for the recommendation,
   valuable comments and the interesting suggestions which have
   considerably improved the presentation of the paper. The first author is
   also grateful to the Ministry of Human Resource Development, India, for
   financial support, to carry out this work.},
Cited-References = {{[}Anonymous], 1994, MACHINE LEARNING NEU.
   Atanassov K. T, 1999, INTUITIONISTIC FUZZY, V35, DOI DOI 10.1007/978-3-7908-1870-3\_2.
   Bambil Deborah, 2020, Environment Systems \& Decisions, V40, P480, DOI 10.1007/s10669-020-09769-w.
   Blake C., 1998, UCI REPOSITORY MACHI.
   BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371.
   Bodhwani Vinit, 2019, Procedia Computer Science, V152, P186, DOI 10.1016/j.procs.2019.05.042.
   Charbuty B., 2021, J APP SCI TECHN 3, V2, P20, DOI {[}10.38094/jastt20165, DOI 10.38094/JASTT20165].
   Chen DD, 2016, PATTERN RECOGN, V60, P296, DOI 10.1016/j.patcog.2016.04.017.
   Cheng CH, 2019, ENG APPL ARTIF INTEL, V81, P283, DOI 10.1016/j.engappai.2019.03.003.
   Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482.
   Chowdhary C.L., 2018, NAT INSP COMPUT, DOI {[}10.1007/978-981-10-6747-1-9, DOI 10.1007/978-981-10-6747-1-9].
   Chowdhary CL, 2017, J BIOMIM BIOMATER BI, V30, P12, DOI 10.4028/www.scientific.net/JBBBE.30.12.
   Christianini N., 2000, INTRO SUPPORT VECTOR.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Dai HL, 2015, APPL SOFT COMPUT, V31, P172, DOI 10.1016/j.asoc.2015.02.025.
   Demsar J, 2006, J MACH LEARN RES, V7, P1.
   Fang X, 2019, ENG APPL ARTIF INTEL, V85, P533, DOI 10.1016/j.engappai.2019.07.011.
   Fung GM, 2005, MACH LEARN, V59, P77, DOI 10.1007/s10994-005-0463-6.
   Gao BB, 2015, 2015 IEEE 14TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P7, DOI 10.1109/ICMLA.2015.35.
   Ha MH., 2011, J HEBEI U NATURAL SC, V3, P225, DOI DOI 10.1109/TFUZZ.2019.2893863.
   Ha MH, 2013, SOFT COMPUT, V17, P635, DOI 10.1007/s00500-012-0937-y.
   Han L, 2013, ENG APPL ARTIF INTEL, V26, P848, DOI 10.1016/j.engappai.2012.10.005.
   Haykin S., 2010, NEURAL NETWORKS LEAR.
   Jayadeva, 2007, IEEE T PATTERN ANAL, V29, P905, DOI 10.1109/TPAMI.2007.1068.
   Kerhet A, 2006, ENG APPL ARTIF INTEL, V19, P807, DOI 10.1016/j.engappai.2006.05.010.
   Li K., 2013, INT J APPL INNOV ENG, V2, P459.
   Lin CF, 2002, IEEE T NEURAL NETWOR, V13, P464, DOI 10.1109/72.991432.
   Liu J, 2018, EXPERT SYST APPL, V102, P36, DOI 10.1016/j.eswa.2018.02.017.
   Mangasarian OL, 2006, IEEE T PATTERN ANAL, V28, P69, DOI 10.1109/TPAMI.2006.17.
   Mangasarian OL., 1969, NONLINEAR PROGRAMMIN.
   Mansournia MA, 2018, AM J EPIDEMIOL, V187, P864, DOI 10.1093/aje/kwx299.
   Menke W., 2018, GEOPHYS DATA ANAL DI.
   Milgram J., 2006, 10 INT WORKSHOP FRON.
   Nilsson N.J., 2014, PRINCIPLES ARTIFICIA, V2nd.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Parvin H, 2015, ENG APPL ARTIF INTEL, V37, P34, DOI 10.1016/j.engappai.2014.08.005.
   Qi ZQ, 2013, KNOWL-BASED SYST, V43, P74, DOI 10.1016/j.knosys.2013.01.008.
   Raileanu LE, 2004, ANN MATH ARTIF INTEL, V41, P77, DOI 10.1023/B:AMAI.0000018580.96245.c6.
   Rezvani S, 2019, IEEE T FUZZY SYST, V27, P2140, DOI 10.1109/TFUZZ.2019.2893863.
   Roy T. K., 2013, J UNCERTAIN SYSTEMS, V7, P92.
   Sartakhti JS, 2019, ENG APPL ARTIF INTEL, V85, P402, DOI 10.1016/j.engappai.2019.06.018.
   Soman K., 2009, MACHINE LEARNING SVM.
   Song Q, 2002, IEEE T SYST MAN CY C, V32, P440, DOI 10.1109/TSMCC.2002.807277.
   Stace C.A, 1991, PLANT TAXONOMY BIOSY.
   Suwantra MAP, 2020, PROCEEDINGS OF THE 2020 3RD INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS PROCESSING (ICIGP 2020), P41, DOI 10.1145/3383812.3383835.
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742.
   Svetnik V, 2003, J CHEM INF COMP SCI, V43, P1947, DOI 10.1021/ci034160g.
   Tian ZD, 2020, ENG APPL ARTIF INTEL, V94, DOI 10.1016/j.engappai.2020.103801.
   Bao TQ, 2020, J INFORM TELECOMMUN, V4, P140, DOI 10.1080/24751839.2019.1666625.
   Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640.
   Wu Q., 2006, ADV ARTIF INTELL, V20, P3.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Wu ZN, 2014, NEUROCOMPUTING, V125, P119, DOI 10.1016/j.neucom.2012.07.049.
   Yang CY, 2015, NEUROCOMPUTING, V162, P256, DOI 10.1016/j.neucom.2015.03.046.
   Yang X, 2007, INT J PATTERN RECOGN, V21, P961, DOI 10.1142/S0218001407005703.
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X.
   Zhang B, 2018, IEEE ACCESS, V6, P21758, DOI 10.1109/ACCESS.2017.2787980.
   Zhu WX, 2020, ENG APPL ARTIF INTEL, V91, DOI 10.1016/j.engappai.2020.103635.},
Number-of-Cited-References = {58},
Times-Cited = {7},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {14},
Journal-ISO = {Eng. Appl. Artif. Intell.},
Doc-Delivery-Number = {YW9BE},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000753705600008},
DA = {2023-08-12},
}

@inproceedings{ WOS:000922767200027,
Author = {Pires, Willian Oliveira and Fernandes, Jr., Ricardo Corso and de Paula
   Filho, Pedro Luiz and Candido Junior, Arnaldo and Teixeira, Joao Paulo},
Editor = {Pereira, AI and Fernandes, FP and Coelho, JP and Teixeira, JP and Pacheco, MF and Alves, P and Lopes, RP},
Title = {Leaf-Based Species Recognition Using Convolutional Neural Networks},
Booktitle = {OPTIMIZATION, LEARNING ALGORITHMS AND APPLICATIONS, OL2A 2021},
Series = {Communications in Computer and Information Science},
Year = {2021},
Volume = {1488},
Pages = {367-380},
Note = {1st International Conference on Optimization, Learning Algorithms and
   Applications (OL2A), ELECTR NETWORK, JUL 19-21, 2021},
Abstract = {Identifying plant species is an important activity in specie control and
   preservation. The identification process is carried out mainly by
   botanists, consisting of a comparison of already known specimens or
   using the aid of books, manuals or identification keys. Artificial
   Neural Networks have been shown to perform well in classification
   problems and are a suitable approach for species identification. This
   work uses Convolutional Neural Networks to classify tree species by leaf
   images. In total, 29 species were collected. This work analyzed two
   network models, Darknet-19 and GoogLeNet (Inception-v3), presenting a
   comparison between them. The Darknet and GoogLeNet models achieved
   recognition rates of 86.2\% and 90.3\%, respectively.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Candido, A (Corresponding Author), Univ Tecnol Fed Parana, Medianeira Campus, Curitiba, Brazil.
   Pires, Willian Oliveira; Fernandes, Ricardo Corso, Jr.; de Paula Filho, Pedro Luiz; Candido Junior, Arnaldo, Univ Tecnol Fed Parana, Medianeira Campus, Curitiba, Brazil.
   Teixeira, Joao Paulo, Inst Politecn Braganca, Res Ctr Digitalizat \& Intelligent Robot CEDRI, Braganca, Portugal.},
DOI = {10.1007/978-3-030-91885-9\_27},
ISSN = {1865-0929},
EISSN = {1865-0937},
ISBN = {978-3-030-91885-9; 978-3-030-91884-2},
Keywords = {Deep learning; Leaf recognition; Tree classification},
Research-Areas = {Computer Science; Operations Research \& Management Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Operations Research \& Management
   Science},
Author-Email = {willianpires@alunos.utfpr.edu.br
   ricjun@alunos.utfpr.edu.br
   plpf@utfpr.edu.br
   arnaldoc@utfpr.edu.br
   joaopt@ipb.pt},
Affiliations = {Pontificia Universidade Catolica do Parana; Universidade Federal do
   Parana; Universidade Tecnologica Federal do Parana; Instituto
   Politecnico de Braganca},
ResearcherID-Numbers = {Teixeira, João/N-6576-2013
   },
ORCID-Numbers = {Teixeira, João/0000-0002-6679-5702
   Candido Junior, Arnaldo/0000-0002-5647-0891},
Cited-References = {Baldi P., 2013, 2013 NEURAL INFORM P.
   da Silva NR, 2016, SCI REP-UK, V6, DOI 10.1038/srep25994.
   Farhadi A., 2015, YOU ONLY LOOK ONCE U.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P77.
   Jones R.P., 2018, DARKFLOW.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Marchiori J.N.C., 1995, ELEMENTOS DENDROLOGI.
   Paulo Filho P.L., 2012, UFPR, P9.
   Pearlstein L, 2016, IEEE APP IMG PAT.
   Pinheiro A.L, 2000, FUNDAMENTOS TAXONOMI.
   Pires W.O, 2017, 7 SEMINARIO EXTENCAO.
   SHRIVASTAVA P, 1995, ACAD MANAGE REV, V20, P936, DOI 10.2307/258961.
   Szegedy C., 2015, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2015.7298594.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Vargas A.C.G., 2015, IEEECONFERENCIA UNIV.
   Wurtz RH, 2009, J PHYSIOL-LONDON, V587, P2817, DOI 10.1113/jphysiol.2009.170209.
   Yalcin H, 2016, INT CONF AGRO-GEOINF, P233.},
Number-of-Cited-References = {17},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BU6EN},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000922767200027},
OA = {Green Submitted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000848377500092,
Author = {Todorov, Pavel and Christoff, Nicole},
Book-Group-Author = {IEEE},
Title = {Vessels Detection Based on Neural Network with Application in Wood
   Recognition},
Booktitle = {PROCEEDINGS OF THE THE 11TH IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT
   DATA ACQUISITION AND ADVANCED COMPUTING SYSTEMS: TECHNOLOGY AND
   APPLICATIONS (IDAACS'2021), VOL 1},
Series = {IEEE International Workshop on Intelligent Data Acquisition and Advanced
   Computing Systems-Technology and Applications-IDAACS},
Year = {2021},
Pages = {516-520},
Note = {11th IEEE International Conference on Intelligent Data Acquisition and
   Advanced Computing Systems - Technology and Applications (IDAACS),
   ELECTR NETWORK, SEP 22-25, 2021},
Organization = {IEEE; Cracow Univ Technol, Fac Elect \& Comp Engn; West Ukrainian Natl
   Univ, Res Inst Intelligent Comp Syst; Natl Acad Sci Ukraine, V M
   Glushkov Inst Cybernet; West Ukrainian Natl Univ, Dept Informat Comp
   Syst \& Control; IEEE Ukraine Sect I \& M CI Joint Soc Chapter; IEEE
   Ukraine Sect; IEEE Poland Sect; MDPI Sensors; River Publishers; EFENTO;
   Fachhochschule Dortmund Univ Appl Sci \& Arts, Inst Digital Transformat
   Applicat \& Living Domains; Dortmund Univ Appl Sci \& Arts},
Abstract = {For the conservation of plant species and the control of timber commerce
   across the world, tree identification is becoming increasingly crucial.
   It relies heavily on anatomical traits such as vessels, fibers,
   parenchyma, and rays. The cell structure of each hardwood species
   differs greatly across intraspecific species and defines their
   differences. In this article, we propose a method for the detection of
   vessels through two steps: feature extraction and classification. They
   are based on grayscale image entropy and classification using a neural
   network.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Todorov, P (Corresponding Author), Tech Univ Sofia, Fac Telecommun, 8 Kl Ohridski Blvd, Sofia 1756, Bulgaria.
   Todorov, Pavel; Christoff, Nicole, Tech Univ Sofia, Fac Telecommun, 8 Kl Ohridski Blvd, Sofia 1756, Bulgaria.},
DOI = {10.1109/IDAACS53288.2021.9660958},
ISBN = {978-1-6654-2605-3},
Keywords = {vessel detection; feature extraction; classification},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Interdisciplinary Applications; Engineering,
   Electrical \& Electronic},
Author-Email = {patodorov@tu-sofia.bg
   nicole.christoff@tu-sofia.bg},
Affiliations = {Technical University Sofia},
Funding-Acknowledgement = {Ministry of Education and Science of Republic of Bulgaria {[}KP-06M47/2
   26.11.2020]},
Funding-Text = {This work was supported by the National Science Fund at the Ministry of
   Education and Science of Republic of Bulgaria under project KP-06M47/2
   26.11.2020 ``Geometric features extraction and classification of tree
   species in Bulgaria in order to protect the environment, part of NATURA
   2000},
Cited-References = {Bardarov N, 2014, GUIDE IDENTI FICATIO.
   da Silva NR, 2017, ANN FOREST SCI, V74, DOI 10.1007/s13595-017-0619-0.
   Figueroa-Mata G, 2018, 2018 IEEE INT WORK C, DOI {[}10.1109/iwobi.2018.8464216, DOI 10.1109/IWOBI.2018.8464216].
   Golovko V, 2000, IEEE IJCNN, P323, DOI 10.1109/IJCNN.2000.857856.
   Ibrahim I, 2018, EUR J WOOD WOOD PROD, V76, P345, DOI 10.1007/s00107-017-1163-1.
   Kobayashi K, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0220762.
   Maiti R., 2016, Forest Research: Open Access, V5, P166.
   Paliy I, 2005, INT WORKSH INT DATA, P112, DOI 10.1109/IDAACS.2005.282951.
   Ravindran P, 2019, ARXIV.
   Toosi T, 2019, METHOD DETECTING AGI.
   TRIWIJOYO B. K, 2021, NEW HYPERTENSIVE RET.
   Urbonas A, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9224898.
   Yadav AS, 2018, 2018 5TH IEEE UTTAR PRADESH SECTION INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS AND COMPUTER ENGINEERING (UPCON), P1201.
   Yeqin Wang, 2010, Proceedings of the 2010 International Conference on Intelligent Control and Information Processing (ICICIP 2010), P512, DOI 10.1109/ICICIP.2010.5564210.},
Number-of-Cited-References = {14},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BT7GQ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000848377500092},
DA = {2023-08-12},
}

@inproceedings{ WOS:000839140800050,
Author = {Aminuddin, Raihah and Maskan, Farizul Azlan and Jalil, Ummu Mardhiah
   Abdul and Fesol, Siti Feirusz Ahmad and Ibrahim, Shafaf},
Book-Group-Author = {IEEE},
Title = {Support Vector Machine-based approach for Recognizing Bonsai Species
   using Leaf Image},
Booktitle = {2022 IEEE 18TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING \&
   APPLICATIONS (CSPA 2022)},
Year = {2022},
Pages = {266-271},
Note = {18th IEEE International Colloquium on Signal Processing and Applications
   (CSPA), Selangor, MALAYSIA, MAY 12, 2022},
Organization = {IEEE; Univ Teknologi Mara; IEEE Control Syst Soc; IEEE Control Syst Soc
   Malaysia Chapter},
Abstract = {Recognition of Bonsai plant is one of the most challenging task. This is
   because most of the people have less knowledge about Bonsai especially
   for a beginner. For those who new to this field, it might be hard for
   them to recognize and identify the species of Bonsai because of its
   similarity in terms of shape, colour and etc. The incorrect
   identification of species, may resulting in damaging the Bonsai plant.
   Furthermore, different species of Bonsai may have different ways to take
   care of it. Therefore, the information about the Bonsai need to be
   accessible with the recognition of the species. As a solution, the aims
   of this project is to develop a system for recognising three species of
   Bonsai: 1) Adenium, 2) Red Japanese Maple and 3) Natal Plum by using its
   leaf. The project implemented a Rapid Application Development (RAD)
   Model as the methodology. There are four phases in RAD: 1) Planning, 2)
   Design, 3) Implementation and 4) Finalization. In pre-processed phase,
   feature extraction of the leaf is using colour moment and Gray-Level Co-
   occurrence Matrix (GLCM) were used for extracting the colour of the
   leaf. The species of Bonsai has been classified using Support Vector
   Machine-based approach and the system has been successfully recognize
   the species of Bonsai with accuracy of 98.2\%.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Jalil, UMA (Corresponding Author), Univ Teknol MARA, Fac Comp \& Math Sci, Melaka Branch, Jasin Campus, Melaka, Malaysia.
   Aminuddin, Raihah; Jalil, Ummu Mardhiah Abdul; Fesol, Siti Feirusz Ahmad; Ibrahim, Shafaf, Univ Teknol MARA, Fac Comp \& Math Sci, Melaka Branch, Jasin Campus, Melaka, Malaysia.
   Maskan, Farizul Azlan, Protech Synergy Sdn Bhd, Petaling Jaya, Selangor, Malaysia.},
DOI = {10.1109/CSPA55076.2022.9781913},
ISBN = {978-1-6654-8529-6},
Keywords = {bonsai; support vector machine; machine learning; leaf recognition},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Software Engineering; Engineering, Electrical \&
   Electronic},
Author-Email = {ummu.mardhiah.ajalil@uitm.edu.my},
Affiliations = {Universiti Teknologi MARA},
Funding-Acknowledgement = {Universiti Teknologi MARA Malacca Branch, Center of Vision and
   Algorithms Analytics (C-VAA); Information Technology for Organizations
   (ITFO) Research Groups {[}GDT2021/1-26, GDT2021/1-25]},
Funding-Text = {The authors would like to thank Universiti Teknologi MARA Malacca
   Branch, Center of Vision and Algorithms Analytics (C-VAA), and
   Information Technology for Organizations (ITFO) Research Groups for the
   support throughout this research. (Project No: GDT2021/1-26 and
   GDT2021/125).},
Cited-References = {Adhikari S., 2018, TOMATO PLANT DIS DET, VI, P81.
   Chakraborty Soarov, 2021, 2021 2nd International Conference on Robotics, Electrical and Signal Processing Techniques (ICREST), P147, DOI 10.1109/ICREST51555.2021.9331132.
   Chapin N., 2003, ENCY COMPUTER SCI, P714.
   Cohen E, 2019, TOURIST STUD, V19, P585, DOI 10.1177/1468797619864940.
   Farrugia L., 1997, J APPL CRYSTALLOGR, V30, P565, DOI {[}10.1107/S0021889897003117, DOI 10.1107/S0021889897003117].
   Ganthimathi M., 2020, INT J RES ADV DEV, V4, P1.
   Han LW, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105454.
   Hasegawa A, 2002, 26 INT HORTICULTURAL, V642, P217.
   Hermann C, 2021, PSYCH J, V10, P177, DOI 10.1002/pchj.440.
   Joshi BM, 2020, J INFORM OPTIM SCI, V41, P475, DOI 10.1080/02522667.2020.1734295.
   Kanchanadevi P., 2020, J CRIT REV, V7, P10.
   Kshirsagar G, 2018, INT J RECENT INNOVAT, V6, P113.
   Lu JZ, 2021, AGRICULTURE-BASEL, V11, DOI 10.3390/agriculture11080707.
   Madni AM, 2018, SYSTEMS ENG, V21, P172, DOI 10.1002/sys.21438.
   Mitarai Yozo, 2021, Journal of Environmental Information Science, V2020, P42, DOI 10.11492/ceispapersen.2020.2\_42.
   Mohamed Shafiq S. Mohamed, 2018, PERTANIKA J TROP AGR, V41.
   Qian MR, 2022, J PUBLIC HEALTH-HEID, V30, P259, DOI 10.1007/s10389-020-01321-z.
   Qodim H., 2019, 7 INT C CYBER IT SER, P1.
   Shamsuddin M. S., 2020, THESIS INT ISLAMIC U.
   Shruthi U, 2019, INT CONF ADVAN COMPU, P281, DOI 10.1109/ICACCS.2019.8728415.
   Skeffington D, 2020, J AUSTR CERAMICS, V59, P110.
   Snyder H, 2019, J BUS RES, V104, P333, DOI 10.1016/j.jbusres.2019.07.039.
   Subramanian EK, 2019, SERV ORIENTED COMPUT, V13, P237, DOI 10.1007/s11761-019-00270-0.
   Pham TN, 2020, IEEE ACCESS, V8, P189960, DOI 10.1109/ACCESS.2020.3031914.},
Number-of-Cited-References = {24},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BT5WW},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000839140800050},
DA = {2023-08-12},
}

@article{ WOS:000915446500001,
Author = {Cui, Liang and Chen, Shengbo and Mu, Yongling and Xu, Xitong and Zhang,
   Bin and Zhao, Xiuying},
Title = {Tree Species Classification over Cloudy Mountainous Regions by
   Spatiotemporal Fusion and Ensemble Classifier},
Journal = {FORESTS},
Year = {2023},
Volume = {14},
Number = {1},
Month = {JAN},
Abstract = {Accurate mapping of tree species is critical for the sustainable
   development of the forestry industry. However, the lack of cloud-free
   optical images makes it challenging to map tree species accurately in
   cloudy mountainous regions. In order to improve tree species
   identification in this context, a classification method using
   spatiotemporal fusion and ensemble classifier is proposed. The
   applicability of three spatiotemporal fusion methods, i.e., the spatial
   and temporal adaptive reflectance fusion model (STARFM), the flexible
   spatiotemporal data fusion (FSDAF), and the spatial and temporal
   nonlocal filter-based fusion model (STNLFFM), in fusing MODIS and
   Landsat 8 images was investigated. The fusion results in Helong City
   show that the STNLFFM algorithm generated the best fused images. The
   correlation coefficients between the fusion images and actual Landsat
   images on May 28 and October 19 were 0.9746 and 0.9226, respectively,
   with an average of 0.9486. Dense Landsat-like time series at 8-day time
   intervals were generated using this method. This time series imagery and
   topography-derived features were used as predictor variables. Four
   machine learning methods, i.e., K-nearest neighbors (KNN), random forest
   (RF), artificial neural networks (ANNs), and light gradient boosting
   machine (LightGBM), were selected for tree species classification in
   Helong City, Jilin Province. An ensemble classifier combining these
   classifiers was constructed to further improve the accuracy. The
   ensemble classifier consistently achieved the highest accuracy in almost
   all classification scenarios, with a maximum overall accuracy
   improvement of approximately 3.4\% compared to the best base classifier.
   Compared to only using a single temporal image, utilizing dense time
   series and the ensemble classifier can improve the classification
   accuracy by about 20\%, and the overall accuracy reaches 84.32\%. In
   conclusion, using spatiotemporal fusion and the ensemble classifier can
   significantly enhance tree species identification in cloudy mountainous
   areas with poor data availability.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Chen, SB (Corresponding Author), Jilin Univ, Coll Geo Explorat Sci \& Technol, Changchun 130026, Peoples R China.
   Cui, Liang; Chen, Shengbo; Mu, Yongling; Xu, Xitong; Zhang, Bin, Jilin Univ, Coll Geo Explorat Sci \& Technol, Changchun 130026, Peoples R China.
   Zhao, Xiuying, Air Force Aviat Univ, Flight Res Inst, Changchun 130022, Peoples R China.},
DOI = {10.3390/f14010107},
Article-Number = {107},
EISSN = {1999-4907},
Keywords = {time series; tree species mapping; image fusion; machine learning;
   Landsat 8 OLI; MODIS},
Keywords-Plus = {TIME-SERIES; COVER CLASSIFICATION; SURFACE REFLECTANCE; SATELLITE
   IMAGES; LANDSAT; INFORMATION; SELECTION; FEATURES; SUPPORT},
Research-Areas = {Forestry},
Web-of-Science-Categories  = {Forestry},
Author-Email = {chensb@jlu.edu.cn},
Affiliations = {Jilin University},
Funding-Acknowledgement = {National Key Research and Development Program of China
   {[}2020YFA0714103]; Capital construction funds (innovative capacity
   building); Jilin Province Science and Technology Development Plan
   Project {[}20210201138GX]; Science and technology project in Chaoyang
   District, Changchun City (Chaoyang Science and Technology) {[}Co-202101]},
Funding-Text = {This research was funded and supported by the National Key Research and
   Development Program of China (No. 2020YFA0714103), capital construction
   funds (innovative capacity building) within the provincial budget in
   2021 (No.2021C045-8), the Jilin Province Science and Technology
   Development Plan Project (No.20210201138GX), and the Science and
   technology project in Chaoyang District, Changchun City (Chaoyang
   Science and Technology Co-202101).},
Cited-References = {Axelsson A, 2021, INT J APPL EARTH OBS, V100, DOI 10.1016/j.jag.2021.102318.
   Belgiu M, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11070818.
   Bjerreskov KS, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13050950.
   Chaudhuri P, 2009, IEEE T PATTERN ANAL, V31, P1153, DOI 10.1109/TPAMI.2008.149.
   Chen B, 2017, ISPRS J PHOTOGRAMM, V124, P27, DOI 10.1016/j.isprsjprs.2016.12.008.
   Chen B, 2015, REMOTE SENS-BASEL, V7, P1798, DOI 10.3390/rs70201798.
   Chen H., 2020, P IGARSS 2020 2020 I, P7058.
   Cheng Q, 2017, IEEE T GEOSCI REMOTE, V55, P4476, DOI 10.1109/TGRS.2017.2692802.
   Chiarucci A, 2020, CONSERV BIOL, V34, P368, DOI 10.1111/cobi.13408.
   Deepan P., 2021, Machine Learning, Deep Learning and Computational Intelligence for Wireless Communication. Proceedings of MDCWC 2020. Lecture Notes in Electrical Engineering (LNEE 749), P535, DOI 10.1007/978-981-16-0289-4\_39.
   Dennison PE, 2004, REMOTE SENS ENVIRON, V93, P359, DOI 10.1016/j.rse.2004.07.013.
   Du PJ, 2012, SENSORS-BASEL, V12, P4764, DOI 10.3390/s120404764.
   Farr TG, 2007, REV GEOPHYS, V45, DOI 10.1029/2005RG000183.
   Fuhrer E, 2000, FOREST ECOL MANAG, V132, P29, DOI 10.1016/S0378-1127(00)00377-7.
   Gao F, 2006, IEEE T GEOSCI REMOTE, V44, P2207, DOI 10.1109/TGRS.2006.872081.
   Gao F, 2017, REMOTE SENS ENVIRON, V188, P9, DOI 10.1016/j.rse.2016.11.004.
   Gao F, 2015, IEEE GEOSC REM SEN M, V3, P47, DOI 10.1109/MGRS.2015.2434351.
   Ge HX, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13142678.
   Gorelick N, 2017, REMOTE SENS ENVIRON, V202, P18, DOI 10.1016/j.rse.2017.06.031.
   Grabska E, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11101197.
   Grybas H, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13132631.
   Harikumar A, 2021, IEEE T GEOSCI REMOTE, V59, P4444, DOI 10.1109/TGRS.2020.3012343.
   Hauser LT, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12223729.
   Hemmerling J, 2021, REMOTE SENS ENVIRON, V267, DOI 10.1016/j.rse.2021.112743.
   HEPNER GF, 1990, PHOTOGRAMM ENG REM S, V56, P469.
   Pham H, 2020, J CLASSIF, V37, P223, DOI 10.1007/s00357-019-09322-8.
   Pham H, 2019, COMPUT INTELL-US, V35, P184, DOI 10.1111/coin.12198.
   Hoscilo A, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11080929.
   Immitzer M, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11222599.
   Jia D, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13040645.
   Jia K, 2014, REMOTE SENS-BASEL, V6, P11518, DOI 10.3390/rs61111518.
   Jia K, 2014, INT J APPL EARTH OBS, V33, P32, DOI 10.1016/j.jag.2014.04.015.
   Ke GL, 2017, ADV NEUR IN, V30.
   Khan MM, 2009, IEEE T GEOSCI REMOTE, V47, P3880, DOI 10.1109/TGRS.2009.2029094.
   Kim H, 2011, J KOREAN STAT SOC, V40, P437, DOI 10.1016/j.jkss.2011.03.002.
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260.
   Li DW, 2017, J INDIAN SOC REMOTE, V45, P229, DOI 10.1007/s12524-016-0597-y.
   Li WS, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14174282.
   Li Zhe, 2019, Yingyong Shengtai Xuebao, V30, P4059, DOI 10.13287/j.1001-9332.201912.016.
   Liao CH, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111125.
   Lim J, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12122049.
   Lim J, 2019, ISPRS INT J GEO-INF, V8, DOI 10.3390/ijgi8030150.
   Liu HP, 2022, COMPUT ELECTRON AGR, V194, DOI 10.1016/j.compag.2022.106794.
   Ma XS, 2014, IEEE J-STARS, V7, P961, DOI 10.1109/JSTARS.2013.2265331.
   McRoberts RE, 2007, REMOTE SENS ENVIRON, V110, P412, DOI 10.1016/j.rse.2006.09.034.
   Noble WS, 2006, NAT BIOTECHNOL, V24, P1565, DOI 10.1038/nbt1206-1565.
   Olofsson P, 2014, REMOTE SENS ENVIRON, V148, P42, DOI 10.1016/j.rse.2014.02.015.
   Pal M, 2005, INT J REMOTE SENS, V26, P217, DOI 10.1080/01431160412331269698.
   Persson M, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111794.
   Ping B, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10060881.
   Portillo-Quintero C, 2015, REG ENVIRON CHANGE, V15, P1039, DOI 10.1007/s10113-014-0689-6.
   Qiu YA, 2021, INT J APPL EARTH OBS, V100, DOI 10.1016/j.jag.2021.102333.
   Sheeren D, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8090734.
   Tetteh GO, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105696.
   Vermote E, 2016, REMOTE SENS ENVIRON, V185, P46, DOI 10.1016/j.rse.2016.04.008.
   Wan HM, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13010144.
   Wang JL, 2019, INT GEOSCI REMOTE SE, P6047, DOI 10.1109/IGARSS.2019.8897907.
   Wang YC, 2012, GEOGR RES-AUST, V50, P320, DOI 10.1111/j.1745-5871.2011.00732.x.
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861.
   Watson JEM, 2018, NAT ECOL EVOL, V2, P599, DOI 10.1038/s41559-018-0490-x.
   Weiss DJ, 2014, ISPRS J PHOTOGRAMM, V98, P106, DOI 10.1016/j.isprsjprs.2014.10.001.
   Xie DH, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10071142.
   Xu KJ, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13142716.
   Xu KJ, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12101554.
   Zhang BH, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8010010.
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019.
   Zhang QC, 2018, ACTA OPT SIN, V38, DOI 10.3788/AOS201838.0628002.
   Zhou JX, 2021, REMOTE SENS ENVIRON, V252, DOI 10.1016/j.rse.2020.112130.
   Zhou ZH, 2019, NATL SCI REV, V6, P74, DOI 10.1093/nsr/nwy108.
   Zhu XL, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10040527.
   Zhu XL, 2016, REMOTE SENS ENVIRON, V172, P165, DOI 10.1016/j.rse.2015.11.016.
   Zhu XL, 2014, ISPRS J PHOTOGRAMM, V96, P1, DOI 10.1016/j.isprsjprs.2014.06.012.},
Number-of-Cited-References = {72},
Times-Cited = {0},
Usage-Count-Last-180-days = {11},
Usage-Count-Since-2013 = {11},
Journal-ISO = {Forests},
Doc-Delivery-Number = {7Z3EP},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000915446500001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000228793100008,
Author = {Zou, J and Gattani, A},
Editor = {Santini, S and Schettini, R and Gevers, T},
Title = {Computer-assisted visual InterActive recognition and its prospects of
   implementation over the Internet},
Booktitle = {INTERNET IMAGING VI},
Series = {Proceedings of SPIE},
Year = {2005},
Volume = {5670},
Pages = {76-87},
Note = {Conference on Internet Imaging VI, San Jose, CA, JAN 18-20, 2005},
Organization = {Soc Imaging Sci \& Technol; SPIE},
Abstract = {When completely automated systems don't yield acceptable accuracy, Many
   practical pattern recognition systems involve the human either at the
   beginning (pre-processing) or towards the end (handling rejects). We
   believe that it may be more useful to involve the human throughout the
   recognition process rather than just at the beginning or end. We
   describe a methodology of interactive visual recognition for
   human-centered low-throughput applications, Computer Assisted Visual
   InterActive Recognition (CAVIAR), and discuss the prospects of
   implementing CAVIAR over the Internet.
   The novelty of CAVIAR is image-based interaction through a
   domain-specific parameterized geometrical model, which reduces the
   semantic gap between humans and computers. The user may interact with
   the computer anytime that she considers its response unsatisfactory. The
   interaction improves the accuracy of the classification features by
   improving the fit of the computer-proposed model. The computer makes
   subsequent use of the parameters of the improved model to refine not
   only its own statistical model-fitting process, but also its internal
   classifier. The CAVIAR methodology was applied to implement a flower
   recognition system. The principal conclusions from the evaluation of the
   system include: 1) the average recognition time of the CAVIAR system is
   significantly shorter than that of the unaided human, 2) its accuracy is
   significantly higher than that of the unaided machine; 3) it can be
   initialized with as few as one training sample per class and still
   achieve high accuracy; and 4) it demonstrates a self-learning ability.
   We have also implemented a Mobile CAVIAR system, where a pocket PC, as a
   client, connects to a server through wireless communication. The
   motivation behind a mobile platform for CAVIAR is to apply the
   methodology in a human-centered pervasive environment, where the user
   can seamlessly interact with the system for classifying field-data.
   Deploying CAVIAR to a networked mobile platform poses the challenge of
   classifying field images and programming under constraints of display
   size, network bandwidth, processor speed, and memory size. Editing of
   the computer-proposed model is performed on the handheld while
   statistical model fitting and classification take place on the server.
   The possibility that the user can easily take several photos of the
   object poses an interesting information fusion problem. The advantage of
   the Internet is that the patterns identified by different users can be
   pooled together to benefit all peer users. When users identify patterns
   with CAVIAR in a networked setting, they also collect training samples
   and provide opportunities for machine learning from their intervention.
   CAVIAR implemented over the Internet provides a perfect test bed for,
   and extends, the concept of Open Mind Initiative proposed by David
   Stork.
   Our experimental evaluation focuses on human time, machine and human
   accuracy, and machine learning. We devoted much effort to evaluating the
   use of our image-based user interface and on developing principles for
   the evaluation of interactive pattern recognition system. The Internet
   architecture and Mobile CAVIAR methodology have many applications. We
   are exploring in the directions of teledermatology, face recognition,
   and education.},
Publisher = {SPIE-INT SOC OPTICAL ENGINEERING},
Address = {1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zou, J (Corresponding Author), Rensselaer Polytech Inst, Dept Elect Comp \& Syst Engn, 110 8th St, Troy, NY 12181 USA.
   Rensselaer Polytech Inst, Dept Elect Comp \& Syst Engn, Troy, NY 12181 USA.},
DOI = {10.1117/12.585112},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {0-8194-5643-8},
Keywords = {visual pattern recognition; human computer interaction; flower
   recognition; mobile; Internet; open mind; image based interaction;
   interactive segmentation; shape model; image retrieval},
Keywords-Plus = {IMAGES},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering},
Author-Email = {zouj@rpi.edu
   gattani@gmx.net},
Affiliations = {Rensselaer Polytechnic Institute},
Cited-References = {BAIRD HS, 1994, P SOC PHOTO-OPT INS, V2181, P106, DOI 10.1117/12.171098.
   Chastel S, 2004, PROC SPIE, V5304, P233.
   Duda RO, 1973, PATTERN CLASSIFICATI.
   EVANS A, 2003, 196 CSIS.
   Gao YS, 2002, IEEE T PATTERN ANAL, V24, P764, DOI 10.1109/TPAMI.2002.1008383.
   GATTANI A, 2004, THESIS ECSE DEPT.
   Han CY, 2003, P SOC PHOTO-OPT INS, V5018, P111, DOI 10.1117/12.476180.
   Haritaoglu I, 2001, PROC CVPR IEEE, P408.
   HARITAOGLU I, 2001, P UBICOMP, P247.
   Hopgood AA, 2003, COMPUTER, V36, P24, DOI 10.1109/MC.2003.1198233.
   JAIN R, 1992, US NSF WORKSH VIS IN.
   Kak AC, 2002, INT C PATT RECOG, P839, DOI 10.1109/ICPR.2002.1048433.
   Liptay TE, 2000, PROC SPIE, V3964, P330.
   Masseroli M, 2001, PROC SPIE, V4311, P325.
   MERICSKO RJ, 1998, P SPIE, V3584.
   NAGY G, 1966, IEEE T INFORM THEORY, V12, P215, DOI 10.1109/TIT.1966.1053864.
   Pfeiffer D, 2001, P SOC PHOTO-OPT INS, V4311, P44.
   PILU M, 2002, HPL200282.
   RUDRAPATNAA M, 2004, P SPIE IS T EL IM IN, V5018, P307.
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510.
   Santini S, 2003, P SOC PHOTO-OPT INS, V5018, P99, DOI 10.1117/12.476184.
   STORK DG, 2000, IEEE COMPUTER    OCT, P104.
   STORK DG, 2001, P COMP LEARN THEOR C.
   TSAI YH, 2003, P 7 DIG IM COMP TECH.
   von Ahn L, 2004, COMMUN ACM, V47, P57, DOI 10.1145/966389.966390.
   WEINSTEIN E, 2002, P PERV 2002 ZUR SWIT, P48.
   Yang J, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P19, DOI 10.1109/ACV.2002.1182149.
   Zhang J, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P217, DOI 10.1109/ICMI.2002.1166996.
   ZOU J, 2004, THESIS ECSE DEPT REN.
   ZOU J, 2005, IN PRESS IEEE WORKSH.},
Number-of-Cited-References = {30},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BCD92},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000228793100008},
DA = {2023-08-12},
}

@inproceedings{ WOS:000792687200025,
Author = {Lin, Ruikai and Ma, Junwei and Yu, Huiling and Zhang, Yizhuo},
Editor = {Wu, F and Cen, F},
Title = {Accurate Recognition Method of Plant Leaves based on Multi-feature
   Fusion},
Booktitle = {INTERNATIONAL CONFERENCE ON IMAGE PROCESSING AND INTELLIGENT CONTROL
   (IPIC 2021)},
Series = {Proceedings of SPIE},
Year = {2021},
Volume = {11928},
Note = {International Conference on Image Processing and Intelligent Control
   (IPIC), Lanzhou, PEOPLES R CHINA, JUL 30-AUG 01, 2021},
Organization = {Acad Exchange Informat Ctr; Wuhan Univ},
Abstract = {During the use of a convolutional neural network to train a recognition
   model of plant leaves, the convolutional layers focus on the appearance
   of leaves in learning the features of them, while ignoring their
   internal texture features, thereby resulting in the misclassification of
   plant leaves with similar appearance. Aiming at this problem, this paper
   proposes an accurate identification method of plant leaves based on
   multi-feature fusion, which can be applied to extract the appearance and
   texture features of leaves simultaneously, and to conduct fusion and
   summation for these two types of features. The experimental results
   indicate that compared with the accuracy of the ordinary convolutional
   neural network recognition method and traditional machine learning
   method, the accuracy of this method has been improved substantially.},
Publisher = {SPIE-INT SOC OPTICAL ENGINEERING},
Address = {1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Yu, HL; Zhang, YZ (Corresponding Author), Northeast Forestry Univ, Harbin 150000, Heilongjiang, Peoples R China.
   Lin, Ruikai; Ma, Junwei; Yu, Huiling; Zhang, Yizhuo, Northeast Forestry Univ, Harbin 150000, Heilongjiang, Peoples R China.},
DOI = {10.1117/12.2611757},
Article-Number = {119280Q},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-1-5106-4725-1; 978-1-5106-4724-4},
Keywords = {convolutional neural network; leaf recognition; texture feature;
   appearance feature},
Research-Areas = {Optics; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Optics; Imaging Science \& Photographic Technology},
Author-Email = {yhl@nefu.edu.cn
   nefuzyz@163.com},
Affiliations = {Northeast Forestry University - China},
Funding-Acknowledgement = {Natural Science Foundation of Heilongjiang Province {[}LH2020F002]},
Funding-Text = {The authors gratefully acknowledge funding by the Natural Science
   Foundation of Heilongjiang Province (LH2020F002).},
Cited-References = {Huang F L., 2020, J ANQING NORMAL U.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lu Hongtao, 2016, Journal of Data Acquisition and Processing, V31, P1, DOI 10.16337/j.1004-9037.2016.01.001.
   Wang LiJun, 2015, Journal of Beijing Forestry University, V37, P55.
   Wang Y., 2020, INFORM CHINESE MED, V37, P21.
   Wei L, 2013, J AGR MECH RES, V35, P12.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhang N., 2013, COMPUTER APPL.
   {[}张帅 Zhang Shuai], 2016, {[}北京林业大学学报, Journal of Beijing Forestry University], V38, P108.},
Number-of-Cited-References = {11},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {11},
Doc-Delivery-Number = {BT0UP},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000792687200025},
DA = {2023-08-12},
}

@inproceedings{ WOS:000426941100214,
Author = {Yang, Meng-Meng and Nayeem, Arifur and Shen, Ling-Ling},
Editor = {Xu, B},
Title = {Plant classification based on Stacked Autoencoder},
Booktitle = {PROCEEDINGS OF 2017 IEEE 2ND INFORMATION TECHNOLOGY, NETWORKING,
   ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC)},
Year = {2017},
Pages = {1082-1086},
Note = {IEEE 2nd Information Technology, Networking, Electronic and Automation
   Control Conference (ITNEC), Chengdu, PEOPLES R CHINA, DEC 15-17, 2017},
Organization = {IEEE PRESS},
Abstract = {With the development of rapid technology, the similarity between plants
   is increasing, which will enhance the classified workload of botanists.
   Therefore, it is urge to find a quick automatic classification method.
   In recent years, the performance of autoencoder has become more and more
   prominent. Consequently, in this paper, we employ stacked autoencoder to
   classify three plants, including 630 images in total. The result of this
   experience shows that the accuracy of classification is 93.3\%.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Shen, LL (Corresponding Author), Nanjing Normal Univ, Sch Comp Sci \& Technol, Nanjing 210023, Jiangsu, Peoples R China.
   Shen, LL (Corresponding Author), Nanjing Normal Univ, Sch Business, Nanjing 210023, Jiangsu, Peoples R China.
   Yang, Meng-Meng; Shen, Ling-Ling, Nanjing Normal Univ, Sch Comp Sci \& Technol, Nanjing 210023, Jiangsu, Peoples R China.
   Nayeem, Arifur, Saidpur Govt Tech Sch \& Coll, Saidpur 5310, Bangladesh.
   Nayeem, Arifur, Nanjing Univ Post \& Telecommun, Sch Overseas Educ, Nanjing 210046, Jiangsu, Peoples R China.
   Shen, Ling-Ling, Nanjing Normal Univ, Sch Business, Nanjing 210023, Jiangsu, Peoples R China.},
ISBN = {978-1-5090-6414-4},
Keywords = {stacked autoencoder; plant classification; deep learning},
Keywords-Plus = {PATHOLOGICAL BRAIN DETECTION; MACHINE},
Research-Areas = {Automation \& Control Systems; Engineering},
Web-of-Science-Categories  = {Automation \& Control Systems; Engineering, Electrical \& Electronic},
Author-Email = {1320981274@qq.com
   arifurnayeem9@yahoo.com
   llshen509@163.com},
Affiliations = {Nanjing Normal University; Nanjing University of Posts \&
   Telecommunications; Nanjing Normal University},
ResearcherID-Numbers = {Yang, Mengmeng/AAV-5795-2021},
ORCID-Numbers = {Yang, Mengmeng/0000-0002-8988-269X},
Funding-Acknowledgement = {Postgraduate Research AMP; Practice Innovation Program of Jiangsu
   Province {[}SJCX17\_0342]},
Funding-Text = {This paper is financially supported by Postgraduate Research \& Practice
   Innovation Program of Jiangsu Province (SJCX17\_0342).},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Abdolvahab E., 2010, INT J COMPUTER SCI I, V8, P100.
   Caraka R. E., 2017, PREDICTION EURO 50 U.
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199.
   Chen H., 2017, MULTIMEDIA TOOLS APP.
   Deng J, 2013, INT CONF AFFECT, P511, DOI 10.1109/ACII.2013.90.
   Jia WJ, 2017, J MED SYST, V41, DOI 10.1007/s10916-017-0814-4.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Liang G. E., 2017, MODERN COMPUTER.
   Liu YN, 2015, IEEE INT CONF HIGH, P125.
   Liu YH, 2016, APPL SCI-BASEL, V6, DOI 10.3390/app6050142.
   dos Santos WNL, 2017, MICROCHEM J, V133, P583, DOI 10.1016/j.microc.2017.04.029.
   Lu ZH, 2016, J MED IMAG HEALTH IN, V6, P1218, DOI 10.1166/jmihi.2016.1901.
   Sun J., 2017, J REAL TIME IMAGE PR.
   Tao C, 2015, IEEE GEOSCI REMOTE S, V12, P2438, DOI 10.1109/LGRS.2015.2482520.
   Wang SH, 2017, IEEE ACCESS, V5, P16576, DOI 10.1109/ACCESS.2017.2736558.
   Wang SH, 2017, FUND INFORM, V151, P191, DOI 10.3233/FI-2017-1487.
   Wang SH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00106.
   Xu J, 2014, I S BIOMED IMAGING, P999, DOI 10.1109/ISBI.2014.6868041.
   Xu J, 2016, IEEE T MED IMAGING, V35, P119, DOI 10.1109/TMI.2015.2458702.
   Zabalza J, 2016, NEUROCOMPUTING, V185, P1, DOI 10.1016/j.neucom.2015.11.044.
   Zhang YC, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON ORANGE TECHNOLOGIES (ICOT), P3.
   Zhang YD, 2016, EXPERT SYST, V33, P239, DOI 10.1111/exsy.12146.
   Zhang YD, 2017, J EXP THEOR ARTIF IN, V29, P299, DOI 10.1080/0952813X.2015.1132274.
   Zhang YD, 2016, SIMUL-T SOC MOD SIM, V92, P861, DOI 10.1177/0037549716666962.
   Zhang YD, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00066.
   Zhang YD, 2010, SCI CHINA INFORM SCI, V53, P1963, DOI 10.1007/s11432-010-4075-9.
   Zhang YD, 2009, SCI CHINA SER F, V52, P914, DOI 10.1007/s11432-009-0019-7.},
Number-of-Cited-References = {28},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BJ6SO},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000426941100214},
DA = {2023-08-12},
}

@inproceedings{ WOS:000390842400019,
Author = {Mukherjee, Gunjan and Chatterjee, Arpitam and Tudu, Bipan},
Book-Group-Author = {IEEE},
Title = {Study on the potential of combined GLCM features towards medicinal plant
   classification},
Booktitle = {2016 2ND INTERNATIONAL CONFERENCE ON CONTROL, INSTRUMENTATION, ENERGY \&
   COMMUNICATION (CIEC)},
Year = {2016},
Pages = {98-102},
Note = {2nd International Conference on Control, Instrumentation, Energy \&
   Communication (CIEC), Univ Calcutta, Dept Appl Phys, Kolkata, INDIA, JAN
   28-30, 2016},
Organization = {IEEE Kolkata Sect},
Abstract = {The gray level co-occurrence matrix (GLCM) is widely used for textural
   feature extraction. The features obtained from GLCM matrix are subjected
   to the classifiers for the purpose of identification and classification.
   In this paper the combinations between different features, obtained from
   GLCM matrix, are studied. For experiments, the leaves of medicinal
   plants are considered, as proper identification of medicinal plant is
   crucial for appropriate utilization of their medicinal values. Two
   popular Indian medicinal plants, namely, Neem and Tulsi have been
   considered for classification using back propagation multi layer
   perceptron (BP-MLP) neural network classifier. Beside combination the
   further classification improvements may be achieved using different
   preprocessing techniques, which have also been experimented. The results
   show that preprocessed combined GLCM features can provide higher
   classification rate compared to raw single GLCM features.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Mukherjee, G (Corresponding Author), Regent Educ \& Res Fdn, Dept MCA, Kolkata, India.
   Mukherjee, Gunjan, Regent Educ \& Res Fdn, Dept MCA, Kolkata, India.
   Chatterjee, Arpitam, Jadavpur Univ, Dept Printing Engn, Kolkata, India.
   Tudu, Bipan, Jadavpur Univ, Dept Instrument \& Elect Engn, Kolkata, India.},
ISBN = {978-1-5090-0035-7},
Keywords = {BP-MLP; Computer vision; data preprocessing; GLCM; medicinal plants
   classification; neural network},
Research-Areas = {Engineering; Instruments \& Instrumentation},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Instruments \& Instrumentation},
Author-Email = {mukherjee.gunjan@gmail.com
   arpitamchatterjee@print.jusl.ac.in
   bt@iee.jusl.ac.in},
Affiliations = {Regent Education \& Research Foundation Group of Institutions; Jadavpur
   University; Jadavpur University},
ResearcherID-Numbers = {/U-1130-2019
   },
ORCID-Numbers = {tudu, bipan/0000-0001-9323-4276},
Cited-References = {Agrawal K., 2012, J SIGNAL INF PROCESS, V03, P146, DOI DOI 10.4236/JSIP.2012.32019.
   Bama B. Sathys, 2011, INDIAN J COMPUTER SC, V2.
   Deng G., 1994, P IEEE NUCL SCI S ME, P1615.
   Ghazali K. H., 2007, P INT C EL ENG INF, P607.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   HAYKINS S, 2001, NEURAL NETWORKS COMP.
   Joy P. P., MED PLANTS.
   Kadir A., 2012, INT J ADV SCI TECHNO, V44.
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424.
   Nurhayati O. D., 2013, INT ENG TECHNOLOGY R, V1, P72.
   Palit M, 2010, ANAL CHIM ACTA, V675, P8, DOI 10.1016/j.aca.2010.06.036.
   Park D.K., 2000, P 2000 ACM WORKSH MU, P51, DOI DOI 10.1145/357744.357758.
   Pietikainen M., 2002, P 2 INT WORKSH TEXT, P109.
   Shi D. -cheng, 2008, ELECT IMAGING MULTIM, V6833.
   Shi D.-cheng, 2007, J CHINA U POSTS TELE, V14, P94.
   Sural S, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P589.
   TANG L, 1999, 993036 ASAE.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.},
Number-of-Cited-References = {18},
Times-Cited = {8},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BG6TQ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000390842400019},
DA = {2023-08-12},
}

@article{ WOS:000295392800005,
Author = {Sa Junior, Jarbas Joaci de M. and Backes, Andre R. and Rossatto, Davi
   Rodrigo and Kolb, Rosana M. and Bruno, Odemir M.},
Title = {Measuring and analyzing color and texture information in anatomical leaf
   cross sections: an approach using computer vision to aid plant species
   identification},
Journal = {BOTANY},
Year = {2011},
Volume = {89},
Number = {7},
Pages = {467-479},
Month = {JUL},
Abstract = {Currently, studies on leaf anatomy have provided an important source of
   characters helping taxonomic, systematic, and phylogenetic studies.
   These studies strongly rely on measurements of characters (such as
   tissue thickness) and qualitative information (structures description,
   presence absence of structures). In this work, we provide a new
   computational approach that semiautomates the collection of some
   quantitative data (cuticle, adaxial epidermis, and total leaf thickness)
   and accesses a new source of information in leaf cross-section images:
   the texture and the color of leaf tissues. Our aim was to evaluate this
   information for plant identification purposes. We successfully tested
   our system identifying eight species from different phylogenetic
   positions in the angiosperm phylogeny from the neotropical savanna of
   central Brazil. The proposed system checks the potential of identifying
   the species for each extracted measure using the Jeffrey-Matusita
   distance and composes a feature vector with the most important metrics.
   A linear discriminant analysis with leave-one-out to classify the
   samples was used. The experiments achieved a 100\% success rate in terms
   of identifying the studied species accessing the above-described
   parameters, demonstrating that our computational approach can be a
   helpful tool for anatomical studies, especially ones devoted to plant
   identification and systematic studies.},
Publisher = {CANADIAN SCIENCE PUBLISHING},
Address = {65 AURIGA DR, SUITE 203, OTTAWA, ON K2E 7W6, CANADA},
Type = {Article},
Language = {English},
Affiliation = {Rossatto, DR (Corresponding Author), UNESP, Univ Estadual Paulista, Fac Ciencias \& Letras, Dept Ciencias Biol, Av Dom Antonio 2100, BR-19806900 Assis, SP, Brazil.
   Rossatto, Davi Rodrigo; Kolb, Rosana M., UNESP, Univ Estadual Paulista, Fac Ciencias \& Letras, Dept Ciencias Biol, BR-19806900 Assis, SP, Brazil.
   Sa Junior, Jarbas Joaci de M., Dept Ciencias Matemat \& Comp, BR-13560970 Sao Carlos, SP, Brazil.
   Backes, Andre R., Univ Fed Uberlandia, Fac Comp, BR-38408100 Uberlandia, MG, Brazil.
   Bruno, Odemir M., Inst Fis Sao Carlos, BR-13560970 Sao Carlos, SP, Brazil.},
DOI = {10.1139/B11-038},
ISSN = {1916-2790},
EISSN = {1916-2804},
Keywords = {taxonomy; plant identification; feature extraction; Jeffrey-Matusita
   distance; linear discriminant analysis},
Keywords-Plus = {SEGMENTATION; SYSTEMATICS; MORPHOLOGY; MUMFORD; TOOL},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {drrossatto@gmail.com},
Affiliations = {Universidade Estadual Paulista; Universidade Federal de Uberlandia},
ResearcherID-Numbers = {Bruno, Odemir M/A-5279-2009
   Bruno, Odemir/AAU-7209-2020
   Kolb, Rosana M./D-3592-2012
   Rossatto, Davi Rodrigo/A-3521-2011
   },
ORCID-Numbers = {Bruno, Odemir M/0000-0002-2945-1556
   Bruno, Odemir/0000-0002-2945-1556
   Kolb, Rosana M./0000-0003-3841-5597
   Rossatto, Davi Rodrigo/0000-0001-9510-8345
   Backes, Andre/0000-0002-7486-4253},
Funding-Acknowledgement = {CNPq {[}135251/2006, 306628/2007-4, 484474/2007-3]; FAPESP
   {[}2006/54367-9]},
Funding-Text = {J.J.M.S. Jr. acknowledges support from CNPq (135251/2006), A.R.B.
   acknowledges support from FAPESP (2006/54367-9), and O.M.B. acknowledges
   support from CNPq (306628/2007-4 and 484474/2007-3). The authors
   gratefully acknowledge the reviewers},
Cited-References = {ADAMS RP, 1977, ANN MO BOT GARD, V64, P184, DOI 10.2307/2395332.
   {[}Anonymous], 2005, ANALISE DADOS ATRAVE.
   {[}Anonymous], 1990, STAT PATTERN RECOGNI.
   {[}Anonymous], 1992, APPL MULTIVARIATE DA.
   Araujo JS, 2010, PLANT SYST EVOL, V286, P117, DOI 10.1007/s00606-010-0268-3.
   Backes AR, 2006, LECT NOTES COMPUT SC, V4225, P784.
   Backes AR, 2011, PATTERN RECOGN, V44, P1684, DOI 10.1016/j.patcog.2011.01.018.
   Backes AR, 2010, PATTERN RECOGN, V43, P685, DOI 10.1016/j.patcog.2009.07.017.
   Barthlott W., 1981, Nordic Journal of Botany, V1, P345, DOI 10.1111/j.1756-1051.1981.tb00704.x.
   Bieras AC, 2009, TREES-STRUCT FUNCT, V23, P451, DOI 10.1007/s00468-008-0295-7.
   Boerjan W, 2003, ANNU REV PLANT BIOL, V54, P519, DOI 10.1146/annurev.arplant.54.031902.134938.
   Bylesjo M, 2008, BMC PLANT BIOL, V8, DOI 10.1186/1471-2229-8-82.
   Byng JW, 2016, BOT J LINN SOC, V181, P1, DOI 10.1111/boj.12385.
   Campiteli MG, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.026703.
   Carlin M, 2000, PATTERN RECOGN LETT, V21, P1013, DOI 10.1016/S0167-8655(00)00061-1.
   Carpita NC, 1996, ANNU REV PLANT PHYS, V47, P445, DOI 10.1146/annurev.arplant.47.1.445.
   CHAMBOLLE A, 1995, SIAM J APPL MATH, V55, P827, DOI 10.1137/S0036139993257132.
   Chen C. H., 1992, HDB PATTERN RECOGNIT.
   De Villiers BJ, 2010, BOT J LINN SOC, V164, P246, DOI 10.1111/j.1095-8339.2010.01085.x.
   Donaldson LA, 2001, PHYTOCHEMISTRY, V57, P859, DOI 10.1016/S0031-9422(01)00049-8.
   Durigan Giselda, 1999, Hoehnea, V26, P149.
   Endress PK, 2002, BOT REV, V68, P545, DOI 10.1663/0006-8101(2002)068{[}0545:MAASIT]2.0.CO;2.
   Evert R.F., 2006, ESAUS PLANT ANATOMY, V3.
   Fontenelle G. B., 1994, Botanical Journal of the Linnean Society, V116, P111, DOI 10.1111/j.1095-8339.1994.tb00426.x.
   Foroughbakhch R, 2008, PHYTON-INT J EXP BOT, V77, P113.
   Gonzalez R.C., 1987, DIGITAL IMAGE PROCES, Vsecond.
   Gratani L, 2006, TREES-STRUCT FUNCT, V20, P549, DOI 10.1007/s00468-006-0070-6.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   Judd W.S., 2008, PLANT SYSTEMATICS PH, V3rd.
   KEATING RC, 1984, ANN MO BOT GARD, V62, P538.
   Kocsis M, 2004, PLANT SYST EVOL, V248, P205, DOI 10.1007/s00606-002-0144-0.
   Lexer C, 2009, TAXON, V58, P349, DOI 10.1002/tax.582003.
   Lim FS, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P825.
   Materka A., 1998, TEXTURE ANAL METHODS, P9.
   Metcalf C. R., 1979, ANATOMY DICOTYLEDONS.
   Morellato LPC, 2000, BIOTROPICA, V32, P811, DOI 10.1111/j.1744-7429.2000.tb00620.x.
   Mugnai S, 2008, PLANT SYST EVOL, V270, P95, DOI 10.1007/s00606-007-0601-7.
   MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503.
   Oliveira-Filho A.T., 2002, CERRADOS BRAZIL ECOL, P91, DOI DOI 10.7312/OLIV12042-007.
   PARKER ML, 1995, J SCI FOOD AGR, V68, P337, DOI 10.1002/jsfa.2740680313.
   Petitot J, 2003, J PHYSIOLOGY-PARIS, V97, P335, DOI 10.1016/j.jphysparis.2003.09.007.
   Plotze RD, 2005, CAN J BOT, V83, P287, DOI {[}10.1139/b05-002, 10.1139/B05-002].
   RATHCKE B, 1985, ANNU REV ECOL SYST, V16, P179, DOI 10.1146/annurev.es.16.110185.001143.
   Rossatto DR, 2009, AUST J BOT, V57, P439, DOI 10.1071/BT09045.
   Rossatto DR, 2011, PLANT SYST EVOL, V291, P103, DOI 10.1007/s00606-010-0366-2.
   Samuels L, 2008, ANNU REV PLANT BIOL, V59, P683, DOI 10.1146/annurev.arplant.59.103006.093219.
   Stern WL, 2009, BOT J LINN SOC, V160, P21, DOI 10.1111/j.1095-8339.2009.00818.x.
   Stuessy T.F., 1990, PLANT TAXONOMY.
   WU CM, 1992, IEEE T MED IMAGING, V11, P141, DOI 10.1109/42.141636.
   Xu SJ, 2006, PATTERN RECOGN LETT, V27, P1942, DOI 10.1016/j.patrec.2006.05.005.},
Number-of-Cited-References = {50},
Times-Cited = {14},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {15},
Journal-ISO = {Botany},
Doc-Delivery-Number = {826YZ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000295392800005},
DA = {2023-08-12},
}

@article{ WOS:000933818300001,
Author = {Kang, Feilong and Li, Jia and Wang, Chunguang and Wang, Fuxiang},
Title = {A Lightweight Neural Network-Based Method for Identifying Early-Blight
   and Late-Blight Leaves of Potato},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2023},
Volume = {13},
Number = {3},
Month = {FEB},
Abstract = {Crop pests and diseases are one of the most critical disasters that
   limit agricultural production. In this paper, we trained a lightweight
   convolutional neural network model and built a Django framework-based
   potato disease leaf recognition system, which can recognize three types
   of potato leaf images including early blight, late blight, and healthy.
   A lightweight, neural network-based model for the identification of
   early potato leaf diseases significantly reduces the number of model
   parameters, whereas the accuracy of Top-1 identification is over 93\%.
   We imported the trained model into the Django framework to build a
   website for a potato early leaf disease identification system, thus
   providing technical support for the implementation of a mobile-based
   potato leaf disease identification and early warning system.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Li, J (Corresponding Author), Inner Mongolia Agr Univ, Hohhot 010018, Peoples R China.
   Li, J (Corresponding Author), Inner Mongolia Autonomous Reg Key Lab Big Data Res, Hohhot 010018, Peoples R China.
   Kang, Feilong; Li, Jia; Wang, Chunguang; Wang, Fuxiang, Inner Mongolia Agr Univ, Hohhot 010018, Peoples R China.
   Li, Jia, Inner Mongolia Autonomous Reg Key Lab Big Data Res, Hohhot 010018, Peoples R China.},
DOI = {10.3390/app13031487},
Article-Number = {1487},
EISSN = {2076-3417},
Keywords = {convolutional neural networks; machine learning; potato disease leaf;
   Django framework},
Research-Areas = {Chemistry; Engineering; Materials Science; Physics},
Web-of-Science-Categories  = {Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied},
Author-Email = {lijia@imau.edu.cn},
Affiliations = {Inner Mongolia Agricultural University},
ResearcherID-Numbers = {.., What/IXW-6776-2023
   Yang, Ying/ADL-4165-2022
   li, wei/IUQ-2973-2023
   liu, xinyu/IWD-6630-2023
   ZHOU, YUE/IZE-6277-2023
   yang, yun/IZE-1092-2023
   Li, Jiaxin/IWM-4023-2023
   li, yan/ITU-9719-2023
   liu, lingling/IUQ-7478-2023},
ORCID-Numbers = {Yang, Ying/0000-0002-3469-7681
   },
Funding-Acknowledgement = {Natural Science Foundation of Inner Mongolia Autonomous Region of China
   {[}2021BS03038]; Research Program of science and technology at
   Universities of Inner Mongolia Autonomous Region of China {[}NJZY21457];
   National Natural Science Foundation of China {[}NDYB2018-38]; Inner
   Mongolia Agricultural University High-level Talent Research Start-up
   Project {[}32060415, NDYB2019-27];  {[}32160813]},
Funding-Text = {This research was funded by Natural Science Foundation of Inner Mongolia
   Autonomous Region of China, grant number 2021BS03038; Research Program
   of science and technology at Universities of Inner Mongolia Autonomous
   Region of China, grant number NJZY21457; National Natural Science
   Foundation of China, grant number 32160813; Inner Mongolia Agricultural
   University High-level Talent Research Start-up Project, grant number
   NDYB2019-27; Inner Mongolia Agricultural University High-level Talent
   Research Start-up Project, grant number NDYB2018-38; and National
   Natural Science Foundation of China, grant number 32060415.},
Cited-References = {Amara J., 2017, P BTW WORKSH STUTTG, P79.
   Brahimi M, 2017, APPL ARTIF INTELL, V31, P299, DOI 10.1080/08839514.2017.1315516.
   Brock Andrew, 2018, PREPRINT.
   Caron M., 2021, ABS210414294 CORR, P9650.
   Chen WR, 2022, MULTIMED TOOLS APPL, V81, P20797, DOI 10.1007/s11042-022-12620-w.
   Deng C, 2019, IEEE T GEOSCI REMOTE, V57, P1741, DOI 10.1109/TGRS.2018.2868851.
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921.
   Elsken T., 2018, ARXIV180805377.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Howard A.G., 2017, ABS170404861 CORR.
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140.
   Huang G., 2017, P 2017 IEEE C COMP V, P4700, DOI {[}DOI 10.1109/CVPR.2017.243, 10.1109/CVPR.2017.243].
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Liu C, 2022, LECT NOTES COMPUT SC, V13683, P370, DOI 10.1007/978-3-031-20050-2\_22.
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0\_2.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Nazki H, 2020, COMPUT ELECTRON AGR, V168, DOI 10.1016/j.compag.2019.105117.
   Pandey P, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01348.
   Pradhan Shradha S., 2020, Proceedings of International Conference on Wireless Communication. ICWiCOM 2019. Lecture Notes on Data Engineering and Communications Technologies (LNDECT 36), P575, DOI 10.1007/978-981-15-1002-1\_58.
   Qian W, 2021, AAAI CONF ARTIF INTE, V35, P2458.
   Ramcharan A, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01852.
   Raza SEA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0123262.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Too EC, 2019, COMPUT ELECTRON AGR, V161, P272, DOI 10.1016/j.compag.2018.03.032.
   Wan H, 2020, ICMLSC 2020: PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND SOFT COMPUTING, P5, DOI 10.1145/3380688.3380697.
   Wang FX, 2021, FOOD SCI NUTR, V9, P4420, DOI 10.1002/fsn3.2415.
   Wang XX, 2022, LECT NOTES COMPUT SC, V13680, P668, DOI 10.1007/978-3-031-20044-1\_38.
   Yan JC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4988.
   Yan JC, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P167, DOI 10.1145/2911996.2912035.
   Yang X, 2023, IEEE T PATTERN ANAL, V45, P2384, DOI 10.1109/TPAMI.2022.3166956.
   Yuan CG, 2019, APPL INTELL, V49, P3570, DOI 10.1007/s10489-019-01468-7.
   Zhao YR, 2016, SCI REP-UK, V6, DOI 10.1038/srep27790.
   Zhong Y, 2020, COMPUT ELECTRON AGR, V168, DOI 10.1016/j.compag.2019.105146.},
Number-of-Cited-References = {35},
Times-Cited = {3},
Usage-Count-Last-180-days = {16},
Usage-Count-Since-2013 = {16},
Journal-ISO = {Appl. Sci.-Basel},
Doc-Delivery-Number = {9A1II},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000933818300001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000499179900001,
Author = {Kattenborn, Teja and Eichel, Jana and Fassnacht, Fabian Ewald},
Title = {Convolutional Neural Networks enable efficient, accurate and
   fine-grained segmentation of plant species and communities from
   high-resolution UAV imagery},
Journal = {SCIENTIFIC REPORTS},
Year = {2019},
Volume = {9},
Month = {NOV 27},
Abstract = {Recent technological advances in remote sensing sensors and platforms,
   such as high-resolution satellite imagers or unmanned aerial vehicles
   (UAV), facilitate the availability of fine-grained earth observation
   data. Such data reveal vegetation canopies in high spatial detail.
   Efficient methods are needed to fully harness this unpreceded source of
   information for vegetation mapping. Deep learning algorithms such as
   Convolutional Neural Networks (CNN) are currently paving new avenues in
   the field of image analysis and computer vision. Using multiple
   datasets, we test a CNN-based segmentation approach (U-net) in
   combination with training data directly derived from visual
   interpretation of UAV-based high-resolution RGB imagery for fine-grained
   mapping of vegetation species and communities. We demonstrate that this
   approach indeed accurately segments and maps vegetation species and
   communities (at least 84\% accuracy). The fact that we only used RGB
   imagery suggests that plant identification at very high spatial
   resolutions is facilitated through spatial patterns rather than spectral
   information. Accordingly, the presented approach is compatible with
   low-cost UAV systems that are easy to operate and thus applicable to a
   wide range of users.},
Publisher = {NATURE PORTFOLIO},
Address = {HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY},
Type = {Article},
Language = {English},
Affiliation = {Kattenborn, T (Corresponding Author), KIT, Inst Geog \& Geoecol IFGG, Kaiserstr 12, D-76131 Karlsruhe, Germany.
   Kattenborn, Teja; Eichel, Jana; Fassnacht, Fabian Ewald, KIT, Inst Geog \& Geoecol IFGG, Kaiserstr 12, D-76131 Karlsruhe, Germany.},
DOI = {10.1038/s41598-019-53797-9},
Article-Number = {17656},
ISSN = {2045-2322},
Keywords-Plus = {SPATIAL-RESOLUTION; VEGETATION; CLASSIFICATION; SYSTEMS; MODEL},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {teja.kattenborn@kit.edu},
Affiliations = {Helmholtz Association; Karlsruhe Institute of Technology},
ResearcherID-Numbers = {Kattenborn, Teja/I-7457-2019
   Fassnacht, Fabian/AAL-5524-2020
   Fassnacht, Fabian/F-3860-2017},
ORCID-Numbers = {Kattenborn, Teja/0000-0001-7381-3828
   Fassnacht, Fabian/0000-0003-1284-9573},
Funding-Acknowledgement = {German National Space Agency DLR (Deutsches Zentrum fur Luft-und
   Raumfahrt e.V.) {[}50EE1535, 50EE1536]; Hanna Bremer Foundation},
Funding-Text = {Datasets used in this study were partly acquired within the SaMovar
   project (Satellite-based Monitoring of invasive species in central
   Chile) funded by the German National Space Agency DLR (Deutsches Zentrum
   fur Luft-und Raumfahrt e.V.) on behalf of the German Federal Ministry of
   Economy and Technology based on the Bundestag resolution 50EE1535 and
   50EE1536. J. Eichel appreciates funding by the Hanna Bremer Foundation
   for fieldwork in Mueller glacier foreland. Many thanks to Timo Schmid,
   Felix Schiefer, Denis Debroize and Jorge Petri for assistance in image
   interpretation. Furthermore, we thank David Hedding, Stefan Winkler, and
   Daniel Draebing for support during UAV and vegetation surveys and the
   Department of Conservation for granting permission for research and
   drone use in the New Zealand test site.},
Cited-References = {Adam E, 2010, WETL ECOL MANAG, V18, P281, DOI 10.1007/s11273-009-9169-z.
   Angermueller C, 2016, MOL SYST BIOL, V12, DOI 10.15252/msb.20156651.
   {[}Anonymous], 2016, IEEE IJCNN, DOI {[}10.1109/IJCNN.2016.7727386, DOI 10.1109/IJCNN.2016.7727386].
   Aplin P, 2006, INT J REMOTE SENS, V27, P2123, DOI 10.1080/01431160500396477.
   Brodrick PG, 2019, TRENDS ECOL EVOL, V34, P734, DOI 10.1016/j.tree.2019.03.006.
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963.
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP).
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184.
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195.
   Colomina I, 2014, ISPRS J PHOTOGRAMM, V92, P79, DOI 10.1016/j.isprsjprs.2014.02.013.
   Corbane C, 2015, INT J APPL EARTH OBS, V37, P7, DOI 10.1016/j.jag.2014.11.005.
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693.
   Doll P., 2017, ARXIV170306870, P2961.
   Fassnacht FE, 2017, FORESTRY, V90, P613, DOI 10.1093/forestry/cpx014.
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013.
   Foody GM, 1996, ECOL MODEL, V85, P3, DOI 10.1016/0304-3800(95)00012-7.
   GELLATLY AF, 1982, NEW ZEAL J BOT, V20, P343, DOI 10.1080/0028825X.1982.10428503.
   Goeau H., 1866, CEUR WORKSH P 2017.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   He K., 2016, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2016.90.
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI {[}10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175].
   Hsieh PF, 2001, IEEE T GEOSCI REMOTE, V39, P2657, DOI 10.1109/36.975000.
   Huang BH, 2018, INT GEOSCI REMOTE SE, P6947, DOI 10.1109/IGARSS.2018.8518525.
   Immitzer M, 2018, REMOTE SENS ENVIRON, V204, P690, DOI 10.1016/j.rse.2017.09.031.
   Ioffe S., 2015, INT C MACHINE LEARNI, DOI DOI 10.1007/S13398-014-0173-7.2.
   Jegou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156.
   Joly A, 2016, MULTIMEDIA SYST, V22, P751, DOI 10.1007/s00530-015-0462-9.
   Kaartinen H, 2015, FORESTS, V6, P3218, DOI 10.3390/f6093218.
   Kattenborn T, 2019, REMOTE SENS ENVIRON, V227, P61, DOI 10.1016/j.rse.2019.03.025.
   Krahenbuhl P, 2011, PROC NEURIPS, P109.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Leps J., 1992, RELIABLE ARE OUR VEG, V3.
   Lettao PJ, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2298.
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549.
   Lisein J, 2013, FORESTS, V4, P922, DOI 10.3390/f4040922.
   Lopatin J, 2019, REMOTE SENS ECOL CON, V5, P302, DOI 10.1002/rse2.109.
   Lopatin J, 2017, REMOTE SENS ENVIRON, V201, P12, DOI 10.1016/j.rse.2017.08.031.
   Lunetta R. S., 1991, PHOTOGRAMM ENG REMOT.
   Maggiori E, 2017, IEEE T GEOSCI REMOTE, V55, P7092, DOI 10.1109/TGRS.2017.2740362.
   Maninis KK, 2018, IEEE T PATTERN ANAL, V40, P819, DOI 10.1109/TPAMI.2017.2700300.
   Milas AS, 2017, INT J REMOTE SENS, V38, P3084, DOI 10.1080/01431161.2016.1274449.
   Mullerova J, 2013, INT J APPL EARTH OBS, V25, P55, DOI 10.1016/j.jag.2013.03.004.
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI {[}10.1162/NECO\_a\_00990, 10.1162/neco\_a\_00990].
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4\_28.
   Rzanny M, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0462-4.
   Schmidtlein S, 2010, J VEG SCI, V21, P1162, DOI 10.1111/j.1654-1103.2010.01221.x.
   Schwarz M., 2015, P INT C ROB AUT SEAT, DOI {[}10.1109/ICRA.2015.7139363, DOI 10.1109/ICRA.2015.7139363].
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9\_28.
   Toth C, 2016, ISPRS J PHOTOGRAMM, V115, P22, DOI 10.1016/j.isprsjprs.2015.10.004.
   Ustin SL, 2010, NEW PHYTOL, V186, P795, DOI 10.1111/j.1469-8137.2010.03284.x.
   Valbuena R, 2010, SPAN J AGRIC RES, V8, P1047.
   Verrelst J, 2015, ISPRS J PHOTOGRAMM, V108, P273, DOI 10.1016/j.isprsjprs.2015.05.005.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Wagner FH, 2019, REMOTE SENS ECOL CON, V5, P360, DOI 10.1002/rse2.111.
   White JC, 2016, CAN J REMOTE SENS, V42, P619, DOI 10.1080/07038992.2016.1207484.
   Winkler S, 2018, HOLOCENE, V28, P778, DOI 10.1177/0959683618756802.
   Xie YC, 2008, J PLANT ECOL, V1, P9, DOI 10.1093/jpe/rtm005.},
Number-of-Cited-References = {57},
Times-Cited = {115},
Usage-Count-Last-180-days = {15},
Usage-Count-Since-2013 = {68},
Journal-ISO = {Sci Rep},
Doc-Delivery-Number = {JQ8HP},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000499179900001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000736740600010,
Author = {Castro, Pedro and Moreira, Gladston and Luz, Eduardo},
Title = {An End-to-End Deep Learning System for Hop Classification},
Journal = {IEEE LATIN AMERICA TRANSACTIONS},
Year = {2022},
Volume = {20},
Number = {3},
Pages = {430-442},
Month = {MAR},
Abstract = {Automatic classification of plant species is a very challenging and
   widely studied problem in the literature. Distinguishing different
   varieties within the same species is an even more challenging task
   although less explored. Nevertheless, for some species distinguishing
   the varieties within the species can be of paramount importance.Hops, a
   plant widely used in beer production, has over 250 cataloged varieties.
   Although the varieties have similar appearances, their chemical
   components, which influence the aroma and flavor of the drink, are quite
   heterogeneous. Therefore, it is important for producers to distinguish
   which variety the plant belongs to in a simple manner.In this work, an
   end-to-end deep learning system is proposed to automate the task of hop
   classification. Five architectures are proposed and evaluated with an
   uncontrolled environment dataset that includes 12 varieties of hops on
   1592 images, from three different cell phone cameras. The best
   architecture automatically detects the hop leaves on the image and
   performs the classification using the information of up to 10 leaves.
   The method achieved an accuracy of 95.69\% with an inference time of
   672ms. To reach such figures, state-of-the-art convolutional blocks were
   explored along with data augmentation techniques. Our results show that
   the system is robust and has a low computational cost.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Castro, P (Corresponding Author), Univ Fed Ouro Preto, Dept Comp, BR-35400000 Ouro Preto, MG, Brazil.
   Castro, Pedro; Moreira, Gladston; Luz, Eduardo, Univ Fed Ouro Preto, Dept Comp, BR-35400000 Ouro Preto, MG, Brazil.},
DOI = {10.1109/TLA.2022.9667141},
ISSN = {1548-0992},
Keywords = {Deep learning; Convolutional neural networks; Visualization; Task
   analysis; Image segmentation; IEEE transactions; Computer architecture;
   Hop; Convolutional neural network; Leaf recognition; Data augmentation},
Keywords-Plus = {HUMULUS-LUPULUS L.; ACIDS},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic},
Author-Email = {pedro.hnc@aluno.ufop.edu.br
   gladston@ufop.edu.br
   eduluz@ufop.edu.br},
Affiliations = {Universidade Federal de Ouro Preto},
ResearcherID-Numbers = {Moreira, Gladston J. P./H-9396-2012
   },
ORCID-Numbers = {Moreira, Gladston J. P./0000-0001-7747-5926
   Castro, Pedro/0000-0001-5006-6508
   Luz, Eduardo/0000-0001-5249-1559},
Cited-References = {A Fakhri A. Nasir, 2014, Modern Applied Science, V8, P121.
   Almaguer C, 2014, J I BREWING, V120, P289, DOI 10.1002/jib.160.
   Almeida AD, 2020, INT J FOOD SCI TECH, V55, P340, DOI 10.1111/ijfs.14311.
   Arruda TR, 2021, LWT-FOOD SCI TECHNOL, V141, DOI 10.1016/j.lwt.2021.110905.
   Astray G, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10155074.
   Bhattacharjee S, 2017, IEEE CONF COMM NETW, P200.
   Biendl M., 2009, MBAA TQ, V16, P84048, DOI 10.1094/TQ-46-2-0416-01.
   Bradski G, 2000, DR DOBBS J, V25, P120.
   Castro P., 2021, UFOP HOP VARIETIES D, DOI {[}10.6084/m9.figshare.14933178, DOI 10.6084/M9.FIGSHARE.14933178].
   Chadwick LR, 2006, PHYTOMEDICINE, V13, P119, DOI 10.1016/j.phymed.2004.07.006.
   Almeida AD, 2021, J AM SOC BREW CHEM, V79, P156, DOI 10.1080/03610470.2020.1795586.
   De Cooman L, 1998, PHYTOCHEM ANALYSIS, V9, P145, DOI 10.1002/(SICI)1099-1565(199805/06)9:3\&lt;145::AID-PCA393\&gt;3.0.CO;2-K.
   DeVries T., 2017, ARXIV170804552.
   Duarte LM, 2018, ELECTROPHORESIS, V39, P1399, DOI 10.1002/elps.201700420.
   Duchi J, 2011, J MACH LEARN RES, V12, P2121.
   Everingham M., 2010, INT J COMPUT VISION, DOI DOI 10.1007/s11263-009-0275-4.
   Farag MA, 2014, METABOLOMICS, V10, P21, DOI 10.1007/s11306-013-0547-4.
   Glorot Xavier, 2010, P 13 INT C ARTIFICIA, P249.
   Han Lee Sue, 2016, CLEF WORK NOTES, V1, P502.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Healey J., 2016, HOPS LIST 265 BEER H, V1.
   Ho Y, 2020, IEEE ACCESS, V8, P4806, DOI 10.1109/ACCESS.2019.2962617.
   Horgan G. W., 1998, CHALLENGE IMAGE RETR, P1.
   Hosang J, 2017, PROC CVPR IEEE, P6469, DOI 10.1109/CVPR.2017.685.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Javanmardi S, 2021, J STORED PROD RES, V92, DOI 10.1016/j.jspr.2021.101800.
   Jenks M. A, 2011, PLANT NOMENCLATURE.
   Jeon WS, 2017, INT J FUZZY LOG INTE, V17, P26, DOI 10.5391/IJFIS.2017.17.1.26.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Kandel I, 2020, J IMAGING, V6, DOI 10.3390/jimaging6090092.
   Kovacevic M, 2002, FOOD CHEM, V77, P489, DOI 10.1016/S0308-8146(02)00114-0.
   Kramer B, 2015, J APPL MICROBIOL, V118, P648, DOI 10.1111/jam.12717.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8.
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1\_48.
   Lingwal S, 2021, MULTIMED TOOLS APPL, V80, P35441, DOI 10.1007/s11042-020-10174-3.
   Mercurio DI, 2019, INT CONF SYST ENG, P120.
   Namakvs I, 2017, INFORM TECHNOL MANAG, V20, P40, DOI DOI 10.1515/ITMS-2017-0007.
   Nuutinen T, 2018, EUR J MED CHEM, V157, P198, DOI 10.1016/j.ejmech.2018.07.076.
   oth T, 2016, CLEF WORKING NOTES, P569.
   Rahman Md Atiqur, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P234, DOI 10.1007/978-3-319-50835-1\_22.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075.
   Shellie RA, 2009, J SEP SCI, V32, P3720, DOI 10.1002/jssc.200900422.
   Steenackers B, 2015, FOOD CHEM, V172, P742, DOI 10.1016/j.foodchem.2014.09.139.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Sunderhauf Niko, 2014, P CLEF WORK NOT, P756.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Tavakoli H, 2021, COMPUT ELECTRON AGR, V181, DOI 10.1016/j.compag.2020.105935.
   Van Cleemput M, 2009, J NAT PROD, V72, P1220, DOI 10.1021/np800740m.
   Wang B, 2019, IEEE ACCESS, V7, P151754, DOI 10.1109/ACCESS.2019.2947510.
   Wiatowski T, 2018, IEEE T INFORM THEORY, V64, P1845, DOI 10.1109/TIT.2017.2776228.
   Yalcin H, 2016, INT CONF AGRO-GEOINF, P233.
   Yang KL, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10111721.
   Yu Han Liu, 2018, Journal of Physics: Conference Series, V1087, DOI 10.1088/1742-6596/1087/6/062032.
   Zanoli P, 2008, J ETHNOPHARMACOL, V116, P383, DOI 10.1016/j.jep.2008.01.011.
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993.
   Zhu XL, 2018, COGN SYST RES, V52, P223, DOI 10.1016/j.cogsys.2018.06.008.
   Zhu YX, 2019, NEUROCOMPUTING, V365, P191, DOI 10.1016/j.neucom.2019.07.016.},
Number-of-Cited-References = {60},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {5},
Journal-ISO = {IEEE Latin Am. Trans.},
Doc-Delivery-Number = {XY1KO},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000736740600010},
DA = {2023-08-12},
}

@article{ WOS:000496333300011,
Author = {Amend, Sandra and Brandt, David and Di Marco, Daniel and Dipper, Tobias
   and Gaessler, Gabriel and Hoeferlin, Markus and Gohlke, Maurice and
   Kesenheimer, Katharina and Lindner, Peter and Leidenfrost, Roland and
   Michaels, Andreas and Mugele, Tobias and Mueller, Arthur and Riffel,
   Tanja and Sampangi, Yeshwanth and Winkler, Jan},
Title = {Weed Management of the Future},
Journal = {KUNSTLICHE INTELLIGENZ},
Year = {2019},
Volume = {33},
Number = {4, SI},
Pages = {411-415},
Month = {DEC},
Abstract = {The methods used to protect agricultural products currently undergo
   drastic changes. Artificial Intelligence is a prime candidate to
   overcome two challenges faced by farmers around the world: The
   increasing cost and decreasing availability of human labor for weed
   control, and the growing global restriction of herbicides. Deep Learning
   is one of the most prominent approaches for applying AI to all kinds of
   use cases in industrial applications, entertainment, and security. Its
   latest field of application is plant classification that enables
   automated weed control and precise spot spraying of herbicides. While
   cheap, powerful platforms for deploying classification mechanisms are
   widely available, this comes at the cost of expensive and effort rich
   classifier training. This effectively makes Deep Learning-based
   approaches unavailable for the majority of the agricultural sector.
   Deepfield Robotics presents a systematic approach for deploying AI onto
   fields at large, including the learnings that led to their
   self-contained AI driven plant classification modules that relieve
   individuals from having to deploy their own AI solution. The same
   technology acts as enabler for more agricultural domains, such as
   targeted fertilization, nano irrigation, and automated phenotyping. This
   article documents Deepfield Robotics' findings and vision on how AI can
   be the workhorse for agricultural weeding labor.},
Publisher = {SPRINGER HEIDELBERG},
Address = {TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY},
Type = {Article},
Language = {English},
Affiliation = {Winkler, J (Corresponding Author), Deepfield Robot, Gronerstr 5, D-71636 Ludwigsburg, Germany.
   Amend, Sandra; Brandt, David; Di Marco, Daniel; Dipper, Tobias; Gaessler, Gabriel; Hoeferlin, Markus; Gohlke, Maurice; Kesenheimer, Katharina; Lindner, Peter; Leidenfrost, Roland; Michaels, Andreas; Mugele, Tobias; Mueller, Arthur; Riffel, Tanja; Sampangi, Yeshwanth; Winkler, Jan, Deepfield Robot, Gronerstr 5, D-71636 Ludwigsburg, Germany.},
DOI = {10.1007/s13218-019-00617-x},
ISSN = {0933-1875},
EISSN = {1610-1987},
Keywords = {Autonomous robots; Agricultural robotics; Weed management; Deep learning},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {jan.winkler@de.bosch.com},
ORCID-Numbers = {Winkler, Jan/0000-0002-5064-4459},
Cited-References = {Hedlund B, 2019, IS GLYPHOSATE BANNED.
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.
   Jegou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156.
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
   Lottes P, 2018, JOINT STEM DETECTION.
   Lottes P, 2018, FULLY CONVOLUTIONAL.
   Milioto A, 2017, REAL TIME BLOB WISE.
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4\_28.
   Ruckelshausen A, 2009, BONIROB AUTONOMOUS F.
   Union of concerned scientists, 2013, RIS SUP WHAT DO IT.
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634.},
Number-of-Cited-References = {11},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {16},
Journal-ISO = {Kunstl. Intell.},
Doc-Delivery-Number = {JM6PE},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000496333300011},
DA = {2023-08-12},
}

@inproceedings{ WOS:000240893200054,
Author = {Zhang, Yun and He, Yong and Fang, Hui},
Editor = {Xu, K and Luo, Q and Xing, D and Priezzhev, AV and Tuchin, VV},
Title = {Computer-vision-based weed identification of images acquired by 3CCD
   camera},
Booktitle = {FOURTH INTERNATIONAL CONFERENCE ON PHOTONICS AND IMAGING IN BIOLOGY AND
   MEDICINE, PTS 1 AND 2},
Series = {Proceedings of SPIE},
Year = {2006},
Volume = {6047},
Number = {1\&2},
Note = {4th International Conference on Photonics and Imaging in Biology and
   Medicine, Tianjin, PEOPLES R CHINA, SEP 03-06, 2005},
Organization = {Tianjin Univ; State Key Lab Precis Measuring Technol \& Instruments;
   Huazhong Univ Sci \& Technol; S China Normal Univ; Natl Nat Sci Fdn
   China; SPIE Russia Chapter; Int Laser Ctr M V Lomonosov Moscow State
   Univ; Bio-Opt \& Laser Med Comm Chinese Opt Soc; Minist Educ China, Key
   Lab Opto-Elect Informat Sci \& Technol; Tianjin Univ, Sci \& Technol
   Garden; Tianjin Econ-Technol Dev Area},
Abstract = {Selective application of herbicide to weeds at an earlier stage in crop
   growth is an important aspect of site-specific management of field
   crops, For approaches more adaptive in developing the on-line weed
   detecting application, more researchers involves in studies on image
   processing techniques for intensive computation and feature extraction
   tasks to identify the weeds from the other crops and soil background.
   This paper investigated the potentiality of applying the digital images
   acquired by the MegaPlus (TM) MS3100 3-CCD camera to segment the
   background soil from the plants in question and further recognize weeds
   from the crops using the Matlab script language. The image of the
   near-infrared waveband (center 800 nm; width 65 nm) was selected
   principally for segmenting soil and identifying the cottons from the
   thistles was achieved based on their respective relative area (pixel
   amount) in the whole image. The results show adequate recognition that
   the pixel proportion of soil, cotton leaves and thistle leaves were
   78.24\% (-0.20\% deviation). 16.66\% (+2.71\% S.D) and 4.68\% (-4.19\%
   S.D). However, problems still exists by separating and allocating single
   plants for their clustering in the images. The information in the images
   acquired via the other two channels. i.e., the green and the red bands,
   need to be extracted to help the crop/weed discrimination. More optical
   specimens should be acquired for calibration and validation to establish
   the weed-detection model that could be effectively applied in fields.},
Publisher = {SPIE-INT SOC OPTICAL ENGINEERING},
Address = {1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zhang, Y (Corresponding Author), Zhejiang Univ, Coll Biosyst Engn \& Food Sci, 268 Kaixuan Rd, Hangzhou 310029, Peoples R China.
   Zhang, Yun; He, Yong; Fang, Hui, Zhejiang Univ, Coll Biosyst Engn \& Food Sci, 268 Kaixuan Rd, Hangzhou 310029, Peoples R China.},
DOI = {10.1117/12.710881},
Article-Number = {604711},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {0-8194-6080-X},
Keywords = {plant recognition; weed discrimination; soil background segmentation;
   leaf area; 3CCD camera; Matlab},
Keywords-Plus = {DISCRIMINATION; DESIGN},
Research-Areas = {Biophysics; Optics; Physics; Imaging Science \& Photographic Technology;
   Spectroscopy},
Web-of-Science-Categories  = {Biophysics; Optics; Physics, Applied; Imaging Science \& Photographic
   Technology; Spectroscopy},
Author-Email = {yhe@zju.edu.cn},
Affiliations = {Zhejiang University},
ResearcherID-Numbers = {He, Yong/E-3218-2010},
ORCID-Numbers = {He, Yong/0000-0001-6752-1757},
Funding-Acknowledgement = {Teaching and Research Award Program for Outstanding Ytiung Teachers in
   Higher Education Institutions of MOE, PRC, Natural Science Foundation of
   China {[}30270773]; Specialized Research Fund for the Doctoral Program
   of Higher Education {[}20040335034]; Natural Science Foundation of
   Zhejiang Province, China {[}RC02067]},
Funding-Text = {This study was supported by the Teaching and Research Award Program for
   Outstanding Ytiung Teachers in Higher Education Institutions of MOE,
   PRC, Natural Science Foundation of China (Project No: 30270773),
   Specialized Research Fund for the Doctoral Program of Higher Education
   (Project No: 20040335034) and Natural Science Foundation of Zhejiang
   Province, China (Project No: RC02067).},
Cited-References = {Brown RB, 1995, T ASAE, V38, P1659, DOI 10.13031/2013.27992.
   Burks TF, 2000, T ASAE, V43, P1029, DOI 10.13031/2013.2971.
   Burks TF, 2000, T ASAE, V43, P441, DOI 10.13031/2013.2723.
   {*}CHIN AGR YB COMM, 2003, CHIN AGR YB.
   Christensen S, 2003, WEED RES, V43, P276, DOI 10.1046/j.1365-3180.2003.00344.x.
   {*}DUNC TECH, 2002, DTCONTR CAM CONTR SO.
   Gonzalez R. C., 2006, DIGITAL IMAGE PROCES.
   Hemming J, 2001, J AGR ENG RES, V78, P233, DOI 10.1006/jaer.2000.0639.
   Kavdir I, 2004, COMPUT ELECTRON AGR, V44, P153, DOI 10.1016/j.compag.2004.03.006.
   Lamb DW, 2001, J AGR ENG RES, V78, P117, DOI 10.1006/jaer.2000.0630.
   Mao WenHua, 2004, Transactions of the Chinese Society of Agricultural Engineering, V20, P43.
   {*}MATHW INC, 2002, MATL 6 5 US MANN.
   Moshou D, 2001, COMPUT ELECTRON AGR, V31, P5, DOI 10.1016/S0168-1699(00)00170-8.
   Tian L, 1999, T ASAE, V42, P893, DOI 10.13031/2013.13269.
   Wang N, 2001, T ASAE, V44, P409, DOI 10.13031/2013.4673.
   Zwiggelaar R, 1998, CROP PROT, V17, P189, DOI 10.1016/S0261-2194(98)00009-X.},
Number-of-Cited-References = {16},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BFB74},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000240893200054},
DA = {2023-08-12},
}

@inproceedings{ WOS:000349255600043,
Author = {Russin, Nuril Aslina Che and Jamil, Nursuriati and Nordin, Sharifalillah
   and Awang, Khalil},
Book-Group-Author = {IEEE},
Title = {Plant Species Identification by using Scale Invariant Feature Transform
   (SIFT) and Grid Based Colour Moment (GBCM)},
Booktitle = {2013 IEEE CONFERENCE ON OPEN SYSTEMS (ICOS)},
Series = {IEEE Conference on Open Systems},
Year = {2013},
Pages = {226-230},
Note = {IEEE Conference on Open Systems (ICOS), Kuching, MALAYSIA, DEC 02-04,
   2013},
Organization = {IEEE; IEEE Comp Soc; Univ Technol Malaysia; IEEE Comp Soc Malaysia Sect},
Abstract = {Plant identification using plant leaves is a very challenging task. The
   most important and crucial phase in plant identification is the phase of
   feature extraction. This paper presents a method of shape feature
   extraction that is Scale Invariant Feature Transform (SIFT) and colour
   feature extraction Grid Based Colour Moment (GBCM) to identify plant.
   Forty plant species images were collected from their natural habitats
   and captured under various time of the day. These plant images are then
   used as ground truth images. These images are further rotated and scaled
   to produce another forty test images. The extracted features of the test
   images are then identified by calculating their Euclidean Distance (ED)
   against the ground truth and achieved identification accuracy rate of
   87.5 percent. The proposed feature extraction methods showed potential
   in identifying plant images captured under natural illumination.
   However, further work need to be done to improve accuracy of plant
   identification.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Russin, NAC (Corresponding Author), Univ Teknol MARA, Dept Comp Sci, Fac Math \& Comp Sci, Shah Alam 40450, Selangor, Malaysia.
   Russin, Nuril Aslina Che; Jamil, Nursuriati; Nordin, Sharifalillah; Awang, Khalil, Univ Teknol MARA, Dept Comp Sci, Fac Math \& Comp Sci, Shah Alam 40450, Selangor, Malaysia.},
ISSN = {2381-3474},
Keywords = {Plant Leaf Identification; Scale Invariant Feature Transform; Colour
   Moment; Euclidean Distance},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Software Engineering; Computer Science, Theory \&
   Methods},
Author-Email = {nurilaslina@yahoo.com
   liza@tmsk.uitm.edu.my
   sharifa@tmsk.uitm.edu.my
   khalil@tmsk.uitm.edu.my},
Affiliations = {Universiti Teknologi MARA},
ResearcherID-Numbers = {JAMIL, NURSURIATI/D-1494-2012
   Jamil, Nursuriati/AAV-9816-2020
   JAMIL, PROFESOR DR NURSURIATI/HLG-1516-2023
   Nordin, Sharifalillah/ABB-6094-2021},
ORCID-Numbers = {JAMIL, NURSURIATI/0000-0003-4634-9833
   },
Cited-References = {Arora A., 2012, PLANT IDENTIFICATION.
   Bama B. S., 2011, IND J COMP SCI ENG, V2, P202.
   Chen MY, 2010, PROC SPIE, V7704, DOI 10.1117/12.853465.
   Hiremath P., 2011, P INT J COMPUTER SCI, V1, P44.
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235.
   Kadir A., 2012, INT J COMPUTER TREND, P225.
   Lowe D.G., 2001, P IEEE COMP SOC C CO, V1, P1682.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Shanwen Zhang, 2010, 2010 International Conference on Computer Application and System Modeling (ICCASM 2010), P521, DOI 10.1109/ICCASM.2010.5622528.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Ying D., 2010, P 2 INT C FUT COMP C, P437.
   Yu H, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P929, DOI 10.1109/ICIP.2002.1039125.
   Zhai CM, 2008, 2008 INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION, VOLS 1-4, P879, DOI 10.1109/ICINFA.2008.4608123.
   Zhang SW, 2009, LECT NOTES COMPUT SC, V5754, P948, DOI 10.1007/978-3-642-04070-2\_100.
   Zulkifli Zalikha, 2009, THESIS U TEKNOLOGI M.},
Number-of-Cited-References = {15},
Times-Cited = {9},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Journal-ISO = {IEEE Conference on Open Systems},
Doc-Delivery-Number = {BC0OH},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000349255600043},
DA = {2023-08-12},
}

@article{ WOS:000954846600014,
Author = {Cen, Yi and Shen, Changming and Zheng, Xiaorong and Li, Junfei and
   Jiang, Jianwei},
Title = {Development of A Fast Method for Fructus Aurantii Identification by
   Electrochemical Fingerprint},
Journal = {INTERNATIONAL JOURNAL OF ELECTROCHEMICAL SCIENCE},
Year = {2022},
Volume = {17},
Number = {11},
Month = {NOV},
Abstract = {Electrochemical analysis techniques can be used for the identification
   of plant samples. This work describes the identification of fructus
   aurantii and its closely related species by electrochemical
   fingerprinting. For a better extraction of electrochemically active
   substances, DMSO-CHCl3-CH3OH, 2:2:1, v/v was used as a solvent.
   Electrochemical fingerprints were collected in two different buffer
   solutions. The collected fingerprint profiles can be used for density
   plots construction. The plants can be automatically identified by
   feature extraction of density plot. The oxidation peaks exhibited in the
   electrochemical fingerprint are most likely the oxidation of narirutin,
   naringin, hesperidin, and neohesperidin. Therefore, HPLC was used for
   the validation of the standards and samples. Finally, electrochemical
   techniques were used to document the electrochemical behavior of these
   four substances. The results suggest that narirutin, naringin,
   hesperidin, and neohesperidin may be the most significant substances
   contributing to the electrochemical fingerprinting of fructus aurantii.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Jiang, JW (Corresponding Author), Univ Chinese Acad Sci, Canc Hosp, Zhejiang Canc Hosp, Dept Pharm, Hangzhou 310022, Zhejiang, Peoples R China.
   Jiang, JW (Corresponding Author), Chinese Acad Sci, Inst Canc \& Basic Med IBMC, Dept Pharm, Hangzhou 310022, Zhejiang, Peoples R China.
   Jiang, JW (Corresponding Author), Key Lab Tradit Chinese Med Zhejiang Prov, Integrated Tradit Chinese \& Western Med Oncol Lab, Hangzhou 310022, Zhejiang, Peoples R China.
   Cen, Yi, Ningbo Beilun Dist Peoples Hosp, Dept Pharm, Ningbo 315800, Zhejiang, Peoples R China.
   Shen, Changming; Zheng, Xiaorong; Li, Junfei; Jiang, Jianwei, Univ Chinese Acad Sci, Canc Hosp, Zhejiang Canc Hosp, Dept Pharm, Hangzhou 310022, Zhejiang, Peoples R China.
   Shen, Changming; Zheng, Xiaorong; Li, Junfei; Jiang, Jianwei, Chinese Acad Sci, Inst Canc \& Basic Med IBMC, Dept Pharm, Hangzhou 310022, Zhejiang, Peoples R China.
   Jiang, Jianwei, Key Lab Tradit Chinese Med Zhejiang Prov, Integrated Tradit Chinese \& Western Med Oncol Lab, Hangzhou 310022, Zhejiang, Peoples R China.},
DOI = {10.20964/2022.11.66},
EarlyAccessDate = {JUN 2023},
Article-Number = {221176},
ISSN = {1452-3981},
Keywords = {Fructus aurantii; Electrochemical fingerprints; Plant identification;
   Pattern recognition; Feature extraction},
Keywords-Plus = {SIMULTANEOUS QUANTIFICATION; EXTRACTION; PLANTS; HPLC},
Research-Areas = {Electrochemistry},
Web-of-Science-Categories  = {Electrochemistry},
Author-Email = {jiangjw@zjcc.org.cn},
Affiliations = {Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Zhejiang Cancer Hospital; Chinese Academy of Sciences},
Funding-Acknowledgement = {Zhejiang Province Medical and Health Technology Program;  {[}2016KYA052]},
Funding-Text = {ACKNOWLEDGEMENT This work was supported by the Zhejiang Province Medical
   and Health Technology Program (2016KYA052) .},
Cited-References = {Ahmad W, 2020, J AOAC INT, V103, P659, DOI 10.5740/jaoacint.19-0286.
   Bai Y, 2018, MOLECULES, V23, DOI 10.3390/molecules23040803.
   Belkheiri A, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11146596.
   Beluomini MA, 2021, FOOD CHEM, V353, DOI 10.1016/j.foodchem.2021.129427.
   Bortnikova S, 2018, APPL GEOCHEM, V93, P145, DOI 10.1016/j.apgeochem.2018.04.009.
   Buitrago D, 2019, ANTIOXIDANTS-BASEL, V8, DOI 10.3390/antiox8080238.
   Deconinck E, 2019, J PHARMACEUT BIOMED, V166, P189, DOI 10.1016/j.jpba.2019.01.015.
   Fang CF, 2022, MOLECULES, V27, DOI 10.3390/molecules27144537.
   Fang W, 2018, CMC-COMPUT MATER CON, V57, P167, DOI 10.32604/cmc.2018.02356.
   Fang Y, 2019, J ELECTROANAL CHEM, V840, P74, DOI 10.1016/j.jelechem.2019.03.052.
   Fu L, 2020, BIOSENS BIOELECTRON, V159, DOI 10.1016/j.bios.2020.112212.
   Fu L, 2019, BIOELECTROCHEMISTRY, V129, P199, DOI 10.1016/j.bioelechem.2019.06.001.
   Ganzenko O, 2021, CHEM ENG J, V419, DOI 10.1016/j.cej.2021.129467.
   Gao TH, 2021, PHYTOCHEM REV, V20, P909, DOI 10.1007/s11101-020-09725-1.
   Gupta AK, 2021, J FOOD ENG, V306, DOI 10.1016/j.jfoodeng.2021.110637.
   He WH, 2019, WATER RES, V155, P372, DOI 10.1016/j.watres.2019.01.062.
   He YJ, 2018, MOLECULES, V23, DOI 10.3390/molecules23092189.
   Jin QL, 2021, EVID-BASED COMPL ALT, V2021, DOI 10.1155/2021/6236135.
   Johnston-Monje D, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11334.
   Kharbach M, 2020, J PHARMACEUT BIOMED, V177, DOI 10.1016/j.jpba.2019.112849.
   Kumar M, 2021, FOOD CHEM, V353, DOI 10.1016/j.foodchem.2021.129431.
   Masek A, 2014, INT J ELECTROCHEM SC, V9, P7875.
   Mohammed K, 2019, BIOL CONTROL, V133, P103, DOI 10.1016/j.biocontrol.2019.03.015.
   Putnik P, 2018, FOODS, V7, DOI 10.3390/foods7070106.
   Rafi M, 2018, IND CROP PROD, V122, P93, DOI 10.1016/j.indcrop.2018.05.062.
   Reddy PNK, 2019, CERAM INT, V45, P16251, DOI 10.1016/j.ceramint.2019.05.147.
   Shadrunova I.V., 2018, INT J APPL ENG RES, V13, P6340.
   Sun D, 2009, MICROCHIM ACTA, V167, P35, DOI 10.1007/s00604-009-0200-0.
   Tamborrino A, 2019, J FOOD ENG, V245, P124, DOI 10.1016/j.jfoodeng.2018.10.019.
   Traore BB, 2018, ECOL INFORM, V48, P257, DOI 10.1016/j.ecoinf.2018.10.002.
   Wang YK, 2021, J SEP SCI, V44, P2189, DOI 10.1002/jssc.202001190.
   Xiong JB, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10010081.
   Yang J, 2021, CURR OPIN COLLOID IN, V56, DOI 10.1016/j.cocis.2021.101499.},
Number-of-Cited-References = {33},
Times-Cited = {1},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Int. J. Electrochem. Sci.},
Doc-Delivery-Number = {A4LA4},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000954846600014},
OA = {hybrid},
DA = {2023-08-12},
}

@article{ WOS:000451780800034,
Author = {Zhu, Heyan and Liu, Qinglin and Qi, Yuankai and Huang, Xinyuan and
   Jiang, Feng and Zhang, Shengping},
Title = {Plant identification based on very deep convolutional neural networks},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2018},
Volume = {77},
Number = {22},
Pages = {29779-29797},
Month = {NOV},
Abstract = {Plant identification is a critical step in protecting plant diversity.
   However, many existing identification systems prohibitively rely on
   hand-crafted features for plant species identification. In this paper, a
   deep learning method is employed to extract discriminative features from
   plant images along with a linear SVM for plant identification. To offer
   a self-learning feature representation for different plant organs, we
   choose a very deep convolutional neural networks (CNNs), which consists
   of sixteen convolutional layers followed by three Fully-Connected (FC)
   layers and a final soft-max layer. Five max-pooling layers are performed
   over a 2x2 pixel window with stride 2. Extensive experiments on several
   plant datasets demonstrate the remarkable performance of the very deep
   neural network compared to the hand-crafted features.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Zhang, SP (Corresponding Author), Harbin Inst Technol, Sch Comp Sci \& Technol, Weihai, Peoples R China.
   Zhu, Heyan, Beijing Forestry Univ, Sch Informat, Beijing, Peoples R China.
   Zhu, Heyan, Yantai Univ, Sch Optoelect Informat, Yantai, Peoples R China.
   Liu, Qinglin; Zhang, Shengping, Harbin Inst Technol, Sch Comp Sci \& Technol, Weihai, Peoples R China.
   Qi, Yuankai; Jiang, Feng, Harbin Inst Technol, Sch Comp Sci \& Technol, Harbin, Heilongjiang, Peoples R China.
   Huang, Xinyuan, Commun Univ China, Inst Animat \& Digital Art, Beijing, Peoples R China.},
DOI = {10.1007/s11042-017-5578-9},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Plant identification; CNN; Linear SVM},
Keywords-Plus = {LEAF; RECOGNITION; FEATURES; SYSTEM},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {s.zhang@hit.edu.cn},
Affiliations = {Beijing Forestry University; Yantai University; Harbin Institute of
   Technology; Harbin Institute of Technology; Communication University of
   China},
ResearcherID-Numbers = {Qi, Yuankai/GYE-2289-2022
   JIANG, Feng/HTP-2862-2023
   Liu, Qinglin/HMD-1377-2023
   Chen, Rainie/ISS-6016-2023},
ORCID-Numbers = {Liu, Qinglin/0000-0002-2408-3344
   },
Funding-Acknowledgement = {key R\&D program of Yantai City {[}2016YT06000609]},
Funding-Text = {This work is supported by the key R\&D program of Yantai City (No.
   2016YT06000609).},
Cited-References = {Ahmed N, 2016, P 5 INT MULT C, P29.
   {[}Anonymous], 2006, IEEE COMPUTER SOC C.
   {[}Anonymous], 2013, INT J ENG SCI TECHNO.
   {[}Anonymous], 2009, P 26 ANN INT C MACH.
   {[}Anonymous], 2014, P INT C ADV COMP COM.
   Aranda M. C., 2010, P ACM INT C IM VID R, P327, DOI {[}10.1145/1816041.1816089, DOI 10.1145/1816041.1816089].
   Arora A, 2012, CLEF.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Ben Mabrouk A, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P201.
   Bo LF, 2013, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2013.91.
   BOUREAU YL, 2010, PROC CVPR IEEE, P2559, DOI DOI 10.1109/CVPR.2010.5539963.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Chopra M., 2015, TREEID IMAGE RECOGNI.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Donahue J, 2014, PR MACH LEARN RES, V32.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Fiel S, 2011, P 16 COMP VIS WINT W.
   Goau H, 2016, P C LABS EV FOR, P428.
   Goau H, 2015, LIFECLEF PLANT IDENT.
   Goeau H., 2012, P 1 ACM INT WORKSH M, P41.
   Guo YC, 2017, IEEE T IMAGE PROCESS, V26, P3277, DOI 10.1109/TIP.2017.2696747.
   Guo YC, 2017, IEEE T IMAGE PROCESS, V26, P1344, DOI 10.1109/TIP.2017.2652730.
   Guru DS., 2010, INT J COMPUTERS APPL, V1, P21.
   Hsiao JK, 2014, 2014 SCIENCE AND INFORMATION CONFERENCE (SAI), P389, DOI 10.1109/SAI.2014.6918216.
   Jiang F, 2015, J MACH LEARN RES, V16, P227.
   Jou-Ken Hsiao, 2014, 2014 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW), P209, DOI 10.1109/ICCE-TW.2014.6904061.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Kulkarni T, 2014, INT J COMPUT SCI BUS, V13, P35.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Kumar T.P., 2017, P INT C COMP VIS IM, P531, DOI {[}10.1007/978-981-10-2107-7\_48., DOI 10.1007/978-981-10-2107-7].
   Kumar TP, 2016, P INT C COMP VIS IM, V460, P531.
   Lee K.-B., 2013, INT J BIOSCI BIOTECH, V5, P57, DOI {[}10.14257/ijbsbt.2013.5.5.06, DOI 10.14257/IJBSBT.2013.5.5.06, 10.3724/sp.j.1087.2009.01707, DOI 10.1007/978-94-007-5857-5\_12].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lin ZJ, 2017, IEEE T CYBERNETICS, V47, P4342, DOI 10.1109/TCYB.2016.2608906.
   Liu Q, 2017, KNOWL-BASED SYST, V134, P189, DOI 10.1016/j.knosys.2017.07.032.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Ma LH, 2013, LECT NOTES COMPUT SC, V7995, P106, DOI 10.1007/978-3-642-39479-9\_13.
   Metre V, 2013, INT J COMPUTER SCI N, V2, P25.
   Mouine S, 2013, P 3 ACM C INT C MULT, P309.
   Mouine S., 2012, P 2 ACM INT C MULT R, P1.
   N Sunderhauf, 2014, CLEF.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Patel H.N., 2011, INT J COMPUT APPL, V13, P1, DOI {[}10.5120/1756-2395, DOI 10.5120/1756-2395].
   Qi Y., IEEE T PATTERN ANAL.
   Nguyen QK, 2013, PROC INT CONF ADV, P404, DOI 10.1109/ATC.2013.6698145.
   Ren XM, 2012, LECT NOTES ARTIF INT, V7390, P237, DOI 10.1007/978-3-642-31576-3\_31.
   Seon-Jong Kim, 2011, SICE 2011 - 50th Annual Conference of the Society of Instrument and Control Engineers of Japan, P1147.
   Sheng Zhen, 2016, 2016 13th International Conference on Service Systems and Service Management (ICSSSM), P1, DOI 10.1109/ICSSSM.2016.7538651.
   Simonyan K, 2014, P COMPUTER VISION PA.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Song Y, 2014, BIOSYST ENG, V118, P203, DOI 10.1016/j.biosystemseng.2013.12.008.
   Sun BY, 2004, LECT NOTES COMPUT SC, V3173, P648.
   Tsolakidis DG, 2014, LECT NOTES ARTIF INT, V8445, P406, DOI 10.1007/978-3-319-07064-3\_33.
   WANG JJ, 2010, PROC CVPR IEEE, P3360, DOI DOI 10.1109/CVPR.2010.5540018.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113.
   Xiao XY, 2010, LECT NOTES ARTIF INT, V6216, P149, DOI 10.1007/978-3-642-14932-0\_19.
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474.
   Zhang BC, 2017, IEEE T IMAGE PROCESS, V26, P4648, DOI 10.1109/TIP.2017.2718189.
   Zhang SP, 2018, IEEE T INTELL TRANSP, V19, P187, DOI 10.1109/TITS.2017.2766093.
   Zhang SP, 2017, IEEE T CIRC SYST VID, V27, P421, DOI 10.1109/TCSVT.2016.2539860.
   Zhang SP, 2015, IEEE T CIRC SYST VID, V25, P1749, DOI 10.1109/TCSVT.2015.2406194.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhao Z, 2015, J COMPUT INF SYST, V11, P857.
   Zhao ZQ, 2015, LECT NOTES COMPUT SC, V9004, P348, DOI 10.1007/978-3-319-16808-1\_24.
   Zhiyong Wang, 2011, Proceedings of the 2011 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2011), P650, DOI 10.1109/DICTA.2011.115.
   Zhu HY, 2017, MULTIMED TOOLS APPL, V76, P4599, DOI 10.1007/s11042-016-3538-4.},
Number-of-Cited-References = {72},
Times-Cited = {22},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {36},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {HC4NQ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000451780800034},
DA = {2023-08-12},
}

@article{ WOS:000250899800017,
Author = {Zou, Jie and Nagy, George},
Title = {Visible models for interactive pattern recognition},
Journal = {PATTERN RECOGNITION LETTERS},
Year = {2007},
Volume = {28},
Number = {16},
Pages = {2335-2342},
Month = {DEC 1},
Abstract = {The exchange of information between human and machine has been a
   bottleneck in interactive visual classification. The visible model of an
   object to be recognized is an abstraction of the object superimposed on
   its picture. It is constructed by the machine but it can be modified by
   the operator. The model guides the extraction of features from the
   picture. The classes are rank ordered according to the similarities (in
   the hidden high-dimensional feature space) between the unknown picture
   and a set of labeled reference pictures. The operator can either accept
   one of the top three candidates by clicking on a displayed reference
   picture, or modify the model. Model adjustment results in the extraction
   of new features, and a new rank ordering. The model and feature
   extraction parameters are re-estimated after each classified object,
   with its model and label, is added to the reference database. Pilot
   experiments show that interactive recognition of flowers and faces is
   more accurate than automated classification, faster than unaided human
   classification, and that both machine and human performance improve with
   use. (c) 2007 Elsevier B.V. All rights reserved.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Nagy, G (Corresponding Author), Rensselaer Polytech Inst, Dept Elect Comp \& Syst Engn, DocLab, JEC 6020,110 8th St, Troy, NY 12180 USA.
   Rensselaer Polytech Inst, Dept Elect Comp \& Syst Engn, DocLab, Troy, NY 12180 USA.
   Natl Lib Med, Bethesda, MD 20894 USA.},
DOI = {10.1016/j.patrec.2007.08.005},
ISSN = {0167-8655},
EISSN = {1872-7344},
Keywords = {pattern recognition; human-computer interaction; visible model; face
   recognition; flower recognition},
Keywords-Plus = {SEGMENTATION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {jzou@mail.nlm.nih.gov
   nagy@ecse.rpi.edu},
Affiliations = {Rensselaer Polytechnic Institute; National Institutes of Health (NIH) -
   USA; NIH National Library of Medicine (NLM)},
ORCID-Numbers = {Nagy, George/0000-0002-0521-1443},
Cited-References = {{[}Anonymous], 1999, VISION SCI PHOTONS P.
   BAIRD HS, 1994, P SOC PHOTO-OPT INS, V2181, P106, DOI 10.1117/12.171098.
   BRADFORD R, 1991, P DOE INF C.
   Carson C, 2002, IEEE T PATTERN ANAL, V24, P1026, DOI 10.1109/TPAMI.2002.1023800.
   Chen YX, 2005, FIFTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P593.
   Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295.
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596.
   Das M, 1999, IEEE INTELL SYST APP, V14, P24, DOI 10.1109/5254.796084.
   DICKEY LA, 1991, P DOE INF C.
   Drewniok C, 1997, INT J COMPUT VISION, V24, P187, DOI 10.1023/A:1007919223573.
   EVANS A, 2005, IEEE INT C EL INF TE, P22.
   GATTANI A, 2004, THESIS RENSSELAER PO.
   Haritaoglu I, 2001, PROC CVPR IEEE, P408.
   Harman D, 1992, INFORMATION RETRIEVA, P241.
   HO TK, 1994, IEEE T PATTERN ANAL, V16, P66, DOI 10.1109/34.273716.
   JAIN R, 1992, US NSF WORKSH VIS IN.
   Kak AC, 2002, INT C PATT RECOG, P839, DOI 10.1109/ICPR.2002.1048433.
   Klein B., 2004, International Journal on Document Analysis and Recognition, V6, P167, DOI 10.1007/s10032-004-0122-7.
   MACK JA, 1992, WOLVES YELLOWSTONE R, V4, P4.
   MERICSKO RJ, 1998, P SPIE, V3584.
   Mortensen EN, 1998, GRAPH MODEL IM PROC, V60, P349, DOI 10.1006/gmip.1998.0480.
   Nagy G, 2004, INT C PATT RECOG, P7, DOI 10.1109/ICPR.2004.1333692.
   Nagy G, 2002, INT C PATT RECOG, P478, DOI 10.1109/ICPR.2002.1048342.
   NAGYG, 1966, IEEE T INFORM THEORY, V12, P215.
   NILSSON K, 2002, P ICPR, V3, P395.
   PENTLAND A, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P84, DOI 10.1109/CVPR.1994.323814.
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790.
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X.
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510.
   Saitoh T, 2004, INT C PATT RECOG, P27, DOI 10.1109/ICPR.2004.1333997.
   SARKAR P, 2002, P SPIE IS T 2002 DOC, P20.
   Veeramachaneni S., 2003, INT J DOC ANAL RECOG, V6, P154.
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235.
   Yang J, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P19, DOI 10.1109/ACV.2002.1182149.
   YUE W, 2005, IEEE INT C IMAGE PRO, V2, P1246.
   Zhang J, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P217, DOI 10.1109/ICMI.2002.1166996.
   ZHAO W, 2004, ACM COMPUT SURVEYS, V35.
   Zou J, 2004, INT C PATT RECOG, P311, DOI 10.1109/ICPR.2004.1334185.
   ZOU J, IEEE T IMAGE PROCESS.
   ZOU J, 2004, THESIS RENSSELAER PO.
   ZOU J, 2005, IS T SPIE 17 S EL IM.
   ZOU J, 2005, IEEE WORKSH APPL COM, V1, P522.
   Zou J, 2006, ADV INFO KNOW PROC, P271.},
Number-of-Cited-References = {43},
Times-Cited = {10},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Pattern Recognit. Lett.},
Doc-Delivery-Number = {230TW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000250899800017},
DA = {2023-08-12},
}

@inproceedings{ WOS:000532493700037,
Author = {Kazerouni, Masoud Fathi and Saeed, Nazeer T. Mohammed and Kuhnert,
   Klaus-Dieter},
Editor = {Nedevschi, S and Potolea, R and Slavescu, RR},
Title = {Exploration of Autonomous Mobile Robots through Challenging Outdoor
   Environments for Natural Plant Recognition Using Deep Neural Network},
Booktitle = {2019 IEEE 15TH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTER
   COMMUNICATION AND PROCESSING (ICCP 2019)},
Series = {IEEE International Conference on Intelligent Computer Communication and
   Processing ICCP},
Year = {2019},
Pages = {279-285},
Note = {IEEE 15th International Conference on Intelligent Computer Communication
   and Processing (ICCP), Cluj Napoca, ROMANIA, SEP 05-07, 2019},
Organization = {IEEE; IEEE Romanian Comp Soc Chapter; Tech Univ Cluj Napoca, Comp Sci
   Dept; Tech Univ Cluj Napoca; Acad Tech Sci; IEEE Romania Sect; Robert
   Bosch SRL; Porsche Engn Romania SRL},
Abstract = {Modernization of living environments and human activities have severe
   effects on many parameters and factors such as climate change and global
   warming, an increase of incidence and the severity of wildfires, land
   surfaces and ice sheets, ecological imbalance, change of fertility of
   the soil, flow of energy, food security, etc. In addition, human
   modernization has had a negative impact on biodiversity and the natural
   environment. An integral component of modernization is agriculture that
   associates with the outdoor environment and relevant issues. Automation
   of agricultural activities contributes to reducing the dependency on
   human labor and the harmful effects on the natural environment. The
   correct identification of plants in outdoor environments has been
   neglected and many critical environmental and non-environmental limits
   and factors, such as weather conditions, time, viewpoint, lighting
   conditions (illuminations and light intensity), distance, etc., have not
   been considered in existing plant recognition systems and technologies.
   Hence, there is a demand to develop mobile and real-time systems for
   plant recognition in natural environments. This paper addresses these
   challenges and introduces the application of autonomous mobile robot and
   semi-robots for recognition of natural plant species in outdoor
   environments. Furthermore, the proposed system presents the use of
   employing low-cost cameras, such as iPhone 6s, Canon EOS 6001) and
   Samsung Galaxy Note 4, for plant recognition system in real-time. The
   performance of the system has been tested with a number of experiments
   in different years, 2017 and 2018, and at different times of day,
   morning and evening. The proposed system is a combination of new
   technologies involving deep learning concepts and an autonomous field
   robot to carry out precise plant recognition in challenging
   environments. The final accuracy of the mobile real-time system is
   84.1666\%.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kazerouni, MF (Corresponding Author), Univ Siegen, Inst Real Time Learning Syst, Siegen, Germany.
   Kazerouni, Masoud Fathi; Saeed, Nazeer T. Mohammed; Kuhnert, Klaus-Dieter, Univ Siegen, Inst Real Time Learning Syst, Siegen, Germany.},
ISSN = {2065-9946},
ISBN = {978-1-7281-4914-1},
Keywords = {Deep Learning; Natural Plant Recognition; Autonomous Mobile Robot;
   Intelligent Agriculture Robotics; Agricultural Robot},
Keywords-Plus = {CROP},
Research-Areas = {Computer Science; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Hardware \&
   Architecture; Telecommunications},
Author-Email = {masoud.fathi@uni-siegen.de
   nazeersaeed@eti.uni-siegen.de
   kuhnert@fb12.uni-siegen.de},
Affiliations = {Universitat Siegen},
Cited-References = {Amer G, 2015, 2015 INTERNATIONAL CONFERENCE ON INDUSTRIAL INSTRUMENTATION AND CONTROL (ICIC), P207, DOI 10.1109/IIC.2015.7150739.
   {[}Anonymous], 2015, MOD NAT PLANTS DAT I.
   Arakeri MP, 2017, 2017 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P1201, DOI 10.1109/ICACCI.2017.8126005.
   Arun C. H., 2013, INT J COMPUTER APPL, V62, P1, DOI 10.5120/10129-4920.
   Astrand B, 2002, AUTON ROBOT, V13, P21, DOI 10.1023/A:1015674004201.
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
   da Costa FGI, 2015, INT SYM COMP ARCHIT, P122, DOI 10.1109/SBAC-PAD.2015.30.
   Fathi Kazerouni M, 2015, ADV IMAGE VIDEO PROC, V3, P10.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   HE DC, 1990, IEEE T GEOSCI REMOTE, V28, P509.
   Husin Z, 2012, COMPUT ELECTRON AGR, V89, P18, DOI 10.1016/j.compag.2012.07.009.
   Jeon HY, 2011, SENSORS-BASEL, V11, P6270, DOI 10.3390/s110606270.
   Jorgensen R. RN, 2007, HORTIBOT SYSTEM DESI.
   Kargar Amir H. B., 2013, 2013 First RSI/ISM International Conference on Robotics and Mechatronics (ICRoM 2013). Proceedings, P468, DOI 10.1109/ICRoM.2013.6510152.
   Kazerouni MF, 2019, SN APPL SCI, V1, DOI 10.1007/s42452-019-0785-9.
   Kazerouni MF., 2017, INT J COMPUT ELECT A, V10, P1497.
   Kazerouni MF, 2015, SIGNAL IMAGE PROCESS, V6, P1.
   Kazerouni MF, 2017, AUTOMATIC PLANT RECO.
   Kumar VS, 2016, PROCEDIA COMPUT SCI, V93, P975, DOI 10.1016/j.procs.2016.07.289.
   Kunze J., 2016, P 14 FIELD ROB EV 20, P154.
   Kurfess T. R., 2004, ROBOTICS AUTOMATION.
   Li J., 2014, 3D MACHINE VISION SY.
   Li J, 2018, J FIELD ROBOT, V35, P596, DOI 10.1002/rob.21763.
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410.
   Potena C, 2017, ADV INTELL SYST COMP, V531, P105, DOI 10.1007/978-3-319-48036-7\_9.
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
   Sukkarieh S., 2014, PMA FRESH CONNECTION.
   Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881.
   WANG L, 1990, PATTERN RECOGN, V23, P905, DOI 10.1016/0031-3203(90)90135-8.
   Yanikoglu B., 2014, MACHINE VISION APPL, V25.
   Yanikoglu B., 2011, SABANCI OKAN SYSTEM, V1177.
   Yosinski J, 2015, UNDERSTANDING NEURAL.
   Zhang NQ, 2002, COMPUT ELECTRON AGR, V36, P113, DOI 10.1016/S0168-1699(02)00096-0.},
Number-of-Cited-References = {33},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BO9UT},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000532493700037},
DA = {2023-08-12},
}

@inproceedings{ WOS:000662554703059,
Author = {Wan, Weilin and Tang, Bingyu and Sun, Ziheng and Ye, Haochen},
Editor = {Wu, XT and Jermaine, C and Xiong, L and Hu, XH and Kotevska, O and Lu, SY and Xu, WJ and Aluru, S and Zhai, CX and Al-Masri, E and Chen, ZY and Saltz, J},
Title = {Design of Fine-grained Plant Dataset and A Plant Image Acquisition Tool},
Booktitle = {2020 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)},
Series = {IEEE International Conference on Big Data},
Year = {2020},
Pages = {3341-3346},
Note = {8th IEEE International Conference on Big Data (Big Data), ELECTR
   NETWORK, DEC 10-13, 2020},
Organization = {IEEE; IEEE Comp Soc; IBM; Ankura},
Abstract = {Fine-grained plant classification has attracted great interest for its
   wide application. However, there is currently no suitable fine-grained
   plant dataset for deep learning. In this paper, we propose a method of
   constructing fine-grained plant dataset with adequate annotations. the
   dataset can help the methods to achieve better plant classification
   performance. Moreover, we develop an acquisition tool for collecting
   fine-grained plant dataset. The tool can mitigate the problem of
   collecting plant images in the fields.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Wan, WL (Corresponding Author), Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China.
   Wan, Weilin; Tang, Bingyu; Sun, Ziheng; Ye, Haochen, Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China.},
DOI = {10.1109/BigData50022.2020.9377857},
ISSN = {2639-1589},
ISBN = {978-1-7281-6251-5},
Keywords = {fine-grained classification; plants; fine-grained dataset; acquisition
   tool},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory \& Methods},
Author-Email = {17307130106@fudan.edu.cn
   17307130357@fudan.edu.cn
   20210240133@fudan.edu.cn
   20210240104@fudan.edu.cn},
Affiliations = {Fudan University},
Funding-Acknowledgement = {Shanghai Municipal RD Foundation {[}19511104401]},
Funding-Text = {This paper is supported by Shanghai Municipal R\&D Foundation (No.
   19511104401).},
Cited-References = {{[}Anonymous], 2013, TECH REP.
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47.
   Flora of China, 2018, FLORA CHINA.
   Gao Libin, 2015, J PUER COLL.
   Khosla A, 2011, P 1 WROKSH FIN GRAIN.
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77.
   Maji S., 2013, TECH REP.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Peng Yuxin, 2017, ARXIV170401740V2CSCV.
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136.
   Wah C., 2011, CALTECH UCSD BIRDS 2.
   Wah C., 2011, CNSTR2011001 CALTECH.
   {[}王晓峰 Wang Xiaofeng], 2006, {[}计算机工程与应用, Computer Engineering and Application], V42, P190.
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1\_54.
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128.
   Zhou F, 2016, PROC CVPR IEEE, P1124, DOI 10.1109/CVPR.2016.127.},
Number-of-Cited-References = {16},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {9},
Doc-Delivery-Number = {BR6NZ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000662554703059},
DA = {2023-08-12},
}

@inproceedings{ WOS:000803995400070,
Author = {Sani, Radhwan and Cheaitou, Ali and Rabie, Tamer},
Book-Group-Author = {IEEE},
Title = {Autonomous localization of the best depth blob using TOPSIS: application
   on forage plants for the Sharjah pastures project},
Booktitle = {2021 14TH INTERNATIONAL CONFERENCE ON DEVELOPMENTS IN ESYSTEMS
   ENGINEERING (DESE)},
Series = {International Conference on Developments in eSystems Engineering},
Year = {2021},
Pages = {394-400},
Note = {14th International Conference on Developments in eSystems Engineering
   (DeSE), Univ Sharjah, Sharjah, U ARAB EMIRATES, DEC 07-10, 2021},
Abstract = {This research constitutes a building block in a larger decision support
   system, which combines computer vision and multi-criteria decision
   making, to identify indigenous forage plants in close-range aerial
   images. Such a system enables managing the open pastures project of the
   Emirate of Sharjah, United Arab Emirates (UAE). The system aims at the
   identification of forage plants through the detection of plant
   inflorescences in depth images, and applying TOPSIS, to select and
   localize the best plant inflorescence.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Sani, R (Corresponding Author), Univ Sharjah, Coll Engn, Dept Ind Engn \& Engn Management, Sharjah, U Arab Emirates.
   Sani, Radhwan; Cheaitou, Ali, Univ Sharjah, Coll Engn, Dept Ind Engn \& Engn Management, Sharjah, U Arab Emirates.
   Rabie, Tamer, Univ Sharjah, Coll Comp \& Informat, Dept Comp Engn, Sharjah, U Arab Emirates.},
DOI = {10.1109/DESE54285.2021.9719515},
ISSN = {2161-1343},
ISBN = {978-1-6654-0888-2},
Keywords = {Decision Support System; DSS; TOPSIS; Plant Identification; Decision
   Making Process; Computer Vision; Depth; Blobs; Cenchrus ciliaris;
   Pennisetum divisum; forage},
Keywords-Plus = {PATTERN},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Computer Science, Interdisciplinary Applications;
   Engineering, Multidisciplinary},
Author-Email = {u15100761@sharjah.ac.ae
   acheaitou@sharjah.ac.ae
   trabie@sharjah.ac.ae},
Affiliations = {University of Sharjah; University of Sharjah},
ORCID-Numbers = {Sani, Radhwan/0000-0002-1111-8763},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Alhadrami G, 2000, HERBACEOUS PLANTS UN.
   Assaeed AM, 1997, J ARID ENVIRON, V36, P103, DOI 10.1006/jare.1996.0200.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Fu LS, 2020, COMPUT ELECTRON AGR, V177, DOI 10.1016/j.compag.2020.105687.
   Gao GD, 2020, COMPUT ELECTRON AGR, V173, DOI 10.1016/j.compag.2020.105412.
   GUO ZC, 1989, COMMUN ACM, V32, P359, DOI 10.1145/62065.62074.
   Jia N, 2021, COMPUT ELECTRON AGR, V180, DOI 10.1016/j.compag.2020.105897.
   Jongbloed M., 2003, COMPREHENSIVE GUIDE.
   Karim F. M., 2007, FLORA UNITED ARAB EM.
   Karim FM, 2013, SALT TOLERANT PLANTS.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lin GC, 2019, BIOSYST ENG, V186, P34, DOI 10.1016/j.biosystemseng.2019.06.019.
   Marshall VM, 2012, J ARID ENVIRON, V78, P1, DOI 10.1016/j.jaridenv.2011.11.005.
   Noland RL, 2018, FIELD CROP RES, V222, P189, DOI 10.1016/j.fcr.2018.01.017.
   Qureshi MRN, 2018, ENVIRON DEV SUSTAIN, V20, P641, DOI 10.1007/s10668-016-9903-7.
   Salve P, 2022, J KING SAUD UNIV-COM, V34, P1361, DOI 10.1016/j.jksuci.2018.09.018.
   Shahin S., 2018, GRASSES FOOD FEED, V45, P467.
   Vercellis C., 2009, BUSINESS INTELLIGENC.
   Wang ZY, 2021, WATER-SUI, V13, DOI 10.3390/w13111532.
   Wang ZB, 2016, NEURAL COMPUT APPL, V27, P899, DOI 10.1007/s00521-015-1904-1.
   Yu DJ, 2021, EXPERT SYST APPL, V168, DOI 10.1016/j.eswa.2020.114238.
   Zhang DY, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0187470.
   Zhang HF, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10060851.
   Zhang J, 2018, COMPUT ELECTRON AGR, V155, P386, DOI 10.1016/j.compag.2018.10.029.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {26},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BT1XQ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000803995400070},
DA = {2023-08-12},
}

@inproceedings{ WOS:000459239300022,
Author = {Arrasco, Carlos and Khlebnikov, Sofia and Oncevay, Arturo and Castanon,
   Cesar Beltran},
Book-Group-Author = {IEEE},
Title = {Leaf Venation Enhancing for Texture Feature Extraction in a Plant
   Classification Task},
Booktitle = {2018 IEEE LATIN AMERICAN CONFERENCE ON COMPUTATIONAL INTELLIGENCE
   (LA-CCI)},
Year = {2018},
Note = {5th IEEE Latin American Conference on Computational Intelligence
   (LA-CCI), Guadalajara, MEXICO, NOV 06-09, 2018},
Organization = {IEEE Computat Intelligence Soc; IEEE; Univ Guadalajara; Centro Univ
   Ciencias Exactas Ingn},
Abstract = {In a computer science approach, the plant classification task focuses on
   the extraction of many leaf attributes, such as texture or veins, which
   are closely related and commonly analyzed together. Thereby, this study
   proposes a method to enhance the venation patterns over the leaf area,
   in order to improve the texture feature extraction in windows areas in
   the plant species identification. Regarding the experimentation, two
   types of texture features are contrasted, and it is performed over an
   own dataset with high-resolution image of 10 plant species. The obtained
   results demonstrate that the veins enhancing process improve the species
   classification task significantly for a texture descriptor based on the
   analysis of relation between neighboring pixels.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Arrasco, C (Corresponding Author), Pontificia Univ Catolica Peru, Dept Engn, Artificial Intelligence Res Grp IA PUCP, Lima, Peru.
   Arrasco, Carlos; Khlebnikov, Sofia; Oncevay, Arturo; Castanon, Cesar Beltran, Pontificia Univ Catolica Peru, Dept Engn, Artificial Intelligence Res Grp IA PUCP, Lima, Peru.},
ISBN = {978-1-5386-4626-7},
Keywords = {leaf venation enhancing; texture feature extraction; leaf veins
   extraction; leaf-based plant classification; Haralick's descriptors;
   multi-scale fractal dimension},
Keywords-Plus = {FRACTAL DIMENSION; IDENTIFICATION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {arrasco.c@pucp.edu.pe
   sjlebn@pucp.edu.pe
   arturo.oncevay@pucp.edu.pe
   cbeltran@pucp.pe},
Affiliations = {Pontificia Universidad Catolica del Peru},
Funding-Acknowledgement = {``Programa Nacional de Innovacion para la Competitividad y
   Productividad{''}, INNOVATE Peru {[}183-FINCyT-IA-2013]},
Funding-Text = {For this study, the authors acknowledge the support of the ``Programa
   Nacional de Innovacion para la Competitividad y Productividad{''},
   INNOVATE Peru, under the contract 183-FINCyT-IA-2013.},
Cited-References = {{[}Anonymous], 2013, ARXIV14014447.
   {[}Anonymous], 1979, IEEE T SYST MAN CYBE, DOI DOI 10.1109/TSMC.1979.4310076.
   Backes AR, 2010, LECT NOTES COMPUT SC, V6134, P463, DOI 10.1007/978-3-642-13681-8\_54.
   Backes AR, 2009, LECT NOTES COMPUT SC, V5716, P143, DOI 10.1007/978-3-642-04146-4\_17.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Du JX, 2011, LECT NOTES COMPUT SC, V6838, P364, DOI 10.1007/978-3-642-24728-6\_49.
   EHSANIRAD A., 2010, INT J COMPUTER SCI I, V8, P78.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Kadir A., 2013, ARXIV13115829.
   Kebapci H., 2010, COMPUT J.
   Lee K.-B., 2013, INT J BIOSCI BIOTECH, V5, P57, DOI {[}10.14257/ijbsbt.2013.5.5.06, DOI 10.14257/IJBSBT.2013.5.5.06, 10.3724/sp.j.1087.2009.01707, DOI 10.1007/978-94-007-5857-5\_12].
   Lin FY, 2008, COMM COM INF SC, V15, P432.
   Oncevay-Marcos A, 2015, LECT NOTES COMPUT SC, V9257, P326, DOI 10.1007/978-3-319-23117-4\_28.
   Plotze RD, 2005, CAN J BOT, V83, P287, DOI {[}10.1139/b05-002, 10.1139/B05-002].
   Salima A, 2015, INT C ADV COMP SCI I, P275, DOI 10.1109/ICACSIS.2015.7415152.
   Yin WS, 2015, 2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P952, DOI 10.1109/IAEAC.2015.7428697.},
Number-of-Cited-References = {16},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BM0WG},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000459239300022},
DA = {2023-08-12},
}

@inproceedings{ WOS:000903679300009,
Author = {Bermejo, Danitza and Sotomayor Alzamora, Guina},
Book-Group-Author = {IEEE},
Title = {A comparative study on plant classification using convolutional neural
   networks architectures},
Booktitle = {2022 XVLIII LATIN AMERICAN COMPUTER CONFERENCE (CLEI 2022)},
Year = {2022},
Note = {48th Latin American Computer Conference (CLEI), Quindio Univ, Armenia,
   COLOMBIA, OCT 17-21, 2022},
Organization = {IEEE Reg 9; Univ Quindio, Acreditada Calidad, Fac Ingn; Ambassade France
   Colombie; Colifri; Perficient; Sonicwall; Grupo Investigac Sistemas
   Informac \& Control Ind; Laboratorio Usabilidad; ICETEX; Espa Todas;
   Camara Comercio Armenia Quindio; Quindio Convent Bur; UINDIO Coraz
   Colombia; TUYYO; Expertos Internacionales; Rama IEEE Uniquindio},
Abstract = {Determining the species of a plant is important to know its ecological
   and economic importance. Recently, deep learning (DL) models,
   specifically convolutional neural networks (CNN), have achieved
   outstanding results in several applications, including the
   classification of plants. This work focused on the evaluation and
   compassion of transfer learning models: Alexnet, VGG-16, ResNet-18,
   ResNet-50, DenseNet, and Inception V3. The datasets used were the
   Peruvian Forestry Amazon dataset and PlantVillage. For the training,
   therefore, we used two instances. We evaluated the models by different
   multiclass metrics: accuracy, sensitivity, precision, F-score. The
   results present significant values obtained by the VGG-16 model, with
   97,79\% accuracy, 98,00\% sensitivity, 98,00\% precision, and 98,00\%
   F-score to the Peruvian Forestry Amazon dataset. It is possible to
   conclude that the VGG-16 model got an acceptable level of accuracy,
   which makes it a useful tool to help classify plant species from the
   Amazon.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Bermejo, D (Corresponding Author), Univ Nacl Altiplano, Puno, Peru.
   Bermejo, Danitza; Sotomayor Alzamora, Guina, Univ Nacl Altiplano, Puno, Peru.},
DOI = {10.1109/CLEI56649.2022.9959905},
ISBN = {978-1-6654-7671-3},
Keywords = {Deep Learning; Leaf Image; Plant Classification; Transfer Learning},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Author-Email = {danitza.bermejo@gmail.com
   gsotomayor@unap.edu.pe},
Affiliations = {Universidad Nacional del Altiplano},
Cited-References = {Apolinario M, 2019, IEEE LAT AM T, V17, P2005, DOI 10.1109/TLA.2019.9011545.
   Ashqar B., 2019, INT J ACAD INF SYST, V3, P7.
   Azlah MAF, 2019, COMPUTERS, V8, DOI 10.3390/computers8040077.
   Deng L., 2018, DEEP LEARNING NATURA.
   Elnemr HA, 2019, INT J ADV COMPUT SC, V10, P319.
   Goeau H., 2019, CLEF 2019 C LABS EVA.
   Hassan SM, 2022, IEEE ACCESS, V10, P5390, DOI 10.1109/ACCESS.2022.3141371.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.
   Ioffe S., 2015, P INT C MACH LEARN.
   Kaur P, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22020575.
   Kingma D.P., 2014, INT C LEARN REPR ICL, V3, DOI DOI 10.1063/1.4902458.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Le Quoc V, 2011, ICML.
   Maeda-Gutierrez V, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10041245.
   Montenegro R., 2018, CLASIFICACION ESPECI.
   Otter DW, 2021, IEEE T NEUR NET LEAR, V32, P604, DOI 10.1109/TNNLS.2020.2979670.
   Hughes DP, 2016, Arxiv, DOI arXiv:1511.08060.
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X.
   Qian WQ, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105519.
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI {[}10.1162/NECO\_a\_00990, 10.1162/neco\_a\_00990].
   Reategui A., 2018, IQUITOS.
   Rizk S, 2019, PLANT LEAF CLASSIFIC.
   Saini G, 2020, ADV INTELL SYST COMP, V1045, P551, DOI 10.1007/978-981-15-0029-9\_44.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Syarief M., 2020, TELKOMNIKA TELECOMMU, V18, P1376, DOI {[}10.12928/telkomnika.v18i3.14840, DOI 10.12928/TELKOMNIKA.V18I3.14840].
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Too EC, 2019, COMPUT ELECTRON AGR, V161, P272, DOI 10.1016/j.compag.2018.03.032.
   Vizcarra G, 2021, ECOL INFORM, V62, DOI 10.1016/j.ecoinf.2021.101268.
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349.
   Zarski M, 2022, COMPUT-AIDED CIV INF, V37, P500, DOI 10.1111/mice.12755.
   Zhang HX, 2021, J ELECTRON IMAGING, V30, DOI 10.1117/1.JEI.30.2.023019.},
Number-of-Cited-References = {33},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BU4MN},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000903679300009},
DA = {2023-08-12},
}

@inproceedings{ WOS:000560681600102,
Author = {Pechebovicz, Denise and Premebida, Sthefanie and Soares, Vinicios and
   Camargo, Thiago and Bittencourt, Jakson L. and Baroncini, Virginia and
   Martins, Marcella},
Book-Group-Author = {IEEE},
Title = {Plants recognition using embedded Convolutional Neural Networks on
   Mobile devices},
Booktitle = {2020 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT)},
Series = {IEEE International Conference on Industrial Technology},
Year = {2020},
Pages = {674-679},
Note = {IEEE International Conference on Industrial Technology (ICIT), Buenos
   Aires, ARGENTINA, FEB 26-28, 2020},
Organization = {IEEE},
Abstract = {In this work we propose a mobile application capable of recognizing
   Brazilian medicinal plants to be used by universities, students that
   have not previous contact with the species and professionals working on
   health centers. We describe the database generation based on the
   Brazilian Ministry of Health list of medicinal and common toxic plants.
   We also implement artificial intelligence techniques to perform the
   recognition task using a class of convolutional neural networks (CNN)
   focused on lowering the computation resource necessary to run deep
   learning tasks and also optimizing the execution of the architectures on
   embedded and mobile devices.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Pechebovicz, D (Corresponding Author), Fed Univ Technol Parana Ponta Grossa UTFPR PG, Ponta Grossa, Parana, Brazil.
   Pechebovicz, Denise; Premebida, Sthefanie; Soares, Vinicios; Camargo, Thiago; Bittencourt, Jakson L.; Baroncini, Virginia; Martins, Marcella, Fed Univ Technol Parana Ponta Grossa UTFPR PG, Ponta Grossa, Parana, Brazil.},
DOI = {10.1109/ICIT45562.2020.9067289},
ISSN = {2643-2978},
ISBN = {978-1-7281-5754-2},
Keywords = {Image Classification; Convolutional Neural Networks; Embedded
   Applications; Plant Recognition},
Keywords-Plus = {MEDICINAL-PLANTS},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {pechebovicz@alunos.utfpr.edu.br
   sthefanie@alunos.utfpr.edu.br
   viniciossoares@alunos.utfpr.edu.br
   tcamargo@alunos.utfpr.edu.br
   jakson@alunos.utfpr.edu.br
   virginia@utfpr.edu.br
   marcella@utfpr.edu.br},
ResearcherID-Numbers = {Martins, Marcella Scoczynski Ribeiro/C-7029-2019},
ORCID-Numbers = {Martins, Marcella Scoczynski Ribeiro/0000-0002-5716-4968},
Funding-Acknowledgement = {Fundacao Araucaria},
Funding-Text = {We gratefully acknowledge the support of UTFPR/Scientific Research
   program. T. Camargo acknowledges Fundacao Araucaria.},
Cited-References = {Adhikari SP, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.01404.
   {[}Anonymous], 2013, IMAGE CONVOLUTION.
   Antonio GD, 2013, INTERFACE-BOTUCATU, V17, P615, DOI 10.1590/S1414-32832013005000014.
   Bloice M. D., 2017, J OPEN SOURCE SOFTW, DOI {[}DOI 10.21105/JOSS.00432, 10.21105/joss.00432].
   CAMPOS S.C., 2016, Rev. bras. plantas med., V18, P373, DOI 10.1590/1983-084X/15\_057.
   Howard A. G., 2017, EFFICIENT CONVOLUTIO.
   Krizhevsky A., 2010, CONVOLUTIONAL DEEP B.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Lee S. H., DEEP PLANT PLANT IDE.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   PEDRO F.G.G., 2016, Rev. bras. plantas med., V18, P297, DOI 10.1590/1983-084X/15\_144.
   Sandler M., 2018, CORR.
   Schulz V., 2001, RATIONAL PHYTOTHERAP, V4th.
   Vasa H., GOOGLE IMAGES DOWNLO.
   Visen NS, 2002, BIOSYST ENG, V82, P151, DOI 10.1006/bioe.2002.0064.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {16},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BP6TN},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000560681600102},
DA = {2023-08-12},
}

@inproceedings{ WOS:000426116300051,
Author = {de Luna, Robert G. and Baldovino, Renann G. and Cotoco, Ezekiel A. and
   de Ocampo, Anton Louise P. and Valenzuela, Ira C. and Culaba, Alvin B.
   and Dadios, Elmer P.},
Book-Group-Author = {IEEE},
Title = {Identification of Philippine Herbal Medicine Plant Leaf Using Artificial
   Neural Network},
Booktitle = {2017 IEEE 9TH INTERNATIONAL CONFERENCE ON HUMANOID, NANOTECHNOLOGY,
   INFORMATION TECHNOLOGY, COMMUNICATION AND CONTROL, ENVIRONMENT AND
   MANAGEMENT (IEEE HNICEM)},
Series = {IEEE International Conference on Humanoid Nanotechnology Information
   Technology Communication and Control Environment and Management},
Year = {2017},
Note = {9th IEEE International Conference on Humanoid, Nanotechnology,
   Information Technology, Communication and Control, Environment and
   Management (IEEE HNICEM), Pasay, PHILIPPINES, NOV 29-DEC 03, 2017},
Organization = {IEEE; IEEE Philippines},
Abstract = {The study described in this paper consists of a system that involves
   image processing techniques to extract relevant features related to leaf
   in conjunction with using artificial neural network in order to detect
   and identify some Philippine herbal plants. Real samples of twelve
   different herbal medicine plant leaves are collected where each leaf are
   isolated in single image. Several features are extracted using
   techniques in image processing. With the artificial neural network
   acting as autonomous brain network, the system can identify the species
   of the herbal medicine plant leaves being tested. The system can also
   provide information about the diseases the herbal plant can cure.
   For the training, a features dataset of 600 images coming from 50 images
   per herbal plant are used. With the aid of Python, a neural network
   model with optimized parameters are established producing 98.16 \%
   identification for the whole dataset. To evaluate the actual performance
   of the system, a separate 72 sample images of herbal plants are tested
   with the neural network model implemented in MATLAB. Experimental
   results demonstrate a 98.61 \% accuracy of herbal plant identification.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {de Luna, RG (Corresponding Author), De La Salle Univ, Gokongwei Coll Engn, Manila, Philippines.
   de Luna, Robert G.; Baldovino, Renann G.; Cotoco, Ezekiel A.; de Ocampo, Anton Louise P.; Valenzuela, Ira C.; Culaba, Alvin B.; Dadios, Elmer P., De La Salle Univ, Gokongwei Coll Engn, Manila, Philippines.},
ISSN = {2475-7152},
ISBN = {978-1-5386-0912-5},
Keywords = {feature extraction; image processing; artificial neural network; leaf
   identification},
Research-Areas = {Engineering; Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Engineering, Multidisciplinary; Engineering, Environmental; Nanoscience
   \& Nanotechnology},
Author-Email = {robert\_g\_deluna@dlsu.edu.ph},
Affiliations = {De La Salle University},
ResearcherID-Numbers = {Valenzuela, Ira/GYJ-6853-2022
   Valenzuela, Ira C./S-4768-2017
   },
ORCID-Numbers = {Valenzuela, Ira C./0000-0002-7413-4808
   Baldovino, Renann/0000-0002-8709-5692},
Funding-Acknowledgement = {Engineering Research and Development for Technology (ERDT) of the
   Department of Science and Technology (DOST)},
Funding-Text = {The authors would like to acknowledge the Engineering Research and
   Development for Technology (ERDT) of the Department of Science and
   Technology (DOST) for funding this research. The authors also recognize
   the assistance and technical expertise of members of the Intelligent
   Systems Learning (ISL) Laboratory of De La Salle University (DLSU).},
Cited-References = {{[}Anonymous], 2000, WORLD PAT INF, DOI DOI 10.1016/S0172-2190(00)00083-1.
   {[}Anonymous], 2013, ARXIV14014447.
   {[}Anonymous], 2013, INT J ENG SCI TECHNO.
   {[}Anonymous], 1992, NEURAL NETWORKS ALGO.
   Arun C. H, 2013, INT J COMPUTER APPL, V62.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Charters J., 2014, 2014 IEEE INT C MULT, P1, DOI {[}10.1109/ICMEW.2014.6890557, DOI 10.1109/ICMEW.2014.6890557].
   Cope JS, 2010, LECT NOTES COMPUT SC, V6474, P135, DOI 10.1007/978-3-642-17688-3\_14.
   Hati S, 2013, INT J COMPUTER APPL, V62, P15, DOI {[}10.5120/10172-4897, DOI 10.5120/10172-4897].
   Haykin S., 1994, NEURAL NETWORKS COMP.
   Janani R., 2013, 2013 International Conference on Advanced Electronic Systems (ICAES), P238, DOI 10.1109/ICAES.2013.6659400.
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Kaushal Manisha, 2010, INT J ENG SCI TECHNO, V2, P2077.
   Kim JH, 2009, 2009 THIRD INTERNATIONAL SYMPOSIUM ON INTELLIGENT INFORMATION TECHNOLOGY APPLICATION, VOL 3, PROCEEDINGS, P580, DOI 10.1109/IITA.2009.407.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee K. B., 2012, 2012 INT C INF SCI T, V3, P133.
   Lin T.-T., 2000, 2000 ASAE ANN INT M.
   Liu Xun, 2009, INDEX RECOGNITION SH, P436.
   Mani Raman, INT J IMAGE PROCESSI, V3, P1.
   Rocha Anderson, 2009, COMPUT ELECTRON AGR, DOI {[}10.1016/j.compag.2009, DOI 10.1016/J.COMPAG.2009].
   Saitosh Takeshi, ICPR0410514651 04IEE.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wang Xiao-Feng, RECOGNITION LEAF IMA.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Wu S. G., SIGN PROC INF TECHN, P11.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {27},
Times-Cited = {5},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BJ5OG},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000426116300051},
DA = {2023-08-12},
}

@article{ WOS:000915774400001,
Author = {Mostafa, Sakib and Mondal, Debajyoti and Beck, Michael A. and Bidinosti,
   Christopher P. and Henry, Christopher J. and Stavness, Ian},
Title = {Leveraging Guided Backpropagation to Select Convolutional Neural
   Networks for Plant Classification},
Journal = {FRONTIERS IN ARTIFICIAL INTELLIGENCE},
Year = {2022},
Volume = {5},
Month = {MAY 11},
Abstract = {The development of state-of-the-art convolutional neural networks (CNN)
   has allowed researchers to perform plant classification tasks previously
   thought impossible and rely on human judgment. Researchers often develop
   complex CNN models to achieve better performances, introducing
   over-parameterization and forcing the model to overfit on a training
   dataset. The most popular process for evaluating overfitting in a deep
   learning model is using accuracy and loss curves. Train and loss curves
   may help understand the performance of a model but do not provide
   guidance on how the model could be modified to attain better
   performance. In this article, we analyzed the relation between the
   features learned by a model and its capacity and showed that a model
   with higher representational capacity might learn many subtle features
   that may negatively affect its performance. Next, we showed that the
   shallow layers of a deep learning model learn more diverse features than
   the ones learned by the deeper layers. Finally, we propose SSIM cut
   curve, a new way to select the depth of a CNN model by using the
   pairwise similarity matrix between the visualization of the features
   learned at different depths by using Guided Backpropagation. We showed
   that our proposed method could potentially pave a new way to select a
   better CNN model.},
Publisher = {FRONTIERS MEDIA SA},
Address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Mostafa, S (Corresponding Author), Univ Saskatchewan, Dept Comp Sci, Saskatoon, SK, Canada.
   Mostafa, Sakib; Mondal, Debajyoti; Stavness, Ian, Univ Saskatchewan, Dept Comp Sci, Saskatoon, SK, Canada.
   Beck, Michael A.; Bidinosti, Christopher P., Univ Winnipeg, Dept Phys, Winnipeg, MB, Canada.
   Beck, Michael A.; Henry, Christopher J., Univ Winnipeg, Dept Appl Sci, Winnipeg, MB, Canada.},
DOI = {10.3389/frai.2022.871162},
Article-Number = {871162},
EISSN = {2624-8212},
Keywords = {explainable AI; deep learning-artificial neural network; Guided
   Backpropagation; neural network visualization; convolutional neural
   network},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems},
Author-Email = {sakib.mostafa@usask.ca},
Affiliations = {University of Saskatchewan; University of Winnipeg; University of
   Winnipeg},
ResearcherID-Numbers = {Beck, Michael/HTM-1856-2023},
Funding-Acknowledgement = {Canada First Research Excellence Fund},
Funding-Text = {This research was undertaken thank in part to funding from the Canada
   First Research Excellence Fund.},
Cited-References = {Aich S, 2018, IEEE WINT CONF APPL, P323, DOI 10.1109/WACV.2018.00042.
   Aich S, 2017, IEEE INT CONF COMP V, P2080, DOI 10.1109/ICCVW.2017.244.
   Azlah MAF, 2019, COMPUTERS, V8, DOI 10.3390/computers8040077.
   Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140.
   Beck Michael, 2020, Dryad, DOI 10.5061/DRYAD.GTHT76HHZ.
   Beck MA, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0243923.
   Caruana R, 2001, ADV NEUR IN, V13, P402.
   Chandra A.L., 2020, ARXIV.
   Dobrescu A, 2019, IEEE COMPUT SOC CONF, P2600, DOI 10.1109/CVPRW.2019.00316.
   Dobrescu A, 2017, IEEE INT CONF COMP V, P2072, DOI 10.1109/ICCVW.2017.243.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Escorcia V, 2015, PROC CVPR IEEE, P1256, DOI 10.1109/CVPR.2015.7298730.
   Feldman V, 2020, Arxiv, DOI arXiv:2008.03703.
   Feldman V, 2020, ACM S THEORY COMPUT, P954, DOI 10.1145/3357713.3384290.
   Geron A., 2019, HANDS ON MACHINE LEA, V2nd ed..
   Ghosal S, 2018, P NATL ACAD SCI USA, V115, P4613, DOI 10.1073/pnas.1716999115.
   Gigante S., 2019, ARXIV.
   GIROSI F, 1995, NEURAL COMPUT, V7, P219, DOI 10.1162/neco.1995.7.2.219.
   Giselsson TM, 2017, Arxiv, DOI arXiv:1711.05458.
   Hati AJ, 2021, AI-BASEL, V2, P274, DOI 10.3390/ai2020017.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Humphrey GB, 2017, ENVIRON MODELL SOFTW, V92, P82, DOI 10.1016/j.envsoft.2017.01.023.
   Kamal KC, 2021, AGRICULTURE-BASEL, V11, DOI 10.3390/agriculture11090827.
   Ketchen DJ, 1996, STRATEGIC MANAGE J, V17, P441, DOI 10.1002/(SICI)1097-0266(199606)17:6<441::AID-SMJ819>3.0.CO;2-G.
   Li D The, 2012, IEEE SIGNAL PROC MAG, V29, P141, DOI {[}DOI 10.1109/MSP.2012.2211477, 10.1109/MSP.2012.2211477].
   Lu H, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3058962.
   Lundberg S, 2017, Arxiv, DOI arXiv:1705.07874.
   Mohanty S. P., 2018, PLANT VILLAGE.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Montavon G., 2019, EXPLAINABLE INTERPRE, P193, DOI DOI 10.1007/978-3-030-28954-6\_10.
   Mostafa S., 2021, PREPRINT.
   Mostafa S, 2021, IEEE INT CONF COMP V, P1362, DOI 10.1109/ICCVW54120.2021.00157.
   Nagasubramanian K, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0479-8.
   Oh SJ., 2019, EXPLAINABLE INTERPRE, P121, DOI {[}DOI 10.1007/978-3-030-28954-6\_7, DOI 10.1007/978-3-030-28954-67].
   Prechelt L, 1998, LECT NOTES COMPUT SC, V1524, P55.
   Reed R., 1999, NEURAL SMITHING.
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778.
   Salman S, 2019, Arxiv, DOI {[}arXiv:1901.06566, 10.48550/ARXIV.1901.06566, DOI 10.48550/ARXIV.1901.06566].
   Scharr H, 2016, MACH VISION APPL, V27, P585, DOI 10.1007/s00138-015-0737-3.
   Selvaraju R.R., 2016, ARXIV, DOI DOI 10.48550/ARXIV.1611.07450.
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74.
   Simonyan K, 2014, Arxiv, DOI {[}arXiv:1312.6034, DOI 10.48550/ARXIV.1312.6034].
   Springenberg JT, 2015, Arxiv, DOI {[}arXiv:1412.6806, DOI 10.48550/ARXIV.1412.6806].
   Toneva M., 2018, ARXIV.
   Tzeng FY, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P383.
   Ubbens J, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0273-z.
   Ubbens JR, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01190.
   Weis M, 2008, GESUNDE PFLANZ, V60, P171, DOI 10.1007/s10343-008-0195-1.
   {[}翁杨 Weng Yang], 2019, {[}中国科学. 生命科学, Scientia Sinica Vitae], V49, P698.
   Xiao H, 2017, Arxiv, DOI {[}arXiv:1708.07747, DOI 10.48550/ARXIV.1708.07747].
   Xiao K., 2020, ARXIV.
   Ying X, 2019, J PHYS CONF SER, V1168, DOI 10.1088/1742-6596/1168/2/022022.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zenkl R, 2022, FRONT PLANT SCI, V12, DOI 10.3389/fpls.2021.774068.
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319.},
Number-of-Cited-References = {55},
Times-Cited = {2},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {7},
Journal-ISO = {Front. Artif. Intell.},
Doc-Delivery-Number = {7Z7ZV},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000915774400001},
OA = {gold, Green Published},
DA = {2023-08-12},
}

@inproceedings{ WOS:000380514500322,
Author = {Zhang, Chaoyun and Zhou, Pan and Li, Chenghua and Liu, Lijun},
Editor = {Wu, YL and Min, GY and Georgalas, N and Hu, J and Atzori, L and Jin, XL and Jarvis, S and Liu, L and Calvo, RA},
Title = {A Convolutional Neural Network for Leaves Recognition Using Data
   Augmentation},
Booktitle = {CIT/IUCC/DASC/PICOM 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND
   INFORMATION TECHNOLOGY - UBIQUITOUS COMPUTING AND COMMUNICATIONS -
   DEPENDABLE, AUTONOMIC AND SECURE COMPUTING - PERVASIVE INTELLIGENCE AND
   COMPUTING},
Year = {2015},
Pages = {2147-2154},
Note = {IEEE International Conference on Computer and Information, Liverpool,
   UNITED KINGDOM, OCT 26-28, 2015},
Organization = {IEEE; IEEE Comp Soc; TCSC IEEE; Univ EXETER; Liverpool Hope
   Univ,Liverpool JOHN MOORES UNIV},
Abstract = {Recently, convolutional neural networks (ConvNets) have achieved
   marvellous results in different field of recognition, especially in
   computer vision. In this paper, a seven-layer ConvNet using data
   augmentation is proposed for leaves recognition. First, we implement
   multiform transformations (e.g., rotation and translation etc.) to
   enlarge the dataset without changing their labels. This novel technique
   recently makes tremendous contribution to the performance of ConvNets as
   it is able to reduce the over-fitting degree and enhance the
   generalization ability of the ConvNet. Moreover, in order to get the
   shapes of leaves, we sharpen all the images with a random parameter.
   This method is similar to the edge detection, which has been proved
   useful in the image classification. Then we train a deep convolutional
   neural network to classify the augmented leaves data with three groups
   of test set and finally find that the method is quite feasible and
   effective. The accuracy achieved by our algorithm outperforms other
   methods for supervised learning on the popular leaf dataset Flavia.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zhang, CY (Corresponding Author), Huazhong Univ Sci \& Technol, Sch Elect Informat \& Commun, Wuhan, Peoples R China.
   Zhang, Chaoyun; Zhou, Pan, Huazhong Univ Sci \& Technol, Sch Elect Informat \& Commun, Wuhan, Peoples R China.
   Li, Chenghua, Beijing JingDong Century Trade Ltd JD COM, DNN Deep Neural Network Lab, Beijing, Peoples R China.
   Liu, Lijun, TipDM Intelligent Technol, Wuhan, Peoples R China.},
DOI = {10.1109/CIT/IUCC/DASC/PICOM.2015.318},
ISBN = {978-1-5090-0154-5},
Keywords-Plus = {SHAPE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Hardware \& Architecture; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods},
Author-Email = {s1500945@sms.ed.ac.uk
   panzhou@hust.edu.cn
   lchbidatech@163.com
   liu\_lijun9@hotmail.com},
Affiliations = {Huazhong University of Science \& Technology},
ResearcherID-Numbers = {Li, Chenghua/L-9825-2019},
ORCID-Numbers = {Li, Chenghua/0000-0003-2978-8762},
Cited-References = {{[}Anonymous], ARXIV14085093.
   {[}Anonymous], 2014, ECCV.
   Bouvrie J., 2006, NEURAL NETS.
   Ciresan D C, 2011, P 21 INT JOINT C ART.
   Dosovitskiy A, 2014, ARXIV1406.
   Du J., 2005, LNCS, V3497.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Eigen D., 2014, 2 INT C LEARN REPR.
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231.
   Gimpel K, 2010, SOFTMAX MARGIN CRFS.
   Gu X., 2005, LNCS.
   GUYER DE, 1993, T ASAE, V36, P163.
   He K., 2015, ARXIV150201852.
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   Krizhevsky A., 2012, P ADV NEUR INF PROC, P1106.
   Oide M, 2000, COMPUT ELECTRON AGR, V29, P59, DOI 10.1016/S0168-1699(00)00136-8.
   Qingfeng W, 2007, ADV ARTIFICIAL INTEL.
   Simard PY, 2003, PROC INT CONF DOC, P958.
   Soderkvist O, 2001, COMPUTER VISION CLAS.
   van Dyk DA, 2001, J COMPUT GRAPH STAT, V10, P1, DOI 10.1198/10618600152418584.
   WANG X, 2005, LNCS.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yan-fei Wang, 2012, COMPUTER ENG DESIGN, V33.
   Ye Y., 2004, P 2004 INT S INT MUL.
   Zhiwei N., 1997, J ANHUI U.},
Number-of-Cited-References = {26},
Times-Cited = {4},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BF2ZY},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000380514500322},
DA = {2023-08-12},
}

@inproceedings{ WOS:000310375800079,
Author = {Zhou, Xianping and Xu, Lijia and Zhao, Juan and Jiang, Nan and Wei, Yao
   and Li, Yang},
Book-Group-Author = {IEEE},
Title = {A Method on Leaves Recognition Based on Multiple Image Characteristics},
Booktitle = {2012 7TH INTERNATIONAL CONFERENCE ON SYSTEM OF SYSTEMS ENGINEERING
   (SOSE)},
Year = {2012},
Pages = {263-266},
Note = {7th International Conference on System of Systems Engineering (SoSE),
   Genoa, ITALY, JUL 16-19, 2012},
Organization = {IEEE Syst, Man \& Cybernet Soc (IEEE SMC); IEEE Reliabil Soc; Univ
   Genoa, Dept Commun Comp \& Syst Sci (dist); ACE; MABL},
Abstract = {Use MATLAB Image Processing Toolbox to extract eight typical
   characteristics from leaves, i.e., circumference, roundness, complexity,
   extension, sphericity, mean variation coefficient and saw-teeth degree.
   Then, using three kinds of classifier such as BP network, PNN and SVM to
   recognize the 400 samples of leaves, the recognition rate can be reached
   to 87.22\%, 88.95\%, and 95.15\%, respectively. Furthermore, B\&B
   algorithm is employed to select these characteristic sets and remove
   their redundancy, and thus, the processed characteristic sets are sent
   to SVM to accomplish the recognition of leaves. The experiment results
   show that the proposed method has better calculation efficiency than the
   other methods. This study provides an effective and useful method for
   the recognition of plant.},
Publisher = {IEEE COMPUTER SOC},
Address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Xu, LJ (Corresponding Author), Sichuan Agr Univ, Coll Informat \& Engn Technol, Yaan 625014, Peoples R China.
   Zhou, Xianping; Xu, Lijia; Zhao, Juan; Jiang, Nan; Wei, Yao; Li, Yang, Sichuan Agr Univ, Coll Informat \& Engn Technol, Yaan 625014, Peoples R China.},
ISBN = {978-1-4673-2975-0},
Keywords = {Leaves classification; Image processing; BP network; B\&B algorithm; SVM},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {lijiaxu13@163.com},
Affiliations = {Sichuan Agricultural University},
Cited-References = {He Peng, 2008, RES MACHINE RECOGNIT, P12.
   He Peng, 2008, J AGR MECH RES FORUM, V168-170, P6.
   Hou Tong, 2009, HUNAN AGR SCI FORUM, V123-129, P4.
   Hu Fei, 2011, 30 CASES INTELLIGENT, P248.
   LI Wei, 2006, J ENG GRAPHICS FORUM, V118-120, P2.
   Li Yang, 2010, 30 CASES NEURAL NETW, P176.
   Ma Xiulian, 2011, T CHINESE SOC AGR MA, V188-191, P42.
   Ridler T W, 1978, IEEE T SYST MAN CYB, V630\~{}632, P8.
   Wang Xiaofeng, 2006, COMPUTER ENG APPL FO, V190-193, P03.
   XU Hui, 2010, J BEIJING FORESTRY U, V85-89, P32.},
Number-of-Cited-References = {10},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BCK14},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000310375800079},
DA = {2023-08-12},
}

@article{ WOS:000595996000001,
Author = {Su, Jianyu and Wang, Meihua and Wu, Zhenxin and Chen, Qingliang},
Title = {Fast Plant Leaf Recognition Using Improved Multiscale Triangle
   Representation and KNN for Optimization},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {208753-208766},
Abstract = {Due to the complexity and similarity of plant leaves, it is very
   important to study an effective leaf-feature extraction method to
   improve the recognition rate of plant leaves. We study five multi-scale
   triangle representations: the triangle unsigned area representation
   (TUA), the triangle vertex angle representation (TVA) and three new
   representations, which we define as the gray average (TGA), the gray
   standard deviation (TGSD) and the side length integral (TSLI) on the
   triangle. In this method the curvature features of the contour, the
   texture features and the shape area feature are extracted to provide a
   multiscale leaf-feature description, and a new adaptive KNN for
   optimization method is proposed to improve the retrieval rate of leaf
   datasets. Experiments show that compared with the state-of-the-art
   methods, our method has higher accuracy on the Swedish and Flavia plant
   leaf datasets, which are respectively 99.35\% and 99.43\% with 84.76\%
   Mean Average Precision (MAP) value and has comparable results on MPEG-7,
   kimia99 and kimia216 datasets. When our method is combined with KNN for
   optimization, the retrieval rate of the above datasets has been
   significantly improved, especially MAP on the Flavia dataset increases
   to 94.48\%.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Wang, MH (Corresponding Author), South China Agr Univ, Coll Math \& Informat, Guangzhou 510642, Peoples R China.
   Su, Jianyu; Wu, Zhenxin; Chen, Qingliang, Jinan Univ, Dept Comp Sci, Guangzhou 510632, Peoples R China.
   Wang, Meihua, South China Agr Univ, Coll Math \& Informat, Guangzhou 510642, Peoples R China.
   Chen, Qingliang, Guangzhou Xuanyuan Res Inst Co Ltd, Guangzhou 510006, Peoples R China.},
DOI = {10.1109/ACCESS.2020.3037649},
ISSN = {2169-3536},
Keywords = {Shape; Transform coding; Optimization methods; Feature extraction;
   Complexity theory; Standards; Plant leaf recognition; multi-scale
   leaf-feature description; multi-scale triangle representation; adaptive
   KNN for optimization},
Keywords-Plus = {CLASSIFICATION; SHAPES; RETRIEVAL; FEATURES; TEXTURE; IMAGE},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {wangmeihua@scau.edu.cn},
Affiliations = {Jinan University; South China Agricultural University},
Funding-Acknowledgement = {Basic and Applied Basic Research Fund of Guangdong Province
   {[}2019B1515210009]; National Natural Science Foundation of China
   {[}62071201, 61976052]},
Funding-Text = {This work was supported in part by the Basic and Applied Basic Research
   Fund of Guangdong Province under Grant 2019B1515210009 and in part by
   the National Natural Science Foundation of China under Grant 62071201
   and Grant 61976052.},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Abdul K., 2011, SIGNAL IMAGE PROCESS, V2, P1, DOI 10.5121/sipij.2011.2301.
   Ajij M, 2019, INT CONF IMAG VIS, DOI 10.1109/ivcnz48456.2019.8961036.
   Aptoula E, 2013, IEEE IMAGE PROC, P1496, DOI 10.1109/ICIP.2013.6738307.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Bai X, 2008, IEEE T PATTERN ANAL, V30, P1282, DOI 10.1109/TPAMI.2007.70769.
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
   Cao J, 2016, INFORM SCIENCES, V374, P51, DOI 10.1016/j.ins.2016.09.023.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chaki J, 2011, INT J ADV COMPUT SC, V2, P41.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Demisse GG, 2018, IEEE T PATTERN ANAL, V40, P1338, DOI 10.1109/TPAMI.2017.2711607.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   El Massie I, 2016, I C COMP GRAPH IM VI, P131, DOI 10.1109/CGiV.2016.34.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Herdiyeni Y, 2012, 2012 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER SCIENCE AND INFORMATION SYSTEMS, P301.
   Horaisova K, 2016, BIOSYST ENG, V142, P83, DOI 10.1016/j.biosystemseng.2015.12.007.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Kadir Abdul, 2012, Journal of Theoretical and Applied Information Technology, V41, P82.
   Kumar M, 2019, IEEE ACCESS, V7, P163912, DOI 10.1109/ACCESS.2019.2952176.
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850.
   Lee K.-B., 2013, LNEE, V214, P109.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Liu MZ, 2012, IEEE T PATTERN ANAL, V34, P2407, DOI 10.1109/TPAMI.2012.44.
   Man QK, 2008, COMM COM INF SC, V15, P192.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Nijalingappa P, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON APPLIED AND THEORETICAL COMPUTING AND COMMUNICATION TECHNOLOGY (ICATCCT), P338, DOI 10.1109/ICATCCT.2015.7456906.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Sari Cihan, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P23.
   Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924.
   Shabanzade M., 2011, SIGNAL IMAGE PROCESS, V2, P23, DOI DOI 10.5121/sipij.2011.2303.
   Soderkvist O. O., 2010, TEKNIK TEKNOLOGIER, V33, P202.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Turkoglu M, 2019, APPL MATH COMPUT, V352, P1, DOI 10.1016/j.amc.2019.01.054.
   Wang B, 2015, INFORM SCIENCES, V302, P132, DOI 10.1016/j.ins.2014.07.028.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xuming Yu, 2016, 2016 IEEE International Workshop on Electromagnetics (iWEM): Applications and Student Innovation Competition, P1, DOI 10.1109/iWEM.2016.7505042.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhou CG, 2019, IEEE ACCESS, V7, P148779, DOI 10.1109/ACCESS.2019.2946681.},
Number-of-Cited-References = {42},
Times-Cited = {11},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {9},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {PB0CP},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000595996000001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000939104200001,
Author = {Yuan, Jiayu and Wu, Zhiwei and Li, Shun and Kang, Ping and Zhu, Shihao},
Title = {Multi-Feature-Based Identification of Subtropical Evergreen Tree Species
   Using Gaofen-2 Imagery and Algorithm Comparison},
Journal = {FORESTS},
Year = {2023},
Volume = {14},
Number = {2},
Month = {FEB},
Abstract = {The species and distribution of trees in a forest are critical to the
   understanding of forest ecosystem processes and the development of
   forest management strategies. Subtropical forest landscapes feature a
   complex canopy structure and high stand density. Studies on the effects
   of classification algorithms on the remote sensing-based identification
   of tree species are few. GF-2 is the first satellite in China with
   sub-meter accuracy which has the high resolution and short replay cycle.
   Here, we considered three representative tree types (Masson pine,
   Chinese fir, and broadleaved evergreen trees) in the southern
   subtropical evergreen broadleaved forest region of China as research
   objects. We quantitatively compared the effects of five machine learning
   algorithms, including the backpropagation neural network, k-nearest
   neighbour, polytomous logistic regression, random forest (RF) and
   support vector machine (SVM), and four features (vegetation index, band
   reflectance, textural features, and topographic factors) on tree species
   identification using Gaofen-2 panchromatic and multispectral remote
   sensing images and field survey data. All five classification algorithms
   could effectively identify major tree species in subtropical forest
   areas (overall accuracy {[}OA] > 87.40\%, kappa coefficient > 81.08\%).
   The SVM model exhibited the best identification ability (OA = 90.27\%,
   kappa coefficient = 85.37\%), followed by RF (OA = 88.90\%, Kappa
   coefficient = 83.30\%). The combination of band reflectance, vegetation
   index, and the topographic factor performed exhibited the best, followed
   by the combination of band reflectance, vegetation index, textural
   feature, and topographic factor. In addition, we find that the
   classifier constructed by a single feature is not as effective as the
   combination of multiple feature factors. The addition of topographic
   factors can significantly improve the ability of tree species
   identification. According to the results of the five classifiers, the
   separability of the three tree species was good. The producer's accuracy
   and user's accuracy of Masson pine were more than 90\%, and the
   evergreen broad-leaved tree and Chinese fir were more than 80\%. The
   commission errors and omission errors of the three tree species were
   evergreen broadleaved tree > Chinese fir > Masson pine. The variable
   importance assessment results showed that the normalized difference
   greenness index, altitude, and the modified soil-adjusted vegetation
   index were the key variables. The results of this study used GF-2 to
   accurately identify the main tree species of subtropical evergreen
   forests in China, which can help forest managers to regularly monitor
   tree species composition and provide theoretical support for forest
   managers to formulate policies, monitor sustainable plans for wood
   mining, and forest conservation and management measures.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Wu, ZW (Corresponding Author), Jiangxi Normal Univ, Key Lab Poyang Lake Wetland \& Watershed Res, Minist Educ, Nanchang 330022, Peoples R China.
   Wu, ZW (Corresponding Author), Jiangxi Normal Univ, Key Lab Nat Disaster Monitoring Early Warning \& As, Nanchang 330022, Peoples R China.
   Wu, ZW (Corresponding Author), Jiangxi Normal Univ, Sch Geog \& Environm, Nanchang 330022, Peoples R China.
   Yuan, Jiayu; Wu, Zhiwei; Li, Shun; Kang, Ping; Zhu, Shihao, Jiangxi Normal Univ, Key Lab Poyang Lake Wetland \& Watershed Res, Minist Educ, Nanchang 330022, Peoples R China.
   Yuan, Jiayu; Wu, Zhiwei; Li, Shun; Kang, Ping; Zhu, Shihao, Jiangxi Normal Univ, Key Lab Nat Disaster Monitoring Early Warning \& As, Nanchang 330022, Peoples R China.
   Yuan, Jiayu; Wu, Zhiwei; Li, Shun; Kang, Ping; Zhu, Shihao, Jiangxi Normal Univ, Sch Geog \& Environm, Nanchang 330022, Peoples R China.},
DOI = {10.3390/f14020292},
Article-Number = {292},
EISSN = {1999-4907},
Keywords = {tree species identification; remote sensing identification; machine
   learning; multi-feature combination; subtropical forest},
Keywords-Plus = {SPATIAL-DISTRIBUTION; FOREST; CLASSIFICATION; VEGETATION; SATELLITE;
   INDEX; CLASSIFIERS; AREA; RED},
Research-Areas = {Forestry},
Web-of-Science-Categories  = {Forestry},
Author-Email = {wuzhiwei@jxnu.edu.cn},
Affiliations = {Jiangxi Normal University; Jiangxi Normal University; Jiangxi Normal
   University},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}31960253]},
Funding-Text = {This research was funded by the National Natural Science Foundation of
   China, grant number is 31960253.},
Cited-References = {Ali A, 2021, ECOL INDIC, V127, DOI 10.1016/j.ecolind.2021.107760.
   Astorga A, 2018, FORESTS, V9, DOI 10.3390/f9070385.
   Ballanti L, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8060445.
   Becker A, 2023, ISPRS J PHOTOGRAMM, V195, P269, DOI 10.1016/j.isprsjprs.2022.11.011.
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324.
   Buhvald AP, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14143387.
   Calle M Luz, 2011, Brief Bioinform, V12, P86, DOI 10.1093/bib/bbq011.
   Carleer A, 2004, PHOTOGRAMM ENG REM S, V70, P135, DOI 10.14358/PERS.70.1.135.
   Che SH, 2019, J FORESTRY RES, V30, P1641, DOI 10.1007/s11676-018-0711-9.
   Cheng K, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13050973.
   Chuvieco E, 2002, INT J REMOTE SENS, V23, P5103, DOI 10.1080/01431160210153129.
   Cilek A, 2022, ENVIRON SCI POLLUT R, V29, P23665, DOI 10.1007/s11356-021-17333-5.
   Dalponte M, 2008, IEEE T GEOSCI REMOTE, V46, P1416, DOI 10.1109/TGRS.2008.916480.
   Ding Shi-fei, 2011, Journal of University of Electronic Science and Technology of China, V40, P2, DOI 10.3969/j.issn.1001-0548.2011.01.001.
   Dyderski MK, 2020, FOREST ECOL MANAG, V474, DOI 10.1016/j.foreco.2020.118366.
   Eckert S, 2012, REMOTE SENS-BASEL, V4, P810, DOI 10.3390/rs4040810.
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013.
   Fei H, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14040829.
   Feret JB, 2013, IEEE T GEOSCI REMOTE, V51, P73, DOI 10.1109/TGRS.2012.2199323.
   Forster M., 2008, OBJECT BASED IMAGE A, P275.
   Gao Jinping, 2022, Spacecraft Engineering, P187, DOI 10.3969/j.issn.1673-8748.2022.03.027.
   Ghimire BR, 2017, FORESTS, V8, DOI 10.3390/f8100384.
   Ghosh A, 2014, INT J APPL EARTH OBS, V26, P49, DOI 10.1016/j.jag.2013.05.017.
   Gitelson AA, 2005, GEOPHYS RES LETT, V32, DOI 10.1029/2005GL022688.
   Gitelson AA, 1996, J PLANT PHYSIOL, V148, P494, DOI 10.1016/S0176-1617(96)80284-7.
   Gyamfi-Ampadu E, 2021, FORESTS, V12, DOI 10.3390/f12060739.
   Heinzel J, 2012, INT J APPL EARTH OBS, V18, P101, DOI 10.1016/j.jag.2012.01.025.
   Hogland J, 2013, EUR J REMOTE SENS, V46, P623, DOI 10.5721/EuJRS20134637.
   Hoscilo A, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11080929.
   Hua LZ, 2017, ISPRS INT J GEO-INF, V6, DOI 10.3390/ijgi6110331.
   HUETE A R, 1988, Remote Sensing of Environment, V25, P295.
   Jia W, 2016, INT GEOSCI REMOTE SE, P2284, DOI 10.1109/IGARSS.2016.7729590.
   JORDAN CF, 1969, ECOLOGY, V50, P663, DOI 10.2307/1936256.
   Justice CO, 1998, IEEE T GEOSCI REMOTE, V36, P1228, DOI 10.1109/36.701075.
   Kampouri M, 2019, GEOCARTO INT, V34, P1273, DOI 10.1080/10106049.2018.1489424.
   Li LH, 2021, IEEE SENS J, V21, P17447, DOI 10.1109/JSEN.2020.3045501.
   Li XuSheng, 2020, Scientia Silvae Sinicae, V56, P93, DOI 10.11707/j.1001-7488.20201010.
   {[}梁爽 Liang Shuang], 2021, {[}遥感技术与应用, Remote Sensing Technology and Application], V36, P777.
   Linfei C.A., 2019, FOREST RESOUR MANAG, V5, P44, DOI {[}10.13466/j.cnki.lyzygl.2019.05.009, DOI 10.13466/J.CNKI.LYZYGL.2019.05.009].
   Luo YM, 2017, ISPRS INT GEO-INF, V6, DOI 10.3390/ijgi6060177.
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865.
   Meyer GE, 2008, COMPUT ELECTRON AGR, V63, P282, DOI 10.1016/j.compag.2008.03.009.
   Modzelewska A, 2020, INT J APPL EARTH OBS, V84, DOI 10.1016/j.jag.2019.101960.
   Mori AS, 2017, J APPL ECOL, V54, P12, DOI 10.1111/1365-2664.12669.
   Mountrakis G, 2011, ISPRS J PHOTOGRAMM, V66, P247, DOI 10.1016/j.isprsjprs.2010.11.001.
   O'Brien RM, 2007, QUAL QUANT, V41, P673, DOI 10.1007/s11135-006-9018-6.
   Oke OA, 2015, ECOL MODEL, V301, P72, DOI 10.1016/j.ecolmodel.2015.01.019.
   Olofsson P, 2014, REMOTE SENS ENVIRON, V148, P42, DOI 10.1016/j.rse.2014.02.015.
   Pei Huan, 2018, Transactions of the Chinese Society of Agricultural Engineering, V34, P248.
   PINTY B, 1992, VEGETATIO, V101, P15, DOI 10.1007/BF00031911.
   QI J, 1994, REMOTE SENS ENVIRON, V48, P119, DOI 10.1016/0034-4257(94)90134-1.
   Qin HM, 2022, REMOTE SENS ENVIRON, V280, DOI 10.1016/j.rse.2022.113143.
   Rautiainen M, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10020207.
   Richter R, 2016, INT J APPL EARTH OBS, V52, P464, DOI 10.1016/j.jag.2016.07.018.
   Rondeaux G, 1996, REMOTE SENS ENVIRON, V55, P95, DOI 10.1016/0034-4257(95)00186-7.
   Sarker LR, 2011, REMOTE SENS ENVIRON, V115, P968, DOI 10.1016/j.rse.2010.11.010.
   Scornet E, 2016, IEEE T INFORM THEORY, V62, P1485, DOI 10.1109/TIT.2016.2514489.
   Shen L, 2016, KNOWL-BASED SYST, V96, P61, DOI 10.1016/j.knosys.2016.01.002.
   Song ZS, 2018, FOREST ECOL MANAG, V425, P189, DOI 10.1016/j.foreco.2018.05.046.
   STEWART JB, 1993, J HYDROL, V150, P701, DOI 10.1016/0022-1694(93)90132-S.
   Tian Tian, 2015, Yingyong Shengtai Xuebao, V26, P1665.
   TUCKER CJ, 1979, REMOTE SENS ENVIRON, V8, P127, DOI 10.1016/0034-4257(79)90013-0.
   Valjarevic A, 2018, APPL GEOGR, V92, P131, DOI 10.1016/j.apgeog.2018.01.016.
   {[}王怀警 Wang Huaijing], 2019, {[}遥感信息, Remote Sensing Information], V34, P104.
   Wang MC, 2022, FORESTS, V13, DOI 10.3390/f13071058.
   {[}王修信 Wang Xiuxin], 2013, {[}计算机工程与应用, Computer Engineering and Application], V49, P259.
   Wu YS, 2020, FORESTS, V11, DOI 10.3390/f11010032.
   Wurm M, 2017, REMOTE SENS ENVIRON, V194, P190, DOI 10.1016/j.rse.2017.03.030.
   Xu ZH, 2013, SPECTROSC SPECT ANAL, V33, P3359, DOI 10.3964/j.issn.1000-0593(2013)12-3359-07.
   Yao X, 2019, CATENA, V178, P189, DOI 10.1016/j.catena.2019.03.004.
   Zhang LJ, 2019, ANAL METHODS-UK, V11, P4566, DOI 10.1039/c9ay01155b.
   Zhang YH, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13224682.
   Zhang Yue-nan, 2020, Shengtaixue Zazhi, V39, P1636, DOI 10.13292/j.1000-4890.202005.016.
   Zhang Y, 2022, PALAEOGEOGR PALAEOCL, V608, DOI 10.1016/j.palaeo.2022.111293.
   Zhu J, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19092087.},
Number-of-Cited-References = {75},
Times-Cited = {0},
Usage-Count-Last-180-days = {7},
Usage-Count-Since-2013 = {7},
Journal-ISO = {Forests},
Doc-Delivery-Number = {9H8VU},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000939104200001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000349495800042,
Author = {Fern, Bong Mei and Bin Sulong, Ghazali and Rahim, Mohd Shafry Mohd},
Title = {Leaf Recognition Based on Leaf Tip and Leaf Base Using Centroid Contour
   Gradient},
Journal = {ADVANCED SCIENCE LETTERS},
Year = {2014},
Volume = {20},
Number = {1, SI},
Pages = {209-212},
Month = {JAN},
Note = {International Conference on Internet Services Technology and Information
   Engineering (ISTIE), Bogor, INDONESIA, MAY 11-12, 2013},
Abstract = {In this paper, we suggest to normalize the leaf tip and leaf base as
   both of them may incline to one direction which able to influence the
   data extraction process. The features extraction method we used is
   Centroid Contour Gradient (CCG) which calculate the gradient between
   pairs of boundary point corresponding to interval angle, theta. CCG had
   outperformed its competitors which is Centroid Contours Distance (CCD)
   as it is successfully captures the curvature of leaf tip and leaf base.
   The accuracy to classify the leaf tip using CCG is 99.47\%, and CCD is
   only 80.30\%. For the accuracy of leaf base classification, CCG (98\%)
   also outperforms CCD (88\%). The average accuracy to recognize the 5
   classes of plant is 96.6\% for CCG and 74.4\% for CCD. In this research,
   we utilized the Feed-forwad Back-propagation as our classifier.},
Publisher = {AMER SCIENTIFIC PUBLISHERS},
Address = {26650 THE OLD RD, STE 208, VALENCIA, CA 91381-0751 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Fern, BM (Corresponding Author), Univ Teknol Malaysia, Fac Comp, Johor Baharu 81310, Malaysia.
   Fern, Bong Mei; Bin Sulong, Ghazali; Rahim, Mohd Shafry Mohd, Univ Teknol Malaysia, Fac Comp, Johor Baharu 81310, Malaysia.},
DOI = {10.1166/asl.2014.5300},
ISSN = {1936-6612},
EISSN = {1936-7317},
Keywords = {Leaf Recognition; Centroid Contour Distance; Centroid Contour Gradient;
   Leaf Tip; Leaf Base},
Keywords-Plus = {IMAGE RETRIEVAL},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Affiliations = {Universiti Teknologi Malaysia},
ResearcherID-Numbers = {Bin Mohd Rahim, Mohd Shafry/B-7552-2016},
Cited-References = {{[}Anonymous], IEEE 7 INT S SIGN PR.
   {[}Anonymous], 2013, INT J COMPUTER APPL.
   Backes A. R., 2009, INT J PATTERN RECOGN.
   Chaki J., 2012, INT J ADV ENG TECHNO.
   chaki Jyotismita, 2011, INT J ADV COMPUTER S.
   Cope J., 2012, UCI MACH LEARN REPOS.
   Du J., 2005, LNCS, V3497.
   Du J. X., 2007, APPL MATH COMPUT, V185.
   Fu H, 2006, IEE P-VIS IMAGE SIGN, V153, P881, DOI 10.1049/ip-vis:20060061.
   Fu H, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS \& SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P208.
   Fu H., 2004, IEEE 2004 8 INT C CO.
   Gu X, 2005, LECT NOTES COMPUT SC, V3644, P253.
   Gu X., 2005, ORIENTAL J SCI TECHN.
   Kadir A., 2011, INT J COMPUTER SCI I.
   Kadir A., 2011, INT J COMPUTER APPL.
   Kadir A., 2011, INT J COMPUTER TREND.
   Li YF, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P885.
   Man QK, 2008, COMM COM INF SC, V15, P192.
   Nam Y, 2005, LECT NOTES COMPUT SC, V3687, P589.
   QI H, 2003, P 2 INT C MACH LEARN.
   Shrestha D. S., 2004, BIOSYST ENG, V89.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Wang QP, 2010, LECT NOTES ARTIF INT, V6216, P240, DOI 10.1007/978-3-642-14932-0\_30.
   WANG X, 2005, LNCS.
   Wang Z., 2003, IEE P-VIS IMAGE SIGN, P150.
   Wang ZY, 2000, LECT NOTES COMPUT SC, V1929, P477.
   Ye Y., 2004, P 2004 INT S INT MUL.
   Zhung GJ, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P165, DOI 10.1109/ISIMP.2004.1434026.
   Zulkii Z., 2009, THESIS U TEKNOLOGI M.},
Number-of-Cited-References = {29},
Times-Cited = {4},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Adv. Sci. Lett.},
Doc-Delivery-Number = {CB2ZA},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S); Conference Proceedings Citation Index- Social Science &amp; Humanities (CPCI-SSH)},
Unique-ID = {WOS:000349495800042},
DA = {2023-08-12},
}

@article{ WOS:000395219700024,
Author = {Ghazi, Mostafa Mehdipour and Yanikoglu, Berrin and Aptoula, Erchan},
Title = {Plant identification using deep neural networks via optimization of
   transfer learning parameters},
Journal = {NEUROCOMPUTING},
Year = {2017},
Volume = {235},
Pages = {228-235},
Month = {APR 26},
Abstract = {We use deep convolutional neural networks to identify the plant species
   captured in a photograph and evaluate different factors affecting the
   performance of these networks. Three powerful and popular deep learning
   architectures, namely GoogLeNet, AlexNet, and VGGNet, are used for this
   purpose. Transfer learning is used to fine-tune the pre-trained models,
   using the plant task datasets of LifeCLEF 2015. To decrease the chance
   of overfitting, data augmentation techniques are applied based on image
   transforms such as rotation, translation, reflection, and scaling.
   Furthermore, the networks' parameters are adjusted and different
   classifiers are fused to improve overall performance. Our best combined
   system has achieved an overall accuracy of 80\% on the validation set
   and an overall inverse rank score of 0.752 on the official test set. A
   comparison of our results against the results of the LifeCLEF 2015 plant
   identification campaign shows that we have improved the overall
   validation accuracy of the top system by 15\% points and its overall
   inverse rank score on the test set by 0.1 while outperforming the top
   three competition participants in all categories. The system recently
   obtained a very close second place in the P1antCLEF 2016.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Ghazi, MM (Corresponding Author), Sabanci Univ, Fac Engn \& Nat Sci, Istanbul, Turkey.
   Ghazi, Mostafa Mehdipour; Yanikoglu, Berrin, Sabanci Univ, Fac Engn \& Nat Sci, Istanbul, Turkey.
   Aptoula, Erchan, Gebze Tech Univ, Kocaeli, Turkey.},
DOI = {10.1016/j.neucom.2017.01.018},
ISSN = {0925-2312},
EISSN = {1872-8286},
Keywords = {Convolutional neural networks; Deep learning; Plant identification;
   Transfer learning; Inverse rank score},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {mehdipour@sabanciuniv.edu
   berrin@sabanciuniv.edu
   eaptoula@gtu.edu.tr},
Affiliations = {Sabanci University; Gebze Technical University},
ResearcherID-Numbers = {Yanikoglu, Berrin/AAE-4843-2022
   Aptoula, Erchan/AAI-1070-2020
   },
ORCID-Numbers = {Yanikoglu, Berrin/0000-0001-7403-7592
   Aptoula, Erchan/0000-0001-6168-2883
   Mehdipour Ghazi, Mostafa/0000-0002-8473-281X},
Funding-Acknowledgement = {Scientific and Technological Research Council of Turkey (TUBITAK)
   {[}113E499]},
Funding-Text = {This work is supported by the Scientific and Technological Research
   Council of Turkey (TUBITAK) under the grant number 113E499.},
Cited-References = {{[}Anonymous], 2016, LIFECLEF MULT RETR C.
   Aptoula E, 2013, IEEE IMAGE PROC, P1496, DOI 10.1109/ICIP.2013.6738307.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Champ J., 2015, CLEF WORKING NOTES.
   Chen Q., 2014, CLEF WORKING NOTES.
   Choi Sungbin, 2015, CLEF.
   Clarke J, 2006, LECT NOTES COMPUT SC, V4292, P427.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6474, P135, DOI 10.1007/978-3-642-17688-3\_14.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231.
   Goeau H., 2013, CLEF WORKING NOTES.
   Goeau H., 2015, CLEF WORKING NOTES.
   GUYER DE, 1986, T ASAE, V29, P1500.
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527.
   Hong An-Xiang, 2004, J Zhejiang Univ Sci, V5, P764, DOI 10.1631/jzus.2004.0764.
   Hussein A. N., 2011, 2011 Proceedings of IEEE 7th International Colloquium on Signal Processing \& its Applications (CSPA 2011), P11, DOI 10.1109/CSPA.2011.5759833.
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lee CL, 2006, INT J IMAG SYST TECH, V16, P15, DOI 10.1002/ima.20063.
   Li Y, 2006, IEEE SYS MAN CYBERN, P3890, DOI 10.1109/ICSMC.2006.384738.
   Lin FY, 2008, COMM COM INF SC, V15, P432.
   Lin M., ARXIV13124400 CORR.
   McCool C., 2015, P C LABS EV FOR JAN.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Nilsback ME, 2010, IMAGE VISION COMPUT, V28, P1049, DOI 10.1016/j.imavis.2009.10.001.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Szegedy C., 2015, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2015.7298594.
   Torrey L., 2009, MACH LEARNING APPL T, P242.
   Wang L., ARXIV150702159 CORR.
   Wang YF, 2015, IEEE WINT CONF APPL, P876, DOI 10.1109/WACV.2015.121.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197.
   Zhou Bolei, 2014, ADV NEURAL INFORM PR, P487.},
Number-of-Cited-References = {41},
Times-Cited = {277},
Usage-Count-Last-180-days = {11},
Usage-Count-Since-2013 = {160},
Journal-ISO = {Neurocomputing},
Doc-Delivery-Number = {EM3MQ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000395219700024},
DA = {2023-08-12},
}

@inproceedings{ WOS:000303356500055,
Author = {Yusof, Rubiyah and Rosli, Nenny Ruthfalydia and Khalid, Marzuki},
Editor = {AlDabass, D and Orsoni, A and Cant, R and Abraham, A},
Title = {Using Gabor Filters as Image Multiplier for Tropical Wood Species
   Recognition System},
Booktitle = {2010 12TH INTERNATIONAL CONFERENCE ON COMPUTER MODELLING AND SIMULATION
   (UKSIM)},
Year = {2010},
Pages = {289-294},
Note = {12th International Conference on Computer Modelling and Simulation
   (UKSim), Emmanuel Coll, Cambridge, ENGLAND, MAR 24-26, 2010},
Organization = {IEEE; IEEE Comp Soc (UK \& RI); Nottingham Trent Univ; United Kingdom
   Simulat Soc; European Federat Simulat Soc (EUROSIM); European Council
   Modelling \& Simulat (ECMS); Asia Modelling \& Simulat Soc (AMSS);
   Kingston Univ; Machine Intelligence Res Labs (MIR Labs); Norwegian Univ
   Sci \& Technol (NTNU); Univ Technol Malaysia (UTM)},
Abstract = {One of the main problems in wood species recognition systems is the lack
   of discriminative features of the texture images. In order to overcome
   this, we use Gabor filter in the pre-processing stage of the wood
   texture image to multiply the number of features for a single image,
   thus providing more information for feature extractor to capture. The
   textural wood features are extracted using two feature extraction
   methods which are co-occurrence matrix approach, known as grey level
   co-occurrence matrix (GLCM) and also Gabor filters to generate more
   variation of features and to improve the accuracy rate. The combined
   features extracted from GLCM and Gabor filters are sent to the
   classifier module. A multi-layer neural network based on the popular
   back propagation (MLBP) algorithm is used for classification. The
   results show that increasing the number of features by using Gabor
   filters as image multiplier and the combination of features from Gabor
   filters and GLCM feature extractors improved the accuracy rate of the
   wood species recognition system.},
Publisher = {IEEE COMPUTER SOC},
Address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Yusof, R (Corresponding Author), Univ Teknol Malaysia, Ctr Artificial Intelligence \& Robot, Int Campus, Kuala Lumpur 54100, Malaysia.
   Yusof, Rubiyah; Rosli, Nenny Ruthfalydia; Khalid, Marzuki, Univ Teknol Malaysia, Ctr Artificial Intelligence \& Robot, Kuala Lumpur 54100, Malaysia.},
DOI = {10.1109/UKSIM.2010.61},
ISBN = {978-0-7695-4016-0},
Keywords = {image multiplier; texture pattern recognition; Gabor filter; grey level
   co-occurrence matrix (GLCM); neural network; wood recognition},
Research-Areas = {Computer Science; Engineering; Materials Science},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications; Engineering,
   Electrical \& Electronic; Materials Science, Multidisciplinary},
Author-Email = {rubiyah@ic.utm.my},
Affiliations = {Universiti Teknologi Malaysia},
ResearcherID-Numbers = {yusof, rubiyah/AAV-9212-2020
   yusof, rubiyah/A-1706-2013},
Cited-References = {ALLIER B, 2003, P 7 INT C DOC AN REC.
   Andrysiak T, 2005, INT J AP MAT COM-POL, V15, P471.
   {[}Anonymous], 1974, P 2 INT JOINT C PATT.
   {[}Anonymous], 1998, HDB PATTERN RECOGNIT.
   Bhuiyan AA, 2007, PROC WRLD ACAD SCI E, V22, P51.
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160.
   Delac K., 2006, P INT C SYST SIGN IM, P95.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Huang LL, 2005, PATTERN RECOGN LETT, V26, P1641, DOI 10.1016/j.patrec.2005.01.015.
   Khalid M, 2008, INT J SIMUL SYST SCI, V9, P9.
   Lew Y. L., 2005, DESIGN INTELLIGENT W.
   MACLENNAN BJ, 1991, CS91144 U TENN.
   {*}MAL TIMB COUNC, 2007, MAL SUST FOR MAN.
   Menon P. K. B., 1993, STRUCTURE IDENTIFICA.
   OMATU S, 1995, NEUROCONTROL ITS APP.
   Tuceryan M., 1993, TEXTURE ANAL HDB PAT, V2, P207, DOI DOI 10.1142/9789814343138\_0010.
   Wheeler EA, 1998, IAWA J, V19, P241, DOI 10.1163/22941932-90001528.},
Number-of-Cited-References = {18},
Times-Cited = {23},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BZY27},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000303356500055},
DA = {2023-08-12},
}

@article{ WOS:000859495800001,
Author = {Zhang, Jian-Lin and Su, Wen-Hao and Zhang, He-Yi and Peng, Yankun},
Title = {SE-YOLOv5x: An Optimized Model Based on Transfer Learning and Visual
   Attention Mechanism for Identifying and Localizing Weeds and Vegetables},
Journal = {AGRONOMY-BASEL},
Year = {2022},
Volume = {12},
Number = {9},
Month = {SEP},
Abstract = {Weeds in the field affect the normal growth of lettuce crops by
   competing with them for resources such as water and sunlight. The
   increasing costs of weed management and limited herbicide choices are
   threatening the profitability, yield, and quality of lettuce. The
   application of intelligent weeding robots is an alternative to control
   intra-row weeds. The prerequisite for automatic weeding is accurate
   differentiation and rapid localization of different plants. In this
   study, a squeeze-and-excitation (SE) network combined with You Only Look
   Once v5 (SE-YOLOv5x) is proposed for weed-crop classification and
   lettuce localization in the field. Compared with models including
   classical support vector machines (SVM), YOLOv5x, single-shot multibox
   detector (SSD), and faster-RCNN, the SE-YOLOv5x exhibited the highest
   performance in weed and lettuce plant identifications, with precision,
   recall, mean average precision (mAP), and Fl-score values of 97.6\%,
   95.6\%, 97.1\%, and 97.3\%, respectively. Based on plant morphological
   characteristics, the SE-YOLOv5x model detected the location of lettuce
   stem emerging points in the field with an accuracy of 97.14\%. This
   study demonstrates the capability of SE-YOLOv5x for the classification
   of lettuce and weeds and the localization of lettuce, which provides
   theoretical and technical support for automated weed control.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Su, WH (Corresponding Author), China Agr Univ, Coll Engn, Beijing 100083, Peoples R China.
   Zhang, Jian-Lin; Su, Wen-Hao; Zhang, He-Yi; Peng, Yankun, China Agr Univ, Coll Engn, Beijing 100083, Peoples R China.},
DOI = {10.3390/agronomy12092061},
Article-Number = {2061},
EISSN = {2073-4395},
Keywords = {YOLOv5; attention mechanism; transfer learning; deep learning; weed
   identification; SVM; machine learning},
Keywords-Plus = {CROPS; CLASSIFICATION; SYSTEM},
Research-Areas = {Agriculture; Plant Sciences},
Web-of-Science-Categories  = {Agronomy; Plant Sciences},
Author-Email = {wenhao.su@cau.edu.cn},
Affiliations = {China Agricultural University},
ResearcherID-Numbers = {Su, Wen-Hao/Q-2431-2019},
ORCID-Numbers = {Su, Wen-Hao/0000-0003-1745-4722},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}32101610]},
Funding-Text = {This research was funded by National Natural Science Foundation of
   China, grant number 32101610.},
Cited-References = {Abdalla A, 2019, COMPUT ELECTRON AGR, V167, DOI 10.1016/j.compag.2019.105091.
   Ahmad A, 2021, COMPUT ELECTRON AGR, V184, DOI 10.1016/j.compag.2021.106081.
   Ahmed F, 2012, CROP PROT, V40, P98, DOI 10.1016/j.cropro.2012.04.024.
   {[}Anonymous], 2018, P IEEE C COMP VIS PA.
   Chen D, 2022, COMPUT ELECTRON AGR, V198, DOI 10.1016/j.compag.2022.107091.
   Chen JD, 2021, EXPERT SYST APPL, V169, DOI 10.1016/j.eswa.2020.114514.
   Christopher M, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-35044-9.
   Ferreira AD, 2017, COMPUT ELECTRON AGR, V143, P314, DOI 10.1016/j.compag.2017.10.027.
   Gao JF, 2018, BIOSYST ENG, V170, P39, DOI 10.1016/j.biosystemseng.2018.03.006.
   Garibaldi-Marquez F, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22083021.
   Gong H, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14122861.
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI {[}10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372].
   Hu K, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105520.
   Jiang HH, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105450.
   Jin XJ, 2022, PEST MANAG SCI, V78, P1861, DOI 10.1002/ps.6804.
   Jin XJ, 2021, IEEE ACCESS, V9, P10940, DOI 10.1109/ACCESS.2021.3050296.
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0\_2.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Osorio K, 2020, AGRIENGINEERING, V2, P471, DOI 10.3390/agriengineering2030032.
   Perez-Ruiz M, 2012, COMPUT ELECTRON AGR, V80, P41, DOI 10.1016/j.compag.2011.10.006.
   Perez-Ruiz M, 2014, BIOSYST ENG, V126, P45, DOI 10.1016/j.biosystemseng.2014.07.009.
   Picon A, 2022, COMPUT ELECTRON AGR, V194, DOI 10.1016/j.compag.2022.106719.
   Qi JT, 2022, COMPUT ELECTRON AGR, V194, DOI 10.1016/j.compag.2022.106780.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Su W.H., 2020, ARTIF INTELL AGR, V4, P262, DOI {[}10.1016/j.aiia.2020.11.001, DOI 10.1016/J.AIIA.2020.11.001].
   Su WH, 2020, SMART CITIES-BASEL, V3, P767, DOI 10.3390/smartcities3030039.
   Su WH, 2020, BIOSYST ENG, V193, P62, DOI 10.1016/j.biosystemseng.2020.02.011.
   Su WH, 2020, COMPUT ELECTRON AGR, V168, DOI 10.1016/j.compag.2019.105155.
   Su WH, 2019, BIOSYST ENG, V186, P156, DOI 10.1016/j.biosystemseng.2019.07.009.
   Tang JL, 2017, COMPUT ELECTRON AGR, V135, P63, DOI 10.1016/j.compag.2017.01.001.
   Sivakumar ANV, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12132136.
   Vi Nguyen Thanh Le, 2019, Information Processing in Agriculture, V6, P116, DOI 10.1016/j.inpa.2018.08.002.
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683.
   Wang QF, 2022, COMPUT ELECTRON AGR, V199, DOI 10.1016/j.compag.2022.107194.
   Wang QF, 2021, COMPUT ELECTRON AGR, V188, DOI 10.1016/j.compag.2021.106320.
   Wang ZP, 2022, POSTHARVEST BIOL TEC, V185, DOI 10.1016/j.postharvbio.2021.111808.
   Zhang DY, 2022, COMPUT ELECTRON AGR, V198, DOI 10.1016/j.compag.2022.107086.
   Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679.
   Zou KL, 2021, COMPUT ELECTRON AGR, V187, DOI 10.1016/j.compag.2021.106242.},
Number-of-Cited-References = {40},
Times-Cited = {6},
Usage-Count-Last-180-days = {18},
Usage-Count-Since-2013 = {44},
Journal-ISO = {Agronomy-Basel},
Doc-Delivery-Number = {4V5CY},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000859495800001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000791948700001,
Author = {Liu, Keng-Hao and Yang, Meng-Hsien and Huang, Sheng-Ting and Lin, Chinsu},
Title = {Plant Species Classification Based on Hyperspectral Imaging via a
   Lightweight Convolutional Neural Network Model},
Journal = {FRONTIERS IN PLANT SCIENCE},
Year = {2022},
Volume = {13},
Month = {APR 13},
Abstract = {In recent years, many image-based approaches have been proposed to
   classify plant species. Most methods utilized red green blue (RGB)
   imaging materials and designed custom features to classify the plant
   images using machine learning algorithms. Those works primarily focused
   on analyzing single-leaf images instead of live-crown images. Without
   considering the additional features of the leaves' color and spatial
   pattern, they failed to handle cases that contained leaves similar in
   appearance due to the limited spectral information of RGB imaging. To
   tackle this dilemma, this study proposes a novel framework that combines
   hyperspectral imaging (HSI) and deep learning techniques for plant image
   classification. We built a plant image dataset containing 1,500 images
   of 30 different plant species taken by a 470-900 nm hyperspectral camera
   and designed a lightweight conventional neural network (CNN) model
   (LtCNN) to perform image classification. Several state-of-art CNN
   classifiers are chosen for comparison. The impact of using different
   band combinations as the network input is also investigated. Results
   show that using simulated RGB images achieves a kappa coefficient of
   nearly 0.90 while using the combination of 3-band RGB and 3-band
   near-infrared images can improve to 0.95. It is also found that the
   proposed LtCNN can obtain a satisfactory performance of plant
   classification (kappa = 0.95) using critical spectral features of the
   green edge (591 nm), red-edge (682 nm), and near-infrared (762 nm)
   bands. This study also demonstrates the excellent adaptability of the
   LtCNN model in recognizing leaf features of plant live-crown images
   while using a relatively smaller number of training samples than complex
   CNN models such as AlexNet, GoogLeNet, and VGGNet.},
Publisher = {FRONTIERS MEDIA SA},
Address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Lin, CS (Corresponding Author), Natl Chiayi Univ, Dept Forestry \& Nat Resources, Chiayi, Taiwan.
   Liu, Keng-Hao; Yang, Meng-Hsien; Huang, Sheng-Ting, Natl Sun Yat sen Univ, Dept Mech \& Electromech Engn, Kaohsiung, Taiwan.
   Lin, Chinsu, Natl Chiayi Univ, Dept Forestry \& Nat Resources, Chiayi, Taiwan.},
DOI = {10.3389/fpls.2022.855660},
Article-Number = {855660},
ISSN = {1664-462X},
Keywords = {plant species classification; live-crown features; leaf feature
   recognition; plant stress detection; dimensionality reduction;
   convolutional neural network; hyperspectral imaging; deep learning},
Keywords-Plus = {IMAGES; LEAVES; DYNAMICS; FRESH},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {chinsu@mail.ncyu.edu.tw},
Affiliations = {National Sun Yat Sen University; National Chiayi University},
Funding-Acknowledgement = {Ministry of Science and Technology, Taiwan {[}MOST
   109-2221-E-110-047-MY2, 109-2221-E-415-003]; Council of Agriculture of
   the Executive Yuan, Taiwan {[}110AS-8.3.2-ST-a7]},
Funding-Text = {Funding This work was supported by the Ministry of Science and
   Technology, Taiwan, in Grant No.: MOST 109-2221-E-110-047-MY2 and
   109-2221-E-415-003, and in part by the Council of Agriculture of the
   Executive Yuan, Taiwan, under Grant 110AS-8.3.2-ST-a7.},
Cited-References = {Abbas S, 2021, ISPRS J PHOTOGRAMM, V177, P204, DOI 10.1016/j.isprsjprs.2021.05.003.
   Adao T, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111110.
   {[}Anonymous], 2017, P 2017 25 TELECOMMUN, DOI {[}10.1109/ICISC.2017.8068597, DOI 10.1109/ICISC.2017.8068597].
   {[}Anonymous], 2013, HYPERSPECTRAL DATA P.
   Baiano A, 2012, COMPUT ELECTRON AGR, V87, P142, DOI 10.1016/j.compag.2012.06.002.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Bengio Y., 2017, DEEP LEARNING, V1.
   BRIOTTET X, 2006, P INT SOC OPTICS PHO, DOI DOI 10.1117/12.672030.
   Carranza-Rojas J, 2017, BMC EVOL BIOL, V17, P1, DOI 10.1186/s12862-017-1014-z.
   Chen SY, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21062077.
   Chen SY, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010096.
   Chinsu Lin, 2015, Information Processing in Agriculture, V2, P25, DOI 10.1016/j.inpa.2015.01.003.
   Fei BW, 2020, DATA HANDL SCI TECHN, V32, P523, DOI 10.1016/B978-0-444-63977-6.00021-3.
   Feng L, 2021, FRONT PLANT SCI, V12, DOI 10.3389/fpls.2021.693521.
   Gao JF, 2018, BIOSYST ENG, V170, P39, DOI 10.1016/j.biosystemseng.2018.03.006.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Harrison D, 2018, INT J APPL EARTH OBS, V66, P93, DOI 10.1016/j.jag.2017.11.009.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hopkins W.G., 2004, INTRO PLAT PHYSL.
   Hossain J., 2010, Proceedings of the 13th International Conference on Computer and Information Technology (ICCIT 2010), P458, DOI 10.1109/ICCITECHN.2010.5723901.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   HUGHES GF, 1968, IEEE T INFORM THEORY, V14, P55, DOI 10.1109/TIT.1968.1054102.
   Ioffe S., 2015, INT C MACHINE LEARNI, DOI DOI 10.1007/S13398-014-0173-7.2.
   Jaskierniak D, 2021, ISPRS J PHOTOGRAMM, V171, P171, DOI 10.1016/j.isprsjprs.2020.10.016.
   Jung A, 2015, REMOTE SENS-BASEL, V7, P11434, DOI 10.3390/rs70911434.
   Kadir A., 2013, ARXIV.
   Khmag A, 2017, IEEE ST CONF RES DEV, P467, DOI 10.1109/SCORED.2017.8305438.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Li F, 2019, INT GEOSCI REMOTE SE, P3816, DOI 10.1109/IGARSS.2019.8900363.
   Lin C, 2015, BIOGEOSCIENCES, V12, P49, DOI 10.5194/bg-12-49-2015.
   Lin CY, 2019, INT GEOSCI REMOTE SE, P1777, DOI 10.1109/IGARSS.2019.8900593.
   Lin CY, 2018, INT GEOSCI REMOTE SE, P7524, DOI 10.1109/IGARSS.2018.8517654.
   Lin CS, 2019, REMOTE SENS ENVIRON, V235, DOI 10.1016/j.rse.2019.111436.
   Lin CS, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10071062.
   Lin CS, 2018, ISPRS J PHOTOGRAMM, V142, P174, DOI 10.1016/j.isprsjprs.2018.05.022.
   Lin CS, 2017, INT GEOSCI REMOTE SE, P3870, DOI 10.1109/IGARSS.2017.8127847.
   Lin C, 2016, FOREST ECOL MANAG, V378, P111, DOI 10.1016/j.foreco.2016.07.022.
   Lin CS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0125554.
   Lin CS, 2012, PHOTOGRAMM ENG REM S, V78, P119, DOI 10.14358/PERS.78.2.119.
   Liu Z, 2007, APPL OPTICS, V46, P8328, DOI 10.1364/AO.46.008328.
   Ma J, 2015, INT J REFRIG, V50, P10, DOI 10.1016/j.ijrefrig.2014.10.024.
   Marshall S, 2015, EUR SIGNAL PR CONF, P2854, DOI 10.1109/EUSIPCO.2015.7362906.
   Mirzaei M, 2019, INT J APPL EARTH OBS, V80, P26, DOI 10.1016/j.jag.2019.04.002.
   Nasiri A, 2021, PLANTS-BASEL, V10, DOI 10.3390/plants10081628.
   Nicolai BM, 2006, POSTHARVEST BIOL TEC, V40, P1, DOI 10.1016/j.postharvbio.2005.12.006.
   Rapaport T, 2015, ISPRS J PHOTOGRAMM, V109, P88, DOI 10.1016/j.isprsjprs.2015.09.003.
   Santos F, 2019, EUR J REMOTE SENS, V52, P62, DOI 10.1080/22797254.2018.1533793.
   Schmitter P, 2017, ISPRS J PHOTOGRAMM, V131, P65, DOI 10.1016/j.isprsjprs.2017.07.003.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sinha P, 2020, ISPRS J PHOTOGRAMM, V167, P85, DOI 10.1016/j.isprsjprs.2020.06.023.
   Sun J, 2019, COMPUT ELECTRON AGR, V160, P153, DOI 10.1016/j.compag.2019.03.004.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Teena MA, 2014, J STORED PROD RES, V59, P306, DOI 10.1016/j.jspr.2014.09.005.
   Ubbens JR, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01190.
   Wang Q, 2021, IEEE T GEOSCI REMOTE, V59, P5028, DOI 10.1109/TGRS.2020.3011002.
   Wang YD, 2021, FRONT PLANT SCI, V12, DOI 10.3389/fpls.2021.645899.
   Yang GF, 2021, FRONT PLANT SCI, V12, DOI 10.3389/fpls.2021.671134.
   Zhang, 2020, MATRIX ALGEBRA APPRO, P223, DOI {[}DOI 10.1007/978-981-15-2770-8\_6, 10.1103/PhysRevD.97.056009].
   Zhang B, 2012, ENVIRON EARTH SCI, V65, P649, DOI 10.1007/s12665-011-1112-y.
   Zhang H, 2012, 2012 INTERNATIONAL CONFERENCE ON INDUSTRIAL CONTROL AND ELECTRONICS ENGINEERING (ICICEE), P2025, DOI 10.1109/ICICEE.2012.538.
   Zhu YX, 2019, NEUROCOMPUTING, V365, P191, DOI 10.1016/j.neucom.2019.07.016.},
Number-of-Cited-References = {63},
Times-Cited = {7},
Usage-Count-Last-180-days = {7},
Usage-Count-Since-2013 = {29},
Journal-ISO = {Front. Plant Sci.},
Doc-Delivery-Number = {1A7RI},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000791948700001},
OA = {gold, Green Published},
DA = {2023-08-12},
}

@inproceedings{ WOS:000330026100058,
Author = {Fan, Guang-rui and Yuan, Ye and Ma, Di},
Book-Group-Author = {DEStech},
Title = {A Fast Method to Rectify the Direction of Leaf Images},
Booktitle = {2013 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ARTIFICIAL
   INTELLIGENCE (ICCSAI 2013)},
Year = {2013},
Pages = {265-269},
Note = {International Conference on Computer Science and Artificial Intelligence
   (ICCSAI), Chengdu, PEOPLES R CHINA, NOV 16-17, 2013},
Abstract = {Living plant recognition mainly depends on the identification of leaf
   images. However, a considerable amount of plant recognition systems need
   manual operations to rectify the leaf image directions, which make the
   system couldn't fully automated. In this article, a new algorithm is
   proposed to rectify the directions of plant images and some basic
   concepts in Morphological Image Processing are involved in this
   algorithm. After applying the algorithm to the leaf image, the primary
   vein of the leaf will be placed in vertical direction. This allows the
   recognition of plant species more automatic and more effective.},
Publisher = {DESTECH PUBLICATIONS, INC},
Address = {439 DUKE STREET, LANCASTER, PA 17602-4967 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Fan, GR (Corresponding Author), Huazhong Univ Sci \& Technol, Sch Software Engn, Wuhan 430074, Peoples R China.
   Fan, Guang-rui; Yuan, Ye; Ma, Di, Huazhong Univ Sci \& Technol, Sch Software Engn, Wuhan 430074, Peoples R China.},
ISBN = {978-1-60595-132-4},
Keywords = {Plant leaf image rectification; Image processing; Automation},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {fgr.hust@gmail.com
   yuanyegraphics@163.com
   mddongman@163.com},
Affiliations = {Huazhong University of Science \& Technology},
Cited-References = {Cerutti1 Guillaume, INT C COMP VIS THEOR.
   Chaki J., 2011, INT J ADV COMPUTER S, V2.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Fu H, 2006, IEE P-VIS IMAGE SIGN, V153, P881, DOI 10.1049/ip-vis:20060061.
   Kirchgessner N., 2002, 2 IASTED INT C VIS I.
   Lee CL, 2006, INT J IMAG SYST TECH, V16, P15, DOI 10.1002/ima.20063.
   Mokhtarian F, 2004, IEEE T IMAGE PROCESS, V13, P653, DOI 10.1109/TIP.2004.826126.
   Xu Xiaohong, 2011 4 INT C INT COM.},
Number-of-Cited-References = {9},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BJS65},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000330026100058},
DA = {2023-08-12},
}

@article{ WOS:000371309600017,
Author = {Bonnet, Pierre and Joly, Alexis and Goeau, Herve and Champ, Julien and
   Vignau, Christel and Molino, Jean-Francois and Barthelemy, Daniel and
   Boujemaa, Nozha},
Title = {Plant identification: man vs. machine LifeCLEF 2014 plant identification
   challenge},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2016},
Volume = {75},
Number = {3},
Pages = {1647-1665},
Month = {FEB},
Abstract = {This paper reports a large-scale experiment aimed at evaluating how
   state-of-art computer vision systems perform in identifying plants
   compared to human expertise. A subset of the evaluation dataset used
   within LifeCLEF 2014 plant identification challenge was therefore shared
   with volunteers of diverse expertise, ranging from the leading experts
   of the targeted flora to inexperienced test subjects. In total, 16 human
   runs were collected and evaluated comparatively to the 27 machine-based
   runs of LifeCLEF challenge. One of the main outcomes of the experiment
   is that machines are still far from outperforming the best expert
   botanists at the image-based plant identification competition. On the
   other side, the best machine runs are competing with experienced
   botanists and clearly outperform beginners and inexperienced test
   subjects. This shows that the performances of automated plant
   identification systems are very promising and may open the door to a new
   generation of ecological surveillance systems.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Goeau, H (Corresponding Author), Inria ZENITH Team, Montpellier, France.
   Bonnet, Pierre, CIRAD, UMR AMAP, Montpellier, France.
   Joly, Alexis; Champ, Julien, LIRMM, Montpellier, France.
   Joly, Alexis; Goeau, Herve, Inria ZENITH Team, Montpellier, France.
   Champ, Julien, INRA UMR AGAP, Montpellier, France.
   Vignau, Christel, Tela Bot, Montpellier, France.
   Molino, Jean-Francois, IRD, Montpellier, France.
   Barthelemy, Daniel, CIRAD, BIOS Direct, F-34398 Montpellier, France.
   Barthelemy, Daniel, INRA, UMR AMAP, F-34398 Montpellier, France.
   Boujemaa, Nozha, INRIA, Direct Saclay Ctr, Paris, France.},
DOI = {10.1007/s11042-015-2607-4},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Visual plant identification; Human evaluation; Digital data; Image
   analysis},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {pierre.bonnet@cirad.fr
   alexis.joly@inria.fr
   herve.goeau@inria.fr
   julien.champ@lirmm.fr
   christel@tela-botanica.org
   jean-francois.molino@ird.fr
   daniel.barthelemy@cirad.fr
   nozha.boujemaa@inria.fr},
Affiliations = {CIRAD; Centre National de la Recherche Scientifique (CNRS); Institut de
   Recherche pour le Developpement (IRD); Universite de Montpellier; Centre
   National de la Recherche Scientifique (CNRS); Universite Paul-Valery;
   Universite Perpignan Via Domitia; Universite de Montpellier; INRAE;
   Institut de Recherche pour le Developpement (IRD); CIRAD; CIRAD; Centre
   National de la Recherche Scientifique (CNRS); Institut de Recherche pour
   le Developpement (IRD); Universite de Montpellier; INRAE; Inria},
ResearcherID-Numbers = {joly, alexis/AAV-3101-2021
   Bonnet, Pierre/AAG-6819-2020
   Molino, Jean-François/C-5011-2009
   Champ, Julien/AAG-1092-2020
   },
ORCID-Numbers = {joly, alexis/0000-0002-2161-9940
   Bonnet, Pierre/0000-0002-2828-4389
   Molino, Jean-François/0000-0001-8853-7133
   Champ, Julien/0000-0002-2042-0411
   barthelemy, Daniel/0000-0003-3187-2517
   Goeau, Herve/0000-0003-3296-3795},
Funding-Acknowledgement = {Agropolis foundation},
Funding-Text = {Part of this work was funded by the Agropolis foundation through the
   project Pl@ntNet (http://www.plantnet-project.org/).},
Cited-References = {Angelova A, 2012, DEV DEPLOYMENT LARGE.
   {[}Anonymous], 1999, TREC.
   Backes AR, 2009, INT J PATTERN RECOGN, V23, P1145, DOI 10.1142/S0218001409007508.
   Cai J, 2007, 3 INT C INT SENS ISS.
   Cerutti Guillaume, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P202.
   Cerutti G, 2011, INT S VIS COMP.
   Chen Q, 2014, CLEF 2014 C.
   Dimitrovski I, 2014, CLEF 2014 C.
   Fakhfakh S, 2014, CLEF 2014 C.
   Farnsworth EJ, 2013, BIOSCIENCE, V63, P891, DOI 10.1525/bio.2013.63.11.8.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Goeau H, 2013, P 21 ACM INT C MULT.
   Goeau H, 2011, ACM C MULT.
   Goeau H., 2012, CLEF WORKING NOTES.
   Goeau H., 2011, CLEF WORKING NOTES.
   Goeau H, 2014, CLEF WORKING NOTES.
   Goeau H, 2014, P INT C MULT RETR, P2014.
   Goeau H, 2014, CLEF 2014 C.
   Hsu TH, 2011, MULTIMED TOOLS APPL, V53, P53, DOI 10.1007/s11042-010-0490-6.
   Issolah M, 2014, CLEF 2014 C.
   Joly A, 2013, ECOLOGICAL INFORM.
   Joly A, 2013, INT WORKSH MULT AN E, P2013.
   Joly A, 2014, P CLEF.
   Karamti H, 2014, CLEF 2014 C.
   Kebapci H, 2010, COMPUTER J.
   Kumar N., 2012, EUR C COMP VIS.
   Mouine S., 2012, P 2 ACM INT C MULT R, P1.
   Nilsback M-E, 2008, IND C COMP VIS GRAPH.
   Paczolay D, 2014, CLEF 2014 C.
   Spampinato, 2012, MAED 12 P 1 ACM INT.
   Sunderhauf N, 2014, CLEF 2014 C.
   Szucs G, 2014, CLEF 2014 C.
   Trifa VM, 2008, J ACOUST SOC AM, V123, P2424, DOI 10.1121/1.2839017.
   Yanikoglu B, 2014, CLEF 2014 C.},
Number-of-Cited-References = {34},
Times-Cited = {16},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {10},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {DF4HW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000371309600017},
OA = {Green Submitted},
DA = {2023-08-12},
}

@article{ WOS:000461189300005,
Author = {Gyires-Toth, Balint Pal and Osvath, Marton and Papp, David and Szucs,
   Gabor},
Title = {Deep Learning for Plant Classification and Content-Based Image Retrieval},
Journal = {CYBERNETICS AND INFORMATION TECHNOLOGIES},
Year = {2019},
Volume = {19},
Number = {1},
Pages = {88-100},
Abstract = {The main goal of the present research is to classify images of plants to
   species with deep learning. We used convolutional neural network
   architectures for feature learning and fully connected layers with
   logsoftmax output for classification. Pretrained models on ImageNet were
   used, and transfer learning was applied. In the current research image
   sets published in the scope of the PlantCLEF 2015 challenge were used.
   The proposed system surpasses the results of all top competitors of the
   challenge by 8\% and 7\% at observation and image levels, respectively.
   Our secondary goal was to satisfy the users' needs in content-based
   image retrieval to give relevant hits during species search task. We
   optimized the length of the returned lists in order to maximize MAP
   (Mean Average Precision), which is critical to the performance of image
   retrieval. Thus, we achieved more than 50\% improvement of MAP in the
   test set compared to the baseline.},
Publisher = {INST INFORMATION \& COMMUNICATION TECHNOLOGIES-BULGARIAN ACAD SCIENCES},
Address = {2, ACAD G BONCHEV, SOFIA, 1113, BULGARIA},
Type = {Article},
Language = {English},
Affiliation = {Gyires-Toth, BP (Corresponding Author), Budapest Univ Technol \& Econ, Dept Telecommun \& Media Informat, 2nd Magyar Tudosok Krt, H-1117 Budapest, Hungary.
   Gyires-Toth, Balint Pal; Osvath, Marton; Papp, David; Szucs, Gabor, Budapest Univ Technol \& Econ, Dept Telecommun \& Media Informat, 2nd Magyar Tudosok Krt, H-1117 Budapest, Hungary.},
DOI = {10.2478/cait-2019-0005},
ISSN = {1311-9702},
EISSN = {1314-4081},
Keywords = {deep learning; convolutional neural networks; Inception V3; MAP; image
   retrieval},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems},
Author-Email = {toth.b@tmit.bme.hu
   osvathmarton@gmail.com
   pappd@tmit.bme.hu
   szucs@tmit.bme.hu},
Affiliations = {Budapest University of Technology \& Economics},
Funding-Acknowledgement = {European Union; European Social Fund {[}EFOP-3.6.2-16-2017-00013];
   BME-Artificial Intelligence FIKP grant of Ministry of Human Resources
   (BME FIKP-MI/SC); Ministry of Human Resources {[}UNKP-18-4-BME-394];
   Janos Bolyai Research Scholarship of the Hungarian Academy of Sciences;
   NVIDIA Corporation},
Funding-Text = {The research presented in this paper has been supported by the European
   Union, co-financed by the European Social Fund
   (EFOP-3.6.2-16-2017-00013), by the BME-Artificial Intelligence FIKP
   grant of Ministry of Human Resources (BME FIKP-MI/SC), by Doctoral
   Research Scholarship of Ministry of Human Resources (UNKP-18-4-BME-394)
   in the scope of New National Excellence Program, by Janos Bolyai
   Research Scholarship of the Hungarian Academy of Sciences. We gratefully
   acknowledge the support of NVIDIA Corporation with the donation of the
   Titan Xp GPU used for this research.},
Cited-References = {Bonnet P, 2016, MULTIMED TOOLS APPL, V75, P1647, DOI 10.1007/s11042-015-2607-4.
   Champ Julien, 2015, CLEF 2015 C.
   Chen Q, 2014, CLEF WORKING NOTES, P693.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Duchi J, 2011, J MACH LEARN RES, V12, P2121.
   Ge ZongYuan, 2015, CLEF 2015 C.
   Goeau H., 2015, CLEF WORKING NOTES.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123.
   Ioffe S., 2015, INT C MACHINE LEARNI, DOI DOI 10.1007/S13398-014-0173-7.2.
   Kadir A., 2011, INT J COMPUTER TREND, V1, P306.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008.
   Le Thi-Lan, 2015, CLEF 2015 C.
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8\_2.
   LeCun Y., 1989, ADV NEURAL INFORM PR, V2.
   Long XZ, 2016, MULTIMED TOOLS APPL, V75, P5533, DOI 10.1007/s11042-015-2524-6.
   Maas, 2013, RECTIFIER NONLINEARI, DOI DOI 10.1016/0010-0277(84)90022-2.
   Nair V., 2010, ICML, P8, DOI DOI 10.5555/3104322.3104425.
   Reyes A.K., 2015, CLEF 2015 C.
   Robertson S, 2008, P 31 ANN INT ACM SIG, P689, DOI DOI 10.1145/1390334.1390453.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Sermanet P, 2014, ICLR.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Sunderhauf Niko, 2014, P CLEF WORK NOT, P756.
   Sungbin C, 2015, CLEF 2015 C.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Szucs G., 2014, CLEF 2014 C SHEFF GR, P763.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zeiler Matthew, 2012, ARXIV12125701.},
Number-of-Cited-References = {32},
Times-Cited = {9},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Cybern. Inf. Technol.},
Doc-Delivery-Number = {HO8HA},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000461189300005},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000500781500002,
Author = {Wu, Danzi and Han, Xue and Wang, Guan and Sun, Yu and Zhang, Haiyan and
   Fu, Hongping},
Title = {Deep Learning with Taxonomic Loss for Plant Identification},
Journal = {COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE},
Year = {2019},
Volume = {2019},
Month = {NOV 21},
Abstract = {Plant identification is a fine-grained classification task which aims to
   identify the family, genus, and species according to plant appearance
   features. Inspired by the hierarchical structure of taxonomic tree, the
   taxonomic loss was proposed, which could encode the hierarchical
   relationships among multilevel labels into the deep learning objective
   function by simple group and sum operation. By training various neural
   networks on PlantCLEF 2015 and PlantCLEF 2017 datasets, the experimental
   results demonstrated that the proposed loss function was easy to
   implement and outperformed the most commonly adopted cross-entropy loss.
   Eight neural networks were trained, respectively, by two different loss
   functions on PlantCLEF 2015 dataset, and the models trained by taxonomic
   loss led to significant performance improvements. On PlantCLEF 2017
   dataset with 10,000 species, the SENet-154 model trained by taxonomic
   loss achieved the accuracies of 84.07\%, 79.97\%, and 73.61\% at family,
   genus and species levels, which improved those of model trained by
   cross-entropy loss by 2.23\%, 1.34\%, and 1.08\%, respectively. The
   taxonomic loss could further facilitate the fine-grained classification
   task with hierarchical labels.},
Publisher = {HINDAWI LTD},
Address = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Sun, Y; Zhang, HY (Corresponding Author), Beijing Forestry Univ, Sch Informat Sci \& Technol, Beijing 100083, Peoples R China.
   Sun, Y (Corresponding Author), Beihang Univ, Sch Cyber Sci \& Technol, Beijing 100191, Peoples R China.
   Wu, Danzi, Beijing Forestry Univ, Sch Landscape Architecture, Beijing 100083, Peoples R China.
   Han, Xue; Wang, Guan; Sun, Yu; Zhang, Haiyan; Fu, Hongping, Beijing Forestry Univ, Sch Informat Sci \& Technol, Beijing 100083, Peoples R China.
   Sun, Yu, Beihang Univ, Sch Cyber Sci \& Technol, Beijing 100191, Peoples R China.},
DOI = {10.1155/2019/2015017},
Article-Number = {2015017},
ISSN = {1687-5265},
EISSN = {1687-5273},
Research-Areas = {Mathematical \& Computational Biology; Neurosciences \& Neurology},
Web-of-Science-Categories  = {Mathematical \& Computational Biology; Neurosciences},
Author-Email = {sunyv@buaa.edu.cn
   zhyzml@bjfu.edu.cn},
Affiliations = {Beijing Forestry University; Beijing Forestry University; Beihang
   University},
ORCID-Numbers = {Sun, Yu/0000-0003-3206-9515},
Funding-Acknowledgement = {Fundamental Research Funds for the Central Universities {[}2019ZY38];
   Natural Science Foundation of China {[}61702038]; Special Fund for
   Beijing Common Construction Project},
Funding-Text = {This work was supported by the Fundamental Research Funds for the
   Central Universities (no. 2019ZY38), the Natural Science Foundation of
   China (no. 61702038), and the Special Fund for Beijing Common
   Construction Project.},
Cited-References = {Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Goeau H., 2017, WORKSH P C LAB EV FO.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0\_38.
   Hopkins GW, 2002, ANIM CONSERV, V5, P245, DOI 10.1017/S1367943002002299.
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI {[}10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372].
   Huang G., 2017, P 2017 IEEE C COMP V, P4700, DOI {[}DOI 10.1109/CVPR.2017.243, 10.1109/CVPR.2017.243].
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Judd W. S., 2002, PLANT SYSTEMATICS.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Ma N., 2018, ARXIV180711164, DOI DOI 10.1007/978-3-030-01264-9\_8.
   Mora C, 2011, PLOS BIOL, V9, DOI 10.1371/journal.pbio.1000606.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   SUN Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI DOI 10.1155/2017/7361042.
   Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1609/AAAI.V31I1.11231.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Ubbens JR, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01190.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Zhang SW, 2017, CLUSTER COMPUT, V20, P1517, DOI 10.1007/s10586-017-0859-7.
   Zhu HY, 2018, MULTIMED TOOLS APPL, V77, P29779, DOI 10.1007/s11042-017-5578-9.},
Number-of-Cited-References = {27},
Times-Cited = {5},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {20},
Journal-ISO = {Comput. Intell. Neurosci.},
Doc-Delivery-Number = {JT1TY},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000500781500002},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000622728100001,
Author = {Hwang, Sung-Wook and Sugiyama, Junji},
Title = {Evaluation of image partitioning strategies for preserving spatial
   information of cross-sectional micrographs in automated wood recognition
   of Fagaceae},
Journal = {JOURNAL OF WOOD SCIENCE},
Year = {2021},
Volume = {67},
Number = {1},
Month = {FEB 28},
Abstract = {Although wood cross sections contain spatiotemporal information
   regarding tree growth, computer vision-based wood identification studies
   have traditionally favored disordered image representations that do not
   take such information into account. This paper describes image
   partitioning strategies that preserve the spatial information of wood
   cross-sectional images. Three partitioning strategies are designed,
   namely grid partitioning based on spatial pyramid matching and its
   variants, radial and tangential partitioning, and their recognition
   performance is evaluated for the Fagaceae micrograph dataset. The grid
   and radial partitioning strategies achieve better recognition
   performance than the bag-of-features model that constitutes their
   underlying framework. Radial partitioning, which is a strategy for
   preserving spatial information from pith to bark, further improves the
   performance, especially for radial-porous species. The Pearson
   correlation and autocorrelation coefficients produced from radially
   partitioned sub-images have the potential to be used as auxiliaries in
   the construction of multi-feature datasets. The contribution of image
   partitioning strategies is found to be limited to species recognition
   and is unremarkable at the genus level.},
Publisher = {SPRINGER JAPAN KK},
Address = {SHIROYAMA TRUST TOWER 5F, 4-3-1 TORANOMON, MINATO-KU, TOKYO, 105-6005,
   JAPAN},
Type = {Article},
Language = {English},
Affiliation = {Sugiyama, J (Corresponding Author), Kyoto Univ, Grad Sch Agr, Kyoto 6068502, Japan.
   Hwang, Sung-Wook; Sugiyama, Junji, Kyoto Univ, Grad Sch Agr, Kyoto 6068502, Japan.
   Sugiyama, Junji, Nanjing Forestry Univ, Coll Mat Sci \& Engn, Nanjing 210037, Peoples R China.},
DOI = {10.1186/s10086-021-01953-z},
Article-Number = {18},
ISSN = {1435-0211},
EISSN = {1611-4663},
Keywords = {Computer vision; Image recognition; Spatial pyramid matching; Wood
   identification},
Research-Areas = {Forestry; Materials Science},
Web-of-Science-Categories  = {Forestry; Materials Science, Paper \& Wood},
Author-Email = {sugiyama.junji.6m@kyoto-u.ac.jp},
Affiliations = {Kyoto University; Nanjing Forestry University},
ORCID-Numbers = {Sugiyama, Junji/0000-0002-5388-4925
   Hwang, Sung-Wook/0000-0001-8265-8700},
Funding-Acknowledgement = {Japan Society for the Promotion of Science {[}H1805485]},
Funding-Text = {This study was supported by a Grant-in-Aid for Scientific Research
   (Grant Number H1805485) from the Japan Society for the Promotion of
   Science.},
Cited-References = {{[}Anonymous], 2014, FOR SPEC DAT MACR.
   {[}Anonymous], 2020, XYL DIG DAT WOOD INF.
   {[}Anonymous], 2013, FOR SPEC DAT MICR.
   Barmpoutis P., 2018, WOOD AUTH DATASET, DOI {[}10.2018/wood.auth, DOI 10.2018/WOOD.AUTH].
   Barmpoutis P, 2018, COMPUT ELECTRON AGR, V144, P241, DOI 10.1016/j.compag.2017.12.011.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678.
   Figueroa-Mata G, 2018, INT WORK C BIOINSPIR, P1, DOI DOI 10.1109/IWOBI.2018.8464206.
   Forestry and forest products research institute, MICROSCOPIC IDENTIFI.
   Gasim, 2013, INT J ADV COMPUT SC, V4, P48.
   Hafemann LG, 2014, INT C PATT RECOG, P1103, DOI 10.1109/ICPR.2014.199.
   He JF, 2008, PROC CVPR IEEE, P2293.
   Hwang SW, 2020, J WOOD SCI, V66, DOI 10.1186/s10086-020-01864-5.
   Kanai K, 2020, XYLARIUM DIGITAL DAT.
   Kobayashi K, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0220762.
   Lazebnik S., 2006, P IEEE COMP SOC C CO.
   Martins J, 2013, MACH VISION APPL, V24, P567, DOI 10.1007/s00138-012-0417-5.
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188.
   Nadaraj M, 2008, INT J SIMUL SYST SCI, V9, P9.
   Noshiro S, 2011, IAWA J, V32, P383, DOI 10.1163/22941932-90000066.
   Sculley D., 2010, P 19 INT C WORLD WID, P1177, DOI {[}10.1145/1772690.1772862, DOI 10.1145/1772690.1772862].
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sugiyama J, 2019, IOP C SERIES EARTH E, V415.
   Tou J.Y., 2007, P INT WORKSH ADV IM.
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757.},
Number-of-Cited-References = {25},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {27},
Journal-ISO = {J. Wood Sci.},
Doc-Delivery-Number = {QN8UW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000622728100001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000566993400001,
Author = {Pawara, Pornntiwa and Okafor, Emmanuel and Groefsema, Marc and He, Sheng
   and Schomaker, Lambert R. B. and Wiering, Marco A.},
Title = {One-vs-One classification for deep neural networks},
Journal = {PATTERN RECOGNITION},
Year = {2020},
Volume = {108},
Month = {DEC},
Abstract = {For performing multi-class classification, deep neural networks almost
   always employ a One-vs-All (OvA) classification scheme with as many
   output units as there are classes in a dataset. The problem of this
   approach is that each output unit requires a complex decision boundary
   to separate examples from one class from all other examples. In this
   paper, we propose a novel One-vs-One (OvO) classification scheme for
   deep neural networks that trains each output unit to distinguish between
   a specific pair of classes. This method increases the number of output
   units compared to the One-vs-All classification scheme but makes
   learning correct decision boundaries much easier. In addition to
   changing the neural network architecture, we changed the loss function,
   created a code matrix to transform the one-hot encoding to a new label
   encoding, and changed the method for classifying examples. To analyze
   the advantages of the proposed method, we compared the One-vs-One and
   One-vs-All classification methods on three plant recognition datasets
   (including a novel dataset that we created) and a dataset with images of
   different monkey species using two deep architectures. The two deep
   convolutional neural network (CNN) architectures, Inception-V3 and
   ResNet-50, are trained from scratch or pre-trained weights. The results
   show that the One-vs-One classification method outperforms the
   One-vs-All method on all four datasets when training the CNNs from
   scratch. However, when using the two classification schemes for
   fine-tuning pre-trained CNNs, the One-vs-All method leads to the best
   performances, which is presumably because the CNNs had been pre-trained
   using the One-vs-All scheme. (C) 2020 The Authors. Published by Elsevier
   Ltd.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Pawara, P (Corresponding Author), Univ Groningen, Bernoulli Inst Math Comp Sci \& Artificial Intelli, NL-9747 AG Groningen, Netherlands.
   Pawara, Pornntiwa; Groefsema, Marc; Schomaker, Lambert R. B.; Wiering, Marco A., Univ Groningen, Bernoulli Inst Math Comp Sci \& Artificial Intelli, NL-9747 AG Groningen, Netherlands.
   Okafor, Emmanuel, Ahmadu Bello Univ, Dept Comp Engn, Zaria, Nigeria.
   He, Sheng, Harvard Med Sch, Boston Childrens Hosp, Boston, MA 02115 USA.},
DOI = {10.1016/j.patcog.2020.107528},
Article-Number = {107528},
ISSN = {0031-3203},
EISSN = {1873-5142},
Keywords = {Deep learning; Computer vision; Multi-class classification; One-vs-One
   classification; Plant recognition},
Keywords-Plus = {MULTICLASS; CLASSIFIERS; STRATEGY},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic},
Author-Email = {p.pawara@rug.nl
   m.a.wiering@rug.nl},
Affiliations = {University of Groningen; Ahmadu Bello University; Harvard University;
   Boston Children's Hospital; Harvard Medical School},
ResearcherID-Numbers = {Schomaker, Lambert/GYU-5840-2022
   Pawara, Pornntiwa/IUM-7264-2023
   Okafor, Emmanuel/AAB-5120-2019
   He, Sheng/P-2466-2016
   Schomaker, Lambert RB/A-9489-2008
   },
ORCID-Numbers = {He, Sheng/0000-0003-4159-8611
   Schomaker, Lambert RB/0000-0003-2351-930X
   Okafor, Emmanuel/0000-0001-6929-6880
   Wiering, Marco/0000-0003-4331-7537},
Funding-Acknowledgement = {Center for Information Technology of the University of Groningen},
Funding-Text = {We would like to thank the Center for Information Technology of the
   University of Groningen for their support and for providing access to
   the Peregrine high performance computing cluster.},
Cited-References = {Allwein E.L., 2001, J MACHINE LEARNING R, V1.
   Alpaydin E, 2014, ADAPT COMPUT MACH LE, P115.
   {[}Anonymous], 2005, NEURAL NETWORKS.
   Ban T, 2006, IEEE IJCNN, P327.
   Cruz AC, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01741.
   Dietterich T. G., 1995, Journal of Artificial Intelligence Research, V2, P263.
   Douarre C, 2018, J IMAGING, V4, DOI 10.3390/jimaging4050065.
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772.
   Fuentes A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17092022.
   Galar M., 2011, PATTERN RECOGN, V44.
   Galar M, 2015, PATTERN RECOGN, V48, P28, DOI 10.1016/j.patcog.2014.07.023.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Guru DS., 2010, INT J COMPUTERS APPL, V1, P21.
   He K., 2016, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2016.90.
   He S., 2018, ARXIV180808993.
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427.
   Kumar S, 2002, PATTERN ANAL APPL, V5, P210, DOI 10.1007/s100440200019.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Liu Y, 2017, INFORM SCIENCES, V394, P38, DOI 10.1016/j.ins.2017.02.016.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Ou GB, 2007, PATTERN RECOGN, V40, P4, DOI 10.1016/j.patcog.2006.04.041.
   Pawara P, 2017, LECT NOTES COMPUT SC, V10617, P615, DOI 10.1007/978-3-319-70353-4\_52.
   Pawara P, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P479, DOI 10.5220/0006196204790486.
   Rifkin R, 2004, J MACH LEARN RES, V5, P101.
   Rocha A, 2014, IEEE T NEUR NET LEAR, V25, P289, DOI 10.1109/TNNLS.2013.2274735.
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003.
   Soderkvist O., 2001, THESIS.
   Songsiri P, 2018, KNOWL-BASED SYST, V159, P9, DOI 10.1016/j.knosys.2018.05.025.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Tax D., 2001, THESIS.
   Too EC, 2019, COMPUT ELECTRON AGR, V161, P272, DOI 10.1016/j.compag.2018.03.032.
   Ubbens J, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0273-z.
   Ubbens JR, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01190.
   Vural V., P 21 INT C MACH LEAR, DOI DOI 10.1145/1015330.1015427.
   Wang X, 2014, DIGIT SIGNAL PROCESS, V34, P101, DOI 10.1016/j.dsp.2014.08.005.
   Zhang HY, 2015, AAAI CONF ARTIF INTE, P3143.
   Zhang ZL, 2017, KNOWL-BASED SYST, V125, P53, DOI 10.1016/j.knosys.2017.03.026.},
Number-of-Cited-References = {37},
Times-Cited = {34},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {24},
Journal-ISO = {Pattern Recognit.},
Doc-Delivery-Number = {NK8PT},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000566993400001},
OA = {hybrid, Green Published},
DA = {2023-08-12},
}

@article{ WOS:000424092300038,
Author = {Zhou, Tan and Popescu, Sorin C. and Lawing, A. Michelle and Eriksson,
   Marian and Strimbu, Bogdan M. and Buerkner, Paul C.},
Title = {Bayesian and Classical Machine Learning Methods: A Comparison for Tree
   Species Classification with LiDAR Waveform Signatures},
Journal = {REMOTE SENSING},
Year = {2018},
Volume = {10},
Number = {1},
Month = {JAN},
Abstract = {A plethora of information contained in full-waveform (FW) Light
   Detection and Ranging (LiDAR) data offers prospects for characterizing
   vegetation structures. This study aims to investigate the capacity of FW
   LiDAR data alone for tree species identification through the integration
   of waveform metrics with machine learning methods and Bayesian
   inference. Specifically, we first conducted automatic tree segmentation
   based on the waveform-based canopy height model (CHM) using three
   approaches including TreeVaW, watershed algorithms and the combination
   of TreeVaW and watershed (TW) algorithms. Subsequently, the Random
   forests (RF) and Conditional inference forests (CF) models were employed
   to identify important tree-level waveform metrics derived from three
   distinct sources, such as raw waveforms, composite waveforms, the
   waveform-based point cloud and the combined variables from these three
   sources. Further, we discriminated tree (gray pine, blue oak, interior
   live oak) and shrub species through the RF, CF and Bayesian multinomial
   logistic regression (BMLR) using important waveform metrics identified
   in this study. Results of the tree segmentation demonstrated that the TW
   algorithms outperformed other algorithms for delineating individual tree
   crowns. The CF model overcomes waveform metrics selection bias caused by
   the RF model which favors correlated metrics and enhances the accuracy
   of subsequent classification. We also found that composite waveforms are
   more informative than raw waveforms and waveform-based point cloud for
   characterizing tree species in our study area. Both classical machine
   learning methods (the RF and CF) and the BMLR generated satisfactory
   average overall accuracy (74\% for the RF, 77\% for the CF and 81\% for
   the BMLR) and the BMLR slightly outperformed the other two methods.
   However, these three methods suffered from low individual classification
   accuracy for the blue oak which is prone to being misclassified as the
   interior live oak due to the similar characteristics of blue oak and
   interior live oak. Uncertainty estimates from the BMLR method compensate
   for this downside by providing classification results in a probabilistic
   sense and rendering users with more confidence in interpreting and
   applying classification results to real-world tasks such as forest
   inventory. Overall, this study recommends the CF method for feature
   selection and suggests that BMLR could be a superior alternative to
   classical machining learning methods.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Zhou, T (Corresponding Author), Texas A\&M Univ, Dept Ecosyst Sci \& Management, LiDAR Applicat Study Ecosyst Remote Sensing LASER, College Stn, TX 77843 USA.
   Zhou, Tan; Popescu, Sorin C., Texas A\&M Univ, Dept Ecosyst Sci \& Management, LiDAR Applicat Study Ecosyst Remote Sensing LASER, College Stn, TX 77843 USA.
   Lawing, A. Michelle; Eriksson, Marian, Texas A\&M Univ, Dept Ecosyst Sci \& Management, College Stn, TX 77843 USA.
   Strimbu, Bogdan M., Oregon State Univ, Dept Forest Engn Resources \& Management, Corvallis, OR 97331 USA.
   Buerkner, Paul C., Univ Munster, Inst Psychol, D-48149 Munster, Germany.},
DOI = {10.3390/rs10010039},
Article-Number = {39},
EISSN = {2072-4292},
Keywords = {Bayesian multinomial logistic regression; conditional inference forests;
   Random forests; waveform signatures; tree segmentation; watershed;
   composite waveform; decomposition},
Keywords-Plus = {INDIVIDUAL TREES; ABOVEGROUND BIOMASS; RANDOM FOREST; STEM VOLUME;
   EXTRACTION; PARAMETERS},
Research-Areas = {Environmental Sciences \& Ecology; Geology; Remote Sensing; Imaging
   Science \& Photographic Technology},
Web-of-Science-Categories  = {Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science \& Photographic Technology},
Author-Email = {tankwin0@tamu.edu
   s-popescu@tamu.edu
   alawing@tamu.edu
   m-eriksson@tamu.edu
   bogdan.strimbu@oregonstate.edu
   paul.buerkner@gmail.com},
Affiliations = {Texas A\&M University System; Texas A\&M University College Station;
   Texas A\&M University System; Texas A\&M University College Station;
   Oregon State University; University of Munster},
ResearcherID-Numbers = {Popescu, Sorin C/D-5981-2015
   Zhou, Tan/U-3953-2019
   Lawing, Michelle/F-7453-2019
   Strimbu, Bogdan/A-5045-2014
   Strimbu, Bogdan/GZG-1790-2022},
ORCID-Numbers = {Popescu, Sorin C/0000-0002-8155-8801
   Zhou, Tan/0000-0002-9193-5113
   Lawing, Michelle/0000-0003-4041-6177
   Strimbu, Bogdan/0000-0002-9068-9010
   Strimbu, Bogdan/0000-0002-9068-9010},
Funding-Acknowledgement = {NSF Doctoral Dissertation Improvement Grant {[}DEB1702008]; NASA's
   ICESat-2 SDT {[}NNX15AD02G]; Office of the Vice President for Research;
   Texas AM University},
Funding-Text = {We gratefully acknowledge the support from NSF Doctoral Dissertation
   Improvement Grant (DEB1702008) and NASA's ICESat-2 SDT (Grant \#
   NNX15AD02G). We also thank the National Ecological Observatory Network
   (NEON) for providing waveform LiDAR and field data. The open access
   publishing fees for this article have been covered by the Texas A\&M
   University Open Access to Knowledge Fund (OAKFund), supported by the
   University Libraries and the Office of the Vice President for Research.
   Last but not least, we greatly appreciate the constructive comments from
   the three anonymous reviewers.},
Cited-References = {Acevedo MA, 2009, ECOL INFORM, V4, P206, DOI 10.1016/j.ecoinf.2009.06.005.
   Allouis T, 2013, IEEE J-STARS, V6, P924, DOI 10.1109/JSTARS.2012.2211863.
   {[}Anonymous], BAYESIAN DATA ANAL.
   {[}Anonymous], 2003, INT ARCH PHOTOGRAMM.
   {[}Anonymous], 2008, INT ARCH PHOTOGRAMME.
   Babcock C, 2016, REMOTE SENS ENVIRON, V182, P1, DOI 10.1016/j.rse.2016.04.014.
   Belgiu M, 2016, ISPRS J PHOTOGRAMM, V114, P24, DOI 10.1016/j.isprsjprs.2016.01.011.
   Beucher S., 1992, MORPHOLOGICAL APPROA, V34, P433, DOI 10.1201/9781482277234-12.
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324.
   Burkner PC, 2017, J STAT SOFTW, V80, P1, DOI 10.18637/jss.v080.i01.
   Cao L, 2016, INT J APPL EARTH OBS, V49, P39, DOI 10.1016/j.jag.2016.01.007.
   Das A, 2009, J SAFETY RES, V40, P317, DOI 10.1016/j.jsr.2009.05.003.
   Drake JB, 2002, REMOTE SENS ENVIRON, V79, P305, DOI 10.1016/S0034-4257(01)00281-4.
   Finley AO, 2013, INT J APPL EARTH OBS, V22, P147, DOI 10.1016/j.jag.2012.04.007.
   Freeman E.A., 2016, PICK YOUR FLAVOR RAN.
   Fruhwirth-Schnatter S, 2012, AUST J STAT, V41, P27.
   Ghosh A, 2014, INT J APPL EARTH OBS, V26, P49, DOI 10.1016/j.jag.2013.05.017.
   Heinzel J, 2011, INT J APPL EARTH OBS, V13, P152, DOI 10.1016/j.jag.2010.09.010.
   Hermosilla T, 2014, INT J WILDLAND FIRE, V23, P224, DOI 10.1071/WF13086.
   Hollaus M., 2009, P SILV, V2009, P54.
   Holmgren J, 2004, REMOTE SENS ENVIRON, V90, P415, DOI 10.1016/S0034-4257(03)00140-8.
   Isenburg M, LASTOOLS EFFICIENT T.
   Kaartinen H, 2012, REMOTE SENS-BASEL, V4, P950, DOI 10.3390/rs4040950.
   Karlson M, 2015, REMOTE SENS-BASEL, V7, P10017, DOI 10.3390/rs70810017.
   Koch B, 2006, PHOTOGRAMM ENG REM S, V72, P357, DOI 10.14358/PERS.72.4.357.
   Kruschke J., 2014, DOING BAYESIAN DATA.
   Lefsky MA, 2005, GEOPHYS RES LETT, V32, DOI 10.1029/2005GL023971.
   Li WK, 2012, PHOTOGRAMM ENG REM S, V78, P75, DOI 10.14358/PERS.78.1.75.
   Patenaude G, 2008, INT J REMOTE SENS, V29, P1295, DOI 10.1080/01431160701736414.
   Plowright A., 2017, FORESTTOOLS ANAL REM.
   Popescu SC, 2003, CAN J REMOTE SENS, V29, P564, DOI 10.5589/m03-027.
   Popescu SC, 2002, COMPUT ELECTRON AGR, V37, P71, DOI 10.1016/S0168-1699(02)00121-7.
   Popescu SC, 2004, PHOTOGRAMM ENG REM S, V70, P589, DOI 10.14358/PERS.70.5.589.
   Chen Q, 2006, PHOTOGRAMM ENG REM S, V72, P923, DOI 10.14358/PERS.72.8.923.
   R Core Team, 2013, R LANG ENV STAT COMP.
   Reitberger J, 2008, INT J REMOTE SENS, V29, P1407, DOI 10.1080/01431160701736448.
   Schlerf M, 2005, REMOTE SENS ENVIRON, V95, P177, DOI 10.1016/j.rse.2004.12.016.
   Strobl C., 2009, TECHNICAL REPORT.
   Strobl C, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-25.
   Treitz P, 2000, REMOTE SENS ENVIRON, V72, P268, DOI 10.1016/S0034-4257(99)00098-X.
   Vaughn NR, 2012, REMOTE SENS-BASEL, V4, P377, DOI 10.3390/rs4020377.
   Vauhkonen J, 2012, FORESTRY, V85, P27, DOI 10.1093/forestry/cpr051.
   Yao W, 2012, REMOTE SENS ENVIRON, V123, P368, DOI 10.1016/j.rse.2012.03.027.
   Yu XW, 2014, FORESTS, V5, P1011, DOI 10.3390/f5051011.
   Zhao KG, 2011, REMOTE SENS ENVIRON, V115, P1978, DOI 10.1016/j.rse.2011.04.001.
   Zhou T, 2017, REMOTE SENS ENVIRON, V200, P43, DOI 10.1016/j.rse.2017.08.012.
   Zhou T, 2017, ISPRS J PHOTOGRAMM, V129, P131, DOI 10.1016/j.isprsjprs.2017.04.021.},
Number-of-Cited-References = {47},
Times-Cited = {23},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {30},
Journal-ISO = {Remote Sens.},
Doc-Delivery-Number = {FU8GW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000424092300038},
OA = {Green Submitted, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000310817100013,
Author = {Bhattacharyya, Debnath and Kim, Tai-hoon and Lee, Gang-soo},
Editor = {Kim, TH and Adeli, H and Ramos, C and Kang, BH},
Title = {Leaf Image Analysis towards Plant Identification},
Booktitle = {SIGNAL PROCESSING, IMAGE PROCESSING AND PATTERN RECOGNITION},
Series = {Communications in Computer and Information Science},
Year = {2011},
Volume = {260},
Pages = {113+},
Note = {International Conference on Signal Processing, Image Processing and
   Pattern Recognition (SIP 2011), Jeju Island, SOUTH KOREA, DEC 08-10,
   2011-2012},
Abstract = {Plants can be classified and identified by naturally or artificially as
   per the botanists. Plants can be identified by their leaves also. There
   are different varieties of plants grown throughout the world. Their
   identifications are studied using various laboratory methods. The
   morphological and genetically characteristics are employed to classify
   different leafs as well as plants. However, the presence of wide
   morphological varieties through evolution among the various leaf
   cultivars made it more complex and difficult to classify them. Leaf
   structures play a very crucial role in determining the characteristics
   of a plant. The broad and narrow shaped leaves, leaf arrangement, leaf
   margin characteristics features which differentiate various leaf of a
   plant. In this paper, we propose the methods to identify the leaf using
   an image analysis based approach.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kim, TH (Corresponding Author), Univ Tasmania, GVSA, Hobart, Tas 7001, Australia.
   Kim, Tai-hoon, Univ Tasmania, GVSA, Hobart, Tas 7001, Australia.
   Bhattacharyya, Debnath, MCKV Inst Engn, Dept Informat Technol, Liluah 711204, Howrah, India.
   Lee, Gang-soo, Hannam Univ, Dept Comp Engn, Daejeon, South Korea.},
ISSN = {1865-0929},
EISSN = {1865-0937},
ISBN = {978-3-642-27182-3},
Keywords = {image processing; pattern recognition; image segmentation;
   classification},
Keywords-Plus = {COLOR},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic},
Author-Email = {debnathb@gmail.com
   taihoonk@utas.edu.au
   gslee@hnu.kr},
Affiliations = {University of Tasmania; Hannam University},
ResearcherID-Numbers = {Bhattacharyya, Debnath/A-3144-2016},
ORCID-Numbers = {Bhattacharyya, Debnath/0000-0003-0140-9644},
Funding-Acknowledgement = {Security Engineering Research Center; Korea Ministry of Knowledge
   Economy},
Funding-Text = {This work was supported by the Security Engineering Research Center,
   granted by the Korea Ministry of Knowledge Economy.},
Cited-References = {Damian M., POLLEN CLASSIFICATIO.
   NEUMAN MR, 1989, J CEREAL SCI, V10, P175, DOI 10.1016/S0733-5210(89)80046-3.
   Polder G., 2000, ASAE INT M.
   Sabino DMU, 2004, REAL-TIME IMAGING, V10, P205, DOI 10.1016/j.rti.2004.02.007.
   Sanyal P., 2008, IEEE INT C AMS 2008.
   Sanyal P., 2006, 20 CSI C P, P45.
   SLAUGHTER DC, 1989, T ASAE, V32, P757.
   Soille P, 2000, IMAGE VISION COMPUT, V18, P1025, DOI 10.1016/S0262-8856(00)00043-3.
   Stojanovic R, 2001, REAL-TIME IMAGING, V7, P507, DOI 10.1006/rtim.2001.0231.
   Tian II L., MACHIEN VISION IDENT.
   Tzionas P., 2005, 5 INT C TECHN AUT TH, P365.
   Zhao-yan L., 2005, J ZHEJIANG U SCI, P1095.},
Number-of-Cited-References = {12},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BCO30},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000310817100013},
DA = {2023-08-12},
}

@article{ WOS:000743794900025,
Author = {Heidary-Sharifabad, Ahmad and Zarchi, Mohsen Sardari and Emadi, Sima and
   Zarei, Gholamreza},
Title = {ACHENY: A standard Chenopodiaceae image dataset for deep learning models},
Journal = {DATA IN BRIEF},
Year = {2021},
Volume = {39},
Month = {DEC},
Abstract = {This paper contains datasets related to the ``Efficient Deep Learning
   Models for Categorizing Chenopodiaceae in the wild{''}
   (Heidary-Sharifabad et al., 2021). There are about 1500 species of
   Chenopodiaceae that are spread worldwide and often are ecologically
   important. Biodiversity conservation of these species is critical due to
   the destructive effects of human activities on them. For this purpose,
   identification and surveillance of Chenopodiaceae species in their
   natural habitat are necessary and can be facilitated by deep learning.
   The feasibility of applying deep learning algorithms to identify
   Chenopodiaceae species depends on access to the appropriate relevant
   dataset.
   Therefore, ACHENY dataset was collected from natural habitats of
   different bushes of Chenopodiaceae species, in realworld conditions from
   desert and semi-desert areas of the Yazd province of IRAN. This
   imbalanced dataset is compiled of 27,030 RGB color images from 30
   Chenopodiaceae species, each species 300-1461 images. Imaging is
   performed from multiple bushes for each species, with different
   camera-totarget distances, viewpoints, angles, and natural sunlight in
   November and December. The collected images are not preprocessed, only
   are resized to 224 x224 dimensions which can be used on some of the
   successful deep learning models and then were grouped into their
   respective class. The images in each class are separated by 10\% for
   testing, 18\% for validation, and 72\% for training. Test images are
   often manually selected from plant bushes different from the training
   set. Then training and validation images are randomly separated from the
   remaining images in each category. The smallsized images with 64 x 64
   dimensions also are included in ACHENY which can be used on some other
   deep models. (C) 2021 The Authors. Published by Elsevier Inc.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Heidary-Sharifabad, A (Corresponding Author), Islamic Azad Univ, Dept Comp Engn, Maybod Branch, Maybod, Iran.
   Heidary-Sharifabad, Ahmad, Islamic Azad Univ, Dept Comp Engn, Maybod Branch, Maybod, Iran.
   Zarchi, Mohsen Sardari, Meybod Univ, Dept Comp Engn, Meybod, Iran.
   Emadi, Sima, Islamic Azad Univ, Dept Comp Engn, Yazd Branch, Yazd, Iran.
   Zarei, Gholamreza, Islamic Azad Univ, Dept Agron, Maybod Branch, Maybod, Iran.},
DOI = {10.1016/j.dib.2021.107478},
EarlyAccessDate = {OCT 2021},
Article-Number = {107478},
ISSN = {2352-3409},
Keywords = {Biodiversity protection; Chenopodiaceae; Deep learning; Image
   classification; Plant classification},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {ahmad.heidary@maybodiau.ac.ir
   sardari@meybod.ac.ir
   emadi@iauyazd.ac.ir
   zareigholamreza@maybodiau.ac.ir},
Affiliations = {Islamic Azad University; Islamic Azad University; Islamic Azad
   University},
ORCID-Numbers = {, Ahmad/0000-0002-3356-4334
   Sardari Zarchi, Mohsen/0000-0003-0831-3426},
Cited-References = {Assadi M, 2001, FLORA IRAN NO 38 CHE.
   Heidary-Sharifabad A, 2021, INT J PATTERN RECOGN, V35, DOI 10.1142/S0218001421520157.
   Heidary-Sharifabad A, 2021, BRIT FOOD J, V123, P3592, DOI 10.1108/BFJ-12-2020-1100.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Tan MX, 2019, PR MACH LEARN RES, V97.},
Number-of-Cited-References = {8},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Journal-ISO = {Data Brief},
Doc-Delivery-Number = {YI4BN},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000743794900025},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000547349000019,
Author = {Imamoglu, Nevrez and Martinez-Gomez, Pascual and Hamaguchi, Ryuhei and
   Sakurada, Ken and Nakamura, Ryosuke},
Editor = {Dagli, CH},
Title = {Exploring Recurrent and Feedback CNNs for Multi-Spectral Satellite Image
   Classification},
Booktitle = {CYBER PHYSICAL SYSTEMS AND DEEP LEARNING},
Series = {Procedia Computer Science},
Year = {2018},
Volume = {140},
Pages = {162-169},
Note = {Complex Adaptive Systems Conference (CAS) - Cyber Physical Systems and
   Deep Learning, Chicago, IL, NOV 05-07, 2018},
Abstract = {The emergence of deep learning applications such as convolutional neural
   networks (CNNs) have resulted in huge improvements on computer vision
   applications in a wide variety of fields. However, several works
   demonstrated that low-quality or noisy data (even including perceptually
   not visible noises) may have a huge impact on the accuracy of CNN
   models. Therefore, as inspired by biological perception systems, some
   recent works proposed the use of recurrent and feedback features in CNNs
   as an improvement to the existing feed-forward CNNs. These recent works
   on the integration of recurrence and/or feedback to CNNs mostly tested
   deep networks on natural scenes with relatively perceptually good
   resolution color images. In this work, we explored the effectiveness of
   CNNs with recurrent and feedback features for the solar-power plant
   classification task on mid-resolution (1 pixel - 30x30 square meters per
   pixel) multi-spectral satellite images. Experiments show promising
   results when using top-down signals (especially recurrent and feedback
   features together) on CNNs for multi-spectral image classification
   tasks, outperforming the baseline CNN model without any recurrent and
   feedback structure and other approaches in the literature including deep
   models. (C) 2018 The Authors. Published by Elsevier B.V.},
Publisher = {ELSEVIER SCIENCE BV},
Address = {SARA BURGERHARTSTRAAT 25, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Imamoglu, N (Corresponding Author), Natl Inst Adv Ind Sci \& Technol, Artificial Intelligence Res Ctr, Tokyo, Japan.
   Imamoglu, Nevrez; Martinez-Gomez, Pascual; Hamaguchi, Ryuhei; Sakurada, Ken; Nakamura, Ryosuke, Natl Inst Adv Ind Sci \& Technol, Artificial Intelligence Res Ctr, Tokyo, Japan.},
DOI = {10.1016/j.procs.2018.10.325},
ISSN = {1877-0509},
Keywords = {Convolutional Neural Network; CNNs with Recurrent and Feedback;
   Classification; Remote Sensing; Multi-Spectral Images},
Keywords-Plus = {SUPER-RESOLUTION; SUPERRESOLUTION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Author-Email = {nevrez.imamoglu@aist.go.jp},
Affiliations = {National Institute of Advanced Industrial Science \& Technology (AIST)},
ResearcherID-Numbers = {İmamoğlu, Nevrez/B-1746-2017
   Sakurada, Ken/M-4818-2018
   İmamoğlu, Nevrez/AAC-4082-2019},
ORCID-Numbers = {İmamoğlu, Nevrez/0000-0002-2661-599X
   Sakurada, Ken/0000-0003-3386-1547
   İmamoğlu, Nevrez/0000-0002-2661-599X},
Cited-References = {{[}Anonymous], 2016, INT C MACH LEARN ICM.
   Fawzi Alhussein, 2017, IEEE INT C COMP VIS.
   Gao Q, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10091495.
   Guana V, 2015, 2015 IEEE/ACM 4TH INTERNATIONAL WORKSHOP ON GAMES AND SOFTWARE ENGINEERING, P15, DOI 10.1109/GAS.2015.11.
   Hao SY, 2018, IEEE T GEOSCI REMOTE, V56, P4650, DOI 10.1109/TGRS.2018.2832228.
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680.
   Kimura Motoki, 2017, PROCEEDINGS OF THE B.
   Kunchur MN, 2015, NATO SCI PEACE SECUR, P15, DOI 10.1007/978-94-017-9005-5\_2.
   Li XD, 2014, IEEE T GEOSCI REMOTE, V52, P2810, DOI 10.1109/TGRS.2013.2266345.
   Liu B, 2018, PROCEEDINGS OF 2018 TENTH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTATIONAL INTELLIGENCE (ICACI), P11, DOI 10.1109/ICACI.2018.8377596.
   Luo YM, 2017, IEEE GEOSCI REMOTE S, V14, P2398, DOI 10.1109/LGRS.2017.2766204.
   Masci Jonathan, 2014, C NEUR INF PROC SYST.
   Penatti Otavio A. B., 2015, INT C COMP VIS PATT.
   Roy DP, 2014, REMOTE SENS ENVIRON, V145, P154, DOI 10.1016/j.rse.2014.02.001.
   Simo-Serra Edgar, 2016, INT C PATT REC ICPR.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Tokui S., 2015, P ADV NEUR INF PROC, P1.
   Wu Te-Lin, 2017, IEEE INT C COMP VIS.
   Wu Xiangqian, 2016, P 2016 ACM MULT C AC.
   Yuan Y, 2017, IEEE J-STARS, V10, P1963, DOI 10.1109/JSTARS.2017.2655112.
   Zhang Amy, 2016, ARXIV161202766.
   Zhao W, 2017, IEEE T GEOSCI REMOTE, V55, P4141, DOI 10.1109/TGRS.2017.2689018.},
Number-of-Cited-References = {22},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BP3JU},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000547349000019},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000752943300001,
Author = {Rzanny, Michael and Wittich, Hans Christian and Maeder, Patrick and
   Deggelmann, Alice and Boho, David and Waeldchen, Jana},
Title = {Image-Based Automated Recognition of 31 Poaceae Species: The Most
   Relevant Perspectives},
Journal = {FRONTIERS IN PLANT SCIENCE},
Year = {2022},
Volume = {12},
Month = {JAN 26},
Abstract = {Poaceae represent one of the largest plant families in the world. Many
   species are of great economic importance as food and forage plants while
   others represent important weeds in agriculture. Although a large number
   of studies currently address the question of how plants can be best
   recognized on images, there is a lack of studies evaluating specific
   approaches for uniform species groups considered difficult to identify
   because they lack obvious visual characteristics. Poaceae represent an
   example of such a species group, especially when they are non-flowering.
   Here we present the results from an experiment to automatically identify
   Poaceae species based on images depicting six well-defined perspectives.
   One perspective shows the inflorescence while the others show vegetative
   parts of the plant such as the collar region with the ligule, adaxial
   and abaxial side of the leaf and culm nodes. For each species we
   collected 80 observations, each representing a series of six images
   taken with a smartphone camera. We extract feature representations from
   the images using five different convolutional neural networks (CNN)
   trained on objects from different domains and classify them using four
   state-of-the art classification algorithms. We combine these
   perspectives via score level fusion. In order to evaluate the potential
   of identifying non-flowering Poaceae we separately compared perspective
   combinations either comprising inflorescences or not. We find that for a
   fusion of all six perspectives, using the best combination of feature
   extraction CNN and classifier, an accuracy of 96.1\% can be achieved.
   Without the inflorescence, the overall accuracy is still as high as
   90.3\%. In all but one case the perspective conveying the most
   information about the species (excluding inflorescence) is the ligule in
   frontal view. Our results show that even species considered very
   difficult to identify can achieve high accuracies in automatic
   identification as long as images depicting suitable perspectives are
   available. We suggest that our approach could be transferred to other
   difficult-to-distinguish species groups in order to identify the most
   relevant perspectives.},
Publisher = {FRONTIERS MEDIA SA},
Address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Rzanny, M (Corresponding Author), Max Planck Inst Biogeochem, Dept Biogeochem Integrat, Jena, Germany.
   Rzanny, Michael; Deggelmann, Alice; Waeldchen, Jana, Max Planck Inst Biogeochem, Dept Biogeochem Integrat, Jena, Germany.
   Wittich, Hans Christian; Maeder, Patrick; Boho, David, Tech Univ Ilmenau, Data Intens Syst \& Visualisat, Ilmenau, Germany.
   Maeder, Patrick, Friedrich Schiller Univ, Fac Biol Sci, Jena, Germany.},
DOI = {10.3389/fpls.2021.804140},
Article-Number = {804140},
ISSN = {1664-462X},
Keywords = {deep learning; machine learning; accuracy; Poaceae; plant perspective;
   image recognition; fine-grained image classification; automated plant
   identification},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {mrzanny@bgc-jena.mpg.de},
Affiliations = {Max Planck Society; Technische Universitat Ilmenau; Friedrich Schiller
   University of Jena},
ResearcherID-Numbers = {Mäder, Patrick/A-1848-2018
   Rzanny, Michael/GOH-1028-2022},
ORCID-Numbers = {Mäder, Patrick/0000-0001-6871-2707
   Rzanny, Michael/0000-0002-7232-5547},
Cited-References = {ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209.
   {[}Anonymous], MACH LEARN.
   Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259.
   Boho D, 2020, BMC BIOINFORMATICS, V21, DOI 10.1186/s12859-020-03920-9.
   Bonnet P, 2020, ECOL SOLUT EVID, V1, DOI 10.1002/2688-8319.12023.
   Bonnet P, 2018, MULTIMED SYST APPL, P131, DOI 10.1007/978-3-319-76445-0\_8.
   Christenhusz MJM, 2016, PHYTOTAXA, V261, P201, DOI 10.11646/phytotaxa.261.3.1.
   Cope T.A., 2009, GRASSES BRIT ISLES, V13, P612.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   DENGLER J, 1998, KIELER NOTIZEN PFLAN, V25, P6.
   Diekmann M, 2019, J VEG SCI, V30, P187, DOI 10.1111/jvs.12727.
   Dressler S, 2017, PLANT SYST EVOL, V303, P1109, DOI 10.1007/s00606-017-1419-6.
   Durso AM, 2021, FRONT ARTIF INTELL, V4, DOI 10.3389/frai.2021.582110.
   Goeau H., 2016, WORKING NOTES CLEF 2, P428.
   Golzarian MR, 2011, PLANT METHODS, V7, DOI 10.1186/1746-4811-7-28.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Heidary-Sharifabad A, 2021, INT J PATTERN RECOGN, V35, DOI 10.1142/S0218001421520157.
   Hoye TT, 2021, P NATL ACAD SCI USA, V118, DOI 10.1073/pnas.2002545117.
   Jakel L., 2004, BIOL LEHREN LERN F R, V13, P1, DOI 10.4119/zdb-1672.
   Joly Alexis, 2021, Experimental IR Meets Multilinguality, Multimodality, and Interaction: 12th International Conference of the CLEF Association, CLEF 2021, Proceedings. Lecture Notes in Computer Science, Information Systems and Applications, incl. Internet/Web, and HCI (12880), P371, DOI 10.1007/978-3-030-85251-1\_24.
   Jones HG, 2020, AOB PLANTS, V12, DOI 10.1093/aobpla/plaa052.
   Krasin I., 2017, OPENIMAGES PUBLIC DA.
   Kuhn M., 2016, CARET CLASSIFICATION.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Luder R., 2011, WILDPFLANZEN ZUM GEN.
   Mader P, 2021, METHODS ECOL EVOL, V12, P1335, DOI 10.1111/2041-210X.13611.
   Mahecha MD, 2021, ECOGRAPHY, V44, P1131, DOI 10.1111/ecog.05492.
   Majka Michal, 2019, NAIVEBAYES HIGH PERF.
   Meyer D., 2020, R PACKAGE VERSION 17.
   Muller F., 2021, ROTHMALER EXKURSIONS.
   Nhan N.T.T., 2020, 2020 INT C MULT AN P, P1.
   Partel J, 2021, AOB PLANTS, V13, DOI 10.1093/aobpla/plab050.
   R Core Team, 2013, R LANG ENV STAT COMP.
   ROSKOV Y, 2019, SPECIES 2000 ITIS CA.
   Rzanny M, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0462-4.
   SCHROEDER D, 1993, WEED RES, V33, P449, DOI 10.1111/j.1365-3180.1993.tb01961.x.
   Seeland M, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0245230.
   Soreng RJ, 2017, J SYST EVOL, V55, P259, DOI 10.1111/jse.12262.
   STACE C A, 1992, Watsonia, V19, P107.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Do TB, 2017, INT CONF KNOWL SYS, P191, DOI 10.1109/KSE.2017.8119457.
   Thomas H, 2019, PLANTS PEOPLE PLANET, V1, P197, DOI 10.1002/ppp3.28.
   Veen P., 2009, GRASSLANDS EUROPE HI.
   Venables W.N., 2002, MODERN APPL STAT S.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Wright MN, 2017, J STAT SOFTW, V77, P1, DOI 10.18637/jss.v077.i01.
   Zhang CS, 2017, EXPERT SYST APPL, V82, P128, DOI 10.1016/j.eswa.2017.04.003.
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907.},
Number-of-Cited-References = {48},
Times-Cited = {4},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {19},
Journal-ISO = {Front. Plant Sci.},
Doc-Delivery-Number = {YV7YX},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000752943300001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000564659100011,
Author = {Kazerouni, Masoud Fathi and Schlemper, Jens and Kuhnert, Klaus-Dieter},
Editor = {Skala, V},
Title = {Automatic Plant Recognition System for Challenging Natural Plant Species},
Booktitle = {25. INTERNATIONAL CONFERENCE IN CENTRAL EUROPE ON COMPUTER GRAPHICS,
   VISUALIZATION AND COMPUTER VISION (WSCG 2017)},
Series = {Computer Science Research Notes},
Year = {2017},
Volume = {2702},
Pages = {81-90},
Note = {25th International Conference on Central Europe on Computer Graphics,
   Visualization and Computer Vision (WSCG), Plzen, CZECH REPUBLIC, MAY
   29-JUN 02, 2017},
Abstract = {Photosynthesis is one of turning points to shape the world. Plants use
   this process to convert light energy into chemical energy. Some of the
   early microorganisms evolved a way to use the energy from sunlight to
   make sugar out of simpler molecules, but unlike green plants today, the
   first photosynthesizing organisms did not release oxygen as waste
   product. so there was no oxygen in the air. Plants are very busy
   factories and leaves are the main place for production. A useful plant
   recognition system is capable of identification of different species in
   natural environment. In natural environment, plants and leaves grow in
   different regions and climates. During day, variation of light intensity
   can be considered as an important factor. Thus. recognition of species
   in different conditions is a real need as plants are ubiquitous in human
   life. A dataset of natural images has been utilized. The dataset
   contains four different plant species of Siegerland, Germany. Modern
   combined description algorithms SURF, FAST-SURF, and HARRIS-SURF. have
   been carried out to implement a reliable system for plants species
   recognition and classification in natural environment. One of well known
   methods in machine learning community, Support Vector Machine, has been
   applied in the implemented systems. All steps of system's implementation
   arc described in related sections. The highest obtained accuracy belongs
   to the implemented system by means of SURF algorithm and equals to
   93.9575.},
Publisher = {UNIV WEST BOHEMIA},
Address = {PILSEN UNIVERZITNI 8, PLZEN 306 14, CZECH REPUBLIC},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kazerouni, MF (Corresponding Author), Inst Real Time Learning Syst, Holderlinstr 3, D-57076 Siegen, NRW, Germany.
   Kazerouni, Masoud Fathi; Schlemper, Jens; Kuhnert, Klaus-Dieter, Inst Real Time Learning Syst, Holderlinstr 3, D-57076 Siegen, NRW, Germany.},
ISSN = {2464-4617},
ISBN = {978-80-86943-50-3},
Keywords = {Component; SURF; combination; FAST; HARRIS; FAST-SURF; HARRIS-SURF;
   feature extraction; feature detection; natural images; natural plant
   species recognition; weather condition; light intensity},
Keywords-Plus = {DESCRIPTORS},
Research-Areas = {Computer Science; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Computer Science, Software Engineering; Imaging Science \& Photographic
   Technology},
Author-Email = {masoud.fathi@uni-siegen.de
   schlemper@fb12.uni-siegen.de
   kuhnert@fb12.uni-siegen.de},
Cited-References = {{[}Anonymous], 2013, SIGNAL PROCESS PATTE.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
   Bianco S, 2015, DIGIT SIGNAL PROCESS, V44, P1, DOI 10.1016/j.dsp.2015.06.001.
   Brown M., 2002, BMVC, P656.
   Caicedo JC, 2009, LECT NOTES ARTIF INT, V5651, P126, DOI 10.1007/978-3-642-02976-9\_17.
   Drucker H, 1997, ADV NEUR IN, V9, P155.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Fathi Kazerouni M, 2015, ADV IMAGE VIDEO PROC, V3, P10.
   Gouveia F, 1997, ISIE `97 - PROCEEDINGS OF THE IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS, VOLS 1-3, P757, DOI 10.1109/ISIE.1997.648634.
   Harris C, 1988, P 4 ALV VIS C, V15, P10.
   Hossain J., 2010, Proceedings of the 13th International Conference on Computer and Information Technology (ICCIT 2010), P458, DOI 10.1109/ICCITECHN.2010.5723901.
   Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235.
   Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57.
   Kazerouni M. F., ICMLA 2017 19 INT C.
   Kazerouni MF, 2015, SIGNAL IMAGE PROCESS, V6, P1.
   MacQueen J., 1967, P 5 BERKELEY S MATH, V1, P281.
   Miao ZJ, 2006, ENG APPL ARTIF INTEL, V19, P79, DOI 10.1016/j.engappai.2005.05.009.
   Mishra P. K., 2012, 2012 International Conference on Advances in Engineering, Science and Management (ICAESM), P68.
   Moravec H. P., 1977, INT C ART INT.
   Osuna E, 1997, NEURAL NETWORKS FOR SIGNAL PROCESSING VII, P276, DOI 10.1109/NNSP.1997.622408.
   Osuna E., 1997, NEURAL NETWORKS SIGN.
   Plotze RD, 2005, CAN J BOT, V83, P287, DOI {[}10.1139/b05-002, 10.1139/B05-002].
   Purohit S., 2015, COMPUTING NETWORK CO.
   Purohit S, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING AND NETWORK COMMUNICATIONS (COCONET), P710, DOI 10.1109/CoCoNet.2015.7411268.
   Rahman MA, 2011, CHEM SCI J, V29, P1, DOI DOI 10.4172/2150-3494.1000021.
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023\_34.
   Saitoh T, 2000, INT C PATT RECOG, P507, DOI 10.1109/ICPR.2000.906123.
   Vapnik V., 1995, NATURAL STAT THEORY.
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Ye YH, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P723, DOI 10.1109/ISIMP.2004.1434166.},
Number-of-Cited-References = {32},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BP8EN},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000564659100011},
DA = {2023-08-12},
}

@inproceedings{ WOS:000835706300046,
Author = {Moreno, Bruno M. and Cruvinel, Paulo E.},
Book-Group-Author = {IEEE},
Title = {Computer Vision System for Identifying on Farming Weed Species},
Booktitle = {16TH IEEE INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING (ICSC 2022)},
Series = {IEEE International Conference on Semantic Computing},
Year = {2022},
Pages = {287-292},
Note = {16th IEEE International Conference on Semantic Computing (ICSC), ELECTR
   NETWORK, JAN 26-28, 2022},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {In the world, agriculture has been developed by combining new
   technologies for aid production and profitability while keeping
   environmental and social responsibility. The sector is primarily
   responsible to supply food for people, as well as fibers and energy. To
   keep such results, farmers have faced the need to seek, increasingly
   rational use of inputs, as is the use of pesticides, plant regulators,
   and liquid fertilizers. This paper presents a discussion related to the
   design and development of a computer vision system for precision
   spraying in the control of weed species into agricultural crops, based
   on the identification of invasive plants and their quantities. Concepts
   such as plant segmentation using field-acquired images, leaf features
   and descriptors, and classification of species are analyzed and
   implemented in a prototype.},
Publisher = {IEEE COMPUTER SOC},
Address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Moreno, BM (Corresponding Author), Embrapa Instrumentat CNPDIA, POB 741, BR-13560970 Sao Carlos, SP, Brazil.
   Moreno, BM (Corresponding Author), Univ Fed Sao Carlos, Comp Sci Program, Sao Carlos, SP, Brazil.
   Moreno, Bruno M.; Cruvinel, Paulo E., Embrapa Instrumentat CNPDIA, POB 741, BR-13560970 Sao Carlos, SP, Brazil.
   Moreno, Bruno M.; Cruvinel, Paulo E., Univ Fed Sao Carlos, Comp Sci Program, Sao Carlos, SP, Brazil.},
DOI = {10.1109/ICSC52841.2022.00054},
ISSN = {2325-6516},
ISBN = {978-1-6654-3418-8},
Keywords = {embedded platform; plant recognition; decision making; weed control;
   agricultural industry},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {bruno.moreno@estudante.ufscar.br
   paulo.cruvinel@embrapa.br},
Affiliations = {Empresa Brasileira de Pesquisa Agropecuaria (EMBRAPA); Universidade
   Federal de Sao Carlos},
ResearcherID-Numbers = {Moreno, Bruno/HGB-2630-2022},
Funding-Acknowledgement = {Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior - Brasil
   (CAPES) {[}001]; Embrapa Instrumentation {[}11.14.09.001.05.06]},
Funding-Text = {This study was financed in part by the Coordenacao de Aperfeicoamento de
   Pessoal de Nivel Superior - Brasil (CAPES) - Finance Code 001 and by
   Embrapa Instrumentation (grants project 11.14.09.001.05.06).},
Cited-References = {Abouziena HF, 2016, PLANTA DANINHA, V34, P377, DOI 10.1590/S0100-83582016340200019.
   Al-Shakarji N. M., 2017, UNSUPERVISED LEARNIN, P1, DOI {[}10.1109/AIPR.2017.8457935, DOI 10.1109/AIPR.2017.8457935].
   Cruvinel P. E, 2010, CONSTRUCTION APPL MA.
   Cruvinel P. E., 2015, 7 SINTAG S INT TECN, P4.
   Cruvinel P. E, 2014, AGRICULTURA PRECISAO, V1, P135.
   Karam D, 2006, WEEDS MAIZE CROP.
   Liu T, 2018, GISCI REMOTE SENS, V55, P243, DOI 10.1080/15481603.2018.1426091.
   Miflin B, 2000, J EXP BOT, V51, P1, DOI 10.1093/jexbot/51.342.1.
   Milioto A, 2018, IEEE INT CONF ROBOT, P2229.
   Mukhtar Hamza, 2021, 2021 1st International Conference on Artificial Intelligence and Data Analytics (CAIDA), P257, DOI 10.1109/CAIDA51941.2021.9425252.
   Nogueira K, 2017, INT GEOSCI REMOTE SE, P3787, DOI 10.1109/IGARSS.2017.8127824.
   Pereira CS, 2017, PROCEEDINGS OF THE 2017 INTELLIGENT SYSTEMS CONFERENCE (INTELLISYS), P566, DOI 10.1109/IntelliSys.2017.8324352.
   Sabu A, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON INVENTIVE COMMUNICATION AND COMPUTATIONAL TECHNOLOGIES (ICICCT), P145, DOI 10.1109/ICICCT.2017.7975176.
   Sheikh R, 2020, IEEE INT CONF ROBOT, P1350, DOI 10.1109/ICRA40945.2020.9196722.
   Szachowicz J, 2021, KOMATSUNA DATASET PY, P1.
   USDA ERS, 2021, INT BASELINE DATA 20.
   Wang ZB, 2017, ARCH COMPUT METHOD E, V24, P637, DOI 10.1007/s11831-016-9181-4.
   Wen-Tse Chiu, 2020, 2020 International Computer Symposium (ICS), P535, DOI 10.1109/ICS51289.2020.00110.
   Yamada T., 2006, Informacoes Agronomicas, P1, DOI 10.1109/GLOCOM.2006.898.
   Yu JG, 2020, IEEE T IMAGE PROCESS, V29, P389, DOI 10.1109/TIP.2019.2923571.},
Number-of-Cited-References = {20},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BT5CD},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000835706300046},
DA = {2023-08-12},
}

@article{ WOS:000858393000004,
Author = {Mustafa, Hassan and Umer, Muhammad and Hafeez, Umair and Hameed, Ahmad
   and Sohaib, Ahmed and Ullah, Saleem and Madni, Hamza Ahmad},
Title = {Pepper bell leaf disease detection and classification using optimized
   convolutional neural network},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2023},
Volume = {82},
Number = {8},
Pages = {12065-12080},
Month = {MAR},
Abstract = {Agriculture production plays a significant role in the country's
   economy. Diseases are quite natural and common among plants.
   Identification of diseases in plants is necessary for averting losses in
   the yield of agricultural products. Manual monitoring of plants requires
   expertise, immense effort, and excessive time. Automatic detection will
   not only help in reducing time and effort but will also help in
   detecting disease at an early stage, as soon as it will start appearing
   on plant leaves. Recently, image processing in agriculture has attained
   a surge of interest by researchers. This study presents a five-layered
   CNN model for automatic detection of plant disease utilizing leaf
   images. In order to better train a CNN model, 20,000 augmented images
   are generated. Experimental results demonstrate that proposed
   optimized-CNN model can predict pepper bell plant leaf as healthy or
   bacterial with 99.99\% accuracy. Robust results make the proposed
   optimized-CNN model a preliminary warning tool that can be applied as a
   disease identification system in a real cultivation environment.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Umer, M (Corresponding Author), Islamia Univ Bahawalpur, Dept Comp Sci, Bahawalpur, Pakistan.
   Mustafa, Hassan; Hafeez, Umair; Hameed, Ahmad; Sohaib, Ahmed; Madni, Hamza Ahmad, Khwaja Fareed Univ Engn \& Informat Technol, Dept Comp Engn, Rahim Yar Khan, Pakistan.
   Umer, Muhammad, Islamia Univ Bahawalpur, Dept Comp Sci, Bahawalpur, Pakistan.
   Ullah, Saleem, Khwaja Fareed Univ Engn \& Informat Technol, Dept Comp Sci, Rahim Yar Khan, Pakistan.},
DOI = {10.1007/s11042-022-13737-8},
EarlyAccessDate = {SEP 2022},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Leaf disease; Image classification; Deep learning; Optimized
   convolutional neural network},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {umersabir1996@gmail.com
   ahmed.sohaib@kfueit.edu.pk
   saleem.ullah@kfueit.edu.pk
   101101770@seu.edu.cn},
Affiliations = {Khwaja Fareed University of Engineering \& Information Technology,
   Pakistan; Islamia University of Bahawalpur; Khwaja Fareed University of
   Engineering \& Information Technology, Pakistan},
ResearcherID-Numbers = {Ullah, Saleem/D-2644-2014
   Madni, Hamza Ahmad/S-3459-2018
   },
ORCID-Numbers = {Ullah, Saleem/0000-0003-3747-1263
   Madni, Hamza Ahmad/0000-0003-1303-2493
   Umer, Muhammad/0000-0002-6015-9326},
Funding-Acknowledgement = {Department of Computer Engineering under Khwaja Fareed University of
   Engineering and Information Technology (KFUEIT), Punjab, Rahim Yar Khan,
   Pakistan},
Funding-Text = {This research was supported by Department of Computer Engineering under
   Khwaja Fareed University of Engineering and Information Technology
   (KFUEIT), Punjab, Rahim Yar Khan, Pakistan.},
Cited-References = {Alom MZ, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8030292.
   Baranwal S., 2019, P INT C SUST COMP SC, P260.
   Bock CH, 2010, CRIT REV PLANT SCI, V29, P59, DOI 10.1080/07352681003617285.
   Bouaziz B., 2017, LECT NOTES INFORM LN.
   Boulent J, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.00941.
   Castiglione A, 2021, IEEE T IND INFORM, V17, P6480, DOI 10.1109/TII.2021.3057524.
   Chouhan Siddharth Singh, 2019, 2019 4th International Conference on Information Systems and Computer Networks (ISCON), P700, DOI 10.1109/ISCON47742.2019.9036158.
   Esteva A, 2019, NAT MED, V25, P24, DOI 10.1038/s41591-018-0316-z.
   Fujita E., 2018, INT J ENG TECHNOLOGY, V7, P49, DOI {[}10.14419/ijet.v7i4.11.20687, DOI 10.14419/IJET.V7I4.11.20687].
   Glorot X., 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1002/ECS2.1832.
   Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Kim P., 2017, MATLAB DEEP LEARNING, P121, DOI {[}10.1007/978-1-4842-2845-6, DOI 10.1007/978-1-4842-2845-6].
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kundu Nidhi, 2020, 2020 Sixth International Conference on Parallel, Distributed and Grid Computing (PDGC), P243, DOI 10.1109/PDGC50313.2020.9315821.
   Lamichhane JR, 2015, FRONT PLANT SCI, V6, DOI 10.3389/fpls.2015.00385.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Liakos KG, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18082674.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Pagare R, 2013, INT J COMPUT APPL, V67.
   Pandey P, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.00537.
   Parul Sharma, 2020, Information Processing in Agriculture, V7, P566, DOI 10.1016/j.inpa.2019.11.001.
   Pawara P, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P479, DOI 10.5220/0006196204790486.
   Purwins H, 2019, IEEE J-STSP, V13, P206, DOI 10.1109/JSTSP.2019.2908700.
   Rajaraman S, 2018, PEERJ, V6, DOI 10.7717/peerj.4568.
   Savary S, 2012, FOOD SECUR, V4, P519, DOI 10.1007/s12571-012-0200-5.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Tan QY, 2019, FRONT BIG DATA, V2, DOI 10.3389/fdata.2019.00002.
   Tian K, 2019, COMPUT ELECTRON AGR, V165, DOI 10.1016/j.compag.2019.104962.
   Umer M, 2020, IEEE ACCESS, V8, P93782, DOI 10.1109/ACCESS.2020.2994810.
   Umer R. Muhammad, 2020, IEEE COMPUT SOC CONF.
   Yu J, 2018, MACH VISION APPL, V29, P929, DOI 10.1007/s00138-018-0948-5.},
Number-of-Cited-References = {32},
Times-Cited = {2},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {11},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {9O4PB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000858393000004},
DA = {2023-08-12},
}

@article{ WOS:000519652000016,
Author = {Lee, Sue Han and Goeau, Herve and Bonnet, Pierre and Joly, Alexis},
Title = {New perspectives on plant disease characterization based on deep
   learning},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2020},
Volume = {170},
Month = {MAR},
Abstract = {The control of plant diseases is a major challenge to ensure global food
   security and sustainable agriculture. Several recent studies have
   proposed to improve existing procedures for early detection of plant
   diseases through modern automatic image recognition systems based on
   deep learning. In this article, we study these methods in detail,
   especially those based on convolutional neural networks. We first
   examine whether it is more relevant to fine-tune a pre-trained model on
   a plant identification task rather than a general object recognition
   task. In particular, we show, through visualization techniques, that the
   characteristics learned differ according to the approach adopted and
   that they do not necessarily focus on the part affected by the disease.
   Therefore, we introduce a more intuitive method that considers diseases
   independently of crops, and we show that it is more effective than the
   classic crop-disease pair approach, especially when dealing with disease
   involving crops that are not illustrated in the training database. This
   finding therefore encourages future research to rethink the current de
   facto paradigm of crop disease categorization.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Lee, SH (Corresponding Author), Univ Montpellier, INRA, CNRS, AMAP,CIRAD,IRD, Montpellier, France.
   Lee, Sue Han; Goeau, Herve; Bonnet, Pierre, Univ Montpellier, INRA, CNRS, AMAP,CIRAD,IRD, Montpellier, France.
   Goeau, Herve; Bonnet, Pierre, CIRAD, UMR, AMAP, Montpellier, France.
   Joly, Alexis, INRIA Sophia Antipolis, LIRMM, ZENITH Team, Montpellier, France.},
DOI = {10.1016/j.compag.2020.105220},
Article-Number = {105220},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Plant diseases; Automated visual crops analysis; Deep learning; Transfer
   learning},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {sue-han.lee@cirad.fr
   herve.goeau@inria.fr
   pierre.bonnet@cirad.fr
   alexis.joly@inria.fr},
Affiliations = {Centre National de la Recherche Scientifique (CNRS); CIRAD; INRAE;
   Institut de Recherche pour le Developpement (IRD); Universite de
   Montpellier; CIRAD; Universite de Montpellier; Centre National de la
   Recherche Scientifique (CNRS); Universite Paul-Valery; Universite
   Perpignan Via Domitia; Universite de Montpellier},
ResearcherID-Numbers = {joly, alexis/AAV-3101-2021
   Bonnet, Pierre/AAG-6819-2020
   Lee, Sue Han/AAM-6250-2021
   },
ORCID-Numbers = {joly, alexis/0000-0002-2161-9940
   Bonnet, Pierre/0000-0002-2828-4389
   Goeau, Herve/0000-0003-3296-3795},
Funding-Acknowledgement = {Agropolis Fondation, Numev, Cemeb, \#DigitAG through the
   ``Investissements davenir{''} programme {[}1604-019, Labex
   Agro:ANR-10-LABX-0001-01]},
Funding-Text = {This project is supported by Agropolis Fondation, Numev, Cemeb,
   \#DigitAG, under the reference ID 1604-019 through the ``Investissements
   davenir{''} programme (Labex Agro:ANR-10-LABX-0001-01).},
Cited-References = {{[}Anonymous], DEEP LEARN WORKSH IN.
   {[}Anonymous], NATURE, DOI 10.1038/nature14539.
   {[}Anonymous], WORK NOT CLEF 2015 C.
   {[}Anonymous], ADAPT COMPUTAT MACH.
   Barbedo JGA, 2018, COMPUT ELECTRON AGR, V153, P46, DOI 10.1016/j.compag.2018.08.013.
   Barbedo JGA, 2016, BIOSYST ENG, V144, P52, DOI 10.1016/j.biosystemseng.2016.01.017.
   Barbedo JGA, 2018, IEEE LAT AM T, V16, P1749.
   Barbedo JGA, 2018, BIOSYST ENG, V172, P84, DOI 10.1016/j.biosystemseng.2018.05.013.
   Durmus H, 2017, INT CONF AGRO-GEOINF, P46.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Fuentes A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17092022.
   Fuentes AF, 2018, FRONT PLANT SCI, V9, DOI 10.3389/fpls.2018.01162.
   HE K, 2018, ABS181108883 CORR.
   Hughes D.P., 2015, ABS151108060 CORR.
   Huh M., 2016, ARXIV160808614.
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448.
   Iqbal Z, 2018, COMPUT ELECTRON AGR, V153, P12, DOI 10.1016/j.compag.2018.07.032.
   Johannes A, 2017, COMPUT ELECTRON AGR, V138, P200, DOI 10.1016/j.compag.2017.04.013.
   Karpathy A., 2019, CS231N CONVOLUTIONAL.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Leveau V, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P155, DOI 10.1145/2671188.2749328.
   Liu B, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10010011.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Oppenheim D., 2017, ADV ANIMAL BIOSCIENC, V8, P244, DOI DOI 10.1017/S2040470017001376.
   Picon A, 2019, COMPUT ELECTRON AGR, V161, P280, DOI 10.1016/j.compag.2018.04.002.
   Prospero S, 2017, FORESTS, V8, DOI 10.3390/f8030080.
   Ramcharan A, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01852.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Toda Y, 2019, PLANT PHENOMICS, V2019, DOI 10.34133/2019/9237136.
   Too E. C., 2018, COMPUT ELECT AGR.
   Too EC, 2019, COMPUT ELECTRON AGR, V161, P272, DOI 10.1016/j.compag.2018.03.032.
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347.
   Wiesner-Hanks Tyr, 2018, BMC Res Notes, V11, P440, DOI 10.1186/s13104-018-3548-6.
   Yosinski J, 2014, ADV NEUR IN, V27.
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474.
   Zhou Bolei, 2014, ADV NEURAL INFORM PR, P487.},
Number-of-Cited-References = {40},
Times-Cited = {83},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {34},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {KU4AJ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000519652000016},
OA = {Green Published, hybrid},
DA = {2023-08-12},
}

@article{ WOS:000853855100014,
Author = {Liu, Zihao and Zhang, Sulan and Jia, Xiaojun and Yang, Jun},
Title = {A NOVEL WOOD FEATURE EXTRACTION METHOD BASED ON IMPROVED BLOCKED
   HIGHER-ORDER LOCAL AUTO-CORRELATION},
Journal = {WOOD RESEARCH},
Year = {2022},
Volume = {67},
Number = {4},
Pages = {686-699},
Abstract = {Traditionally, HLAC (Higher-order Local Auto-Correlation) algorithm was
   used to extract texture features of wood images. However, heavy memory
   consumption and complexity of high-order mask pattern were common in
   HLAC. A novel feature extraction strategy based on improved blocked
   higher-order local auto-correlation (IBHLAC) is proposed to circumvent
   these problems. Initially, sequences of the whole wood image frames,
   which are the grayscale treatment, were being divided into series of
   subdivisions vertically and horizontally. Additionally, to enhance
   auto-correlation ability of the proposed method, different high-order
   patterns of masks were rebuilt based on zero-order mask by introducing
   the morphology and affine transformation. Finally, time-consumption and
   memory occupation of related four methods were compared. Experiment
   results indicated IBHLAC costs less time and fewer memory consumption on
   the wood texture database compared with other methods, which reveal that
   IBHLAC is efficient.},
Publisher = {SLOVAK FOREST PRODUCTS RESEARCH INST},
Address = {LAMACSKA CESTA 3, BRATISLAVA, SK-841 04, SLOVAKIA},
Type = {Article},
Language = {English},
Affiliation = {Liu, ZH (Corresponding Author), Jiaxing Univ, Coll Informat Sci \& Engn, 414 Room,8 Bldg,118 Jiahang Rd, Jiaxing City, Zhejiang, Peoples R China.
   Liu, Zihao; Zhang, Sulan; Jia, Xiaojun; Yang, Jun, Jiaxing Univ, Coll Informat Sci \& Engn, 414 Room,8 Bldg,118 Jiahang Rd, Jiaxing City, Zhejiang, Peoples R China.},
DOI = {10.37763/wr.1336-4561/67.4.686699},
ISSN = {1336-4561},
Keywords = {Feature extraction; wood recognition; mask pattern; HLAC},
Research-Areas = {Materials Science},
Web-of-Science-Categories  = {Materials Science, Paper \& Wood},
Author-Email = {lzh@zjxu.edu.cn},
Affiliations = {Jiaxing University},
Funding-Acknowledgement = {Scientific Research Foundation of Jiaxing University {[}CD70519085];
   Public Welfare Technology Application Research Project of Jiaxing City
   {[}2020AY10009]; Public Welfare Technology Application Research Project
   of Zhejiang province {[}LGG21F030013, LGG20F010010]; Humanities and
   Social Sciences of Ministry of Education Planning Fund {[}18YJA880032]},
Funding-Text = {This work was financially supported by the Scientific Research
   Foundation of Jiaxing University (No. CD70519085), Public Welfare
   Technology Application Research Project of Jiaxing City (No.
   2020AY10009), Public Welfare Technology Application Research Project of
   Zhejiang province (No. LGG21F030013 and No. LGG20F010010), Humanities
   and Social Sciences of Ministry of Education Planning Fund (No.
   18YJA880032).},
Cited-References = {Bernard F, 2019, PATTERN RECOGN, V92, P146, DOI 10.1016/j.patcog.2019.03.021.
   Bi-hui Wang, 2010, 2010 International Conference on Computer Application and System Modeling (ICCASM 2010), P269, DOI 10.1109/ICCASM.2010.5619388.
   Bulugu I, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON MULTIMEDIA AND IMAGE PROCESSING (ICMIP), P83, DOI 10.1109/ICMIP.2017.16.
   Eriksson A, 2021, IEEE T PATTERN ANAL, V43, P256, DOI 10.1109/TPAMI.2019.2930051.
   Hofmeyr DP, 2021, IEEE T PATTERN ANAL, V43, P447, DOI 10.1109/TPAMI.2019.2930501.
   Hu JF, 2022, WOOD RES-SLOVAKIA, V67, P501, DOI 10.37763/wr.1336-4561/67.3.501510.
   Hutzenthaler M, 2020, P ROY SOC A-MATH PHY, V476, DOI 10.1098/rspa.2019.0630.
   Kreutz M, 1996, PATTERN RECOGN, V29, P19, DOI 10.1016/0031-3203(95)00078-X.
   Kuksenok O, 2005, PHYS REV LETT, V95, DOI 10.1103/PhysRevLett.95.240603.
   Lajevardi SM, 2010, IET IMAGE PROCESS, V4, P114, DOI 10.1049/iet-ipr.2009.0100.
   Lin QZ, 2022, WOOD RES-SLOVAKIA, V67, P488, DOI 10.37763/wr.1336-4561/67.3.488500.
   Liu Z, 2013, RES WOOD CLASSIFICAT, P68.
   Liu Z., 2013, FOREST SCI, V6, P123.
   Liu Z., 2013, FOREST SCI, V11, P117.
   Nie Q, 2019, IEEE T IMAGE PROCESS, V28, P3959, DOI 10.1109/TIP.2019.2907048.
   Nwali M, 2019, PATTERN RECOGN, V89, P151, DOI 10.1016/j.patcog.2019.01.001.
   Ramesh B, 2020, IEEE T PATTERN ANAL, V42, P2767, DOI 10.1109/TPAMI.2019.2919301.
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1476, DOI 10.1109/TPAMI.2016.2601099.
   Rubiyah Y., 2009, 2 INT C IMAGE SIGNAL.
   Suzuki M, 2014, INT C KANSEI ENG EMO.
   Toyoda T., 2005, 4 INT WORKSH TEXT AN, V12, P131.
   TSATSANIS MK, 1992, IEEE T PATTERN ANAL, V14, P733, DOI 10.1109/34.142910.
   Wang HangJun, 2009, Journal of Zhejiang Forestry College, V26, P896.
   Wang H, 2013, PLOS ONE, V8, DOI {[}10.1371/journal.pone.0053879, 10.1371/journal.pone.0057030, 10.1371/journal.pone.0067140, 10.1371/journal.pone.0062841].
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882.},
Number-of-Cited-References = {25},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Wood Res.},
Doc-Delivery-Number = {4N2MZ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000853855100014},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000828066500016,
Author = {Medhi, Epsita and Deb, Nabamita},
Title = {PSFD-Musa: A dataset of banana plant, stem, fruit, leaf, and disease},
Journal = {DATA IN BRIEF},
Year = {2022},
Volume = {43},
Month = {AUG},
Abstract = {In recent times, the classification and identification of different
   fruits and food crops have become a necessity in the field of
   agricultural science; for sustainable growth. Probable processes have
   been developed worldwide to improve the production of food crops.
   Problem-specific, clean and crisp datasets are also lagging in the
   sector. This article introduces an image dataset of varieties of banana
   plants and the diseases related to them. The varieties of Banana plants
   that we have considered in the dataset are the Malbhog (Musa assamica),
   Jahaji (Musa chinensis), Kachkol (Musa paradisiaca L.), Bhimkol (M.
   Balbisiana Colla). And the diseases and pathogens that we have
   considered here are the Bacterial Soft Rot, Banana Fruit Scarring
   Beetle, Black Sigatoka, Yellow Sigatoka, Panama disease, Banana Aphids,
   and Pseudo-Stem Weevil. A dataset of Potassium deficiency has been also
   considered in this article. A total of 8000+ processed images are
   present in the dataset. The purpose of this article is to pro-vide the
   Researchers and Students in getting access to our dataset that would
   help them in their research and in developing some machine learning
   models. (C) 2022 The Authors. Published by Elsevier Inc.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article; Data Paper},
Language = {English},
Affiliation = {Medhi, E (Corresponding Author), Gauhati Univ, Dept Informat Technol, Gauhati 781014, Assam, India.
   Medhi, Epsita; Deb, Nabamita, Gauhati Univ, Dept Informat Technol, Gauhati 781014, Assam, India.},
DOI = {10.1016/j.dib.2022.108427},
EarlyAccessDate = {JUL 2022},
Article-Number = {108427},
ISSN = {2352-3409},
Keywords = {Plant classification; Disease classification; Image processing; Sigatoka
   disease; Banana aphid},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {epsitamedhi12@gmail.com},
Affiliations = {Gauhati University},
ResearcherID-Numbers = {Deb, Nabamita/GPX-0682-2022
   Deb, Nabamita/I-8451-2018
   Deb, Nabamita/AAA-8622-2022},
ORCID-Numbers = {Deb, Nabamita/0000-0002-1860-5548
   Deb, Nabamita/0000-0002-1860-5548
   Deb, Nabamita/0000-0002-1860-5548},
Cited-References = {Igwe DO, 2022, GENET RESOUR CROP EV, V69, P49, DOI 10.1007/s10722-021-01202-8.
   Jeyalakshmi S., 2017, ICTACT Journal on Image and Video Processing, V7, P1515, DOI 10.21917/ijivp.2017.0216.
   Kambale G., 2017, IOSR J COMPUT ENG, V4, P14.
   medindia, LIF BAN TREEF.
   Meshram V, 2022, DATA BRIEF, V40, DOI 10.1016/j.dib.2021.107686.
   Singh V, 2015, 2015 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER ENGINEERING AND APPLICATIONS (ICACEA), P1028, DOI 10.1109/ICACEA.2015.7164858.},
Number-of-Cited-References = {6},
Times-Cited = {4},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {9},
Journal-ISO = {Data Brief},
Doc-Delivery-Number = {3B6RV},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000828066500016},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000494343900044,
Author = {Alimboyong, Catherine R. and Hernandez, Alexander A.},
Book-Group-Author = {IEEE},
Title = {An Improved Deep Neural Network for Classification of Plant Seedling
   Images},
Booktitle = {2019 IEEE 15TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING \& ITS
   APPLICATIONS (CSPA 2019)},
Year = {2019},
Pages = {217-222},
Note = {IEEE 15th International Colloquium on Signal Processing and its
   Applications (CSPA), Penang, MALAYSIA, MAR 08-09, 2019},
Organization = {IEEE; IEEE Malaysia Section Control Systems Chapter; IEEE Control Syst
   Soc},
Abstract = {This scientific pursuit aimed to develop a deep learning architecture
   tailored to classify plant seedling images. Our architecture encompasses
   seven learned layers - five convolutions and two fully connected. We
   performed full training on the network using 4, 234 plant seedling
   images belonging to twelve plant species from Aarhus University Signal
   Processing group. The system is fine-tuned for the architecture to have
   greater processing time and low memory consumption. The architecture was
   evaluated using different network parameters. Furthermore, we used
   training loss function, accuracy, sensitivity, and specificity to
   evaluate the system performance. Experimental results proved that the
   developed architecture has reached excellent performance with overall
   accuracy of 90.15\%. Results were achieved in 111 minutes and 36
   seconds. Future work includes, first, use the model with greater amount
   of datasets through data augmentation and compare the results to other
   existing deep learning architectures using same datasets. Second,
   authors will consider CNN and RNN architectures together using several
   other plant datasets. Third, create a portable mobile application for
   plant seedling images classification utilizing the developed model.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Alimboyong, CR (Corresponding Author), Technol Inst Philippines, Grad Programs, Quezon City, Philippines.
   Alimboyong, Catherine R., Technol Inst Philippines, Grad Programs, Quezon City, Philippines.
   Hernandez, Alexander A., Technol Inst Philippines, Coll Informat Technol Educ, Manila, Philippines.},
ISBN = {978-1-5386-7563-2},
Keywords = {plant classification; CNN; deep learning},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Multidisciplinary; Engineering, Electrical \& Electronic},
Author-Email = {catherinealimboyong@gmail.com
   alexander.hernandez@tip.edu.ph},
Affiliations = {Technological Institute of the Philippines; Technological Institute of
   the Philippines},
ResearcherID-Numbers = {Hernandez, Alexander A./AAN-8160-2020
   Alimboyong, Catherine/HOF-5597-2023
   Alimboyong, Catherine/AFM-6879-2022
   },
ORCID-Numbers = {Hernandez, Alexander A./0000-0003-4586-2651
   alimboyong, catherine/0000-0002-6209-0289
   Hernandez, Alexander/0000-0002-1264-7499},
Cited-References = {Anthimopoulos M, 2016, IEEE T MED IMAGING, V35, P1207, DOI 10.1109/TMI.2016.2535865.
   Azizpour Hossein, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P36, DOI 10.1109/CVPRW.2015.7301270.
   Cortes C., 2016, ADANET ADAPTIVE STRU.
   Dyrmann M., 2017, ARXIV171105458.
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056.
   Frizzi S, 2016, IEEE IND ELEC, P877, DOI 10.1109/IECON.2016.7793196.
   Gheisari M., 2017, SURVEY DEEP LEARNING.
   Gopalakrishnan K, 2017, CONSTR BUILD MATER, V157, P322, DOI 10.1016/j.conbuildmat.2017.09.110.
   Hawkins DM, 2004, J CHEM INF COMP SCI, V44, P1, DOI 10.1021/ci0342472.
   Ioffe S., 2015, INT C MACHINE LEARNI, DOI DOI 10.1007/S13398-014-0173-7.2.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Le Y., 2016, TINY IMAGENET VISUAL.
   LeCun Y., 2015, DEEP LEARNING.
   Penatti Otavio A. B., 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P44, DOI 10.1109/CVPRW.2015.7301382.
   Picon A, 2019, COMPUT ELECTRON AGR, V161, P280, DOI 10.1016/j.compag.2018.04.002.
   Razavian A. S., 2014, DEEP WORK.
   Shallu, 2018, ICT EXPRESS, V4, P247, DOI 10.1016/j.icte.2018.10.007.
   Simons KA, 2014, ADV ACC EDUC-TEACH, V15, P1, DOI 10.1108/S1085-462220140000015001.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Zoph B., 2016, NEURAL ARCHITECTURE, P1.},
Number-of-Cited-References = {23},
Times-Cited = {7},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BO1AD},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000494343900044},
DA = {2023-08-12},
}

@inproceedings{ WOS:000380311100033,
Author = {Gaber, Tarek and Tharwat, Alaa and Snasel, Vaclav and Hassanien, Aboul
   Ella},
Editor = {Herrero, A and Sedano, J and Baruque, B and Quintian, H and Corchado, E},
Title = {Plant Identification: Two Dimensional-Based Vs. One Dimensional-Based
   Feature Extraction Methods},
Booktitle = {10TH INTERNATIONAL CONFERENCE ON SOFT COMPUTING MODELS IN INDUSTRIAL AND
   ENVIRONMENTAL APPLICATIONS},
Series = {Advances in Intelligent Systems and Computing},
Year = {2015},
Volume = {368},
Pages = {375-385},
Note = {10th International Conference on Soft Computing Models in Industrial and
   Environmental Applications (SOCO), Burgos, SPAIN, JUN 15-17, 2015},
Organization = {IEEE Spain Sect; IEEE Syst Man \& Cybernet Spanish Chapter; Int Federat
   Computat Log; BISITE; GICAP Res Grp},
Abstract = {In this paper, a plant identification approach using 2D digital leaves
   images is proposed. The approach made use of two methods of features
   extraction (one-dimensional (1D) and two-dimensional (2D) techniques)
   and the Bagging classifier. For the 1D-based method, PCA and LDA
   techniques were applied, while 2D-PCA and 2D-LDA algorithms were used
   for the 2D-based method. To classify the extracted features in both
   methods, the Bagging classifier, with the decision tree as a weak
   learner, was used. The proposed approach, with its four feature
   extraction techniques, was tested using Flavia dataset which consists of
   1907 colored leaves images. The experimental results showed that the
   accuracy and the performance of our approach, with the 2D-PCA and
   2D-LDA, was much better than using the PCA and LDA. Furthermore, it was
   proven that the 2D-LDA-based method gave the best plant identification
   accuracy and increasing the weak learners of the Bagging classifier
   leaded to a better accuracy. Also, a comparison with the most related
   work showed that our approach achieved better accuracy under the same
   dataset and same experimental setup.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Gaber, T (Corresponding Author), Suez Canal Univ, Fac Comp \& Informat, Ismailia, Egypt.
   Gaber, Tarek; Snasel, Vaclav, Suez Canal Univ, Fac Comp \& Informat, Ismailia, Egypt.
   Tharwat, Alaa, Suez Canal Univ, Fac Engn, Ismailia, Egypt.
   Snasel, Vaclav, VSB TU Ostrava, Fac Elect Engn \& Comp Sci, It4innovat, Ostrava, Czech Republic.
   Hassanien, Aboul Ella, Cairo Univ, Fac Comp \& Informat, Giza, Egypt.},
DOI = {10.1007/978-3-319-19719-7\_33},
ISSN = {2194-5357},
EISSN = {2194-5365},
ISBN = {978-3-319-19719-7; 978-3-319-19718-0},
Keywords = {Plant identification; Principal component analysis; Linear discriminant
   analysis; Bagging classifier; Decision tree; Weak learners; 2D-LDA;
   2D-PCA; Leaf image; Leaves images},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Affiliations = {Egyptian Knowledge Bank (EKB); Suez Canal University; Egyptian Knowledge
   Bank (EKB); Suez Canal University; Technical University of Ostrava;
   Egyptian Knowledge Bank (EKB); Cairo University},
ResearcherID-Numbers = {Tharwat, Alaa/AAS-5682-2020
   Gaber, Tarek/B-9472-2011
   Hassanien, Aboul ella/O-5672-2014
   Snasel, Vaclav/B-8094-2009
   Othman, Alaa Tharwat/O-6010-2014},
ORCID-Numbers = {Tharwat, Alaa/0000-0003-4204-4506
   Gaber, Tarek/0000-0003-4065-4191
   Hassanien, Aboul ella/0000-0002-9989-6681
   Snasel, Vaclav/0000-0002-9600-8319
   Othman, Alaa Tharwat/0000-0003-4204-4506},
Cited-References = {{[}Anonymous], 2000, WORLD PAT INF, DOI DOI 10.1016/S0172-2190(00)00083-1.
   {[}Anonymous], 2007, 2007 1 IEEE INT C BI, DOI DOI 10.1145/1239451.1239452.
   {[}Anonymous], 8 INT C INF SYST INF.
   {[}Anonymous], 2012, INN INT SYST APPL IN.
   {[}Anonymous], MACH LEARN.
   {[}Anonymous], INT J ENG SCI TECHNO.
   {[}Anonymous], 2012, CLEF ONLINE WORKING.
   {[}Anonymous], 2011, COMPUT SCI ENG.
   {[}Anonymous], 2005, TECHNICAL REPORTS.
   {[}Anonymous], P 7 IEEE INT C INT S.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Cui JR, 2012, INT J BIOMETRICS, V4, P106, DOI 10.1504/IJBM.2012.046244.
   Jolliffe IT, 2002, PRINCIPAL COMPONENT.
   Lu JW, 2003, IEEE T NEURAL NETWOR, V14, P195, DOI 10.1109/TNN.2002.806647.
   Ma LH, 2013, LECT NOTES COMPUT SC, V7995, P106, DOI 10.1007/978-3-642-39479-9\_13.
   MOORE BC, 1981, IEEE T AUTOMAT CONTR, V26, P17, DOI 10.1109/TAC.1981.1102568.
   Tharwat A, 2015, ADV INTELL SYST, V334, P359, DOI 10.1007/978-3-319-13572-4\_30.
   Tharwat A, 2014, ADV INTELL SYST, V303, P217, DOI 10.1007/978-3-319-08156-4\_22.
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758.
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097.
   Ye J., 2004, ADV NEURAL INF PROCE, P1569.},
Number-of-Cited-References = {22},
Times-Cited = {15},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BF1BY},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000380311100033},
DA = {2023-08-12},
}

@article{ WOS:000440659600001,
Author = {Namin, Sarah Taghavi and Esmaeilzadeh, Mohammad and Najafi, Mohammad and
   Brown, Tim B. and Borevitz, Justin O.},
Title = {Deep phenotyping: deep learning for temporal phenotype/genotype
   classification},
Journal = {PLANT METHODS},
Year = {2018},
Volume = {14},
Month = {AUG 4},
Abstract = {Background: High resolution and high throughput genotype to phenotype
   studies in plants are underway to accelerate breeding of climate ready
   crops. In the recent years, deep learning techniques and in particular
   Convolutional Neural Networks (CNNs), Recurrent Neural Networks and
   Long-Short Term Memories (LSTMs), have shown great success in visual
   data recognition, classification, and sequence learning tasks. More
   recently, CNNs have been used for plant classification and phenotyping,
   using individual static images of the plants. On the other hand, dynamic
   behavior of the plants as well as their growth has been an important
   phenotype for plant biologists, and this motivated us to study the
   potential of LSTMs in encoding these temporal information for the
   accession classification task, which is useful in automation of plant
   production and care.
   Methods: In this paper, we propose a CNN-LSTM framework for plant
   classification of various genotypes. Here, we exploit the power of deep
   CNNs for automatic joint feature and classifier learning, compared to
   using hand-crafted features. In addition, we leverage the potential of
   LSTMs to study the growth of the plants and their dynamic behaviors as
   important discriminative phenotypes for accession classification.
   Moreover, we collected a dataset of time-series image sequences of four
   accessions of Arabidopsis, captured in similar imaging conditions, which
   could be used as a standard benchmark by researchers in the field. We
   made this dataset publicly available.
   Conclusion: The results provide evidence of the benefits of our
   accession classification approach over using traditional hand-crafted
   image analysis features and other accession classification frameworks.
   We also demonstrate that utilizing temporal information using LSTMs can
   further improve the performance of the system. The proposed framework
   can be used in other applications such as in plant classification given
   the environment conditions or in distinguishing diseased plants from
   healthy ones.},
Publisher = {BMC},
Address = {CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Namin, ST (Corresponding Author), Australian Natl Univ, Res Sch Biol, Canberra, ACT, Australia.
   Namin, ST (Corresponding Author), Australian Natl Univ, Res Sch Engn \& Comp Sci, Canberra, ACT, Australia.
   Namin, Sarah Taghavi; Esmaeilzadeh, Mohammad; Brown, Tim B.; Borevitz, Justin O., Australian Natl Univ, Res Sch Biol, Canberra, ACT, Australia.
   Namin, Sarah Taghavi; Esmaeilzadeh, Mohammad; Najafi, Mohammad, Australian Natl Univ, Res Sch Engn \& Comp Sci, Canberra, ACT, Australia.},
DOI = {10.1186/s13007-018-0333-4},
Article-Number = {66},
EISSN = {1746-4811},
Keywords = {Deep learning; Temporal information; Deep features; Accession
   classification},
Keywords-Plus = {PLATFORM; ENVIRONMENT; PHENOMICS; RESPONSES; ENERGY; PLANTS},
Research-Areas = {Biochemistry \& Molecular Biology; Plant Sciences},
Web-of-Science-Categories  = {Biochemical Research Methods; Plant Sciences},
Author-Email = {sarah.taghavi-namin@anu.edu.au},
Affiliations = {Australian National University; Australian National University},
ResearcherID-Numbers = {Borevitz, Justin O/B-5423-2012
   Esmaeilzadeh, Mohammad/O-8024-2017
   },
ORCID-Numbers = {Borevitz, Justin O/0000-0001-8408-3699
   Esmaeilzadeh, Mohammad/0000-0002-1574-2651
   Brown, Tim/0000-0002-4735-9526},
Funding-Acknowledgement = {Australian Research Council (ARC) Centre of Excellence in Plant Energy
   Biology {[}CE140100008]; ARC Linkage Grant {[}LP140100572]; National
   Collaborative Research Infrastructure Scheme - Australian Plant
   Phenomics Facility; Australian Research Council {[}LP140100572] Funding
   Source: Australian Research Council},
Funding-Text = {We thank funding sources including, Australian Research Council (ARC)
   Centre of Excellence in Plant Energy Biology CE140100008, ARC Linkage
   Grant LP140100572 and the National Collaborative Research Infrastructure
   Scheme - Australian Plant Phenomics Facility. All research carried out
   in this manuscript follows the local, national or international
   guidelines and legislation and the required or appropriate permissions
   and/or licences for the study.},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Akbarian MSA, 2016, CORR.
   Alonso-Blanco C, 2016, CELL, V166, P481, DOI 10.1016/j.cell.2016.05.063.
   Amean ZM, 2013, ACRA.
   {[}Anonymous], 2015, KERAS, DOI DOI 10.1016/J.COMPELECENG.2012.04.014.
   {[}Anonymous], 2012, LSVRC.
   {[}Anonymous], AAMAS.
   Antipov G, 2015, ACM MULTIMEDIA.
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8\_4.
   Backhaus A, 2010, NEW PHYTOL, V187, P251, DOI 10.1111/j.1469-8137.2010.03266.x.
   Bell J, 2017, IET COMPUT VIS, V11, P113, DOI 10.1049/iet-cvi.2016.0127.
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181.
   Bilen Hakan, 2016, CVPR, DOI DOI 10.1109/CVPR.2016.331.
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114.
   Brown TB, 2014, CURR OPIN PLANT BIOL, V18, P73, DOI 10.1016/j.pbi.2014.02.002.
   Camargo A, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0096889.
   Chen DJ, 2014, PLANT CELL, V26, P4636, DOI 10.1105/tpc.114.129601.
   DeChant C, 2017, PHYTOPATHOLOGY, V107, P1426, DOI 10.1094/PHYTO-11-16-0417-R.
   Dee H, 2015, FUNCT PLANT BIOL, V42, pIII, DOI 10.1071/FPv42n5\_FO.
   Dey D, 2012, WACV.
   Dhondt S, 2013, TRENDS PLANT SCI, V18, P433, DOI 10.1016/j.tplants.2013.04.008.
   Donahue J, 2014, PR MACH LEARN RES, V32.
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878.
   Douillard B, 2011, INT J ROBOT RES, V30, P5, DOI 10.1177/0278364910373409.
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714.
   Fahlgren N, 2015, MOL PLANT, V8, P1520, DOI 10.1016/j.molp.2015.06.005.
   Fernando B, 2016, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR.2016.212.
   Fernando Basura, 2016, TPAMI.
   Fiel S, 2010, WORKSH AUSTR ASS PAT.
   Furbank RT, 2011, TRENDS PLANT SCI, V16, P635, DOI 10.1016/j.tplants.2011.09.005.
   Goeau H, 2011, CLEF.
   Goodfellow I, 2016, DEEP LEARNING SEQUEN.
   Granier C, 2014, CURR OPIN PLANT BIOL, V18, P96, DOI 10.1016/j.pbi.2014.02.009.
   GRANLUND GH, 1972, IEEE T COMPUT, VC 21, P195, DOI 10.1109/TC.1972.5008926.
   Grushin A, 2013, IJCNN.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   Hartmann A, 2011, BMC BIOINFORMATICS, V12, DOI 10.1186/1471-2105-12-148.
   Haug S, 2014, WACV.
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI {[}10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2].
   Kadir A, 2011, IJCSIT.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932.
   Knecht AC, 2016, J EXP BOT, V67, P3587, DOI 10.1093/jxb/erw176.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Ladicky L, 2013, INT J COMPUT VISION, V103, P213, DOI 10.1007/s11263-012-0583-y.
   Lafferty JD, 2001, P INT C MACH LEARN, P282.
   LeCun Yann, 1990, ADV NEURAL INFORM PR, P598.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, ICIP.
   Lee SH, 2016, LIFECLEF.
   Lefebvre G, 2013, ICANN.
   Lottes P, 2017, J FIELD ROBOT, V34, P1160, DOI 10.1002/rob.21675.
   Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359.
   Mahasseni B, 2016, CVPR.
   Minervini M, 2017, PLANT J, V90, P204, DOI 10.1111/tpj.13472.
   Minervini M, 2016, PATTERN RECOGN LETT, V81, P80, DOI 10.1016/j.patrec.2015.10.013.
   Minervini M, 2015, IEEE SIGNAL PROC MAG, V32, P126, DOI 10.1109/MSP.2015.2405111.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Monsalve D, 2015, LACNEM.
   Mouine S., 2013, ICMR.
   Najafi M, 2016, CVPR.
   Namin ST, 2012, IEEE INT C INT ROBOT, P1393, DOI 10.1109/IROS.2012.6386074.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Pahikkala T, 2015, COMPUT ELECTRON AGR, V118, P186, DOI 10.1016/j.compag.2015.09.003.
   Pound MP, 2016, BIORXIV.
   Rahaman MM, 2015, FRONT PLANT SCI, V6, DOI 10.3389/fpls.2015.00619.
   Rashad MZ, 2011, IJCSIT.
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131.
   Reyes A, 2015, WORKING NOTES CLEF 2.
   Rivers J, 2015, FOOD SECUR, V7, P375, DOI 10.1007/s12571-015-0431-3.
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720.
   Schikora M, 2010, GI JAHRESTAG, V2, P2709.
   Schikora M, 2012, BMC BIOINFORMATICS, V13, DOI 10.1186/1471-2105-13-171.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Singh A, 2016, TRENDS PLANT SCI, V21, P110, DOI 10.1016/j.tplants.2015.10.015.
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216.
   Song Y, 2013, CVPR.
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843.
   Theano Development Team, 2016, ARXIVABS16050268.
   Tsaftaris SA, 2016, TRENDS PLANT SCI, V21, P989, DOI 10.1016/j.tplants.2016.10.002.
   Ubbens JR, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01190.
   Vanhaeren H, 2015, JOURNEY LEAF PHENOMI.
   Vezzani R, 2010, ICPR.
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251.
   Wang Y, 2009, PROC CVPR IEEE, P872, DOI 10.1109/CVPRW.2009.5206709.
   Wang Z., 2016, INT C MACHINE LEARNI, P2939, DOI DOI 10.1007/S11831-016-9181-4.
   Wu D, 2014, ICCV.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xia F, 2016, POSE GUIDED HUMAN PA.
   Yang WN, 2013, CURR OPIN PLANT BIOL, V16, P180, DOI 10.1016/j.pbi.2013.03.005.
   Yin X, 2014, ICIP.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.},
Number-of-Cited-References = {92},
Times-Cited = {92},
Usage-Count-Last-180-days = {11},
Usage-Count-Since-2013 = {63},
Journal-ISO = {Plant Methods},
Doc-Delivery-Number = {GP2KF},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000440659600001},
OA = {Green Published, Green Submitted, gold},
DA = {2023-08-12},
}

@article{ WOS:000609456300002,
Author = {He, Guiqing and Huo, Yincheng and Ao, Zhen and Zhang, Haixi},
Title = {Toward plant organs in nature: a new database for plant organ system},
Journal = {JOURNAL OF ELECTRONIC IMAGING},
Year = {2020},
Volume = {29},
Number = {6},
Month = {NOV},
Abstract = {The detection of plant organs is an important research field of plant
   recognition area. However, due to the lack of database of plant organs,
   the application of convolutional neural network-based object detection
   on plant species is very limited. A database of plant organs for deep
   learning-based object detection is constructed. A huge number of plant
   images are clawed using specific keywords through keyword search engines
   such as Baidu and Google. After that, an automatic junk image cleaning
   method is performed to remove junk images. Finally, artificial labeling
   is used to delineate plant organ regions. To evaluate the quality of the
   database, experiments in different object detection models are
   implemented. Results show that the established plant organ database has
   good performance in plant organs positioning and classification. (C)
   2020 SPIE and IS\&T},
Publisher = {SPIE-SOC PHOTO-OPTICAL INSTRUMENTATION ENGINEERS},
Address = {1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98225 USA},
Type = {Article},
Language = {English},
Affiliation = {He, GQ (Corresponding Author), Northwestern Polytech Univ, Sch Elect \& Informat, Xian, Peoples R China.
   He, Guiqing; Huo, Yincheng; Ao, Zhen, Northwestern Polytech Univ, Sch Elect \& Informat, Xian, Peoples R China.
   Zhang, Haixi, Northwest A\&F Univ, Coll Informat Engn, Yangling, Shaanxi, Peoples R China.},
DOI = {10.1117/1.JEI.29.6.063009},
Article-Number = {063009},
ISSN = {1017-9909},
EISSN = {1560-229X},
Keywords = {deep learning; convolutional neural network; object detection; plant
   organ database},
Research-Areas = {Engineering; Optics; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Optics; Imaging Science \&
   Photographic Technology},
Author-Email = {guiqing\_he@nwpu.edu.cn},
Affiliations = {Northwestern Polytechnical University; Northwest A\&F University - China},
ResearcherID-Numbers = {Zhang, Haixi/HGB-7131-2022},
Funding-Acknowledgement = {National Nature Science Foundation of China {[}61402368]; Aerospace
   Science and Technology Innovation Foundation of China {[}2017ZD53047,
   20175896]; Common Technology Foundation for Pre-research and Development
   of Equipment in the 13th Five-Year Plan {[}41412010402]},
Funding-Text = {This research was funded by the National Nature Science Foundation of
   China (Grant No. 61402368), Aerospace Science and Technology Innovation
   Foundation of China (Grant Nos. 2017ZD53047 and 20175896), and Common
   Technology Foundation for Pre-research and Development of Equipment in
   the 13th Five-Year Plan (Grant No. 41412010402).},
Cited-References = {{[}Anonymous], 2005, COMPUTER VISION PATT, V1, P886, DOI DOI 10.1109/CVPR.2005.177.
   Chinese Academy of Sciences Institute of Botany, 2008, PLANT PHOT BANK CHIN.
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036.
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4.
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984.
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167.
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800.
   Fu Cheng-Yang, 2017, ABS170106659 CORR.
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169.
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81.
   Gong D.X., 2014, COMPUTER MODERNIZATI, V4, P12.
   Hariharan B., 2015, CVPR.
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI {[}10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175].
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI {[}arXiv:1406.4729, 10.1007/978-3-319-10578-9\_23].
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kuang ZZ, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P310, DOI 10.1109/BigMM.2017.72.
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541.
   Li Z., 2017, ARXIV171200960.
   Li Zeming, 2017, ARXIV171107264.
   Lin T.Y., 2017, ARXIV, DOI DOI 10.1109/CVPR.2017.106.
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1\_48.
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6\_24.
   Longquan, 2014, COMPUT APPL SOFTWARE.
   Minervini M, 2016, PATTERN RECOGN LETT, V81, P80, DOI 10.1016/j.patrec.2015.10.013.
   Peng XL, 2016, IEEE COMPUT SOC CONF, P1544, DOI 10.1109/CVPRW.2016.192.
   Redmon J., 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.91.
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Simonyan K, 2015, 3 INT C LEARNING REP, DOI DOI 10.2146/AJHP170251.
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5.
   Vincent P., 2008, P 25 INT C MACHINE L, P1096, DOI DOI 10.1145/1390156.1390294.
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517.
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb.
   Xiao-Yong, 2017, OPT OPTOELECTRON TEC.
   Zhou K, 2016, DESTECH TRANS COMP.
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1\_26.},
Number-of-Cited-References = {40},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {10},
Journal-ISO = {J. Electron. Imaging},
Doc-Delivery-Number = {PU7BY},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000609456300002},
DA = {2023-08-12},
}

@inproceedings{ WOS:000262505800029,
Author = {Sole-Casals, Jordi and Travieso, Carlos M. and Alonso, Jesus B. and
   Ferrer, Miguel A.},
Editor = {Corchado, JM and DePaz, JF and Rocha, MP and Riverola, FF},
Title = {Improving a Leaves Automatic Recognition Process Using PCA},
Booktitle = {2ND INTERNATIONAL WORKSHOP ON PRACTICAL APPLICATIONS OF COMPUTATIONAL
   BIOLOGY AND BIOINFORMATICS (IWPACBB 2008)},
Series = {ADVANCES IN SOFT COMPUTING},
Year = {2009},
Volume = {49},
Pages = {243+},
Note = {2nd International Workshop on Practical Applications of Computational
   Biology and Bioinformatics (IWPACBB 08), Salamanca, SPAIN, OCT 22-24,
   2008},
Abstract = {In this work we present a simulation of a recognition process with
   perimeter characterization of a simple plant leaves as a unique
   discriminating parameter. Data coding allowing for independence of
   leaves size and orientation may penalize performance recognition for
   some varieties. Border description sequences are then used, and
   Principal Component Analysis (PCA) is applied in order to study which is
   the best number of components for the classification task, implemented
   by means of a Support Vector Machine (SVM) System. Obtained results are
   satisfactory, and compared with {[}4] our system improves the
   recognition success, diminishing the variance at the same time.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Sole-Casals, J (Corresponding Author), Univ Vic, Signal Proc Grp, C Laura 13, E-08500 Barcelona, Spain.
   Sole-Casals, Jordi, Univ Vic, Signal Proc Grp, C Laura 13, E-08500 Barcelona, Spain.
   Travieso, Carlos M.; Alonso, Jesus B.; Ferrer, Miguel A., Univ Las Palmas Gran Canaria, CeTIC, Dept Signals \& Commun, E-35017 Las Palmas Gran Canaria, Spain.},
ISSN = {1615-3871},
ISBN = {978-3-540-85860-7},
Keywords = {Principal Component Analysis; Pattern Recognition; Leaves Recognition;
   Parameterization; Characteristics selection},
Research-Areas = {Mathematical \& Computational Biology},
Web-of-Science-Categories  = {Mathematical \& Computational Biology},
Author-Email = {jordi.sole@uvic.ca
   ctravieso@dsc.ulpgc.es
   jalonso@dsc.ulpgc.es
   mferrer@dsc.ulpgc.es},
Affiliations = {Universitat de Vic - Universitat Central de Catalunya (UVic-UCC);
   Universidad de Las Palmas de Gran Canaria},
ResearcherID-Numbers = {Ferrer, Miguel A A/L-3863-2013
   Alonso-Hernández, Jesús B./N-5977-2014
   Solé-Casals, Jordi/GRX-7991-2022
   Solé-Casals, Jordi/B-7754-2008
   Travieso-González, Carlos M./N-5967-2014
   Ferrer, Miguel/AFU-8286-2022},
ORCID-Numbers = {Ferrer, Miguel A A/0000-0002-2924-1225
   Alonso-Hernández, Jesús B./0000-0002-7866-585X
   Solé-Casals, Jordi/0000-0002-6534-1979
   Travieso-González, Carlos M./0000-0002-4621-2768
   },
Funding-Acknowledgement = {Ministerio de Educacion y Ciencia of Spain {[}TEC2007-61535/TCM];
   Universitat de Vic {[}R0912]},
Funding-Text = {The first author acknowledges support from the Ministerio de Educacion y
   Ciencia of Spain under the grant TEC2007-61535/TCM, and from the
   Universitat de Vic under the grant R0912.},
Cited-References = {BRICENO JC, 2002, IASTED INT C SIGN PR, P249.
   CUI G, 2002, P 5 AS C COMP VIS, P23.
   Guo GD, 2001, IMAGE VISION COMPUT, V19, P631, DOI 10.1016/S0262-8856(01)00046-4.
   Huang Z., 1996, IEEE T IMAGE PROCESS, V5, P824.
   Hyvarinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71.
   Jolliffe I., 2011, WILEY INTERDISCIPLIN, DOI DOI 10.1002/9781118445112.STAT02052.
   JUTTEN C, 1991, SIGNAL PROCESSING, V24.
   Loncaric S, 1998, PATTERN RECOGN, V31, P983, DOI 10.1016/S0031-2023(97)00122-2.
   LU F, 1994, SIGNAL PROCESS, V37, P129, DOI 10.1016/0165-1684(94)90171-6.
   POBLADOR VS, 2004, LNCS, V3195, P1165.
   Rabiner L., 1993, FUNDAMENTALS SPEECH.},
Number-of-Cited-References = {11},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BIT38},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000262505800029},
DA = {2023-08-12},
}

@article{ WOS:000325819400038,
Author = {Wang, Hang-jun and Zhang, Guang-qun and Qi, Heng-nian},
Title = {Wood Recognition Using Image Texture Features},
Journal = {PLOS ONE},
Year = {2013},
Volume = {8},
Number = {10},
Month = {OCT 11},
Abstract = {Inspired by theories of higher local order autocorrelation (HLAC), this
   paper presents a simple, novel, yet very powerful approach for wood
   recognition. The method is suitable for wood database applications,
   which are of great importance in wood related industries and
   administrations. At the feature extraction stage, a set of features is
   extracted from Mask Matching Image (MMI). The MMI features preserve the
   mask matching information gathered from the HLAC methods. The texture
   information in the image can then be accurately extracted from the
   statistical and geometrical features. In particular, richer information
   and enhanced discriminative power is achieved through the length
   histogram, a new histogram that embodies the width and height
   histograms. The performance of the proposed approach is compared to the
   state-of-the-art HLAC approaches using the wood stereogram dataset ZAFU
   WS 24. By conducting extensive experiments on ZAFU WS 24, we show that
   our approach significantly improves the classification accuracy.},
Publisher = {PUBLIC LIBRARY SCIENCE},
Address = {1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA},
Type = {Article},
Language = {English},
Affiliation = {Wang, HJ (Corresponding Author), Zhejiang A\&F Univ, Coll Tianmu, Linan, Peoples R China.
   Wang, Hang-jun, Zhejiang A\&F Univ, Coll Tianmu, Linan, Peoples R China.
   Wang, Hang-jun, Chinese Acad Sci, Hefei Inst Intelligent Machines, Hefei, Peoples R China.
   Wang, Hang-jun, Univ Sci \& Technol China, Dept Automat, Hefei 230026, Peoples R China.
   Zhang, Guang-qun; Qi, Heng-nian, Zhejiang A\&F Univ, Sch Informat \& Technol, Linan, Peoples R China.},
DOI = {10.1371/journal.pone.0076101},
Article-Number = {e76101},
ISSN = {1932-6203},
Keywords-Plus = {AUTOCORRELATIONS},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {hangjunw@mail.ustc.edu.cn},
Affiliations = {Zhejiang A\&F University; Chinese Academy of Sciences; Chinese Academy
   of Sciences; University of Science \& Technology of China, CAS; Zhejiang
   A\&F University},
Funding-Acknowledgement = {National Science Foundation of China {[}60970082]; Talent Start-up
   Foundation of Zhejiang AF University {[}2013FR051]},
Funding-Text = {This work is supported by the grants of the National Science Foundation
   of China, No. 60970082, and the Talent Start-up Foundation of Zhejiang
   A\&F University under grant No. 2013FR051. The funders had no role in
   study design, data collection and analysis, decision to publish, or
   preparation of the manuscript.},
Cited-References = {Akaho S, 1993, B ELECTROTECHNICAL L, V57, P973.
   {[}Anonymous], 2006, IEEE COMPUTER SOC C.
   {[}Anonymous], 1989, IAWA BULL.
   Baas P, 2004, IAWA J, V25, P1, DOI 10.1163/22941932-90000349.
   Bremananth R, 2009, INT J INTELL SYST TE, V4, P54.
   Canadian Wood Council, 2012, DES WOOD SUST.
   CHEN YQ, 1995, PATTERN RECOGN, V28, P537, DOI 10.1016/0031-3203(94)00116-4.
   Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678.
   Dalal N, 2005, P IEEE COMPUTER SOC, P20.
   Douglas JG, 2004, WOOD SCI.
   Francisco-Fernandez M, 2012, CHEMOMETR INTELL LAB, V118, P159, DOI 10.1016/j.chemolab.2012.07.003.
   Galasso Fabio, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2342, DOI 10.1109/CVPRW.2009.5206591.
   Goudail F, 1996, IEEE T PATTERN ANAL, V18, P1024, DOI 10.1109/34.541411.
   Hang-jun Wang, 2010, 2010 International Conference on Computer Application and System Modeling (ICCASM 2010), P477, DOI 10.1109/ICCASM.2010.5620573.
   Kobayashi T, 2008, LECT NOTES COMPUT SC, V5302, P346, DOI 10.1007/978-3-540-88682-2\_27.
   Kreutz M, 1996, PATTERN RECOGN, V29, P19, DOI 10.1016/0031-3203(95)00078-X.
   Kurita T., 1998, P AS C COMP VIS, P89.
   Lajevardi S, 2010, SIGNAL IMAGE VIDEO P, V4, P113.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Mallik A, 2011, CHEMOMETR INTELL LAB, V107, P351, DOI 10.1016/j.chemolab.2011.05.005.
   Matsukawa T, 2010, VISAPP 2010: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P325.
   Matsukawa T, 2010, LECT NOTES COMPUT SC, V5996, P384.
   McGraw-Hill Editors, 2002, MCGRAW HILL ENC SCI, P600.
   MCLAUGHLIN JA, 1968, INFORM CONTROL, V12, P121, DOI 10.1016/S0019-9958(68)90241-6.
   Nadaraj M, 2008, INT J SIMUL SYST SCI, V9, P9.
   Nomotoy N, 2005, P IAPR C MACH VIS AP, P265.
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
   Otsu N., 1988, Proceedings of IAPR Workshop on Computer Vision: Special Hardware and Industrial Applications, P431.
   Pan S., 2012, TRANS MACH LEARN DAT, V5, P45.
   Pan S, 2011, COMPUT ELECTRON AGR, V75, P250, DOI 10.1016/j.compag.2010.11.010.
   Piuri V, 2010, IEEE T SYST MAN CY C, V40, P358, DOI 10.1109/TSMCC.2009.2039479.
   Rojas JAM, 2011, APPL ACOUST, V72, P934, DOI 10.1016/j.apacoust.2011.05.016.
   Schoch W, PREPARATION WOOD MIC.
   Tanaka Y, 2010, IEEE T IMAGE PROCESS, V19, P934, DOI 10.1109/TIP.2009.2038820.
   Tarrio-Saavedra J, 2011, J THERM ANAL CALORIM, V104, P87, DOI 10.1007/s10973-010-1157-2.
   Toyoda T., 2005, 4 INT WORKSH TEXT AN, V12, P131.
   Toyoda T, 2007, PATTERN RECOGN, V40, P1466, DOI 10.1016/j.patcog.2006.10.006.
   Wang XF, 2010, PATTERN RECOGN, V43, P603, DOI 10.1016/j.patcog.2009.08.002.
   WWF, 2011, LIV PLAN REP 2010.
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097.
   Yusof R, 2010, 2010 12TH INTERNATIONAL CONFERENCE ON COMPUTER MODELLING AND SIMULATION (UKSIM), P289, DOI 10.1109/UKSIM.2010.61.},
Number-of-Cited-References = {41},
Times-Cited = {21},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {11},
Journal-ISO = {PLoS One},
Doc-Delivery-Number = {236SL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000325819400038},
OA = {gold, Green Submitted, Green Published},
DA = {2023-08-12},
}

@inproceedings{ WOS:000428410700171,
Author = {Shah, Meet P. and Singha, Sougata and Awate, Suyash P.},
Book-Group-Author = {IEEE},
Title = {LEAF CLASSIFICATION USING MARGINALIZED SHAPE CONTEXT AND SHAPE plus
   TEXTURE DUAL-PATH DEEP CONVOLUTIONAL NEURAL NETWORK},
Booktitle = {2017 24TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
Series = {IEEE International Conference on Image Processing ICIP},
Year = {2017},
Pages = {860-864},
Note = {24th IEEE International Conference on Image Processing (ICIP), Beijing,
   PEOPLES R CHINA, SEP 17-20, 2017},
Organization = {Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Signal Proc
   Soc},
Abstract = {Identifying plant species based on photographs of their leaves is an
   important problem in computer vision and biology. Previous approaches
   for leaf image classification typically rely on hand-crafted shape
   features or texture features. In contrast, we propose a dual-path deep
   convolutional neural network (CNN) to (i) learn joint feature
   representations for leaf images, exploiting their shape and texture
   characteristics, and (ii) optimize these features for the classification
   task. We compare our CNN approach against (i) vanilla CNN classifiers
   and (ii) popular hand-crafted shape features, including a novel
   shape-context based feature that is extremely computationally efficient,
   which we call the marginalized shape context. Our results on three large
   public datasets demonstrate that our dual-path CNN leads to higher
   accuracy and consistency than the state of the art.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Shah, MP (Corresponding Author), Indian Inst Technol, Elect Engn Dept, Bombay, Maharashtra, India.
   Shah, Meet P., Indian Inst Technol, Elect Engn Dept, Bombay, Maharashtra, India.
   Singha, Sougata, Yodlee, Bombay, Maharashtra, India.
   Awate, Suyash P., Indian Inst Technol, Comp Sci \& Engn Dept, Bombay, Maharashtra, India.},
ISSN = {1522-4880},
ISBN = {978-1-5090-2175-8},
Keywords = {Leaf recognition; shape; texture; marginalized shape context; dual-path
   deep convolutional neural net},
Research-Areas = {Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Imaging Science \& Photographic Technology},
Affiliations = {Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Bombay; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Bombay},
Funding-Acknowledgement = {IIT Bombay Seed Grant {[}14IRCCSG010]; Nvidia GPU Grant},
Funding-Text = {Thanks to IIT Bombay Seed Grant 14IRCCSG010 and Nvidia GPU Grant. All
   work done at IIT Bombay.},
Cited-References = {Barla A, 2003, P INT C IM PROC, V3, P510.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
   Caruana R, 2001, ADV NEUR IN, V13, P402.
   Chaki J, PATTERN RECOG LET, V58, P61.
   Clarke J, 2006, LECT NOTES COMPUT SC, V4292, P427.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Dryden I. L., 1998, STAT SHAPE ANAL, V4.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kuhn H. W., 1955, NAV RES LOG, V2, P83, DOI DOI 10.1002/NAV.3800020109.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Kumar TP, 2016, P INT C COMP VIS IM, V460, P531.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Loshchilov I, 2016, INT C LEARN REPR.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Mouine S., 2012, P 2 ACM INT C MULT R, P49.
   Nair V., 2010, ICML 10 PROC 27 INT.
   Nam Y, 2008, COMPUT VIS IMAGE UND, V110, P245, DOI 10.1016/j.cviu.2007.08.002.
   Park J, 2008, J SYST SOFTWARE, V81, P71, DOI 10.1016/j.jss.2007.05.001.
   Scholkopf B., 2002, ENCY BIOSTATISTICS.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zhao ZQ, 2015, LECT NOTES COMPUT SC, V9004, P348, DOI 10.1007/978-3-319-16808-1\_24.},
Number-of-Cited-References = {26},
Times-Cited = {20},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BJ8KY},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000428410700171},
DA = {2023-08-12},
}

@article{ WOS:000777332300034,
Author = {Wang, Bin and Gao, Yongsheng and Yuan, Xiaohui and Xiong, Shengwu},
Title = {Local R-Symmetry Co-Occurrence: Characterising Leaf Image Patterns for
   Identifying Cultivars},
Journal = {IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS},
Year = {2022},
Volume = {19},
Number = {2},
Pages = {1018-1031},
Month = {MAR-APR},
Abstract = {Leaf image recognition techniques have been actively researched for
   plant species identification. However it remains unclear whether
   analysing leaf patterns can provide sufficient information for further
   differentiating cultivars. This paper reports our attempt on cultivar
   recognition from leaves as a general very fine-grained pattern
   recognition problem, which is not only a challenging research problem
   but also important for cultivar evaluation, selection and production in
   agriculture. We propose a novel local R-symmetry co-occurrence method
   for characterising discriminative local symmetry patterns to distinguish
   subtle differences among cultivars. Through scalable and moving
   R-relation radius pairs, we generate a set of radius symmetry
   co-occurrence matrices (RsCoM)and their measures for describing the
   local symmetry properties of interior regions. By varying the size of
   the radius pair, the RsCoM measures local R-symmetry co-occurrence from
   global/coarse to fine scales. A new two-phase strategy of analysing the
   distribution of local RsCoM measures is designed to match the multiple
   scale appearance symmetry pattern distributions of similar cultivar leaf
   images. We constructed three leaf image databases, SoyCultivar,
   CottCultivar, and PeanCultivar, for an extensive experimental evaluation
   on recognition across soybean, cotton and peanut cultivars. Encouraging
   experimental results of the proposed method in comparison with the
   state-of-the-art leaf species recognition methods demonstrate the
   effectiveness of the proposed method for cultivar identification, which
   may advance the research in leaf recognition from species to cultivar.},
Publisher = {IEEE COMPUTER SOC},
Address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
Type = {Article},
Language = {English},
Affiliation = {Gao, YS (Corresponding Author), Griffith Univ, Sch Engn \& Built Environm, Brisbane, Qld, Australia.
   Wang, Bin; Gao, Yongsheng, Griffith Univ, Sch Engn \& Built Environm, Brisbane, Qld, Australia.
   Yuan, Xiaohui; Xiong, Shengwu, Wuhan Univ Technol, Sch Comp Sci \& Technol, Wuhan, Peoples R China.},
DOI = {10.1109/TCBB.2020.3031280},
ISSN = {1545-5963},
EISSN = {1557-9964},
Keywords = {Shape; Image recognition; Pattern recognition; Cotton; Feature
   extraction; Veins; Shape measurement; Plant species recognition;
   cultivar classification; soybean cultivar; cotton cultivar; peanut
   cultivar; fine-grained recognition; leaf image; R-symmetry co-occurrence},
Keywords-Plus = {SHAPE; MULTISCALE; CLASSIFICATION; IDENTIFICATION; RECOGNITION; TEXTURE;
   REPRESENTATION; FEATURES; RETRIEVAL; SYSTEM},
Research-Areas = {Biochemistry \& Molecular Biology; Computer Science; Mathematics},
Web-of-Science-Categories  = {Biochemical Research Methods; Computer Science, Interdisciplinary
   Applications; Mathematics, Interdisciplinary Applications; Statistics \&
   Probability},
Author-Email = {bin.wang@griffith.edu.au
   yongsheng.gao@griffith.edu.au
   yuanxiaohui@whut.edu.cn
   xiongsw@whut.edu.cn},
Affiliations = {Griffith University; Wuhan University of Technology},
ResearcherID-Numbers = {Xiong, Shou-Mei/A-4225-2009},
Funding-Acknowledgement = {Australian Research Council (ARC) {[}DP180100958]; ARC Industrial
   Transformation Research Hub for Driving Farming Productivity and Disease
   Prevention {[}IH180100002]; National Natural Science Foundation of China
   {[}61372158]},
Funding-Text = {This work is supported in part by Australian Research Council (ARC)
   under Discovery Grant DP180100958 and under ARC Industrial
   Transformation Research Hub for Driving Farming Productivity and Disease
   Prevention grant IH180100002, and National Natural Science Foundation of
   China under Grant No. 61372158.},
Cited-References = {Alajlan N, 2007, PATTERN RECOGN, V40, P1911, DOI 10.1016/j.patcog.2006.12.005.
   Backes AR, 2009, PATTERN RECOGN, V42, P54, DOI 10.1016/j.patcog.2008.07.006.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
   Bookstein F L, 1997, Med Image Anal, V1, P225.
   Bryner D, 2012, PROC CVPR IEEE, P390, DOI 10.1109/CVPR.2012.6247700.
   Casanova D, 2009, INT J IMAG SYST TECH, V19, P236, DOI 10.1002/ima.20201.
   Cavassim J. E., 2013, Journal of Agronomy, V12, P168, DOI 10.3923/ja.2013.168.178.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chen YM, 2011, BIOSYST ENG, V109, P186, DOI 10.1016/j.biosystemseng.2011.03.004.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Gomez W, 2012, IEEE T MED IMAGING, V31, P1889, DOI 10.1109/TMI.2012.2206398.
   Hearn DJ, 2009, TAXON, V58, P934, DOI 10.1002/tax.583021.
   Hong BW, 2015, IEEE T PATTERN ANAL, V37, P151, DOI 10.1109/TPAMI.2014.2342215.
   Hu RX, 2012, PATTERN RECOGN, V45, P3222, DOI 10.1016/j.patcog.2012.02.020.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   KUHL FP, 1982, COMPUT VISION GRAPH, V18, P236, DOI 10.1016/0146-664X(82)90034-X.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Manay S, 2006, IEEE T PATTERN ANAL, V28, P1602, DOI 10.1109/TPAMI.2006.208.
   Manning CD, 2008, INTRO INFORM RETRIEV.
   MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591.
   Mokhtarian F, 2004, IEEE T IMAGE PROCESS, V13, P653, DOI 10.1109/TIP.2004.826126.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Park J, 2008, J SYST SOFTWARE, V81, P71, DOI 10.1016/j.jss.2007.05.001.
   Plotze RD, 2005, CAN J BOT, V83, P287, DOI {[}10.1139/b05-002, 10.1139/B05-002].
   Premachandran V, 2013, PATTERN RECOGN, V46, P2092, DOI 10.1016/j.patcog.2013.01.030.
   Setiyono TD, 2008, FIELD CROP RES, V108, P82, DOI 10.1016/j.fcr.2008.03.005.
   Soh LK, 1999, IEEE T GEOSCI REMOTE, V37, P780, DOI 10.1109/36.752194.
   Venkatesh S. K., 2011, Proceedings of the 2011 Third National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG 2011), P211, DOI 10.1109/NCVPRIPG.2011.52.
   Wang B, 2015, INFORM SCIENCES, V302, P132, DOI 10.1016/j.ins.2014.07.028.
   Wang B, 2014, IEEE T IMAGE PROCESS, V23, P4101, DOI 10.1109/TIP.2014.2343457.
   Wang XG, 2014, PATTERN RECOGN, V47, P2116, DOI 10.1016/j.patcog.2013.12.008.
   Xu CJ, 2009, IEEE T PATTERN ANAL, V31, P180, DOI 10.1109/TPAMI.2008.199.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {40},
Times-Cited = {4},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {3},
Journal-ISO = {IEEE-ACM Trans. Comput. Biol. Bioinform.},
Doc-Delivery-Number = {0F4LG},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000777332300034},
DA = {2023-08-12},
}

@inproceedings{ WOS:000405550200006,
Author = {Carranza-Rojas, Jose and Mata-Montero, Erick},
Book-Group-Author = {IEEE},
Title = {On the Significance of Leaf Sides in Automatic Leaf-based Plant Species
   Identification},
Booktitle = {2016 IEEE 36TH CENTRAL AMERICAN AND PANAMA CONVENTION (CONCAPAN XXXVI)},
Year = {2016},
Note = {36th IEEE Central American and Panama Convention (CONCAPAN), San Jose,
   COSTA RICA, NOV 09-11, 2016},
Organization = {IEEE},
Abstract = {Because the front side of a leaf and the underside are functionally very
   different - the former captures sunlight to produce photosynthesis and
   the latter absorbs carbon dioxide and releases oxygen and vapor - they
   typically have different visual features. In this paper we study the
   significance of leaf sides in visual recognition systems for automatic
   plant species identification. We measure the accuracy of species
   identifications with a dataset of 63 species of trees from Costa Rica
   that includes pictures of both, front sides and undersides of tree
   leaves. The dataset is used as a global dataset and is also partitioned
   as two datasets: one of front side pictures and one of underside
   pictures. Training and testing of different algorithms is performed and
   their accuracies computed for the group of species and for each
   individual species. For the tested dataset, leaf side is a significant
   factor for automatic plant species identification. On the average, and
   for most cases, underside pictures lead to more accurate
   identifications.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Carranza-Rojas, J (Corresponding Author), Costa Rica Inst Technol, Sch Comp, PARMA Grp, Cartago, Costa Rica.
   Carranza-Rojas, Jose; Mata-Montero, Erick, Costa Rica Inst Technol, Sch Comp, PARMA Grp, Cartago, Costa Rica.},
ISBN = {978-1-4673-9578-6},
Keywords = {Biodiversity Informatics; Computer Vision; Image Processing; Plant
   Identification},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {jcarranza@itcr.ac.cr
   emata@itcr.ac.cr},
Affiliations = {Instituto Tecnologico de Costa Rica},
Cited-References = {Carranza-Rojas J., 2016, P IFIP WORL IN PRESS.
   Carranza-Rojas Jose, 2016, CLEIej, V19, P7.
   Fitzgibbon A., 2012, LECT NOTES COMPUTER, P502.
   Herdiyeni Y, 2013, INT C ADV COMP SCI I, P353, DOI 10.1109/ICACSIS.2013.6761601.
   Kumar N, COMPUTER VISION ECCV.
   MacLeod N., 2007, AUTOMATED TAXON IDEN.
   Nguyen Q., 2013, INT C ADV TECHN COMM.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Simard PY, 2003, PROC INT CONF DOC, P958.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {10},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BI1CK},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000405550200006},
DA = {2023-08-12},
}

@article{ WOS:000735089700001,
Author = {Lu, Jianqiang and Lin, Weize and Chen, Pingfu and Lan, Yubin and Deng,
   Xiaoling and Niu, Hongyu and Mo, Jiawei and Li, Jiaxing and Luo, Shengfu},
Title = {Research on Lightweight Citrus Flowering Rate Statistical Model Combined
   with Anchor Frame Clustering Optimization},
Journal = {SENSORS},
Year = {2021},
Volume = {21},
Number = {23},
Month = {DEC},
Abstract = {At present, learning-based citrus blossom recognition models based on
   deep learning are highly complicated and have a large number of
   parameters. In order to estimate citrus flower quantities in natural
   orchards, this study proposes a lightweight citrus flower recognition
   model based on improved YOLOv4. In order to compress the backbone
   network, we utilize MobileNetv3 as a feature extractor, combined with
   deep separable convolution for further acceleration. The Cutout data
   enhancement method is also introduced to simulate citrus in nature for
   data enhancement. The test results show that the improved model has an
   mAP of 84.84\%, 22\% smaller than that of YOLOv4, and approximately two
   times faster. Compared with the Faster R-CNN, the improved citrus flower
   rate statistical model proposed in this study has the advantages of less
   memory usage and fast detection speed under the premise of ensuring a
   certain accuracy. Therefore, our solution can be used as a reference for
   the edge detection of citrus flowering.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Deng, XL (Corresponding Author), South China Agr Univ, Sch Coll Elect Engn, Guangzhou 510642, Peoples R China.
   Deng, XL (Corresponding Author), South China Agr Univ, Sch Coll Artificial Intelligence, Guangzhou 510642, Peoples R China.
   Deng, XL (Corresponding Author), Natl Int Joint Res Ctr Precis Agr Aviat Applicat, Guangzhou 510642, Peoples R China.
   Lu, Jianqiang; Lin, Weize; Chen, Pingfu; Lan, Yubin; Deng, Xiaoling; Niu, Hongyu; Mo, Jiawei; Li, Jiaxing; Luo, Shengfu, South China Agr Univ, Sch Coll Elect Engn, Guangzhou 510642, Peoples R China.
   Lu, Jianqiang; Lan, Yubin; Deng, Xiaoling, South China Agr Univ, Sch Coll Artificial Intelligence, Guangzhou 510642, Peoples R China.
   Lu, Jianqiang; Lan, Yubin; Deng, Xiaoling, Natl Int Joint Res Ctr Precis Agr Aviat Applicat, Guangzhou 510642, Peoples R China.
   Lingnan Modern Agr Guangdong Lab, Guangzhou 510642, Peoples R China.},
DOI = {10.3390/s21237929},
Article-Number = {7929},
EISSN = {1424-8220},
Keywords = {citrus flowering rate; light weight; deep learning; edge computing;
   YOLOv4},
Research-Areas = {Chemistry; Engineering; Instruments \& Instrumentation},
Web-of-Science-Categories  = {Chemistry, Analytical; Engineering, Electrical \& Electronic;
   Instruments \& Instrumentation},
Author-Email = {ljq@scau.edu.cn
   wei\_ze\_lin@163.com
   chenpingfu273@163.com
   ylan@scau.edu.cn
   dengxl@scau.edu.cn
   niuhongyu0207@163.com
   jiaweimo@stu.scau.edu.cn
   lee740464450@163.com
   luo8732402@163.com},
Affiliations = {South China Agricultural University; South China Agricultural
   University; Guangdong Laboratory for Lingnan Modern Agriculture},
ResearcherID-Numbers = {Lan, Yubin/AHA-6347-2022
   },
ORCID-Numbers = {Deng, Xiaoling/0000-0001-5588-3443
   Lu, Jianqiang/0000-0002-6417-4646},
Funding-Acknowledgement = {Key-Area Research and Development Program of Guangdong Province
   {[}2019B020214003]; Basic and Applied Basic Research Project of
   Guangzhou Basic Research Plan in 2022; Key-Area Research and Development
   Program of Guangzhou {[}202103000090]; Key-Areas of Artificial
   Intelligence in General Colleges and Universities of Guangdong Province
   {[}2019KZDZX1012]},
Funding-Text = {Funding This study was supported by the Key-Area Research and
   Development Program of Guangdong Province (Grant No. 2019B020214003),
   Basic and Applied Basic Research Project of Guangzhou Basic Research
   Plan in 2022, Key-Area Research and Development Program of Guangzhou
   (Grant No. 202103000090), Key-Areas of Artificial Intelligence in
   General Colleges and Universities of Guangdong Province (Grant No.
   2019KZDZX1012).},
Cited-References = {{[}Anonymous], 1967, P 5 BERKELEY S MATH.
   Bochkovskiy A., 2020, YOLOV4 OPTIMAL SPEED.
   {[}邓颖 Deng Ying], 2020, {[}农业工程学报, Transactions of the Chinese Society of Agricultural Engineering], V36, P200.
   DeVries T., 2017, ARXIV170804552.
   Gao Zong, 2018, Computer Engineering, V44, P215, DOI 10.19678/j.issn.1000-3428.0046885.
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169.
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81.
   Guan Y, 2019, J COMPUT ENG APPL, V55, P174.
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165.
   Han S., 2015, DEEP COMPRESSION COM.
   Hinton G., 2015, DISTILLING KNOWLEDGE.
   Howard A. G., 2017, EFFICIENT CONVOLUTIO.
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140.
   Iandola F.N., 2016, ARXIV160207360, DOI DOI 10.48550/ARXIV.1602.07360.
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1.
   Khan S, 2021, PRECIS AGRIC, V22, P1711, DOI 10.1007/s11119-021-09808-9.
   Li WJ, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9010022.
   Liu CC, 2018, FUNCT ECOL, V32, P20, DOI 10.1111/1365-2435.12973.
   Liu J, 2021, PLANT METHODS, V17, DOI 10.1186/s13007-021-00708-7.
   Liu Y., 2021, IEEE INTERNET THINGS, DOI {[}10.1109/JIOT.2021.3099028, DOI 10.1109/JIOT.2021.3099028].
   Liu YX, 2022, IEEE INTERNET THINGS, V9, P11385, DOI 10.1109/JIOT.2021.3126819.
   Liu YX, 2021, IEEE IPCCC, DOI 10.1109/IPCCC51483.2021.9679426.
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9\_8.
   Niu ST, 2022, ACM TRANS MANAG INF, V13, DOI 10.1145/3464324.
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281.
   Qin Z, 2019, IEEE I CONF COMP VIS, P6717, DOI 10.1109/ICCV.2019.00682.
   Redmon J, ARXIV.
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Shen S, 2020, AAAI CONF ARTIF INTE, V34, P8815.
   Shuteng Niu, 2020, IEEE Transactions on Artificial Intelligence, V1, P151, DOI 10.1109/TAI.2021.3054609.
   Tan M, 2021, ARXIV210400298, V139, P1.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Tu SQ, 2020, PRECIS AGRIC, V21, P1072, DOI 10.1007/s11119-020-09709-3.
   Wang K, 2022, APPL INTELL, V52, P2070, DOI 10.1007/s10489-021-02491-3.
   Wang M., 2021, Identification Method of Citrus Red Spider Pests Based on Deep Learning, Patent No. {[}CN112597907A, 112597907].
   Wang X., 2020, THESIS WUHAN U LIGHT.
   Wang Yu-ning, 2016, Journal of Wuhan University of Technology, V38, P41, DOI 10.3963/j.issn.1671-4431.2016.10.008.
   Xu Y., 2019, INT C INTELLIGENT IN, P664.
   Xu ZF, 2020, APPL INTELL, V50, P4670, DOI 10.1007/s10489-020-01818-w.
   Yang J.Y., 2019, ENG J HEILONGJIANG U, V10, P90.
   Zafrir Ofir, 2019, 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS), P36, DOI 10.1109/EMC2-NIPS53020.2019.00016.
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716.
   Zhao D. A., 2019, T CHIN SOC AGR ENG, V35, P172.
   Zhou BG, 2019, PROC CIRP, V83, P675, DOI 10.1016/j.procir.2019.04.107.},
Number-of-Cited-References = {46},
Times-Cited = {2},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {21},
Journal-ISO = {Sensors},
Doc-Delivery-Number = {XV7AG},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000735089700001},
OA = {gold, Green Published},
DA = {2023-08-12},
}

@article{ WOS:000404059400021,
Author = {Servajean, Maximilien and Joly, Alexis and Shasha, Dennis and Champ,
   Julien and Pacitti, Esther},
Title = {Crowdsourcing Thousands of Specialized Labels: A Bayesian Active
   Training Approach},
Journal = {IEEE TRANSACTIONS ON MULTIMEDIA},
Year = {2017},
Volume = {19},
Number = {6},
Pages = {1376-1391},
Month = {JUN},
Abstract = {Large-scale annotated corpora have yielded impressive performance
   improvements in computer vision and multimedia content analysis.
   However, such datasets depend on an enormous amount of human labeling
   effort. When the labels correspond to well-known concepts, it is
   straightforward to train the annotators by giving a few examples with
   known answers. It is also straightforward to judge the quality of their
   labels. Neither is true when there are thousands of complex
   domain-specific labels. Training on all labels is infeasible and the
   quality of an annotator's judgements may be vastly different for some
   subsets of labels than for others. This paper proposes a set of
   data-driven algorithms to 1) train image annotators on how to
   disambiguate among automatically generated candidate labels, 2) evaluate
   the quality of annotators' label suggestions, and 3) weigh predictions.
   The algorithms adapt to the skills of each annotator both in the
   questions asked and the weights given to their answers. The underlying
   judgements are Bayesian, based on adaptive priors. We measure the
   benefits of these algorithms on a live user experiment related to
   image-based plant identification involving around 1000 people. The
   proposed methods are shown to enable huge gains in annotation accuracy.
   A standard user can correctly label around 2\% of our data. This goes up
   to 80\% with machine learning assisted training and assignment and up to
   almost 90\% when doing a weighted combination of several annotators'
   labels.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Servajean, M (Corresponding Author), INRIA, LIRMM, Zenith Team, F-34095 Montpellier, France.
   Servajean, Maximilien; Joly, Alexis; Champ, Julien; Pacitti, Esther, INRIA, LIRMM, Zenith Team, F-34095 Montpellier, France.
   Shasha, Dennis, NYU, Dept Comp Sci, 550 1St Ave, New York, NY 10003 USA.
   Shasha, Dennis, INRIA, F-34095 Montpellier, France.},
DOI = {10.1109/TMM.2017.2653763},
ISSN = {1520-9210},
EISSN = {1941-0077},
Keywords = {Crowdsourcing; Bayes methods; parameter estimation; Taylor series},
Research-Areas = {Computer Science; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications},
Author-Email = {servajean@lirmm.fr
   alexis.joly@inria.fr
   shasha@courant.nyu.edu
   champ@lirmm.fr
   pacitti@lirmm.fr},
Affiliations = {Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; Inria; New York University; Inria},
ResearcherID-Numbers = {Servajean, Maximilien/IQW-9683-2023
   Champ, Julien/AAG-1092-2020
   joly, alexis/AAV-3101-2021
   },
ORCID-Numbers = {Servajean, Maximilien/0000-0002-9426-2583
   joly, alexis/0000-0002-2161-9940
   Champ, Julien/0000-0002-2042-0411},
Funding-Acknowledgement = {INRIA International Chair},
Funding-Text = {The work of D. Shasha was supported by the INRIA International Chair.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Dong Xu.},
Cited-References = {{[}Anonymous], 2013, DECISION MAKING IMPE, DOI DOI 10.1007/978-3-642-36406-8\_1.
   Basu S., 2013, P AAAI C ARTIFICIAL, P109.
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032.
   Borne Kirk D., 2011, AGU FALL M, V1, P650.
   Bragg J, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS \& MULTIAGENT SYSTEMS, P966.
   Champ J., 2015, P C LABS EV FOR, V1391.
   Champ J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P721, DOI 10.1145/2647868.2654875.
   DANTZIG GB, 1957, OPER RES, V5, P266, DOI 10.1287/opre.5.2.266.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Fang M, 2012, IEEE DATA MINING, P858, DOI 10.1109/ICDM.2012.64.
   Gottlieb L, 2014, IEEE T MULTIMEDIA, V16, P2075, DOI 10.1109/TMM.2014.2347268.
   Ipeirotis P., 2011, P 8 INT WORKSH INF I, P1.
   Joly A, 2016, MULTIMEDIA SYST, V22, P751, DOI 10.1007/s00530-015-0462-9.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kamar E., 2012, INT C AUT AG MULT SY.
   Kim H.-C., 2012, PROC ARTIF INTELL ST, P619.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Liu D, 2011, IEEE T MULTIMEDIA, V13, P702, DOI 10.1109/TMM.2011.2134078.
   Parameswaran A. G., 2012, SIGMOD, P361, DOI DOI 10.1145/2213836.2213878.
   Parameswaran A, 2011, PROC VLDB ENDOW, V4, P267, DOI 10.14778/1952376.1952377.
   Quinn A.J., 2010, HCIL201009 U MAR.
   Rahman H., 2016, CORR.
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297.
   Roberts, 2015, DECISION MAKING UNCE, P1.
   Roy SB, 2015, VLDB J, V24, P467, DOI 10.1007/s00778-015-0385-2.
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8.
   SAHNI S, 1976, J ACM, V23, P555, DOI 10.1145/321958.321975.
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782.
   Sheng V. S., 2008, P 14 ACM SIGKDD INT, P614.
   Skene A. M, 1979, J ROY STAT SOC SER A, P20, DOI DOI 10.2307/2346806.
   Szegedy C, 2015, PROC CVPR IEEE, P1.
   Thomee B., 2015, CORR.
   Tulyakov S, 2008, STUD COMPUT INTELL, V90, P361.
   Venanzi M., 2013, P AAMAS 2013, P901.
   Venanzi M, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P155, DOI 10.1145/2566486.2567989.
   Wang C, 2013, J MACH LEARN RES, V14, P1005.
   Welinder P., 2010, 2010 IEEE COMP SOC C, P25, DOI {[}10.1109/CVPRW.2010.5543189, DOI 10.1109/CVPRW.2010.5543189].},
Number-of-Cited-References = {38},
Times-Cited = {5},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Journal-ISO = {IEEE Trans. Multimedia},
Doc-Delivery-Number = {EY5YS},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000404059400021},
OA = {Green Submitted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000256425000087,
Author = {Ishak, Asnor Juraiza and Mokri, Siti Salasiah and Mustafa, Mohd Marzuki
   and Hussain, Aini},
Book-Group-Author = {IEEE},
Title = {Weed detection utilizing quadratic polynomial and ROI techniques},
Booktitle = {2007 5TH STUDENT CONFERENCE ON RESEARCH AND DEVELOPMENT},
Series = {IEEE Student Conference on Research and Development SCOReD},
Year = {2007},
Pages = {437+},
Note = {5th Student Conference on Research and Development, Selangor, MALAYSIA,
   DEC 11-12, 2007},
Organization = {IEEE},
Abstract = {Machine vision for selective weeding or selective herbicide spraying
   relies substantially on the ability of the system to analyze weed images
   and process the extracted knowledge for decision making prior to
   implementing the identified control action. To control weed, different
   weed type would require different herbicide formulation. Consequently
   the weed must be identified and classified accordingly. In this work,
   weed images were classified as either broad or narrow weed type. A
   fundamental problem in weed image recognition using planar curve
   analysis is to detect curve. It is difficult to successfully extract
   curve from the image of weed edges since the appropriate scale to use
   for extraction is not known a priori As such, this paper considers a
   curve detection method based on the quadratic polynomial technique which
   include the use of the region-of-interests (ROI) technique. The ROI
   technique creates image subsets by selecting regions of the displayed
   image. The ROIs are typically used to extract statistics for image
   operations such as classification. As such, the objective of this paper
   is to present a novel application of curve detection feature extraction
   technique in weed classification.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Ishak, AJ (Corresponding Author), Univ Putra Malaysia, Fac Engn, Dept Elect \& Elect Engn, Control Unit, Serdang 43300, Selangor, Malaysia.
   Ishak, Asnor Juraiza, Univ Putra Malaysia, Fac Engn, Dept Elect \& Elect Engn, Control Unit, Serdang 43300, Selangor, Malaysia.
   Mokri, Siti Salasiah; Mustafa, Mohd Marzuki; Hussain, Aini, Univ Kembangsaan, Dept Elect Elect \& Syst Engn, Fac Engn, Bangi 43600, Malaysia.},
ISSN = {2643-2439},
EISSN = {2643-2447},
ISBN = {978-1-4244-1469-7},
Keywords = {plant identification; pattern recognition; region of interest; feature
   extraction; neural network},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {asnodi@eng.upm.edu.my
   siti1950@vlsi.eng.ukm.my
   marzuki@vlsi.eng.ukm.my
   aini@vlsi.eng.ukm.my},
Affiliations = {Universiti Putra Malaysia},
ResearcherID-Numbers = {Hussain, Aini/G-4074-2011
   Hussain, A./D-6915-2017
   mustafa, marzuki/AHA-9319-2022},
ORCID-Numbers = {Hussain, A./0000-0001-7347-7879
   },
Funding-Acknowledgement = {Department of Electrical, Electronic and System; Universiti Kebangsaan
   Malaysia},
Funding-Text = {This work was supported in part by the Department of Electrical,
   Electronic and System, Universiti Kebangsaan Malaysia},
Cited-References = {AGILI S, ADAPTIVE METHOD REGI.
   {[}Anonymous], 2000, PATTERN CLASSIFICATI.
   Clark A, 1999, TRENDS COGN SCI, V3, P345, DOI 10.1016/S1364-6613(99)01361-3.
   Cong G, 1998, PATTERN RECOGN, V31, P1491, DOI 10.1016/S0031-3203(98)00003-X.
   GINKEL M, P 2000 7 ANN C ADV S, P299.
   HONGMEI G, P 2004 INT WORKSH DI, P13.
   JACK S, 1978, IEEE T COMPUT, V27, P923.
   JUNG J, P 2000 INT C CONS EL, V1, P62.
   KIM YO, P 2003 IEEE C ADV VI, P29.
   LEE WS, 1998, ASAE ANN M JUL.
   MANG C, P 2004 17 INT C PATT, P1051.
   Mark SN., 2002, FEATURE EXTRACTION I.
   MICHAEL K, P 2004 TEL COMP SCI, P223.
   Rafael C.G., 1993, DIGITAL IMAGE PROCES.
   SEUL M, 2001, PRACTICAL ALGORITHMS.
   Xu T, 2003, RADIOLOGY, V226, P585, DOI 10.1148/radiol.2262011812.},
Number-of-Cited-References = {16},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BHU35},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000256425000087},
DA = {2023-08-12},
}

@article{ WOS:000427460700009,
Author = {Botella, Christophe and Joly, Alexis and Bonnet, Pierre and Monestiez,
   Pascal and Munoz, Francois},
Title = {Species distribution modeling based on the automated identification of
   citizen observations},
Journal = {APPLICATIONS IN PLANT SCIENCES},
Year = {2018},
Volume = {6},
Number = {2},
Month = {FEB},
Abstract = {PREMISE OF THE STUDY: A species distribution model computed with
   automatically identified plant observations was developed and evaluated
   to contribute to future ecological studies.
   METHODS: We used deep learning techniques to automatically identify
   opportunistic plant observations made by citizens through a popular
   mobile application. We compared species distribution modeling of
   invasive alien plants based on these data to inventories made by
   experts.
   RESULTS: The trained models have a reasonable predictive effectiveness
   for some species, but they are biased by the massive presence of
   cultivated specimens.
   DISCUSSION: The method proposed here allows for fine-grained and regular
   monitoring of some species of interest based on opportunistic
   observations. More in-depth investigation of the typology of the
   observations and the sampling bias should help improve the approach in
   the future.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Bonnet, P (Corresponding Author), Univ Montpellier, IRD, Ctr Cooperat Int Rech Agron Dev CIRAD,AMAP,INRA, French Natl Ctr Sci Res, Montpellier, France.
   Bonnet, P (Corresponding Author), UMR AMAP, CIRAD, F-34398 Montpellier, France.
   Botella, Christophe; Joly, Alexis, Inst Natl Rech Informat \& Automat INRIA Sophia An, ZENITH Team, Lab Informat Robot \& Microelect, Joint Res Unit 5506 CC 477, 161 Rue Ada, F-34095 Montpellier 5, France.
   Botella, Christophe, INRA, Joint Res Unit, Bot \& Modelisat Architecture Plantes \& Vegetat, UMR AMAP, F-34398 Montpellier, France.
   Botella, Christophe; Bonnet, Pierre, Univ Montpellier, IRD, French Natl Ctr Sci Res, Ctr Cooperat Int Rech Agron Dev CIRAD,AMAP,INRA, Montpellier, France.
   Botella, Christophe; Monestiez, Pascal, INRA, BioSP, Site Agroparc, F-84914 Avignon, France.
   Bonnet, Pierre, UMR AMAP, CIRAD, F-34398 Montpellier, France.
   Munoz, Francois, Univ Grenoble Alpes, Lab Ecol Alpine, CS 40700, F-38058 Grenoble, France.},
DOI = {10.1002/aps3.1029},
Article-Number = {e1029},
ISSN = {2168-0450},
Keywords = {automated species identification; citizen science; crowdsourcing; deep
   learning; invasive alien species; species distribution modeling},
Keywords-Plus = {PLANT-IDENTIFICATION},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {pierre.bonnet@cirad.fr},
Affiliations = {CIRAD; Centre National de la Recherche Scientifique (CNRS); Institut de
   Recherche pour le Developpement (IRD); Universite de Montpellier; INRAE;
   CIRAD; INRAE; Institut de Recherche pour le Developpement (IRD);
   Universite de Montpellier; INRAE; CIRAD; Centre National de la Recherche
   Scientifique (CNRS); Institut de Recherche pour le Developpement (IRD);
   Universite de Montpellier; UDICE-French Research Universities;
   Communaute Universite Grenoble Alpes; Universite Grenoble Alpes (UGA);
   Centre National de la Recherche Scientifique (CNRS); Universite de
   Savoie},
ResearcherID-Numbers = {joly, alexis/AAV-3101-2021
   BOTELLA, Christophe/AAJ-3667-2021
   Bonnet, Pierre/AAG-6819-2020},
ORCID-Numbers = {joly, alexis/0000-0002-2161-9940
   Bonnet, Pierre/0000-0002-2828-4389},
Cited-References = {Affouard A., 2017, 5 INT C LEARN REPR 2.
   Allouche O, 2006, J APPL ECOL, V43, P1223, DOI 10.1111/j.1365-2664.2006.01214.x.
   Carranza-Rojas J, 2017, BMC EVOL BIOL, V17, P1, DOI 10.1186/s12862-017-1014-z.
   Casanova D, 2009, INT J IMAG SYST TECH, V19, P236, DOI 10.1002/ima.20201.
   Champ J, 2016, PATTERN RECOGN LETT, V81, P71, DOI 10.1016/j.patrec.2016.05.022.
   Conservatoire botanique national mediterraneen de Porquerolles, 2018, ESP VEG EX ENV EVEE.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Dutreve B., 2016, INPN INVENTAIRE NATL.
   Fithian W, 2013, ANN APPL STAT, V7, P1917, DOI 10.1214/13-AOAS667.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Giraud C, 2016, BIOMETRICS, V72, P649, DOI 10.1111/biom.12431.
   Goeau H., 2016, WORKING NOTES CLEF 2, P428.
   Goeau H., 2017, CEUR WORKSHOP P.
   Goodfellow I., 2016, DEEP LEARNING, V1.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123.
   Ioffe S., 2015, INT C MACHINE LEARNI, DOI DOI 10.1007/S13398-014-0173-7.2.
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Joly A, 2016, MULTIMEDIA SYST, V22, P751, DOI 10.1007/s00530-015-0462-9.
   Karger DN, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.122.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Merow C, 2013, ECOGRAPHY, V36, P1058, DOI 10.1111/j.1600-0587.2013.07872.x.
   Panagos P., 2006, GEOCONNEXION, V5, P32.
   Panagos P, 2012, LAND USE POLICY, V29, P329, DOI 10.1016/j.landusepol.2011.07.003.
   Phillips S. J., 2004, P 21 INT C MACH LEAR, P655, DOI DOI 10.1145/1015330.1015412.
   Phillips SJ, 2006, ECOL MODEL, V190, P231, DOI 10.1016/j.ecolmodel.2005.03.026.
   Phillips SJ, 2017, ECOGRAPHY, V40, P887, DOI 10.1111/ecog.03049.
   Stolar J, 2015, DIVERS DISTRIB, V21, P595, DOI 10.1111/ddi.12279.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Van Liedekerke M., 2006, ESDBV2 RASTER LIB SE.
   Waldchen J., 2017, ARCH COMPUTATIONAL M.
   Warton DI, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079168.
   Weber Ewald, 2004, Journal for Nature Conservation (Jena), V12, P171, DOI 10.1016/j.jnc.2004.04.002.
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197.
   Zomer R. J., 2007, TREES WATER SMALLHOL.
   Zorner RJ, 2008, AGR ECOSYST ENVIRON, V126, P67, DOI 10.1016/j.agee.2008.01.014.},
Number-of-Cited-References = {39},
Times-Cited = {21},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {20},
Journal-ISO = {Appl. Plant Sci.},
Doc-Delivery-Number = {FZ3BK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000427460700009},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000680655600123,
Author = {Lv, Rongxin and Li, Zhongzhi and Zuo, Jiankai and Liu, Jing},
Book-Group-Author = {IEEE},
Title = {Flower Classification and Recognition Based on Significance Test and
   Transfer Learning},
Booktitle = {2021 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS AND COMPUTER
   ENGINEERING (ICCECE)},
Year = {2021},
Pages = {649-652},
Note = {IEEE International Conference on Consumer Electronics and Computer
   Engineering (ICCECE), Guangzhou, PEOPLES R CHINA, JAN 15-17, 2021},
Organization = {IEEE},
Abstract = {timing at the problem that traditional flower classification methods and
   ordinary convolutional neural networks are difficult to reduce the
   effect of flower background, the classification effect is not ideat.
   This paper designs a flower classification model that combines saliency
   detection and VGG-convolutional neural network, and adopts stochastic
   gradient descent algorithm and presents over-fitting technology to
   impasse the model. Experiments on the international public flower
   recomition data set Oxford flower-102 show that the model proposed in
   this paper is better than other traditional network models and has high
   recognition accuracy , robustness and generalization ability, which can
   classify flowers and have higher practical value accurately and quickly.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Li, ZZ (Corresponding Author), Shenyang Aerosp Univ, Sch Comp Sci, Shenyang 110136, Peoples R China.
   Lv, Rongxin, Shenyang Aerosp Univ, Sch Innovat \& Entrepreneurship, Shenyang 110136, Peoples R China.
   Li, Zhongzhi; Liu, Jing, Shenyang Aerosp Univ, Sch Comp Sci, Shenyang 110136, Peoples R China.
   Zuo, Jiankai, Tongji Univ, Dept Comp Sci \& Technol, Shanghai 201804, Peoples R China.},
DOI = {10.1109/ICCECE51280.2021.9342468},
ISBN = {978-1-7281-8319-0},
Keywords = {Deep Learning; Transfer Learning; Significancy Detecting; Flower
   recognition},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications; Engineering,
   Electrical \& Electronic; Telecommunications},
Author-Email = {lrxin@msn.com
   lizhongzhi811116@gmail.com},
Affiliations = {Shenyang Aerospace University; Shenyang Aerospace University; Tongji
   University},
ResearcherID-Numbers = {Zuo, Jiankai/ABF-7287-2021},
ORCID-Numbers = {Zuo, Jiankai/0000-0002-4026-134X},
Funding-Acknowledgement = {2020 Provincial College Student Innovation and Entrepreneurship Training
   Program {[}S202010143042]},
Funding-Text = {This research was supported by 2020 Provincial College Student
   Innovation and Entrepreneurship Training Program (Project Number:
   S202010143042).},
Cited-References = {Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3\_16.
   Gao Xiang, 2018, FLOWER IMAGE CLASSIF.
   HOU Xiao-Di, 2007, P IEEE C COMP VIS PA P IEEE C COMP VIS PA.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Pei Xiaofang, 2020, Chinese Journal of Electron Devices, P698, DOI 10.3969/j.issn.1005-9490.2020.03.041.
   Qu N, 2020, IEEE ACCESS, V8, P87060, DOI 10.1109/ACCESS.2020.2992790.
   Simonyan K., 2014, 14091556V6 ARXIV, P1, DOI DOI 10.48550/ARXIV.1409.1556.
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
   Wu Di, 2019, J HENAN U NATURAL SC, V49, P192.
   Wu Y, 2021, INT J CONTROL, V94, P999, DOI 10.1080/00207179.2019.1626024.
   Yin H., 2020, MATH PROBL ENG.
   {[}尹红 Yin Hong], 2019, {[}中国图象图形学报, Journal of Image and Graphics], V24, P762.
   Zhou Wei, 2011, COMPUTER TECHNOLOGY, V21.
   Zuo J., LECT NOTES ELECT ENG LECT NOTES ELECT ENG, V551.},
Number-of-Cited-References = {14},
Times-Cited = {5},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {16},
Doc-Delivery-Number = {BS0AM},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000680655600123},
DA = {2023-08-12},
}

@article{ WOS:000509483800064,
Author = {Yang, Chengzhuan and Wei, Hui},
Title = {Plant Species Recognition Using Triangle-Distance Representation},
Journal = {IEEE ACCESS},
Year = {2019},
Volume = {7},
Pages = {178108-178120},
Abstract = {Plant species recognition using leaf images is a highly important and
   challenging issue in botany and pattern recognition. A center problem of
   this task is how to accurately extract leaf image characteristics and
   quickly calculate the similarity between them. This article presents a
   new shape description approach called triangle-distance representation
   (TDR) for plant leaf recognition. The IDR descriptor is represented by
   two matrices: a sign matrix and a triangle center distance matrix. The
   sign matrix is used to characterize the convex/concave property of a
   shape contour, while the triangle center distance matrix is used to
   represent the bending degree and spatial information of a shape contour.
   This method can effectively capture the detailed and global
   characteristics of a leaf shape while keeping the similarity
   transformations (translation, rotation, and scaling) unchanged. In
   addition, this method is quite compact and has low computational
   complexity. We tested our method on four standard plant leaf datasets,
   including the famous Swedish, Smithsonian, Flavia, and ImageCLEF2012
   datasets. The results confirm that our approach exceeds the prior
   state-of-the-art shape-based plant leaf recognition approaches. An extra
   experiment on the MPEG-7 shape dataset further shows that our method can
   be applied to general shape recognition.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Yang, CZ (Corresponding Author), Zhejiang Univ Finance \& Econ, Sch Informat, Hangzhou 310018, Peoples R China.
   Yang, Chengzhuan, Zhejiang Univ Finance \& Econ, Sch Informat, Hangzhou 310018, Peoples R China.
   Wei, Hui, Fudan Univ, Sch Comp Sci, Lab Cognit Algorithm \& Model, Shanghai Key Lab Data Sci, Shanghai 201203, Peoples R China.},
DOI = {10.1109/ACCESS.2019.2958416},
ISSN = {2169-3536},
Keywords = {Plant species recognition; shape matching; triangle-distance
   representation; shape descriptor},
Keywords-Plus = {CLASSIFICATION; IDENTIFICATION; IMAGES; PROJECTION; SHAPES; LEAVES},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {chengzhuanyang@zufe.edu.cn},
Affiliations = {Zhejiang University of Finance \& Economics; Fudan University},
ResearcherID-Numbers = {Yang, Chengzhuan/GXF-1310-2022
   Wei, Hui/K-5819-2019
   },
ORCID-Numbers = {Yang, Chengzhuan/0000-0003-4340-506X
   Wei, Hui/0000-0003-2696-0707},
Funding-Acknowledgement = {Zhejiang Provincial Natural Science Foundation of China {[}LQ19F020003];
   National Natural Science Foundation of China {[}61375122, 61902159];
   Shanghai Science and Technology Development Funds {[}13dz2260200,
   13511504300]},
Funding-Text = {This work was supported in part by the Zhejiang Provincial Natural
   Science Foundation of China under Grant LQ19F020003, in part by the
   National Natural Science Foundation of China under Grant 61375122 and
   Grant 61902159, and in part by the Shanghai Science and Technology
   Development Funds under Grant 13dz2260200 and Grant 13511504300.},
Cited-References = {{[}Anonymous], 2007, 2007 IEEE C COMP VIS, DOI {[}10.1109/CVPR.2007.383018, DOI 10.1109/CVPR.2007.383018].
   Aptoula E, 2013, IEEE IMAGE PROC, P1496, DOI 10.1109/ICIP.2013.6738307.
   Aranda M. C., 2010, P ACM INT C IM VID R, P327, DOI {[}10.1145/1816041.1816089, DOI 10.1145/1816041.1816089].
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
   Cerutti G, 2013, COMPUT VIS IMAGE UND, V117, P1482, DOI 10.1016/j.cviu.2013.07.003.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Du JX, 2013, NEUROCOMPUTING, V116, P150, DOI 10.1016/j.neucom.2012.03.028.
   Fan JP, 2015, IEEE T IMAGE PROCESS, V24, P4172, DOI 10.1109/TIP.2015.2457337.
   Goeau H., 2012, P CLEF ROM IT SEP, p1\~{}25.
   Hearn DJ, 2009, TAXON, V58, P934, DOI 10.1002/tax.583021.
   Horaisova K, 2016, BIOSYST ENG, V142, P83, DOI 10.1016/j.biosystemseng.2015.12.007.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lagar-Cavilla H. A., 2009, 2012 INT C DIG IM CO, P1, DOI DOI 10.1109/DICTA.2012.6411702.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850.
   Lei YK, 2014, COMPUT VIS IMAGE UND, V119, P116, DOI 10.1016/j.cviu.2013.12.001.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Martinez J. M., 2003, N5525 INT ORG STAND.
   Mouine S., 2012, ICMR, P1, DOI DOI 10.1145/2324796.2324853.
   Mouine S, 2013, IEEE IMAGE PROC, P1466, DOI 10.1109/ICIP.2013.6738301.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Shao Y, 2019, COMPUT ELECTRON AGR, V158, P102, DOI 10.1016/j.compag.2019.01.022.
   Singh UP, 2019, IEEE ACCESS, V7, P43721, DOI 10.1109/ACCESS.2019.2907383.
   Soderkvist O., 2001, THESIS.
   Turkoglu M, 2019, APPL MATH COMPUT, V352, P1, DOI 10.1016/j.amc.2019.01.054.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yang CZ, 2016, FRONT ARTIF INTEL AP, V285, P269, DOI 10.3233/978-1-61499-672-9-269.
   Zhang SW, 2018, APPL SOFT COMPUT, V67, P164, DOI 10.1016/j.asoc.2018.02.052.
   Zhang SW, 2013, PATTERN RECOGN, V46, P1891, DOI 10.1016/j.patcog.2013.01.015.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zisserman A, 2015, ICLR 2015 INT C LEAR, P1097.},
Number-of-Cited-References = {42},
Times-Cited = {12},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {KF8JT},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000509483800064},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000426901900155,
Author = {Muthevi, Anilkumar and Uppu, Ravi Babu},
Editor = {Sai, YP and Garg, D},
Title = {Leaf Classification Using Completed Local Binary Pattern Of Textures},
Booktitle = {2017 7TH IEEE INTERNATIONAL ADVANCE COMPUTING CONFERENCE (IACC)},
Series = {IEEE International Advance Computing Conference},
Year = {2017},
Pages = {870-874},
Note = {7th IEEE International Advance Computing Conference (IACC), VNR Vignana
   Jyothi Insti Engn \& Technol, Hyderabad, INDIA, JAN 05-07, 2017},
Organization = {IEEE; Indian Soc Tech Educ; ATL; Tata Consultancy Serv; Avantel;
   Synergy; Princeton Review},
Abstract = {This paper, introduces utilizing the magnitude component of Local Binary
   Pattern (LBP) apart from sign component (which is considered as
   conventional method). We applied this Completed Local Binary Pattern
   (CLBP) on plant leaf classification by randomly taken divergent blocks
   of each texture data set. This approach is also useful for the
   identification of quality leaves for the automation of grading process
   in commercial crops like Tobacco etc. By combining Center pixel CLBP
   (CCLBP), Signed component of CLBP (SCLBP) and magnitude part of CLBP
   (MCLBP) there is a considerable development can be achieved for
   rotationally invariant texture classification.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Muthevi, A (Corresponding Author), Acharya Nagarjuna Univ, Aditya Coll Engn \& Technol, Dept Comp Sci \& Engn, Surampalem, AP, India.
   Muthevi, Anilkumar, Acharya Nagarjuna Univ, Aditya Coll Engn \& Technol, Dept Comp Sci \& Engn, Surampalem, AP, India.
   Uppu, Ravi Babu, Narsimha Reddy Engn Coll, Dept Comp Sci \& Engn, Secunderabad, TS, India.},
DOI = {10.1109/IACC.2017.169},
ISSN = {2164-8263},
ISBN = {978-1-5090-1560-3},
Keywords = {Local Binary Pattern; Leaf Recognition; Rotation invariance; Image
   processing; Feature selection; Texture classification},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Author-Email = {lettertoanil@gmail.com
   uppu.ravibabu@gmail.com},
Affiliations = {Acharya Nagarjuna University},
ResearcherID-Numbers = {Muthevi, Anilkumar/ABF-1516-2021
   },
ORCID-Numbers = {Muthevi, Anil kumar/0000-0003-1195-4216},
Cited-References = {Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244.
   Anilkumar M., P ICETSETM 2015, P387.
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   KASHYAP RL, 1986, IEEE T PATTERN ANAL, V8, P472, DOI 10.1109/TPAMI.1986.4767811.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Ojala T, 2002, INT C PATT RECOG, P701, DOI 10.1109/ICPR.2002.1044854.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Ojala T, 2001, PATTERN RECOGN, V34, P727, DOI 10.1016/S0031-3203(00)00010-8.
   Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261.
   Soderkvist O, 2001, COMPUTER VISION CLAS.
   Sule Milan, 2014, 10 CMP CZECH TU PRAG.
   Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4.
   Varma M, 2003, PROC CVPR IEEE, P691.
   Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182.
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110.},
Number-of-Cited-References = {16},
Times-Cited = {6},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BJ6PC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000426901900155},
DA = {2023-08-12},
}

@article{ WOS:000784322500023,
Author = {Pushpanathan, Kalananthni and Hanafi, Marsyita and Masohor, Syamsiah and
   Ilahi, Wan Fazilah Fazlil},
Title = {MYLPHerb-1: A Dataset of Malaysian Local Perennial Herbs for the Study
   of Plant Images Classification under Uncontrolled Environment},
Journal = {PERTANIKA JOURNAL OF SCIENCE AND TECHNOLOGY},
Year = {2022},
Volume = {30},
Number = {1},
Pages = {413-431},
Month = {JAN},
Abstract = {Research in the medicinal plants' recognition field has received great
   attention due to the need of producing a reliable and accurate system
   that can recognise medicinal plants under various imaging conditions.
   Nevertheless, the standard medicinal plant datasets publicly available
   for research are very limited. This paper proposes a dataset consisting
   of 34200 images of twelve different high medicinal value local perennial
   herbs in Malaysia. The images were captured under various imaging
   conditions, such as different scales, illuminations, and angles. It will
   enable larger interclass and intraclass variability, creating abundant
   opportunities for new findings in leaf classification. The complexity of
   the dataset is investigated through automatic classification using
   several high-performance deep learning algorithms. The experiment
   results showed that the dataset creates more opportunities for advanced
   classification research due to the complexity of the images. The dataset
   can be accessed through https://www.mylpherbs.com/.},
Publisher = {UNIV PUTRA MALAYSIA PRESS},
Address = {SERDANG, SELANGOR, 00000, MALAYSIA},
Type = {Article},
Language = {English},
Affiliation = {Hanafi, M (Corresponding Author), Univ Putra Malaysia, Fac Engn, Dept Comp \& Commun Syst Engn, Serdang 43400, Selangor Darul, Malaysia.
   Pushpanathan, Kalananthni; Hanafi, Marsyita; Masohor, Syamsiah, Univ Putra Malaysia, Fac Engn, Dept Comp \& Commun Syst Engn, Serdang 43400, Selangor Darul, Malaysia.
   Ilahi, Wan Fazilah Fazlil, Univ Putra Malaysia, Fac Agr, Dept Agr Technol, Serdang 43400, Selangor Darul, Malaysia.},
DOI = {10.47836/pjst.30.1.23},
ISSN = {0128-7680},
Keywords = {Deep learning; leaf identification; medicinal plants; perennial herbs;
   plant dataset},
Keywords-Plus = {ANDROGRAPHIS-PANICULATA},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {kalananthni19@gmail.com
   marsyita@upm.edu.my
   syamsiah@upm.edu.my
   wanfazilah@upm.edu.my},
Affiliations = {Universiti Putra Malaysia; Universiti Putra Malaysia},
Funding-Acknowledgement = {Universiti Putra Malaysia under Putra Graduate Initiative {[}:9529000,
   9634300]},
Funding-Text = {The authors wish to acknowledge the financial support received from
   Universiti Putra Malaysia under Putra Graduate Initiative (Vot
   number:9529000 and 9634300) and colleagues for their insight and
   expertise.},
Cited-References = {Abdelwahab SI, 2011, EVID-BASED COMPL ALT, V2011, DOI 10.1093/ecam/neq010.
   Alam MA, 2016, ASIAN PAC J TROP MED, V9, P393, DOI 10.1016/j.apjtm.2016.03.011.
   Arun C. H., 2013, INT J COMPUTER APPL, V62, P1, DOI 10.5120/10129-4920.
   Ashaari NS, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0235416.
   Ashraf K, 2020, SAUDI J BIOL SCI, V27, P417, DOI 10.1016/j.sjbs.2019.11.003.
   Ashraf K, 2018, J PHARM BIOALLIED SC, V10, P109, DOI {[}10.4103/JPBS.JPBS\_253\_17, 10.4103/jpbs.JPBS\_253\_17].
   Begue A, 2017, INT J ADV COMPUT SC, V8, P166.
   Bhatt P, 2013, J CHEM-NY, V2013, DOI 10.1155/2013/320329.
   Christapher PV, 2015, PHARMACOGN RES, V7, P1, DOI 10.4103/0974-8490.147125.
   Dahigaonkar T.D., 2018, INT RES J ENG TECHNO, V5, P351.
   Deshpande P, 2017, WORLD J PHARM RES, P1335, DOI {[}10.20959/wjpr20176-8658, DOI 10.20959/WJPR20176-8658].
   Giribabu N, 2020, INFLAMMOPHARMACOLOGY, V28, P1599, DOI 10.1007/s10787-020-00733-3.
   Gohil KJ, 2010, INDIAN J PHARM SCI, V72, P546, DOI 10.4103/0250-474X.78519.
   Habiba S. U., 2019, 2019 INT C COMP COMM, P1, DOI {[}10.1109/ IC4ME247184.2019.9036515, DOI 10.1109/IC4ME247184.2019.9036515].
   Haida Z, 2020, PLANTS-BASEL, V9, DOI 10.3390/plants9081030.
   Harsani P, 2016, JURNAL ILMIAH KURSOR, V8, P181, DOI {[}10.28961/kursor.v8i4.112, DOI 10.28961/KURSOR.V8I4.112].
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Janani R., 2013, 2013 International Conference on Advanced Electronic Systems (ICAES), P238, DOI 10.1109/ICAES.2013.6659400.
   Karthika S, 2020, INT J GREEN PHARM IJ, V14, P175.
   Khoo LW, 2018, EVID-BASED COMPL ALT, V2018, DOI 10.1155/2018/9276260.
   Kurzawa M, 2015, J CHROMATOGR B, V995, P101, DOI 10.1016/j.jchromb.2015.05.021.
   Lau H, 2020, BMC COMPLEMENT MED, V20, DOI 10.1186/s12906-020-03092-2.
   Lulekal E, 2008, J ETHNOBIOL ETHNOMED, V4, DOI 10.1186/1746-4269-4-10.
   Majdi C, 2020, ANTIOXIDANTS-BASEL, V9, DOI 10.3390/antiox9050369.
   Manab Mandal, 2016, Asian Pacific Journal of Tropical Disease, V6, P54.
   Mandal M, 2017, ASIAN PAC J TROP BIO, V7, P979, DOI 10.1016/j.apjtb.2017.10.001.
   Murugan NA, 2021, J BIOMOL STRUCT DYN, V39, P4415, DOI 10.1080/07391102.2020.1777901.
   Najafabadi M.M., 2015, J BIG DATA-GER, V2, P1, DOI 10.1186/s40537-014-0007-7.
   Okhuarobo A., 2014, Asian Pacific Journal of Tropical Disease, V4, P213.
   OSU, 2021, PLANT IDENTIFICATION.
   Pornpanomchai C., 2011, Kasetsart Journal, Natural Sciences, V45, P551.
   Proklamasiningsih E, 2020, IOP C SER EARTH ENV, V593, DOI 10.1088/1755-1315/593/1/012026.
   Rahman A. F. M. M., 2013, International Journal of Biosciences (IJB), V3, P36.
   Rangarajan AK, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-59108-x.
   Sack L, 2013, NEW PHYTOL, V198, P983, DOI 10.1111/nph.12253.
   Sahu PK, 2020, BIOL CONTROL, V150, DOI 10.1016/j.biocontrol.2020.104353.
   Samidurai D, 2020, PROCESS BIOCHEM, V88, P213, DOI 10.1016/j.procbio.2019.09.031.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Santos M. S. dos, 2016, African Journal of Agricultural Research, V11, P1924.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Singh D, 2018, IND CROP PROD, V118, P367, DOI 10.1016/j.indcrop.2018.03.048.
   Singh Vijai, 2017, Information Processing in Agriculture, V4, P41, DOI 10.1016/j.inpa.2016.10.005.
   Siriwatanametanon N., 2011, NAT PROD COMMUN, V6, DOI {[}10.1177/1934578X1100600512, DOI 10.1177/1934578X1100600512].
   Siriwatanametanon N, 2010, J ETHNOPHARMACOL, V130, P196, DOI 10.1016/j.jep.2010.04.036.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Swamy MK, 2017, EVID-BASED COMPL ALT, V2017, DOI 10.1155/2017/1517683.
   Tan HL, 2016, FRONT PHARMACOL, V7, DOI 10.3389/fphar.2016.00052.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Vijayashree T, 2017, 2017 INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS), P1304, DOI 10.1109/ICCONS.2017.8250679.
   Vimala S., 2012, SCI J MED CLIN TRIAL, V1, P9.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Yamani HA, 2016, FRONT MICROBIOL, V7, DOI 10.3389/fmicb.2016.00681.},
Number-of-Cited-References = {54},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Pertanika J. Sci. Technol.},
Doc-Delivery-Number = {0P6FM},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000784322500023},
OA = {Green Accepted, hybrid},
DA = {2023-08-12},
}

@article{ WOS:000835086000001,
Author = {Pukhrambam, Banita and Sahayadhas, Arun},
Title = {Advanced Medicinal Plant Classification and Bioactivity Identification
   based on Dense Net Architecture},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
Year = {2022},
Volume = {13},
Number = {6},
Pages = {104-109},
Month = {JUN},
Abstract = {Plant species identification helps a wide range of stakeholders,
   including forestry services, botanists, taxonomists, physicians and
   pharmaceutical laboratories, endangered species organizations, the
   government, and the general public. As a result, there has been a spike
   in interest in developing automated plant species recognition systems.
   Using computer vision and deep learning approaches, this work proposes a
   fully automated system for finding medical plants. As a result, work is
   being done to classify the correct therapeutic plants based on their
   images. A training data set contains image data; this work uses the
   Indian Medicinal Plants, Photochemistry, and Therapeutics (IMPPAT)
   benchmark dataset. Convolutional Neural Network (CNN) with DenseNet
   algorithm is a classification system for medicinal plants that explains
   how they work and what they're efficient. This study also suggests a
   standard dataset for medicinal plants that can be found in various parts
   of Manipur, India's northwest coast state. On the IMPPAT dataset, the
   suggested DenseNet model has a recognition rate of 99.56\% and on the
   Manipuri dataset; it has a recognition rate of 98.51\%, suggesting that
   the DenseNet method is a promising technique for smart forestry.},
Publisher = {SCIENCE \& INFORMATION SAI ORGANIZATION LTD},
Address = {19 BOLLING RD, BRADFORD, WEST YORKSHIRE, 00000, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Pukhrambam, B (Corresponding Author), Vels Inst Sci Technol \& Adv Studies, Dept Comp Sci \& Engn, Chennai 600117, Tamil Nadu, India.
   Pukhrambam, Banita; Sahayadhas, Arun, Vels Inst Sci Technol \& Adv Studies, Dept Comp Sci \& Engn, Chennai 600117, Tamil Nadu, India.},
ISSN = {2158-107X},
EISSN = {2156-5570},
Keywords = {Indian medicinal plants; convolutional neural network; DenseNet; IMPPAT
   dataset},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Affiliations = {Vels Institute of Science, Technology \& Advanced Studies},
Cited-References = {Ahmed N, 2016, SCI INT, V28, DOI DOI 10.9790/0661-17134853.
   Bisen D, 2021, MULTIMED TOOLS APPL, V80, P6443, DOI 10.1007/s11042-020-10038-w.
   Fitzgerald M, 2020, FRONT PHARMACOL, V10, DOI 10.3389/fphar.2019.01480.
   Fujii Hiroyuki, 2008, SICE 2008 - 47th Annual Conference of the Society of Instrument and Control Engineers of Japan, P293, DOI 10.1109/SICE.2008.4654666.
   Gabbar H. A., 2006, SICE ICASE INT JOINT, P1866.
   Gabbar H. A, 2006, SICE ICASE INT JOINT, P5737.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Hossain E., 2019, P 2019 INT C ELECT C, P1.
   Islam M. A., 2019, INT J COMPUTER IJC, V33, P26.
   Kazerouni MF, 2019, SN APPL SCI, V1, DOI 10.1007/s42452-019-0785-9.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lin YH, 2006, INT CONF NANO MICRO, P1354, DOI 10.1109/NEMS.2006.334747.
   Mohanraj K., 2018, IMPPAT CURATED DATAB.
   Olsen A., 2015, 2015 INT C DIG IM CO, P1, DOI {[}10.1109/DICTA.2015.7371274, DOI 10.1109/DICTA.2015.7371274].
   Paolone M, 2003, IEEE BOLOGNA POWER T, V4, P5.
   Roy Amarjit Singha, 2014, IET IMAGE PROCESS.
   Sangle Shailesh, 2013, INT J ENG RES TECHNO, V02.
   Singh Kh Bora., 2014, IMAGING SCI J.
   Swu Vikaho Bora., 2020, IDENTIFICATION DIFFE, P172.
   Talwar S., 2020, CURRENT PHARM REPORT, P354.
   Tomar D, 2016, INT J IMAGE GRAPH, V16, DOI 10.1142/S0219467816500121.
   Tomar D, 2015, KNOWL-BASED SYST, V81, P131, DOI 10.1016/j.knosys.2015.02.009.
   Bao TQ, 2020, J INFORM TELECOMMUN, V4, P140, DOI 10.1080/24751839.2019.1666625.
   Zaidah Ibrahim, 2018, INDONES J ELECT ENG, V9, P152.},
Number-of-Cited-References = {24},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Int. J. Adv. Comput. Sci. Appl.},
Doc-Delivery-Number = {3L9NC},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000835086000001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000380407300127,
Author = {Amlekar, M. M. and Gaikwad, A. T. and Manza, R. R. and Yannawar, P. L.},
Book-Group-Author = {IEEE},
Title = {Leaf Shape Extraction For Plant Classification},
Booktitle = {2015 INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING (ICPC)},
Year = {2015},
Note = {International Conference on Pervasive Computing (ICPC), Pune, INDIA, JAN
   08-10, 2015},
Organization = {IEEE Pune Sect; IEEE Comp Soc; Savitribai Phule Pune Univ; IEEE Commun
   Soc Pune Chapter; Sinhgad Inst; Sakal Times},
Abstract = {This research paper presents the leaf shape extraction for plant
   classification. Leaves are very important component of the plant which
   actually identifies and classify the plants. Classification of the plant
   by their leaf biometric features is commonly performed task of trained
   botanist and taxonomist. To perform this task they need to perform
   various set of operations. Because of this the task of classification of
   plants manually is time consuming. There are many biometric features of
   leaves of the plants for classification. Here the shape of leaves of the
   plant species are extracted for plant classification. In this paper,
   various operators are studied for the leaf extraction from images by
   using the image processing techniques.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Amlekar, MM (Corresponding Author), Inst Management Studies \& Informat Technol, Aurangabad, Maharashtra, India.
   Amlekar, M. M.; Gaikwad, A. T., Inst Management Studies \& Informat Technol, Aurangabad, Maharashtra, India.
   Manza, R. R.; Yannawar, P. L., Dr BAM Univ, Dept CS \& IT, Aurangabad, Maharashtra, India.},
ISBN = {978-1-4799-6272-3},
Keywords = {Canny; leaf shape; plant classification; Prewitt; Roberts; Sobel},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {manishaak2012@gmail.com
   drashokgaikwad@gmail.com
   manzaramesh@gmail.com
   pravinyannawar@gmail.com},
Affiliations = {Dr. Babasaheb Ambedkar Marathwada University (BAMU)},
ResearcherID-Numbers = {Yannawar, Pravin/AAC-4327-2021
   },
ORCID-Numbers = {Yannawar, Pravin/0000-0002-3398-0565
   Manza, Ramesh/0000-0002-4510-9224},
Cited-References = {Hossain J., 2010, Proceedings of the 13th International Conference on Computer and Information Technology (ICCIT 2010), P458, DOI 10.1109/ICCITECHN.2010.5723901.
   Kaur Khiranjeet, 2013, INT J APPL INNOVATIO, V2.
   Kui Fang, 2012, INT J COMPUTER INFOR, V01.
   Kumar G. H., 2012, INT J COMPUTER APPL, V47.
   Pan Jiazhi, 2008, INT C COMP SCI SOFTW, V4, P906.
   Shrivakshan G.T., 2012, IJCSI INT J COMPUTER, V9.
   Vairalkar Manoj, 2012, INT J EMERGING TECHN, V2.
   Valliammal N., 2012, IJCSI INT J COMPUTER, V9.
   Zulkifli Z., 2011, 2011 11 INT C HYBR I.},
Number-of-Cited-References = {9},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BF1LK},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000380407300127},
DA = {2023-08-12},
}

@article{ WOS:000934657100001,
Author = {Xu, Guoqing},
Title = {Effective scale-generation scheme based shape descriptor for fast leaf
   recognition and retrieval},
Journal = {JOURNAL OF ENGINEERING-JOE},
Year = {2023},
Volume = {2023},
Number = {2},
Month = {FEB},
Abstract = {Plant species recognition using leaf images is a very challenging task
   in the field of pattern recognition. In this paper, an effective
   scale-generation scheme is proposed to extract leaf contour features.
   For each point on sampled leaf contour, the scale-generation scheme
   iteratively utilized trisections of leaf contour to find paired
   neighbour points under different scales. Then paired neighbour points
   under each scale were used to extract angle information to form
   multi-scale angle representation. Subsequently, Fast Fourier transform
   was applied on the multi-scale angle representation to make it compact
   and facilitate similarity measurement. City block metric was used to
   compute similarity between leaves for retrieval task. Both Support
   Vector Machine and 1-nearest neighbour were used as classifiers for
   recognition. Finally, the proposed descriptor was evaluated on four
   challenging leaf datasets, including Swedish, Flavia, MEW2012 and
   ImageCLEF 2012 datasets. The recognition accuracy of the proposed
   descriptor over Swedish and Flavia datasets reaches 97.03\% and 94.03\%,
   respectively. The mean average precision scores over Swedish, Flavia,
   MEW2012 and ImageCLEF 2012 datasets are 77.39\%, 69.47\%, 47.51\% and
   37.13\%, respectively. Performance comparisons with both classical and
   state-of-the-art methods were made in terms of performance evaluation
   metrics. The results demonstrate that the proposed method has prominent
   performance.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Xu, GQ (Corresponding Author), Henan Polytech Inst, Sch Elect Informat Engn, 1666 Dushi Rd, Nanyang, Peoples R China.
   Xu, Guoqing, Henan Polytech Inst, Sch Elect Informat Engn, Nanyang, Peoples R China.
   Xu, Guoqing, Henan Polytech Inst, Sch Elect Informat Engn, 1666 Dushi Rd, Nanyang, Peoples R China.},
DOI = {10.1049/tje2.12241},
Article-Number = {e212241},
EISSN = {2051-3305},
Keywords = {image classification; image processing; support vector machine (SVM)},
Keywords-Plus = {REPRESENTATION; CLASSIFICATION},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Multidisciplinary},
Author-Email = {xgq0209@163.com},
Funding-Acknowledgement = { {[}222102320375]},
Funding-Text = {ACKNOWLEDGEMENTS This paper is supported partially by Research Start-up
   Fund Project of Henan Polytechnic Institute and Science and technology
   research project of He'nan province (No. 222102320375).},
Cited-References = {Adamek T, 2004, IEEE T CIRC SYST VID, V14, P742, DOI 10.1109/TCSVT.2004.826776.
   Alajlan N, 2007, PATTERN RECOGN, V40, P1911, DOI 10.1016/j.patcog.2006.12.005.
   Anubha Pearline S., 2022, J INTELL FUZZY SYST, V67.
   Cao J, 2016, INFORM SCIENCES, V374, P51, DOI 10.1016/j.ins.2016.09.023.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Jiang HX, 2020, IEEE ACCESS, V8, P68828, DOI 10.1109/ACCESS.2020.2986946.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Kheirkhah FM, 2019, IET COMPUT VIS, V13, P369, DOI 10.1049/iet-cvi.2018.5028.
   Kurnianggoro L, 2018, NEUROCOMPUTING, V300, P1, DOI 10.1016/j.neucom.2018.02.093.
   Laga H, 2012, 2012 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING TECHNIQUES AND APPLICATIONS (DICTA).
   Lavania S., 2014, P 2014 IEEE INT C CO.
   Laxmi S, 2022, ENG APPL ARTIF INTEL, V110, DOI 10.1016/j.engappai.2022.104687.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Liu J., 2018, P 2018 CHINESE AUTOM.
   Mouine S., 2013, P 3 ACM INT C MULTIM.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Pham N., 2013, P 2013 INT C COMPUTI.
   Rojas-Hernandez R., 2016, P 2016 IEEE 13 INT C.
   Sachar S, 2021, EXPERT SYST APPL, V167, DOI 10.1016/j.eswa.2020.114181.
   Shao Y, 2019, COMPUT ELECTRON AGR, V158, P102, DOI 10.1016/j.compag.2019.01.022.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang B, 2022, COMPUT ELECTRON AGR, V197, DOI 10.1016/j.compag.2022.106914.
   Wang B, 2015, INFORM SCIENCES, V302, P132, DOI 10.1016/j.ins.2014.07.028.
   Wang XG, 2014, PATTERN RECOGN, V47, P2116, DOI 10.1016/j.patcog.2013.12.008.
   Xu G., 2021, IEEE T KNOWL DATA EN, V2021, P467.
   Xu GQ, 2019, IET IMAGE PROCESS, V13, P2328, DOI 10.1049/iet-ipr.2018.6551.
   Yang C., 2022, IEEE ACM T COMPUT BI.
   Yang C., 2016, P 22 EUROPEAN C ARTI.
   Yang CZ, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107809.
   Yang CZ, 2019, IEEE ACCESS, V7, P178108, DOI 10.1109/ACCESS.2019.2958416.
   Zhang SW, 2020, NEUROCOMPUTING, V408, P246, DOI 10.1016/j.neucom.2019.09.113.
   Zhang SW, 2020, KNOWL-BASED SYST, V200, DOI 10.1016/j.knosys.2020.105998.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {34},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Journal-ISO = {J. Eng.-JOE},
Doc-Delivery-Number = {9B3RH},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000934657100001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000314766200064,
Author = {Zhi, Zhi-De and Hu, Rong-Xiang and Wang, Xiao-Feng},
Editor = {Huang, DS and Ma, J and Jo, KH and Gromiha, MM},
Title = {A New Weighted ARC-SC Approach for Leaf Image Recognition},
Booktitle = {INTELLIGENT COMPUTING THEORIES AND APPLICATIONS, ICIC 2012},
Series = {Lecture Notes in Artificial Intelligence},
Year = {2012},
Volume = {7390},
Pages = {503-509},
Note = {8th International Conference on Intelligent Computing (ICIC), Huangshan,
   PEOPLES R CHINA, JUL 25-29, 2012},
Organization = {IEEE Computat Intelligence Soc; Int Neural Network Soc; Natl Sci Fdn
   China; Tongji Univ},
Abstract = {In this paper, we present a novel feature extraction approach for plant
   leaf image recognition, which applies the arc length information to
   replace the Euclidean distance in traditional Shape Context (SC) method.
   Meanwhile, the shape is divided by the arc length into two parts, i.e.
   local and global feature. It can obtain the weighed cost of shape
   matching by combining the local with global feature. We compare this
   algorithm with the classic Inner-Distance Shape Context (IDSC) method on
   both Swedish and ICL leaf image dataset. Experimental results show that
   the proposed method achieves better performance compared with SC and
   IDSC methods.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Wang, XF (Corresponding Author), Chinese Acad Sci, Intelligent Comp Lab, Hefei Inst Intelligent Machines, Hefei 230031, Anhui, Peoples R China.
   Zhi, Zhi-De; Hu, Rong-Xiang, Univ Sci \& Technol China, Dept Automat, Hefei 230027, Anhui, Peoples R China.
   Wang, Xiao-Feng, Hefei Univ, Key Lab Network \& Intelligent Informat Proc, Hefei 230601, Peoples R China.},
ISSN = {0302-9743},
ISBN = {978-3-642-31575-6; 978-3-642-31576-3},
Keywords = {Leaf recognition; arc length; dynamic programming; shape context},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {zhizhide321@163.com
   xfwang@iim.ac.cn},
Affiliations = {Chinese Academy of Sciences; University of Science \& Technology of
   China, CAS; Hefei University},
Funding-Acknowledgement = {National Science Foundation of China {[}61005010, 60975005, 60905023,
   60873012, 71001072]; China Postdoctoral Science Foundation
   {[}20100480708]; Key Scientific Research Foundation of Education
   Department of Anhui Province {[}KJ2010A289]; Scientific Research
   Foundation for Talents of Hefei University {[}11RC05]},
Funding-Text = {This work was supported by the grants of the National Science Foundation
   of China, Nos. 61005010, 60975005, 60905023, 60873012, 71001072, the
   grant of China Postdoctoral Science Foundation, No. 20100480708, the
   grant of the Key Scientific Research Foundation of Education Department
   of Anhui Province, No. KJ2010A289, the grant of Scientific Research
   Foundation for Talents of Hefei University, No. 11RC05.},
Cited-References = {Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Petrakis EGM, 2002, IEEE T PATTERN ANAL, V24, P1501, DOI 10.1109/TPAMI.2002.1046166.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Zhang D.S., 2007, INT C COMP INF SCI I, P801.},
Number-of-Cited-References = {6},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BDT30},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000314766200064},
DA = {2023-08-12},
}

@article{ WOS:000901072600001,
Author = {Zhang, Ruolei and Zhu, Yijun and Ge, Zhangshangjie and Mu, Hongbo and
   Qi, Dawei and Ni, Haiming},
Title = {Transfer Learning for Leaf Small Dataset Using Improved ResNet50 Network
   with Mixed Activation Functions},
Journal = {FORESTS},
Year = {2022},
Volume = {13},
Number = {12},
Month = {DEC},
Abstract = {Taxonomic studies of leaves are one of the most effective means of
   correctly identifying plant species. In this paper, mixed activation
   function is used to improve the ResNet50 network in order to further
   improve the accuracy of leaf recognition. Firstly, leaf images of 15
   common tree species in northern China were collected from the Urban
   Forestry Demonstration Base of Northeast Forestry University (45 degrees
   43 `-45 degrees 44 ` N, 126 degrees 37 `-126 degrees 38 ` E, forest type
   was artificial forest), and a small leaf dataset was established. After
   that, seven commonly used activation functions were selected to improve
   the ResNet50 network structure, and the improved network was applied to
   the transfer learning research of the leaf small dataset. On this basis,
   five activation functions with better performance were selected for the
   study of mixed activation functions in deep learning. Two of these five
   activation functions are arbitrarily selected for combination, and a
   total of twenty combinations are obtained. Further, the first activation
   function was used in each combination to replace the first ReLU function
   after all addition operations in the ResNet50 network residual block
   structure, and another activation function was used to replace the other
   position ReLU functions. The experimental results show that in the
   transfer learning of the leaf small dataset using the ResNet50 deep
   residual network, the appropriate combination of mixed activation
   functions can increase the performance of the improved network to a
   certain extent. Among them, the ELU-Swish1 combination has the most
   significant improvement effect on the network performance, whose final
   effective validation accuracy reaches 98.17\%. Furthermore, the
   comparison with GoogLeNet and VGG-16 also demonstrates the excellent
   performance of the improved ELU-Swish1 ResNet50 (ES-ResNet50) network
   architecture. Finally, tests on the other two small leaf datasets,
   Flavia and Swedish, also demonstrate the performance improvement of
   ES-ResNet50. The validation accuracy of the improved ES-Resnet 50
   algorithm on these two datasets reaches 99.30\% and 99.39\%,
   respectively. All these experiments prove that the recognition
   performance of leaf transfer learning using the ES-ResNet50 network is
   indeed improved, which may be caused by the complementarity of the
   e-exponential gradient of ELU and Swish1 activation functions in the
   negative region.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Ni, HM (Corresponding Author), Northeast Forestry Univ, Coll Sci, Hexing Rd 26, Harbin 150040, Peoples R China.
   Zhang, Ruolei; Zhu, Yijun; Ge, Zhangshangjie; Mu, Hongbo; Qi, Dawei; Ni, Haiming, Northeast Forestry Univ, Coll Sci, Hexing Rd 26, Harbin 150040, Peoples R China.},
DOI = {10.3390/f13122072},
Article-Number = {2072},
EISSN = {1999-4907},
Keywords = {residual block; neural network; mixed activation functions; deep
   learning; leaves recognition},
Keywords-Plus = {PLANT; CLASSIFICATION},
Research-Areas = {Forestry},
Web-of-Science-Categories  = {Forestry},
Author-Email = {nihaiming2013@nefu.edu.cn},
Affiliations = {Northeast Forestry University - China},
ResearcherID-Numbers = {Zhu, Yi-Jun/IVH-5331-2023
   },
ORCID-Numbers = {Zhu, Yi-Jun/0009-0007-7556-2605
   Ge-Zhang, Shangjie/0000-0002-2439-0897},
Cited-References = {Abdul K., 2011, SIGNAL IMAGE PROCESS, V2, P1, DOI 10.5121/sipij.2011.2301.
   Agyepong JT, 2021, IEEE ACCESS, V9, P118271, DOI 10.1109/ACCESS.2021.3106888.
   Al-gaashani MSAM, 2022, IET IMAGE PROCESS, V16, P913, DOI 10.1049/ipr2.12397.
   Apicella A, 2021, NEURAL NETWORKS, V138, P14, DOI 10.1016/j.neunet.2021.01.026.
   Ariyapadath S, 2021, TRAIT SIGNAL, V38, P1587, DOI 10.18280/ts.380603.
   Bawa VS, 2019, EXPERT SYST APPL, V120, P346, DOI 10.1016/j.eswa.2018.11.042.
   Brahimi M, 2017, APPL ARTIF INTELL, V31, P299, DOI 10.1080/08839514.2017.1315516.
   Chen JD, 2020, COMPUT ELECTRON AGR, V173, DOI 10.1016/j.compag.2020.105393.
   Chen X, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105714.
   Han L, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3056470.
   He K., 2016, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2016.90.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Jendoubi S, 2020, EXPERT SYST APPL, V146, DOI 10.1016/j.eswa.2019.113154.
   Jiang ZC, 2021, COMPUT ELECTRON AGR, V186, DOI 10.1016/j.compag.2021.106184.
   Jinsakul N, 2019, MATHEMATICS-BASEL, V7, DOI 10.3390/math7121170.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Khan A, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0243243.
   Korznikov KA, 2021, FORESTS, V12, DOI 10.3390/f12010066.
   Krishnamoorthy N, 2021, ENVIRON RES, V198, DOI 10.1016/j.envres.2021.111275.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Ohn I, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21070627.
   Pereira CS, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19224850.
   Privietha P., 2022, P 2022 INT VIRTUAL C, V5, DOI {[}10.1109/PECCON55017.2022.9851128, DOI 10.1109/PECCON55017.2022.9851128].
   Qian S, 2018, NEUROCOMPUTING, V272, P204, DOI 10.1016/j.neucom.2017.06.070.
   Shah MP, 2017, IEEE IMAGE PROC, P860, DOI 10.1109/ICIP.2017.8296403.
   Shao Y, 2019, COMPUT ELECTRON AGR, V158, P102, DOI 10.1016/j.compag.2019.01.022.
   Shi P, 2019, COMPUT ELECTRON AGR, V157, P329, DOI 10.1016/j.compag.2019.01.004.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Suto J, 2020, INTELL DATA ANAL, V24, P1311, DOI 10.3233/IDA-194821.
   Thangaraj R, 2021, J PLANT DIS PROTECT, V128, P73, DOI 10.1007/s41348-020-00403-0.
   Varshney M, 2021, SIGNAL IMAGE VIDEO P, V15, P1323, DOI 10.1007/s11760-021-01863-z.
   Wan LT, 2022, INFORM FUSION, V78, P90, DOI 10.1016/j.inffus.2021.09.007.
   Wang SH, 2020, NEURAL COMPUT APPL, V32, P665, DOI 10.1007/s00521-018-3924-0.
   Wang SH, 2018, J MED SYST, V42, DOI 10.1007/s10916-018-0932-7.
   Wang X, 2019, NEUROCOMPUTING, V363, P88, DOI 10.1016/j.neucom.2019.07.017.
   Yan Q, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20123535.
   Yang CZ, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107809.
   Ying Y, 2019, IEEE ACCESS, V7, P101633, DOI 10.1109/ACCESS.2019.2928442.
   Yuan S., 2013, J ENG HEILONGJIANG U, V4, P51, DOI {[}10.13524/j.2095-008x.2013.03.009, DOI 10.13524/J.2095-008X.2013.03.009].
   Zhang BY, 2021, FORESTS, V12, DOI 10.3390/f12070937.
   Zhang SW, 2011, NEUROCOMPUTING, V74, P2284, DOI 10.1016/j.neucom.2011.03.007.
   Zhang X, 2019, MULTIMED TOOLS APPL, V78, P27463, DOI 10.1007/s11042-019-07846-0.
   Zhang Y, 2021, REMOTE SENS ENVIRON, V267, DOI 10.1016/j.rse.2021.112724.},
Number-of-Cited-References = {44},
Times-Cited = {2},
Usage-Count-Last-180-days = {10},
Usage-Count-Since-2013 = {14},
Journal-ISO = {Forests},
Doc-Delivery-Number = {7E3KY},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000901072600001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000778922000012,
Author = {Ferentinos, Konstantinos P. and Barda, Myrto and Damer, Dave},
Editor = {Oliveira, PM and Novais, P and Reis, LP},
Title = {An Image-Based Deep Learning Model for Cannabis Diseases, Nutrient
   Deficiencies and Pests Identification},
Booktitle = {PROGRESS IN ARTIFICIAL INTELLIGENCE, EPIA 2019, PT I},
Series = {Lecture Notes in Artificial Intelligence},
Year = {2019},
Volume = {11804},
Pages = {134-145},
Note = {19th EPIA Conference on Artificial Intelligence (EPIA), Univ Tras Os
   Montes \& Alto Douro, Vila Real, PORTUGAL, SEP 03-06, 2019},
Organization = {Soc Experimental Mech; Portuguese Assoc Artificial Intelligence; SISCOG},
Abstract = {In this work, a deep learning system for cannabis plants disease,
   nutrient deficiencies and pests identification is developed, based on
   image data processed by convolutional neural network models. Training of
   the models was performed using image data available on the Internet,
   while database development included data cleansing by expert
   agronomists, basic image editing, and data augmentation techniques
   commonly used in deep learning applications in order to expand the
   rather limited amount of available data. Three fungi diseases, two pests
   and three nutrient deficiencies were included in the identification
   system, together with healthy plants identification. The final model
   reached a performance of 90.79\% in successfully identifying cannabis
   diseases (or healthy plants) in previously ``unseen{''} plant images.
   The most difficult cannabis problems to be identified were powdery
   mildew and potassium deficiency. Results showed that transfer learning
   from existing models specialized in similar tasks to the one under
   development, is more successful than using transfer learning from more
   general models. Finally, even though the amount of training images in
   some of the considered problems was significantly small, no correlation
   between model performance and the size of the training dataset for each
   category was found.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Ferentinos, KP (Corresponding Author), Hellen Agr Org Demeter, Soil \& Water Resources Inst, Dept Agr Engn, Athens, Greece.
   Ferentinos, Konstantinos P.; Barda, Myrto, Hellen Agr Org Demeter, Soil \& Water Resources Inst, Dept Agr Engn, Athens, Greece.
   Damer, Dave, Testfire Labs, Edmonton, AB, Canada.},
DOI = {10.1007/978-3-030-30241-2\_12},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-030-30241-2; 978-3-030-30240-5},
Keywords = {Cannabis; Convolutional Neural Networks; Disease detection; Disease
   diagnosis},
Keywords-Plus = {PLANT; AGRICULTURE; PATHOGENS; SATIVA},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications},
Author-Email = {k.ferentinos@swri.gr
   dave@testfirelabs.com},
ORCID-Numbers = {, Myrto S. Barda/0000-0003-0661-4946},
Funding-Acknowledgement = {European Union; Greek national funds through the Operational Program
   Competitiveness, Entrepreneurship and Innovation, under the call
   RESEARCH -CREATE -INNOVATE {[}T1EDK-02182]},
Funding-Text = {This research has been co-financed by the European Union and Greek
   national funds through the Operational Program Competitiveness,
   Entrepreneurship and Innovation, under the call RESEARCH -CREATE
   -INNOVATE (project code: T1EDK-02182).},
Cited-References = {An JY, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11020256.
   Andre CM, 2016, FRONT PLANT SCI, V7, DOI {[}10.3389/fpls.2016.00019, 10.3389/fpls.2016.00463].
   Barbedo JGA, 2019, BIOSYST ENG, V180, P96, DOI 10.1016/j.biosystemseng.2019.02.002.
   Barbedo JGA, 2018, BIOSYST ENG, V172, P84, DOI 10.1016/j.biosystemseng.2018.05.013.
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006.
   Cheng X, 2017, COMPUT ELECTRON AGR, V141, P351, DOI 10.1016/j.compag.2017.08.005.
   Ciresan D. C., 2011, 22 INT JOINT C ART I.
   Clarke R.C., 1981, MARIJUANA BOT ADV ST, DOI DOI 10.1080/02791072.1981.10471901.
   Clarke RC, 2013, CANNABIS: EVOLUTION AND ETHNOBOTANY, P1.
   Dyrmann M., 2017, ADV ANIM BIOSCI, V8, P842, DOI {[}DOI 10.1017/S2040470017000206, 10.1017/S2040470017000206].
   Dyrmann M., 2018, 14 INT C PREC AGR MO.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Frank M., 1988, MARIJUANA GROWERS IN.
   Gebbers R, 2010, SCIENCE, V327, P828, DOI 10.1126/science.1183899.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Krizhevsky, 2014, ARXIV PREPRINT ARXIV.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   LeCun Y., 1995, CONVOLUTIONAL NETWOR, P255, DOI {[}10.5555/303568.303704, DOI 10.5555/303568.303704].
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lin K, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.00155.
   McPartland J. M., 2000, Hemp diseases and pests: management and biological control - an advanced treatise., DOI 10.1079/9780851994543.0000.
   McPartland J. M., 1996, Journal of the International Hemp Association, V3, P52.
   McPartland J. M., 1996, Journal of the International Hemp Association, V3, P19.
   MCPARTLAND JM, 1994, MYCOLOGIA, V86, P870, DOI 10.2307/3760600.
   McPartland JM, 1991, PLANT DIS, V75, P226.
   McPartland John M., 1995, Sydowia, V47, P44.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Pawara P, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P479, DOI 10.5220/0006196204790486.
   Punja ZK, 2018, CAN J PLANT PATHOL, V40, P514, DOI 10.1080/07060661.2018.1535467.
   Sermanet P, 2014, ICLR.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Small E., 2017, CANNABIS COMPLETE GU.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   Thompson GR, 2017, CLIN MICROBIOL INFEC, V23, P269, DOI 10.1016/j.cmi.2016.12.001.
   Toda Y, 2019, PLANT PHENOMICS, V2019, DOI 10.34133/2019/9237136.
   Tyagi AC, 2016, IRRIG DRAIN, V65, P388, DOI 10.1002/ird.2076.
   Yang X, 2017, EUR J BIOMED RES, V3, P6, DOI {[}DOI 10.18088/EJBMR.3.1.2017, DOI 10.18088/EJBMR.3.1.2017.PP6-9].},
Number-of-Cited-References = {41},
Times-Cited = {4},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BS9BD},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000778922000012},
DA = {2023-08-12},
}

@inproceedings{ WOS:000280804301222,
Author = {Yusof, Rubiyah and Rosli, Nenny Ruthfalydia and Khalid, Marzuki},
Editor = {Qiu, PH and Yiu, C and Zhang, H and Wen, XB},
Title = {Tropical Wood Species Recognition Based on Gabor Filter},
Booktitle = {PROCEEDINGS OF THE 2009 2ND INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL
   PROCESSING, VOLS 1-9},
Year = {2009},
Pages = {2705-2709},
Note = {2nd International Congress on Image and Signal Processing, Tianjin,
   PEOPLES R CHINA, OCT 17-19, 2009},
Organization = {Tianjin Univ Technol; IEEE Engn Med \& Biol Soc (EMBS)},
Abstract = {Tropical timber woods have more than 1,000 species. Some of the species
   have similar patterns with others and some have different patterns even
   though they are of the same species. One of the main problems in wood
   species recognition system is the lack of discriminative features of the
   texture images. Gabor filter has been extensively used as feature
   extractor for various applications such as face detection, face
   recognition, image retrieval and font type extraction. In our work, we
   propose the use of Gabor filter to generate multiple processed images
   from a single image so that more features can be extracted and will be
   trained by neural network. The use of Gabor filters will optimally
   localized the properties of the images in both spatial and frequency
   domain. The features of the filtered images are extracted using
   co-occurrence matrix approach, known as grey level co-occurrence matrix
   (GLCM). A multi-layer neural network based on the popular BP (back
   propagation) algorithm is used for classification. The results show that
   increasing the number of features by means of Gabor filters as well as
   the right combination of Gabor filters increases the accuracy rate of
   the system.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Yusof, R (Corresponding Author), Univ Teknol Malaysia, Ctr Artificial Intelligence \& Robot, Int Campus, Kuala Lumpur 54100, Malaysia.
   Yusof, Rubiyah; Rosli, Nenny Ruthfalydia; Khalid, Marzuki, Univ Teknol Malaysia, Ctr Artificial Intelligence \& Robot, Kuala Lumpur 54100, Malaysia.},
ISBN = {978-1-4244-4130-3},
Keywords = {image processing; texture pattern recognition; Gabor filter; grey level
   co-occurrence matrix (GLCM); neural network; wood recognition},
Research-Areas = {Computer Science; Imaging Science \& Photographic Technology;
   Telecommunications},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Imaging Science \&
   Photographic Technology; Telecommunications},
Affiliations = {Universiti Teknologi Malaysia},
ResearcherID-Numbers = {yusof, rubiyah/A-1706-2013
   yusof, rubiyah/AAV-9212-2020},
Cited-References = {ALLIER B, 2003, P 7 INT C DOC AN REC.
   Andrysiak T, 2005, INT J AP MAT COM-POL, V15, P471.
   {[}Anonymous], 1974, P 2 INT JOINT C PATT.
   {[}Anonymous], 1998, HDB PATTERN RECOGNIT.
   Bhuiyan AA, 2007, PROC WRLD ACAD SCI E, V22, P51.
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160.
   GONZALEZ RC, 1992, DIGITAL IMAGE PROCES, P195.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Huang LL, 2005, PATTERN RECOGN LETT, V26, P1641, DOI 10.1016/j.patrec.2005.01.015.
   Khalid M, 2008, INT J SIMUL SYST SCI, V9, P9.
   Lew Y. L., 2005, DESIGN INTELLIGENT W.
   MACLENNAN B, 1991, GABOR REPRESENTATION, P91.
   {*}MAL TIMB COUNC, 2007, MAL SUST FOR MAN.
   Menon P. K. B., 1993, STRUCTURE IDENTIFICA.
   Omatu S., 1995, NEURO CONTROL ITS AP.
   Tuceryan M., 1993, TEXTURE ANAL HDB PAT, V2, P207, DOI DOI 10.1142/9789814343138\_0010.
   {*}U TEKN MAL, 2006, VIS SYST DEV PLATF F.
   Wheeler EA, 1998, IAWA J, V19, P241, DOI 10.1163/22941932-90001528.},
Number-of-Cited-References = {19},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BQE82},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000280804301222},
DA = {2023-08-12},
}

@inproceedings{ WOS:000428410704119,
Author = {Lee, Sue Han and Chang, Yang Loong and Chan, Chee Seng and Remagnino,
   Paolo},
Book-Group-Author = {IEEE},
Title = {HGO-CNN: HYBRID GENERIC-ORGAN CONVOLUTIONAL NEURAL NETWORK FOR
   MULTI-ORGAN PLANT CLASSIFICATION},
Booktitle = {2017 24TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)},
Series = {IEEE International Conference on Image Processing ICIP},
Year = {2017},
Pages = {4462-4466},
Note = {24th IEEE International Conference on Image Processing (ICIP), Beijing,
   PEOPLES R CHINA, SEP 17-20, 2017},
Organization = {Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Signal Proc
   Soc},
Abstract = {Classification of plants based on a multi-organ approach is very
   challenging. Although additional data provides more information that
   might help to disambiguate between species, the variability in shape and
   appearance in plant organs also raises the degree of complexity of the
   problem. Existing approaches focus mainly on generic features for
   species classification, disregarding the features representing the
   organs. In fact, plants are complex entities sustained by a number of
   organ systems. In our approach, we exploit the PlantClef2015 benchmark,
   and introduce a hybrid generic-organ convolutional neural network
   (HGO-CNN), which takes into account both organ and generic information,
   combining them using a new feature fusion scheme for species
   classification. We show that our proposed method outperforms the
   state-of-the-art results.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Lee, SH (Corresponding Author), Univ Malaya, Fac Comp Sci \& Info Tech, Ctr Image \& Signal Proc, Kuala Lumpur, Malaysia.
   Lee, Sue Han; Chang, Yang Loong; Chan, Chee Seng, Univ Malaya, Fac Comp Sci \& Info Tech, Ctr Image \& Signal Proc, Kuala Lumpur, Malaysia.
   Remagnino, Paolo, Kingston Univ, Comp Sci Dept, Robot Vis Team, Kingston, England.},
ISSN = {1522-4880},
ISBN = {978-1-5090-2175-8},
Keywords = {Plant classification; deep learning},
Research-Areas = {Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Imaging Science \& Photographic Technology},
Author-Email = {leesuehan@siswa.um.edu.my
   yangloong@siswa.um.edu.my
   cs.chan@um.edu.my
   p.remagnino@kingston.ac.uk},
Affiliations = {Universiti Malaya; Kingston University},
ResearcherID-Numbers = {Remagnino, Paolo/K-1829-2012
   Lee, Sue Han/AAM-6250-2021
   Chan, Chee Seng/B-9754-2011},
ORCID-Numbers = {Remagnino, Paolo/0000-0002-9168-7746
   Chan, Chee Seng/0000-0001-7677-2865},
Funding-Acknowledgement = {UM PPP {[}PG007-2016A]},
Funding-Text = {This research is supported by the UM PPP Grant PG007-2016A, and the used
   Titan X was donated by NVIDIA.},
Cited-References = {{[}Anonymous], 2009, MANUAL LEAF ARCHITEC.
   Champ Julien, 2015, CLEF 2015 C.
   Choi Sungbin, 2015, CLEF 2015 C.
   Clarke J, 2006, LECT NOTES COMPUT SC, V4292, P427.
   Ge ZongYuan, 2015, CLEF 2015 C.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Le Thi-Lan, 2015, CLEF 2015 C.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Remagnino P., 2017, COMPUTATIONAL BOT ME.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Tang P, 2017, IEEE T IMAGE PROCESS, V26, P3385, DOI 10.1109/TIP.2016.2642781.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Yan ZC, 2015, IEEE I CONF COMP VIS, P2740, DOI 10.1109/ICCV.2015.314.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.},
Number-of-Cited-References = {21},
Times-Cited = {8},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BJ8KY},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000428410704119},
DA = {2023-08-12},
}

@inproceedings{ WOS:000369336200251,
Author = {Wang, Zhaobin and Zheng, Xu and Sun, Xiaoguang and Wang, Hao and Zhu,
   Ying and Liu, Jianpeng and Ma, Yide},
Book-Group-Author = {IEEE},
Title = {A New Petiole Detection Algorithm Based on Leaf Image},
Booktitle = {2015 IEEE 28TH CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER
   ENGINEERING (CCECE)},
Series = {Canadian Conference on Electrical and Computer Engineering},
Year = {2015},
Pages = {1430-1434},
Note = {IEEE 28th Canadian Conference on Electrical and Computer Engineering
   (CCECE), Halifax, CANADA, MAY 03-06, 2015},
Organization = {IEEE},
Abstract = {Leaf is one of the most important organs of plant and often used as one
   of the basic characters in plant classification. The developmental
   condition of leaf can provide us with lots of critical information, such
   as the plant's health condition, the prospection of crop yield and so
   on. Leaf image processing by computer has been widely used for the
   extraction and dissection of leaf images in relevant researches. Image
   processing of leaf also offers an effective platform for plant
   classification and growth observation. A basic problem of leaf image
   processing is detecting and dislodging the petiole from the whole leaf
   image. Here this paper presents an algorithm which combines the
   dual-channel pulse coupled neural network (PCNN) model and HSI color
   space for leaf petiole detection. Totally 169 sorts of leaf images are
   tested by the proposed algorithm. The experimental results show that our
   method has potential availability in reducing mis-evaluation and
   increasing application scale as a tool in relative study.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Wang, ZB (Corresponding Author), Lanzhou Univ, Sch Informat Sci \& Engn, Lanzhou 730000, Gansu, Peoples R China.
   Wang, Zhaobin; Zheng, Xu; Sun, Xiaoguang; Wang, Hao; Ma, Yide, Lanzhou Univ, Sch Informat Sci \& Engn, Lanzhou 730000, Gansu, Peoples R China.
   Zhu, Ying, Gansu Acad Sci, Inst Biol, Lanzhou 730000, Gansu, Peoples R China.
   Liu, Jianpeng, Xian Inst Appl Opt, Xian 710065, Shaanxi, Peoples R China.},
ISSN = {0840-7789},
ISBN = {978-1-4799-5829-0},
Keywords-Plus = {COUPLED NEURAL-NETWORKS; PCNN},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \&
   Electronic},
Author-Email = {wangzhb@lzu.edu.cn
   zhengx13@lzu.edu},
Affiliations = {Lanzhou University; Chinese Academy of Sciences},
ResearcherID-Numbers = {li, jian/IAQ-2794-2023},
Cited-References = {Eckhorn R, 1990, NEURAL COMPUT, V2, P293, DOI 10.1162/neco.1990.2.3.293.
   GAUCH J, 1992, P SOC PHOTO-OPT INS, V1818, P1168, DOI 10.1117/12.131388.
   Johnson JL, 1999, IEEE T NEURAL NETWOR, V10, P480, DOI 10.1109/72.761706.
   Kuntimad G, 1999, IEEE T NEURAL NETWOR, V10, P591, DOI 10.1109/72.761716.
   Li M, 2006, PATTERN RECOGN LETT, V27, P1948, DOI 10.1016/j.patrec.2006.05.004.
   Mzoughi O, 2012, IEEE IMAGE PROC, P1033, DOI 10.1109/ICIP.2012.6467039.
   Ranganath HS, 1999, IEEE T NEURAL NETWOR, V10, P615, DOI 10.1109/72.761720.
   Wang ZB, 2008, INFORM FUSION, V9, P176, DOI 10.1016/j.inffus.2007.04.003.
   Wang ZB, 2010, IMAGE VISION COMPUT, V28, P5, DOI 10.1016/j.imavis.2009.06.007.
   Wei S, 2011, NEUROCOMPUTING, V74, P1485, DOI 10.1016/j.neucom.2011.01.005.
   Xiao Z.H, 2009, IM SIGN PROC 2009 CI, P17.
   Xiaojie W, 2009, J AGR MECH RES, V31, P42.
   Zhao YQ, 2014, BIO-MED MATER ENG, V24, P221, DOI 10.3233/BME-130802.
   Zheng Xiao-dong, 2010, Computer Engineering and Design, V31, P918.
   Zou B, 2012, J COMPUTATIONAL INFO, V8, P4303.},
Number-of-Cited-References = {15},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BE2IE},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000369336200251},
DA = {2023-08-12},
}

@inproceedings{ WOS:000481984000079,
Author = {Goyal, Neha and Kapil and Kumar, Nitin},
Editor = {Shaw, RN and Verma, JK and Johri, P},
Title = {Plant Species Identification using Leaf Image Retrieval: A Study},
Booktitle = {2018 INTERNATIONAL CONFERENCE ON COMPUTING, POWER AND COMMUNICATION
   TECHNOLOGIES (GUCON)},
Year = {2018},
Pages = {398-404},
Note = {International Conference on Computing, Power and Communication
   Technologies (GUCON), Galgotias Univ, Noida, INDIA, SEP 28-29, 2018},
Abstract = {Human beings along with other living beings and their ecological system
   are completely inter-dependent. In the past few decades, the
   technological development has affected the environment more radically
   than ever before. It has posed grave threats to the natural resources
   including habitat loss and degradation, over-exploitation of resources
   and change in climatic condition. Most of plant species are on the verge
   of extinction. In the present circumstances, it is essential to conserve
   ecological system. Plant identification is a crucial step towards
   ecosystem diversity conservation. It is time consuming and requires lots
   of efforts, specialized knowledge and in-depth training. Recent
   technological advancement in the field of imaging, data analysis, and
   plant morphology has improved the decision such as yield prediction,
   crop management, veterinary diet, improving climate and many more. This
   has also made it possible to develop plant species identification
   system. In this paper, we present a comprehensive study about plant
   species identification methods based on various feature extraction
   methods, classification and other challenges. Furthermore, the
   performance of two widely used classifiers viz. Support Vector Machine
   (SVM) and Probabilistic Neural Network (PNN) on Flavia dataset is
   examined in terms of Precision, Recall and F-Score. It is observed that
   SVM performs better than PNN for plant species identification..},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Goyal, N (Corresponding Author), Natl Inst Technol, Dept Comp Applicat, Kurukshetra, Haryana, India.
   Goyal, Neha; Kapil, Natl Inst Technol, Dept Comp Applicat, Kurukshetra, Haryana, India.
   Kumar, Nitin, Natl Inst Technol, Dept Comp Sci \& Engn, Uttarakhand, India.},
ISBN = {978-1-5386-4491-1},
Keywords = {Plant Morphology; Pattern recognition; Morphological feature; Feature
   extraction; Classification},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Energy \& Fuels; Engineering; Telecommunications},
Web-of-Science-Categories  = {Energy \& Fuels; Engineering, Electrical \& Electronic;
   Telecommunications},
Author-Email = {neha.goyal2309@gmail.com
   kapil@nitkkr.ac.in
   nitin2689@gmail.com},
Affiliations = {National Institute of Technology (NIT System); National Institute of
   Technology Kurukshetra; National Institute of Technology (NIT System);
   National Institute of Technology Uttarakhand},
ResearcherID-Numbers = {Kumar, Nitin/AAT-9454-2020
   Gupta, Kapil/AAN-8584-2020},
ORCID-Numbers = {Gupta, Kapil/0000-0003-0264-948X},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   {[}Anonymous], INT J ADV SCI ENG IN.
   {[}Anonymous], 2007, SIGN PROC INF TECHN.
   Arivazhagan Sai, AGRICULTURAL.
   Asif, 2016, SCI INT, V28, P1.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Dallimer M, 2012, BIOSCIENCE, V62, P47, DOI 10.1525/bio.2012.62.1.9.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Gopal A., 2012, MACH VIS IM PROC 201.
   Jayadeva, 2007, IEEE T PATTERN ANAL, V29, P905, DOI 10.1109/TPAMI.2007.1068.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Mangold Jane, 2013, PLANT IDENTIFICATION.
   Mouine S., 2012, P 2 ACM INT C MULT R, P1.
   Pilgrim Sarah E., 2008, ECOLOGICAL KNOWLEDGE, P1004.
   Punyasena SW, 2014, APPL PLANT SCI, V2, DOI 10.3732/apps.1400071.
   Pyle RM, 2003, ORYX, V37, P206, DOI 10.1017/S0030605303000383.
   Robinson BS, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0156572.
   Trias-Blasi A, 2015, NATURE, V521, P161, DOI 10.1038/521161c.
   Wang ZB, 2016, NEURAL COMPUT APPL, V27, P899, DOI 10.1007/s00521-015-1904-1.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.
   Zeinali Y, 2017, INTEGR COMPUT-AID E, V24, P105, DOI 10.3233/ICA-170540.},
Number-of-Cited-References = {24},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BN4JF},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000481984000079},
DA = {2023-08-12},
}

@inproceedings{ WOS:000315409700048,
Author = {Casanova, Dalcimar and Backes, Andre Ricardo and Bruno, Odemir Martinez},
Book-Group-Author = {IOP},
Title = {Pattern recognition tool based on complex network-based approach},
Booktitle = {IC-MSQUARE 2012: INTERNATIONAL CONFERENCE ON MATHEMATICAL MODELLING IN
   PHYSICAL SCIENCES},
Series = {Journal of Physics Conference Series},
Year = {2013},
Volume = {410},
Note = {1st International Conference on Mathematical Modelling in Physical
   Sciences (IC-MSQUARE), Budapest, HUNGARY, SEP 03-07, 2012},
Abstract = {This work proposed a generalization of the method proposed by the
   authors: `A complex network-based approach for boundary shape analysis'.
   Instead of modelling a contour into a graph and use complex networks
   rules to characterize it, here, we generalize the technique. This way,
   the work proposes a mathematical tool for characterization signals,
   curves and set of points. To evaluate the pattern description power of
   the proposal, an experiment of plat identification based on leaf veins
   image are conducted. Leaf vein is a taxon characteristic used to plant
   identification proposes, and one of its characteristics is that these
   structures are complex, and difficult to be represented as a signal or
   curves and this way to be analyzed in a classical pattern recognition
   approach. Here, we model the veins as a set of points and model as
   graphs. As features, we use the degree and joint degree measurements in
   a dynamic evolution. The results demonstrates that the technique has a
   good power of discrimination and can be used for plant identification,
   as well as other complex pattern recognition tasks},
Publisher = {IOP PUBLISHING LTD},
Address = {DIRAC HOUSE, TEMPLE BACK, BRISTOL BS1 6BE, ENGLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Casanova, D (Corresponding Author), Univ Sao Paulo, IFSC, Av Trabalhador Sao Carlense 400, BR-13560970 Sao Carlos, SP, Brazil.
   Casanova, Dalcimar; Bruno, Odemir Martinez, Univ Sao Paulo, IFSC, BR-13560970 Sao Carlos, SP, Brazil.},
DOI = {10.1088/1742-6596/410/1/012048},
Article-Number = {012048},
ISSN = {1742-6588},
EISSN = {1742-6596},
Keywords-Plus = {FRACTAL DIMENSION; LEAF},
Research-Areas = {Physics},
Web-of-Science-Categories  = {Physics, Applied; Physics, Multidisciplinary},
Author-Email = {arbackes@yahoo.com.br
   dalcimar@gmail.com
   bruno@ifsc.usp.br},
Affiliations = {Universidade de Sao Paulo},
ResearcherID-Numbers = {Bruno, Odemir/AAU-7209-2020
   Bruno, Odemir M/A-5279-2009
   },
ORCID-Numbers = {Bruno, Odemir/0000-0002-2945-1556
   Bruno, Odemir M/0000-0002-2945-1556
   Casanova, Dalcimar/0000-0002-1905-4602
   Backes, Andre/0000-0002-7486-4253},
Cited-References = {Backes AR, 2009, PATTERN RECOGN, V42, P54, DOI 10.1016/j.patcog.2008.07.006.
   Bohn S, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.061914.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Costa LD, 2007, ADV PHYS, V56, P167, DOI 10.1080/00018730601170527.
   Costa L. F., 2008, ADV PHYS, V60, P329.
   Everitt B.S., 2001, APPL MULTIVARIATE AN.
   Fu H, 2006, IEE P-VIS IMAGE SIGN, V153, P881, DOI 10.1049/ip-vis:20060061.
   Gouveia F., 1997, ISIE.
   HICKEY LJ, 1973, AM J BOT, V60, P17, DOI 10.2307/2441319.
   Nam Y, 2005, LECT NOTES COMPUT SC, V3815, P139.
   Plotze RD, 2005, CAN J BOT, V83, P287, DOI {[}10.1139/b05-002, 10.1139/B05-002].
   Plotze RD, 2009, INT J PATTERN RECOGN, V23, P247, DOI 10.1142/S0218001409007156.
   Tang J, 2012, DIGIT SIGNAL PROCESS, V22, P713, DOI 10.1016/j.dsp.2012.04.011.},
Number-of-Cited-References = {14},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {8},
Doc-Delivery-Number = {BDW73},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000315409700048},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:001015084500001,
Author = {Rajkomar, Gandhinee and Pudaruth, Sameerchand},
Title = {A Mobile App for the Identification of Flowers Using Deep Learning},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
Year = {2023},
Volume = {14},
Number = {5},
Pages = {76-102},
Month = {MAY},
Abstract = {Flowers are admired and used by people all around the world for their
   fragrance, religious significance, and medicinal capabilities. The
   accurate taxonomy of these flower species is critical for biodiversity
   conservation and research. Non-experts typically need to spend a lot of
   time examining botanical guides in order to accurately identify a
   flower, which can be challenging and time-consuming. In this study, an
   innovative mobile application named FloralCam has been developed for the
   identification of flower species that are commonly found in Mauritius.
   Our dataset, named FlowerNet, was collected using a smartphone in a
   natural environment setting and consists of 11660 images, with 110
   images for each of the 106 flower species. Seventy percent of the data
   was used for training, twenty percent for validation and the remaining
   ten percent for testing. Using the approach of trans-fer learning,
   pre-trained convolutional neural networks (CNNs) such as the
   InceptionV3, MobileNetV2 and ResNet50V2 were fine tuned on the custom
   dataset created. The best performance was achieved with the fine tuned
   MobileNetV2 model with accuracy 99.74\% and prediction time 0.09
   seconds. The best model was then converted to TensorFlow Lite format and
   integrated in a mobile application which was built using Flutter.
   Furthermore, the models were also tested on the benchmark Oxford 102
   dataset and MobileNetV2 obtained the highest classification accuracy of
   95.90\%. The mobile application, the dataset and the deep learning
   models developed can be used to support future research in the field of
   flower recognition.},
Publisher = {SCIENCE \& INFORMATION SAI ORGANIZATION LTD},
Address = {19 BOLLING RD, BRADFORD, WEST YORKSHIRE, 00000, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Rajkomar, G (Corresponding Author), Univ Mauritius, ICT Dept, FoICDT, Moka, Mauritius.
   Rajkomar, Gandhinee; Pudaruth, Sameerchand, Univ Mauritius, ICT Dept, FoICDT, Moka, Mauritius.},
ISSN = {2158-107X},
EISSN = {2156-5570},
Keywords = {Flowers; deep learning; mobile application; Mauri-tius},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods},
Affiliations = {University of Mauritius},
Cited-References = {{[}Anonymous], 2023, WORLD FLORA ONLINE P.
   Austen GE, 2016, SCI REP-UK, V6, DOI 10.1038/srep33634.
   Aydin D, 2010, CYBERNET SYST, V41, P416, DOI 10.1080/01969722.2010.500799.
   Chanderbali AS, 2016, GENETICS, V202, P1255, DOI 10.1534/genetics.115.182964.
   Das M, 1999, IEEE INTELL SYST APP, V14, P24, DOI 10.1109/5254.796084.
   Dias PA, 2018, COMPUT IND, V99, P17, DOI 10.1016/j.compind.2018.03.010.
   Guru DS, 2011, MATH COMPUT MODEL, V54, P1030, DOI 10.1016/j.mcm.2010.11.032.
   Hong SW, 2012, INT CONF IMAG PROC, P141, DOI 10.1109/IPTA.2012.6469535.
   Hsu TH, 2011, MULTIMED TOOLS APPL, V53, P53, DOI 10.1007/s11042-010-0490-6.
   Islam T, 2021, COMM COM INF SC, V1435, P163, DOI 10.1007/978-3-030-82269-9\_13.
   Kenrick P., 2005, ENCY GEOLOGY, P418, DOI {[}10.1016/b0-12-369396-9/00018-6, DOI 10.1016/B0-12-369396-9/00018-6].
   Kim JH, 2009, 2009 THIRD INTERNATIONAL SYMPOSIUM ON INTELLIGENT INFORMATION TECHNOLOGY APPLICATION, VOL 3, PROCEEDINGS, P580, DOI 10.1109/IITA.2009.407.
   Kumar R. S., 2017, INT J RES SOCIAL SCI, V7, P244.
   Liao LF, 2020, 2020 16TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS 2020), P25, DOI 10.1109/CIS52066.2020.00014.
   Liu YY, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON FUNCTIONAL-STRUCTURAL PLANT GROWTH MODELING, SIMULATION, VISUALIZATION AND APPLICATIONS (FSPMA), P110, DOI 10.1109/FSPMA.2016.7818296.
   Lv RX, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS AND COMPUTER ENGINEERING (ICCECE), P649, DOI 10.1109/ICCECE51280.2021.9342468.
   Mortensen E. N., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P191, DOI 10.1145/218380.218442.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   ODonoghue J., 2017, WHAT 1 FLOWER EARTH.
   Saitoh T, 2004, INT C PATT RECOG, P27, DOI 10.1109/ICPR.2004.1333997.
   Saitoh T, 2000, INT C PATT RECOG, P507, DOI 10.1109/ICPR.2000.906123.
   Soleimanipour A, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105460.
   Tiay T, 2014, 2014 THIRD ICT INTERNATIONAL STUDENT PROJECT CONFERENCE (ICT-ISPC), P99, DOI 10.1109/ICT-ISPC.2014.6923227.
   Togacar M, 2020, MEASUREMENT, V158, DOI 10.1016/j.measurement.2020.107703.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Zhibin Wang, 2020, ICCPR 2020: Proceedings of the 2020 9th International Conference on Computing and Pattern Recognition, P225, DOI 10.1145/3436369.3437427.},
Number-of-Cited-References = {27},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Int. J. Adv. Comput. Sci. Appl.},
Doc-Delivery-Number = {K2WC6},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:001015084500001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000412737200022,
Author = {Ben Mabrouk, Amira and Najjar, Asma and Zagrouba, Ezzeddine},
Editor = {Battiato, S and Braz, J},
Title = {Image Flower Recognition based on a New Method for Color Feature
   Extraction},
Booktitle = {PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION,
   THEORY AND APPLICATIONS (VISAPP 2014), VOL 2},
Year = {2014},
Pages = {201-206},
Note = {9th International Conference on Computer Vision Theory and Applications
   (VISAPP), Lisbon, PORTUGAL, JAN 05-08, 2014},
Organization = {Inst Syst \& Technologies Informat, Control \& Commun; IEEE Comp Soc;
   IEEE Tech Community Visualizat \& Graph},
Abstract = {In this paper, we present, first, a new method for color feature
   extraction based on SURF detectors. Then, we proved its efficiency for
   flower image classification. Therefore, we described visual content of
   the flower images using compact and accurate descriptors. These features
   are combined and the learning process is performed using a multiple
   kernel framework with a SVM classifier. The proposed method has been
   tested on the dataset provided by the university of oxford and achieved
   better results than our implementation of the method proposed by
   Nilsback and Zisserman (Nilsback and Zisserman, 2008) in terms of
   classification rate and execution time.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Ben Mabrouk, A (Corresponding Author), Univ Tunis Elmanar, Inst Super Informat, Team Res SIIVA Lab RIADI, Tunis, Tunisia.
   Ben Mabrouk, Amira; Najjar, Asma; Zagrouba, Ezzeddine, Univ Tunis Elmanar, Inst Super Informat, Team Res SIIVA Lab RIADI, Tunis, Tunisia.},
ISBN = {978-9-8975-8133-5},
Keywords = {SURF; Lab Color Space; Visual Vocabulary; SVM; MKL},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory \& Methods},
Author-Email = {amira\_benmabrouk@yahoo.fr
   najjar.asma@yahoo.fr
   ezzeddine.zagrouba@fsm.rnu.tn},
Affiliations = {Universite de Tunis-El-Manar},
ResearcherID-Numbers = {zagrouba, ezzeddine/AAO-7281-2020
   Ben Mabrouk, Amira/AHD-3796-2022
   Zagrouba, Ezzeddine/D-7896-2014},
ORCID-Numbers = {zagrouba, ezzeddine/0000-0002-2574-9080
   Ben Mabrouk, Amira/0000-0003-2243-6246
   Zagrouba, Ezzeddine/0000-0002-2574-9080},
Cited-References = {Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
   Chai YN, 2011, IEEE I CONF COMP VIS, P2579, DOI 10.1109/ICCV.2011.6126546.
   Guru DS., 2010, INT J COMPUTERS APPL, V1, P21.
   Kadir A, 2011, INT J COMPUTER TREND, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Louradour J, 2007, IEEE T AUDIO SPEECH, V15, P2465, DOI 10.1109/TASL.2007.905147.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Luo J, 2009, INT J IMAGE PROCESS, V3, P143, DOI DOI 10.1007/S11270-006-2859-8.
   Najjar A., 2012, 2012 International Conference on Communications and Information Technology (ICCIT), P397, DOI 10.1109/ICCITechnol.2012.6285834.
   Nilsback M. E., 2007, P BMVC, V1, P570.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.},
Number-of-Cited-References = {11},
Times-Cited = {13},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BI5RE},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000412737200022},
DA = {2023-08-12},
}

@article{ WOS:000658891600001,
Author = {Turkoglu, Muammer and Aslan, Muzaffer and Ari, Ali and Alcin, Zeynep
   Mine and Hanbay, Davut},
Title = {A multi-division convolutional neural network-based plant identification
   system},
Journal = {PEERJ COMPUTER SCIENCE},
Year = {2021},
Month = {MAY 28},
Abstract = {Background. Plants have an important place in the life of all living
   things. Today, there is a risk of extinction for many plant species due
   to climate change and its environmental impact. Therefore, researchers
   have conducted various studies with the aim of protecting the diversity
   of the planet's plant life. Generally, research in this area is aimed at
   determining plant species and diseases, with works predominantly based
   on plant images. Advances in deep learning techniques have provided very
   successful results in this field, and have become widely used in
   research studies to identify plant species.
   Methods. In this paper, a Multi-Division Convolutional Neural Network
   (MD-CNN)-based plant recognition system was developed in order to
   address an agricultural problem related to the classification of plant
   species. In the proposed system, we divide plant images into equal
   nxn-sized pieces, and then deep features are extracted for each piece
   using a Convolutional Neural Network (CNN). For each part of the
   obtained deep features, effective features are selected using the
   Principal Component Analysis (PCA) algorithm. Finally, the obtained
   effective features are combined and classification conducted using the
   Support Vector Machine (SVM) method.
   Results. In order to test the performance of the proposed deep-based
   system, eight different plant datasets were used: Flavia, Swedish, ICL,
   Foliage, Folio, Flowerl7, Flower102, and LeafSnap. According to the
   results of these experimental studies, 100\% accuracy scores were
   achieved for the Flavia, Swedish, and Folio datasets, whilst the ICL,
   Foliage, Flower17, Flower102, and LeafSnap datasets achieved results of
   99.77\%, 99.93\%, 97.87\%, 98.03\%, and 94.38\%, respectively.},
Publisher = {PEERJ INC},
Address = {341-345 OLD ST, THIRD FLR, LONDON, EC1V 9LL, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Aslan, M (Corresponding Author), Bingol Univ, Engn Fac, Elect \& Elect Engn Dept, Bingol, Turkey.
   Turkoglu, Muammer, Samsun Univ, Fac Engn, Dept Software Engn, Samsun, Turkey.
   Aslan, Muzaffer, Bingol Univ, Engn Fac, Elect \& Elect Engn Dept, Bingol, Turkey.
   Ari, Ali; Hanbay, Davut, Inonu Univ, Engn Fac, Comp Engn Dept, Malatya, Turkey.
   Alcin, Zeynep Mine, Vedat Topcuoglu Anatolian Vocat High Sch, Elect \& Elect Dept, Gaziantep, Turkey.},
DOI = {10.7717/peerj-cs.572},
Article-Number = {e572},
EISSN = {2376-5992},
Keywords = {Plant Identification System; Deep features; Support Vector Machine;
   Principal component analysis; Division process},
Keywords-Plus = {CLASSIFICATION; RECOGNITION; FEATURES},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory \& Methods},
Author-Email = {muzafferaslan@bingol.edu.tr},
Affiliations = {Ondokuz Mayis University; Bingol University; Inonu University},
ResearcherID-Numbers = {Aslan, Muzaffer/U-5355-2018
   Hanbay, Davut/AAG-8511-2019},
ORCID-Numbers = {Aslan, Muzaffer/0000-0002-2418-9472
   Hanbay, Davut/0000-0003-2271-7865},
Cited-References = {Aslan M, 2017, J FAC ENG ARCHIT GAZ, V32, P1025.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Beikmohammadi A, 2018, 2018 4TH IRANIAN CONFERENCE ON SIGNAL PROCESSING AND INTELLIGENT SYSTEMS (ICSPIS), P21, DOI 10.1109/ICSPIS.2018.8700547.
   Bertrand S, 2018, ECOL INFORM, V46, P57, DOI 10.1016/j.ecoinf.2018.05.007.
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555.
   Chen X, 2017, CHINESEFOODNET LARGE.
   Cibuk M, 2019, MEASUREMENT, V137, P7, DOI 10.1016/j.measurement.2019.01.041.
   Demir F, 2020, APPL ACOUST, V170, DOI 10.1016/j.apacoust.2020.107520.
   Elhariri E, 2014, 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING \& SYSTEMS (ICCES), P271, DOI 10.1109/ICCES.2014.7030971.
   He K, 2015, C COMPUTER VISION PA.
   Hewitt C., 2018, ARXIV PREPRINT ARXIV, V1811, P08398, DOI {[}10.48550/arXiv.1811.08398, DOI 10.48550/ARXIV.1811.08398].
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.
   Jolliffe IT, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0202.
   Kadir A., 2014, INT J ADV SCI TECHNO, V44, P113.
   Kadir A., 2011, INT J COMPUT APPL, V29, P15, DOI DOI 10.5120/3592-4981.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Liang S, 2018, GENES-BASEL, V9, DOI 10.3390/genes9080382.
   Lopez-Jimenez E, 2019, ECOL INFORM, V52, P131, DOI 10.1016/j.ecoinf.2019.05.005.
   MOORE BC, 1981, IEEE T AUTOMAT CONTR, V26, P17, DOI 10.1109/TAC.1981.1102568.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Nguyen L, 2018, METHODS MOL BIOL, V1795, P1, DOI 10.1007/978-1-4939-7874-8\_1.
   Nijalingappa P, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON APPLIED AND THEORETICAL COMPUTING AND COMMUNICATION TECHNOLOGY (ICATCCT), P338, DOI 10.1109/ICATCCT.2015.7456906.
   Nilsback M.-E., 2006, P IEEE COMP SOC C CO, V2, P1447, DOI DOI 10.1109/CVPR.2006.42.
   Turkmen G., 2018, STAT GROWING DATA SE, DOI {[}10.5772/intechopen.72139, DOI 10.5772/INTECHOPEN.72139].
   Turkoglu M, 2019, TURK J ELECTR ENG CO, V27, P1636, DOI 10.3906/elk-1809-181.
   Turkolu I., 2018, SAKARYA U J COMPUTER, V1, P10.
   Vanrell, 2011, ADV NEURAL INF PROCE, P1.},
Number-of-Cited-References = {29},
Times-Cited = {1},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {12},
Journal-ISO = {PeerJ Comput. Sci.},
Doc-Delivery-Number = {SO3QZ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000658891600001},
OA = {gold, Green Published},
DA = {2023-08-12},
}

@inproceedings{ WOS:000364991400009,
Author = {Backes, Andre Richard and de Mesquita Sa Junior, Jarbas Joaci and Kolb,
   Rosana Marta},
Editor = {Murino, V and Puppo, E},
Title = {A Gravitational Model for Plant Classification Using Adaxial Epidermis
   Texture},
Booktitle = {IMAGE ANALYSIS AND PROCESSING - ICIAP 2015, PT II},
Series = {Lecture Notes in Computer Science},
Year = {2015},
Volume = {9280},
Pages = {89-96},
Note = {18th International Conference on Image Analysis and Processing (ICIAP),
   Genoa, ITALY, SEP 07-11, 2015},
Organization = {Datalogic; Google Inc; Centro Studi Gruppo Orizzonti Holding; Ansaldo
   Energia; EBIT Esaote; Softeco; eVS embedded Vis Syst S r l; 3DFlow S r
   l; Camelot Biomed Syst S r l; Ist Italiano Tecnologia, Pattern Anal \&
   Comp Vis Dept; Univ Genoa; Univ Verona; Camera Commercio Genova; Comune
   Genova},
Abstract = {The leaves are very informative plant organs. They are extensively used
   in plant anatomical studies focusing taxonomy. Their both inner and
   outer structures provide very discriminant features from vegetal
   species. In this study, we propose using images from adaxial epidermis
   for plant classification. The adaxial epidermis is a very variable
   region in a plant leaf cross-section. It differs in color, number of
   layers and presence/absence of hypodermis. To accomplish this task, we
   propose combining complexity analysis methods with a gravitational
   collapsing system to extract texture features from adaxial epidermis
   samples. Experimental results show that this combination of techniques
   surpasses traditional and state-of-the-art methods in both grayscale and
   color images of adaxial epidermis.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Backes, AR (Corresponding Author), Univ Fed Uberlandia, Fac Comp, Av Joao Naves de Avila 2121, BR-38408100 Uberlandia, MG, Brazil.
   Backes, Andre Richard, Univ Fed Uberlandia, Fac Comp, BR-38408100 Uberlandia, MG, Brazil.
   de Mesquita Sa Junior, Jarbas Joaci, Univ Fed Ceara, Dept Engn Comp, BR-62010560 Sobral, Ceara, Brazil.
   Kolb, Rosana Marta, Univ Estadual Paulista, Dept Ciencias Biol, Fac Ciencias \& Letras, BR-19806900 Assis, SP, Brazil.},
DOI = {10.1007/978-3-319-23234-8\_9},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-23234-8; 978-3-319-23233-1},
Keywords = {Adaxial epidermis; Texture analysis; Color; Gravitational system},
Keywords-Plus = {FRACTAL DIMENSION; COMPUTER VISION; COLOR; LACUNARITY; FEATURES},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {arbackes@yahoo.com.br
   jarbasjoaci@yahoo.com.br
   rosanakolb@hotmail.com},
Affiliations = {Universidade Federal de Uberlandia; Universidade Federal do Ceara;
   Universidade Estadual Paulista},
ResearcherID-Numbers = {Kolb, Rosana M./D-3592-2012
   },
ORCID-Numbers = {Kolb, Rosana M./0000-0003-3841-5597
   Backes, Andre/0000-0002-7486-4253},
Cited-References = {ALLAIN C, 1991, PHYS REV A, V44, P3552, DOI 10.1103/PhysRevA.44.3552.
   Azencott R, 1997, IEEE T PATTERN ANAL, V19, P148, DOI 10.1109/34.574796.
   Backes AR, 2009, LECT NOTES COMPUT SC, V5702, P680, DOI 10.1007/978-3-642-03767-2\_83.
   Backes AR, 2012, PATTERN RECOGN, V45, P1984, DOI 10.1016/j.patcog.2011.11.009.
   Backes AR, 2009, INT J PATTERN RECOGN, V23, P1145, DOI 10.1142/S0218001409007508.
   Bianconi F, 2009, PATTERN RECOGN LETT, V30, P765, DOI 10.1016/j.patrec.2009.02.006.
   Casanova D, 2009, INT J IMAG SYST TECH, V19, P236, DOI 10.1002/ima.20201.
   Sa JJD, 2013, J MATH IMAGING VIS, V47, P70, DOI 10.1007/s10851-012-0408-1.
   Sa JJD, 2013, ECOL INFORM, V15, P34, DOI 10.1016/j.ecoinf.2013.02.007.
   Sa JJD, 2013, PATTERN RECOGN, V46, P1628, DOI 10.1016/j.patcog.2012.12.008.
   Sa JJD, 2012, PATTERN RECOGN, V45, P732, DOI 10.1016/j.patcog.2011.07.023.
   Du G, 2002, IEEE T GEOSCI REMOTE, V40, P2687, DOI {[}10.1109/TGRS.2002.807001, 10.1109/tgrs.2002.807001].
   Everitt B.S., 2001, APPL MULTIVARIATE AN.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   Hoang MA, 2005, SIGNAL PROCESS, V85, P265, DOI 10.1016/j.sigpro.2004.10.009.
   Kaplan LM, 1999, IEEE T IMAGE PROCESS, V8, P1572, DOI 10.1109/83.799885.
   LAINE A, 1993, IEEE T PATTERN ANAL, V15, P1186, DOI 10.1109/34.244679.
   Liu GH, 2011, PATTERN RECOGN, V44, P2123, DOI 10.1016/j.patcog.2011.02.003.
   Mandelbrot B.B., 2000, FRACTAL GEOMETRY NAT, V19th.
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803.
   Paschos G, 2003, PATTERN RECOGN LETT, V24, P309, DOI 10.1016/S0167-8655(02)00244-1.
   Plotze RD, 2005, CAN J BOT, V83, P287, DOI {[}10.1139/b05-002, 10.1139/B05-002].
   Porebski A., 2008, 2008 1 WORKSHOPS IMA, P1, DOI DOI 10.1109/IPTA.2008.4743780.
   Sa JJD, 2011, BOTANY, V89, P467, DOI {[}10.1139/B11-038, 10.1139/b11-038].
   Tricot C., 1995, CURVES FRACTAL DIMEN.},
Number-of-Cited-References = {25},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BD9OF},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000364991400009},
DA = {2023-08-12},
}

@inproceedings{ WOS:000360831800056,
Author = {Zhang, William Y. and Hua, Xian-Sheng},
Book-Group-Author = {IEEE},
Title = {PLANT IDENTIFICATION WITH NOISY WEB DATA},
Booktitle = {2014 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME)},
Series = {IEEE International Conference on Multimedia and Expo},
Year = {2014},
Note = {IEEE International Conference on Multimedia and Expo Workshops (ICMEW),
   Chengdu, PEOPLES R CHINA, JUL 14-18, 2014},
Organization = {IEEE; Baidu; NSFC; NSF; QIY; BOCOM; Yahoo; IBM Res; RICOH; Microsoft Res},
Abstract = {One of the main problems in image based plant identification has been
   the lack of quality training image data. A few attempts for solving this
   problem through generating high quality plant images from crowd sourced
   Web image collections like Flickr are proposed in this paper. These
   methods try to automatically identify correct and informative training
   images from those Web images, which typically have very noisy metadata
   (for example, user tags in Flickr), to enhance existing manually labeled
   training set. Firstly, for each plant, a set of images is collected from
   searching Flickr by using the plant name as the query. Then, images are
   clustered into visually consistent clusters, and in each cluster
   hopefully a majority of the images are all relevant or irrelevant to the
   particular plant. From these clusters, a managed plant image data set
   from ImageCLEF is used as reference to automatically select the highest
   quality cluster for each plant. The image quality of the selected
   clusters is further improved by two algorithms: an iterative method and
   image similarity based ranking. We show that the larger training data
   set automatically selected by this method significantly increases the
   accuracy of image based plant identification. In addition, this approach
   is a generic solution to almost all image recognition problems as long
   as additional (noisy) training data can be obtained from the Internet
   automatically.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zhang, WY (Corresponding Author), Monta Vista High Sch, Cupertino, CA 95014 USA.
   Zhang, William Y., Monta Vista High Sch, Cupertino, CA 95014 USA.
   Hua, Xian-Sheng, Microsoft Res, Philadelphia, PA USA.},
ISSN = {1945-7871},
ISBN = {978-1-4799-4761-4},
Keywords = {Image classification; machine learning; plant identification; crowd
   sourced big data},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Software Engineering; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic},
Author-Email = {billzhang88@gmail.com
   xshua@microsoft.com},
Affiliations = {Microsoft},
Cited-References = {Elkan C., 2003, ICML.
   Fan RE, 2008, J MACH LEARN RES, V9, P1871.
   Goeau H., IMAGECLEF 2012 PLANT.
   Hastie, ELEMENTS STAT LEARNI, P520.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Paris Sebastien, PARTICIPATION LSIS D.
   Vedaldi A., 2008, VLFEAT OPEN PORTABLE.
   Wang Gang, 2012, IEEE T PATTERN ANAL.},
Number-of-Cited-References = {8},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BD4JD},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000360831800056},
DA = {2023-08-12},
}

@inproceedings{ WOS:000271604900029,
Author = {Liu, Jiandu and Zhang, Shanwen and Deng, Shengli},
Editor = {Huang, DS and Jo, KH and Lee, HH and Kang, HJ and Bevilacqua, V},
Title = {A Method of Plant Classification Based on Wavelet Transforms and Support
   Vector Machines},
Booktitle = {EMERGING INTELLIGENT COMPUTING TECHNOLOGY AND APPLICATIONS, PROCEEDINGS},
Series = {Lecture Notes in Computer Science},
Year = {2009},
Volume = {5754},
Pages = {253+},
Note = {5th International Conference on Intelligent Computing, Ulsan, SOUTH
   KOREA, SEP 16-19, 2009},
Organization = {IEEE Computat Intelligence Soc; Int Neural Network Soc; Natl Nat Sci Fdn
   China},
Abstract = {As one of the most important morphological taxonomy features, plant leaf
   with many strong points has significant influence on research. In this
   paper, we propose a novel method of plant classification froth leaf
   image set based on wavelet transforms and support vector machines
   (SVMS). Firstly, the leaf images are converted into the time-frequency
   domain image by wavelet transforms without any further preprocessing
   such as image enhancement and texture thinning, and then feature
   extraction vector is conducted. Then the effectiveness of the proposed
   method is evaluated by the classification accuracy of SVM classifier.
   The experimental results about the data set with 300 leaf images show
   that the method has higher recognition rate and faster processing speed.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Liu, JD (Corresponding Author), AF Engn Univ, Missile Inst, Sanyuan 713800, Peoples R China.
   Liu, Jiandu, AF Engn Univ, Missile Inst, Sanyuan 713800, Peoples R China.
   Zhang, Shanwen, Hefei Inst Intelligent Machines, Chinese Acad Sci, Hefei 230031, Anhui, Peoples R China.
   Deng, Shengli, Air Force 93861 Army, Hefei 713800, Peoples R China.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-642-04069-6},
Keywords = {Plant leaf image feature extraction; Wavelet transforms; Support vector
   machines},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory \& Methods},
Author-Email = {ljdzxj@sohu.com},
Affiliations = {Air Force Engineering University; Chinese Academy of Sciences},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}60805021]},
Funding-Text = {This work was supported by the grant of the National Natural Science
   Foundation of China, No. 60805021.},
Cited-References = {BRENDEL T, 1995, P SPIE, V2345.
   Brown MPS, 2000, P NATL ACAD SCI USA, V97, P262, DOI 10.1073/pnas.97.1.262.
   DALLWITZ MJ, 1980, TAXON, V29, P41, DOI 10.2307/1219595.
   DEOLIVEIRA P, 2005, CANADA J BOT, V83.
   Du J. X., 2007, APPL MATH COMPUTATIO, V185.
   Fu H., 2004, IEEE 2004 8 INT C CO.
   FU H, 2006, IEE P VISION IMAGE S, V153.
   FU H, 2003, P IEEE INT C NEUR NE.
   GOUVEIA F, 1997, P IEEE INT S IND EL.
   Gu X, 2005, LECT NOTES COMPUT SC, V3644, P253.
   HONG SM, 2005, COMPUTER ANIMATION V, V16.
   KADAMBE S, 1992, IEEE T INFORM THEORY, V32, P712.
   LI Y, 2005, P IEEE INT C NEUR NE.
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463.
   MIAO Z, 2006, ENG APPL ARTIFICIAL, V19.
   MUKHERJEE S, 1999, 182 CBCL MIT ART INT.
   Nam Y, 2005, LECT NOTES COMPUT SC, V3687, P589.
   QI H, 2003, P 2 INT C MACH LEARN.
   Wang XF, 2005, LECT NOTES COMPUT SC, V3644, P87.
   Wang Z., 2003, IEE P VISION IMAGE S, V150.
   WARREN D, 1997, P IEE 6 INT C IM PRO.
   Ye Y., 2004, P 2004 INT S INT MUL.},
Number-of-Cited-References = {22},
Times-Cited = {17},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BLZ67},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000271604900029},
DA = {2023-08-12},
}

@article{ WOS:000534628800067,
Author = {Miyoshi, Gabriela Takahashi and Arruda, Mauro dos Santos and Osco, Lucas
   Prado and Marcato Junior, Jose and Goncalves, Diogo Nunes and Imai,
   Nilton Nobuhiro and Garcia Tommaselli, Antonio Maria and Honkavaara,
   Eija and Goncalves, Wesley Nunes},
Title = {A Novel Deep Learning Method to Identify Single Tree Species in
   UAV-Based Hyperspectral Images},
Journal = {REMOTE SENSING},
Year = {2020},
Volume = {12},
Number = {8},
Month = {APR},
Abstract = {Deep neural networks are currently the focus of many remote sensing
   approaches related to forest management. Although they return
   satisfactory results in most tasks, some challenges related to
   hyperspectral data remain, like the curse of data dimensionality. In
   forested areas, another common problem is the highly-dense distribution
   of trees. In this paper, we propose a novel deep learning approach for
   hyperspectral imagery to identify single-tree species in highly-dense
   areas. We evaluated images with 25 spectral bands ranging from 506 to
   820 nm taken over a semideciduous forest of the Brazilian Atlantic
   biome. We included in our network's architecture a band combination
   selection phase. This phase learns from multiple combinations between
   bands which contributed the most for the tree identification task. This
   is followed by a feature map extraction and a multi-stage model
   refinement of the confidence map to produce accurate results of a
   highly-dense target. Our method returned an f-measure, precision and
   recall values of 0.959, 0.973, and 0.945, respectively. The results were
   superior when compared with a principal component analysis (PCA)
   approach. Compared to other learning methods, ours estimate a
   combination of hyperspectral bands that most contribute to the mentioned
   task within the network's architecture. With this, the proposed method
   achieved state-of-the-art performance for detecting and geolocating
   individual tree-species in UAV-based hyperspectral images in a complex
   forest.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Miyoshi, GT (Corresponding Author), Sao Paulo State Univ UNESP, Grad Program Cartog Sci, BR-19060900 Presidente Prudente, SP, Brazil.
   Miyoshi, Gabriela Takahashi; Imai, Nilton Nobuhiro; Garcia Tommaselli, Antonio Maria, Sao Paulo State Univ UNESP, Grad Program Cartog Sci, BR-19060900 Presidente Prudente, SP, Brazil.
   Arruda, Mauro dos Santos; Goncalves, Diogo Nunes; Goncalves, Wesley Nunes, Fed Univ Mato Grosso do Sul UFMS, Fac Comp Sci, Grad Program Comp Sci, Av Costa \& Silva, BR-79070900 Campo Grande, MS, Brazil.
   Osco, Lucas Prado, Univ Western Sao Paulo UNOESTE, Fac Engn \& Architecture \& Urbanism, R Jose Bongiovani,Cidade Univ, BR-19050920 Presidente Prudente, SP, Brazil.
   Osco, Lucas Prado; Marcato Junior, Jose; Goncalves, Wesley Nunes, Fed Univ Mato Grosso do Sul UFMS, Fac Engn Architecture \& Urbanism \& Geog, Av Costa \& Silva, BR-79070900 Campo Grande, MS, Brazil.
   Imai, Nilton Nobuhiro; Garcia Tommaselli, Antonio Maria, Sao Paulo State Univ UNESP, Dept Cartog, BR-19060900 Presidente Prudente, SP, Brazil.
   Honkavaara, Eija, Natl Land Survey Finland, Finnish Geospatial Res Inst, Geodeetinrinne 2, Masala 02430, Finland.},
DOI = {10.3390/rs12081294},
Article-Number = {1294},
EISSN = {2072-4292},
Keywords = {high-density object; data-reduction; band selection; convolutional
   neural network; tree species identification},
Keywords-Plus = {CONVOLUTIONAL NEURAL-NETWORK; SUPPORT VECTOR MACHINE; BARK BEETLE
   DAMAGE; SUCCESSIONAL STAGES; RANDOM FOREST; CLASSIFICATION; CAMERA},
Research-Areas = {Environmental Sciences \& Ecology; Geology; Remote Sensing; Imaging
   Science \& Photographic Technology},
Web-of-Science-Categories  = {Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science \& Photographic Technology},
Author-Email = {gabriela.t.miyoshi@unesp.br
   mauro.arruda@ufms.br
   lucasosco@unoeste.br
   jose.marcato@ufms.br
   diogo.goncalves@ufms.br
   nilton.imai@unesp.br
   a.tommaselli@unesp.br
   eija.honkavaara@nls.fi
   wesley.goncalves@ufms.br},
Affiliations = {Universidade Estadual Paulista; Universidade Federal de Mato Grosso do
   Sul; Universidade do Oeste Paulista; Universidade Federal de Mato Grosso
   do Sul; Universidade Estadual Paulista; The National Land Survey of
   Finland; Finnish Geospatial Research Institute (FGI)},
ResearcherID-Numbers = {Osco, Lucas Prado/V-3997-2018
   Tommaselli, Antonio/D-1739-2012
   Marcato Junior, Jose/H-4406-2017
   Imai, Nilton/O-8909-2018},
ORCID-Numbers = {Osco, Lucas Prado/0000-0002-0258-536X
   Tommaselli, Antonio/0000-0003-0483-1103
   dos Santos de Arruda, Mauro/0000-0002-2273-8968
   Takahashi Miyoshi, Gabriela/0000-0002-8571-1383
   Honkavaara, Eija/0000-0002-7236-2145
   Marcato Junior, Jose/0000-0002-9096-6866
   Imai, Nilton/0000-0003-0516-0567},
Funding-Acknowledgement = {Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior-Brasil
   (CAPES) {[}001]; CAPES/PDSE {[}88881.187406/2018-01]; National Council
   for Scientific and Technological Development (CNPq) {[}303559/2019-5,
   433783/2018-4, 153854/2016-2]; Sao Paulo Research Foundation (FAPESP)
   {[}2013/50426-4]; Academy of Finland {[}327861]; CAPES/PrInt; Academy of
   Finland (AKA) {[}327861, 327861] Funding Source: Academy of Finland
   (AKA)},
Funding-Text = {This study was financed in part by the Coordenacao de Aperfeicoamento de
   Pessoal de Nivel Superior-Brasil (CAPES)-Finance Code 001, CAPES/PrInt,
   and CAPES/PDSE, grant number 88881.187406/2018-01; in part by the
   National Council for Scientific and Technological Development (CNPq),
   grant number 303559/2019-5, 433783/2018-4, and 153854/2016-2; in part by
   the Sao Paulo Research Foundation (FAPESP), grant number 2013/50426-4;
   and in part by the Academy of Finland, grant number 327861.},
Cited-References = {Aasen H, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10071091.
   Aich S., 2018, ARXIV180305494.
   Alshehhi R, 2017, ISPRS J PHOTOGRAMM, V130, P139, DOI 10.1016/j.isprsjprs.2017.05.002.
   Ampatzidis Y, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11040410.
   {[}Anonymous], 2005, REMOTE SENSING DIGIT.
   {[}Anonymous], 2007, APPL MULTIVARIATE ST, DOI DOI 10.4236/JWARP.2010.26066.
   Audebert N, 2019, IEEE GEOSC REM SEN M, V7, P159, DOI 10.1109/MGRS.2019.2912563.
   Belgiu M, 2016, ISPRS J PHOTOGRAMM, V114, P24, DOI 10.1016/j.isprsjprs.2016.01.011.
   Berveglieri A, 2018, ISPRS J PHOTOGRAMM, V146, P548, DOI 10.1016/j.isprsjprs.2018.11.002.
   Berveglieri A, 2016, IEEE J-STARS, V9, P5385, DOI 10.1109/JSTARS.2016.2606320.
   Bioucas-Dias JM, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2244672.
   Brasil D.F., 2011, ESPECIES NATIVAS FLO.
   Cao JJ, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010089.
   Clark ML, 2012, REMOTE SENS-BASEL, V4, P1820, DOI 10.3390/rs4061820.
   Colgan MS, 2012, REMOTE SENS-BASEL, V4, P3462, DOI 10.3390/rs4113462.
   Csillik O, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2040039.
   da Silva FR, 2011, STUD NEOTROP FAUNA E, V46, P163, DOI 10.1080/01650521.2011.617065.
   Dalponte M, 2012, REMOTE SENS ENVIRON, V123, P258, DOI 10.1016/j.rse.2012.03.013.
   dos Santos AA, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19163595.
   Elias GA, 2019, FLORESTA AMBIENTE, V26, DOI 10.1590/2179-8087.041318.
   Giombini MI, 2017, HEREDITY, V118, P568, DOI 10.1038/hdy.2016.130.
   Guimaraes N, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12061046.
   Hartling S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061284.
   Hennessy A, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12010113.
   Honkavaara E, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10020256.
   Honkavaara E, 2017, ISPRS J PHOTOGRAMM, V134, P96, DOI 10.1016/j.isprsjprs.2017.10.014.
   Honkavaara E, 2013, REMOTE SENS-BASEL, V5, P5006, DOI 10.3390/rs5105006.
   Imangholiloo M, 2019, FORESTS, V10, DOI 10.3390/f10050415.
   Jensen JR., 2007, REMOTE SENSING ENV E.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Khamparia A, 2019, EXPERT SYST, V36, DOI 10.1111/exsy.12400.
   Li LY, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12020298.
   Li WJ, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9010022.
   Lin T., ARXIV170802002.
   Liu LY, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111113.
   Torres DL, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020563.
   LORENZI H, 1992, ARVORES BRASILEIRAS, V1.
   Ma L, 2019, ISPRS J PHOTOGRAMM, V152, P166, DOI 10.1016/j.isprsjprs.2019.04.015.
   Marrs J, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11070819.
   Maschler J, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081218.
   Maxwell AE, 2018, INT J REMOTE SENS, V39, P2784, DOI 10.1080/01431161.2018.1433343.
   Mendes CP, 2016, ECOGRAPHY, V39, P465, DOI 10.1111/ecog.01592.
   Miyoshi GT, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12020244.
   Miyoshi GT, 2018, INT J REMOTE SENS, V39, P4910, DOI 10.1080/01431161.2018.1425570.
   Nasi R, 2018, URBAN FOR URBAN GREE, V30, P72, DOI 10.1016/j.ufug.2018.01.010.
   Nasi R, 2015, REMOTE SENS-BASEL, V7, P15467, DOI 10.3390/rs71115467.
   Natesan S., 2019, INT ARCH PHOTOGRAMME, V4213, P475, DOI {[}DOI 10.5194/ISPRS-ARCHIVES-XLII-2-W13-475-2019, 10.5194/isprs-archives-XLII-2-W13-475-2019].
   Navarro A, 2020, REMOTE SENS ENVIRON, V242, DOI 10.1016/j.rse.2020.111747.
   Nevalainen O, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9030185.
   Nezami S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12071070.
   Osco LP, 2020, ISPRS J PHOTOGRAMM, V160, P97, DOI 10.1016/j.isprsjprs.2019.12.010.
   Osco LP, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11242925.
   Ozcan AH, 2017, REMOTE SENS LETT, V8, P761, DOI 10.1080/2150704X.2017.1322733.
   Raczko E, 2017, EUR J REMOTE SENS, V50, P144, DOI 10.1080/22797254.2017.1299557.
   Redmon J., ARXIV180402767.
   Reis BP, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11131508.
   Ren S, 2015, P 28 INT C NEURAL IN.
   Saarinen N, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10020338.
   Safonova A, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11060643.
   Sica YV, 2014, BIOTROPICA, V46, P696, DOI 10.1111/btp.12152.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Smith GM, 1999, INT J REMOTE SENS, V20, P2653, DOI 10.1080/014311699211994.
   Sothe C, 2020, GISCI REMOTE SENS, V57, P369, DOI 10.1080/15481603.2020.1712102.
   STORY M, 1986, PHOTOGRAMM ENG REM S, V52, P397.
   Sylvain JD, 2019, ISPRS J PHOTOGRAMM, V156, P14, DOI 10.1016/j.isprsjprs.2019.07.010.
   Casapia XT, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12010009.
   Pham TD, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11030230.
   Tuominen S, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10050714.
   Weinstein B., 2019, BIORXIV, DOI 10.1101/532952.
   Xie ZL, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11020164.
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660.},
Number-of-Cited-References = {71},
Times-Cited = {40},
Usage-Count-Last-180-days = {10},
Usage-Count-Since-2013 = {50},
Journal-ISO = {Remote Sens.},
Doc-Delivery-Number = {LP9II},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000534628800067},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000550799000016,
Author = {Lee, Sue Han and Chang, Yang Loong and Chan, Chee Seng and Alexis, Joly
   and Bonnet, Pierre and Goeau, Herve},
Editor = {Bellot, P and Trabelsi, C and Mothe, J and Murtagh, F and Nie, JY and Soulier, L and SanJuan, E and Cappellato, L and Ferro, N},
Title = {Plant Classification Based on Gated Recurrent Unit},
Booktitle = {EXPERIMENTAL IR MEETS MULTILINGUALITY, MULTIMODALITY, AND INTERACTION
   (CLEF 2018)},
Series = {Lecture Notes in Computer Science},
Year = {2018},
Volume = {11018},
Pages = {169-180},
Note = {9th International Conference of the
   Conference-and-Labs-of-the-Evaluation-Forum-Association, (CLEF), Univ
   Avignon, Avignon, FRANCE, SEP 10-14, 2018},
Organization = {Conf \& Labs Evaluat Forum Assoc; Marseille Univ; Toulon Univ; Special
   Interest Grp on Informat Retrieval; Lab Informatique Avignon; Aix
   Marseille Univ; Brain \& Language Res Inst; Assoc Francophone Rech
   Informat Applicat; Avignon Univ},
Abstract = {Classification of plants based on a multi-organ approach is very
   challenging due to the variability in shape and appearance in plant
   organs. Despite promising solutions built using convolutional neural
   network (CNN) for plant classification, the existing approaches do not
   consider the correspondence between different views captured of a plant.
   In fact, botanists usually observe and study simultaneously a plant from
   different vintage points, as a whole and also analyse different organs
   in order to disambiguate species. Driven by this insight, we introduce a
   new framework for plant structural learning using the recurrent neural
   network (RNN) approach. This novel approach supports classification
   based on a varying number of plant views composed of one or more organs
   of a plant, by optimizing the dependencies between them. We also present
   the qualitative results of our proposed models by visualizing the
   learned attention maps. To our knowledge, this is the first study to
   venture into such dependencies modeling and interpret the respective
   neural net for plant classification. Finally, we show that our proposed
   method outperforms the conventional CNN approach on the PlantClef2015
   benchmark. The source code and models are available at
   https://github.com/cschan/Deep-Plant.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Lee, SH (Corresponding Author), Univ Malaya, Fac Comp Sci \& Informat Technol, Ctr Image \& Signal Proc, Kuala Lumpur, Malaysia.
   Lee, Sue Han; Chang, Yang Loong; Chan, Chee Seng, Univ Malaya, Fac Comp Sci \& Informat Technol, Ctr Image \& Signal Proc, Kuala Lumpur, Malaysia.
   Alexis, Joly, INRIA, Montpellier, France.
   Bonnet, Pierre; Goeau, Herve, CIRAD Amap, Montpellier, France.},
DOI = {10.1007/978-3-319-98932-7\_16},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-98932-7; 978-3-319-98931-0},
Keywords = {Plant classification; Deep learning; Recurrent neural network},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Interdisciplinary Applications; Computer
   Science, Software Engineering},
Author-Email = {leesuehan@siswa.um.edu.my
   yangloong@siswa.umedu.my
   cs.chan@um.edu.my
   alexis.joly@inria.fr
   pierre.bonnet@cirad.fr
   herve.goeau@inria.fr},
Affiliations = {Universiti Malaya; Inria; CIRAD; Universite de Montpellier},
ResearcherID-Numbers = {Chan, Chee Seng/B-9754-2011
   Lee, Sue Han/AAM-6250-2021
   joly, alexis/AAV-3101-2021
   Bonnet, Pierre/AAG-6819-2020
   },
ORCID-Numbers = {Chan, Chee Seng/0000-0001-7677-2865
   joly, alexis/0000-0002-2161-9940
   Bonnet, Pierre/0000-0002-2828-4389
   Goeau, Herve/0000-0003-3296-3795},
Cited-References = {Abadi M., 2016, TENSORFLOW LARGE SCA, DOI {[}DOI 10.1038/NN.3331, DOI 10.5555/3026877.3026899].
   Ba J., 2014, MULTIPLE OBJECT RECO, V1412, P7755.
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314.
   BYEON W, 2015, PROC CVPR IEEE, P3547, DOI DOI 10.1109/CVPR.2015.7298977.
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044.
   Choi Sungbin, 2015, CLEF.
   Chung J., 2014, ARXIV14123555.
   Clark A, 2013, BEHAV BRAIN SCI, V36, P181, DOI 10.1017/S0140525X12000477.
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953.
   Goeau H., 2014, P CLEF C SHEFF UK SE, P724.
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947.
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462.
   Joly A, 2016, LECT NOTES COMPUT SC, V9822, P286, DOI 10.1007/978-3-319-44564-9\_26.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Kumar A, 2016, PR MACH LEARN RES, V48.
   Lee S.H., 2017, WORKING NOTES CLEF 2.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Lee SH, 2017, IEEE IMAGE PROC, P4462, DOI 10.1109/ICIP.2017.8297126.
   McCool C., 2015, P C LABS EV FOR JAN.
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101.
   Ren M., 2017, ARXUV160509410.
   Reyes A.K., 2015, C LABS EV FOR SEPT, V1391, P9.
   Romera-Paredes B, 2016, LECT NOTES COMPUT SC, V9910, P312, DOI 10.1007/978-3-319-46466-4\_19.
   Shuai B, 2016, PROC CVPR IEEE, P3620, DOI 10.1109/CVPR.2016.394.
   Szucs G., 2014, CLEF 2014 C SHEFF GR, P763.
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640.
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328.
   Xiong CM, 2016, PR MACH LEARN RES, V48.
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7\_28.
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10.
   Yanikoglu B., 2014, CLEF WORKING NOTES.
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496.},
Number-of-Cited-References = {34},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BP4CE},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000550799000016},
DA = {2023-08-12},
}

@inproceedings{ WOS:000889380800115,
Author = {Bacanin, Nebojsa and Zivkovic, Miodrag and Sarac, Marko and Petrovic,
   Aleksandar and Strumberger, Ivana and Antonijevic, Milos and Petrovic,
   Andrija and Venkatachalam, K.},
Editor = {Kahraman, C and Tolga, AC and Onar, SC and Cebi, S and Oztaysi, B and Sari, IU},
Title = {A Novel Multiswarm Firefly Algorithm: An Application for Plant
   Classification},
Booktitle = {INTELLIGENT AND FUZZY SYSTEMS: DIGITAL ACCELERATION AND THE NEW NORMAL,
   INFUS 2022, VOL 1},
Series = {Lecture Notes in Networks and Systems},
Year = {2022},
Volume = {504},
Pages = {1007-1016},
Note = {4th International Conference on Intelligent and Fuzzy Systems (INFUS),
   Bornova, TURKEY, JUL 19-21, 2022},
Abstract = {Areas of swarm intelligence and machine learning are constantly
   evolving, recently attracting even more researchers world-wide. This
   stems from the no free lunch which states that universal approach that
   could render satisfying results for all practical challenges does not
   exist. Therefore, in this research a novel multi-swarm firefly
   algorithm, that tries to address flaws of original firefly
   metaheuristics, is proposed. Devised algorithm is applied to interesting
   and important practical challenge of plants classification, as part of
   the hybrid framework between machine learning and optimization
   metaheuristics. For this purpose, a set of 1,000 random images of
   healthy leaves, from one public repository, is retrieved for the
   following plants: apple, cherry, pepper and tomato. Hybrid framework
   includes pre-processing, constructing bag of features and classification
   steps. After pre-processing, a bag of features is constructed by
   utilizing well-known scale-invariant feature transform algorithm,
   K-means-based vocabulary generation and histogram. Such images are then
   categorized with support vector machine classifier. However, to obtain
   satisfying results for a particular dataset, the support vector machines
   hyper-parameters' need to be tuned and in the proposed research
   multi-swarm firefly algorithm is employed to determine optimal
   (sub-optimal) hyper-parameters' values for this practical challenge.
   Comparative analysis with the basic firefly metaheuristics and other
   well-known swarm intelligence algorithms was conducted to assess the
   performance of the proposed method in terms of precision, recall,
   F-score for this multi-class classification challenge. Obtained results
   show significant performance improvements of devised method over the
   original firefly algorithm and also better metrics than other
   state-of-the-art techniques in the majority of cases.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Bacanin, N (Corresponding Author), Singidunum Univ, Danijelova 32, Belgrade 11000, Serbia.
   Bacanin, Nebojsa; Zivkovic, Miodrag; Sarac, Marko; Petrovic, Aleksandar; Strumberger, Ivana; Antonijevic, Milos; Petrovic, Andrija, Singidunum Univ, Danijelova 32, Belgrade 11000, Serbia.
   Venkatachalam, K., Univ Hradec Kralove, Hradec Kralove, Czech Republic.},
DOI = {10.1007/978-3-031-09173-5\_115},
ISSN = {2367-3370},
EISSN = {2367-3389},
ISBN = {978-3-031-09173-5; 978-3-031-09172-8},
Keywords = {Multi-swarm firefly algorithm; Optimization; Plan classification; Swarm
   intelligence; Support vector machine},
Keywords-Plus = {FEATURES},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications},
Author-Email = {nbacanin@singidunum.ac.rs
   mzivkovic@singidunum.ac.rs
   msarac@singidunum.ac.rs
   aleksandar.petrovic@singidunum.ac.rs
   istrumberger@singidunum.ac.rs
   mantonijevic@singidunum.ac.rs
   apetrovic@singidunum.ac.rs
   venkatachalam.k@ieee.org},
Affiliations = {University of Hradec Kralove},
ResearcherID-Numbers = {Antonijevic, Milos/GVS-6027-2022
   Bacanin, Nebojsa/L-5328-2019
   Petrović, Aleksandar/GZG-9930-2022
   },
ORCID-Numbers = {Antonijevic, Milos/0000-0002-5511-2531
   Bacanin, Nebojsa/0000-0002-2062-924X
   K, Venkatachalam/0000-0002-2353-8853
   Sarac, Marko/0000-0001-8241-2778
   Zivkovic, Miodrag/0000-0002-4351-068X
   Petrovic, Aleksandar/0000-0003-3324-3909
   Strumberger, Ivana/0000-0002-1154-6696},
Funding-Acknowledgement = {Ministry of Education, Science and Technological Development of Republic
   of Serbia {[}III-44006]},
Funding-Text = {The paper is supported by the Ministry of Education, Science and
   Technological Development of Republic of Serbia, Grant No. III-44006.},
Cited-References = {Altameem A, 2022, CMC-COMPUT MATER CON, V71, P4719, DOI 10.32604/cmc.2022.022177.
   Azhar R, 2015, PROCEDIA COMPUT SCI, V72, P24, DOI 10.1016/j.procs.2015.12.101.
   Bacanin Nebojsa, 2022, Intelligent and Fuzzy Techniques for Emerging Conditions and Digital Transformation: Proceedings of the INFUS 2021 Conference. Lecture Notes in Networks and Systems (308), P281, DOI 10.1007/978-3-030-85577-2\_33.
   Bacanin N., 2021, IEEE ACCESS.
   Bacanin N, 2022, LECT NOTE DATA ENG, V68, P397, DOI 10.1007/978-981-16-1866-6\_29.
   Bacanin N, 2022, SUSTAIN COMPUT-INFOR, V35, DOI 10.1016/j.suscom.2022.100711.
   Bacanin N, 2022, NEURAL COMPUT APPL, V34, P9043, DOI 10.1007/s00521-022-06925-y.
   Bacanin N, 2021, MATHEMATICS-BASEL, V9, DOI 10.3390/math9212705.
   Bacanin N, 2022, CMC-COMPUT MATER CON, V70, P4199, DOI 10.32604/cmc.2022.020449.
   Bacanin N, 2021, J REAL-TIME IMAGE PR, V18, P1085, DOI 10.1007/s11554-021-01106-x.
   Bacanin N, 2014, SCI WORLD J, DOI 10.1155/2014/721521.
   Bezdan Timea, 2021, Intelligent and Fuzzy Techniques: Smart and Innovative Solutions. Proceedings of the INFUS 2020 Conference. Advances in Intelligent Systems and Computing (AISC 1197), P955, DOI 10.1007/978-3-030-51156-2\_111.
   Bezdan T, 2021, 7 C ENG COMP BAS SYS, P1.
   Chen HL, 2012, J MED SYST, V36, P2505, DOI 10.1007/s10916-011-9723-0.
   Chen JD, 2020, COMPUT ELECTRON AGR, V173, DOI 10.1016/j.compag.2020.105393.
   Dutta K., 2018, CCIS, V841, P470, DOI 10.1007/978-981-13-0020-241.
   Kaur Prableen, 2022, Proceedings of Data Analytics and Management: ICDAM 2021. Lecture Notes on Data Engineering and Communications Technologies (91), P357, DOI 10.1007/978-981-16-6285-0\_29.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Lv SJ, 2022, APPL MATH COMPUT, V412, DOI 10.1016/j.amc.2021.126586.
   Mirjalili S, 2017, ADV ENG SOFTW, V114, P163, DOI 10.1016/j.advengsoft.2017.07.002.
   Mirjalili S, 2016, ADV ENG SOFTW, V95, P51, DOI 10.1016/j.advengsoft.2016.01.008.
   Mirjalili S, 2016, KNOWL-BASED SYST, V96, P120, DOI 10.1016/j.knosys.2015.12.022.
   Hughes DP, 2016, Arxiv, DOI arXiv:1511.08060.
   Rzanny M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0245-8.
   Storn R, 1996, 1996 BIENNIAL CONFERENCE OF THE NORTH AMERICAN FUZZY INFORMATION PROCESSING SOCIETY - NAFIPS, P519, DOI 10.1109/NAFIPS.1996.534789.
   Strumberger Ivana, 2019, 2019 International Young Engineers Forum (YEF-ECE). Proceedings, P59, DOI 10.1109/YEF-ECE.2019.8740818.
   Wang MJ, 2020, APPL SOFT COMPUT, V88, DOI 10.1016/j.asoc.2019.105946.
   Yang XS, 2012, ENG COMPUTATION, V29, P464, DOI 10.1108/02644401211235834.
   Yang XS, 2009, LECT NOTES COMPUT SC, V5792, P169, DOI 10.1007/978-3-642-04944-6\_14.
   Zivkovic Miodrag, 2021, Proceedings of International Conference on Sustainable Expert Systems. ICSES 2020. Lecture Notes in Networks and Systems (LNNS 176), P169, DOI 10.1007/978-981-33-4355-9\_14.
   Zivkovic Miodrag, 2020, 2020 International Wireless Communications and Mobile Computing (IWCMC), P1176, DOI 10.1109/IWCMC48107.2020.9148087.
   Zivkovic M, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22051711.
   Zivkovic M, 2021, SUSTAIN CITIES SOC, V66, DOI 10.1016/j.scs.2020.102669.},
Number-of-Cited-References = {33},
Times-Cited = {4},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BU2ZR},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000889380800115},
DA = {2023-08-12},
}

@article{ WOS:000845195700001,
Author = {Perrin, Jackson E. and Jernigan, Shaphan R. and Thayer, Jacob D. and
   Howell, Andrew W. and Leary, James K. and Buckner, Gregory D.},
Title = {Sensor Fusion with Deep Learning for Autonomous Classification and
   Management of Aquatic Invasive Plant Species},
Journal = {ROBOTICS},
Year = {2022},
Volume = {11},
Number = {4},
Month = {AUG},
Abstract = {Recent advances in deep learning, including the development of AlexNet,
   Residual Network (ResNet), and transfer learning, offer unprecedented
   classification accuracy in the field of machine vision. A developing
   application of deep learning is the automated identification and
   management of aquatic invasive plants. Classification of submersed
   aquatic vegetation (SAV) presents a unique challenge, namely, the lack
   of a single source of sensor data that can produce robust, interpretable
   images across a variable range of depth, turbidity, and lighting
   conditions. This paper focuses on the development of a multi-sensor (RGB
   and hydroacoustic) classification system for SAV that is robust to
   environmental conditions and combines the strengths of each sensing
   modality. The detection of invasive Hydrilla verticillata (hydrilla) is
   the primary goal. Over 5000 aerial RGB and hydroacoustic images were
   generated from two Florida lakes via an unmanned aerial vehicle and
   boat-mounted sonar unit, and tagged for neural network training and
   evaluation. Classes included ``HYDR{''}, containing hydrilla;
   ``NONE{''}, lacking SAV, and ``OTHER{''}, containing SAV other than
   hydrilla. Using a transfer learning approach, deep neural networks with
   the ResNet architecture were individually trained on the RGB and
   hydroacoustic datasets. Multiple data fusion methodologies were
   evaluated to ensemble the outputs of these neural networks for optimal
   classification accuracy. A method incorporating logic and a Monte Carlo
   dropout approach yielded the best overall classification accuracy
   (84\%), with recall and precision of 84.5\% and 77.5\%, respectively,
   for the hydrilla class. The training and ensembling approaches were
   repeated for a DenseNet model with identical training and testing
   datasets. The overall classification accuracy was similar between the
   ResNet and DenseNet models when averaged across all approaches (1.9\%
   higher accuracy for the ResNet vs. the DenseNet).},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Buckner, GD (Corresponding Author), North Carolina State Univ, Mech \& Aerosp Engn, Raleigh, NC 27695 USA.
   Perrin, Jackson E.; Jernigan, Shaphan R.; Buckner, Gregory D., North Carolina State Univ, Mech \& Aerosp Engn, Raleigh, NC 27695 USA.
   Thayer, Jacob D.; Leary, James K., Univ Florida, Ctr Aquat \& Invas Plants, Gainesville, FL 32653 USA.
   Howell, Andrew W., North Carolina State Univ, Crop \& Soil Sci, Raleigh, NC 27695 USA.},
DOI = {10.3390/robotics11040068},
Article-Number = {68},
EISSN = {2218-6581},
Keywords = {aquatic invasive plants; deep learning; sensor fusion; autonomous
   robotics},
Research-Areas = {Robotics},
Web-of-Science-Categories  = {Robotics},
Author-Email = {jeperrin@ncsu.edu
   srjernig@ncsu.edu
   jacobthayer@ufl.edu
   awhowell@ncsu.edu
   learyj@ufl.edu
   gbuckner@ncsu.edu},
Affiliations = {North Carolina State University; State University System of Florida;
   University of Florida; North Carolina State University},
ORCID-Numbers = {Buckner, Gregory/0000-0002-6601-4814},
Funding-Acknowledgement = {Florida Fish \& Wildlife Conservation Commission, FWC {[}17008]},
Funding-Text = {This research was funded by the Florida Fish \& Wildlife Conservation
   Commission, FWC Contract Number 17008.},
Cited-References = {ASHFAHANI A, 2019, P 2019 SIAM INT C DA.
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274.
   Gal Y, 2016, PR MACH LEARN RES, V48.
   Ganaie M., 2021, ARXIV.
   HAUXWELL J, 2010, PUB WISCONSIN DEP NA.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Houlahan JE, 2004, CONSERV BIOL, V18, P1132, DOI 10.1111/j.1523-1739.2004.00391.x.
   Jha S, 2021, INFORM SCIENCES, V575, P1, DOI 10.1016/j.ins.2021.04.062.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Laakom F., 2021, P 2021 INT JOINT C N, P1.
   Li D, 2020, APPL SOFT COMPUT, V94, DOI 10.1016/j.asoc.2020.106423.
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324.
   Madsen JD., 1999, APCRPM102 TN USA ENG, P17.
   Madsen JD., 2014, BIOL CONTROL AQUATIC, V3rd edn, P1.
   Olsen A, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-38343-3.
   Patel M, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9122410.
   Pimentel D, 2005, ECOL ECON, V52, P273, DOI 10.1016/j.ecolecon.2004.10.002.
   Rostami M, 2020, AAAI CONF ARTIF INTE, V34, P5545.
   Szegedy C., 2015, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2015.7298594.
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555.},
Number-of-Cited-References = {20},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Robotics},
Doc-Delivery-Number = {4A6FW},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000845195700001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000266369700071,
Author = {Sole-Casals, Jordi and Travieso, Carlos M. and Ferrer, Miguel A. and
   Alonso, Jesus B. and Briceno, Juan Carlos},
Editor = {Encarnacao, P and Veloso, A},
Title = {AUTOMATIC RECOGNITION OF LEAVES BY SHAPE DETECTION PRE-PROCESSING WITH
   ICA},
Booktitle = {BIOSIGNALS 2009: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON
   BIO-INSPIRED SYSTEMS AND SIGNAL PROCESSING},
Year = {2009},
Pages = {462+},
Note = {2nd International Conference on Bio-Inspired Systems and Signal
   Processing, Oporto, PORTUGAL, JAN 14-17, 2009},
Organization = {Inst Syst \& Technologies Informat, Control \& Commun; IEEE Engn Med \&
   Biol Soc; IEEE Circuits \& Syst Soc; Assoc Advancement Artificial
   Intelligence; ACM SIGART},
Abstract = {In this work we present a simulation of a recognition process with
   perimeter characterization of a simple plant leaves as a unique
   discriminating parameter. Data coding allowing for independence of
   leaves size and orientation may penalize performance recognition for
   some varieties. Border description sequences are then used to
   characterize the leaves. Independent Component Analysis (ICA) is then
   applied in order to study which is the best number of components to be
   considered for the classification task, implemented by means of an
   Artificial Neural Network (ANN). Obtained results with ICA as a
   pre-processing tool are satisfactory, and compared with some references
   our system improves the recognition success up to 80.8\% depending on
   the number of considered independent components.},
Publisher = {INSTICC-INST SYST TECHNOLOGIES INFORMATION CONTROL \& COMMUNICATION},
Address = {AVENIDA D MANUEL L, 27A 2 ESQUERDO, SETUBAL, 2910-595, PORTUGAL},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Sole-Casals, J (Corresponding Author), Univ Vic, Signal Processign Grp, Sagrada Familia 7, E-08500 Vic, Spain.
   Sole-Casals, Jordi, Univ Vic, Signal Processign Grp, Sagrada Familia 7, E-08500 Vic, Spain.
   Sole-Casals, Jordi; Travieso, Carlos M.; Ferrer, Miguel A.; Alonso, Jesus B., Univ Las Palmas Gran Canaria, Technol Ctr Innovat Commun CeTIC, Dept Signals \& Commun, E-35017 Las Palmas Gran Canaria, Spain.
   Briceno, Juan Carlos, Univ Costa Rica, Dept Comp Sci, San Jose, Costa Rica.},
ISBN = {978-989-8111-65-4},
Keywords = {Independent Component Analysis; Pattern Recognition; Leaves Recognition;
   Parameterization; Artificial Neural Networks},
Research-Areas = {Computer Science; Mathematical \& Computational Biology;
   Telecommunications},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Mathematical \& Computational
   Biology; Telecommunications},
Author-Email = {jordi.sole@uvic.cat
   ctravieso@dsc.ulpgc.es
   mferrer@dsc.ulpgc.es
   jalonso@dsc.ulpgc.es
   juancarlos.briceno@ecci.ucr.ac.cr},
Affiliations = {Universitat de Vic - Universitat Central de Catalunya (UVic-UCC);
   Universidad de Las Palmas de Gran Canaria; Universidad Costa Rica},
ResearcherID-Numbers = {Solé-Casals, Jordi/B-7754-2008
   Solé-Casals, Jordi/GRX-7991-2022
   Alonso-Hernández, Jesús B./N-5977-2014
   Ferrer, Miguel/AFU-8286-2022
   Ferrer, Miguel A A/L-3863-2013
   Travieso-González, Carlos M./N-5967-2014},
ORCID-Numbers = {Solé-Casals, Jordi/0000-0002-6534-1979
   Alonso-Hernández, Jesús B./0000-0002-7866-585X
   Ferrer, Miguel A A/0000-0002-2924-1225
   Travieso-González, Carlos M./0000-0002-4621-2768},
Funding-Acknowledgement = {Ministerio de Educacion y Ciencia of Spain {[}TEC2007-61535/TCM];
   Universitat de Vic {[}R0912]},
Funding-Text = {The first author acknowledges support from the Ministerio de Educacion y
   Ciencia of Spain under the grant TEC2007-61535/TCM, and from the
   Universitat de Vic under the grant R0912.},
Cited-References = {{[}Anonymous], 2005, NEURAL NETWORKS PATT.
   {[}Anonymous], 2002, SPRINGER SERIES STAT.
   BRICENO JC, 2002, IASTED INT C SIGN PR, P249.
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555.
   Cardoso JF, 1996, SIAM J MATRIX ANAL A, V17, P161, DOI 10.1137/S0895479893259546.
   Duda RO, 1973, PATTERN CLASSIFICATI.
   Huang Z., 1996, IEEE T IMAGE PROCESS, V5, P824.
   Hyvarinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71.
   JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X.
   Loncaric S, 1998, PATTERN RECOGN, V31, P983, DOI 10.1016/S0031-2023(97)00122-2.
   LU F, 1994, SIGNAL PROCESS, V37, P129, DOI 10.1016/0165-1684(94)90171-6.
   LU F, 1994, ELSEVIER SIGNAL PROC, P129.
   Mitchell T.M., 1997, MACH LEARN, V1.
   Rabiner L., 1993, FUNDAMENTALS SPEECH.
   Sanchez-Poblador V, 2004, LECT NOTES COMPUT SC, V3195, P1165.},
Number-of-Cited-References = {15},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BJJ24},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000266369700071},
DA = {2023-08-12},
}

@inproceedings{ WOS:000570722600019,
Author = {Shobana, K. B. and Perumal, P.},
Book-Group-Author = {IEEE},
Title = {Plants Classification Using Machine Learning Algorithm},
Booktitle = {2020 6TH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTING AND
   COMMUNICATION SYSTEMS (ICACCS)},
Series = {International Conference on Advanced Computing and Communication Systems},
Year = {2020},
Pages = {96-100},
Note = {6th International Conference on Advanced Computing and Communication
   Systems (ICACCS), Coimbatore, INDIA, MAR 06-07, 2020},
Abstract = {Water is the principle content in a plant. Along these lines, the
   development of plants are enormously reliant on the adjustments in plant
   water content. To advance plant development in water short and dry spell
   pressure conditions, numerous methods have been considered. The target
   of this paper is to structure and build up an astute framework, in light
   of Support Vector Machine (SVM) and machine vision that would advance
   plant development in a restricted water condition. At long last shading,
   morphological and textural highlights were separated from a lot of turf
   grass, wheat, rice plant pictures under dry season pressure conditions.
   At that point an information arrangement process was overseen utilizing
   an SVM and ANN. Results indicated that the general arrangement exactness
   of ANN was 92\% and higher correct nesses were acquired when the SVM was
   utilized as the classifier with a general precision of 98.00\% for plant
   condition (Fresh and Wilted).},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Shobana, KB (Corresponding Author), Sri Ramakrishna Engn Coll, Comp Sci \& Engn Dept, Coimbatore 641022, Tamil Nadu, India.
   Shobana, K. B.; Perumal, P., Sri Ramakrishna Engn Coll, Comp Sci \& Engn Dept, Coimbatore 641022, Tamil Nadu, India.},
DOI = {10.1109/icaccs48705.2020.9074416},
ISSN = {2469-5556},
ISBN = {978-1-7281-5197-7; 978-1-7281-5196-0},
Keywords = {Support Vector Machine; imaging; plant classification; plant
   segmentation; ANN},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {shobana.1852005@srec.ac.in
   perumalp@srec.ac.in},
Affiliations = {Sri Ramakrishna Engineering College},
ResearcherID-Numbers = {Pitchandi, Perumal/G-8438-2016},
ORCID-Numbers = {Pitchandi, Perumal/0000-0003-0121-8936},
Cited-References = {Akbarzadeh S, 2018, COMPUT ELECTRON AGR, V148, P250, DOI 10.1016/j.compag.2018.03.026.
   Ali H, 2017, COMPUT BIOL MED, V88, P84, DOI 10.1016/j.compbiomed.2017.07.002.
   Costa N. L., 2019, Information Processing in Agriculture, V6, P265.
   Dhingra G, 2019, MEASUREMENT, V135, P782, DOI 10.1016/j.measurement.2018.12.027.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Horaisova K, 2016, BIOSYST ENG, V142, P83, DOI 10.1016/j.biosystemseng.2015.12.007.
   Olayide O.E., 2019, AGR WATER MANAGE, V178, P30.
   Pei JH, 2018, SIGNAL PROCESS, V153, P197, DOI 10.1016/j.sigpro.2018.07.010.
   Richardson Heather J., 2018, EVALUATION SUPPORT V.
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   Sulc M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0265-4.
   Tewatia R.K., 2018, INDIAN J FERTIL, V13, P20.
   Zhang Y., 2016, P IEEE C COMP VIS PA, P589597.},
Number-of-Cited-References = {13},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BP9RJ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000570722600019},
DA = {2023-08-12},
}

@inproceedings{ WOS:000598548200051,
Author = {Kusumawardani, Wahyu and Muzzazinah and Ramli, Murni},
Editor = {Indriyanti, NY and Ramli, M and Nurhasanah, F},
Title = {Plant Taxonomy Learning and Research: A Systematics Review},
Booktitle = {2ND INTERNATIONAL CONFERENCE ON SCIENCE, MATHEMATICS, ENVIRONMENT, AND
   EDUCATION, 2019},
Series = {AIP Conference Proceedings},
Year = {2019},
Volume = {2194},
Note = {2nd International Conference on Science, Mathematics, Environment, and
   Education (ICoSMEE), Surakarta, INDONESIA, JUL 26-28, 2019},
Abstract = {Concepts of plant identification and classification were important basic
   knowledge to be mastered by biology students. The research was aimed to
   find out what concepts and methods of learning plant taxonomy, and find
   out the objects and methods in plant taxonomy research. Seventeen
   articles published from 2005-2019 were selected as the review materials.
   Nine articles were about learning the plant taxonomy, and eight articles
   were about research on plant taxonomy. The articles were obtained from
   Journal of Biological Education and Science Direct. The results showed
   the common concepts learned about plant identification and
   classification. The prominent plant groups used in the learning were:
   the Bryophytes, Pteridophytes, Gymnosperms, and Angiosperm with the
   example of the native species and focal species. The learning methods
   and approaches were varied, including: using real plant specimens,
   dichotomous key method, word association exercise based on mnemonics
   approach, or pictorial card games for identification native plants.
   Others use an electronic multi-access key, iOS app on the iPod for plant
   identification guide, interactive multimedia dichotomous key for plant
   identification, labeled drawing and descriptive writing of native plant
   identification. Various aspects used as the object of the research on
   plants taxonomy, one of them was the leaves. Various methods used in the
   research on plant taxonomy, such as: FRT, LDC Linear, kNN, SIFT, Color
   moments, SFTA, ANNs, Deep learning techniques, hierarchical approach -
   NFC, and AIT.},
Publisher = {AMER INST PHYSICS},
Address = {2 HUNTINGTON QUADRANGLE, STE 1NO1, MELVILLE, NY 11747-4501 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kusumawardani, W (Corresponding Author), Univ Sebelas Maret, Master Program Biol Educ, Surakarta, Indonesia.
   Kusumawardani, Wahyu; Muzzazinah; Ramli, Murni, Univ Sebelas Maret, Master Program Biol Educ, Surakarta, Indonesia.},
DOI = {10.1063/1.5139783},
Article-Number = {020051},
ISSN = {0094-243X},
ISBN = {978-0-7354-1945-2},
Keywords = {plant taxonomy; learning; research},
Keywords-Plus = {IDENTIFICATION; FEATURES; LEAVES; SHAPE; KEY},
Research-Areas = {Education \& Educational Research; Environmental Sciences \& Ecology;
   Mathematics},
Web-of-Science-Categories  = {Education, Scientific Disciplines; Environmental Sciences; Mathematics},
Author-Email = {wahyukusumawardani@student.uns.ac.id
   yayin\_pbio@fkip.uns.ac.id
   mramlim@staff.uns.ac.id},
Affiliations = {Sebelas Maret University},
ResearcherID-Numbers = {Ramli, Murni/AEA-3030-2022
   Ramli, Murni/AAF-9508-2022},
ORCID-Numbers = {Ramli, Murni/0000-0003-4242-9190
   },
Funding-Acknowledgement = { {[}516/UN27.21/PP/2019]},
Funding-Text = {This paper was the part of the research financed by the Grant for
   Postgraduate Research Universitas Sebelas Maret Fiscal Years 2018-2019
   with number contract 516/UN27.21/PP/2019.},
Cited-References = {Andic B, 2019, J BIOL EDUC, V53, P310, DOI 10.1080/00219266.2018.1469540.
   Bebbington A., 2005, J BIOL ED, P37.
   Chaki J, 2020, J KING SAUD UNIV-COM, V32, P1158, DOI 10.1016/j.jksuci.2018.01.007.
   Jacquemart AL, 2016, J BIOL EDUC, V50, P442, DOI 10.1080/00219266.2016.1150870.
   Jamil N, 2015, PROCEDIA COMPUT SCI, V76, P436, DOI 10.1016/j.procs.2015.12.287.
   Jobin A, 2012, PROC TECH, V1, P171, DOI 10.1016/j.protcy.2012.10.021.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Maskour L., 2016, STUDY SOME LEARNING, V5, P2.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Padial J. M., 2010, INTEGRATIVE FUTURE T, P1.
   ROSENHECK MB, 1989, J EDUC PSYCHOL, V81, P196, DOI 10.1037/0022-0663.81.2.196.
   Sekeroglu B, 2016, PROCEDIA COMPUT SCI, V102, P578, DOI 10.1016/j.procs.2016.09.445.
   Stagg BC, 2019, J BIOL EDUC, V53, P63, DOI 10.1080/00219266.2017.1420683.
   Stagg BC, 2017, J BIOL EDUC, V51, P123, DOI 10.1080/00219266.2016.1177572.
   Stagg BC, 2016, J BIOL EDUC, V50, P24, DOI 10.1080/00219266.2014.1000360.
   Stagg BC, 2015, J BIOL EDUC, V49, P274, DOI 10.1080/00219266.2014.934900.
   Stagg BC, 2013, J BIOL EDUC, V47, P104, DOI 10.1080/00219266.2013.764341.
   Strgar J., 2012, J BIOL ED, P37.
   TILLING SM, 1987, BIOL J LINN SOC, V32, P87, DOI 10.1111/j.1095-8312.1987.tb00414.x.
   Victor J. E., 2015, PLANT TAXONOMIC RES.
   Yigit E, 2019, COMPUT ELECTRON AGR, V156, P369, DOI 10.1016/j.compag.2018.11.036.},
Number-of-Cited-References = {22},
Times-Cited = {3},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {15},
Doc-Delivery-Number = {BQ5CO},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000598548200051},
OA = {Bronze},
DA = {2023-08-12},
}

@article{ WOS:000593620800001,
Author = {Yang, Kunlong and Zhong, Weizhen and Li, Fengguo},
Title = {Leaf Segmentation and Classification with a Complicated Background Using
   Deep Learning},
Journal = {AGRONOMY-BASEL},
Year = {2020},
Volume = {10},
Number = {11},
Month = {NOV},
Abstract = {The segmentation and classification of leaves in plant images are a
   great challenge, especially when several leaves are overlapping in
   images with a complicated background. In this paper, the segmentation
   and classification of leaf images with a complicated background using
   deep learning are studied. First, more than 2500 leaf images with a
   complicated background are collected and artificially labeled with
   target pixels and background pixels. Two-thousand of them are fed into a
   Mask Region-based Convolutional Neural Network (Mask R-CNN) to train a
   model for leaf segmentation. Then, a training set that contains more
   than 1500 training images of 15 species is fed into a very deep
   convolutional network with 16 layers (VGG16) to train a model for leaf
   classification. The best hyperparameters for these methods are found by
   comparing a variety of parameter combinations. The results show that the
   average Misclassification Error (ME) of 80 test images using Mask R-CNN
   is 1.15\%. The average accuracy value for the leaf classification of 150
   test images using VGG16 is up to 91.5\%. This indicates that these
   methods can be used to segment and classify the leaf image with a
   complicated background effectively. It could provide a reference for the
   phenotype analysis and automatic classification of plants.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Li, FG (Corresponding Author), South China Normal Univ, Guangdong Prov Key Lab Quantum Engn \& Quantum Mat, Guangdong Prov Engn Technol Res Ctr Quantum Preci, Natl Demonstrat Ctr Expt Phys Educ,SPTE, Guangzhou 510006, Peoples R China.
   Yang, Kunlong; Zhong, Weizhen; Li, Fengguo, South China Normal Univ, Guangdong Prov Key Lab Quantum Engn \& Quantum Mat, Guangdong Prov Engn Technol Res Ctr Quantum Preci, Natl Demonstrat Ctr Expt Phys Educ,SPTE, Guangzhou 510006, Peoples R China.},
DOI = {10.3390/agronomy10111721},
Article-Number = {1721},
EISSN = {2073-4395},
Keywords = {deep learning; image segmentation; Mask R-CNN; plant classification;
   VGG16},
Keywords-Plus = {APPLE DETECTION; PLANT},
Research-Areas = {Agriculture; Plant Sciences},
Web-of-Science-Categories  = {Agronomy; Plant Sciences},
Author-Email = {2019021917@m.scnu.edu.cn
   2018021895@m.scnu.edu.cn
   lifengguo@m.scnu.edu.cn},
Affiliations = {South China Normal University},
ORCID-Numbers = {Yang, Kunlong/0000-0001-5397-9433
   Li, Fengguo/0000-0003-4784-0691},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}11575064]; Natural
   Science Foundation of Guangdong Province, China {[}2016A030313433]},
Funding-Text = {This research was funded by the National Natural Science Foundation of
   China (No. 11575064) and the Natural Science Foundation of Guangdong
   Province, China (Grant No. 2016A030313433).},
Cited-References = {Aich S, 2017, IEEE INT CONF COMP V, P2080, DOI 10.1109/ICCVW.2017.244.
   Al-Shakarji N., 2017, P 2017 IEEE APPL IM.
   Alenya G, 2013, IEEE ROBOT AUTOM MAG, V20, P50, DOI 10.1109/MRA.2012.2230118.
   {[}Anonymous], 2015, P COMP VIS PROBL PLA.
   Arvidsson S, 2011, NEW PHYTOL, V191, P895, DOI 10.1111/j.1469-8137.2011.03756.x.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Bell J., 2019, ARXIV190403124.
   Camargo A, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0096889.
   Christian S., 2016, ARXIV160207261.
   Dobrescu A, 2017, IEEE INT CONF COMP V, P2072, DOI 10.1109/ICCVW.2017.243.
   Dong Y.X., 2014, ADV MATER RES-KR, V989, P3751, DOI {[}10.4028/www.scientific.net/AMR.989-994.3751, DOI 10.4028/WWW.SCIENTIFIC.NET/AMR.989-994.3751].
   EHSANIRAD A., 2010, INT J COMPUTER SCI I, V8, P78.
   Fu H, 2006, IEE P-VIS IMAGE SIGN, V153, P881, DOI 10.1049/ip-vis:20060061.
   Gene-Mola J, 2019, COMPUT ELECTRON AGR, V162, P689, DOI 10.1016/j.compag.2019.05.016.
   Giuffrida M.V., 2015, P COMP VIS PROBL PLA.
   Giuffrida MV, 2018, PLANT J, V96, P880, DOI 10.1111/tpj.14064.
   Giuffrida MV, 2017, IEEE INT CONF COMP V, P2064, DOI 10.1109/ICCVW.2017.242.
   He K., 2018, P IEEE INT C COMPUTE.
   Itakura K, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103576.
   Kumar J. P., 2019, Information Processing in Agriculture, V6, P233.
   Kuznichov D., 2019, IEEE COMPUT SOC CONF, DOI DOI 10.1109/CVPRW.2019.00314.
   Li L, 2014, SENSORS-BASEL, V14, P20078, DOI 10.3390/s141120078.
   Ozguven MM, 2019, PHYSICA A, V535, DOI 10.1016/j.physa.2019.122537.
   Pape JM, 2015, LECT NOTES COMPUT SC, V8928, P61, DOI 10.1007/978-3-319-16220-1\_5.
   Ren MY, 2017, PROC CVPR IEEE, P293, DOI 10.1109/CVPR.2017.39.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Romera-Paredes B, 2016, LECT NOTES COMPUT SC, V9910, P312, DOI 10.1007/978-3-319-46466-4\_19.
   Salvador A., 2017, ARXIV171200617.
   Scharr H, 2016, MACH VISION APPL, V27, P585, DOI 10.1007/s00138-015-0737-3.
   Simonyan K, 2015, 3 INT C LEARNING REP, DOI DOI 10.2146/AJHP170251.
   Siyang H., 2014, P 2014 7 INT C IM SI.
   Tian YN, 2019, COMPUT ELECTRON AGR, V157, P417, DOI 10.1016/j.compag.2019.01.012.
   Turkoglu M, 2019, PHYSICA A, V527, DOI 10.1016/j.physa.2019.121297.
   Viaud G, 2017, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.02057.
   Walter A, 2015, PLANT METHODS, V11, DOI 10.1186/s13007-015-0056-8.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Zhou YunCheng, 2017, Transactions of the Chinese Society of Agricultural Engineering, V33, P219.},
Number-of-Cited-References = {38},
Times-Cited = {36},
Usage-Count-Last-180-days = {10},
Usage-Count-Since-2013 = {50},
Journal-ISO = {Agronomy-Basel},
Doc-Delivery-Number = {OX5RE},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000593620800001},
OA = {gold, Green Submitted},
DA = {2023-08-12},
}

@article{ WOS:000702752600002,
Author = {Oishi, Yu and Habaragamuwa, Harshana and Zhang, Yu and Sugiura, Ryo and
   Asano, Kenji and Akai, Kotaro and Shibata, Hiroyuki and Fujimoto, Taketo},
Title = {Automated abnormal potato plant detection system using deep learning
   models and portable video cameras},
Journal = {INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION},
Year = {2021},
Volume = {104},
Month = {DEC 15},
Abstract = {Potatoes are the world's most important root and tuber crop. A diseased
   seed potato can produce approximately 10 potato tubers, and the disease
   can propagate through the seed potato production cycle. To promote
   stable potato production, quality seed potatoes that are healthy and
   disease-free should be supplied. The Japanese government established a
   propagation system for the production and distribution of seed potatoes.
   Experienced laborers are required in the fields for visual inspection
   and removal of abnormal plants during seed potato production. To aid
   visual detection, reduce labor effort, and improve assessment time, we
   developed an automated abnormal potato plant detection system that
   utilizes a portable video camera and deep learning models. The proposed
   system detects abnormal plants or leaves considering the stage of
   growth. It detects three cases: (i) abnormal potato plants in the early
   growth stage, (ii) abnormal potato plants in comparison to the
   surrounding plants, and (iii) abnormal potato leaves. For the abnormal
   and healthy potato plant classification, the accuracy was similar to
   90\%, and the average precision (AP) for detection was 78.2\%.
   Furthermore, the classification accuracy of the abnormal and healthy
   potato leaf classification was 96.7\%, and the AP for detection was
   90.5\%. Therefore, the proposed system can be used to detect abnormal
   potato plants.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Oishi, Y (Corresponding Author), Natl Agr \& Food Res Org, Core Technol Res Headquarters, 1-31-1 Kannondai, Tsukuba, Ibaraki 3050856, Japan.
   Oishi, Yu; Habaragamuwa, Harshana; Zhang, Yu; Sugiura, Ryo, Natl Agr \& Food Res Org, Core Technol Res Headquarters, 1-31-1 Kannondai, Tsukuba, Ibaraki 3050856, Japan.
   Asano, Kenji; Akai, Kotaro, Natl Agr \& Food Res Org, Hokkaido Agr Res Ctr, 9-4 Shinseiminami, Kasai, Hokkaido 0820081, Japan.
   Shibata, Hiroyuki, Tokachi Federat Agr Cooperat, Nokyoren Bldg Nishi 3,Minami 7,14, Obihiro, Hokkaido 0800013, Japan.
   Fujimoto, Taketo, Natl Agr \& Food Res Org, Inst Plant Protect, 2-1-18 Kannondai, Tsukuba, Ibaraki 3058666, Japan.},
DOI = {10.1016/j.jag.2021.102509},
Article-Number = {102509},
ISSN = {1569-8432},
EISSN = {1872-826X},
Keywords = {Disease diagnosis; Portable camera; Deep learning; Image classification;
   Object detection},
Keywords-Plus = {SEGMENTATION; IMAGES},
Research-Areas = {Remote Sensing},
Web-of-Science-Categories  = {Remote Sensing},
Author-Email = {oishi.yu@affrc.go.jp
   habaragamuwah433@affrc.go.jp
   choy622@affrc.go.jp
   rsugiura@affre.go.jp
   asanok@affrc.go.jp
   akaik501@affrc.go.jp
   h\_shibata@nkrtwosv.nokyoren.or.jp
   taketof@affrc.go.jp},
Affiliations = {National Agriculture \& Food Research Organization - Japan; National
   Agriculture \& Food Research Organization - Japan; National Agriculture
   \& Food Research Organization - Japan},
ResearcherID-Numbers = {Oishi, Yu/A-5561-2017},
ORCID-Numbers = {Oishi, Yu/0000-0002-0818-9368},
Funding-Acknowledgement = {Project of the Bio-oriented Technology Research Advancement Institution
   (Research Program on Development of Innovative Technology) {[}01022C]},
Funding-Text = {Part of this research was supported by grants from the Project of the
   Bio-oriented Technology Research Advancement Institution (Research
   Program on Development of Innovative Technology, project ID: 01022C).},
Cited-References = {Afonso M, 2019, IFAC PAPERSONLINE, V52, P6, DOI 10.1016/j.ifacol.2019.12.481.
   Aquino A, 2020, COMPUT ELECTRON AGR, V176, DOI 10.1016/j.compag.2020.105616.
   Bai XD, 2013, COMPUT ELECTRON AGR, V99, P21, DOI 10.1016/j.compag.2013.08.022.
   Fuentes A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17092022.
   Geirhos R, 2019, P 7 INT C LEARN REPR.
   Habaragamuwa H., 2021, ARXIV210203082.
   Hernandez-Hernandez JL, 2016, COMPUT ELECTRON AGR, V122, P124, DOI 10.1016/j.compag.2016.01.020.
   Hughes P.D, 2016, ARXIV151108060V2.
   Kawakami T, 2015, BREEDING SCI, V65, P17, DOI 10.1270/jsbbs.65.17.
   Kingma D. P., 2014, INT C LEARNING REPRE.
   Maoka T, 2010, PLANT DIS, V94, P1248, DOI 10.1094/PDIS-12-09-0787.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Polder G, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.00209.
   Redmon J, ARXIV.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Song WJ, 2015, REMOTE SENS-BASEL, V7, P10425, DOI 10.3390/rs70810425.
   Sugizaki Ryuichi, 2018, 2018 IEEE Photonics Conference (IPC), DOI 10.1109/IPCon.2018.8527251.
   Suwa K, 2019, IEEE INT CONF BIG DA, P5195, DOI 10.1109/BigData47090.2019.9006556.
   Zortea M., 2018, P 31 C GRAPHICS PATT.},
Number-of-Cited-References = {19},
Times-Cited = {6},
Usage-Count-Last-180-days = {9},
Usage-Count-Since-2013 = {44},
Journal-ISO = {Int. J. Appl. Earth Obs. Geoinf.},
Doc-Delivery-Number = {WA2WW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000702752600002},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000857749600001,
Author = {Cheng, Wai Khuen and Leong, Wai Chun and Tan, Joi San and Hong, Zeng-Wei
   and Chen, Yen-Lin},
Title = {Affective Recommender System for Pet Social Network},
Journal = {SENSORS},
Year = {2022},
Volume = {22},
Number = {18},
Month = {SEP},
Abstract = {In this new era, it is no longer impossible to create a smart home
   environment around the household. Moreover, users are not limited to
   humans but also include pets such as dogs. Dogs need long-term close
   companionship with their owners; however, owners may occasionally need
   to be away from home for extended periods of time and can only monitor
   their dogs' behaviors through home security cameras. Some dogs are
   sensitive and may develop separation anxiety, which can lead to
   disruptive behavior. Therefore, a novel smart home solution with an
   affective recommendation module is proposed by developing: (1) an
   application to predict the behavior of dogs and, (2) a communication
   platform using smartphones to connect with dog friends from different
   households. To predict the dogs' behaviors, the dog emotion recognition
   and dog barking recognition methods are performed. The ResNet model and
   the sequential model are implemented to recognize dog emotions and dog
   barks. The weighted average is proposed to combine the prediction value
   of dog emotion and dog bark to improve the prediction output.
   Subsequently, the prediction output is forwarded to a recommendation
   module to respond to the dogs' conditions. On the other hand, the
   Real-Time Messaging Protocol (RTMP) server is implemented as a platform
   to contact a dog's friends on a list to interact with each other.
   Various tests were carried out and the proposed weighted average led to
   an improvement in the prediction accuracy. Additionally, the proposed
   communication platform using basic smartphones has successfully
   established the connection between dog friends.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Chen, YL (Corresponding Author), Natl Taipei Univ Technol, Dept Comp Sci \& Informat Engn, Taipei 106344, Taiwan.
   Cheng, Wai Khuen; Leong, Wai Chun; Tan, Joi San, Univ Tunku Abdul Rahman, Fac Informat \& Commun Technol, Kampar 31900, Perak, Malaysia.
   Hong, Zeng-Wei, Feng Chia Univ, Dept Informat Engn \& Comp Sci, Taichung 40724, Taiwan.
   Chen, Yen-Lin, Natl Taipei Univ Technol, Dept Comp Sci \& Informat Engn, Taipei 106344, Taiwan.},
DOI = {10.3390/s22186759},
Article-Number = {6759},
EISSN = {1424-8220},
Keywords = {affective recommendation; pet social network; emotion recognition model;
   dog barking recognition; deep learning},
Keywords-Plus = {SMART HOME; RECOGNITION; BEHAVIOR},
Research-Areas = {Chemistry; Engineering; Instruments \& Instrumentation},
Web-of-Science-Categories  = {Chemistry, Analytical; Engineering, Electrical \& Electronic;
   Instruments \& Instrumentation},
Author-Email = {ylchen@mail.ntut.edu.tw},
Affiliations = {Universiti Tunku Abdul Rahman (UTAR); Feng Chia University; National
   Taipei University of Technology},
ResearcherID-Numbers = {Cheng, Wai Khuen/A-8629-2011
   },
ORCID-Numbers = {Cheng, Wai Khuen/0000-0003-1707-0462
   Hong, Zeng-Wei/0000-0002-5090-2788
   leong, wai yie/0000-0002-5389-1121
   Tan, Joi San/0000-0002-8830-269X
   Chen, Yen-Lin/0000-0001-7717-9393},
Funding-Acknowledgement = {Ministry of Science and Technology in Taiwan
   {[}MOST-109-2628-E-027-004-MY3, MOST-111-2218-E-027-003,
   MOST-110-2622-8-027-006]},
Funding-Text = {This work was funded by the Ministry of Science and Technology in
   Taiwan, under grant numbers MOST-109-2628-E-027-004-MY3,
   MOST-111-2218-E-027-003, and MOST-110-2622-8-027-006.},
Cited-References = {Alsalemi A, 2019, IEEE SYST J, V13, P3376, DOI 10.1109/JSYST.2019.2899832.
   Altulyan M, 2022, COMPUT J, V65, P2098, DOI 10.1093/comjnl/bxab049.
   {[}Anonymous], MANAGE ANTISOCIAL BE.
   {[}Anonymous], PET OWNERSHIP ASIA.
   {[}Anonymous], 6 WAYS EASE POSTPAND.
   Belghini N., 2016, INT J APPL INF SYST, V10, P1, DOI {[}10.5120/ijais2016451528, DOI 10.5120/IJAIS2016451528].
   Blumrosen G, 2017, IEEE INT CONF COMP V, P2810, DOI 10.1109/ICCVW.2017.332.
   Bocaj E, 2020, PROCEEDINGS OF 2020 INTERNATIONAL CONFERENCE ON SMART SYSTEMS AND TECHNOLOGIES (SST 2020), P83, DOI 10.1109/SST49455.2020.9263702.
   Casella E, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON SMART COMPUTING (SMARTCOMP 2019), P409, DOI 10.1109/SMARTCOMP.2019.00080.
   Catia C, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-15091-4.
   Corujo LA, 2021, FUTURE INTERNET, V13, DOI 10.3390/fi13100250.
   Felfernig A, 2019, J INTELL INF SYST, V52, P285, DOI 10.1007/s10844-018-0530-7.
   Ferres K, 2022, FUTURE INTERNET, V14, DOI 10.3390/fi14040097.
   Gardner B, 2015, HEALTH PSYCHOL REV, V9, P277, DOI 10.1080/17437199.2013.876238.
   Gladence LM, 2020, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-01968-2.
   Hantke S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5134, DOI 10.1109/ICASSP.2018.8461757.
   Hassan MM, 2018, FUTURE GENER COMP SY, V81, P307, DOI 10.1016/j.future.2017.11.029.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hossain MS, 2017, J PARALLEL DISTR COM, V103, P11, DOI 10.1016/j.jpdc.2016.10.005.
   Iwashita Y, 2014, INT C PATT RECOG, P4310, DOI 10.1109/ICPR.2014.739.
   Kamminga J.W., 2020, P 6 WORKSHOP MINING.
   Kamminga JW, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2017 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (UBICOMP/ISWC `17 ADJUNCT), P597, DOI 10.1145/31?3024.3174407.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Ladha C, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P415, DOI 10.1145/2493432.2493519.
   Li DTCA, 2021, NEUROCOMPUTING, V455, P283, DOI 10.1016/j.neucom.2021.03.122.
   Liu H, 2022, IEEE T IND INFORM, V18, P7107, DOI 10.1109/TII.2022.3143605.
   Liu H, 2022, IEEE T IND INFORM, V18, P4361, DOI 10.1109/TII.2021.3128240.
   Liu H, 2022, NEUROCOMPUTING, V468, P469, DOI 10.1016/j.neucom.2021.10.050.
   Liu HB, 2021, INTERVIROLOGY, V64, P126, DOI {[}10.1159/000513687, 10.1109/TPWRD.2021.3054889, 10.1109/TMM.2021.3081873].
   Liu TT, 2022, INFRARED PHYS TECHN, V122, DOI 10.1016/j.infrared.2022.104099.
   Lye GX, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20072098.
   MCCRAVE EA, 1991, VET CLIN N AM-SMALL, V21, P247, DOI 10.1016/S0195-5616(91)50030-9.
   Mishra P. K., 2020, 2020 11 INT C COMP C, P1, DOI DOI 10.1109/STPEC49749.2020.9297669.
   Mota-Rojas D, 2021, ANIMALS-BASEL, V11, DOI 10.3390/ani11113334.
   Neethirajan S, 2021, AI-BASEL, V2, P342, DOI 10.3390/ai2030021.
   Ojagh S, 2020, FUTURE GENER COMP SY, V108, P97, DOI 10.1016/j.future.2020.02.041.
   Pons P, 2015, 12TH ADVANCES IN COMPUTER ENTERTAINMENT TECHNOLOGY CONFERENCE (ACE15), DOI 10.1145/2832932.2837007.
   Quaranta A, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10071107.
   Rasch K, 2014, J AMB INTEL SMART EN, V6, P21, DOI 10.3233/AIS-130242.
   Rashidi P, 2011, IEEE T KNOWL DATA EN, V23, P527, DOI 10.1109/TKDE.2010.148.
   Rodriguez Fernandez M, 2016, ENERG EFFIC, V9, P249, DOI 10.1007/s12053-015-9361-3.
   Shannon L., 2020, DOG GONE HANDLE YOUR.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Singh B.K., 2021, DATA DRIVEN APPROACH, P449, DOI {[}10.1007/978-981, DOI 10.1007/978-981].
   Siniscalchi M, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003349.
   Thakur N, 2018, 2018 IEEE 9TH ANNUAL INFORMATION TECHNOLOGY, ELECTRONICS AND MOBILE COMMUNICATION CONFERENCE (IEMCON), P801, DOI 10.1109/IEMCON.2018.8615079.
   Totakura V., 2020, INT J SCI TECHNOL RE, V9, P6007.
   Voorend R.W.A, 2021, DEEP UNSUPERVISED RE.
   Wai Khuen Cheng, 2019, 2019 International Conference on Green and Human Information Technology (ICGHIT), P24, DOI 10.1109/ICGHIT.2019.00013.
   Wang H, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22041556.
   Wang RY, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20071918.
   Yang H, 2018, J SENSORS, V2018, DOI 10.1155/2018/6464036.
   Zhang SA, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3285029.},
Number-of-Cited-References = {53},
Times-Cited = {1},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {7},
Journal-ISO = {Sensors},
Doc-Delivery-Number = {4S9KF},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000857749600001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000818068500076,
Author = {Zhang, Kaidi and Wang, Binjun and Tong, Xin and Liu, Keke},
Title = {Fire detection using vision transformer on power plant},
Journal = {ENERGY REPORTS},
Year = {2022},
Volume = {8},
Number = {10},
Pages = {657-664},
Month = {NOV},
Note = {4th International Conference on Clean Energy and Electrical Systems
   (CEES), Tokyo, JAPAN, APR 02-04, 2022},
Abstract = {The importance of power plant safety is increasing in the era of gradual
   technological development. When a fire occurs in the power plant, it
   will cause huge material losses, social unrest, and even casualties. The
   paper studies the common methods and models of fire warning, and
   introduces several model recognition techniques based on flames or
   smoke. Improved an automated power plant identification system based on
   the vision transformer, and proved the advantages of the technology
   through comparative analysis. (C) 2022 Published by Elsevier Ltd.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article; Proceedings Paper},
Language = {English},
Affiliation = {Wang, BJ (Corresponding Author), Peoples Publ Secur Univ China, Sch Informat \& Cyber Secur, Beijing 100038, Peoples R China.
   Zhang, Kaidi; Wang, Binjun; Tong, Xin, Peoples Publ Secur Univ China, Sch Informat \& Cyber Secur, Beijing 100038, Peoples R China.
   Liu, Keke, Yunnan Univ Finance \& Econ, Kunming 650000, Yunnan, Peoples R China.},
DOI = {10.1016/j.egyr.2022.05.224},
EarlyAccessDate = {JUN 2022},
ISSN = {2352-4847},
Keywords = {Deep learning; Fire detection; Power plant; Vision transformer},
Keywords-Plus = {CONVOLUTIONAL NEURAL-NETWORKS; SURVEILLANCE},
Research-Areas = {Energy \& Fuels},
Web-of-Science-Categories  = {Energy \& Fuels},
Author-Email = {wangbinjun@ppsuc.edu.cn},
Affiliations = {People's Public Security University of China; Yunnan University of
   Finance \& Economics},
ResearcherID-Numbers = {SHI, YAN/HNI-1042-2023
   Liu, keke/HKF-4509-2023},
Funding-Acknowledgement = {Key Project of National Social Science Fund -Key Technical Risk Analysis
   and Countermeasure Research in the Perspective of The New Industry of
   Cybersecurity {[}20AZD114]},
Funding-Text = {The study was funded by Key Project of National Social Science Fund -Key
   Technical Risk Analysis and Countermeasure Research in the Perspective
   of The New Industry of Cybersecurity (No. 20AZD114).},
Cited-References = {Dai Y, TRANSMED TRANSFORMER, P2021.
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.1412.11929.
   Fujiwara N, 2004, IEEE INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES 2004 (ISCIT 2004), PROCEEDINGS, VOLS 1 AND 2, P659.
   Gaur A, 2020, FIRE TECHNOL, V56, P1943, DOI 10.1007/s10694-020-00986-y.
   Jaggi M., 2020, 8 INT C LEARN REPR I.
   Ke Guolin, 2020, RETHINKING POSITIONA.
   Khan TM, 2020, IEEE ACCESS, V8, P131257, DOI 10.1109/ACCESS.2020.3008899.
   Liu CB, 2004, INT C PATT RECOG, P134, DOI 10.1109/ICPR.2004.1333722.
   Masud M, 2020, COMPUT COMMUN, V152, P215, DOI 10.1016/j.comcom.2020.01.050.
   Muhammad K, 2018, IEEE ACCESS, V6, P18174, DOI 10.1109/ACCESS.2018.2812835.
   Muhammad K, 2018, NEUROCOMPUTING, V288, P30, DOI 10.1016/j.neucom.2017.04.083.
   Santhanavijayan A., 2021, Intelligent System Design. Proceedings of Intelligent System Design: INDIA 2019. Advances in Intelligent Systems and Computing (AISC 1171), P247, DOI 10.1007/978-981-15-5400-1\_25.
   Sorin V, 2020, J AM COLL RADIOL, V17, P639, DOI 10.1016/j.jacr.2019.12.026.
   Vaswani A., 2017, P ANN C NEUR INF PRO, P5998, DOI DOI 10.48550/ARXIV.1706.03762.
   Wang SX, 2019, INT J ELEC POWER, V109, P470, DOI 10.1016/j.ijepes.2019.02.022.
   Xu Hongyu, J SHENYANG AEROSP U, V38, P54.
   Zhang X.L., 2018, INFRARED LASER ENG, V47, P49.
   Zhang XB, 2020, CHIN AUTOM CONGR, P3080, DOI 10.1109/CAC51589.2020.9327309.},
Number-of-Cited-References = {18},
Times-Cited = {2},
Usage-Count-Last-180-days = {11},
Usage-Count-Since-2013 = {27},
Journal-ISO = {Energy Rep.},
Doc-Delivery-Number = {2N0GM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000818068500076},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000477058800014,
Author = {Rajagopal, Heshalini and Khairuddin, Anis Salwa Mohd and Mokhtar,
   Norrima and Ahmad, Azlin and Yusof, Rubiyah},
Title = {Application of image quality assessment module to motion-blurred wood
   images for wood species identification system},
Journal = {WOOD SCIENCE AND TECHNOLOGY},
Year = {2019},
Volume = {53},
Number = {4},
Pages = {967-981},
Month = {JUL},
Abstract = {Despite tighter conservation regulations, demand for timber products has
   continued to increase due to growing population. Normally, experts
   identify the wood species based on the pattern of the wood surface
   texture. However, manual inspection on wood texture is tedious,
   time-consuming, impractical and cost-ineffective for a human to analyze
   a large number of timber species. Therefore, a reliable automatic wood
   recognition system is needed in order to classify the wood species
   efficiently. The proposed system includes image acquisition, image
   quality assessment module (IQA), image deblurring, feature extraction
   and classification. In this research, the wood images are motion-blurred
   due to imperfections in the imaging and capturing process. Hence, an IQA
   module is proposed to monitor the quality of images before proceeding to
   the next stage which is the feature extraction process. The IQA module
   will determine whether the image has to undergo the image deblurring
   process based on the image quality value. If the image is of low quality
   based on the image quality value obtained, then the image will be
   deblurred before the feature extraction procedure. A reliable motion
   deblurring technique, which is based on Lucy-Richardson algorithm, is
   employed to enhance the motion-blurred images before proceeding to the
   next stage, which is the feature extraction process. Then, a statistical
   feature extraction technique is proposed to extract 24 features from
   each wood image. Finally, a support vector machine is used to classify
   the 20 tropical wood species.},
Publisher = {SPRINGER},
Address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
Type = {Article},
Language = {English},
Affiliation = {Khairuddin, ASM (Corresponding Author), Univ Malaya, Fac Engn, Dept Elect Engn, Kuala Lumpur, Malaysia.
   Rajagopal, Heshalini; Khairuddin, Anis Salwa Mohd; Mokhtar, Norrima, Univ Malaya, Fac Engn, Dept Elect Engn, Kuala Lumpur, Malaysia.
   Ahmad, Azlin, Univ Teknol Malaysia, Malaysia Japan Int Inst Technol, Kuala Lumpur, Malaysia.
   Yusof, Rubiyah, Univ Teknol MARA, Fac Comp \& Math Sci, Shah Alam, Malaysia.},
DOI = {10.1007/s00226-019-01110-2},
ISSN = {0043-7719},
EISSN = {1432-5225},
Keywords-Plus = {LUCY-RICHARDSON; RECOGNITION; SIMILARITY; STRAIN; BARK},
Research-Areas = {Forestry; Materials Science},
Web-of-Science-Categories  = {Forestry; Materials Science, Paper \& Wood},
Author-Email = {anissalwa@um.edu.my},
Affiliations = {Universiti Malaya; Universiti Teknologi Malaysia; Universiti Teknologi
   MARA},
ResearcherID-Numbers = {KHAIRUDDIN, ANIS SALWA MOHD/B-5340-2010
   Rajagopal, Heshalini/P-5297-2015
   mokhtar, norrima/B-9395-2010
   yusof, rubiyah/AAV-9212-2020
   Ahmad, Azlin/ABA-7891-2021
   },
ORCID-Numbers = {KHAIRUDDIN, ANIS SALWA MOHD/0000-0002-9873-4779
   Rajagopal, Heshalini/0000-0002-6986-1154},
Funding-Acknowledgement = {RU Grant-Faculty Programme by Faculty of Engineering, University of
   Malaya {[}RF001A-2018]},
Funding-Text = {Research funding was provided by RU Grant-Faculty Programme by Faculty
   of Engineering, University of Malaya with Project Number RF001A-2018.},
Cited-References = {Abang MZ, 2017, J FUNDAM APPL SCI, V9, P232, DOI 10.4314/jfas.v9i5s.17.
   Angelis G, 2018, BIOMED PHYS ENG EXPR, V4, DOI 10.1088/2057-1976/aab922.
   Aouinti F, 2018, INT ARAB J INF TECHN, V15, P715.
   Aziz F, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0202092.
   Barrack F, 2018, PHYS MED BIOL, V63, DOI 10.1088/1361-6560/aab13a.
   Bremananth R, 2009, 2009 INTERNATIONAL CONFERENCE ON ADVANCES IN RECENT TECHNOLOGIES IN COMMUNICATION AND COMPUTING (ARTCOM 2009), P615, DOI 10.1109/ARTCom.2009.10.
   Buccini A, 2018, J SCI COMPUT, V74, P786, DOI 10.1007/s10915-017-0461-4.
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199.
   Chow LS, 2016, MAGN RESON IMAGING, V34, P820, DOI 10.1016/j.mri.2016.03.006.
   Daassi-Gnaba H, 2018, WOOD SCI TECHNOL, V52, P1195, DOI 10.1007/s00226-018-1023-0.
   Dahle GA, 2017, WOOD SCI TECHNOL, V51, P1469, DOI 10.1007/s00226-017-0947-0.
   Dawson-Andoh B, 2012, WOOD SCI TECHNOL, V46, P1193, DOI 10.1007/s00226-012-0468-9.
   Defoirdt N, 2017, WOOD SCI TECHNOL, V51, P887, DOI 10.1007/s00226-017-0903-z.
   Denzler JK, 2013, WOOD SCI TECHNOL, V47, P749, DOI 10.1007/s00226-013-0536-9.
   Esteban LG, 2017, WOOD SCI TECHNOL, V51, P1249, DOI 10.1007/s00226-017-0932-7.
   Gu IYH, 2010, WOOD SCI TECHNOL, V44, P693, DOI 10.1007/s00226-009-0287-9.
   Hsu C.-W., 2008, TECH REP, V101, P1.
   Ibrahim I, 2018, EUR J WOOD WOOD PROD, V76, P345, DOI 10.1007/s00107-017-1163-1.
   Ibrahim I, 2017, WOOD SCI TECHNOL, V51, P431, DOI 10.1007/s00226-016-0859-4.
   Iglesias C, 2015, EUR J WOOD WOOD PROD, V73, P347, DOI 10.1007/s00107-015-0885-1.
   Khalid M, 2008, INT J SIMUL SYST SCI, V9, P9.
   Naganathan GK, 2016, J FOOD ENG, V169, P309, DOI 10.1016/j.jfoodeng.2015.09.001.
   Pahlberg T, 2015, WOOD SCI TECHNOL, V49, P7, DOI 10.1007/s00226-014-0679-3.
   Qin RY, 2018, WOOD SCI TECHNOL, V52, P1113, DOI 10.1007/s00226-018-1016-z.
   Sebera V, 2014, TREES-STRUCT FUNCT, V28, P1173, DOI 10.1007/s00468-014-1028-8.
   Wang HJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0076101.
   Wang Z, 2003, CONF REC ASILOMAR C, P1398.
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435.
   Woolley LA, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003233.
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423.
   Yusof R, 2013, MACH VISION APPL, V9, P9.
   Yusof R, 2013, COMPUT ELECTRON AGR, V93, P68, DOI 10.1016/j.compag.2013.01.007.
   Zamri MIP, 2016, COMPUT ELECTRON AGR, V124, P227, DOI 10.1016/j.compag.2016.04.004.
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730.},
Number-of-Cited-References = {34},
Times-Cited = {19},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {20},
Journal-ISO = {Wood Sci. Technol.},
Doc-Delivery-Number = {IL1KL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000477058800014},
DA = {2023-08-12},
}

@article{ WOS:000926223100001,
Author = {Lv, Zhimin and Zhang, Zhibin},
Title = {Research on plant leaf recognition method based on multi-feature fusion
   in different partition blocks},
Journal = {DIGITAL SIGNAL PROCESSING},
Year = {2023},
Volume = {134},
Month = {APR 15},
Abstract = {As an indispensable organism in nature, plants play an essential role in
   the ecological equilibrium. Ecological plant protection is also
   receiving increasing attention. In today's digital era, one of the
   important research topics is recognizing plants by using imaging
   equipment. In this study, we propose a multi-feature fusion approach
   based on the Local Binary Pattern (LBP) feature for plant leaf
   recognition by using the partition block strategy. By comparing the
   original LBP and Multiscale Block Local Binary Pattern (MB-LBP), we
   present an improved LBP feature descriptor. It extends the range of
   feature extraction, considering the effect that the multi-neighbourhood
   pixels have on the central pixels during the LBP encoding process and
   the central pixels are represented by double coding values. Therefore,
   it can extract more detailed information about plant leaves. Moreover,
   considering the leaf boundary and shape information extraction and the
   illumination variations, the Histogram of Oriented Gradient (HOG)
   feature and the colour feature are fused with the improved LBP feature
   descriptor. After dimensionality reduction by Principal Component
   Analysis (PCA), a mixed feature vector of plant leaves is used as input
   to an Extreme Learning Machine (ELM) for identifying plant leaves in two
   publicly available datasets, the Flavia dataset and the Swedish dataset.
   Experimental results show that our algorithm outperforms the
   conventional algorithms, with the recognition accuracy of 99.30\% on the
   Flavia dataset where 4 x 4 partition blocks are used for the improved
   LBP feature and 2 x 2 partition blocks for the colour feature, 99.52\%
   on the Swedish dataset where 4 x 4 partition blocks are used for the
   improved LBP feature and 2 x 2 partition blocks for the colour feature,
   respectively.(c) 2023 The Author(s). Published by Elsevier Inc. This is
   an open access article under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
Publisher = {ACADEMIC PRESS INC ELSEVIER SCIENCE},
Address = {525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA},
Type = {Article},
Language = {English},
Affiliation = {Zhang, ZB (Corresponding Author), Inner Mongolia Univ, Sch Comp Sci, Hohhot 010021, Peoples R China.
   Lv, Zhimin; Zhang, Zhibin, Inner Mongolia Univ, Sch Comp Sci, Hohhot 010021, Peoples R China.
   Lv, Zhimin; Zhang, Zhibin, Inner Mongolia Univ, Sch Comp Sci, Key Lab Wireless Networks \& Mobile Comp 3, Hohhot 010021, Peoples R China.},
DOI = {10.1016/j.dsp.2023.103907},
EarlyAccessDate = {JAN 2023},
Article-Number = {103907},
ISSN = {1051-2004},
EISSN = {1095-4333},
Keywords = {Leaf recognition; LBP; Multi-feature fusion; Partition block},
Keywords-Plus = {EXTREME LEARNING-MACHINE; SHAPE-FEATURES; CLASSIFICATION; TEXTURE},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {cszhibin@imu.edu.cn},
Affiliations = {Inner Mongolia University; Inner Mongolia University},
ResearcherID-Numbers = {张志斌, 内蒙古大学/IQS-5892-2023},
Funding-Acknowledgement = {Major Special Project of the Inner Mongolia Autonomous Region; National
   Natural Science Foundation of China {[}2021SZD0026, 31760345]},
Funding-Text = {We are grateful to all reviewers for their work and patience. This study
   is fully supported by the Major Special Project of the Inner Mongolia
   Autonomous Region and the National Natural Science Foundation of China
   (No. 2021SZD0026; No. 31760345) .},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Abdul K., 2011, SIGNAL IMAGE PROCESS, V2, P1, DOI 10.5121/sipij.2011.2301.
   Adeel A, 2022, EXPERT SYST, V39, DOI 10.1111/exsy.12569.
   Adeel A, 2019, SUSTAIN COMPUT-INFOR, V24, DOI 10.1016/j.suscom.2019.08.002.
   Alcin OF, 2016, NEUROCOMPUTING, V218, P251, DOI 10.1016/j.neucom.2016.08.050.
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Dahmane Mohamed, 2011, Proceedings 2011 IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2011), P884, DOI 10.1109/FG.2011.5771368.
   Dalal Navneet, 2005, PUTER VISION IMAGE U, DOI {[}10.1109/CVPR.2005.177, DOI 10.1016/J.CVIU.2007.09.014].
   Goyal N, 2022, MULTIMED TOOLS APPL, V81, P32243, DOI 10.1007/s11042-022-12825-z.
   Goyal N, 2021, MULTIMED TOOLS APPL, V80, P4533, DOI 10.1007/s11042-020-09899-y.
   Goyal N, 2019, MULTIMED TOOLS APPL, V78, P27785, DOI 10.1007/s11042-019-7588-2.
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407.
   Herdiyeni Y, 2012, 2012 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER SCIENCE AND INFORMATION SYSTEMS, P255.
   Herdiyeni Y, 2012, 2012 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER SCIENCE AND INFORMATION SYSTEMS, P301.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Huang GB, 2004, IEEE IJCNN, P985.
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126.
   Hussain N, 2022, CMC-COMPUT MATER CON, V70, P3281, DOI 10.32604/cmc.2022.019036.
   Iqbal Z, 2018, COMPUT ELECTRON AGR, V153, P12, DOI 10.1016/j.compag.2018.07.032.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Karaaba MF, 2015, 2015 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P203, DOI 10.1109/SSCI.2015.39.
   Khan MA, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12020593.
   Khan MA, 2020, MULTIMED TOOLS APPL, V79, P25763, DOI 10.1007/s11042-020-09244-3.
   Khan MA, 2020, MULTIMED TOOLS APPL, V79, P18627, DOI 10.1007/s11042-020-08726-8.
   Khan MA, 2018, COMPUT ELECTRON AGR, V155, P220, DOI 10.1016/j.compag.2018.10.013.
   Kumar M, 2019, IEEE ACCESS, V7, P163912, DOI 10.1109/ACCESS.2019.2952176.
   Liao SC, 2007, LECT NOTES COMPUT SC, V4642, P828.
   Lima AR, 2015, ENVIRON MODELL SOFTW, V73, P175, DOI 10.1016/j.envsoft.2015.08.002.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Liu J, 2017, PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P1606, DOI 10.1109/CompComm.2017.8322811.
   Muthevi A, 2017, IEEE INT ADV COMPUT, P870, DOI {[}10.1109/IACC.2017.169, 10.1109/IACC.2017.0178].
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Olatunji SO, 2017, CAN CON EL COMP EN.
   Sachar S, 2021, EXPERT SYST APPL, V167, DOI 10.1016/j.eswa.2020.114181.
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   Sari Cihan, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P23.
   Soderkvist O, 2010, COMPUTER VISION CLAS.
   Su BY, 2019, IEEE T INSTRUM MEAS, V68, P4675, DOI 10.1109/TIM.2019.2900961.
   Su JY, 2020, IEEE ACCESS, V8, P208753, DOI 10.1109/ACCESS.2020.3037649.
   Turkoglu M, 2019, PHYSICA A, V527, DOI 10.1016/j.physa.2019.121297.
   Turkoglu M, 2019, APPL MATH COMPUT, V352, P1, DOI 10.1016/j.amc.2019.01.054.
   Vi Nguyen Thanh Le, 2019, Information Processing in Agriculture, V6, P116, DOI 10.1016/j.inpa.2018.08.002.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yang CZ, 2019, IEEE ACCESS, V7, P178108, DOI 10.1109/ACCESS.2019.2958416.
   Zhang H., 2015, J COMPUTIONAL SYSTEM, V11, P141.
   Zhang J, 2020, NEUROCOMPUTING, V396, P383, DOI 10.1016/j.neucom.2018.11.106.},
Number-of-Cited-References = {48},
Times-Cited = {1},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Digit. Signal Prog.},
Doc-Delivery-Number = {8P0LQ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000926223100001},
OA = {hybrid},
DA = {2023-08-12},
}

@article{ WOS:000350930800018,
Author = {Garcia-Mateos, G. and Hernandez-Hernandez, J. L. and
   Escarabajal-Henarejos, D. and Jaen-Terrones, S. and Molina-Martinez, J.
   M.},
Title = {Study and comparison of color models for automatic image analysis in
   irrigation management applications},
Journal = {AGRICULTURAL WATER MANAGEMENT},
Year = {2015},
Volume = {151},
Number = {SI},
Pages = {158-166},
Month = {MAR 31},
Abstract = {Image processing and computer vision are increasingly being used in
   water management applications in agriculture. Images can provide
   valuable information on the percentage of ground cover, which is
   essential in determining crop irrigation needs. Techniques based on
   color analysis allow classifying accurately and efficiently soil/plant
   regions in the images. Many color spaces have been proposed, among them:
   RGB, rgb, XYZ, L{*}a{*}b{*}, L{*}u{*}v{*}, HSV, HIS, YCrCb, YUV, I1I2I3
   and TSL . Different possibilities to model the probability distribution
   of a given color class appear for each space; one of the most widespread
   non-parametric methods is modeling using histograms. This presents
   various alternatives in order to represent a color class: the number of
   channels, which channels to use, and the size of histograms. Using a
   wide and varied set of images of lettuce crops (Lactuca sativa)
   previously classified manually in soil and plant pixels a comprehensive
   analysis and comparison of the proposed color models has been conducted
   for the soil/plant classification problem. The experimental results
   demonstrate the superiority of models that separate luminance from
   chrominance. In particular, L{*}a{*}b{*} provides the best results with
   a{*} channel, producing a 99.2\% of correct classification. Further
   processing stages improve this performance up to 99.5\% accuracy, taking
   less than 1/3 of a second per image in a normal laptop. These results
   can be applied to reduce water consumption by optimizing the accuracy
   and efficiency of automatic image analysis of crops. (C) 2014 Elsevier
   B.V. All rights reserved.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Garcia-Mateos, G (Corresponding Author), Univ Murcia, Dept Comp Sci \& Syst, E-30100 Murcia, Spain.
   Garcia-Mateos, G.; Jaen-Terrones, S., Univ Murcia, Dept Comp Sci \& Syst, E-30100 Murcia, Spain.
   Escarabajal-Henarejos, D.; Molina-Martinez, J. M., Tech Univ Cartagena, Food Engn \& Agr Equipment Dept, Cartagena 30203, Spain.
   Hernandez-Hernandez, J. L., Autonomous Univ Guerrero, Acad Unit Engn, Guerrero, Mexico.},
DOI = {10.1016/j.agwat.2014.08.010},
ISSN = {0378-3774},
EISSN = {1873-2283},
Keywords = {Color spaces; Automatic irrigation computation; Image processing in
   agriculture; Color classification},
Keywords-Plus = {MACHINE VISION; LETTUCE; IDENTIFICATION},
Research-Areas = {Agriculture; Water Resources},
Web-of-Science-Categories  = {Agronomy; Water Resources},
Author-Email = {ginesgm@um.es},
Affiliations = {University of Murcia; Universidad Politecnica de Cartagena},
ResearcherID-Numbers = {Garcia-Mateos, G./G-7779-2015
   Martínez, José Miguel Molina/C-8059-2019
   Hernández-Hernández, José Luis/ABC-5674-2020
   Hernández-Hernández, José Luis/N-8637-2017
   Martínez, José Miguel Molina/AAG-1074-2019
   },
ORCID-Numbers = {Garcia-Mateos, G./0000-0003-2521-4454
   Hernández-Hernández, José Luis/0000-0003-0231-2019
   Martínez, José Miguel Molina/0000-0001-8122-5487
   Escarabajal-Henarejos, David/0000-0002-2210-5984},
Funding-Acknowledgement = {Spanish MINECO {[}TIN2012-38341-004-03]; ``Seneca Foundation{''} (Murcia
   Regional Authority, Spain) {[}08729/PI/08]},
Funding-Text = {This work was supported by the Spanish MINECO under grant
   TIN2012-38341-004-03, and by the ``Seneca Foundation{''} (Murcia
   Regional Authority, Spain) through the project with ref. 08729/PI/08.},
Cited-References = {Allen R.G., 1998, FAO IRRIGATION DRAIN.
   Allen RG, 2009, IRRIGATION SCI, V28, P17, DOI 10.1007/s00271-009-0182-z.
   {[}Anonymous], 2012, IJCA INT J COMPUTER, DOI DOI 10.5120/7773-0856.
   Astrand B, 2002, AUTON ROBOT, V13, P21, DOI 10.1023/A:1015674004201.
   Blasco J, 2002, BIOSYST ENG, V83, P149, DOI 10.1006/bioe.2002.0109.
   Campos I, 2010, AGR WATER MANAGE, V98, P45, DOI 10.1016/j.agwat.2010.07.011.
   Escarabajal-Henarejos D, 2015, AGR WATER MANAGE, V151, P167, DOI 10.1016/j.agwat.2014.10.012.
   Fernandez-Pacheco DG, 2014, BIOSYST ENG, V117, P23, DOI 10.1016/j.biosystemseng.2013.07.014.
   Giacomelli G. A., 1998, HortTechnology, V8, P361.
   Grant OM, 2012, IRRIGATION SCI, V30, P1, DOI 10.1007/s00271-010-0258-9.
   Hanson BR, 2006, AGR WATER MANAGE, V81, P381, DOI 10.1016/j.agwat.2005.04.007.
   Kakumanu P, 2007, PATTERN RECOGN, V40, P1106, DOI 10.1016/j.patcog.2006.06.010.
   Kumar P, 2002, IEEE 5TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, PROCEEDINGS, P100, DOI 10.1109/ITSC.2002.1041196.
   Lin K., 2013, ADV IMAGE GRAPHICS T, V363, P192.
   Ling PP, 1996, J AGR ENG RES, V65, P85, DOI 10.1006/jaer.1996.0082.
   Lopez-Urrea R, 2009, AGR WATER MANAGE, V96, P1031, DOI 10.1016/j.agwat.2009.02.004.
   Luszczkiewicz-Piatek M, 2014, IMAGE PROCESSING COM, V5, P55, DOI {[}10.1007/978-3-319-01622-1\_7, DOI 10.1007/978-3-319-01622-1\_7].
   McCarthy CL, 2010, INTEL SERV ROBOT, V3, P209, DOI 10.1007/s11370-010-0075-2.
   Meyer GE, 1998, T ASAE, V41, P1189, DOI 10.13031/2013.17244.
   OHTA Y, 1980, COMPUT VISION GRAPH, V13, P222, DOI 10.1016/0146-664X(80)90047-7.
   Shih PC, 2005, INT J PATTERN RECOGN, V19, P873, DOI 10.1142/S0218001405004381.
   Shiraishi M, 1996, J MANUF SCI E-T ASME, V118, P382, DOI 10.1115/1.2831041.
   Slaughter DC, 2008, COMPUT ELECTRON AGR, V61, P63, DOI 10.1016/j.compag.2007.05.008.
   Steward BL, 2004, T ASAE, V47, P609, DOI 10.13031/2013.16024.
   Story D, 2010, COMPUT ELECTRON AGR, V74, P238, DOI 10.1016/j.compag.2010.08.010.
   Terrillon J.-C., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P54, DOI 10.1109/AFGR.2000.840612.
   WOEBBECKE DM, 1995, T ASAE, V38, P259, DOI 10.13031/2013.27838.
   Xu X., 2010, P SPIE, V7824.},
Number-of-Cited-References = {28},
Times-Cited = {65},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {61},
Journal-ISO = {Agric. Water Manage.},
Doc-Delivery-Number = {CD2TB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000350930800018},
DA = {2023-08-12},
}

@article{ WOS:000746839100015,
Author = {Ran, Juan and Shi, Yu and Yu, Jinhao and Li, Delong},
Title = {A Multi-Feature Convolution Neural Network for Automatic Flower
   Recognition},
Journal = {JOURNAL OF CIRCUITS SYSTEMS AND COMPUTERS},
Year = {2021},
Volume = {30},
Number = {15},
Month = {DEC 15},
Abstract = {This paper discusses how to efficiently recognize flowers based on a
   convolutional neural network (CNN) using multiple features. Our proposed
   work consists of three phases including segmentation by Otsu
   thresholding with particle swarm optimization algorithms, feature
   extraction of color, shape, texture and recognition with the LeNet-5
   neural network. In the feature extraction, an improved H component with
   the definition of WGB value is applied to extract the color feature, and
   a new algorithm based on local binary pattern (LBP) is proposed to
   enhance the accuracy of texture extraction. Besides this, we replace
   ReLU with Mish as activation function in the network design, and
   therefore increase the accuracy by 8\% accuracy according to our
   comparison. The Oxford-102 and Oxford-17 datasets are adopted for
   benchmarking. The experimental results show that the combination of
   color features and texture features generates the highest recognition
   accuracy as 92.56\% on Oxford-102 and 93\% on Oxford-17.},
Publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
Address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
Type = {Article},
Language = {English},
Affiliation = {Shi, Y (Corresponding Author), Tianjin Univ, Dept Comp Sci \& Technol, Renai Coll, Tianjin, Peoples R China.
   Ran, Juan; Shi, Yu; Yu, Jinhao; Li, Delong, Tianjin Univ, Dept Comp Sci \& Technol, Renai Coll, Tianjin, Peoples R China.},
DOI = {10.1142/S0218126621502819},
Article-Number = {2150281},
ISSN = {0218-1266},
EISSN = {1793-6454},
Keywords = {Image segmentation; feature extraction; Mish; convolutional neural
   network},
Keywords-Plus = {FUSION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Hardware \& Architecture; Engineering, Electrical \&
   Electronic},
Author-Email = {syan\_cn4@163.com
   sonia\_yushi93@outlook.com
   misaka82@outlook.com
   18781103956@163.com},
Affiliations = {Tianjin University},
ResearcherID-Numbers = {Li, Delong/AAK-2554-2020
   Li, Delong/ISU-2722-2023},
ORCID-Numbers = {Li, Delong/0000-0003-4334-6768
   },
Cited-References = {Abdulameer F.S., 2019, ADV PHYS THEOR APPL, V76, P15.
   Ben Mabrouk A, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P201.
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851.
   Cao K, 2019, IEEE T COMPUT AID D, V38, P1799, DOI 10.1109/TCAD.2018.2873239.
   Cao K, 2019, J SYST ARCHITECT, V97, P397, DOI 10.1016/j.sysarc.2019.01.003.
   CHEN Yi-hu, 2013, J BAOJI U ARTS SCI N, V33, P16.
   Chen ZX, 2021, INT J PATTERN RECOGN, V35, DOI 10.1142/S0218001421520054.
   Cibuk M, 2019, MEASUREMENT, V137, P7, DOI 10.1016/j.measurement.2019.01.041.
   Diganta M, 2019, THESIS U CORNEL.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Guifang Lin, 2018, Procedia Computer Science, V131, P977, DOI 10.1016/j.procs.2018.04.239.
   Guru DS, 2011, MATH COMPUT MODEL, V54, P1030, DOI 10.1016/j.mcm.2010.11.032.
   Lee HH, 2017, IMAGE VISION COMPUT, V61, P98, DOI 10.1016/j.imavis.2017.01.013.
   Li LY, 2021, IEEE T COMPUT, V70, P581, DOI 10.1109/TC.2020.2991177.
   Li ZM, 2019, INT J PATTERN RECOGN, V33, DOI 10.1142/S0218001419560044.
   Liu JiaZheng, 2019, Journal of Beijing Forestry University, V41, P76.
   Liu S, 2011, ADV INTEL SOFT COMPU, V104, P415.
   Ma Ling, 2014, Journal of Computer Aided Design \& Computer Graphics, V26, P1272.
   Najjar A., 2012, 2012 INT C COMM INF, V2012, P397, DOI {[}10.1109/ICCITechnol.2012.6285834, DOI 10.1109/ICCITECHNOL.2012.6285834].
   Nilsback M.-E., 2006, P IEEE COMP SOC C CO, V2, P1447, DOI DOI 10.1109/CVPR.2006.42.
   Nilsback ME, 2010, IMAGE VISION COMPUT, V28, P1049, DOI 10.1016/j.imavis.2009.10.001.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Shahabi F, 2020, INT J COMPUT INTELL, V19, DOI 10.1142/S1469026820500157.
   Suta L., 2012, 2012 IEEE International Conference on Intelligent Computer Communication and Processing (ICCP 2012). Proceedings, P181, DOI 10.1109/ICCP.2012.6356183.
   Tian J., 2018, COMPUT SYST APPL, V27, P43, DOI 10.15888/j.cnki.csa.006463.
   Vitabile S, 2001, 11TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P572, DOI 10.1109/ICIAP.2001.957071.
   Wang W., 2018, J HUNAN CITY U NAT S, V27, P45.
   Wu XiaoXin, 2017, Journal of Beijing Forestry University, V39, P86.
   Yang J., 2019, J ENG HEILONGJIANG U, V10, P90.
   Zeng F., 2019, COMPUT KNOWLEDGE TEC, V15, P185.
   Zeng S., 2021, IEEE T CYBERN, V1, P1.
   Zeng SN, 2017, COMPUT ELECTRON AGR, V142, P563, DOI 10.1016/j.compag.2017.11.013.
   Zhang Juan, 2012, Journal of Beijing Forestry University, V34, P96.
   Zhou LJ, 2019, MULTIMED TOOLS APPL, V78, P14971, DOI 10.1007/s11042-018-6868-6.},
Number-of-Cited-References = {36},
Times-Cited = {3},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {22},
Journal-ISO = {J. Circuits Syst. Comput.},
Doc-Delivery-Number = {YM8SN},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000746839100015},
DA = {2023-08-12},
}

@article{ WOS:000632631300002,
Author = {Hussain, Nazar and Farooque, Aitazaz A. and Schumann, Arnold W. and
   Abbas, Farhat and Acharya, Bishnu and McKenzie-Gopsill, Andrew and
   Barrett, Ryan and Afzaal, Hassan and Zaman, Qamar U. and Cheema,
   Muhammad J. M.},
Title = {Application of deep learning to detect Lamb's quarters ( Chenopodium
   album L.) in potato fields of Atlantic Canada},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2021},
Volume = {182},
Month = {MAR},
Abstract = {Excessive use of herbicides for weed control increases the cost of crop
   production and can lead to environmental degradation. An intelligent
   spraying system can apply agrochemicals on an as-needed basis by
   detecting and selectively targeting the weeds. The objective of this
   research was to investigate the feasibility of using deep convolutional
   neural networks (DCNNs) for detecting lamb's quarters (Chenopodium
   album) in potato fields. Five potato fields were selected in Prince
   Edward Island (PEI) and New Brunswick (NB), Canada to collect images of
   spatially and temporally varied potato plants and lamb's quarters. The
   image database included pictures, taken under varying growth stages of
   potato, outdoor light (clear, cloudy, and partly cloudy), and shadowy
   conditions. The images were trained for DCNN models, namely GoogLe Net,
   VGG-16, and Efficient Net to classify lamb's quarters and potato plants.
   Performance of two frameworks, namely Tensor Flow and PyTorch, were
   compared in training, testing, and during inferring the DCNNs. Results
   showed excellent performance of DCNNs in lamb's quarters and potato
   plant classification (accuracy > 90\%). However, the Efficient Net with
   PyTorch framework showed a maximum accuracy of (0.92-0.97) for every
   growth stage of the plants. Inference times of DCNNs were recorded using
   three graphics processing units (GPUs), namely Nvidia GeForce 930MX,
   Nvidia GeForce GTX1080 Ti, and Nvidia GeForce GTX1050. All the DCNNs
   performed better with PyTorch than Tensor Flow frameworks. It was
   concluded that the trained models can be used in automation of the
   spraying systems for the site-specific application of agrochemicals for
   weed control in potato fields. Such precision agriculture technologies
   will ensure economically viable and environmentally safe potato
   cultivation.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Farooque, AA (Corresponding Author), Univ Prince Edward Isl, Fac Sustainable Design Engn, Charlottetown, PE C1A 4P3, Canada.
   Hussain, Nazar; Farooque, Aitazaz A.; Abbas, Farhat; Afzaal, Hassan, Univ Prince Edward Isl, Fac Sustainable Design Engn, Charlottetown, PE C1A 4P3, Canada.
   Farooque, Aitazaz A., Univ Prince Edward Isl, Sch Climate Change \& Adaptat, Charlottetown, PE C1A 4P3, Canada.
   Schumann, Arnold W., Univ Florida, Citrus Res \& Educ Ctr, Gainesville, FL 32611 USA.
   Acharya, Bishnu, Univ Saskatchewan, Dept Chem \& Biol Engn, Saskatoon, SK S7N 5A9, Canada.
   McKenzie-Gopsill, Andrew, Agr \& Agri Food Canada, Charlottetown Res \& Dev Ctr, Charlottetown, PE C1A 4N6, Canada.
   Barrett, Ryan, Prince Edward Isl Potato Board, West Royalty Business Pk,90 Hillstrom Ave, Charlottetown, PE C1E 2C6, Canada.
   Zaman, Qamar U., Dalhousie Univ, Engn Dept, Agr Campus, Truro, NS B2N 5E3, Canada.
   Cheema, Muhammad J. M., PMAS Arid Agr Univ Rawalpindi, Fac Agr Engn \& Technol, Rawalpindi 46000, PB, Pakistan.},
DOI = {10.1016/j.compag.2021.106040},
EarlyAccessDate = {FEB 2021},
Article-Number = {106040},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Agrochemicals; Deep convolutional neural network; Deep learning; Image
   processing; Precision agriculture technologies; Smart sprayer},
Keywords-Plus = {NEURAL-NETWORKS; WEED-CONTROL; MANAGEMENT; CLASSIFICATION; DISEASE},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {afarooque@upei.ca},
Affiliations = {University of Prince Edward Island; University of Prince Edward Island;
   State University System of Florida; University of Florida; University of
   Saskatchewan; Agriculture \& Agri Food Canada; Dalhousie University;
   Arid Agriculture University},
ResearcherID-Numbers = {Cheema, Muhammad Jehanzeb Masud/ITT-1006-2023
   Cheema, Muhammad Jehanzeb Masud/Q-1001-2019
   Mckenzie-Gopsill, Andrew/GMX-4866-2022
   },
ORCID-Numbers = {Cheema, Muhammad Jehanzeb Masud/0000-0002-7911-7548
   Cheema, Muhammad Jehanzeb Masud/0000-0002-7911-7548
   Mckenzie-Gopsill, Andrew/0000-0003-0071-4786
   Abbas, Farhat/0000-0002-2032-8527},
Funding-Acknowledgement = {Potato Board of Prince Edward Island; New Brunswick Potato Board;
   Agriculture and AgriFood Canada (Charlottetown); Precision Agriculture
   Research Group of University of Prince Edward Island},
Funding-Text = {Support from Potato Board of Prince Edward Island, New Brunswick Potato
   Board, Agriculture and AgriFood Canada (Charlottetown) and Precision
   Agriculture Research Group of University of Prince Edward Island is
   acknowledged.},
Cited-References = {Abadi M., 2016, GPU COMPUT GEMS EMER, P277.
   Abdulridha J, 2019, COMPUT ELECTRON AGR, V156, P549, DOI 10.1016/j.compag.2018.12.018.
   {[}Anonymous], 2015, P 3 INT C LEARNING R.
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615.
   BASSETT IJ, 1978, CAN J PLANT SCI, V58, P1061, DOI 10.4141/cjps78-161.
   Bianco S, 2018, IEEE ACCESS, V6, P64270, DOI 10.1109/ACCESS.2018.2877890.
   Boydston RA, 2012, WEED TECHNOL, V26, P731, DOI 10.1614/WT-D-12-00023.1.
   Camire M.E., 2016, ADV POTATO CHEM TECH, P685, DOI 10.1016/B978-0-12-800002-1.00023-6..
   Cox S, 2002, COMPUT ELECTRON AGR, V36, P93, DOI 10.1016/S0168-1699(02)00095-9.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Dutta A., 2018, PUBLIKATIONEN DGPF, P633.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Fernandez-Cornejo J., 2014, EIB124 USDA EC RES S, P124, DOI 10.2139/ssrn.2502986.
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013.
   Hall D, 2015, IEEE WINT CONF APPL, P797, DOI 10.1109/WACV.2015.111.
   Harker KN, 2013, WEED TECHNOL, V27, P1, DOI 10.1614/WT-D-12-00109.1.
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680.
   Huang HS, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0196302.
   Jain A, 2019, IEEE INT C CL COMP, P58.
   Ketkar N., 2017, DEEP LEARNING PYTHON, P195, DOI DOI 10.1007/978-1-4842-2766-4\_12.
   Kochura Y, 2017, PROCEEDINGS OF THE 2017 12TH INTERNATIONAL SCIENTIFIC AND TECHNICAL CONFERENCE ON COMPUTER SCIENCES AND INFORMATION TECHNOLOGIES (CSIT 2017), VOL. 1, P373, DOI 10.1109/STC-CSIT.2017.8098808.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Liu YZ, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P1025.
   McKenzie-Gopsill A, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10091369.
   Melakeberhan H, 2008, PRECIS AGRIC, V9, P341, DOI 10.1007/s11119-008-9071-3.
   Mirshekari B., 2012, World Applied Sciences Journal, V16, P1247.
   Mungofa Perseveranca, 2018, BMC Res Notes, V11, P703, DOI 10.1186/s13104-018-3813-8.
   Norsworthy JK, 2012, WEED SCI, V60, P31, DOI 10.1614/WS-D-11-00155.1.
   Redmon J, 2015, IEEE INT CONF ROBOT, P1316, DOI 10.1109/ICRA.2015.7139361.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Sabzi S, 2018, J AGR SCI-TARIM BILI, V24, P105, DOI 10.15832/ankutbd.446402.
   Shaner DL, 2014, PEST MANAG SCI, V70, P1329, DOI 10.1002/ps.3706.
   Szegedy C., 2015, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2015.7298594.
   Tan M., 2019, PR MACH LEARN RES.
   Wang Y, 2017, P DES AUT C 128280.
   Yang CC, 2000, CAN AGR ENG, V42, P147.
   Yao Y, 2006, ATMOS ENVIRON, V40, P4339, DOI 10.1016/j.atmosenv.2006.03.039.
   Yu JL, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.01422.
   Yu JL, 2019, PEST MANAG SCI, V75, P2211, DOI 10.1002/ps.5349.
   Zaman QU, 2011, COMPUT ELECTRON AGR, V76, P175, DOI 10.1016/j.compag.2011.01.014.
   Zhang X., 2019, P CAMP PERFORMANCE C.},
Number-of-Cited-References = {41},
Times-Cited = {10},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {16},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {RC2JT},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000632631300002},
DA = {2023-08-12},
}

@inproceedings{ WOS:000982380800041,
Author = {Shahmiri, Lida and Wong, Patrick and Dooley, Laurence S.},
Book-Group-Author = {IEEE},
Title = {Accurate Medicinal Plant Identification in Natural Environments by
   Embedding Mutual Information in a Convolution Neural Network Model},
Booktitle = {2022 IEEE 5TH INTERNATIONAL CONFERENCE ON IMAGE PROCESSING APPLICATIONS
   AND SYSTEMS, IPAS},
Year = {2022},
Note = {5th IEEE International Conference on Image Processing Applications and
   Systems (IPAS), Genoa Univ, Genova, ITALY, DEC 05-07, 2022},
Organization = {IEEE; Sfax Univ, Natl Engn Sch Sfax; INRIA; Inria; IEEE Tunisia Sect;
   IEEE Italy Sect Reg 08; ENIS; IEEE Italy Sect Geoscience \& Remote
   Sensing Soc Chapter; IEEE Tunisia Sect Geoscience \& Remote Sensing Soc
   Chapter; IEEE Italy Sect Comp Chapter; IEEE Italy Sect Engn Med \& Biol
   Chapter; IEEE Tunisia Sect Engn Med \& Biol Chapter; Assoc Boussole Rech
   Sci},
Abstract = {Medicinal plants are a primary source of disease treatment in many
   countries. As most are edible however, consumption of the wrong herbal
   plants can have serious consequences and even lead to death. Automatic
   accurate recognition of plant species to help users who do not have
   specialist knowledge of herbal plants is thus a desirable aim. Several
   automatic medicinal plant identification systems have been proposed,
   though most are significantly constrained either in the small number of
   species or in requiring manual image segmentation of plant leaves. This
   means they are captured on a plain background rather than being readily
   identified in their natural surroundings, which often involve complex
   and noisy backgrounds. While deep learning (DL) based methods have made
   considerable strides in recent times, their potential has not always
   been maximised because they are trained with samples which are not
   always fully representative of the intra-class and interclass
   differences between the plant species concerned. This paper addresses
   this challenge by incorporating mutual information into a Convolutional
   Neural Network (CNN) model to select samples for the training,
   validation, and testing sets based on a similarity measure. A critical
   comparative evaluation of this new CNN medicinal plant classification
   model incorporating a mutual information guided training (MIGT)
   algorithm for sample selection, corroborates the superior classification
   performance achieved for the VNPlant-200 dataset, with an average
   accuracy of more than 97\%, while the precision and recall values are
   also consistently above 97\%. This is significantly better than existing
   CNN classification methods for this dataset as it crucially means false
   positive rates are substantially lower thus affording improved
   identification reliability.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Shahmiri, L (Corresponding Author), Open Univ, Sch Comp \& Commun, Milton Keynes, Bucks, England.
   Shahmiri, Lida; Wong, Patrick; Dooley, Laurence S., Open Univ, Sch Comp \& Commun, Milton Keynes, Bucks, England.},
ISBN = {978-1-6654-6219-8},
Keywords = {Medicinal plants; Natural Environment; Convolutional Neural Networks;
   Mutual Information; training dataset},
Research-Areas = {Remote Sensing},
Web-of-Science-Categories  = {Remote Sensing},
Author-Email = {ls26755@open.ac.uk
   patrick.wong@open.ac.uk
   laurence.dooley@open.ac.uk},
Affiliations = {Open University - UK},
Cited-References = {{[}Anonymous], 2017, P 2017 25 TELECOMMUN, DOI {[}10.1109/ICISC.2017.8068597, DOI 10.1109/ICISC.2017.8068597].
   Avery K. R., 2014, SAE INT J MATER MANU, V7, P1251.
   Bojamma A. M., 2021, International Journal of Information Technology, V13, P989, DOI 10.1007/s41870-019-00379-7.
   Dileep MR, 2019, TENCON IEEE REGION, P321, DOI 10.1109/TENCON.2019.8929394.
   Fang H, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20110823.
   Fu YB, 2020, PHYS MED BIOL, V65, DOI 10.1088/1361-6560/ab843e.
   Gokhale A, 2020, IDENTIFICATION MED P, V1155.
   Jayanka M., 2020, VIDYODAYA J SCI, V23, P48.
   Keerthi K. L, 2022, AIP C P, V2463.
   Mareta Affix, 2018, 2018 International Conference on Information and Communications Technology (ICOIACT), P612, DOI 10.1109/ICOIACT.2018.8350775.
   Duong-Trung N, 2019, PROCEEDINGS OF 2019 4TH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION TECHNOLOGY (ICIIT 2019), P83, DOI 10.1145/3321454.3321464.
   Paulson Anu, 2020, 2020 Advanced Computing and Communication Technologies for High Performance Applications (ACCTHPA), P57, DOI 10.1109/ACCTHPA49271.2020.9213224.
   Quoc T. N., 2021, LECT NOTES NETWORKS, V136, P406.
   Roopashree S, 2021, IEEE ACCESS, V9, P135927, DOI 10.1109/ACCESS.2021.3116207.
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x.
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0.
   Soderkvist O., 2001, THESIS.
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002.
   Sun X, 2021, Arxiv, DOI arXiv:2108.02373.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Quoc TN, 2020, I C INF COMM TECH CO, P25, DOI 10.1109/ICTC49870.2020.9289480.
   Vo Anh H., 2019, International Journal of Machine Learning and Computing, V9, P363, DOI 10.18178/ijmlc.2019.9.3.811.
   Waldchen J, 2018, PLANT SPECIES IDENTI, V25.},
Number-of-Cited-References = {23},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BV1AN},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000982380800041},
DA = {2023-08-12},
}

@inproceedings{ WOS:000346159700110,
Author = {Yusof, Rubiyah and Rosli, Nenny Ruthfalydia},
Editor = {Yetongnon, K and Dipanda, A and Chbeir, R},
Title = {Tropical Wood Species Recognition System Based on Gabor Filter as Image
   Multiplier},
Booktitle = {2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY \&
   INTERNET-BASED SYSTEMS (SITIS)},
Year = {2013},
Pages = {737-743},
Note = {9th International Conference on Signal-Image Technology and
   Internet-Based Systems (SITIS), Kyoto, JAPAN, DEC 02-05, 2013},
Abstract = {The main problem in wood species recognition system is the lack of
   discriminative features of the texture images. Some of the wood species
   have similar patterns with others and some have different patterns even
   though they are of the same species. Moreover, the growth rings for
   tropical wood changes slightly due seasonal changes in climate. One of
   the ways to improve the system is by providing more features
   representation of each species. In this work, Gabor filter is proposed
   to generate multiple processed images from a single image so that more
   features can be extracted and trained by the neural network. After the
   raw image has been sharpened and contrast enhancement has been applied
   at the preprocessing stage, the image will be convolved with Gabor
   filters. The output of the convolution generates Gabor images which are
   images extracted based on frequency and spatial information of the
   original images. These Gabor images will be used by grey level
   co-occurrence matrix (GLCM) for feature extraction. A multi-layer neural
   network based on popular back-propagation (MLBP) algorithm is used for
   classification. The result shows that increasing the number of features
   by means of Gabor filters as well as the right combination of Gabor
   filters increases the accuracy rate of the system.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Yusof, R (Corresponding Author), Univ Teknol Malaysia, Ctr Artificial Intelligence \& Robot, Int Campus,Jalan Semarak, Kuala Lumpur 54100, Malaysia.
   Yusof, Rubiyah; Rosli, Nenny Ruthfalydia, Univ Teknol Malaysia, Ctr Artificial Intelligence \& Robot, Kuala Lumpur 54100, Malaysia.},
DOI = {10.1109/SITIS.2013.120},
ISBN = {978-1-4799-3211-5},
Keywords = {image multiplier; texture pattern recognition; Gabor filter; grey level
   co-occurrence matrix (GLCM); neural network; wood recognition},
Keywords-Plus = {FEATURES},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic},
Author-Email = {rubiyah@ic.utm.my
   nenny@ic.utm.my},
Affiliations = {Universiti Teknologi Malaysia},
ResearcherID-Numbers = {yusof, rubiyah/AAV-9212-2020},
Cited-References = {ALLIER B, 2003, P 7 INT C DOC AN REC.
   Andrysiak T, 2005, INT J AP MAT COM-POL, V15, P471.
   {[}Anonymous], 1998, HDB PATTERN RECOGNIT.
   Bhuiyan AA, 2007, PROC WRLD ACAD SCI E, V22, P51.
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160.
   Delac K., 2006, P INT C SYST SIGN IM, P95.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Harowitz S.L, 1974, P 2 IJCPR, P424.
   Huang LL, 2005, PATTERN RECOGN LETT, V26, P1641, DOI 10.1016/j.patrec.2005.01.015.
   Khalid M, 2008, INT J SIMUL SYST SCI, V9, P9.
   Lew Y. L., 2005, DESIGN INTELLIGENT W.
   MACLENNAN BJ, 1991, CS91144 U TENN.
   {*}MAL TIMB COUNC, 2007, MAL SUST FOR MAN.
   Menon P. K. B., 1993, STRUCTURE IDENTIFICA.
   OMATU S, 1995, NEUROCONTROL ITS APP.
   Tuceryan M., 1993, TEXTURE ANAL HDB PAT, V2, P207, DOI DOI 10.1142/9789814343138\_0010.
   Wheeler EA, 1998, IAWA J, V19, P241, DOI 10.1163/22941932-90001528.},
Number-of-Cited-References = {18},
Times-Cited = {3},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BB7ZQ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000346159700110},
DA = {2023-08-12},
}

@article{ WOS:000924774800001,
Author = {Zhang, Xuechen and Wu, Zhengmin and Cao, Chengmao and Luo, Kun and Qin,
   Kuan and Huang, Yangyang and Cao, Jie},
Title = {Design and operation of a deep-learning-based fresh tea-leaf sorting
   robot},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2023},
Volume = {206},
Month = {MAR},
Abstract = {Tea is one of the most popular beverages worldwide. It is rich in
   substances, such as tea polyphenols and polysaccharides, that are
   closely related to human health. However, fresh tea leaves picked by
   machines are neither neat nor uniform. In this study, we apply a deep
   learning algorithm and the DELTA parallel robotic arm for high-precision
   and fast sorting of machine-picked fresh tea leaves. First, a
   convolutional neural network algorithm and a regional segmentation
   method are used to achieve fast identification and localization of
   machine-picked fresh tea leaves. Second, the actual running time model
   of the stepper motor at different pulse frequencies was established by
   the interpolation fitting method. The running times of the parallel
   mechanical arm associated with S-curve type acceleration and
   deceleration motions were calculated accurately using this model, and
   the sorting point of tea fresh leaves was then determined. A
   multi-objective, continuous sorting model of machine-picked fresh tea
   leaves is constructed to verify and optimize the model effects. The
   results of the robot's arm running-time test show that the running time
   from the end of the robotic arm's application to the fresh tea leaf
   sorting plane is in the range 500-1800 ms. The experimental results of
   fresh tea leaf recognition and classification show that after 150
   iterations, the recognition accuracy of the validation set can reach
   99.82\%. Finally, the average sorting accuracy of the four experiments
   reach 89\%, with the highest sorting accuracy reaching up to 92\%. These
   results demonstrate that the proposed method leads to an excellent
   sorting effect pertaining to machine-picked fresh tea leaves. This
   approach could be easily applied to any agricultural product sorting
   application.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Wu, ZM (Corresponding Author), Anhui Agr Univ, Sch Tea \& Food Sand Technol, Hefei 230036, Peoples R China.
   Wu, ZM (Corresponding Author), Minist Agr \& Rural Affairs, Minist Educ, State Key Lab Tea Plant Biol \& Utilizat, Key Lab Tea Biol \& Tea Proc,Int Joint Res Lab Tea, Hefei 230036, Anhui, Peoples R China.
   Zhang, Xuechen; Cao, Chengmao; Luo, Kun; Qin, Kuan; Huang, Yangyang, Anhui Agr Univ, Coll Engn, Hefei 230036, Peoples R China.
   Wu, Zhengmin; Cao, Jie, Anhui Agr Univ, Sch Tea \& Food Sand Technol, Hefei 230036, Peoples R China.
   Wu, Zhengmin, Minist Agr \& Rural Affairs, Minist Educ, State Key Lab Tea Plant Biol \& Utilizat, Key Lab Tea Biol \& Tea Proc,Int Joint Res Lab Tea, Hefei 230036, Anhui, Peoples R China.},
DOI = {10.1016/j.compag.2023.107664},
EarlyAccessDate = {JAN 2023},
Article-Number = {107664},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {DELTA parallel robotic arm; Convolutional neural network; Sorting robot;
   Fresh tea leaf},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {21720680@stu.ahau.edu.cn
   wzmin@ahau.edu.cn
   ccm@ahau.edu.cn
   luokun2071@stu.ahau.edu.cn
   qinkuan@ahau.edu.cn
   hyy2@stu.ahau.edu.cn
   Caojie1125@163.com},
Affiliations = {Anhui Agricultural University; Anhui Agricultural University; Ministry
   of Agriculture \& Rural Affairs},
Funding-Acknowledgement = {National Key Research and Devel- opment Program of China
   {[}2021YFD1601102]; Na- tional Natural Science Foundation of China
   {[}52105239]; Anhui Provincial Education Department Key Projects
   {[}KJ2020A0133]},
Funding-Text = {Funding This work was supported by the National Key Research and Devel-
   opment Program of China {[}grant number 2021YFD1601102] ; the Na- tional
   Natural Science Foundation of China {[}grant number 52105239] ; and the
   Anhui Provincial Education Department Key Projects {[}grant number
   KJ2020A0133] .},
Cited-References = {Bakhshipour A, 2018, FOOD ANAL METHOD, V11, P1041, DOI 10.1007/s12161-017-1075-z.
   Chen QZ, 2018, INT J ADV ROBOT SYST, V15, DOI 10.1177/1729881418813805.
   Chen YT, 2020, COMPUT ELECTRON AGR, V171, DOI 10.1016/j.compag.2020.105298.
   Chen ZW, 2020, J FOOD PROCESS ENG, V43, DOI 10.1111/jfpe.13474.
   Cheng HT, 2018, SHOCK VIB, V2018, DOI 10.1155/2018/2945314.
   Gan N, 2022, J SCI FOOD AGR, V102, P6858, DOI 10.1002/jsfa.12047.
   {[}高震宇 Gao Zhenyu], 2017, {[}农业机械学报, Transactions of the Chinese Society for Agricultural Machinery], V48, P53.
   Hu GS, 2019, SUSTAIN COMPUT-INFOR, V24, DOI 10.1016/j.suscom.2019.100353.
   Jiang MF, 2021, MICROPROCESS MICROSY, V81, DOI 10.1016/j.micpro.2020.103738.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Kuo YL, 2017, INT J ADV ROBOT SYST, V14, DOI 10.1177/1729881417738738.
   Lin LH, 2019, CONCURR COMP-PRACT E, V31, DOI 10.1002/cpe.4519.
   Liu C, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/9681530.
   Liu C, 2019, INT J ELEC ENG EDUC, DOI 10.1177/0020720919894200.
   Mishra P, 2018, J FOOD ENG, V238, P70, DOI 10.1016/j.jfoodeng.2018.06.015.
   Mukhopadhyay S, 2021, MULTIMED TOOLS APPL, V80, P753, DOI 10.1007/s11042-020-09567-1.
   Sethy PK, 2022, MULTIMED TOOLS APPL, V81, P8309, DOI 10.1007/s11042-022-12286-4.
   Song Yan, 2018, Transactions of the Chinese Society of Agricultural Engineering, V34, P279, DOI 10.11975/j.issn.1002-6819.2018.23.036.
   Wagle SA, 2021, TRAIT SIGNAL, V38, P79, DOI 10.18280/ts.380108.
   Wang R, 2020, APPL ENG AGRIC, V36, P399, DOI 10.13031/aea.13467.
   Wu Q.B., 2021, CONTROL ENG CHINA, V28, P1, DOI {[}10.14107/j.cnki.kzgc.20190665, DOI 10.14107/J.CNKI.KZGC.20190665].
   Wu XH, 2019, J FOOD PROCESS ENG, V42, DOI 10.1111/jfpe.13298.
   Wu ZM, 2020, COMPUT ELECTRON AGR, V169, DOI 10.1016/j.compag.2019.105207.
   Xu XY, 2020, CRIT REV FOOD SCI, V60, P1693, DOI 10.1080/10408398.2019.1588223.
   Yan ZM, 2020, ANIM NUTR, V6, P115, DOI 10.1016/j.aninu.2020.01.001.
   Yang HL, 2021, COMPUT ELECTRON AGR, V181, DOI 10.1016/j.compag.2020.105946.},
Number-of-Cited-References = {26},
Times-Cited = {3},
Usage-Count-Last-180-days = {38},
Usage-Count-Since-2013 = {38},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {8M9KJ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000924774800001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000391380904043,
Author = {Bagal, V. C. and Manza, R. R.},
Book-Group-Author = {IEEE},
Title = {Feature Extraction of Plant Species from Leaf Architecture},
Booktitle = {2016 INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS, AND
   OPTIMIZATION TECHNIQUES (ICEEOT)},
Year = {2016},
Pages = {4079-4080},
Note = {International Conference on Electrical, Electronics, and Optimization
   Techniques (ICEEOT), Palanchur, INDIA, MAR 03-05, 2016},
Organization = {DMI Coll Engn; IEEE DMI Coll Student Branch},
Abstract = {Plants are very important for human being as well as for other living
   species on the earth. The food that people eat daily, comes directly or
   indirectly from plants. In medical field, doctors use X-ray image to
   correctly identify disease. Here we used the same principle. Geometrical
   features and digital morphological features are extracted from
   2-dimensional image of leaf. The aim of this study is to introduce
   suitable features of leaf image which can be useful in further research
   on plant identification.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Bagal, VC (Corresponding Author), KK Wagh Inst Engn Educ \& Res, Dept MCA, Nasik, Maharashtra, India.
   Bagal, V. C., KK Wagh Inst Engn Educ \& Res, Dept MCA, Nasik, Maharashtra, India.
   Manza, R. R., Dr Babasaheb Ambedkar Marathwada Univ, Dept CS \& IT, Aurangabad, Maharashtra, India.},
ISBN = {978-1-4673-9939-5},
Keywords = {morphological features; feature extraction; image preprocssing; leaf},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {vanbagal@rediffmail.com
   manzaramesh@gmail.com},
Affiliations = {Dr. Babasaheb Ambedkar Marathwada University (BAMU)},
ORCID-Numbers = {Manza, Ramesh/0000-0002-4510-9224},
Cited-References = {Harish B.S., 2013, CLASSIFICATION PLANT.
   LEE C.L., 2003, 16 IPPR C COMP VIS.
   Najjar A., 2012 P INT C COMM IN, P397.
   Shabanzade, SIGNAL IMAGE PROCESS, V2, P23, DOI {[}10.5121/SIPIJ, DOI 10.5121/SIPIJ].
   Wu Q. W., ADV CIENCIAS COMPUTA, P5.},
Number-of-Cited-References = {5},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BG7JO},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000391380904043},
DA = {2023-08-12},
}

@inproceedings{ WOS:000768800300005,
Author = {Divekar, Sarit and Rabaev, Irina and Litvak, Marina},
Book-Group-Author = {IEEE},
Title = {Urban Planter: A Web App for Automatic Classification of Urban Plants},
Booktitle = {2021 INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE
   PROCESSING (VCIP)},
Series = {IEEE International Conference on Visual Communications and Image
   Processing},
Year = {2021},
Note = {IEEE International Conference on Visual Communications and Image
   Processing (VCIP) - Visual Communications in the Era of AI and Limited
   Resources, Munich, GERMANY, DEC 05-08, 2021},
Organization = {IEEE; Meta; Huawei; Qualcomm; IEEE Circuits \& Syst Soc; Leibniz Univ
   Hannover; FAU; Natl Yang Ming Chiao Tung Univ},
Abstract = {Plant classification requires an expert because subtle differences in
   leaves or petal forms might differentiate between different species. On
   the contrary, some species are characterized by high variability in
   appearance. This paper introduces a web app for assisting people in
   identifying plants for discovering the best growing methods.The uploaded
   picture is submitted to the back-end server, and a pre-trained neural
   network classifies it to one of the predefined classes. The
   classification label and confidence are displayed to the end user on the
   front-end page. The application focuses on the house and garden plant
   species that can be grown mainly in a desert climate and are not covered
   by existing datasets. For training a model, we collected the Urban
   Planter dataset. The installation code of the alpha version and the demo
   video of the app can be found on
   https://github.com/UrbanPlanter/urbanplanterapp.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Divekar, S (Corresponding Author), Shamoon Coll Engn, Software Engn Dept, Ashdod, Israel.
   Divekar, Sarit; Rabaev, Irina; Litvak, Marina, Shamoon Coll Engn, Software Engn Dept, Ashdod, Israel.},
DOI = {10.1109/VCIP53242.2021.9675318},
ISSN = {2642-9357},
ISBN = {978-1-7281-8551-4},
Keywords = {Plant Classification; Deep Learning; Web App; Urban Plants Dataset},
Research-Areas = {Computer Science; Imaging Science \& Photographic Technology;
   Telecommunications},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Imaging Science \& Photographic Technology; Telecommunications},
Author-Email = {saritdi@ac.sce.ac.il
   irinar@ac.sce.ac.il
   marinal@ac.sce.ac.il},
Cited-References = {Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.},
Number-of-Cited-References = {1},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BS7YH},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000768800300005},
DA = {2023-08-12},
}

@inproceedings{ WOS:000470002200041,
Author = {Cervantes, Jair and Garcia Lamont, Farid and Rodriguez Mazahua, Lisbeth
   and Zarco Hidalgo, Alfonso and Ruiz Castilla, Jose S.},
Editor = {Huang, DS and Gromiha, MM and Han, K and Hussain, A},
Title = {Complex Identification of Plants from Leaves},
Booktitle = {INTELLIGENT COMPUTING METHODOLOGIES, ICIC 2018, PT III},
Series = {Lecture Notes in Artificial Intelligence},
Year = {2018},
Volume = {10956},
Pages = {376-387},
Note = {14th International Conference on Intelligent Computing (ICIC), Wuhan,
   PEOPLES R CHINA, AUG 15-18, 2018},
Organization = {IEEE Computat Intelligence Soc; Int Neural Network Soc; Natl Sci Fdn
   China; Tongji Univ; Wuhan Univ Sci, \& Technol; Wuhan Inst Technol},
Abstract = {The automatic identification of plant leaves is a very important current
   topic of research in vision systems. Several researchers have tried to
   solve the problem of identification from plant leaves proposing various
   techniques. The proposed techniques in the literature have obtained
   excellent results on data sets where the leaves have dissimilar features
   to each other. However, in cases where the leaves are very similar to
   each other, the classification accuracy falls significantly. In this
   paper, we proposed a system to deal with the performance problem of
   machine learning algorithms where the leaves are very similar. The
   results obtained show that combination of different features and
   features selection process can improve the classification accuracy.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Cervantes, J (Corresponding Author), UAEMEX Autonomous Univ Mexico State, Posgrad \& Invest, Texcoco 56259, Mexico.
   Cervantes, Jair; Garcia Lamont, Farid; Zarco Hidalgo, Alfonso; Ruiz Castilla, Jose S., UAEMEX Autonomous Univ Mexico State, Posgrad \& Invest, Texcoco 56259, Mexico.
   Rodriguez Mazahua, Lisbeth, Inst Tecnol Orizaba, Div Res \& Postgrad Studies, Ave Oriente 9,852 Col Emiliano Zapata, Orizaba 94320, Mexico.},
DOI = {10.1007/978-3-319-95957-3\_41},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-95957-3; 978-3-319-95956-6},
Keywords = {Plant identification; Vision system; Features selection},
Keywords-Plus = {SHAPE; RECOGNITION; TEXTURE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {jcervantesc@uaemex.mx
   fgarcial@uaemex.mx
   lisbethr08@gmail.com
   jsergioruizc@gmail.com},
ResearcherID-Numbers = {Rodríguez, Lisbeth/O-1484-2015
   Cervantes, Jair/N-2617-2013
   García-Lamont, Farid/B-4009-2016
   },
ORCID-Numbers = {Rodríguez, Lisbeth/0000-0002-9861-3993
   Cervantes, Jair/0000-0003-2012-8151
   García-Lamont, Farid/0000-0002-9739-3802
   Ruiz Castilla, Jose Sergio/0000-0001-7821-4912},
Funding-Acknowledgement = {Research Secretariat of the Autonomous University of the State of Mexico
   {[}5228/2018/CI]},
Funding-Text = {This study was funded by the Research Secretariat of the Autonomous
   University of the State of Mexico with the research project
   5228/2018/CI.},
Cited-References = {Cerutti G, 2013, COMPUT VIS IMAGE UND, V117, P1482, DOI 10.1016/j.cviu.2013.07.003.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   HE DC, 1990, IEEE T GEOSCI REMOTE, V28, P509.
   Huang Y., 2008, AGR ENG INT, V10, P1.
   Husin Z, 2012, COMPUT ELECTRON AGR, V89, P18, DOI 10.1016/j.compag.2012.07.009.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Liu NA, 2016, NEUROCOMPUTING, V216, P460, DOI 10.1016/j.neucom.2016.08.005.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Parekh R., 2012, INT J ADV ENG TECHNO, V2, P149.
   Park JS, 2004, LECT NOTES COMPUT SC, V3332, P146.
   Sampallo G., 2003, INTELIGENCIA ARTIFIC, V7, P55.
   Singh Vijai, 2017, Information Processing in Agriculture, V4, P41, DOI 10.1016/j.inpa.2016.10.005.
   Tico M., 2000, NORSIG2000. Nordic Signal Processing Symposium, P157.
   VijayaLakshmi B, 2016, COMPUT ELECTRON AGR, V125, P99, DOI 10.1016/j.compag.2016.04.033.},
Number-of-Cited-References = {17},
Times-Cited = {4},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BM8UK},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000470002200041},
OA = {Green Submitted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000402613900078,
Author = {Isnanto, R. Rizal and Zahra, Ajub Ajulian and Julietta, Patricia},
Editor = {Facta, M and Riyadi, M and Widianto, ED and Arfan, M},
Title = {Pattern Recognition on Herbs Leaves Using Region-Based Invariants
   Feature Extraction},
Booktitle = {2016 3RD INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY, COMPUTER,
   AND ELECTRICAL ENGINEERING (ICITACEE)},
Year = {2016},
Pages = {455-459},
Note = {3rd International Conference on Information Technology, Computer, and
   Electrical Engineering (ICITACEE), Semarang, INDONESIA, OCT 19-21, 2016},
Organization = {Univ Diponegoro, Engn Fac, Elect Engn Dept; Univ Diponegoro, Engn Fac,
   Comp Engn Dept; Univ Diponegoro; IEEE},
Abstract = {As medicine, herbal plants have been widely used since ancient times,
   and are still used today. There are various types of herbal plants that
   can be used as medicine but due to the limited ability of communities to
   recognize the type of plants and the lack of information, both cause the
   limited use of plants as medicine. In this research, an herbal plants
   identification system based on leaves pattern was developed. This
   identification system is based on the shape of the herbal plants'
   leaves. Before identification, preprocessing stages should be performed
   such as conversion to grayscale image, conversion to binary image, and
   image segmentation using Otsu's method. Feature extraction method used
   in this system is one kind of region-based invariant feature extraction,
   which is well-known as Hu's seven moments invariant and the Euclidean or
   Canberra distance as a recognition method. The research was conducted on
   15 types of herbal plants. Based on the research, the percentage of
   recognition in this identification system using Euclidean Distance
   reached 86.67\% with the lowest recognition rate is 40\% for mangkokan
   leaf. While using Canberra distance for recognizing, the percentage of
   recognition is 72\% and the lowest recognition rate is 20\% for keji
   beling leaf. The best recognition rate of 100\% for Euclidean distance
   similarity measure is reached when 9 (nine) types of leaves were
   implemented, i.e. banyan (beringin), binahong, dolar, keji-beling, laos,
   noni (mengkudu), papaya, red betel (sirih merah), and soursop (sirsak)
   leaves. When Canberra distance used, 100\% recognition rate was reached
   by 5 (five) leaves types, i.e. binahong, dolar, pecut-kuda, papaya, and
   red betel (sirih merah) leaves.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Isnanto, RR (Corresponding Author), Diponegoro Univ, Dept Comp Engn, Semarang, Indonesia.
   Isnanto, R. Rizal, Diponegoro Univ, Dept Comp Engn, Semarang, Indonesia.
   Zahra, Ajub Ajulian; Julietta, Patricia, Diponegoro Univ, Dept Elect Engn, Semarang, Indonesia.},
ISBN = {978-1-5090-0890-2},
Keywords = {Identification System; herbal plant; leaves pattern; Hu's seven moments
   invariant; Euclidean distance},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic},
Author-Email = {rizal@ce.undip.ac.id
   ayub.ayul1an@gmail.com
   ciasdbtr@gmail.com},
Affiliations = {Diponegoro University; Diponegoro University},
ResearcherID-Numbers = {Isnanto, R Rizal/ABH-8272-2020},
Funding-Acknowledgement = {Diponegoro University {[}DIPA-024.01.2.400898/2016]},
Funding-Text = {Authors thank to Diponegoro University in accordance with Letter of
   Assignment No. DIPA-024.01.2.400898/2016, dated December 7, 2015, fiscal
   year 2016 for their financial support to this research.},
Cited-References = {Bishop Ch. M., 2006, PATTERN RECOGN.
   Charulatha B.S., 2013, INT J EMERGING TREND.
   Gonzalez R.C., 2009, DIGITAL IMAGE PROCES, V14.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Jain A. K., 1989, FUNDAMENTALS DIGITAL, V14.
   Marques O., 2002, CONTENT BASED IMAGE.
   Pahalawatta K., 2008, THESIS.
   Putra D., 2004, TEKNOLOGI ELEKTRO, V3, P11.
   Vala J., 2013, INT J ADV RES COMPUT, V2, P387, DOI {[}10.1007/s11548-009-0389-8, DOI 10.1007/S11548-009-0389-8].
   Wurdianarto S.R., 2014, JURNAL TEKNOLOGI INF, V13, P31.
   Zhihu Huang, 2010, 2010 2nd International Conference on Computer Engineering and Technology (ICCET), P476, DOI 10.1109/ICCET.2010.5485542.},
Number-of-Cited-References = {11},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BH7KQ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000402613900078},
DA = {2023-08-12},
}

@inproceedings{ WOS:000342993000023,
Author = {Silva, Pedro F. B. and Marcal, Andre R. S. and Almeida da Silva, Rubim
   M.},
Editor = {Kamel, M and Campilho, A},
Title = {Evaluation of Features for Leaf Discrimination},
Booktitle = {IMAGE ANALYSIS AND RECOGNITION},
Series = {Lecture Notes in Computer Science},
Year = {2013},
Volume = {7950},
Pages = {197-204},
Note = {10th International Conference on Image Analysis and Recognition (ICIAR),
   Povoa do Varzim, PORTUGAL, JUN 26-28, 2013},
Organization = {Assoc Image \& Machine Intelligence},
Abstract = {A number of shape features for automatic plant recognition based on
   digital image processing have been proposed by Pauwels et al. in 2009. A
   database with 15 classes and 171 leaf samples was considered for the
   evaluation of these measures using linear discriminant analysis and
   hierarchical clustering. The results obtained match the human visual
   shape perception with an overall accuracy of 87\%.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Silva, PFB (Corresponding Author), Univ Porto, Fac Ciencias, Dept Matemat, Rua Campo Alegre 823, P-4100 Oporto, Portugal.
   Silva, Pedro F. B.; Marcal, Andre R. S., Univ Porto, Fac Ciencias, Dept Matemat, Rua Campo Alegre 823, P-4100 Oporto, Portugal.
   Almeida da Silva, Rubim M., Univ Porto, Fac Ciencias, Dept Biol, P-4100 Oporto, Portugal.},
ISSN = {0302-9743},
ISBN = {978-3-642-39094-4; 978-3-642-39093-7},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Computer Science, Information Systems; Computer Science,
   Theory \& Methods},
Affiliations = {Universidade do Porto; Universidade do Porto},
ResearcherID-Numbers = {Marcal, Andre/L-9850-2019
   da Silva, Rubim M. Almeida/B-8951-2011
   MARCAL, ANDRE R. S./F-6230-2013},
ORCID-Numbers = {Marcal, Andre/0000-0002-8501-0974
   da Silva, Rubim M. Almeida/0000-0003-4959-0147
   MARCAL, ANDRE R. S./0000-0002-8501-0974},
Cited-References = {{[}Anonymous], 2004, P 21 INT C MACHINE L.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Duda RO, 1973, PATTERN CLASSIFICATI.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Oonincx PJ, 2003, PATTERN RECOGN, V36, P2663, DOI 10.1016/S0031-3203(03)00167-5.
   Pauwels EJ, 2009, ENG APPL ARTIF INTEL, V22, P26, DOI 10.1016/j.engappai.2008.04.017.},
Number-of-Cited-References = {7},
Times-Cited = {59},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BB3QC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000342993000023},
DA = {2023-08-12},
}

@inproceedings{ WOS:000511106700073,
Author = {Zhang, Qi and Zeng, Shaoning and Zhang, Bob},
Editor = {Hwang, JN and Jiang, X},
Title = {Initial investigation of different classifiers for plant leaf
   classification using multiple features},
Booktitle = {ELEVENTH INTERNATIONAL CONFERENCE ON DIGITAL IMAGE PROCESSING (ICDIP
   2019)},
Series = {Proceedings of SPIE},
Year = {2019},
Volume = {11179},
Note = {11th International Conference on Digital Image Processing (ICDIP), Sun
   Yat Sen Univ, Guangzhou, PEOPLES R CHINA, MAY 10-13, 2019},
Organization = {E China Normal Univ; Int Assoc Comp Sci \& Informat Technol},
Abstract = {Plant leaf species classification is an active research area at present
   with many scientists attempting to use different classifiers with
   different leaf features to solve it. In this paper we evaluate 10 common
   classifiers: k-Nearest Neighbors (KNN), support vector machine (SVM),
   nu-SVM, decision tree, random forest, naive bayes, linear discriminant
   analysis (LDA), logistic regression, quadratic discriminant analysis
   (QDA) and sparse representation in leaf species classification with
   different leaf features such as shape, texture and margin. Besides this,
   different numbers of leaf species and training samples for different
   classifiers were also evaluated in this study. The comprehensive results
   indicate that random forest, followed by LDA, logistic regression and
   sparse representation are the most robust and accurate classifiers in
   leaf recognition using various features.},
Publisher = {SPIE-INT SOC OPTICAL ENGINEERING},
Address = {1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zhang, B (Corresponding Author), Univ Macau, Dept Comp \& Informat Sci, PAMI Res Grp, Taipa, Macau, Peoples R China.
   Zhang, Qi; Zeng, Shaoning; Zhang, Bob, Univ Macau, Dept Comp \& Informat Sci, PAMI Res Grp, Taipa, Macau, Peoples R China.},
DOI = {10.1117/12.2539654},
Article-Number = {UNSP 1117922},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-1-5106-3076-5},
Keywords = {Pattern Recognition; Leaf Classification; Leaf Features},
Keywords-Plus = {SPARSE REPRESENTATION; RECOGNITION; IDENTIFICATION},
Research-Areas = {Optics},
Web-of-Science-Categories  = {Optics},
Author-Email = {bobzhang@um.edu.mo},
Affiliations = {University of Macau},
ResearcherID-Numbers = {Zhang, Bob/ABD-5926-2021
   Zhang, Bob/HIR-3656-2022},
ORCID-Numbers = {Zhang, Bob/0000-0003-2497-9519
   Zhang, Bob/0000-0001-6512-0474},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}61602540]},
Funding-Text = {This work is supported by the National Natural Science Foundation of
   China (61602540).},
Cited-References = {{[}Anonymous], 2013, SIGNAL PROCESS PATTE.
   Arai Kohei, 2013, International Journal of Advanced Research in Artificial Intelligence, V2, P34.
   Bravo C, 2003, BIOSYST ENG, V84, P137, DOI 10.1016/S1537-5110(02)00269-6.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Casanova D, 2009, INT J IMAG SYST TECH, V19, P236, DOI 10.1002/ima.20201.
   Cerutti G, 2014, PATTERN RECOGN LETT, V49, P177, DOI 10.1016/j.patrec.2014.07.016.
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Elhariri E, 2014, 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING \& SYSTEMS (ICCES), P271, DOI 10.1109/ICCES.2014.7030971.
   Gaonkar B, 2013, NEUROIMAGE, V78, P270, DOI 10.1016/j.neuroimage.2013.03.066.
   Hastie T., 2001, ELEMENTS STAT LEARNI.
   Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601.
   Kadir A., 2015, GATE COMPUTER VISION, V1, P3, DOI DOI 10.15579/GTCVPR.0101.003007.
   Kiruba Raji I., 2019, ADV ELECTR COMPUT EN, V26, P933, DOI {[}10.1007/s11831-018-9266-3, DOI 10.1007/S11831-018-9266-3].
   Ling TW, 2014, INT CONF BIG DATA, P1, DOI 10.1109/BIGCOMP.2014.6741406.
   McLachlan G.J., 1992, DISCRIMINANT ANAL ST, DOI DOI 10.1002/0471725293.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Nakayama H, 2013, CLEF WORKING NOTES.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Padao F. R. F., 2015, HUM NAN INF TECHN CO, P1, DOI DOI 10.1109/HNICEM.2015.7393179.
   Peterson LE., 2009, SCHOLARPEDIA, V4, P1883, DOI {[}10.4249/scholarpedia.1883, DOI 10.4249/SCHOLARPEDIA.1883].
   Prasad S., 2011, P 2011 INT C COMM CO, P343, DOI {[}10.1145/1947940.1948012, DOI 10.1145/1947940.1948012].
   Rahmani M. E., 2015, ALLDATA 2015, V82.
   Rokach L., 2008, WORLD SCI.
   Russell S.J., 2016, ARTIF INTELL.
   Saimurugan M, 2011, EXPERT SYST APPL, V38, P3819, DOI 10.1016/j.eswa.2010.09.042.
   Scotland RW, 2003, TAXON, V52, P101, DOI 10.2307/3647306.
   Shabanzade M., 2011, SIGNAL IMAGE PROCESS, V2, P23, DOI DOI 10.5121/sipij.2011.2303.
   Silva PFB, 2013, LECT NOTES COMPUT SC, V7950, P197, DOI 10.1007/978-3-642-39094-4\_23.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79.
   Yang JY, 2009, SCI CHINA SER F, V52, P695, DOI 10.1007/s11432-009-0045-5.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.
   Zeng SN, 2017, COMPUT ELECTRON AGR, V142, P563, DOI 10.1016/j.compag.2017.11.013.
   Zhang SW, 2017, CLUSTER COMPUT, V20, P1517, DOI 10.1007/s10586-017-0859-7.
   Zhang SW, 2017, COMPUT ELECTRON AGR, V134, P135, DOI 10.1016/j.compag.2017.01.014.},
Number-of-Cited-References = {42},
Times-Cited = {3},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BO3NJ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000511106700073},
DA = {2023-08-12},
}

@article{ WOS:000783062500001,
Author = {Spagnuolo, Edward J. and Wilf, Peter and Serre, Thomas},
Title = {Decoding family-level features for modern and fossil leaves from
   computer-vision heat maps},
Journal = {AMERICAN JOURNAL OF BOTANY},
Year = {2022},
Volume = {109},
Number = {5},
Pages = {768-788},
Month = {MAY},
Abstract = {Premise Angiosperm leaves present a classic identification problem due
   to their morphological complexity. Computer-vision algorithms can
   identify diagnostic regions in images, and heat map outputs illustrate
   those regions for identification, providing novel insights through
   visual feedback. We investigate the potential of analyzing leaf heat
   maps to reveal novel, human-friendly botanical information with
   applications for extant- and fossil-leaf identification. Methods We
   developed a manual scoring system for hotspot locations on published
   computer-vision heat maps of cleared leaves that showed diagnostic
   regions for family identification. Heat maps of 3114 cleared leaves of
   930 genera in 14 angiosperm families were analyzed. The top-5 and top-1
   hotspot regions of highest diagnostic value were scored for 21 leaf
   locations. The resulting data were viewed using box plots and analyzed
   using cluster and principal component analyses. We manually identified
   similar features in fossil leaves to informally demonstrate potential
   fossil applications. Results The method successfully mapped machine
   strategy using standard botanical language, and distinctive patterns
   emerged for each family. Hotspots were concentrated on secondary veins
   (Salicaceae, Myrtaceae, Anacardiaceae), tooth apices (Betulaceae,
   Rosaceae), and on the little-studied margins of untoothed leaves
   (Rubiaceae, Annonaceae, Ericaceae). Similar features drove the results
   from multivariate analyses. The results echo many traditional
   observations, while also showing that most diagnostic leaf features
   remain undescribed. Conclusions Machine-derived heat maps that initially
   appear to be dominated by noise can be translated into
   human-interpretable knowledge, highlighting paths forward for botanists
   and paleobotanists to discover new diagnostic botanical characters.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Spagnuolo, EJ (Corresponding Author), Penn State Univ, Dept Geosci, University Pk, PA 16802 USA.
   Spagnuolo, EJ (Corresponding Author), Penn State Univ, Earth \& Environm Syst Inst, University Pk, PA 16802 USA.
   Spagnuolo, Edward J.; Wilf, Peter, Penn State Univ, Dept Geosci, University Pk, PA 16802 USA.
   Spagnuolo, Edward J.; Wilf, Peter, Penn State Univ, Earth \& Environm Syst Inst, University Pk, PA 16802 USA.
   Spagnuolo, Edward J., Penn State Univ, Millennium Scholars Program, University Pk, PA 16802 USA.
   Spagnuolo, Edward J., Penn State Univ, Schreyer Honors Coll, University Pk, PA 16802 USA.
   Serre, Thomas, Brown Univ, Carney Inst Brain Sci, Dept Cognit Linguist \& Psychol Sci, Providence, RI 02912 USA.},
DOI = {10.1002/ajb2.1842},
EarlyAccessDate = {APR 2022},
ISSN = {0002-9122},
EISSN = {1537-2197},
Keywords = {cleared leaves; computer vision; fossil identification; fossil leaves;
   heat maps; leaf architecture; leaf identification; leaf margin; leaf
   teeth; leaf venation},
Keywords-Plus = {GREEN RIVER FORMATION; MIDDLE EOCENE; PLANT-IDENTIFICATION;
   PALAEOCARPINUS BETULACEAE; LEAF ARCHITECTURE; ATTACHED LEAVES;
   NORTH-AMERICA; EXTINCT GENUS; FRUITS; ANACARDIACEAE},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {spagnuolo@psu.edu},
Affiliations = {Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Pennsylvania State University -
   University Park; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Pennsylvania State University; Pennsylvania State University -
   University Park; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Pennsylvania State University; Pennsylvania State University -
   University Park; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Pennsylvania State University; Pennsylvania State University -
   University Park; Brown University},
ORCID-Numbers = {Spagnuolo, Edward/0000-0001-6720-900X
   Wilf, Peter/0000-0001-6813-1937
   Serre, Thomas/0000-0003-0846-0039},
Funding-Acknowledgement = {Pennsylvania State University Millennium Scholars Program; Schreyer
   Honors College, and Presidential Leadership Academy; Pennsylvania State
   University Erickson Discovery Grant; NSF {[}DEB-1556666, EAR-1925755,
   EAR1925481]},
Funding-Text = {We thank E. Stiles and M. Patzkowsky for their advice on the statistical
   analyses, C. N. Nunez Sanchez and M. D. Feineman for manuscript
   comments, and Associate Editor A.-L. Decombeix and two anonymous
   reviewers for constructive feedback. For the preceding study (Wilf et
   al., 2016) as used here, J. Kissell and A. Young prepared the
   cleared-leaf images and vetted their taxonomy, and S. Zhang generated
   the heat maps. We are grateful to the Pennsylvania State University
   Paleobiology Seminar, Multivariate Analysis in Geosciences, and
   Geoscholarship courses for fruitful discussions and support from the
   Pennsylvania State University Millennium Scholars Program, Schreyer
   Honors College, and Presidential Leadership Academy to E.J.S. We
   acknowledge funding from the Pennsylvania State University Erickson
   Discovery Grant (to E.J.S.) and NSF Grants NSF DEB-1556666 and
   EAR-1925755 (to P.W.) and EAR1925481 (to T.S.). This research partially
   fulfilled requirements for a 2022 B.S. in Geobiology with Honors from
   the Pennsylvania State University for E.J.S.},
Cited-References = {Almeida BK, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11379.
   Anberree JL, 2015, INT J PLANT SCI, V176, P682, DOI 10.1086/682166.
   Andres-Hernandez A. R., 2009, Feddes Repertorium, V120, P293, DOI 10.1002/fedr.200911109.
   {[}Anonymous], 2011, EARLY FLOWERS ANGIOS.
   {[}Anonymous], U CALIF PUBL GEOL SC.
   Aranda M. C., 2010, P ACM INT C IM VID R, P327, DOI {[}10.1145/1816041.1816089, DOI 10.1145/1816041.1816089].
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Bacon CD, 2016, MOL PHYLOGENET EVOL, V94, P365, DOI 10.1016/j.ympev.2015.09.013.
   Bama B.S., 2011, INDIAN J COMPUTER SC, V2, P202.
   Banerjee Somnath, 2020, Proceedings of the Global AI Congress 2019. Advances in Intelligent Systems and Computing (AISC 1112), P251, DOI 10.1007/978-981-15-2188-1\_20.
   Beaman RS, 2012, ZOOKEYS, P7, DOI 10.3897/zookeys.209.3313.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Boucher LD, 2003, AM J BOT, V90, P1389, DOI 10.3732/ajb.90.9.1389.
   Bremer K, 1998, ANN MO BOT GARD, V85, P531, DOI 10.2307/2992015.
   Bryson AE, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11404.
   Byng JW, 2016, BOT J LINN SOC, V181, P1, DOI 10.1111/boj.12385.
   Carranza Rojas J., 2018, 2018 IEEE INT WORK C, P1, DOI 10.1109/IWOBI.2018.8464187.
   Carranza-Rojas J, 2018, MULTIMED SYST APPL, P151, DOI 10.1007/978-3-319-76445-0\_9.
   Carranza-Rojas J, 2017, BMC EVOL BIOL, V17, P1, DOI 10.1186/s12862-017-1014-z.
   Carvalho MR, 2011, AM J BOT, V98, P1337, DOI 10.3732/ajb.1000539.
   Champ J, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11373.
   Charters J., 2014, 2014 IEEE INT C MULT, P1, DOI {[}10.1109/ICMEW.2014.6890557, DOI 10.1109/ICMEW.2014.6890557].
   Cheng-Li Zhou, 2021, Journal of Physics: Conference Series, V1873, DOI 10.1088/1742-6596/1873/1/012002.
   Correa-Narvaez JE, 2022, BOT REV, V88, P161, DOI 10.1007/s12229-021-09258-y.
   CRANE PR, 1981, BOT J LINN SOC, V83, P103, DOI 10.1111/j.1095-8339.1981.tb01224.x.
   Crane PR, 2004, AM J BOT, V91, P1683, DOI 10.3732/ajb.91.10.1683.
   CREPET WL, 1989, AM J BOT, V76, P842, DOI 10.2307/2444540.
   CROAT TB, 1978, FLORA BARRO COLORADO.
   Cronquist A, 1981, INTEGRATED SYSTEM CL.
   Das A, 2014, PLANT METHODS, V10, DOI 10.1186/1746-4811-10-8.
   Del Rio C, 2020, AM J BOT, V107, P126, DOI 10.1002/ajb2.1418.
   DeVore ML, 2007, PLANT SYST EVOL, V266, P45, DOI 10.1007/s00606-007-0540-3.
   DeVore ML, 2004, RHODORA, V106, P197.
   Dilcher David L., 2005, Bulletin of the Florida Museum of Natural History, V45, P3.
   DILCHER DL, 1974, BOT REV, V40, P1, DOI 10.1007/BF02860067.
   Doyle JA, 2007, COUR FOR SEKENBG, V258, P21.
   Ellis B., 2009, MANUAL LEAF ARCHITEC, V1.
   Feild TS, 2011, P NATL ACAD SCI USA, V108, P8363, DOI 10.1073/pnas.1014456108.
   Gandolfo MA, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0021084.
   Gentry A., 1993, FIELD GUIDE FAMILIES.
   Goh G., 2021, DISTILL, V6, pe30, DOI 10.23915/distill.00030.
   Gouveia F, 1997, ISIE `97 - PROCEEDINGS OF THE IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS, VOLS 1-3, P757, DOI 10.1109/ISIE.1997.648634.
   Graham A, 2009, ANN MO BOT GARD, V96, P90, DOI 10.3417/2006165.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Hammer Oyvind, 2001, Palaeontologia Electronica, V4, pUnpaginated.
   Hedrick BP, 2020, BIOSCIENCE, V70, P243, DOI 10.1093/biosci/biz163.
   Herendeen PS, 2019, INT J PLANT SCI, V180, P220, DOI 10.1086/701468.
   HICKEY LJ, 1975, ANN MO BOT GARD, V62, P538, DOI 10.2307/2395267.
   Hickey LJ., 1977, GEOL SOC AM MEM, V150, P1.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Huff PM, 2003, PALAIOS, V18, P266, DOI 10.1669/0883-1351(2003)018<0266:DFFPEF>2.0.CO;2.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   Jamil N, 2015, PROCEDIA COMPUT SCI, V76, P436, DOI 10.1016/j.procs.2015.12.287.
   Joly A, 2016, MULTIMEDIA SYST, V22, P751, DOI 10.1007/s00530-015-0462-9.
   Jordan GJ, 2010, AM J BOT, V97, P59, DOI 10.3732/ajb.0900109.
   KELLER R., 2004, IDENTIFICATION TROPI.
   Kellner A, 2012, INT J PLANT SCI, V173, P239, DOI 10.1086/663965.
   Kubitzki K., 2013, FLOWERING PLANTS DIC.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lagar-Cavilla H. A., 2009, 2012 INT C DIG IM CO, P1, DOI DOI 10.1109/DICTA.2012.6411702.
   Lapuschkin S, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-08987-4.
   Larese Monica G., 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P447, DOI 10.1007/978-3-642-33275-3\_55.
   Larese MG, 2016, MACH VISION APPL, V27, P709, DOI 10.1007/s00138-015-0732-8.
   Larese MG, 2014, EXPERT SYST APPL, V41, P4638, DOI 10.1016/j.eswa.2014.01.029.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Leebens-Mack JH, 2019, NATURE, V574, P679, DOI 10.1038/s41586-019-1693-2.
   Linsley JW, 2021, SCI ADV, V7, DOI 10.1126/sciadv.abf8142.
   Little DP, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11365.
   Little SA, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0015161.
   Lu HF, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0029704.
   MANCHESTER SR, 1983, AM J BOT, V70, P1147, DOI 10.2307/2443285.
   Manchester SR, 1998, REV PALAEOBOT PALYNO, V102, P153, DOI 10.1016/S0034-6667(97)00056-0.
   Manchester SR, 2001, INT J PLANT SCI, V162, P985, DOI 10.1086/320783.
   Manchester SR, 2006, INT J PLANT SCI, V167, P897, DOI 10.1086/503918.
   Manchester Steven R, 1986, Am J Bot, V73, P156, DOI {[}10.1002/j.1537-2197.1986.tb09691.x, 10.2307/2444288].
   Marshall CR, 2018, BIOL LETTERS, V14, DOI 10.1098/rsbl.2018.0431.
   Martínez-Millán Marcela, 2005, Rev. Mex. Biodiv., V76, P137.
   Mata-Montero E., 2016, PROC IFIP WORLD INFO, P26, DOI {[}10.1007/978-3-319-44447-5\_3, DOI 10.1007/978-3-319-44447-5\_3].
   Mata-Montero E, 2015, PROC LAT AM COMPUT C, P41.
   McClain AM, 2001, AM J BOT, V88, P1316, DOI 10.2307/3558343.
   McGrath T., 2021, ARXIV211109259.
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w.
   Minowa Y., 2020, Journal of Forest Planning, V26, P1, DOI 10.20659/jfp.2020.001.
   Mitchell JD, 2015, PHYTOKEYS, V55, P1, DOI 10.3897/phytokeys.55.8489.
   Mouine S., 2012, ICMR, P1, DOI DOI 10.1145/2324796.2324853.
   Mukherjee G, 2021, SOFT COMPUT, V25, P14119, DOI 10.1007/s00500-021-06139-9.
   Nam Y, 2008, COMPUT VIS IMAGE UND, V110, P245, DOI 10.1016/j.cviu.2007.08.002.
   Olah C., 2018, DISTILL, V3, P10, DOI DOI 10.23915/DISTILL.00010.
   Owens SA, 1998, AM J BOT, V85, P273, DOI 10.2307/2446316.
   Page LM, 2015, BIOSCIENCE, V65, P841, DOI 10.1093/biosci/biv104.
   Park J, 2008, J SYST SOFTWARE, V81, P71, DOI 10.1016/j.jss.2007.05.001.
   Pigg KB, 2003, INT J PLANT SCI, V164, P807, DOI 10.1086/376816.
   Pirie MD, 2012, BOT J LINN SOC, V169, P84, DOI 10.1111/j.1095-8339.2012.01234.x.
   Pryer KM, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11372.
   Punyasena SW, 2012, NEW PHYTOL, V196, P937, DOI 10.1111/j.1469-8137.2012.04291.x.
   Ramirez JL, 2002, AM J BOT, V89, P535, DOI 10.3732/ajb.89.3.535.
   Ramirez JL, 2000, INT J PLANT SCI, V161, P509, DOI 10.1086/314261.
   Romero IC, 2020, P NATL ACAD SCI USA, V117, P28496, DOI 10.1073/pnas.2007324117.
   ROTH JL, 1979, AM J BOT, V66, P1194, DOI 10.2307/2442218.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Rzanny M, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0462-4.
   Sawangchote P., 2010, THAI FOREST B BOT, V38, P8.
   Sawangchote P, 2009, AM J BOT, V96, P2048, DOI 10.3732/ajb.0900086.
   Schuettpelz E, 2017, BIODIVERS DATA J, V5, DOI 10.3897/BDJ.5.e21139.
   Seeland M, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-018-2474-x.
   Serre T, 2019, ANNU REV VIS SCI, V5, P399, DOI 10.1146/annurev-vision-091718-014951.
   Simpson MG, 2010, PLANT SYSTEMATICS, 2ND EDITION, P275, DOI 10.1016/B978-0-12-374380-0.50008-7.
   Soepadmo E., 2000, TREE FLORA SABAH SAR, V3, P1.
   Soltis PS, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11371.
   Spitzer M, 2014, NAT METHODS, V11, P121, DOI 10.1038/nmeth.2811.
   SUN FS, 1992, INT J PLANT SCI, V153, P136, DOI 10.1086/297015.
   Tarran M, 2018, AM J BOT, V105, P1748, DOI 10.1002/ajb2.1163.
   Tcheng DK, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0148879.
   Unger J, 2016, BMC EVOL BIOL, V16, DOI 10.1186/s12862-016-0827-5.
   Unger S, 2021, J BIOL EDUC, V55, P537, DOI 10.1080/00219266.2020.1739114.
   Vizcarra G, 2021, ECOL INFORM, V62, DOI 10.1016/j.ecoinf.2021.101268.
   Voss C., 2021, DISTILL, V6.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   White AE, 2020, P NATL ACAD SCI USA, V117, P29268, DOI 10.1073/pnas.2020870117.
   Wilf P., 2008, PALEONTOL SOC PAP, V14, P319, DOI DOI 10.1017/S1089332600001741.
   Wilf P, 2021, PHYTOKEYS, P93, DOI 10.3897/phytokeys.187.72350.
   Wilf P, 2019, SCIENCE, V364, P972, DOI 10.1126/science.aaw5139.
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113.
   Wolfe J.A., 1987, USGS BULL, V1597, P1.
   Wu JY, 2014, PALAEOWORLD, V23, P370, DOI 10.1016/j.palwor.2014.10.005.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xing YW, 2016, INT J PLANT SCI, V177, P371, DOI 10.1086/685388.
   Yosinski J, 2015, UNDERSTANDING NEURAL.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {132},
Times-Cited = {0},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {9},
Journal-ISO = {Am. J. Bot.},
Doc-Delivery-Number = {1P7WM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000783062500001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000995495200065,
Author = {Joly, Alexis and Goeau, Herve and Kahl, Stefan and Picek, Lukas and
   Botella, Christophe and Marcos, Diego and Sulac, Milan and Hruz, Marek
   and Lorieul, Titouan and Moussi, Sara Si and Servajean, Maximilien and
   Kellenberger, Benjamin and Cole, Elijah and Durso, Andrew and Glotin,
   Herve and Planque, Robert and Vellinga, Willem-Pier and Klinck, Holger
   and Denton, Tom and Eggel, Ivan and Bonnet, Pierre and Muller, Henning},
Editor = {Kamps, J and Goeuriot, L and Crestani, F and Maistro, M and Joho, H and Davis, B and Gurrin, C and Kruschwitz, U and Caputo, A},
Title = {LifeCLEF 2023 Teaser: Species Identification and Prediction Challenges},
Booktitle = {ADVANCES IN INFORMATION RETRIEVAL, ECIR 2023, PT III},
Series = {Lecture Notes in Computer Science},
Year = {2023},
Volume = {13982},
Pages = {568-576},
Note = {45th European Conference on Information Retrieval (ECIR), Dublin,
   IRELAND, APR 02-06, 2023},
Organization = {Dublin City Univ; British Comp Soc, Informat Retrieval Specialist Grp},
Abstract = {Building accurate knowledge of the identity, the geographic distribution
   and the evolution of species is essential for the sustainable
   development of humanity, as well as for biodiversity conservation.
   However, the difficulty of identifying plants, animals and fungi is
   hindering the aggregation of new data and knowledge. Identifying and
   naming living organisms is almost impossible for the general public and
   is often difficult, even for professionals and naturalists. Bridging
   this gap is a key step towards enabling effective biodiversity
   monitoring systems. The LifeCLEF campaign, presented in this paper, has
   been promoting and evaluating advances in this domain since 2011. The
   2023 edition proposes five data-oriented challenges related to the
   identification and prediction of biodiversity: (i) PlantCLEF: very
   large-scale plant identification from images, (ii) BirdCLEF: bird
   species recognition in audio soundscapes, (iii) GeoLifeCLEF: remote
   sensing based prediction of species, (iv) SnakeCLEF: snake recognition
   in medically important scenarios, and (v) FungiCLEF: fungi recognition
   beyond 0-1 cost.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Joly, A (Corresponding Author), Univ Montpellier, CNRS, Inria, LIRMM, Montpellier, France.
   Joly, A (Corresponding Author), Univ Hawaii Hilo, Listening Observ Hawaiian Ecosyst, Hilo, HI USA.
   Joly, Alexis; Lorieul, Titouan, Univ Montpellier, CNRS, Inria, LIRMM, Montpellier, France.
   Goeau, Herve; Bonnet, Pierre, CIRAD, UMR AMAP, Montpellier, France.
   Glotin, Herve, Aix Marseille Univ, Univ Toulon, CNRS, LIS,DYNI Team, Marseille, France.
   Planque, Robert; Vellinga, Willem-Pier, Xeno Canto Fdn, The Hague, Netherlands.
   Eggel, Ivan; Muller, Henning, HES SO, Sierre, Switzerland.
   Kahl, Stefan; Klinck, Holger, Cornell Univ, Cornell Lab Ornithol, KLYCCB, Ithaca, NY USA.
   Servajean, Maximilien, Univ Montpellier, LIRMM, AMI, Univ Paul Valery Montpellier,CNRS, Montpellier, France.
   Cole, Elijah, Caltech, Dept Comp \& Math Sci, Pasadena, CA USA.
   Picek, Lukas; Hruz, Marek, Univ West Bohemia, Dept Cybernet, FAV, Plzen, Czech Republic.
   Durso, Andrew, Florida Gulf Coast Univ, Dept Biol Sci, Ft Myers, FL USA.
   Denton, Tom, Google LLC, San Francisco, CA USA.
   Sulac, Milan, Rossum Ai, Prague, Czech Republic.
   Joly, Alexis; Goeau, Herve; Kahl, Stefan; Picek, Lukas; Botella, Christophe; Marcos, Diego; Sulac, Milan; Hruz, Marek; Lorieul, Titouan; Servajean, Maximilien; Kellenberger, Benjamin; Cole, Elijah; Durso, Andrew; Glotin, Herve; Planque, Robert; Vellinga, Willem-Pier; Klinck, Holger; Denton, Tom; Eggel, Ivan; Bonnet, Pierre; Muller, Henning, Univ Hawaii Hilo, Listening Observ Hawaiian Ecosyst, Hilo, HI USA.
   Botella, Christophe, Stellenbosch Univ, Ctr Invas Biol, Stellenbosch, South Africa.
   Kellenberger, Benjamin, Yale Univ, Dept Ecol \& Evolutionary Biol, New Haven, CT USA.
   Marcos, Diego, Univ Montpellier, Inria, TETIS, Montpellier, France.},
DOI = {10.1007/978-3-031-28241-6\_65},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-031-28240-9; 978-3-031-28241-6},
Keywords = {Biodiversity; Machine learning; AI; Species; identification; Species
   prediction; Plant identification; Bird identification; Species
   distribution model; Snake identification; Fungi identification},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods},
Author-Email = {alexis.joly@inria.fr},
Affiliations = {Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; Inria; CIRAD; Centre National de la Recherche Scientifique
   (CNRS); Institut de Recherche pour le Developpement (IRD); Universite de
   Montpellier; Centre National de la Recherche Scientifique (CNRS);
   UDICE-French Research Universities; Aix-Marseille Universite; University
   of Applied Sciences \& Arts Western Switzerland; Cornell University;
   Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; California Institute of Technology; University of West
   Bohemia Pilsen; State University System of Florida; Florida Gulf Coast
   University; University of Hawaii System; University Hawaii Hilo;
   Stellenbosch University; Yale University; AgroParisTech; Inria;
   Universite de Montpellier},
ResearcherID-Numbers = {Durso, Andrew/D-1657-2012
   },
ORCID-Numbers = {Durso, Andrew/0000-0002-3008-7763
   Hruz, Marek/0000-0002-7851-9879
   Goeau, Herve/0000-0003-3296-3795},
Funding-Acknowledgement = {European Union's Horizon research and innovation program {[}101060639]},
Funding-Text = {This work has received funding from the European Union's Horizon
   research and innovation program under grant agreement No 101060639
   (MAMBO project).},
Cited-References = {{[}Anonymous], 2013, NIPS INT C P NEUR IN.
   Bonnet P, 2018, MULTIMED SYST APPL, P131, DOI 10.1007/978-3-319-76445-0\_8.
   Cai JH, 2007, PROCEEDINGS OF THE 2007 INTERNATIONAL CONFERENCE ON INTELLIGENT SENSORS, SENSOR NETWORKS AND INFORMATION PROCESSING, P293.
   Convention on Biodiversity, US.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Glotin H., 2013, P 1 WORKSH MACH LEAR.
   Goeau H., 2013, P 2 ACM INT WORKSH M, P23.
   Goeau H., 2011, CLEF TASK OVERVIEW 2.
   Goeau H., 2013, CLEF TASK OVERVIEW 2.
   Goeau H., 2012, CLEF TASK OVERVIEW 2.
   ImageCLEF, US.
   Joly Alexis, 2021, Experimental IR Meets Multilinguality, Multimodality, and Interaction: 12th International Conference of the CLEF Association, CLEF 2021, Proceedings. Lecture Notes in Computer Science, Information Systems and Applications, incl. Internet/Web, and HCI (12880), P371, DOI 10.1007/978-3-030-85251-1\_24.
   Joly Alexis, 2017, Experimental IR Meets Multilinguality, Multimodality, and Interaction. 8th International Conference of the CLEF Association, CLEF 2017. Proceedings: LNCS 10456, P255, DOI 10.1007/978-3-319-65813-1\_24.
   Joly Alexis, 2020, Experimental IR Meets Multilinguality, Multimodality, and Interaction. 11th International Conference of the CLEF Association, CLEF 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12260), P342, DOI 10.1007/978-3-030-58219-7\_23.
   Joly A., 2014, LNCS, P229, DOI DOI 10.1007/978-3-319-11382-1\_20.
   Joly A, 2022, LECT NOTES COMPUT SC, V13390, P257, DOI 10.1007/978-3-031-13643-6\_19.
   Joly A, 2019, LECT NOTES COMPUT SC, V11696, P387, DOI 10.1007/978-3-030-28577-7\_29.
   Joly A, 2018, LECT NOTES COMPUT SC, V11018, P247, DOI 10.1007/978-3-319-98932-7\_24.
   Joly A, 2016, LECT NOTES COMPUT SC, V9822, P286, DOI 10.1007/978-3-319-44564-9\_26.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Lee DJ, 2004, PROC SPIE, V5606, P37, DOI 10.1117/12.571789.
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   LifeCLEF, US.
   Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504.
   Towsey M, 2012, BIOACOUSTICS, V21, P107, DOI 10.1080/09524622.2011.648753.
   Trifa VM, 2008, J ACOUST SOC AM, V123, P2424, DOI 10.1121/1.2839017.
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914.
   Villon S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-67573-7.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52.},
Number-of-Cited-References = {33},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BV1RO},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000995495200065},
DA = {2023-08-12},
}

@inproceedings{ WOS:000532483200014,
Author = {Sommana, Benjaphan and Theeramunkong, Thanaruk},
Book-Group-Author = {IEEE},
Title = {Improving Plant Recognition using Hybrid features from Connectionist and
   Knowledge-Based Approaches},
Booktitle = {2018 THIRTEENTH INTERNATIONAL CONFERENCE ON KNOWLEDGE, INFORMATION AND
   CREATIVITY SUPPORT SYSTEMS (KICSS)},
Year = {2018},
Pages = {138-143},
Note = {13th International Conference on Knowledge, Information and Creativity
   Support Systems (KICSS), Pattaya, THAILAND, NOV 15-17, 2018},
Abstract = {Many connectionist approaches get promising result but lack of
   knowledge. In this paper, we proposed architecture that combined
   knowledge-based approach to improve the accuracy of plant recognition.
   Towards this, hybrid features are constructed by merging three types of
   knowledge-based features; morphological feature, texture feature and
   color feature with convolutional neural network extracted features. Our
   architecture consists of three main stages which are data
   pre-processing, feature extraction and classification. Before features
   are extracted, images will be resized and augmented in the
   pre-processing stage. To classify the species of leaf, we consider
   decision tree and artificial neural network as a classifier. We
   experiment on two datasets; Flavia and Swedish dataset. The experimental
   result shows that the proposed architecture can predict unseen images
   correctly more than existing models.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Sommana, B (Corresponding Author), Thammasat Univ, Sirindhorn Int Inst Technol, Pathum Thani, Thailand.
   Sommana, Benjaphan; Theeramunkong, Thanaruk, Thammasat Univ, Sirindhorn Int Inst Technol, Pathum Thani, Thailand.},
ISBN = {978-1-7281-0161-3},
Keywords = {Plant recognition; Leaf classification; Knowledge-based and
   Connectionist features; Decision tree; Artificial neural network},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic},
Author-Email = {benjaphansommana@gmail.com
   thanaruk@siit.tu.ac.th},
Affiliations = {Thammasat University},
Funding-Acknowledgement = {Thailand Advanced Institute of Science and Technology; National Science
   and Technology Development Agency; Tokyo Institute of Technology under
   the TAIST Tokyo Tech Program; Thammasat University, Center of Excellence
   in Intelligent Informatics, Speech and Language Technology and Service
   Innovation (CILS); Thammasat University, Intelligent Informatics and
   Service Innovation (IISI) Research Center; Thailand Research Fund
   {[}RTA6080013]; National Science and Technology Development Agency
   (NSTDA)},
Funding-Text = {This research is financially supported by Thailand Advanced Institute of
   Science and Technology, National Science and Technology Development
   Agency, and Tokyo Institute of Technology under the TAIST Tokyo Tech
   Program. In addition, it is also partially supported under the Thammasat
   University's research fund, Center of Excellence in Intelligent
   Informatics, Speech and Language Technology and Service Innovation
   (CILS), and Intelligent Informatics and Service Innovation (IISI)
   Research Center, the Thailand Research Fund under grant number
   RTA6080013, as well as the STEM work-force Fund by National Science and
   Technology Development Agency (NSTDA).},
Cited-References = {Glorot X, 2010, P 13 INT C ARTIFICIA, P249.
   Kadir A., 2011, INT J COMPUTER APPL, V29.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Mahdikhanlou K., 2014, 22 IR C EL ENG ICEE.
   Nijalingappa P., 2015, INT C APPL THEOR COM.
   on Applied International Conference, 2016, IEEE 13 INT C NETW S.
   Rao A., 2017, INT C INN MECH IND A.
   Samata A. K., 2005, J INDIAN ACAD FORENS, V27.
   Sderkvist O., 2001, THESIS.
   Shin HC, 2017, ADV COMPUT VIS PATT, P113, DOI 10.1007/978-3-319-42999-1\_8.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Tongpoo A., 2015, SE ASIAN J TROPICAL, V46.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhang C., 2015, IEEE INT C COMP INF.},
Number-of-Cited-References = {17},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BO9UI},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000532483200014},
DA = {2023-08-12},
}

@article{ WOS:000777901700003,
Author = {Utsuki-Alexander, Taku and Rios-Martinez, Jorge and Madera, Francisco A.
   and Perez-Espinosa, Humberto},
Title = {Towards an intelligent personal assistant for hearing impaired people},
Journal = {JOURNAL OF INTELLIGENT \& FUZZY SYSTEMS},
Year = {2022},
Volume = {42},
Number = {5},
Pages = {4315-4326},
Abstract = {This work has been focused on the part of the population with hearing
   impairment who owns a dog and that worries about not listening the dog
   barks, specially when a risky situation is taking place at home. A
   survey was carried out on people with deafness problems to find out
   hazard situations which they are exposed at home. A system prototype was
   developed to be integrated as a component of ambient intelligence (AmI)
   for ambient assisted living (AAL) that serves to Hearing Impaired People
   (HIP). The prototype detects dog barks and notifies users through both a
   smart mobile app and a visual feedback. It consists of a connection
   between a Raspberry Pi 3 card and a ReSpeaker Mic Array v2.0 microphone
   array; a communication module with a smartphone was implemented, which
   displays written messages or vibrations when receiving notifications.
   The cylinder-shaped device was designed by the authors and sent it to 3D
   print with a resin material. The prototype recognized the barking
   efficiently by using a machine learning model based on Support Vector
   Machine technique. The prototype was tested with deaf people which were
   satisfied with precision, signal intensity, and activation of lights.},
Publisher = {IOS PRESS},
Address = {NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Rios-Martinez, J (Corresponding Author), UADY, Fac Matemat, Tablaje Cat 13615, Merida, Yucatan, Mexico.
   Utsuki-Alexander, Taku; Rios-Martinez, Jorge; Madera, Francisco A., Univ Autonoma Yucatan, Fac Matemat, Merida, Mexico.
   Perez-Espinosa, Humberto, CICESE UT3, Tepic, Nayarit, Mexico.},
DOI = {10.3233/JIFS-219222},
ISSN = {1064-1246},
EISSN = {1875-8967},
Keywords = {Ambient intelligence; ambient assisted living; dog bark recognition;
   smart assistant device},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {jorge.rios@correo.uady.mx},
Affiliations = {Universidad Autonoma de Yucatan},
ResearcherID-Numbers = {Rios-Martinez, Jorge A/H-1139-2018},
ORCID-Numbers = {Rios-Martinez, Jorge A/0000-0003-4208-0515},
Cited-References = {Amina E, 2020, J KING SAUD UNIV-COM, V32, P1, DOI 10.1016/j.jksuci.2018.04.009.
   {[}Anonymous], 2019, librosa/librosa: 0.6.3, DOI 10.5281/ zenodo.2564164.
   Antonic M., 2021, PROC 2021 INT C SOFT, P1, DOI {[}10.23919/SoftCOM52868.2021.9559074, DOI 10.23919/SOFTCOM52868.2021.9559074].
   Barris T., 2015, THESIS U CAPE TOWN.
   Bhattacharya S., 2021, Proceeding of First Doctoral Symposium on Natural Computing Research (DSNCR 2020). Lecture Notes in Networks and Systems (LNNS 169), P33, DOI 10.1007/978-981-33-4073-2\_4.
   Chang C.-C., 2011, ACM T INTEL SYST TEC, V2, P1, DOI DOI 10.1145/1961189.1961199.
   Dombi S, 2020, MODERNGL HIGH PERFOR.
   Eyben Florian, 2013, P 21 ACM INT C MULTI, P835, DOI {[}DOI 10.1145/2502081.2502224, DOI 10.1016/J.SCHRES.2022.01.019].
   Fanzeres L.A., 2018, MOBILE SOUND RECOGNI.
   Farago T, 2010, ANIM BEHAV, V79, P917, DOI 10.1016/j.anbehav.2010.01.005.
   Giannakopoulos T, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0144610.
   Hantke S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5134, DOI 10.1109/ICASSP.2018.8461757.
   Jadoul Y, 2018, J PHONETICS, V71, P1, DOI 10.1016/j.wocn.2018.07.001.
   Kauss M., 2019, HOME BISHOPH SMARTHO.
   Lavrakas P, 2010, ENCY SURVEY RES METH, V2th.
   Mace RL, 1998, ASSIST TECHNOL, V10, P21, DOI 10.1080/10400435.1998.10131957.
   Marques G., 2021, IOT HEALTHCARE AMBIE.
   Mielke M, 2015, IEEE ENG MED BIO, P5008, DOI 10.1109/EMBC.2015.7319516.
   Molnar C, 2008, ANIM COGN, V11, P389, DOI 10.1007/s10071-007-0129-9.
   Monekosso D, 2015, IEEE INTELL SYST, V30, P2, DOI 10.1109/MIS.2015.63.
   Nanayakkara SC, 2013, HUM-COMPUT INTERACT, V28, P115, DOI 10.1080/07370024.2012.697006.
   Nielsen J., 1994, C COMPANION HUMAN FA, DOI {[}DOI 10.1145/259963.260531, 10.1145/259963.260531].
   Perez-Espinosa H., 2015, RES COMPUT SCI, V100, P63, DOI {[}10.13053/rcs-100-1-6, DOI 10.13053/RCS-100-1-6].
   Perez-Espinosa H, 2018, J INTELL FUZZY SYST, V34, P3273, DOI 10.3233/JIFS-169509.
   Pongracz P, 2010, VET J, V183, P141, DOI 10.1016/j.tvjl.2008.12.010.
   Sadri F, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1978802.1978815.
   Sakamoto S., 2013, T JPN SOC MECH ENG C, V79, P4164, DOI {[}10.1299/kikaic.79.4164, DOI 10.1299/KIKAIC.79.4164].
   Sanchez B., 2018, THESIS U CUENCA.
   Schneider M., 2020, STUDIENTEXTE SPRACHK, P117.
   Tessendorf B., 2012, THESIS ETH ZURICH.
   Traer J, 2021, COGNITION, V214, DOI 10.1016/j.cognition.2021.104627.
   Utsuki T., 2016, THESIS U AUTONOMA YU.
   Visuri A, 2019, INT J HUM-COMPUT ST, V128, P72, DOI 10.1016/j.ijhcs.2019.03.001.
   Wiehr F, 2016, UBICOMP'16 ADJUNCT: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P1578, DOI 10.1145/2968219.2968552.
   Yaganoglu M, 2018, ELECTRONICS-SWITZ, V7, DOI 10.3390/electronics7040050.
   Yaganoglu M, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7121296.},
Number-of-Cited-References = {36},
Times-Cited = {0},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {14},
Journal-ISO = {J. Intell. Fuzzy Syst.},
Doc-Delivery-Number = {0G2TB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000777901700003},
DA = {2023-08-12},
}

@article{ WOS:000498701100001,
Author = {Song, Yupeng and He, Fazhi and Zhang, Xiying},
Title = {To Identify Tree Species With Highly Similar Leaves Based on a Novel
   Attention Mechanism for CNN},
Journal = {IEEE ACCESS},
Year = {2019},
Volume = {7},
Pages = {163277-163286},
Abstract = {Image identification technology has great significance for forestry
   production and forestry management. Highly similar object identification
   tasks, such as tree species with similar leaves, are extremely
   challenging. Simply using typical Convolutional Neural Networks (CNNs)
   or simply adding more convolutional layers still performs poorly in the
   above tasks. In this paper, we present a novel attention mechanism to
   enhance the CNN for identification of tree species with highly similar
   leaves. This paper presents a highly discriminative network, namely
   attention branch based convolutional neural networks (ABCNN), to better
   distinguish the differences between leaves features. Firstly, we
   proposed a novel structure, in which an attention branch is added in all
   block layers of network besides the typical normal branch. Secondly, our
   attention branch adopts a condensation process to obtain a region of
   interest (ROI) from global information of input and designs a
   reconstruction process to amplify the features difference to focus on
   the ROI. Thirdly, we design a fusion process, which carefully combines
   the attention branch with a normal branch to improve the network
   performance in the training process. The proposed ABCNN is tested on
   special dataset of Leafsnap with highly similar tree leaves. Our
   approach achieved 91.43\% classification accuracy, which is higher than
   previous methods. Furthermore, ABCNN is also tested on general data set
   of SVHN and obtains 98.27\% classification accuracy, which is the most
   competitive when considering the lower computational resources for
   ordinary applications. Both above experiments demonstrate the
   discrimination and robustness of the proposed method.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {He, FZ (Corresponding Author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
   Song, Yupeng; He, Fazhi, Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
   Song, Yupeng; Zhang, Xiying, Northeast Forestry Univ, Sch Informat \& Comp Engn, Harbin 150040, Heilongjiang, Peoples R China.},
DOI = {10.1109/ACCESS.2019.2951607},
ISSN = {2169-3536},
Keywords = {Tree species identification; highly similar leaves; image
   classification; deep learning; convolutional neural network; attention
   mechanism},
Keywords-Plus = {DEEP LEARNING APPROACH; CLASSIFICATION; RECOGNITION; MANAGEMENT},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {fzhe@whu.edu.cn},
Affiliations = {Wuhan University; Northeast Forestry University - China},
ResearcherID-Numbers = {He, Fazhi/Q-3691-2018
   Zhang, Xi/HJH-1211-2023
   },
ORCID-Numbers = {Song, Yupeng/0000-0003-0791-6268},
Funding-Acknowledgement = {National Science Foundation of China {[}61472289]},
Funding-Text = {This work was supported in part by the National Science Foundation of
   China under Grant 61472289.},
Cited-References = {Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385.
   Angelstam P, 2011, FORESTRY, V84, P581, DOI 10.1093/forestry/cpr048.
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418.
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709.
   Chen X, 2019, MULTIMED TOOLS APPL, V78, P11173, DOI 10.1007/s11042-018-6690-1.
   Cho J, 2018, J LOSS PREVENT PROC, V56, P548, DOI 10.1016/j.jlp.2018.01.011.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Fu H., 2003, CHINESE B BOT, V21, P429.
   Gan Z, 2015, JMLR WORKSH CONF PRO, V38, P268.
   Gao J, 1999, INT J REMOTE SENS, V20, P2823, DOI 10.1080/014311699211813.
   Goyal P., 2017, CORR.
   He J. F. Yong, APPL MATH J CHIN U, DOI {[}10.1007/s11766-019-3714-1, DOI 10.1007/S11766-019-3714-1].
   He K., 2016, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2016.90.
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065.
   Hossain E, 2018, IEEE ACCESS, V6, P33285, DOI 10.1109/ACCESS.2018.2849065.
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI {[}10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372].
   Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291.
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243.
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0\_39.
   Huang LC, 2013, KSII T INTERNET INF, V7, P132, DOI 10.3837/tiis.2013.01.009.
   Jeelani H, 2018, I S BIOMED IMAGING, P357, DOI 10.1109/ISBI.2018.8363592.
   Kim BC, 2018, T KOREAN SOC MEC ENG, V42, P863, DOI 10.3795/KSME-A.2018.42.9.863.
   Kim BC, 2015, J MECH SCI TECHNOL, V29, P5289, DOI 10.1007/s12206-015-1131-9.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Li HR, 2019, APPL MATH SER B, V34, P1, DOI 10.1007/s11766-019-3706-1.
   Li K, 2019, FRONT COMPUT SCI-CHI, V13, P1116, DOI 10.1007/s11704-018-6442-4.
   Li K, 2018, J COMPUT SCI TECH-CH, V33, P223, DOI 10.1007/s11390-017-1764-5.
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI {[}10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954].
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1\_48.
   Mzoughi O, 2013, IEEE IMAGE PROC, P3967, DOI 10.1109/ICIP.2013.6738817.
   Nassif AB, 2019, IEEE ACCESS, V7, P19143, DOI 10.1109/ACCESS.2019.2896880.
   Omar DA, 2019, INT J COOP INF SYST, V28, DOI 10.1142/S021884301930002X.
   Pena JM, 2014, REMOTE SENS-BASEL, V6, P5019, DOI 10.3390/rs6065019.
   Roegiers S, 2019, INTEGR COMPUT-AID E, V26, P223, DOI 10.3233/ICA-190599.
   Sainath TN, 2015, NEURAL NETWORKS, V64, P39, DOI 10.1016/j.neunet.2014.08.005.
   Sfar AR, 2015, INT J COMPUT VISION, V111, P255, DOI 10.1007/s11263-014-0743-3.
   Soderkvist O., 2001, THESIS.
   Springenberg J. T., 2014, 14126806 ARXIV.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   ULABY FT, 1982, IEEE T GEOSCI REMOTE, V20, P42, DOI 10.1109/TGRS.1982.4307519.
   Vinayakumar R, 2019, IEEE ACCESS, V7, P41525, DOI 10.1109/ACCESS.2019.2895334.
   Wu YQ, 2018, IEEE T SERV COMPUT, V11, P341, DOI 10.1109/TSC.2015.2501981.
   Yan QA, 2016, COMPUT GRAPH FORUM, V35, P1, DOI 10.1111/cgf.12998.
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556.
   Yang L, 2018, IEEE T VIS COMPUT GR, V24, P1190, DOI 10.1109/TVCG.2017.2657766.
   Yin CL, 2017, IEEE ACCESS, V5, P21954, DOI 10.1109/ACCESS.2017.2762418.
   Yu HP, 2019, MULTIMED TOOLS APPL, V78, P11779, DOI 10.1007/s11042-018-6735-5.
   Yuval N., 2011, P NIPS WORKSH DEEP L.
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P4623, DOI 10.1109/TIP.2015.2465159.
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9.
   Zhou H, 2016, INT C INTEL HUM MACH, P103, DOI 10.1109/IHMSC.2016.144.
   Zhou ZY, 2017, IEEE ACCESS, V5, P5731, DOI 10.1109/ACCESS.2017.2658952.
   Zhu Z., 2018, P SOC PHOTO-OPT INS.
   Zhu Z, 2019, COMPUT BIOL MED, V109, P85, DOI 10.1016/j.compbiomed.2019.04.018.
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907.},
Number-of-Cited-References = {56},
Times-Cited = {7},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {11},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {JQ1GG},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000498701100001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000609555300001,
Author = {Masuzawa, H. and Miura, J.},
Title = {Image-based recognition of green perilla leaves using a deep neural
   network for robotic harvest support},
Journal = {ADVANCED ROBOTICS},
Year = {2021},
Volume = {35},
Number = {6},
Pages = {359-367},
Month = {MAR 19},
Abstract = {This paper describes a method of recognizing green perilla leaves using
   a deep neural network for harvest support in greenhouse horticulture. We
   are developing a robot for harvest support, which automates the
   selection and bundling process. In order to manipulate green perilla
   leaves correctly, the robot needs to precisely estimate their
   geometrical parameters such as width, height, and orientation. It also
   needs to detect leaves with anomalies. Therefore, we develop an
   image-based leaf recognition method, adopting deep neural network (DNN)
   techniques. To reduce computation time, we design a network for
   executing multiple tasks simultaneously, namely, segmentation and
   classification. We also developed an annotated dataset using
   conventional image processing techniques. Experimental results show the
   efficiency and effectiveness of the proposed method.},
Publisher = {TAYLOR \& FRANCIS LTD},
Address = {2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Masuzawa, H (Corresponding Author), Toyohashi Univ Technol, 1-1 Hibarigaoka,Tenpaku Cho, Toyohashi, Aichi 4418580, Japan.
   Masuzawa, H.; Miura, J., Toyohashi Univ Technol, Dept Comp Sci \& Engn, Toyohashi, Aichi, Japan.},
DOI = {10.1080/01691864.2021.1873846},
EarlyAccessDate = {JAN 2021},
ISSN = {0169-1864},
EISSN = {1568-5535},
Keywords = {Image recognition; deep neural network; harvest support robot},
Research-Areas = {Robotics},
Web-of-Science-Categories  = {Robotics},
Author-Email = {masuzawa@aisl.cs.tut.ac.jp},
Affiliations = {Toyohashi University of Technology},
ORCID-Numbers = {Miura, Jun/0000-0003-0153-2570},
Funding-Acknowledgement = {Knowledge Hub Aichi's Priority Research Project (2nd term)},
Funding-Text = {The authors would like to thank Mr. Mitsuo Tsume and Mr. Kensuke Suzuki
   of Sinfonia Technology Co. Ltd. for constructing the experimental robot.
   This work is supported by Knowledge Hub Aichi's Priority Research
   Project (2nd term).},
Cited-References = {Badrinarayanan V., ARXIV150507293.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Denman, 2018, 2018 IEEE RSJ INT C.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Girshick R. B., 2015, NEURIPS.
   He K., 2018, P IEEE INT C COMPUTE.
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI {[}arXiv:1406.4729, 10.1007/978-3-319-10578-9\_23].
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Li, 2013, BIOSYSTEMS ENG, V116.
   Li SJ, 2015, INT J COMPUT VISION, V113, P19, DOI 10.1007/s11263-014-0767-8.
   McHugh ML, 2012, BIOCHEM MEDICA, V22, P276, DOI 10.11613/bm.2012.031.
   Miura, TUT RES E NEWSLETTER.
   Miura, 2018, IEEE ICRA 2018 WORKS.
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4\_28.
   Sa I, 2018, IEEE ROBOT AUTOM LET, V3, P588, DOI 10.1109/LRA.2017.2774979.
   VANHENTEN E, AUTON ROBOT, V13.
   Wu SG., 2007, ABS07074289 CORR.
   Zhou, 2014, INT J AGR BIOL ENG, V7.},
Number-of-Cited-References = {19},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {15},
Journal-ISO = {Adv. Robot.},
Doc-Delivery-Number = {QZ3CK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000609555300001},
DA = {2023-08-12},
}

@article{ WOS:000184859600014,
Author = {Codrea, CM and Aittokallio, T and Keranen, M and Tyystjarvi, E and
   Nevalainen, OS},
Title = {Feature learning with a genetic algorithm for fluorescence
   fingerprinting of plant species},
Journal = {PATTERN RECOGNITION LETTERS},
Year = {2003},
Volume = {24},
Number = {15},
Pages = {2663-2673},
Month = {NOV},
Abstract = {Proper feature analysis facilitates recognition by focusing the process
   to those characteristics of observed data that carry the most
   significant information for the given classification task. In this paper
   we address the problem of feature selection from a different point of
   view. Instead of searching for a feature subset out of a large set of
   predefined candidate features we consider the situation where, given the
   form of the features and an algorithm for extracting them from the data,
   the optimizer tunes the feature extraction parameters to improve class
   separability. This process of feature learning will be solved by the
   means of a genetic algorithm. The optimized feature set is subsequently
   used in a neural network classifier. The performance of the feature
   learning approach is demonstrated with the problem of automatic
   identification of plant species from their fluorescence induction
   curves. The general approach should also be useful with other pattern
   recognition problems where a priori unknown characteristics are
   extracted from a large feature space. (C) 2003 Elsevier B.V. All rights
   reserved.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Codrea, CM (Corresponding Author), Turku Ctr Comp Sci, Lemminkaisenkatu 14 A, FIN-20520 Turku, Finland.
   Turku Ctr Comp Sci, FIN-20520 Turku, Finland.
   Univ Turku, Dept Informat Technol, FIN-20520 Turku, Finland.
   Univ Turku, Dept Math, FIN-20014 Turku, Finland.
   Univ Turku, Dept Biol, Lab Plant Physiol \& Mol Biol, FIN-20014 Turku, Finland.},
DOI = {10.1016/S0167-8655(03)00109-0},
ISSN = {0167-8655},
EISSN = {1872-7344},
Keywords = {feature analysis; plant identification; classification; fluorescence
   induction; genetic algorithm; neural network},
Keywords-Plus = {COMPONENT ANALYSIS},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {codrea@cs.utu.fi},
Affiliations = {University of Turku; University of Turku; University of Turku},
ResearcherID-Numbers = {Aittokallio, Tero/B-6583-2009
   Tyystjärvi, Esa/B-2360-2015
   Keränen, Mika/I-5607-2013},
ORCID-Numbers = {Aittokallio, Tero/0000-0002-0886-9769
   Tyystjärvi, Esa/0000-0001-6808-7470
   Keränen, Mika/0000-0002-1353-0698},
Cited-References = {{[}Anonymous], 1990, STAT PATTERN RECOGNI.
   Blickle T, 1996, EVOL COMPUT, V4, P361, DOI 10.1162/evco.1996.4.4.361.
   Devijver P., 1982, PATTERN RECOGN.
   HAND D, 2001, PRINCIPLE DATA MININ.
   Holland J.H., 1975, ADAPTATION NATURAL A.
   Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5.
   Kautsky H., 1931, NATURWISSENSCHAFTEN, V19, P964, DOI DOI 10.1007/BF01516164.
   Keranen Mika, 2003, Precision Agriculture, V4, P53, DOI 10.1023/A:1021863005378.
   KRAUSE GH, 1991, ANNU REV PLANT PHYS, V42, P313, DOI 10.1146/annurev.pp.42.060191.001525.
   Lazar D, 1999, BBA-BIOENERGETICS, V1412, P1, DOI 10.1016/S0005-2728(99)00047-X.
   Lutman PJW, 1999, 1999 BRIGHTON CONFERENCE: WEEDS, VOLS 1-3, P627.
   Moshou D, 2001, COMPUT ELECTRON AGR, V31, P5, DOI 10.1016/S0168-1699(00)00170-8.
   Nilsson N.J., 1998, ARTIF INTELL.
   PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9.
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467.
   SIEDLECKI W, 1989, PATTERN RECOGN LETT, V10, P335, DOI 10.1016/0167-8655(89)90037-8.
   SPATH H, 1980, CLUSTER ANAL ALGORIT.
   Tyystjarvi E, 1999, BIOPHYS J, V77, P1159, DOI 10.1016/S0006-3495(99)76967-5.
   Utku H, 2000, J FOOD ENG, V46, P211, DOI 10.1016/S0260-8774(00)00075-3.
   Zhang HB, 2002, PATTERN RECOGN, V35, P701, DOI 10.1016/S0031-3203(01)00046-2.},
Number-of-Cited-References = {20},
Times-Cited = {13},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Pattern Recognit. Lett.},
Doc-Delivery-Number = {713LN},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000184859600014},
DA = {2023-08-12},
}

@inproceedings{ WOS:000622977500001,
Author = {Barosa, Roysing and Hassen, Sayed Issamuddine Sayed and Nagowah, Leckraj},
Book-Group-Author = {IEEE},
Title = {Smart Aquaponics with Disease Detection},
Booktitle = {2019 SECOND INTERNATIONAL CONFERENCE ON NEXT GENERATION COMPUTING
   APPLICATIONS 2019 (NEXTCOMP 2019)},
Year = {2019},
Note = {2nd International Conference on Next Generation Computing Applications
   (NextComp), MAURITIUS, SEP 19-21, 2019},
Organization = {Ceridian; Infomil Mauritius; SBM; Univ Mauritius},
Abstract = {The vision of the Mauritian's Ministry of Agro-Industry and Food
   security is to further develop the agricultural sector and to promote
   the agro-industry by focusing on safety, supply, quality, innovation and
   new technologies. This research work attempts to develop a smart
   aquaponics system that combines conventional aquaculture with
   hydroponics. The system uses the Internet of Things to continuously
   sense and control the environment and to provide real-time feedback and
   alerts to the owner through a mobile application. It features a camera
   surveillance system that enables live streaming of the aquaponics setup
   and also allows for further image processing. Through the captured
   images, the system performs leaf recognition identifying the type of the
   leaf. Moreover, the system also performs disease detection that
   identifies if the leaf, and consequently the plant, is suffering from a
   specific disease. If the system detects a disease, it automatically
   generates a report which is sent to the owner through the mobile
   application. The prototype has been tested and looks very promising. We
   hope that this work paves the way towards a smart aquaponics system that
   detects, identifies and notifies the owner of possible diseases at early
   stage before their propagation.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Barosa, R (Corresponding Author), Univ Mauritius, Software \& Informat Syst, Reduit, Mauritius.
   Barosa, Roysing; Hassen, Sayed Issamuddine Sayed; Nagowah, Leckraj, Univ Mauritius, Software \& Informat Syst, Reduit, Mauritius.},
ISBN = {978-1-7281-1460-6},
Keywords = {Internet of Things; smart aquaponics; image processing; leaf
   identification; disease detection},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Interdisciplinary Applications; Computer Science,
   Theory \& Methods},
Author-Email = {roysing.barosa@umail.uom.ac.mu
   sayed.sayed1@umail.uom.ac.mu
   l.nagowah@uom.ac.mu},
Affiliations = {University of Mauritius},
Cited-References = {Adhitya RY, 2016, 2016 INTERNATIONAL SYMPOSIUM ON ELECTRONICS AND SMART DEVICES (ISESD), P168, DOI 10.1109/ISESD.2016.7886713.
   {[}Anonymous], 2015, INT J COMPUT APPL T.
   Elia E, 2015, SCI PAP-SER D-ANIM S, V58, P381.
   Gavhale Kiran R., 2014, INT C CONV TECHN 201, P1, DOI DOI 10.1109/I2CT.2014.7092035.
   Goddek S, 2015, SUSTAINABILITY-BASEL, V7, P4199, DOI 10.3390/su7044199.
   Hofmann M., 2013, RAPIDMINER DATA MINI.
   Khirade SD, 2015, 1ST INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION CONTROL AND AUTOMATION ICCUBEA 2015, P768, DOI 10.1109/ICCUBEA.2015.153.
   Ministry of Agro Industry and Food Security, 2019, OV.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Ng A. K., 2016, P NI ENG IMP AW ASEA, P24.
   Sannakki S.S., 2011, INT J COMP TECH APPL, V2, P1709, DOI DOI 10.1109/SPIN.2015.7095350.
   Shafeena T., 2016, EUROPEAN J ADV ENG T, V3, P52.
   Somerville C., 2014, FAO FISH AQUAC.
   Walter A, 2017, P NATL ACAD SCI USA, V114, P6148, DOI 10.1073/pnas.1707462114.
   Wang D., 2015, 5 INT C INF ENG MECH.},
Number-of-Cited-References = {15},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BQ8WG},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000622977500001},
DA = {2023-08-12},
}

@article{ WOS:000702923300013,
Author = {Heidary-Sharifabad, Ahmad and Zarchi, Mohsen Sardari and Zarei,
   Gholamreza},
Title = {ICPTC: Iranian commercial pistachio tree cultivars standard dataset},
Journal = {DATA IN BRIEF},
Year = {2021},
Volume = {38},
Month = {OCT},
Abstract = {This paper contains datasets related to the ``An efficient deep learning
   model for cultivar identification of a pistachio tree{''} {[}1] . There
   are about 11 species of pistachio that often have a high commercial and
   economic value in Iran and United States. The ability to identify
   pistachio tree cultivars, due to differences in the
   characteristics/traits of these species, is crucial for harvest the
   optimal yields, cost reduction, and damage prevention. For this purpose,
   identification of pistachio tree cultivars in their natural habitat is
   necessary. The cultivar identification relying on its appearance is a
   challenging vision task and can be facilitated by deep learning. The
   feasibility of applying deep learning algorithms to identify Pistachio
   tree cultivars depends on access to the appropriate relevant dataset.
   Therefore, ICPTC dataset was collected from natural habitats of
   different trees of Pistachio cultivars, in real-world conditions from
   pistachio orchard farms of Chah-Afzal region in Ardakan County, Yazd,
   Iran. This imbalanced dataset is compiled of 526 RGB color images from 4
   Pistachio tree cultivars, each cultivar 109-171 images. The tree of
   Iranian commercial pistachio cultivars, with names like Jumbo
   (Kalle-Ghuchi), Long (Ahmad-Aghaei), Round (O'hadi), and Super-long
   (Akbari) have distinctive branch expansion, leaf patterns, leaf shapes
   and colors. Imaging is performed from multiple trees for each cultivar,
   with different camera-to -target distances, viewpoints, angles, and
   natural sunlight dur-ing April and May in the spring. The collected
   images are not pre-processed, only grouped into their respective class
   (Jumbo, Long, Round, and Super long). The images in each class are
   separated by 20\% for testing, 17\% for validation, and 63\% for
   training. Test images are selected from trees different from the
   training set. Then training and validation images are randomly separated
   from the remaining images in each category.
   The ICPTC dataset is publicly and freely available at
   https://data.mendeley.com/datasets/6mmjjkpd5m/draft?a= af46a232-
   df30-4cf1- b303-6071d90ac8ad (C) 2021 Published by Elsevier Inc.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article; Data Paper},
Language = {English},
Affiliation = {Heidary-Sharifabad, A (Corresponding Author), Islamic Azad Univ, Maybod Branch, Dept Comp Engn, Maybod, Iran.
   Heidary-Sharifabad, Ahmad, Islamic Azad Univ, Maybod Branch, Dept Comp Engn, Maybod, Iran.
   Zarchi, Mohsen Sardari, Meybod Univ, Dept Comp Engn, Meybod, Iran.
   Zarei, Gholamreza, Islamic Azad Univ, Maybod Branch, Dept Agron, Maybod, Iran.},
DOI = {10.1016/j.dib.2021.107348},
EarlyAccessDate = {SEP 2021},
Article-Number = {107348},
ISSN = {2352-3409},
Keywords = {Pistachio cultivars; Vernalization; Deep learning; Image classification;
   Plant classification},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {ahmad.heidary@maybodiau.ac.ir
   sardari@meybod.ac.ir
   zareigholamreza@maybodiau.ac.ir},
Affiliations = {Islamic Azad University; Islamic Azad University},
ResearcherID-Numbers = {Zarchi, Mohsen Sardari/AAA-7240-2022
   Zarei, Gholamreza/HZJ-8437-2023
   },
ORCID-Numbers = {Zarchi, Mohsen Sardari/0000-0003-0831-3426
   , Ahmad/0000-0002-3356-4334},
Cited-References = {Akbari M., 2018, PHYSL PROTEOME ANAL.
   Andres F, 2012, NAT REV GENET, V13, P627, DOI 10.1038/nrg3291.
   Heidary-Sharifabad A, 2021, BRIT FOOD J, V123, P3592, DOI 10.1108/BFJ-12-2020-1100.
   Khoshnoudi-Nia S, 2019, J FOOD PROCESS PRES, V43, DOI 10.1111/jfpp.14121.
   Rahemi M, 2009, AGR SCI CHINA, V8, P803, DOI 10.1016/S1671-2927(08)60281-3.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Zohary M., 1952, Palestine Journal of Botany (Jerusalem Series), V5, P187.},
Number-of-Cited-References = {8},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Journal-ISO = {Data Brief},
Doc-Delivery-Number = {WA5KG},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000702923300013},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000491283900078,
Author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M. T.},
Book-Group-Author = {IEEE},
Title = {Advanced Plant Leaf Classification Through Image Enhancement and Canny
   Edge Detection},
Booktitle = {2018 7TH INTERNATIONAL CONFERENCE ON RELIABILITY, INFOCOM TECHNOLOGIES
   AND OPTIMIZATION (TRENDS AND FUTURE DIRECTIONS) (ICRITO) (ICRITO)},
Series = {International Conference on Reliability Infocom Technologies and
   Optimization Trends and Future Directions},
Year = {2018},
Pages = {518-522},
Note = {7th International Conference on Reliability, Infocom Technologies and
   Optimization - Trends and Future Directions (ICRITO), Amity Univ, Noida,
   INDIA, AUG 29-31, 2018},
Organization = {Amity Univ, Amity Inst Informat Technol; IEEE; IEEE UP Sect; Comp Soc
   India},
Abstract = {Accuracy in identification of any plant is achieved by understanding and
   extracting the plant features. Image processing techniques has gained
   interest in identifying the plants in realist and accurate manner. Among
   them, Edge detection techniques has very important role in creation of
   database for plant identification. Edge filtering and optimization
   technique to create continuous edges makes Canny edge detection widely
   popular to retrieve image characteristics. The proposed contour based
   image segmentation process filter morphological features from plant
   leaves. Detailed vein and texture extraction of plant leave using Canny
   edge detector are explained.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Thanikkal, JG (Corresponding Author), Amity Univ, Amity Sch Engn \& Technol, Noida, Uttar Pradesh, India.
   Thanikkal, Jibi G.; Dubey, Ashwani Kumar, Amity Univ, Amity Sch Engn \& Technol, Noida, Uttar Pradesh, India.
   Thomas, M. T., St Thomas Coll, Dept Bot, Trichur, Kerala, India.},
ISSN = {2469-875X},
ISBN = {978-1-5386-4692-2},
Keywords = {Plants; Image processing; Canny edge detection; Morphological features},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {jibimary@gmail.com
   dubey1ak@gmail.com
   thomastbgri@gmail.com},
Affiliations = {Amity University Noida},
ResearcherID-Numbers = {Dubey, Ashwani Kumar/ABI-1337-2020
   Thomas, M T/G-5518-2011
   Thanikkal, Jibi/ABG-9262-2021
   },
ORCID-Numbers = {Dubey, Ashwani Kumar/0000-0003-0778-9262
   Thanikkal, Jibi/0000-0002-5577-1158
   M T, Thomas/0000-0001-5952-5125},
Cited-References = {Agaian S, 2009, IEEE SYS MAN CYBERN, P3683, DOI 10.1109/ICSMC.2009.5346873.
   {[}Anonymous], 2016, INDIAN J SCI TECHNOL.
   Bankar S., 2014, INT J COMPUTER SCI A, V5.
   Corney DPA, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0042112.
   Fu H, 2006, IEE P-VIS IMAGE SIGN, V153, P881, DOI 10.1049/ip-vis:20060061.
   Fu H, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS \& SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P208.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Jibi G. T, 2017, RECENT DEV CONTROL A, P404.
   Joly A, 2014, INT WORKSH ENV MULT.
   Mhamed B., 2013, WULFENIA J, V20, P196.
   Salman A, 2017, INT C INV SYST CONTR.
   Sosa J., 2013, MATH COMPUTERS BIOL.
   Valliammal N., 2012, INT J COMPUT APPL, V44, P10, DOI DOI 10.5120/6322-8669.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Waldchen J, 2016, LANDSCH NATURSCH THU, V53, P121.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {16},
Times-Cited = {5},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BO0JF},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000491283900078},
DA = {2023-08-12},
}

@inproceedings{ WOS:000426611300042,
Author = {Manojkumar, P. and Surya, C. M. and Gopi, Varun P.},
Editor = {Bhattacharyya, S and Pan, I and Piuri, V and Panigrahi, BK and Gandhi, T and Maulik, U and Bhattacharjee, D and Mukherjee, A and Mondal, A and Das, A and Bhaumik, H},
Title = {Identification of Ayurvedic Medicinal Plants by Image Processing of Leaf
   Samples},
Booktitle = {2017 THIRD IEEE INTERNATIONAL CONFERENCE ON RESEARCH IN COMPUTATIONAL
   INTELLIGENCE AND COMMUNICATION NETWORKS (ICRCICN)},
Year = {2017},
Pages = {231-238},
Note = {3rd International Conference on Research in Computational Intelligence
   and Communication Networks (ICRCICN), Kolkata, INDIA, NOV 03-05, 2017},
Organization = {RCC Inst Informat Technol, Dept Informat Technol; Govt India, Minist Sci
   \& Technol, Dept Sci \& Technol; Sci \& Engn Res Board; IEEE; All India
   Council for Tech Educ},
Abstract = {Identification of the correct medicinal plants that goes in to the
   preparation of a medicine is very important in ayurvedic medicinal
   industry. The main features required to identify a medicinal plant is
   its leaf shape, colour and texture. Colour and texture from both sides
   of the leaf contain deterministic parameters to identify the species.
   This paper explores feature vectors from both the front and back side of
   a green leaf along with morphological features to arrive at a unique
   optimum combination of features that maximizes the identification rate.
   A database of medicinal plant leaves is created from scanned images of
   front and back side of leaves of commonly used ayurvedic medicinal
   plants. The leaves are classified based on the unique feature
   combination. Identification rates up to 99\% have been obtained when
   tested over a wide spectrum of classifiers. The above work has been
   extended to include identification by dry leaves and a combination of
   feature vectors is obtained, using which, identification rates exceeding
   94\% have been achieved.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Manojkumar, P (Corresponding Author), Govt Engn Coll Wayanad, Dept Elect \& Commun Engn, Wayanad, Kerala, India.
   Manojkumar, P.; Surya, C. M.; Gopi, Varun P., Govt Engn Coll Wayanad, Dept Elect \& Commun Engn, Wayanad, Kerala, India.},
ISBN = {978-1-5386-1931-5},
Keywords = {Plant identification; feature extraction; classification; optimization;
   morphological features; texture features},
Keywords-Plus = {CAPSULE ENDOSCOPY; PERFORMANCE},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic},
Author-Email = {manoj.arur@gmail.com
   suryacm1992@gmail.com
   varunpg@gecwyd.ac.in},
ResearcherID-Numbers = {P Gopi, Varun/S-3943-2019
   vadakala, gopi/AAF-3436-2019},
ORCID-Numbers = {P Gopi, Varun/0000-0001-5593-3949
   },
Funding-Acknowledgement = {TEQIP phase II through Government Engineering College Wayanad},
Funding-Text = {This research has been financially supported by TEQIP phase II through
   Government Engineering College Wayanad. We would also like to thank Dr.
   Pramod Irumbuzhi for his help in acquiring database and identifying
   plants.},
Cited-References = {Adler DG, 2003, HOSP PHYS, P14.
   {[}Anonymous], 2014, INT J COMPUTER APPL.
   {[}Anonymous], DIGITAL IMAGE PROCES.
   Buscaglia JM, 2008, CLIN GASTROENTEROL H, V6, P298, DOI 10.1016/j.cgh.2007.12.029.
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851.
   Fu Y., 2011, P IEEE 54 INT MIDW S, P1.
   Fu Yanan, 2014, IEEE J BIOMEDICAL HL, V18.
   Ghosh T., 2014, INF EL VIS ICIEV 201, P1.
   Ghosh Tonmoy, 2014, 3 INT C INF EL VIS I, P1.
   Iddan G, 2000, NATURE, V405, P417, DOI 10.1038/35013140.
   Li BP, 2009, IEEE T BIO-MED ENG, V56, P1032, DOI 10.1109/TBME.2008.2010526.
   Li Jie, 2012, IEEE EMBS C BIOM ENG, P826.
   Liangpunsakul S, 2003, AM J GASTROENTEROL, V98, P2676, DOI 10.1016/j.amjgastroenterol.2003.07.006.
   Lv GL, 2011, IEEE ENG MED BIO, P6643, DOI 10.1109/IEMBS.2011.6091638.
   Mathew M, 2015, 2015 2ND INTERNATIONAL CONFERENCE ON ELECTRONICS AND COMMUNICATION SYSTEMS (ICECS), P1730, DOI 10.1109/ECS.2015.7124882.
   Ramaraj M., 2014, J MED IMAGING HLTH I, V4.
   Sainju S, 2013, CAN CON EL COMP EN, P539.
   Schanda J, 2007, COLORIMETRY: UNDERSTANDING THE CIE SYSTEM, P1, DOI 10.1002/9780470175637.},
Number-of-Cited-References = {18},
Times-Cited = {6},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BJ6FC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000426611300042},
DA = {2023-08-12},
}

@article{ WOS:000085051800009,
Author = {Li, XD and Purvis, M},
Title = {Pattern recognition by an optical thin-film multilayer model},
Journal = {ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE},
Year = {1999},
Volume = {26},
Number = {1-4},
Pages = {193-213},
Abstract = {This paper describes a computational learning model inspired by the
   technology of optical thin-film multilayers from the field of optics.
   With the thicknesses of thin-film layers serving as adjustable
   ``weights{''} for the computation, the optical thin-film multilayer
   (OTFM) model is capable of approximating virtually any kind of nonlinear
   mapping. This paper describes the architecture of the model and how it
   can be used as a computational learning model. Some sample simulation
   calculations that are typical of connectionist models, including a
   pattern recognition of alphabetic characters, iris plant classification,
   and time series modelling of a gas furnace process, are given to
   demonstrate the model's learning capability.},
Publisher = {BALTZER SCI PUBL BV},
Address = {PO BOX 221, 1400 AE BUSSUM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Li, XD (Corresponding Author), Monash Univ, Gippsland Sch Comp \& Informat Technol, Churchill, Vic, Australia.
   Monash Univ, Gippsland Sch Comp \& Informat Technol, Churchill, Vic, Australia.
   Univ Otago, Dunedin, New Zealand.},
DOI = {10.1023/A:1018911012905},
ISSN = {1012-2443},
Research-Areas = {Computer Science; Mathematics},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Mathematics, Applied},
Affiliations = {Federation University Australia; Monash University; University of Otago},
ResearcherID-Numbers = {Li, Xiaodong/F-4334-2010},
ORCID-Numbers = {Li, Xiaodong/0000-0003-0346-1526},
Cited-References = {{[}Anonymous], APPL OPTICS.
   {[}Anonymous], 1935, B AM IRIS SOC, DOI DOI 10.1007/978-1-4612-5098-2-2.
   {[}Anonymous], 1981, COMPUTER AIDED TECHN.
   BORN M, 1980, PRINCIPLES OPTIC.
   BOX GEP, 1970, TIME SERIES ANAL FOR.
   CASE WE, 1983, APPL OPTICS, V22, P4111, DOI 10.1364/AO.22.004111.
   DOBROWOLSKI JA, 1990, APPL OPTICS, V29, P2876, DOI 10.1364/AO.29.002876.
   FARMER JD, 1990, PHYSICA D, V42, P153, DOI 10.1016/0167-2789(90)90072-W.
   Goldberg D. E., 1989, GENETIC ALGORITHM SE.
   HART A, 1992, J OPER RES SOC, V43, P215, DOI 10.2307/2583712.
   Hertz J., 1991, INTRO THEORY NEURAL.
   HINTON GE, 1989, ARTIF INTELL, V40, P185, DOI 10.1016/0004-3702(89)90049-0.
   LI X, 1997, THESIS U OTAGO DUNED.
   PEDRYCZ W, 1995, IEEE T SYST MAN CYB, V25, P769, DOI 10.1109/21.376490.
   PURVIS MK, 1997, ARTIF INTELL, V4, P239.
   Ripley B.D., 1993, NETWORKS CHAOS STAT, P40, DOI DOI 10.1007/978-1-4899-3099-6\_2.
   Serra R., 1990, COMPLEX SYSTEMS COGN.
   WEISS SM, 1989, P 11 INT JOINT C ART, P781.},
Number-of-Cited-References = {18},
Times-Cited = {3},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Ann. Math. Artif. Intell.},
Doc-Delivery-Number = {279NP},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000085051800009},
DA = {2023-08-12},
}

@article{ WOS:000434293500009,
Author = {Lee, Sue Han and Chan, Chee Seng and Remagnino, Paolo},
Title = {Multi-Organ Plant Classification Based on Convolutional and Recurrent
   Neural Networks},
Journal = {IEEE TRANSACTIONS ON IMAGE PROCESSING},
Year = {2018},
Volume = {27},
Number = {9},
Pages = {4287-4301},
Month = {SEP},
Abstract = {Classification of plants based on a multi-organ approach is very
   challenging. Although additional data provide more information that
   might help to disambiguate between species, the variability in shape and
   appearance in plant organs also raises the degree of complexity of the
   problem. Despite promising solutions built using deep learning enable
   representative features to be learned for plant images, the existing
   approaches focus mainly on generic features for species classification,
   disregarding the features representing plant organs. In fact, plants are
   complex living organisms sustained by a number of organ systems. In our
   approach, we introduce a hybrid generic-organ convolutional neural
   network (HGO-CNN), which takes into account both organ and generic
   information, combining them using a new feature fusion scheme for
   species classification. Next, instead of using a CNN-based method to
   operate on one image with a single organ, we extend our approach. We
   propose a new framework for plant structural learning using the
   recurrent neural network-based method. This novel approach supports
   classification based on a varying number of plant views, capturing one
   or more organs of a plant, by optimizing the contextual dependencies
   between them. We also present the qualitative results of our proposed
   models based on feature visualization techniques and show that the
   outcomes of visualizations depict our hypothesis and expectation.
   Finally, we show that by leveraging and combining the aforementioned
   techniques, our best network outperforms the state of the art on the
   PlantClef2015 benchmark. The source code and models are available at
   https://github. com/cs-chan/Deep-Plant.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Chan, CS (Corresponding Author), Univ Malaya, Fac Comp Sci \& Informat Technol, Ctr Image \& Signal Proc, Kuala Lumpur 50603, Malaysia.
   Lee, Sue Han; Chan, Chee Seng, Univ Malaya, Fac Comp Sci \& Informat Technol, Ctr Image \& Signal Proc, Kuala Lumpur 50603, Malaysia.
   Remagnino, Paolo, Kingston Univ, Fac Sci Engn \& Comp, Kingston Upon Thames KT1 2EE, Surrey, England.},
DOI = {10.1109/TIP.2018.2836321},
ISSN = {1057-7149},
EISSN = {1941-0042},
Keywords = {Plant classification; deep learning},
Keywords-Plus = {DYNAMIC MEMORY NETWORKS; RECOGNITION; ASK},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic},
Author-Email = {leesuehan@siswa.um.edu.my
   cs.chan@um.edu.my
   p.remagnino@kingston.ac.uk},
Affiliations = {Universiti Malaya; Kingston University},
ResearcherID-Numbers = {Chan, Chee Seng/B-9754-2011
   Lee, Sue Han/AAM-6250-2021
   Remagnino, Paolo/K-1829-2012},
ORCID-Numbers = {Chan, Chee Seng/0000-0001-7677-2865
   Remagnino, Paolo/0000-0002-9168-7746},
Funding-Acknowledgement = {Fundamental Research Grant Scheme through the Ministry of Education
   Malaysia {[}FP070-2015A]; University of Malaya {[}PG007-2016A]},
Funding-Text = {This work was supported in part by the Fundamental Research Grant Scheme
   through the Ministry of Education Malaysia under Grant FP070-2015A and
   in part by the Postgraduate Research Fund through the University of
   Malaya under Grant PG007-2016A.},
Cited-References = {Abadi M., 2016, TENSORFLOW LARGE SCA, DOI {[}DOI 10.1038/NN.3331, DOI 10.5555/3026877.3026899].
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314.
   BYEON W, 2015, PROC CVPR IEEE, P3547, DOI DOI 10.1109/CVPR.2015.7298977.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Champ J., 2015, P CLEF, V1391.
   Charters J., 2014, 2014 IEEE INT C MULT, P1, DOI {[}10.1109/ICMEW.2014.6890557, DOI 10.1109/ICMEW.2014.6890557].
   Choi S., 2015, P CLEF C.
   Chung J., 2014, ARXIV14123555.
   Clark A, 2013, BEHAV BRAIN SCI, V36, P181, DOI 10.1017/S0140525X12000477.
   Dimitrovski I., 2014, CLEF WORKING NOTES, P705.
   Fan H., 2016, MULTILEVEL CONTEXTUA.
   Fan JP, 2015, IEEE T IMAGE PROCESS, V24, P4172, DOI 10.1109/TIP.2015.2457337.
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953.
   Ge Z., 2015, P CLEF C.
   Ghazi M. M., 2015, P CLEF C.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Goeau H., 2014, P CLEF C SHEFF UK SE, P724.
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Ioffe S., 2015, INT C MACHINE LEARNI, DOI DOI 10.1007/S13398-014-0173-7.2.
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar A, 2016, PR MACH LEARN RES, V48.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Le T.-L., 2015, P CLEF C.
   Lee HH, 2017, IMAGE VISION COMPUT, V61, P98, DOI 10.1016/j.imavis.2017.01.013.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2017, IEEE IMAGE PROC, P4462, DOI 10.1109/ICIP.2017.8297126.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9.
   Meng F., 2015, DEEP MEMORY BASED AR.
   Meyniel F, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005260.
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090.
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101.
   Ni BB, 2016, PROC CVPR IEEE, P1020, DOI 10.1109/CVPR.2016.116.
   Paczolay D., 2014, P CLEF C.
   Ren MY, 2017, PROC CVPR IEEE, P293, DOI 10.1109/CVPR.2017.39.
   Reyes A. K., 2015, P CLEF C.
   Romera-Paredes B, 2016, LECT NOTES COMPUT SC, V9910, P312, DOI 10.1007/978-3-319-46466-4\_19.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Saitoh T, 2004, INT C PATT RECOG, P27, DOI 10.1109/ICPR.2004.1333997.
   Seeland M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0175101.
   Sfar AR, 2015, INT J COMPUT VISION, V111, P255, DOI 10.1007/s11263-014-0743-3.
   Sharma S., 2015, ARXIV151104119.
   Shuai B, 2016, PROC CVPR IEEE, P3620, DOI 10.1109/CVPR.2016.394.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Szucs G., 2014, CLEF 2014 C SHEFF GR, P763.
   Tan Y. H., 2016, P AS C COMP VIS, P101.
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579.
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515.
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640.
   Visin F., 2016, P IEEE C COMP VIS PA, P4.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang Y., 2017, SKELETON KEY IMAGE C.
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001.
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328.
   Xiong CM, 2016, PR MACH LEARN RES, V48.
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7\_28.
   Xu K, 2015, PR MACH LEARN RES, V37, P2048.
   Yan ZC, 2015, IEEE I CONF COMP VIS, P2740, DOI 10.1109/ICCV.2015.314.
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10.
   Yanikoglu B., 2014, P CLEF C.
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197.
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.},
Number-of-Cited-References = {73},
Times-Cited = {47},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {81},
Journal-ISO = {IEEE Trans. Image Process.},
Doc-Delivery-Number = {GI3TG},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000434293500009},
OA = {Green Accepted},
DA = {2023-08-12},
}

@article{ WOS:000939778200001,
Author = {Hart, Adam G. G. and Bosley, Hayley and Hooper, Chloe and Perry, Jessica
   and Sellors-Moore, Joel and Moore, Oliver and Goodenough, Anne E. E.},
Title = {Assessing the accuracy of free automated plant identification
   applications},
Journal = {PEOPLE AND NATURE},
Year = {2023},
Volume = {5},
Number = {3},
Pages = {929-937},
Month = {JUN},
Abstract = {Widely available and inexpensive mobile phone applications offer users,
   whether professional ecologists or interested amateurs, the potential
   for simple and rapid automated identification of species, without the
   need to use field guides and identification keys. The increasing
   accuracy of machine learning is well established, but it is currently
   unclear if, and under what circumstances, free-to-use mobile phone
   applications are accurate for identifying plants to species level in
   real-world field conditions. We test five popular and free
   identification applications for plants using 857 professionally
   identified images of 277 species from 204 genera. Across all
   applications, 85\% of images were identified correctly in the top five
   suggestions, and 69\% were correct with the first suggestion. Plant type
   (woody, forbs, grasses, rushes/sedges, ferns/horsetails) was a
   significant determinant of identification performance for each
   application. For some applications, image saliency was also important;
   exposure and focus were not significant. Applications performed well,
   with at least one of the three best-performing applications identifying
   96\% of images correctly as their first suggestion. We conclude that,
   subject to some caveats, free phone-based plant identification
   applications are valid and useful tools for those wanting rapid
   identification and for anyone wanting to engage with the natural world.
   Read the free Plain Language Summary for this article on the Journal
   blog.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Hart, AG (Corresponding Author), Univ Gloucestershire, Dept Nat \& Social Sci, Cheltenham, England.
   Hart, Adam G. G.; Bosley, Hayley; Hooper, Chloe; Perry, Jessica; Sellors-Moore, Joel; Goodenough, Anne E. E., Univ Gloucestershire, Dept Nat \& Social Sci, Cheltenham, England.
   Moore, Oliver, Taylor Wildlife, Ballindalloch, Scotland.},
DOI = {10.1002/pan3.10460},
EarlyAccessDate = {FEB 2023},
EISSN = {2575-8314},
Keywords = {automated identification; Google Lens; iNaturalist Seek; LeafSnap; plant
   identification; PlantNet; PlantSnap; species ID},
Research-Areas = {Biodiversity \& Conservation; Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Biodiversity Conservation; Ecology},
Author-Email = {ahart@glos.ac.uk},
Affiliations = {University of Gloucestershire},
ORCID-Numbers = {/0000-0002-4795-9986},
Cited-References = {Akaike H., 1973, 2 INT S INF THEOR, P267, DOI DOI 10.1007/978-1-4612-1694-0\_15.
   Anderson DR, 2002, J WILDLIFE MANAGE, V66, P912, DOI 10.2307/3803155.
   {[}Anonymous], 2021, GOOGLE TRENDS.
   Associated Press, 2020, BIRD WATCH TAK FLIGH.
   August TA, 2020, PATTERNS, V1, DOI 10.1016/j.patter.2020.100116.
   Bevis G., 2020, BBC NEWS        0525.
   Bonnet P, 2018, MULTIMED SYST APPL, P131, DOI 10.1007/978-3-319-76445-0\_8.
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3\_30.
   Dunker S, 2021, NEW PHYTOL, V229, P593, DOI 10.1111/nph.16882.
   Eurekalert, 2021, BIRD CALL APP DOWNL.
   Field A., 2000, DISCOVERING STAT.
   Goeau H., 2018, CLEF C LABS EVALUATI.
   Goodenough A. E., 2017, APPL ECOLOGY MONITOR.
   Grima N, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0243344.
   Hu MC, 2011, AM J DRUG ALCOHOL AB, V37, P367, DOI 10.3109/00952990.2011.597280.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Jones HG, 2020, AOB PLANTS, V12, DOI 10.1093/aobpla/plaa052.
   Kaur S., 2019, J MULTIMEDIA INFORM, V6, P49, DOI DOI 10.33851/JMIS.2019.6.2.49.
   Mader P, 2021, METHODS ECOL EVOL, V12, P1335, DOI 10.1111/2041-210X.13611.
   Myers R., 1990, CLASSICAL MODERN REG.
   Partel J, 2021, AOB PLANTS, V13, DOI 10.1093/aobpla/plab050.
   Rehorek SJ, 2018, AM BIOL TEACH, V80, P446, DOI 10.1525/abt.2018.80.6.446.
   Robinson CV, 2021, ACTA CHIROPTEROL, V23, P247, DOI 10.3161/15081109ACC2021.23.1.021.
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707.
   Schussler EE, 2008, J BIOL EDUC, V42, P112, DOI 10.1080/00219266.2008.9656123.
   Soga M, 2021, PEOPLE NAT, V3, P518, DOI 10.1002/pan3.10201.
   Stace C. A., 2019, NEW FLORA BRIT ISLES, V4.
   Stewart D., 2020, 1252 NATURESCOT RES.
   Tree Isabella, 2020, GUARDIAN 1228.
   Venter ZS, 2020, ENVIRON RES LETT, V15, DOI 10.1088/1748-9326/abb396.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.},
Number-of-Cited-References = {31},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {3},
Journal-ISO = {People Nat.},
Doc-Delivery-Number = {J0ZI8},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000939778200001},
OA = {Green Accepted},
DA = {2023-08-12},
}

@article{ WOS:000842539600001,
Author = {Twum, Frimpong and Missah, Yaw Marfo and Oppong, Stephen Opoku and
   Ussiph, Najim},
Title = {Textural Analysis for Medicinal Plants Identification Using Log Gabor
   Filters},
Journal = {IEEE ACCESS},
Year = {2022},
Volume = {10},
Pages = {83204-83220},
Abstract = {Texture plays a crucial role in computer vision, providing valuable
   information about image regions. Log Gabor filters that mimic the human
   eye's visual cortex are used as feature extractors to identify medicinal
   plants based on the leaf textural features. This method was tested on a
   dataset developed from the Centre of Plant Medicine Research, Ghana,
   consisting of forty-nine (49) plant species as well as the Flavia and
   Swedish Leaf datasets, which are benchmark datasets. The Log Gabor
   filter outperformed the Gabor filters, which have been extensively used
   in this area when tested on nine supervised classifiers (K-Nearest
   Neighbour, Support Vector Machine, Naive Bayes, Logistic Regression,
   Decision tree, Random Forest, Multilayer Perceptron, Gradient Boosting
   and Stochastic Gradient Descent) with 10-fold cross-validation. The
   Support Vector Machine and Multilayer Perceptron were the best
   performing classifiers for both Log Gabor filter and Gabor filter in
   terms of accuracy, precision, true positive rate, F1 score and false
   positive rate. The Log Gabor filter's highest accuracy was 79\% for
   Mydatastet, 97\% for Flavia, and 98\% for the Swedish Leaf dataset
   whiles the Gabor filter's highest accuracy was 66\% for Mydatastet, 92\%
   for Flavia and 96\% for the Swedish Leaf dataset.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Oppong, SO (Corresponding Author), Univ Educ, Dept ICT Educ, Winneba, Ghana.
   Twum, Frimpong; Missah, Yaw Marfo; Ussiph, Najim, Kwame Nkrumah Univ Sci \& Technol, Dept Comp Sci, Kumasi, Ghana.
   Oppong, Stephen Opoku, Univ Educ, Dept ICT Educ, Winneba, Ghana.},
DOI = {10.1109/ACCESS.2022.3196788},
ISSN = {2169-3536},
Keywords = {Gabor filters; Feature extraction; Filter banks; Computational modeling;
   Visualization; Biomedical imaging; Shape; Texture; Gabor filter; log
   Gabor filter; medicinal plants; supervised classifiers},
Keywords-Plus = {FEATURES; CLASSIFICATION; ALGORITHM; LEAVES},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {sooppong@uew.edu.gh},
Affiliations = {Kwame Nkrumah University Science \& Technology},
ORCID-Numbers = {Oppong, Stephen Opoku/0000-0002-1024-5680},
Cited-References = {Abellan J, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19060247.
   Alamoudi S, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207386.
   Arbab-Zavar B, 2008, INT C PATT RECOG, P2735.
   Awad M., 2015, EFFICIENT LEARNING M, P39, DOI {[}DOI 10.1007/978-1-4302-5990-9\_3, 10.1007/978-1-4302-5990-9\_3].
   Backes AR, 2009, INT J PATTERN RECOGN, V23, P1145, DOI 10.1142/S0218001409007508.
   Bianconi F, 2007, PATTERN RECOGN, V40, P3325, DOI 10.1016/j.patcog.2007.04.023.
   Bianconi F, 2021, J IMAGING, V7, DOI 10.3390/jimaging7110245.
   Boateng EY., 2020, J DATA ANAL INFORM P, V08, P341, DOI {[}10.4236/jdaip.2020.84020, DOI 10.4236/JDAIP.2020.84020].
   Casanova D, 2009, INT J IMAG SYST TECH, V19, P236, DOI 10.1002/ima.20201.
   Chaki J., 2020, MODEL BASED TEXTURE, P67, DOI {[}10.1007/978-981-15-0853-0\_5, DOI 10.1007/978-981-15-0853-0\_5].
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Christenhusz MJM, 2016, PHYTOTAXA, V261, P201, DOI 10.11646/phytotaxa.261.3.1.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Dash Sonali, 2017, Journal of Electrical Systems and Information Technology, V4, P185, DOI 10.1016/j.jesit.2016.10.001.
   de Luna R. G., 2017, PROC IEEE 9 INT C HU, P1, DOI {[}10.1109/HNICEM.2017.8269470, DOI 10.1109/HNICEM.2017.8269470].
   Di Cataldo S, 2017, COMPUT STRUCT BIOTEC, V15, P56, DOI 10.1016/j.csbj.2016.11.002.
   Ershad S.-F., 2019, INT ONLINE J IMAGE P, V2, P1, DOI DOI 10.48550/ARKXIV.1904.06554.
   Espinosa R, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23101261.
   Gabor D., 1946, J I ELECT ENGR, V93, P429, DOI {[}DOI 10.1049/JI-3-2.1946.0074, 10.1049/ji-3-2.1946.0074].
   Hubel DH., 2004, BRAIN VISUAL PERCEPT, DOI {[}10.1093/acprof:oso/9780195176186.001.0001, DOI 10.1093/ACPROF:OSO/9780195176186.001.0001].
   Humeau-Heurtier A, 2019, IEEE ACCESS, V7, P8975, DOI 10.1109/ACCESS.2018.2890743.
   Kanagalakshmi K., 2012, EUR J SCI RES, V74, P563.
   Karaca Y., 2018, COMPUTATIONAL METHOD, P229, DOI DOI 10.1515/9783110496369-007.
   Kaur H, 2017, MULTIMED TOOLS APPL, V76, P4673, DOI 10.1007/s11042-016-3652-3.
   Kaur P.P., 2020, SSRN ELECT J, DOI {[}10.2139/ssrn.3565850, DOI 10.2139/SSRN.3565850].
   Kaur S., 2019, J MULTIMEDIA INFORM, V6, P49, DOI DOI 10.33851/JMIS.2019.6.2.49.
   Keyvanpour MR, 2021, INT J COMPUT APPL T, V65, P118, DOI 10.1504/IJCAT.2021.114990.
   Kiranyaz S, 2021, MECH SYST SIGNAL PR, V151, DOI 10.1016/j.ymssp.2020.107398.
   Kong AWK, 2009, LECT NOTES COMPUT SC, V5627, P64, DOI 10.1007/978-3-642-02611-9\_7.
   Kovesi P., 1999, Videre, V1.
   Lee S. H., 2016, PROC CEUR WORKSHOP, V1609, P502.
   Lin FY, 2008, COMM COM INF SC, V15, P432.
   Liu J, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173614.
   Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z.
   Loni P., 2020, INT J RECENT ADV ENG, V8, P18, DOI {[}10.46564/ijraet.2020.v08i02.003, DOI 10.46564/IJRAET.2020.V08I02.003].
   Marrelli M, 2021, PLANTS-BASEL, V10, DOI 10.3390/plants10071355.
   Munawar HS, 2021, BUILDINGS-BASEL, V11, DOI 10.3390/buildings11070302.
   Nava R., 2014, ELECT LETT COMPUT VI, V13, P40, DOI {[}10.5565/rev/elcvia.586, DOI 10.5565/REV/ELCVIA.586].
   Nava R, 2011, INT SYMP IMAGE SIG, P189.
   Obaidullah Sk Md, 2015, International Journal of Image, Graphics and Signal Processing, V7, P49, DOI 10.5815/ijigsp.2015.05.06.
   Oppong S. O., 2021, J COMPUT SCI-NETH, V17, P1210, DOI {[}10.3844/jcssp.2021.1210.1221, DOI 10.3844/JCSSP.2021.1210.1221].
   Oppong S. O., 2021, PROC INT C ELECT COM, P1, DOI {[}10.1109/ICECET52533.2021.9698460, DOI 10.1109/ICECET52533.2021.9698460].
   Oppong S.O., 2022, GHANAIAN LEAF DATASE.
   Pang H, 2017, SAUDI J BIOL SCI, V24, P514, DOI 10.1016/j.sjbs.2017.01.021.
   Panigrahi R, 2021, MATHEMATICS-BASEL, V9, DOI 10.3390/math9060690.
   Patil A. A, 2016, INT J ENG TRENDS TEC, V37, P140, DOI {[}10.14445/22315381/ijett-v37p222, DOI 10.14445/22315381/IJETT-V57P208].
   Plodpradista P., 2021, THESIS U MISSOURI CO.
   Praveena DM, 2022, IETE J RES, V68, P3030, DOI 10.1080/03772063.2020.1749143.
   Pushpanathan K, 2021, ARTIF INTELL REV, V54, P305, DOI 10.1007/s10462-020-09847-0.
   Rai M., 2020, 2020 INT C COMP SCI, P1560, DOI {[}10.1109/CSCI51800.2020.00289, DOI 10.1109/CSCI51800.2020.00289].
   Ramola A, 2020, ENG REP, V2, DOI 10.1002/eng2.12149.
   Ratul M. A. R., 2017, PROC ACM INT C, P187, DOI {[}10.1145/3036331.3036352, DOI 10.1145/3036331.3036352].
   Refaeilzadeh P., 2020, ENCY DATABASE SYSTEM, V5, P532, DOI {[}10.1007/978-1-4899-7993-3, DOI 10.1007/978-1-4899-7993-3].
   Rose N, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P346.
   Salmeron-Manzano E, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17103376.
   Sarker IH, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0219-y.
   Sawayama M, 2017, J VISION, V17, DOI 10.1167/17.4.8.
   Schonlau M, 2020, STATA J, V20, P3, DOI 10.1177/1536867X20909688.
   Seif A., 2006, PROC 46 MIDWEST S CI, P333, DOI {[}10.1109/mwscas.2003.1562286, DOI 10.1109/MWSCAS.2003.1562286].
   Sliti O., 2014, P 3 INT C PATT REC A, P687, DOI {[}10.5220/0004829306870694, DOI 10.5220/0004829306870694].
   Soderkvist O., 2001, THESIS.
   Sudhakar T, 2021, ACM TRANS MANAG INF, V12, DOI 10.1145/3389683.
   Sun SL, 2020, IEEE T CYBERNETICS, V50, P3668, DOI 10.1109/TCYB.2019.2950779.
   Tharwat A., 2018, APPL COMPUTING INFOR, P1, DOI DOI 10.1016/J.ACI.2018.08.003.
   Tong L, 2016, NEUROCOMPUTING, V173, P1386, DOI 10.1016/j.neucom.2015.09.011.
   Uddin S, 2019, BMC MED INFORM DECIS, V19, DOI 10.1186/s12911-019-1004-8.
   Venkatesh S. K., 2011, Proceedings of the 2011 Third National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG 2011), P211, DOI 10.1109/NCVPRIPG.2011.52.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang X, 2020, IEEE ACCESS, V8, P39175, DOI 10.1109/ACCESS.2020.2976117.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.
   Yao P, 2006, INT C PATT RECOG, P461.
   Yigit E, 2019, COMPUT ELECTRON AGR, V156, P369, DOI 10.1016/j.compag.2018.11.036.
   Yuan Y, 2020, NEUROCOMPUTING, V378, P387, DOI 10.1016/j.neucom.2019.10.083.
   Zhang CS, 2019, NEURAL PROCESS LETT, V50, P957, DOI 10.1007/s11063-019-09999-3.
   Zhang Y. J, 2021, HDB IMAGE ENG, P55, DOI 10.1007/978-981-15-5873-3\_2.
   Zhang YN, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10155177.},
Number-of-Cited-References = {77},
Times-Cited = {2},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {7},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {3W7PD},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000842539600001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:001014716100001,
Author = {Sun, Xiaobo and Xu, Lin and Zhou, Yufeng and Shi, Yongjun},
Title = {Leaves and Twigs Image Recognition Based on Deep Learning and Combined
   Classifier Algorithms},
Journal = {FORESTS},
Year = {2023},
Volume = {14},
Number = {6},
Month = {JUN},
Abstract = {In recent years, the automatic recognition of tree species based on
   images taken by digital cameras has been widely applied. However, many
   problems still exist, such as insufficient tree species image
   acquisition, uneven distribution of image categories, and low
   recognition accuracy. Tree leaves can be used to differentiate and
   classify tree species due to their cognitive signatures in color, vein
   texture, shape contour, and edge serration. Moreover, the way the leaves
   are arranged on the twigs has strong characteristics. In this study, we
   first built an image dataset of 21 tree species based on the features of
   the twigs and leaves. The tree species feature dataset was divided into
   the training set and test set, with a ratio of 8:2. Feature extraction
   was performed after training the convolutional neural network (CNN)
   using the k-fold cross-validation (K-Fold-CV) method, and tree species
   classification was performed with classifiers. To improve the accuracy
   of tree species identification, we combined three improved CNN models
   with three classifiers. Evaluation indicators show that the overall
   accuracy of the designed composite model was 1.76\% to 9.57\% higher
   than other CNN models. Furthermore, in the MixNet XL CNN model, combined
   with the K-nearest neighbors (KNN) classifier, the highest overall
   accuracy rate was obtained at 99.86\%. In the experiment, the Grad-CAM
   heatmap was used to analyze the distribution of feature regions that
   play a key role in classification decisions. Observation of the Grad-CAM
   heatmap illustrated that the main observation area of SE-ResNet50 was
   the most accurately positioned, and was mainly concentrated in the
   interior of small twigs and leaflets. Our research showed that modifying
   the training method and classification module of the CNN model and
   combining it with traditional classifiers to form a composite model can
   effectively improve the accuracy of tree species recognition.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Shi, YJ (Corresponding Author), Zhejiang A\&F Univ, State Key Lab Subtrop Silviculture, Hangzhou 311300, Peoples R China.
   Shi, YJ (Corresponding Author), Zhejiang A\&F Univ, Key Lab Carbon Cycling Forest Ecosyst \& Carbon Seq, Hangzhou 311300, Peoples R China.
   Shi, YJ (Corresponding Author), Zhejiang A\&F Univ, Sch Environm \& Resources Sci, Hangzhou 311300, Peoples R China.
   Sun, Xiaobo; Xu, Lin; Zhou, Yufeng; Shi, Yongjun, Zhejiang A\&F Univ, State Key Lab Subtrop Silviculture, Hangzhou 311300, Peoples R China.
   Sun, Xiaobo; Xu, Lin; Zhou, Yufeng; Shi, Yongjun, Zhejiang A\&F Univ, Key Lab Carbon Cycling Forest Ecosyst \& Carbon Seq, Hangzhou 311300, Peoples R China.
   Sun, Xiaobo; Xu, Lin; Zhou, Yufeng; Shi, Yongjun, Zhejiang A\&F Univ, Sch Environm \& Resources Sci, Hangzhou 311300, Peoples R China.},
DOI = {10.3390/f14061083},
Article-Number = {1083},
EISSN = {1999-4907},
Keywords = {tree species recognition; support vector machine (SVM); attention
   mechanism; convolutional neural network (CNN)},
Keywords-Plus = {VALIDATION},
Research-Areas = {Forestry},
Web-of-Science-Categories  = {Forestry},
Author-Email = {19940009@zafu.edu.cn},
Affiliations = {Zhejiang A\&F University; Zhejiang A\&F University; Zhejiang A\&F
   University},
Funding-Acknowledgement = {Key Research and Development Program of Zhejiang Province
   {[}2023C02003]; National Natural Science Foundation of China
   {[}32001315, U1809208, 31870618]; Scientific Research Development Fund
   of Zhejiang Aamp;F University {[}2020FR008]},
Funding-Text = {This research was funded by the Key Research and Development Program of
   Zhejiang Province (Grant number: 2023C02003); the National Natural
   Science Foundation of China (Grant number: 32001315; U1809208;
   31870618); and the Scientific Research Development Fund of Zhejiang A \&
   F University (Grant number: 2020FR008).},
Cited-References = {Abdullah D.M., 2021, QUBAHAN ACAD J, V1, P81.
   Agarap A. F., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1803.08375.
   Albawi S., 2017, 2017 INT C ENG TECHN, P1, DOI {[}10.1109/ICEngTechnol.2017.8308186, DOI 10.1109/ICENGTECHNOL.2017.8308186].
   Alzubaidi L, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00444-8.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Carpentier M, 2018, IEEE INT C INT ROBOT, P1075, DOI 10.1109/IROS.2018.8593514.
   Chandra P, 2004, NEUROCOMPUTING, V61, P429, DOI 10.1016/j.neucom.2004.04.001.
   Chauhan VK, 2019, ARTIF INTELL REV, V52, P803, DOI 10.1007/s10462-018-9614-6.
   Chen ZX, 2017, INTERSPEECH, P102, DOI 10.21437/Interspeech.2017-1085.
   Davis J., 2006, P 23 INT C MACH LEAR, VVolume 148, P233, DOI {[}10.1145/1143844.1143874, DOI 10.1145/1143844.1143874].
   de Geus AR, 2021, WOOD SCI TECHNOL, V55, P857, DOI 10.1007/s00226-021-01282-w.
   Di Ruberto C, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS (VISAPP), VOL 1, P601.
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013.
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010.
   {[}冯海林 Feng Hailin], 2019, {[}农业机械学报, Transactions of the Chinese Society for Agricultural Machinery], V50, P235.
   Glas AS, 2003, J CLIN EPIDEMIOL, V56, P1129, DOI 10.1016/S0895-4356(03)00177-X.
   Gogul I, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION AND NETWORKING (ICSCN).
   Guo GD, 2003, LECT NOTES COMPUT SC, V2888, P986.
   Guo Q, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14163885.
   He FX, 2020, IEEE T NEUR NET LEAR, V31, P5349, DOI 10.1109/TNNLS.2020.2966319.
   He J, 2022, FORESTS, V13, DOI 10.3390/f13091350.
   Homan D, 2021, ECOL INFORM, V66, DOI 10.1016/j.ecoinf.2021.101475.
   Hu J., P IEEE CVF C COMP VI, P7132, DOI DOI 10.1109/CVPR.2018.00745.
   Huang HZ, 2023, FORESTS, V14, DOI 10.3390/f14030549.
   Iwata T, 2013, PROC SICE ANN CONF, P2489.
   Jang E, 2017, Arxiv, DOI arXiv:1611.01144.
   Jung Y, 2018, J NONPARAMETR STAT, V30, P197, DOI 10.1080/10485252.2017.1404598.
   Kattenborn T, 2021, ISPRS J PHOTOGRAMM, V173, P24, DOI 10.1016/j.isprsjprs.2020.12.010.
   Kim TH, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-06562-4.
   Lee J, 2016, IEEE J-STARS, V9, P2554, DOI 10.1109/JSTARS.2016.2569408.
   Lim K, 2003, PROG PHYS GEOG, V27, P88, DOI 10.1191/0309133303pp360ra.
   Lima E, 2017, IEEE GEOSCI REMOTE S, V14, P354, DOI 10.1109/LGRS.2016.2643000.
   Martinez Manuel, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P628, DOI 10.1007/978-3-030-12939-2\_43.
   Mayumi Oshiro Thais, 2012, Machine Learning and Data Mining in Pattern Recognition. Proceedings 8th International Conference, MLDM 2012, P154, DOI 10.1007/978-3-642-31537-4\_13.
   Pal M, 2005, INT J REMOTE SENS, V26, P217, DOI 10.1080/01431160412331269698.
   Parvin H, 2009, AIP CONF PROC, V1127, P153, DOI 10.1063/1.3146187.
   Peterson LE., 2009, SCHOLARPEDIA, V4, P1883, DOI {[}10.4249/scholarpedia.1883, DOI 10.4249/SCHOLARPEDIA.1883].
   Ramachandran P, 2017, Arxiv, DOI arXiv:1710.05941.
   Refaeilzadeh P., 2009, ENCY DATABASE SYST, V5, P532, DOI {[}DOI 10.1007/978-1-4899-7993-3\_565-2, 10.1007/978-0-387-39940-9\_565].
   Selvaraju R.R., 2016, ARXIV, DOI DOI 10.48550/ARXIV.1611.07450.
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74.
   Somers B, 2014, INT J APPL EARTH OBS, V31, P57, DOI 10.1016/j.jag.2014.02.006.
   Song Y, 2021, IEEE ACM T COMPUT BI, V18, P2775, DOI 10.1109/TCBB.2021.3065361.
   Speiser JL, 2019, EXPERT SYST APPL, V134, P93, DOI 10.1016/j.eswa.2019.05.028.
   Sugiarto B, 2017, 2017 2ND INTERNATIONAL CONFERENCES ON INFORMATION TECHNOLOGY, INFORMATION SYSTEMS AND ELECTRICAL ENGINEERING (ICITISEE), P337, DOI 10.1109/ICITISEE.2017.8285523.
   Tan MX, 2019, Arxiv, DOI {[}arXiv:1907.09595, DOI 10.48550/ARXIV.1907.09595].
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang YX, 2021, INT J ELEC POWER, V125, DOI 10.1016/j.ijepes.2020.106484.
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6.
   Yahiaoui I., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P254, DOI 10.1109/ICME.2012.130.
   Yan SJ, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13030479.
   Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738.
   Zhao ZQ, 2015, NEUROCOMPUTING, V151, P1112, DOI 10.1016/j.neucom.2014.02.077.
   Zhu M, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11167639.
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555.},
Number-of-Cited-References = {58},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Forests},
Doc-Delivery-Number = {K2IA2},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:001014716100001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:001037808300001,
Author = {Shah, Sabab Ali and Lakho, Ghulam Mustafa and Keerio, Hareef Ahmed and
   Sattar, Muhammad Nouman and Hussain, Gulzar and Mehdi, Mujahid and
   Vistro, Rahim Bux and Mahmoud, Eman A. and Elansary, Hosam O.},
Title = {Application of Drone Surveillance for Advance Agriculture Monitoring by
   Android Application Using Convolution Neural Network},
Journal = {AGRONOMY-BASEL},
Year = {2023},
Volume = {13},
Number = {7},
Month = {JUL},
Abstract = {Plant diseases are a significant threat to global food security,
   impacting crop yields and economic growth. Accurate identification of
   plant diseases is crucial to minimize crop loses and optimize plant
   health. Traditionally, plant classification is performed manually,
   relying on the expertise of the classifier. However, recent advancements
   in deep learning techniques have enabled the creation of efficient crop
   classification systems using computer technology. In this context, this
   paper proposes an automatic plant identification process based on a
   synthetic neural network with the ability to detect images of plant
   leaves. The trained model EfficientNet-B3 was used to achieve a high
   success rate of 98.80\% in identifying the corresponding combination of
   plant and disease. To make the system user-friendly, an Android
   application and website were developed, which allowed farmers and users
   to easily detect diseases from the leaves. In addition, the paper
   discusses the transfer method for studying various plant diseases, and
   images were captured using a drone or a smartphone camera. The ultimate
   goal is to create a user-friendly leaf disease product that can work
   with mobile and drone cameras. The proposed system provides a powerful
   tool for rapid and efficient plant disease identification, which can aid
   farmers of all levels of experience in making informed decisions about
   the use of chemical pesticides and optimizing plant health.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Shah, SA (Corresponding Author), Hanyang Univ, Res Inst Engn \& Technol, Ansan 15588, South Korea.
   Shah, SA (Corresponding Author), Aror Univ Art Architecture Design \& Heritage, Fac Architecture \& Town Planning, Sukkur 6500, Pakistan.
   Elansary, HO (Corresponding Author), King Saud Univ, Coll Food Agr Sci, Dept Plant Prod, POB 2460, Riyadh 11451, Saudi Arabia.
   Shah, Sabab Ali, Hanyang Univ, Res Inst Engn \& Technol, Ansan 15588, South Korea.
   Shah, Sabab Ali; Hussain, Gulzar, Aror Univ Art Architecture Design \& Heritage, Fac Architecture \& Town Planning, Sukkur 6500, Pakistan.
   Lakho, Ghulam Mustafa, Sun Moon Univ, Dept Comp Engn, Asan 31461, South Korea.
   Keerio, Hareef Ahmed, Quaid Eawam Univ Engn Sci \& Technol, Dept Environm Engn, Nawabshah 67210, Pakistan.
   Sattar, Muhammad Nouman, Natl Univ Technol, Dept Civil Engn, Islamabad 44000, Pakistan.
   Mehdi, Mujahid, Aror Univ Art Architecture Design \& Heritage, Fac Design, Sukkur 6500, Pakistan.
   Vistro, Rahim Bux, Sindh Agr Univ, Fac Agr Engn, Dept Irrigat \& Drainage, Tandojam 70060, Pakistan.
   Mahmoud, Eman A., Damietta Univ, Fac Agr, Dept Food Ind, Dumyat 34511, Egypt.
   Elansary, Hosam O., King Saud Univ, Coll Food Agr Sci, Dept Plant Prod, POB 2460, Riyadh 11451, Saudi Arabia.},
DOI = {10.3390/agronomy13071764},
Article-Number = {1764},
EISSN = {2073-4395},
Keywords = {plant disease identification; EfficientNet-B3; drone; leaf disease
   product},
Keywords-Plus = {DISEASES},
Research-Areas = {Agriculture; Plant Sciences},
Web-of-Science-Categories  = {Agronomy; Plant Sciences},
Author-Email = {sayedsabab@hanyang.ac.kr
   ghulammustafa@sunmoon.ac.kr
   hareefkeerio@quest.edu.pk
   drnoumansattar@nutech.edu.pk
   emanmail2005@yahoo.com
   helansary@ksu.edu.sa},
Affiliations = {Hanyang University; Sun Moon University; Sindh Agricultural University;
   Egyptian Knowledge Bank (EKB); Damietta University; King Saud University},
ResearcherID-Numbers = {mahmoud, eman/AAK-9113-2021},
ORCID-Numbers = {mahmoud, eman/0000-0001-7796-8480},
Funding-Acknowledgement = {``Ministry of Education{''} in Saudi Arabia {[}IFKSUR3-095-02]},
Funding-Text = {Deputyship for Research and Innovations ``Ministry of Education{''} in
   Saudi Arabia (IFKSUR3-095-02).},
Cited-References = {Abayomi-Alli OO, 2021, EXPERT SYST, V38, DOI 10.1111/exsy.12746.
   Albetis J, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9040308.
   Aldhyani T.H., 2022, P 2022 7 INT C COMMU, P1289.
   Aliyu M.A., 2020, IAES INT J ARTIF INT, V9, P670, DOI {[}10.11591/ijai.v9.i4.pp670-683, DOI 10.11591/IJAI.V9.I4.PP670-683].
   Almadhor A, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21113830.
   Amarasingam Narmilan, 2022, Remote Sensing Applications: Society and Environment, DOI 10.1016/j.rsase.2022.100712.
   Atila U, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2020.101182.
   Azimi S, 2021, MEASUREMENT, V173, DOI 10.1016/j.measurement.2020.108650.
   Basavaiah J, 2020, WIRELESS PERS COMMUN, V115, P633, DOI 10.1007/s11277-020-07590-x.
   Bhattacharya S, 2021, SUSTAIN CITIES SOC, V65, DOI 10.1016/j.scs.2020.102589.
   Sorte LXB, 2019, PROCEDIA COMPUT SCI, V159, P135, DOI 10.1016/j.procs.2019.09.168.
   Chivasa W, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12152445.
   DadrasJavan F, 2019, J PLANT DIS PROTECT, V126, P307, DOI 10.1007/s41348-019-00234-8.
   Dawod RG, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22072696.
   De Rosa D, 2021, COMPUT ELECTRON AGR, V180, DOI 10.1016/j.compag.2020.105880.
   Feng LW, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12122028.
   Franczyk Bogdan, 2020, Procedia Computer Science, V176, P1211, DOI 10.1016/j.procs.2020.09.117.
   Gadekallu TR, 2021, J REAL-TIME IMAGE PR, V18, P1383, DOI 10.1007/s11554-020-00987-8.
   Gadekallu TR, 2020, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-01963-7.
   Garcia L, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10196668.
   Hassan M, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7090868.
   Huang HS, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9030558.
   Kallam Suresh, 2018, 2018 International Conference on Advances in Computing and Communication Engineering (ICACCE), P331, DOI 10.1109/ICACCE.2018.8441674.
   Kerkech M, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105446.
   Kerkech M, 2018, COMPUT ELECTRON AGR, V155, P237, DOI 10.1016/j.compag.2018.10.006.
   Kim H, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13010025.
   Kundu N, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21165386.
   Lan YB, 2020, COMPUT ELECTRON AGR, V171, DOI 10.1016/j.compag.2020.105234.
   Leon-Rueda WA, 2022, TROP PLANT PATHOL, V47, P152, DOI 10.1007/s40858-021-00460-2.
   Marin DB, 2021, COMPUT ELECTRON AGR, V190, DOI 10.1016/j.compag.2021.106476.
   Mazzia V, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20092530.
   Miaomiao Ji, 2020, Information Processing in Agriculture, V7, P418, DOI 10.1016/j.inpa.2019.10.003.
   Ministry of Finance Government of Pakistan, 2022, EC SURV PAK 2020 202.
   Kabir MM, 2020, Arxiv, DOI arXiv:2011.05151.
   Mrisho LM, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.590889.
   Musa A, 2019, 2019 2ND INTERNATIONAL CONFERENCE OF THE IEEE NIGERIA COMPUTER CHAPTER (NIGERIACOMPUTCONF), P371, DOI 10.1109/nigeriacomputconf45974.2019.8949669.
   Narmilan A., 2020, Agricultural Reviews, V41, P279, DOI 10.18805/ag.R-151.
   Narmilan A., 2017, J ENG TECHNOL-US, V5, P10.
   Narmilan A., 2020, INT J RES PUBL, V61, P14.
   Osco LP, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11242925.
   Oyewola DO, 2021, PEERJ COMPUT SCI, V7, DOI 10.7717/peerj-cs.352.
   Panigrahi KP., 2020, PROGR COMPUTING ANAL, P659, DOI {[}10.1007/978-981-15-2414-1\_66, DOI 10.1007/978-981-15-2414-1\_66].
   Parul Sharma, 2020, Information Processing in Agriculture, V7, P566, DOI 10.1016/j.inpa.2019.11.001.
   Planet Natural Research Center, EARL BLIGHT.
   Planet Natural Research Center, LAT BLIGHT.
   Planet Natural Research Center, BACT LEAF SPOT.
   PlantVillage, US.
   Prodeep AR, 2022, 2022 INTERNATIONAL CONFERENCE ON DECISION AID SCIENCES AND APPLICATIONS (DASA), P523, DOI 10.1109/DASA54658.2022.9765063.
   Puig E, 2015, 21ST INTERNATIONAL CONGRESS ON MODELLING AND SIMULATION (MODSIM2015), P1420.
   Selvaraj MG, 2020, ISPRS J PHOTOGRAMM, V169, P110, DOI 10.1016/j.isprsjprs.2020.08.025.
   Sinha A, 2020, PROCEDIA COMPUT SCI, V167, P2328, DOI 10.1016/j.procs.2020.03.285.
   Su JY, 2018, COMPUT ELECTRON AGR, V155, P157, DOI 10.1016/j.compag.2018.10.017.
   Tetila EC, 2020, IEEE GEOSCI REMOTE S, V17, P903, DOI 10.1109/LGRS.2019.2932385.
   Tsouros DC, 2019, INFORMATION, V10, DOI 10.3390/info10110349.
   Wang TY, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12081310.
   Xavier TWF, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3020033.
   Xiao YX, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13132437.},
Number-of-Cited-References = {57},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Agronomy-Basel},
Doc-Delivery-Number = {N6AE1},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:001037808300001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000469047400024,
Author = {Al-Qurran, Raffi and Al-Ayyoub, Mahmoud and Shatnawi, Ali},
Book-Group-Author = {IEEE},
Title = {Plant Classification in the Wild: A Transfer Learning Approach},
Booktitle = {2018 19TH INTERNATIONAL ARAB CONFERENCE ON INFORMATION TECHNOLOGY (ACIT)},
Series = {International Arab Conference on Information Technology ACIT},
Year = {2018},
Pages = {148-152},
Note = {19th International Arab Conference on Information Technology (ACIT),
   LEBANON, NOV 28-30, 2018},
Organization = {Islam Univ Lebanon; ACIT Int; AARU CCIS; Zarqa Univ; IEEE; IEEE Engn Med
   \& Biol Soc Lebanon Chapter; IEEE Comp Soc; UFA Assurances; MEAB Bank;
   Informat Technol},
Abstract = {Datasets specialized in wildlife usually contain imbalanced classes of
   natural wild images such as, for instance, plant images, which are
   acquired from the surrounding environment with natural scene background.
   Deep neural networks have proven their efficiency in classifying such
   datasets. However, such an approach requires a workaround to
   approximately balance the classes in order to prevent the occurrence of
   overfitting during the training phase of the neural network. Many
   approaches exist to overcome this problem includes over-sampling, under
   sampling, generating synthetic samples, data augmentation, etc. The
   iNaturalist species classification and detection dataset represents a
   good example of vastly imbalanced datasets. It contains 13 superclasses.
   This work focuses on the Plantae superclass and builds a Convolutional
   Neural Network to distinguish a subset of the subclasses of Plantae. Our
   model benefits from cutting-edge techniques such as transfer learning
   and data augmentation to obtain a reasonably high level of accuracy
   (78.76\%).},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Al-Qurran, R (Corresponding Author), Jordan Univ Sci \& Technol, Irbid, Jordan.
   Al-Qurran, Raffi; Al-Ayyoub, Mahmoud; Shatnawi, Ali, Jordan Univ Sci \& Technol, Irbid, Jordan.},
ISSN = {1812-0857},
EISSN = {2075-2245},
ISBN = {978-1-7281-0385-3},
Keywords = {Deep Learning; Convolutional Neural Networks; Transfer Learning; Data
   Augmentation; Plant classification},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic},
Author-Email = {rlalqurran11@cit.just.edu.jo
   maalshbool@just.edu.jo
   ali@just.edu.jo},
Affiliations = {Jordan University of Science \& Technology},
ResearcherID-Numbers = {Al-Qurran, Raffi/GYU-3687-2022},
Cited-References = {Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Chaurasia G, 2017, IEEE I CONF COMP VIS, P5315, DOI 10.1109/ICCV.2017.567.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Pound MP, 2017, IEEE INT CONF COMP V, P2055, DOI 10.1109/ICCVW.2017.241.
   Reyes A.K., 2015, C LABS EV FOR SEPT, V1391, P9.
   Shatnawi A, 2018, INT CONF INFORM COMM, P72, DOI 10.1109/IACS.2018.8355444.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Toma A., 2017, CLEF WORKING NOTES.
   Ubbens JR, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01190.
   Van Horn G, 2017, ARXIV170706642.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yalcin H, 2016, INT CONF AGRO-GEOINF, P233.},
Number-of-Cited-References = {15},
Times-Cited = {4},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BM8ER},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000469047400024},
DA = {2023-08-12},
}

@article{ WOS:000710569700001,
Author = {Barreto, Abel and Lottes, Philipp and Yamati, Facundo Ramon Ispizua and
   Baumgarten, Stephen and Wolf, Nina Anastasia and Stachniss, Cyrill and
   Mahlein, Anne-Katrin and Paulus, Stefan},
Title = {Automatic UAV-based counting of seedlings in sugar-beet field and
   extension to maize and strawberry},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2021},
Volume = {191},
Month = {DEC},
Abstract = {Counting crop seedlings is a time-demanding activity involved in diverse
   agricultural practices like plant cultivating, experimental trials,
   plant breeding procedures, and weed control. Unmanned Aerial Vehicles
   (UAVs) carrying RGB cameras are novel tools for automatic field mapping,
   and the analysis of UAV images by deep learning methods can provide
   relevant agronomic information. UAV-based camera systems and a deep
   learning image analysis pipeline are implemented for a fully automated
   plant counting in sugar beet, maize, and strawberry fields in the
   present study. Five locations were monitored at different growth stages,
   and the crop number per plot was automatically predicted by using a
   fully convolutional network (FCN) pipeline. Our FCN-based approach is a
   single model for jointly determining both the exact stem location of
   crop and weed plants and a pixel-wise plant classification considering
   crop, weed, and soil. To determinate the approach performance, predicted
   crop counting was compared to visually assessed ground truth data.
   Results show that UAV-based counting of sugar-beet plants delivers
   forecast errors lower than 4.6\%, and the main factors for performance
   are related to the intra-row distance and the growth stage. The
   pipeline's extension to other crops is possible; the errors of the
   predictions are lower than 4\% under practical field conditions for
   maize and strawberry fields. This work highlight the feasibility of
   automatic crop counting, which can reduce manual effort to the farmers.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Barreto, A (Corresponding Author), Inst Sugar Beet Res IfZ, Holtenser Landstr 77, D-37079 Gottingen, Germany.
   Barreto, Abel; Yamati, Facundo Ramon Ispizua; Mahlein, Anne-Katrin; Paulus, Stefan, Inst Sugar Beet Res IfZ, Holtenser Landstr 77, D-37079 Gottingen, Germany.
   Lottes, Philipp, Pheno Inspect GmbH, Strassburger Str 109, D-46047 Oberhausen, Germany.
   Baumgarten, Stephen; Wolf, Nina Anastasia, ARGE NORD eV, Helene Kunne Allee 5, D-38122 Braunschweig, Germany.
   Stachniss, Cyrill, Univ Bonn, Photogrammetry \& Robot Lab, Nussallee 15, D-53115 Bonn, Germany.},
DOI = {10.1016/j.compag.2021.106493},
EarlyAccessDate = {OCT 2021},
Article-Number = {106493},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Deep learning; FCN; UAV; Sugar beet; Plant segmentation; Time-series;
   Intra-row distance; Growth stage},
Keywords-Plus = {PLANT-DENSITY; NETWORKS},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {barreto@ifz-goettingen.de
   philipp.lottes@phenoinspect.de
   ispizua@ifz-goettingen.de
   stephen.baumgarten@arge-nord.de
   ninaanastasia.wolf@arge-nord.de
   cyrill.stachniss@igg.uni-bonn.de
   mahlein@ifz-goettingen.de
   paulus@ifz-goettingen.de},
Affiliations = {University of Bonn},
ResearcherID-Numbers = {Barreto Alcantara, Abel Andree/HGU-7905-2022
   Stachniss, Cyrill/AAH-3034-2019
   },
ORCID-Numbers = {Barreto Alcantara, Abel Andree/0000-0003-2168-327X
   Mahlein, Anne-Katrin/0000-0003-1091-3690
   Stachniss, Cyrill/0000-0003-1173-6972},
Cited-References = {arl ander B., 1990, AGRONOMY CROP SCI, V13.
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615.
   Bosilj P, 2020, J FIELD ROBOT, V37, P7, DOI 10.1002/rob.21869.
   Champ J, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11373.
   Cioni F, 2010, SUGAR TECH, V12, P243, DOI 10.1007/s12355-010-0036-2.
   Deng L, 2018, ISPRS J PHOTOGRAMM, V146, P124, DOI 10.1016/j.isprsjprs.2018.09.008.
   Dumoulin V, 2018, Arxiv, DOI arXiv:1603.07285.
   DURRANT MJ, 1985, J AGR SCI-CAMBRIDGE, V104, P71, DOI 10.1017/S0021859600043008.
   Karami A, 2020, IEEE J-STARS, V13, P5872, DOI 10.1109/JSTARS.2020.3025790.
   Kato T, 2016, INTEGRATION DISTRIBU, P77, DOI DOI 10.1016/B978-0-12-803212-1.00004-0.
   Kunz C., 2015, Landtechnik, V70, P67.
   Lottes Philipp, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3024, DOI 10.1109/ICRA.2017.7989347.
   Lottes P, 2020, J FIELD ROBOT, V37, P20, DOI 10.1002/rob.21901.
   Lottes P, 2018, IEEE INT C INT ROBOT, P8233, DOI 10.1109/IROS.2018.8593678.
   Lottes P, 2018, IEEE ROBOT AUTOM LET, V3, P2870, DOI 10.1109/LRA.2018.2846289.
   Lu H., IEEE T GEOSCI ELECT, P1.
   Pena JM, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0077151.
   McCool C, 2017, IEEE ROBOT AUTOM LET, V2, P1344, DOI 10.1109/LRA.2017.2667039.
   Milosevi c M., 2010, GENETIKA+, V42.
   Oh S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12182981.
   Pepe M, 2018, EUR J REMOTE SENS, V51, P412, DOI 10.1080/22797254.2018.1444945.
   Petersen J., 2004, Weed biology and management, P467.
   Pospisil M, 2000, EUR J AGRON, V12, P69, DOI 10.1016/S1161-0301(99)00045-3.
   Quan LZ, 2019, BIOSYST ENG, V184, P1, DOI 10.1016/j.biosystemseng.2019.05.002.
   Rabah M., 2018, NRIAG Journal of Astronomy and Geophysics, V7, P220, DOI 10.1016/j.nrjag.2018.05.003.
   Rahman Md Atiqur, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P234, DOI 10.1007/978-3-319-50835-1\_22.
   Sa I, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10091423.
   SCHREINER AJ, 1993, B AM METEOROL SOC, V74, P1851, DOI 10.1175/1520-0477(1993)074<1851:ACOGAS>2.0.CO;2.
   Shcherbakov MV, 2013, WORLD APPL SCI J, V24, P171, DOI DOI 10.5829/IDOSI.WASJ.2013.24.ITMIES.80032.
   SMIT AL, 1993, FIELD CROP RES, V34, P159, DOI 10.1016/0378-4290(93)90004-7.
   Sogut T., 2004, Journal of Agronomy, V3, P215.
   Tong P., ENG APPL ARTIF INTEL, V100.
   Wu XL, 2020, J FIELD ROBOT, V37, P322, DOI 10.1002/rob.21938.
   Zarco-Tejada PJ, 2013, AGR FOREST METEOROL, V171, P281, DOI 10.1016/j.agrformet.2012.12.013.},
Number-of-Cited-References = {34},
Times-Cited = {12},
Usage-Count-Last-180-days = {14},
Usage-Count-Since-2013 = {79},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {WL7GM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000710569700001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000464883600109,
Author = {Wu, Yong and Qin, Xiao and Pan, Yonghua and Yuan, Changan},
Book-Group-Author = {IEEE},
Title = {Convolution Neural Network based Transfer Learning for Classification of
   Flowers},
Booktitle = {2018 IEEE 3RD INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING
   (ICSIP)},
Year = {2018},
Pages = {562-566},
Note = {IEEE 3rd International Conference on Signal and Image Processing
   (ICSIP), SE Univ, Shenzhen, PEOPLES R CHINA, JUL 13-15, 2018},
Organization = {IEEE},
Abstract = {Flower plays an extremely important role in our life, which has high
   research value and application value. The traditional methods of flower
   classification is mainly based on shape, color or texture features, and
   this methods needs people to select features for flower classification
   lead to the accuracy of classification is not very high. This paper aims
   to develop an effective flower classification approach using convolution
   neural network and transfer learning. In this paper, based on VGG-16,
   VGG-19, Inception-v3 and ResNet50 models were used to compare the
   network initialization model with the transfer learning model. The
   results show that transfer learning can effectively avoid deep
   convolution networks are prone to local optimal problems and
   over-fitting problems. Compared with the traditional methods, the
   accuracy of flower recognition on Oxford flowers dataset is obviously
   improved, and has better robustness and generalization ability.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Qin, X (Corresponding Author), Guangxi Teachers Educ Univ, Coll Comp \& Informat Engn, Nanning, Peoples R China.
   Wu, Yong; Qin, Xiao; Pan, Yonghua; Yuan, Changan, Guangxi Teachers Educ Univ, Coll Comp \& Informat Engn, Nanning, Peoples R China.},
ISBN = {978-1-5386-6396-7},
Keywords = {deep learning; convolution neural network; transfer learning; flowers
   classification},
Research-Areas = {Engineering; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Imaging Science \& Photographic
   Technology},
Author-Email = {601702620@qq.com
   7670172@qq.com
   283889077@qq.com
   68852917@qq.com},
Affiliations = {Nanning Normal University},
ORCID-Numbers = {Wu, Yong/0000-0001-5628-1858},
Funding-Acknowledgement = {National Natural Science Foundation of Guangxi {[}2016GXNSFAA380209,
   2014GXNSFDA118037]; ``BAGUI Scholar{''} Program of Guangxi Zhuang
   Autonomous Region of China; project of scientific research and
   technology development in Guangxi in 2016 {[}AB16380272]; project of
   scientific research and technology development in Guangxi Nanning 2017
   {[}20175177]; Innovation Project of Guangxi Graduate Education
   {[}YCSW2017187]},
Funding-Text = {This work is partially supported by the National Natural Science
   Foundation of Guangxi under Grant Nos.
   2016GXNSFAA380209,2014GXNSFDA118037; the ``BAGUI Scholar{''} Program of
   Guangxi Zhuang Autonomous Region of China; the project of scientific
   research and technology development (AB16380272) in Guangxi in 2016; the
   project of scientific research and technology development (\#20175177)
   in Guangxi Nanning 2017; Innovation Project of Guangxi Graduate
   Education(YCSW2017187).},
Cited-References = {Angelova A, 2013, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2013.110.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647.
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   Liu W, 2017, AIP CONF PROC, V1794, DOI 10.1063/1.4971938.
   Liu Y, 2017, INT C FUNCT STRUCT P, P110.
   Nilsback M.E., 2006, P 2006 IEEE COMPUTER, VVolume 2, P1447, DOI DOI 10.1109/CVPR.2006.42.
   Nilsback M. -E., 2007, BMVC, P1.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Ping S., 2017, B SCI TECHNOLOGY, V33, P115.
   Saitoh T, 2000, INT C PATT RECOG, P507, DOI 10.1109/ICPR.2000.906123.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Xie Xiaodong, 2014, RES FINE GRAINED CLA.},
Number-of-Cited-References = {17},
Times-Cited = {29},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BM5FP},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000464883600109},
DA = {2023-08-12},
}

@inproceedings{ WOS:000390177400050,
Author = {Putzu, Lorenzo and Di Ruberto, Cecilia and Fenu, Gianni},
Editor = {BlancTalon, J and Distante, C and Philips, W and Popescu, D and Scheunders, P},
Title = {A Mobile Application for Leaf Detection in Complex Background Using
   Saliency Maps},
Booktitle = {ADVANCED CONCEPTS FOR INTELLIGENT VISION SYSTEMS, ACIVS 2016},
Series = {Lecture Notes in Computer Science},
Year = {2016},
Volume = {10016},
Pages = {570-581},
Note = {17th International Conference on Advanced Concepts for Intelligent
   Vision Systems (ACIVS), Lecce, ITALY, OCT 24-27, 2016},
Organization = {Antwerp Univ; Commonwealth Sci \& Ind Res Org; Ghent Univ; Inst Appl Sci
   \& Intelligent Syst; Natl Res Council Italy; Univ Salento},
Abstract = {Plants are fundamental for human beings, so it's very important to
   catalogue and preserve all the plants species. Identifying an unknown
   plant species is not a simple task. The leaf analysis is one of the
   approach used for the plant species identification. This task can be
   completed also automatically by image processing techniques, able to
   analyse the leaf images and provide a classification based on prior
   information. Many methods have been proposed in literature in order to
   complete the whole cataloguing task, providing excellent classification
   results. Nevertheless, many of the proposed methods work only on images
   acquired in controlled lighting conditions and with uniform background.
   In this work we propose a mobile application for leaf analysis for the
   automatic identification of plant species. The application is mainly
   devoted to the identification and segmentation steps, resolving the main
   issues created by uncontrolled lighting conditions with very accurate
   results.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Putzu, L (Corresponding Author), Univ Cagliari, Dept Math \& Comp Sci, Via Osped 72, I-09124 Cagliari, Italy.
   Putzu, Lorenzo; Di Ruberto, Cecilia; Fenu, Gianni, Univ Cagliari, Dept Math \& Comp Sci, Via Osped 72, I-09124 Cagliari, Italy.},
DOI = {10.1007/978-3-319-48680-2\_50},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-48680-2; 978-3-319-48679-6},
Keywords = {Image analysis; Leaf recognition; Saliency maps; Segmentation},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {lorenzo.putzu@gmail.com
   dirubert@unica.it
   fenu@unica.it},
Affiliations = {University of Cagliari},
ResearcherID-Numbers = {PUTZU, LORENZO/J-5200-2019
   Di Ruberto, Cecilia/G-6915-2014
   },
ORCID-Numbers = {PUTZU, LORENZO/0000-0001-5361-8793
   Di Ruberto, Cecilia/0000-0003-4641-0307
   FENU, GIANNI/0000-0003-4668-2476},
Cited-References = {Buoncompagni S., 2015, P BRIT MACH VIS C BM.
   Di Ruberto C, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS (VISAPP), VOL 1, P601.
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77.
   Frintrop S., 2006, LNCS LNAI, V3899, DOI {[}10.1007/11682110, DOI 10.1007/11682110].
   Fua P., 2010, TECHNICAL REPORT.
   GREENSPAN H, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P222, DOI 10.1109/CVPR.1994.323833.
   He SF, 2014, LECT NOTES COMPUT SC, V8691, P110, DOI 10.1007/978-3-319-10578-9\_8.
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558.
   Knight D, 2010, AUTOMATIC PLANT LEAF.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Le T.L., 2015, EMR 2015 P 2 INT WOR, p{[}2, 3].
   Levinshtein A, 2013, INT J COMPUT VISION, V104, P117, DOI 10.1007/s11263-013-0614-3.
   Liu T., 2007, IEEE C COMPUTER VISI, P1, DOI DOI 10.1109/CVPR.2007.383047.
   Montabone S, 2010, IMAGE VISION COMPUT, V28, P391, DOI 10.1016/j.imavis.2009.06.006.
   Mouine S, 2013, P 3 ACM C INT C MULT, P309.
   Prasad S., 2013, I CARE 2013 P 5 IBM.
   Prasvita D.S., 2013, INT J ADV SCI ENG IN, V3, P5, DOI {[}10.18517/ijaseit.3.2.287, DOI 10.18517/IJASEIT.3.2.287].
   Rahman M. N. A., 2015, INT J STW ENG APPL, V9, P135.
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10.
   Shejwal S., 2015, INT J COMPUT SCI TEC, P93.
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688.
   Valliammal N., 2012, INT J COMPUT APPL, V44, P10, DOI DOI 10.5120/6322-8669.
   Vedaldi A, 2008, LECT NOTES COMPUT SC, V5305, P705, DOI 10.1007/978-3-540-88693-8\_52.
   Wang L., 2008, COMPUTER COMPUTING T, V1.
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407.},
Number-of-Cited-References = {25},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BG6EX},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000390177400050},
DA = {2023-08-12},
}

@article{ WOS:000961707600001,
Author = {Acharya, Biswaranjan and Ghosh, Ahona and Panda, Sucheta and Hu, Yu-Chen},
Title = {Automated Plant Recognition System with Geographical Position Selection
   for Medicinal Plants},
Journal = {ADVANCES IN MULTIMEDIA},
Year = {2023},
Volume = {2023},
Month = {MAR 23},
Abstract = {Basically, it is hard for endeavors to recognize plant leaf images by a
   layman due to the varieties in some plant leaves and the extensive
   information collected for investigation. It is hard to build an
   automated recognition framework that can handle massive data and give an
   intermediate analysis. Image examination and order and pattern
   recognition are some issues that are effectively connected to the
   existing methods. This paper focuses on designing an automated plant
   recognition system based on the best recognition algorithm and the
   Google platform to locate all plant locations on a map. A case study of
   India, which has huge biodiversity, is illustrated. The proposed system
   can show the detailed location of that particular species, where they
   can be found, and the shortest distance from the current location.},
Publisher = {HINDAWI LTD},
Address = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Hu, YC (Corresponding Author), Providence Univ, Dept Comp Sci \& Informat Management, Taichung, Taiwan.
   Hu, YC (Corresponding Author), Tunghai Univ, Dept Comp Sci, Taichung, Taiwan.
   Acharya, Biswaranjan; Panda, Sucheta, Veer Surendra Sai Univ Technol, Dept Comp Applicat, Burla, Odisha, India.
   Ghosh, Ahona, Maulana Abul Kalam Azad Univ Technol, Dept Comp Sci \& Engn, Kolkata 741249, West Bengal, India.
   Hu, Yu-Chen, Providence Univ, Dept Comp Sci \& Informat Management, Taichung, Taiwan.
   Hu, Yu-Chen, Tunghai Univ, Dept Comp Sci, Taichung, Taiwan.},
DOI = {10.1155/2023/3974346},
Article-Number = {3974346},
ISSN = {1687-5680},
EISSN = {1687-5699},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {biswaranjanacharya2020@gmail.com
   ahonaghosh95@gmail.com
   suchetapanda\_mca@vssut.ac.in
   ychu@pu.edu.tw},
Affiliations = {Veer Surendra Sai University of Technology; Maulana Abul Kalam Azad
   University of Technology; Providence University - Taiwan; Tunghai
   University},
ResearcherID-Numbers = {Acharya, Biswaranjan/AAL-1977-2020
   },
ORCID-Numbers = {Acharya, Biswaranjan/0000-0002-6506-9207
   Hu, Yu-Chen/0000-0002-5055-3645
   GHOSH, AHONA/0000-0003-0498-285X},
Cited-References = {Adeel A, 2019, SUSTAIN COMPUT-INFOR, V24, DOI 10.1016/j.suscom.2019.08.002.
   Anant Bhardwaj, 2013, International Journal of Innovation and Applied Studies, V3, P237.
   Benzie IFF, 2011, HERBAL MED BIOMOLECU, DOI DOI 10.1201/B10787.
   Bhardwaj A., 2013, INT J ENG TRENDS TEC, V4, P86.
   Budashko V, 2018, INT CONF METH SYS NA, P106, DOI 10.1109/MSNMC.2018.8576266.
   Cassidy R., 2020, WILD THINGS ARE NOW, P1.
   Chan W., 2020, PROC ICML, P1403.
   Du J., 2005, P ADV NEUR NETW ISNN, P811.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   Dwivedi R, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3110287.
   Fu H, 2004, I C CONT AUTOMAT ROB, P681.
   Galappathie S, 2014, J HERB MED, V4, P96, DOI 10.1016/j.hermed.2014.03.001.
   Gu X, 2005, LECT NOTES COMPUT SC, V3644, P253.
   Han F, 2020, ADV MULTIMED, V2020, DOI 10.1155/2020/1909875.
   Hossain J., 2010, Proceedings of the 13th International Conference on Computer and Information Technology (ICCIT 2010), P458, DOI 10.1109/ICCITECHN.2010.5723901.
   Hou Y, 2020, J ETHNOPHARMACOL, V249, DOI 10.1016/j.jep.2019.112426.
   Jeong EK, 2017, GEOSCI J, V21, P483, DOI 10.1007/s12303-017-0004-x.
   Jia Boyan, 2021, Advances in Multimedia, DOI 10.1155/2021/3998933.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kadir Abdul, 2012, Journal of Theoretical and Applied Information Technology, V41, P82.
   Kadir A., 2014, INT J ADV SCI TECHNO, V44, P113.
   Kaya A, 2019, COMPUT ELECTRON AGR, V158, P20, DOI 10.1016/j.compag.2019.01.041.
   Kulkarni A. H., 2013, INT J ADV RES COMPUT, V2, P984.
   Lu GN, 2019, INT J GEOGR INF SCI, V33, P346, DOI 10.1080/13658816.2018.1533136.
   Lv Q, 2022, ADV MULTIMED, V2022, DOI 10.1155/2022/3351256.
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968.
   Prasad S., 2011, 2011 2nd International Conference on Computer and Communication Technology, P646, DOI 10.1109/ICCCT.2011.6075212.
   Rodriguez-Prieto A, 2017, SADHANA-ACAD P ENG S, V42, P2147, DOI 10.1007/s12046-017-0745-2.
   Selvam S, 2016, ARAB J GEOSCI, V9, DOI 10.1007/s12517-016-2417-7.
   Soukand R, 2010, TRAMES-J HUMANIT SOC, V14, P207, DOI 10.3176/tr.2010.3.01.
   Wan SH, 2020, COMPUT NETW, V168, DOI 10.1016/j.comnet.2019.107036.
   Wang XF, 2005, LECT NOTES COMPUT SC, V3644, P87.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.},
Number-of-Cited-References = {33},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Adv. Multimed.},
Doc-Delivery-Number = {C4NU1},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000961707600001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:A1993KY65900023,
Author = {GUYER, DE and MILES, GE and GAULTNEY, LD and SCHREIBER, MM},
Title = {APPLICATION OF MACHINE VISION TO SHAPE-ANALYSIS IN LEAF AND
   PLANT-IDENTIFICATION},
Journal = {TRANSACTIONS OF THE ASAE},
Year = {1993},
Volume = {36},
Number = {1},
Pages = {163-171},
Month = {JAN-FEB},
Abstract = {Intelligent vision systems capable of automatically interpreting images
   are the next generation in machine vision. These systems will be usable
   by nonexperts in the machine vision field, and be flexible and
   reprogrammable for general use. The addition of intelligence into vision
   systems requires an understanding and structuring of human visual and
   scene interpretation techniques.
   This research investigated the interrelationships and combined the areas
   of feature extraction, pattern recognition and techniques of scene
   interpretation by humans. The research is especially unique in its
   rule-based structuring of subjective, qualitative, human-level shape
   features and quantitative machine-level shape features. An algorithm was
   implemented to extract plant/leaf shape features using information
   gathered from critical points along object borders such as the location
   of angles along the border and/or local maxima and minima from the
   plant/leaf centroid A library or set of 17 low-level quantitative
   features and the rule-base combining the low-level features into 13
   higher-level quantitative features, demonstrated the ability to combine
   and structure basic explicit data or information into more subjective
   shape knowledge. Structuring and verifying the features added
   intelligence to the system and illustrated a potential technique for
   bridging the shape interpretation levels of man and machine.
   Results indicate a structuring of knowledge levels can be used to link
   machine and human shape interpretation, especially if descriptive
   features involve minimal ambiguity, i.ev., they are basic and generic
   for the humans assigning shape certainties to the recognition knowledge
   base.},
Publisher = {AMER SOC AGRICULTURAL ENGINEERS},
Address = {2950 NILES RD, ST JOSEPH, MI 49085-9659},
Type = {Article},
Language = {English},
Affiliation = {GUYER, DE (Corresponding Author), MICHIGAN STATE UNIV,E LANSING,MI 48824, USA.
   PURDUE UNIV,USDA ARS,W LAFAYETTE,IN 47907.},
ISSN = {0001-2351},
Keywords = {MACHINE VISION; SHAPE ANALYSIS; LEAF AND PLANT IDENTIFICATION},
Keywords-Plus = {EXPERT SYSTEM; AERIAL},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agricultural Engineering},
Affiliations = {Purdue University System; Purdue University; Purdue University West
   Lafayette Campus; United States Department of Agriculture (USDA)},
Cited-References = {ALEKSANDER I, 1984, DESIGNING INTELLIGEN.
   {[}Anonymous], 1978, REMOTE SENSING QUANT.
   FU KS, 1982, SYNTACTIC PATTERN RE.
   GUYER DE, 1986, T ASAE, V29, P1500.
   GUYER DE, 1988, THESIS PURDUE U.
   HARLOW CA, 1986, OPT ENG, V25, P347, DOI 10.1117/12.7973831.
   MOTT DH, 1985, 5TH P INT C ROB VIS, P335.
   NAZIF AM, 1984, IEEE T PATTERN ANAL, V6, P555, DOI 10.1109/TPAMI.1984.4767570.
   PERKINS WA, 1986, OPT ENG, V25, P356, DOI 10.1117/12.7973832.
   TSAI WH, 1985, IEEE T PATTERN ANAL, V7, P453, DOI 10.1109/TPAMI.1985.4767684.
   UMETANI J, 1981, 1ST P INT C ROB VIS, P135.},
Number-of-Cited-References = {11},
Times-Cited = {62},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {10},
Journal-ISO = {Trans. ASAE},
Doc-Delivery-Number = {KY659},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:A1993KY65900023},
DA = {2023-08-12},
}

@article{ WOS:000967546000032,
Author = {Nasir, Fazal E. and Tufail, Muhammad and Haris, Muhammad and Iqbal,
   Jamshed and Khan, Said and Khan, Muhammad Tahir},
Title = {Precision agricultural robotic sprayer with real-time Tobacco
   recognition and spraying system based on deep learning},
Journal = {PLOS ONE},
Year = {2023},
Volume = {18},
Number = {3},
Month = {MAR 31},
Abstract = {Precision agricultural techniques try to prevent either an excessive or
   inadequate application of agrochemicals during pesticide application. In
   recent years, it has become popular to combine traditional agricultural
   practices with artificial intelligence algorithms. This research
   presents a case study of variable-rate targeted spraying using deep
   learning for tobacco plant recognition and identification in a real
   tobacco field. An extensive comparison of the detection performance of
   six YOLO-based models for the tobacco crop has been performed based on
   experimentation in tobacco fields. An F-1-score of 87.2\% and a frame
   per second rate of 67 were achieved using the YOLOv5n model trained on
   actual field data. Additionally, a novel disturbance-based pressure and
   flow control method has been introduced to address the issue of unwanted
   pressure fluctuations that are typically associated with bang-bang
   control. The quality of spray achieved by attenuation of these
   disturbances has been evaluated both qualitatively and quantitatively
   using three different spraying case studies: broadcast, and selective
   spraying at 20 psi pressure; and variable-rate spraying at pressure
   varying from 15-120 psi. As compared to the broadcast spraying, the
   selective and variable rate spray methods have achieved up to 60\%
   reduction of agrochemicals.},
Publisher = {PUBLIC LIBRARY SCIENCE},
Address = {1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA},
Type = {Article},
Language = {English},
Affiliation = {Iqbal, J (Corresponding Author), Univ Hull, Fac Sci \& Engn, Sch Comp Sci, Kingston Upon Hull, England.
   Nasir, Fazal E.; Tufail, Muhammad; Haris, Muhammad; Khan, Muhammad Tahir, Natl Ctr Robot \& Automat NCRA, Adv Robot \& Automat Lab, Peshawar, Pakistan.
   Tufail, Muhammad; Khan, Muhammad Tahir, Univ Engn \& Technol, Dept Mechatron Engn, Peshawar, Pakistan.
   Iqbal, Jamshed, Univ Hull, Fac Sci \& Engn, Sch Comp Sci, Kingston Upon Hull, England.
   Khan, Said, Univ Bahrain, Coll Engn, Dept Mech Engn, Isa Town, Bahrain.},
DOI = {10.1371/journal.pone.0283801},
ISSN = {1932-6203},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {j.iqbal@hull.ac.uk},
Affiliations = {University of Engineering \& Technology Peshawar; University of Hull;
   University of Bahrain},
ResearcherID-Numbers = {Iqbal, Jamshed/AER-0153-2022
   Haris, Muhammad/IWU-7407-2023
   Tufail, Muhammad/GWZ-7322-2022},
ORCID-Numbers = {Iqbal, Jamshed/0000-0002-0795-0282
   Tufail, Muhammad/0000-0003-3442-7216},
Funding-Acknowledgement = {National Centre of Robotics and Automation (NCRA) through the Advanced
   Robotics and Automation Laboratory (ARAL), Department of Mechatronics
   Engineering, University of Engineering and Technology, Peshawar,
   Pakistan; Higher Education Commission of Pakistan {[}DF-1009-31]},
Funding-Text = {This work was supported in part by the National Centre of Robotics and
   Automation (NCRA) through the Advanced Robotics and Automation
   Laboratory (ARAL), Department of Mechatronics Engineering, University of
   Engineering and Technology, Peshawar, Pakistan, and in part by the
   Higher Education Commission of Pakistan through grant titled
   ``Establishment of National Centre of Robotics and Automation{''} under
   Grant DF-1009-31. The funders had no role in study design, data
   collection and analysis, decision to publish, or preparation of the
   manuscript.},
Cited-References = {Adamides G, 2017, J FIELD ROBOT, V34, P1407, DOI 10.1002/rob.21721.
   Adamides G, 2017, APPL ERGON, V62, P237, DOI 10.1016/j.apergo.2017.03.008.
   Advanced Robotics and Automation Laboratory, US.
   Alam M, 2020, 2020 7TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONICS ENGINEERING (ICEEE 2020), P273, DOI {[}10.1109/ICEEE49618.2020.9102505, 10.1109/iceee49618.2020.9102505].
   Alam MS, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12031308.
   Alexandratos N, 2012, WORLD AGR 2030 2050, DOI {[}10.22004/ag.econ.288998, DOI 10.22004/AG.ECON.288998].
   Alvarez A., 2016, BIOREMEDIATION LATIN.
   Alvarez A, 2017, CHEMOSPHERE, V166, P41, DOI 10.1016/j.chemosphere.2016.09.070.
   Baltazar AR, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10172061.
   Bechar A, 2016, BIOSYST ENG, V149, P94, DOI 10.1016/j.biosystemseng.2016.06.014.
   Berenstein R, 2017, J FIELD ROBOT, V34, P1519, DOI 10.1002/rob.21730.
   Bresilla K, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.00611.
   Cheng BB, 2015, LECT NOTES ARTIF INT, V9119, P517, DOI 10.1007/978-3-319-19324-3\_46.
   Corke P., 2011, ROBOTICS VISION CONT, DOI DOI 10.1007/978-3-642-20144-8.
   DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409.
   Gerhards R, 2022, WEED RES, V62, P123, DOI 10.1111/wre.12526.
   Guanhao Yang, 2020, 2020 IEEE 6th International Conference on Computer and Communications (ICCC), P1398, DOI 10.1109/ICCC51575.2020.9345042.
   Hendry, 2019, IMAGE VISION COMPUT, V87, P47, DOI 10.1016/j.imavis.2019.04.007.
   Iyer R., 2021, INT J RES ENG TECHNO, V8, P1156.
   Jiang D, 2021, J ARTIF INTELL TECHN, V1, P74, DOI {[}10.37965/jait.2020.0037, DOI 10.37965/JAIT.2020.0037].
   Jiang DY, 2020, INFRARED PHYS TECHN, V111, DOI 10.1016/j.infrared.2020.103494.
   Jun Z., 2020, LASER OPTOELECTRON P, V57, P161001.
   Khan N., 2018, GPS GUIDED AUTONOMOU.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Li XL, 2020, CHIN CONTR CONF, P6274, DOI 10.23919/CCC50068.2020.9189186.
   Meshram AT, 2022, J FIELD ROBOT, V39, P153, DOI 10.1002/rob.22043.
   National Centre of Robotics and automation, US.
   Parks CG, 2022, ENVIRON RES, V209, DOI 10.1016/j.envres.2022.112862.
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91.
   Slaughter DC, 2008, COMPUT ELECTRON AGR, V61, P63, DOI 10.1016/j.compag.2007.05.008.
   Song YY, 2015, INTELL AUTOM SOFT CO, V21, P319, DOI 10.1080/10798587.2015.1015781.
   Sozzi M, 2022, AGRONOMY-BASEL, V12, DOI 10.3390/agronomy12020319.
   Sun Hong, 2012, International Journal of Agricultural and Biological Engineering, V5, P10, DOI 10.3965/j.ijabe.20120503.002.
   Tian YN, 2019, COMPUT ELECTRON AGR, V157, P417, DOI 10.1016/j.compag.2019.01.012.
   Tona E, 2018, PRECIS AGRIC, V19, P606, DOI 10.1007/s11119-017-9543-4.
   Ul Hassan M, 2016, INT CONF ROBOT ARTIF, P37, DOI 10.1109/ICRAI.2016.7791225.
   Van De Zande J, 2009, PROC 7 EUR C PRECIS, P715.
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203.
   Yan TT, 2018, COMPUT ELECTRON AGR, V152, P363, DOI 10.1016/j.compag.2018.07.030.
   Zhang Y, 2019, OPTIK, V183, P17, DOI 10.1016/j.ijleo.2019.02.038.
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993.
   Zhong YH, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051489.
   Zhu ZQ, 2022, CHEMOSENSORS, V10, DOI 10.3390/chemosensors10050164.},
Number-of-Cited-References = {43},
Times-Cited = {0},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {5},
Journal-ISO = {PLoS One},
Doc-Delivery-Number = {D3CV3},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000967546000032},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000779045500012,
Author = {Al-Qurran, Raffi and Al-Ayyoub, Mahmoud and Shatnawi, Ali},
Title = {Plant classification in the wild: Energy evaluation for deep learning
   models},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2022},
Volume = {81},
Number = {21},
Pages = {30143-30167},
Month = {SEP},
Abstract = {Having a system that can take an image of a natural scene and accurately
   classify the plants in it is of undeniable importance. However, the
   complexities of dealing with natural scene images and the vast diversity
   of plants in the wild make designing such a classifier a challenging
   task. Deep Learning (DL) lends itself as viable solution to tackle such
   complex problem. However, advanced in DL architectures and software
   (including DL frameworks) come with a high cost in terms of energy
   consumption especially when employing Graphics Processing Units (GPU).
   As data expands rapidly, the need to create energy-aware models
   increases in order to reduce energy consumption and move towards
   ``Greener AI{''}. Since the problem of designing energy-aware
   architectures for plant classification has not been studied
   significantly in the literature, our work comes to start bridging this
   gap by focusing not only on the models' performance, but also on their
   energy usage on both CPU and GPU platforms. We consider different
   state-of-the-art Convolutional Neural Networks (CNN) architectures and
   train them on two famous challenging plants datasets: iNaturalist and
   Herbarium. Our experiments are meant to highlight the trade-off between
   accuracy and energy consumption. For examples, the results show that
   while GPU-bound models can be about 40\% faster in terms of training
   time than simple models running on CPU, the latter's energy consumption
   is only two thirds of the former. We hope that such findings will
   encourage the community to reduce its reliance on accuracy measures to
   compare different architectures and start taking other factors into
   account such as power consumption, simplicity, etc.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Al-Ayyoub, M (Corresponding Author), Jordan Univ Sci \& Technol, Irbid, Jordan.
   Al-Qurran, Raffi; Al-Ayyoub, Mahmoud; Shatnawi, Ali, Jordan Univ Sci \& Technol, Irbid, Jordan.},
DOI = {10.1007/s11042-022-12695-5},
EarlyAccessDate = {APR 2022},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Convolutional neural networks; Green AI; Energy consumption; iNaturlist;
   Herbarium},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {rlalqurran11@xcit.just.edu.jo
   maalshbool@just.edu.jo
   ali@just.edu.jo},
Affiliations = {Jordan University of Science \& Technology},
ResearcherID-Numbers = {Al-Qurran, Raffi/GYU-3687-2022},
Funding-Acknowledgement = {Deanship of Research at the Jordan University of Science and Technology
   {[}20180544]},
Funding-Text = {We gratefully acknowledge the support of the Deanship of Research at the
   Jordan University of Science and Technology for supporting this work via
   Grant \#20180544.},
Cited-References = {Al-Qurran R, 2018, INT ARAB CONF INF TE, P148.
   Angelova A, 2013, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2013.110.
   Carneiro T, 2018, IEEE ACCESS, V6, P61677, DOI 10.1109/ACCESS.2018.2874767.
   Chai YN, 2011, IEEE I CONF COMP VIS, P2579, DOI 10.1109/ICCV.2011.6126546.
   Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318.
   Chen B., 2019, ARXIV PREPRINT ARXIV.
   El Massi I, 2016, LECT NOTES COMPUT SC, V9680, P40, DOI 10.1007/978-3-319-33618-3\_5.
   El Massie I, 2016, I C COMP GRAPH IM VI, P131, DOI 10.1109/CGiV.2016.34.
   Es-Saady Y, 2016, 2016 INTERNATIONAL CONFERENCE ON ELECTRICAL AND INFORMATION TECHNOLOGIES (ICEIT), P561, DOI 10.1109/EITech.2016.7519661.
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Huang G., 2017, P 2017 IEEE C COMP V, P4700, DOI {[}DOI 10.1109/CVPR.2017.243, 10.1109/CVPR.2017.243].
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Kanan C, 2010, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2010.5539947.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Larese MG, 2014, EXPERT SYST APPL, V41, P4638, DOI 10.1016/j.eswa.2014.01.029.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Liu YY, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON FUNCTIONAL-STRUCTURAL PLANT GROWTH MODELING, SIMULATION, VISUALIZATION AND APPLICATIONS (FSPMA), P110, DOI 10.1109/FSPMA.2016.7818296.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Nilsback ME, 2009, THESIS OXFORD U OXFO.
   Ouhami M., 2020, INT C IM SIGN PROC, P65.
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191.
   Qingfeng W, 2007, ADV ARTIFICIAL INTEL.
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131.
   Russell S. J., 2020, ARTIF INTELL, V4.
   Schwartz R, 2020, COMMUN ACM, V63, P54, DOI 10.1145/3381831.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Sunderhauf Niko, 2014, P CLEF WORK NOT, P756.
   Szegedy C., 2015, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2015.7298594.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Toma A., 2017, CLEF WORKING NOTES.
   Wittmann, 2019, TIMELINE TRANSFER LE.
   Yalcin H, 2016, INT CONF AGRO-GEOINF, P233.},
Number-of-Cited-References = {34},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {3W1PB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000779045500012},
DA = {2023-08-12},
}

@article{ WOS:000386645800004,
Author = {Cao, Jie and Wang, Bin and Brown, Douglas},
Title = {Similarity based leaf image retrieval using multiscale R-angle
   description},
Journal = {INFORMATION SCIENCES},
Year = {2016},
Volume = {374},
Pages = {51-64},
Month = {DEC 20},
Abstract = {Leaf image identification is a significant and challenging application
   of computer vision and image processing. A central issue associated with
   this task is how to effectively and efficiently describe the leaf images
   and measure their similarities. In this paper, a novel shape descriptor
   termed R-angle is proposed. R-angle describes the curvature of the
   contour by measuring the angle between the intersections of the shape
   contour with a circle of radius R centered at points sampled around the
   contour. It is intrinsically invariant to group transforms including
   scaling, rotation and translation. Varying the parameter R of the
   proposed R-angle naturally introduces the notation of scale, which we
   leverage to provide a coarse-to-fine description of the local curvature.
   A local scale arrangement is proposed by taking the distance between
   each contour point and the center of the shape to be the maximum scale
   for a given contour point. Two matching schemes, including L-1-norm
   matching and dynamic programming based matching, are applied to measure
   the similarities of the leaf shapes. The retrieval experiments conducted
   on two challenging leaf image datasets indicate that the proposed method
   significantly outperforms the state-of-the-art methods for leaf
   identification. An additional experiment on an animal dataset also
   indicates its potential for general shape recognition. (C) 2016 Elsevier
   Inc. All rights reserved.},
Publisher = {ELSEVIER SCIENCE INC},
Address = {STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA},
Type = {Article},
Language = {English},
Affiliation = {Wang, B (Corresponding Author), Nanjing Univ Finance \& Econ, Sch Informat Engn, Nanjing 210023, Jiangsu, Peoples R China.
   Cao, Jie; Wang, Bin, Nanjing Univ Finance \& Econ, Sch Informat Engn, Nanjing 210023, Jiangsu, Peoples R China.
   Brown, Douglas, Griffith Univ, Sch Engn, Nathan, Qld 4111, Australia.},
DOI = {10.1016/j.ins.2016.09.023},
ISSN = {0020-0255},
EISSN = {1872-6291},
Keywords = {Plant identification; Shape description; Invariant features; Shape
   matching; Leaf image retrieval},
Keywords-Plus = {CURVATURE SCALE-SPACE; SHAPE REPRESENTATION; IDENTIFICATION;
   CLASSIFICATION; RECOGNITION; TEXTURE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems},
Author-Email = {wangbin@njue.edu.cn},
Affiliations = {Nanjing University of Finance \& Economics; Griffith University},
ResearcherID-Numbers = {Cao, Jie/AAD-1518-2019
   },
ORCID-Numbers = {Cao, Jie/0000-0002-9942-3243
   Wang, Bin/0000-0002-9730-374X},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}61372158]; National Key
   Research and Development Program of China {[}2016YFB1000901]; Natural
   Science Foundation of Jiangsu Province {[}BK20141487]; ``333{''}
   Foundation for High Level Talents of Jiangsu Province {[}BRA2015351];
   Priority Academic Program Development of Jiangsu Higher Education
   Institutions (PAPD); Policy Guidance Program (Cooperation of Industry,
   Education and Academy) of Jiangsu Province {[}BY2016009-03]},
Funding-Text = {This work was partially supported by the National Natural Science
   Foundation of China (Grant No. 61372158), the National Key Research and
   Development Program of China under Grant 2016YFB1000901, the Natural
   Science Foundation of Jiangsu Province (Grant No. BK20141487), the
   ``333{''} Foundation for High Level Talents of Jiangsu Province (Grant
   No. BRA2015351), the Project Funded by the Priority Academic Program
   Development of Jiangsu Higher Education Institutions (PAPD) and the
   Policy Guidance Program (Cooperation of Industry, Education and Academy)
   of Jiangsu Province (Grant No. BY2016009-03).},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Abbasi S, 1999, MULTIMEDIA SYST, V7, P467, DOI 10.1007/s005300050147.
   Alajlan N, 2007, PATTERN RECOGN, V40, P1911, DOI 10.1016/j.patcog.2006.12.005.
   Andalo FA, 2010, PATTERN RECOGN, V43, P26, DOI 10.1016/j.patcog.2009.06.012.
   {[}Anonymous], 2012, PLANT ID 2012.
   Bai X, 2009, IEEE I CONF COMP VIS, P575, DOI 10.1109/ICCV.2009.5459188.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cormen Thomas H., 2001, INTRO ALGORITHMS, V2nd.
   Du JX, 2006, T I MEAS CONTROL, V28, P275, DOI 10.1191/0142331206tim176oa.
   Goncalves WN, 2016, INFORM SCIENCES, V364, P51, DOI 10.1016/j.ins.2016.04.052.
   Govaerts R, 2001, TAXON, V50, P1085, DOI 10.2307/1224723.
   Horaisova K, 2016, BIOSYST ENG, V142, P83, DOI 10.1016/j.biosystemseng.2015.12.007.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Jia Y., 2014, ACM INT C MULT, DOI {[}DOI 10.1145/2647868.2654889, 10.1145/2647868.2654889].
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee CL, 2006, INT J IMAG SYST TECH, V16, P15, DOI 10.1002/ima.20063.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Manay S, 2006, IEEE T PATTERN ANAL, V28, P1602, DOI 10.1109/TPAMI.2006.208.
   Manning CD, 2008, INTRO INFORM RETRIEV.
   MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591.
   Mokhtarian F, 2004, IEEE T IMAGE PROCESS, V13, P653, DOI 10.1109/TIP.2004.826126.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Petrakis EGM, 2002, IEEE T PATTERN ANAL, V24, P1501, DOI 10.1109/TPAMI.2002.1046166.
   Soderkvist O., 2001, THESIS.
   Sun KB, 2005, PROC CVPR IEEE, P727.
   Wang B, 2015, INFORM SCIENCES, V302, P132, DOI 10.1016/j.ins.2014.07.028.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008.
   Zhang DS, 2003, J VIS COMMUN IMAGE R, V14, P41, DOI 10.1016/S1047-3203(03)00003-8.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhao ZQ, 2015, NEUROCOMPUTING, V151, P1112, DOI 10.1016/j.neucom.2014.02.077.},
Number-of-Cited-References = {40},
Times-Cited = {34},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {30},
Journal-ISO = {Inf. Sci.},
Doc-Delivery-Number = {EA5FW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000386645800004},
DA = {2023-08-12},
}

@article{ WOS:000552767400002,
Author = {Kamath, Radhika and Balachandra, Mamatha and Prabhu, Srikanth},
Title = {Paddy Crop and Weed Discrimination: A Multiple Classifier System
   Approach},
Journal = {INTERNATIONAL JOURNAL OF AGRONOMY},
Year = {2020},
Volume = {2020},
Month = {JUN 29},
Abstract = {Weeds are unwanted plants that grow among crops. These weeds can
   significantly reduce the yield and quality of the farm output.
   Unfortunately, site-specific weed management is not followed in most of
   the cases. That is, instead of treating a field with a specific type of
   herbicide, the field is treated with a broadcast herbicide application.
   This broadcast application of the herbicide has resulted in
   herbicide-resistant weeds and has many ill effects on the natural
   environment. This has prompted many research studies to seek the most
   effective weed management techniques. One such technique is computer
   vision-based automatic weed detection and identification. Using this
   technique, weeds can be detected and identified and a suitable herbicide
   can be recommended to the farmers. Therefore, it is important for the
   computer vision technique to successfully identify and classify the
   crops and weeds from the digital images. This paper investigates the
   multiple classifier systems built using support vector machines and
   random forest classifiers for plant classification in classifying paddy
   crops and weeds from digital images. Digital images of paddy crops and
   weeds from the paddy fields were acquired using three different cameras
   fixed at different heights from the ground. Texture, color, and shape
   features were extracted from the digital images after background
   subtraction and used for classification. A simple and new method was
   used as a decision function in the multiple classifier systems. An
   accuracy of 91.36\% was obtained by the multiple classifier systems and
   was found to outperform single classifier systems.},
Publisher = {HINDAWI LTD},
Address = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Balachandra, M (Corresponding Author), Manipal Acad Higher Educ, Manipal Inst Technol, Dept Comp Sci \& Engn, Manipal, India.
   Kamath, Radhika; Balachandra, Mamatha; Prabhu, Srikanth, Manipal Acad Higher Educ, Manipal Inst Technol, Dept Comp Sci \& Engn, Manipal, India.},
DOI = {10.1155/2020/6474536},
Article-Number = {6474536},
ISSN = {1687-8159},
EISSN = {1687-8167},
Keywords-Plus = {COMPUTER VISION; AGRICULTURE; COMBINATION; MACHINE; RICE},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agronomy},
Author-Email = {radhika.kamath@manipal.edu
   mamtha.bc@manipal.edu
   srikanth.prabhu@manipal.edu},
Affiliations = {Manipal Academy of Higher Education (MAHE)},
ResearcherID-Numbers = {Prabhu, Srikanth Srinivas/B-7778-2017
   balachandra, mamatha/B-5105-2017
   Prabhu, srikanth srinivas/ABH-5153-2020},
ORCID-Numbers = {balachandra, mamatha/0000-0003-2201-8730
   Prabhu, srikanth srinivas/0000-0002-3826-1084},
Cited-References = {Albon C., 2018, MACHINE LEARNING PYT.
   {[}Anonymous], INDIA J AGR SCI.
   {[}Anonymous], 2019, AGR WEED MANAGEMENT.
   {[}Anonymous], 2019, INDIA BIODIVERSITY M.
   Araujo V, 2017, IEEE SYS MAN CYBERN, P1880, DOI 10.1109/SMC.2017.8122891.
   Balducci F, 2018, MACHINES, V6, DOI 10.3390/machines6030038.
   BARRETT SCH, 1983, ECON BOT, V37, P255, DOI 10.1007/BF02858881.
   Bostrom H, 2008, SEVENTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P121, DOI 10.1109/ICMLA.2008.107.
   Butler HK, 2018, J ALGORITHMS COMPUT, V12, P187, DOI 10.1177/1748301818761132.
   Chang CL, 2018, ROBOTICS, V7, DOI 10.3390/robotics7030038.
   Chen GQ, 2017, AGRON J, V109, P620, DOI 10.2134/agronj2016.06.0348.
   Cheng BL, 2015, PROCEEDINGS OF THE 2015 NORTHEAST ASIA INTERNATIONAL SYMPOSIUM ON LINGUISTICS, LITERATURE AND TEACHING, VOL I, P51.
   Choi SS., 2010, J SYST CYBERN INF, V8, P43.
   Cohen I, 2004, LECT NOTES ARTIF INT, V3202, P125.
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104.
   De Bock K. W., 2018, ENSEMBLE CLASSIFICAT, P175.
   Fagundes D., 2006, P 2006 9 BRAZ S NEUR, P12.
   Ferri C, 2009, PATTERN RECOGN LETT, V30, P27, DOI 10.1016/j.patrec.2008.08.010.
   Gargiulo F., 2013, INTELLIGENT SYSTEMS, P335.
   Gharde Y, 2018, CROP PROT, V107, P12, DOI 10.1016/j.cropro.2018.01.007.
   Gonzales R.C., 2001, DIGITAL IMAGE PROCES, Vsecond.
   Guo LB, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01067-5.
   Ji B, 2009, 2009 SECOND INTERNATIONAL SYMPOSIUM ON KNOWLEDGE ACQUISITION AND MODELING: KAM 2009, VOL 3, P330, DOI 10.1109/KAM.2009.231.
   Kamath R., 2018, INT J ENG TECHNOLOGY, V7, P2909, DOI DOI 10.14419/IJET.V7I4.21511.
   Kamath R, 2020, INT J AGR BIOL ENG, V13, P191, DOI 10.25165/j.ijabe.20201301.4920.
   Kamath R, 2019, IEEE ACCESS, V7, P45110, DOI 10.1109/ACCESS.2019.2908846.
   Kleiman R., 2019, INT C MACH LEARN, P3439.
   Koziarski M, 2017, PATTERN ANAL APPL, V20, P981, DOI 10.1007/s10044-017-0655-2.
   Kuncheva LI, 2000, KES'2000: FOURTH INTERNATIONAL CONFERENCE ON KNOWLEDGE-BASED INTELLIGENT ENGINEERING SYSTEMS \& ALLIED TECHNOLOGIES, VOLS 1 AND 2, PROCEEDINGS, P185, DOI 10.1109/KES.2000.885788.
   LAWS KI, 1979, P IM UND WORKSH, P47, DOI DOI 10.111141600-0846.2009.00354.X.
   Li JL, 2008, BIOSTATISTICS, V9, P566, DOI 10.1093/biostatistics/kxm050.
   Lin FF, 2017, SUSTAINABILITY-BASEL, V9, DOI 10.3390/su9081335.
   Ma X, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0215676.
   Masuda R., 2010, P 17 WORLD C INT COM.
   Mavridou E, 2019, J IMAGING, V5, DOI 10.3390/jimaging5120089.
   Mukherjee P.K., 2008, INDIAN J WEED SCI, V40, P147.
   Niculescu-Mizil A., 2015, PROC 22 INT C MACH L, P625, DOI 10.1145/1102351.1102430.
   Panchard J, 2007, INF TECHNOL INT DEV, V4, P51, DOI 10.1162/itid.2007.4.1.51.
   Parameswari Y. S., 2017, INT J APPL PURE SCI, V3.
   Patricio DI, 2018, COMPUT ELECTRON AGR, V153, P69, DOI 10.1016/j.compag.2018.08.001.
   Platt JC, 2000, ADV NEUR IN, P61.
   Rajagopalachari Kothari C., 2004, REMETHODOLOGY METH.
   Ranawana R., 2006, INT J HYBRID INTELL, V3, P35, DOI {[}10.3233/his-2006-3104, DOI 10.3233/HIS-2006-3104].
   Rao AN, 2015, WEED TECHNOL, V29, P1, DOI 10.1614/WT-D-14-00057.1.
   Roli F., 2009, MULTIPLE CLASSIFIER.
   Singh Vijai, 2017, Information Processing in Agriculture, V4, P41, DOI 10.1016/j.inpa.2016.10.005.
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002.
   Tian HongKun, 2020, Information Processing in Agriculture, V7, P1, DOI 10.1016/j.inpa.2019.09.006.
   Tian Y, 2011, INTELL AUTOM SOFT CO, V17, P519, DOI 10.1080/10798587.2011.10643166.
   Tripathy A. K., 2011, Proceedings of the 2011 World Congress on Information and Communication Technologies (WICT), P1229, DOI 10.1109/WICT.2011.6141424.
   Wozniak M, 2014, INFORM FUSION, V16, P3, DOI 10.1016/j.inffus.2013.04.006.
   Yang Y, 2018, INT CONF CONTR AUTO, P55, DOI 10.1109/ICCAIS.2018.8570328.
   Zhu YQ, 2011, NEURAL COMPUT APPL, V20, P309, DOI 10.1007/s00521-010-0372-x.},
Number-of-Cited-References = {53},
Times-Cited = {11},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Int. J. Agron.},
Doc-Delivery-Number = {MQ2ZY},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000552767400002},
OA = {Green Published},
DA = {2023-08-12},
}

@inproceedings{ WOS:000417413000033,
Author = {Thanh-Binh Do and Huy-Hoang Nguyen and Thi-Thanh-Nhan Nguyen and Hai Vu
   and Thi-Thanh-Hai Tran and Thi-Lan Le},
Editor = {Nguyen, TT and Le, AP and Tojo, S and Nguyen, L and Phan, XH},
Title = {Plant Identification using score-based fusion of multi-organ images},
Booktitle = {2017 9TH INTERNATIONAL CONFERENCE ON KNOWLEDGE AND SYSTEMS ENGINEERING
   (KSE 2017)},
Series = {International Conference on Knowledge and Systems Engineering},
Year = {2017},
Pages = {191-196},
Note = {9th International Conference on Knowledge and Systems Engineering (KSE),
   Hue, VIETNAM, OCT 19-21, 2017},
Organization = {Hue Univ Educ; Hue Univ Sci; VNU Univ Engn \& Technol; Natl Fdn Sci \&
   Technol Dev; IEEE; Hue Univ; Hue Ind Coll; TOPICA AI Lab; QRM},
Abstract = {This paper describes a fusion technique for species identification from
   images of different plant organs. Given a series of image organs such as
   branch, entire, flower or leaf, we firstly extract confidence scores for
   each single organ using a state-of-the-art deep convolutional neural
   network (CNN). After that, we deploy various schemes of the fusion
   approaches including not only conventional transformation-based
   approaches (sum rule, max rule, product rule) but also a
   classification-based approach (support vector machine). Then we proposed
   a hybrid fusion model. To measure the performances of the combination
   schemes, a large number of images of 50 species which are collected from
   two main resources: PlantCLEF 2015 dataset and the Internet resources.
   The experiment exhibits the dominant results of the fusion techniques
   compared with those of individual organs. At rank-1, the highest
   accuracy of a single organ is 73.0\% for flower images, whereas by
   applying our proposed fusion technique for leaf and flower, the accuracy
   reaches to 89.8\%.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Do, TB (Corresponding Author), HUST, CNRS, Int Res Inst MICA, UMI 2954,GRENOBLE INP, Hanoi, Vietnam.
   Thanh-Binh Do; Huy-Hoang Nguyen; Thi-Thanh-Nhan Nguyen; Hai Vu; Thi-Thanh-Hai Tran; Thi-Lan Le, HUST, CNRS, Int Res Inst MICA, UMI 2954,GRENOBLE INP, Hanoi, Vietnam.
   Thanh-Binh Do; Huy-Hoang Nguyen, HUST, Sch Informat \& Commun Technol, Hanoi, Vietnam.
   Thi-Thanh-Nhan Nguyen, Thai Nguyen Univ, Univ Informat \& Commun Technol, ThaiNguyen, Vietnam.},
ISSN = {2164-2508},
ISBN = {978-1-5386-3576-6},
Keywords = {Convolutional Neural Network; Deep Learning; Fusion; Plant
   Identification},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic},
Author-Email = {binhdt.hust@gmail.com},
Affiliations = {Hanoi University of Science \& Technology (HUST); Hanoi University of
   Science \& Technology (HUST); Thai Nguyen University},
ResearcherID-Numbers = {Le, Thi-Lan/AAA-5855-2020},
Funding-Acknowledgement = {ASEAN University Network (Aun-Seed/Net) {[}HUST/CRC/1501]},
Funding-Text = {The authors thank Collaborative Research Program for Common Regional
   Issue (CRC) funded by ASEAN University Network (Aun-Seed/Net), under the
   grant reference HUST/CRC/1501.},
Cited-References = {Aptoula E, 2013, IEEE IMAGE PROC, P1496, DOI 10.1109/ICIP.2013.6738307.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Chen Q, 2014, CLEF WORKING NOTES, P693.
   Ghazi M.M., 2017, NEUROCOMPUTING.
   Goeau H., 2012, P 1 ACM INT WORKSH M, P41.
   Goeau H., 2015, CLEF C LABS EV FOR C, V1391.
   Gu X, 2005, LECT NOTES COMPUT SC, V3644, P253.
   He A., 2016, 2016 IEEE INT C SYST.
   Hussein A., 2011, THESIS.
   James SC, 2012, EXPERT SYSTEMS APPL, V39, P7562.
   Jyotismita C., 2011, INT J ADV COMPUT SC, V2, P10.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Le T. L., 2015, P 2 INT WORKSHOP ENV, DOI {[}10.1145/2764873.2764877, DOI 10.1145/2764873.2764877].
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   LUNAU K, 1991, ETHOLOGY, V88, P203, DOI 10.1111/j.1439-0310.1991.tb00275.x.
   Makihara Yasushi, 2013, 2013 10 IEEE INT C W, P1.
   ROSS A, 2005, DEFENSE AND SECURITY, V5779, P196, DOI DOI 10.1117/12.606093.
   Ross AA, 2006, HDB MULTIBIOMETRICS, V6.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Nguyen TTN, 2017, STUD COMPUT INTELL, V710, P223, DOI 10.1007/978-3-319-56660-3\_20.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.
   Yoo H. J., 2015, IEIE T SMART PROCESS, V4, P35, DOI {[}10.5573/ieiespc.2015.4.1.035, DOI 10.5573/IEIESPC.2015.4.1.035].},
Number-of-Cited-References = {25},
Times-Cited = {12},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BJ1CX},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000417413000033},
DA = {2023-08-12},
}

@article{ WOS:000954097400001,
Author = {Cao, Jie and Wu, Zhengmin and Zhang, Xuechen and Luo, Kun and Zhao, Bo
   and Sun, Changying},
Title = {Sorting of Fresh Tea Leaf Using Deep Learning and Air Blowing},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2023},
Volume = {13},
Number = {6},
Month = {MAR},
Abstract = {The sorting of machine-picked fresh tea leaves after mechanized
   harvesting remains a challenge because of the complex morphological
   characteristics and physicochemical properties of fresh tea leaves.
   First, the recognition results of four types of models, namely, YOLOv5,
   YOLOv3, Fast RCNN, and SSD, were compared. It was found that YOLOv5,
   with guaranteed recognition accuracy, had a recognition speed of 4.7
   ms/frame (about four times that of the second ranked YOLOv3). Therefore,
   this study presents a novel fresh tea leaf sorting system that provides
   rapid and high-precision multi-channel sorting for four grades of tea
   leaves using a tea leaf recognition model based on the You Only Look
   Once (YOLOv5) deep learning model. Subsequently, a raw dataset,
   consisting of 6400 target images of different grades and different
   moisture contents, was used to evaluate three different optimization
   methods. Among these, the Stochastic Gradient Descent (SGD) optimization
   method was found to provide the best model training results with an
   average recognition accuracy of 98.2\%. In addition, the recognition
   efficacy of the recognition model was found to be positively correlated
   with the gradient coverage of tea's moisture content in the training
   set. Theoretical analysis was then conducted, along with the
   experimental investigation of the air-blowing force on the fresh tea
   leaves in the sorting process, with 30 degrees determined to be the
   optimal air-blowing angle. Finally, the overall results showed that the
   construction of the full moisture content training set enabled a model
   recognition accuracy of up to 88.8\%, a recall of 88.4\%, a recognition
   speed of 4.7 ms/frame, and an overall sorting accuracy of 85.4\%. This
   result is promising for multi-channel sorting of fresh tea leaf grades
   in complex situations, and as such provides a strong basis for the
   application of tea leaf sorting equipment.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Wu, ZM (Corresponding Author), Anhui Agr Univ, Sch Tea \& Food Sci \& Technol, Hefei 230036, Peoples R China.
   Wu, ZM (Corresponding Author), State Key Lab Tea Plant Biol \& Utilizat, Hefei 230036, Peoples R China.
   Cao, Jie; Wu, Zhengmin; Zhao, Bo; Sun, Changying, Anhui Agr Univ, Sch Tea \& Food Sci \& Technol, Hefei 230036, Peoples R China.
   Wu, Zhengmin; Sun, Changying, State Key Lab Tea Plant Biol \& Utilizat, Hefei 230036, Peoples R China.
   Zhang, Xuechen; Luo, Kun, Anhui Agr Univ, Sch Engn, Hefei 230036, Peoples R China.},
DOI = {10.3390/app13063551},
Article-Number = {3551},
EISSN = {2076-3417},
Keywords = {tea; sorting; YOLOv5s; air blowing; moisture content},
Research-Areas = {Chemistry; Engineering; Materials Science; Physics},
Web-of-Science-Categories  = {Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied},
Author-Email = {wzmin@ahau.edu.cn},
Affiliations = {Anhui Agricultural University; Anhui Agricultural University},
Funding-Acknowledgement = {National Key Research and Development Program of China
   {[}2021YFD1601102]; National Natural Science Foundation of China
   {[}52105239]; Anhui Provincial Education Department Key Projects
   {[}KJ2020A0133]},
Funding-Text = {This work was supported by National Key Research and Development Program
   of China (2021YFD1601102), the National Natural Science Foundation of
   China (Grant No.52105239), and Anhui Provincial Education Department Key
   Projects (KJ2020A0133).},
Cited-References = {Albarrak K, 2022, SUSTAINABILITY-BASEL, V14, DOI 10.3390/su14106339.
   Bao WX, 2023, COMPUT ELECTRON AGR, V205, DOI 10.1016/j.compag.2023.107637.
   Chen YT, 2020, COMPUT ELECTRON AGR, V171, DOI 10.1016/j.compag.2020.105298.
   Chen ZW, 2020, J FOOD PROCESS ENG, V43, DOI 10.1111/jfpe.13474.
   Dong CW, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-28767-2.
   Gan N, 2022, J SCI FOOD AGR, V102, P6858, DOI 10.1002/jsfa.12047.
   {[}高震宇 Gao Zhenyu], 2017, {[}农业机械学报, Transactions of the Chinese Society for Agricultural Machinery], V48, P53.
   Gui ZY, 2023, COMPUT ELECTRON AGR, V205, DOI 10.1016/j.compag.2023.107636.
   Gulzar Y, 2023, SUSTAINABILITY-BASEL, V15, DOI 10.3390/su15031906.
   Gulzar Y, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12122018.
   Hu GS, 2021, COMPUT ELECTR ENG, V90, DOI 10.1016/j.compeleceng.2021.107023.
   Hu GS, 2019, SUSTAIN COMPUT-INFOR, V24, DOI 10.1016/j.suscom.2019.100353.
   Huang H., 2022, NATURE, V53, P399.
   Liming X, 2010, COMPUT ELECTRON AGR, V71, pS32, DOI 10.1016/j.compag.2009.09.013.
   Loddo A, 2021, COMPUT ELECTRON AGR, V187, DOI 10.1016/j.compag.2021.106269.
   Lu AE, 2022, BIOSYST ENG, V221, P118, DOI 10.1016/j.biosystemseng.2022.06.015.
   Luo K, 2022, AGRICULTURE-BASEL, V12, DOI 10.3390/agriculture12091361.
   Mamat N, 2023, SUSTAINABILITY-BASEL, V15, DOI 10.3390/su15020901.
   Parico AIB, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21144803.
   Sofu MM, 2016, COMPUT ELECTRON AGR, V127, P395, DOI 10.1016/j.compag.2016.06.030.
   Tang X., 2015, J NANOPART RES, V2, P5.
   Thuyet DQ, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105727.
   Wu Z., 2020, THESIS ANHUI AGR U H, DOI {[}10.26919/d.cnki.gannu.2020.000038, DOI 10.26919/D.CNKI.GANNU.2020.000038].
   Yan G., 2022, INTEGR CIRC APPL, V39, P176, DOI {[}10.19339/j.issn.1674-2583.2022.01.071, DOI 10.19339/J.ISSN.1674-2583.2022.01.071].
   Yang J., 2011, NANO ENERGY, V2, P41.
   Yuan H., 2011, CHIN INT CONF ELECTR, V33, P19.
   Zhang LanLan, 2012, Journal of Zhejiang University (Agriculture and Life Sciences), V38, P593.
   Zhang X., 2017, J ZHEJIANG UNIV-SC A, V45, P125.
   Zhu HK, 2019, J FOOD ENG, V263, P165, DOI 10.1016/j.jfoodeng.2019.06.009.},
Number-of-Cited-References = {29},
Times-Cited = {1},
Usage-Count-Last-180-days = {11},
Usage-Count-Since-2013 = {11},
Journal-ISO = {Appl. Sci.-Basel},
Doc-Delivery-Number = {A3IH0},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000954097400001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000580156300001,
Author = {Chen, Xueshen and Mao, Yuanyang and Ma, Xu and Qi, Long},
Title = {A Tactile Method for Rice Plant Recognition Based on Machine Learning},
Journal = {SENSORS},
Year = {2020},
Volume = {20},
Number = {18},
Month = {SEP},
Abstract = {Accurate and real-time recognition of rice plants is the premise
   underlying the implementation of precise weed control. However,
   achieving desired results in paddy fields using the traditional visual
   method is difficult due to the occlusion of rice leaves and the
   interference of weeds. The objective of this study was to develop a
   novel rice plant recognition sensor based on a tactile method which
   acquires tactile information through physical touch. The tactile sensor
   would be mounted on the paddy field weeder to provide identification
   information for the actuator. First, a flexible gasbag filled with air
   was developed, where vibration features produced by tactile and sliding
   feedback were acquired when this apparatus touched rice plants or weeds,
   allowing the subtle vibration data with identification features to be
   reflected through the voltage value of an air-pressured sensor mounted
   inside the gasbag. Second, voltage data were preprocessed by three
   algorithms to optimize recognition features, including dimensional
   feature, dimensionless feature, and fractal dimension. The three types
   of features were used to train and test a neural network classifier. To
   maximize classification accuracy, an optimum set of features (b
   (variance), f (kurtosis), h (waveform factor), l (box dimension), and m
   (Hurst exponent)) were selected using a genetic algorithm. Finally, the
   feature-optimized classifier was trained, and the actual performances of
   the sensor at different contact positions were tested. Experimental
   results showed that the recognition rates of the end, middle, and root
   of the sensor were 90.67\%, 98\%, and 96\% respectively. A tactile-based
   method with intelligence could produce high accuracy for rice plant
   recognition, as demonstrated in this study.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Ma, X (Corresponding Author), South China Agr Univ, Coll Engn, Guangzhou 510642, Peoples R China.
   Chen, Xueshen; Mao, Yuanyang; Ma, Xu; Qi, Long, South China Agr Univ, Coll Engn, Guangzhou 510642, Peoples R China.},
DOI = {10.3390/s20185135},
Article-Number = {5135},
EISSN = {1424-8220},
Keywords = {rice; weeds; recognition; tactile; ANN},
Keywords-Plus = {CROP ROWS; AUTOMATIC DETECTION; WEED MANAGEMENT; SYSTEM; DESIGN},
Research-Areas = {Chemistry; Engineering; Instruments \& Instrumentation},
Web-of-Science-Categories  = {Chemistry, Analytical; Engineering, Electrical \& Electronic;
   Instruments \& Instrumentation},
Author-Email = {chenxs@scau.edu.cn
   mskobe@stu.scau.edu.cn
   maxu1959@scau.edu.cn
   qilong@scau.edu.cn},
Affiliations = {South China Agricultural University},
ORCID-Numbers = {Mao, Yuanyang/0000-0002-4569-0573},
Funding-Acknowledgement = {Key-Area Research and Development Program of Guangdong Province
   {[}2019B020221002]; Science and Technology Program of Guangzhou
   {[}201803020021]; Science Foundation of Guangdong for Distinguished
   Young Scholars {[}2019B151502056]; National Science Foundation for Young
   Scientists of China {[}31801258]},
Funding-Text = {The research was funded by the Key-Area Research and Development Program
   of Guangdong Province, grant number 2019B020221002, the Science and
   Technology Program of Guangzhou, grant number 201803020021, the Science
   Foundation of Guangdong for Distinguished Young Scholars, grant number
   2019B151502056, and the National Science Foundation for Young Scientists
   of China, grant number 31801258.},
Cited-References = {Adler R., 1985, IEEE 1985 Ultrasonics Symposium. Proceedings. (Cat. No.85CH2209-5), P499.
   Alfadhel A, 2015, ADV MATER, V27, P7888, DOI 10.1002/adma.201504015.
   Andujar D, 2013, SENSORS-BASEL, V13, P14662, DOI 10.3390/s131114662.
   Bakker T, 2008, COMPUT ELECTRON AGR, V60, P87, DOI 10.1016/j.compag.2007.07.006.
   Charlebois M, 2000, J ROBOTIC SYST, V17, P643, DOI 10.1002/1097-4563(200012)17:12<643::AID-ROB1>3.0.CO;2-8.
   {[}陈学深 Chen Xueshen], 2020, {[}农业机械学报, Transactions of the Chinese Society for Agricultural Machinery], V51, P45.
   Cheng BL, 2015, PROCEEDINGS OF THE 2015 NORTHEAST ASIA INTERNATIONAL SYMPOSIUM ON LINGUISTICS, LITERATURE AND TEACHING, VOL I, P51.
   Chhokar RS, 2014, CROP PROT, V64, P7, DOI 10.1016/j.cropro.2014.05.016.
   Cordill C, 2011, BIOSYST ENG, V110, P247, DOI 10.1016/j.biosystemseng.2011.07.007.
   Gui Y., 2019, J LUOYANG I SCI TECH, V29, P78.
   HOWE RD, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P145, DOI 10.1109/ROBOT.1989.99981.
   Huang YB, 2016, INT J AGR BIOL ENG, V9, P98, DOI 10.3965/j.ijabe.20160902.2137.
   Hung C, 2014, REMOTE SENS-BASEL, V6, P12037, DOI 10.3390/rs61212037.
   Jia HongLei, 2018, Transactions of the Chinese Society of Agricultural Engineering, V34, P15.
   Jiang GQ, 2015, EXPERT SYST APPL, V42, P2429, DOI 10.1016/j.eswa.2014.10.033.
   Kim K, 2009, SENSOR ACTUAT A-PHYS, V156, P284, DOI 10.1016/j.sna.2009.08.015.
   Longchamps L, 2010, PRECIS AGRIC, V11, P181, DOI 10.1007/s11119-009-9126-0.
   Maheshwari V, 2006, SCIENCE, V312, P1501, DOI 10.1126/science.1126216.
   Montalvo M, 2012, EXPERT SYST APPL, V39, P11889, DOI 10.1016/j.eswa.2012.02.117.
   Nadafzadeh M, 2019, PRECIS AGRIC, V20, P857, DOI 10.1007/s11119-018-9618-x.
   Noda K, 2014, SENSOR ACTUAT A-PHYS, V215, P123, DOI 10.1016/j.sna.2013.09.031.
   Norremark M, 2008, BIOSYST ENG, V101, P396, DOI 10.1016/j.biosystemseng.2008.09.007.
   Ozaki K., 1994, P INT C INSTR CONTR, P595.
   Perez-Ortiz M, 2015, APPL SOFT COMPUT, V37, P533, DOI 10.1016/j.asoc.2015.08.027.
   Reiser D, 2017, J IMAGING, V3, DOI 10.3390/jimaging3010009.
   Rodrigo MA, 2014, CHEM REV, V114, P8720, DOI 10.1021/cr500077e.
   Ruan XM, 2020, J INFORMETR, V14, DOI 10.1016/j.joi.2020.101039.
   Russell R.A., 1993, P 1993 IEEE INT C RO.
   Seminara L, 2013, IEEE SENS J, V13, P4022, DOI 10.1109/JSEN.2013.2268690.
   Shapira U, 2013, INT J REMOTE SENS, V34, P6094, DOI 10.1080/01431161.2013.793860.
   Sogaard HT, 2003, COMPUT ELECTRON AGR, V38, P141, DOI 10.1016/S0168-1699(02)00140-0.
   Sun Q., 2018, MACH TOOL HYDRAUL, V46, P133, DOI {[}10.3969/j.issn.1001-3881.2018.07.030, DOI 10.3969/J.ISSN.1001-3881.2018.07.030].
   Taghadomi-Saberi S., 2015, Agricultural Engineering International: CIGR Journal, V17, P92.
   Tan X., 2014, MACH TOOL HYDRAUL, V42, P187.
   Tanaka M., 2002, Sixth International Conference on Motion and Vibration Control. Proceedings, P762.
   Tillett ND, 2008, BIOSYST ENG, V99, P171, DOI 10.1016/j.biosystemseng.2007.09.026.
   Tshewang S, 2016, CROP PROT, V90, P117, DOI 10.1016/j.cropro.2016.08.031.
   Wang HP, 2014, IEEE SENS J, V14, P55, DOI 10.1109/JSEN.2013.2279394.
   Wang S, 2019, WEAR, V426, P1761, DOI 10.1016/j.wear.2018.12.087.
   {[}吴莉莉 Wu Lili], 2018, {[}农业工程学报, Transactions of the Chinese Society of Agricultural Engineering], V34, P175.
   Wu Y, 2018, EBIOMEDICINE, V34, P27, DOI 10.1016/j.ebiom.2018.07.006.
   Zhang S., 2020, COMPUT ENG DES, V41, P2177.
   {[}张铁民 Zhang Tiemin], 2019, {[}农业工程学报, Transactions of the Chinese Society of Agricultural Engineering], V35, P168.
   Zhou Z, 2019, EUR J IMMUNOL, V49, P23.
   Zwiggelaar R, 1998, CROP PROT, V17, P189, DOI 10.1016/S0261-2194(98)00009-X.},
Number-of-Cited-References = {45},
Times-Cited = {2},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {34},
Journal-ISO = {Sensors},
Doc-Delivery-Number = {OD9HV},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000580156300001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000739262100001,
Author = {Yang, Ziying and He, Wenyan and Fan, Xijian and Tjahjadi, Tardi},
Title = {PlantNet: transfer learning-based fine-grained network for
   high-throughput plants recognition},
Journal = {SOFT COMPUTING},
Year = {2022},
Volume = {26},
Number = {20, SI},
Pages = {10581-10590},
Month = {OCT},
Abstract = {In high-throughput phenotyping, recognizing individual plant categories
   is a vital support process for plant breeding. However, different plant
   categories have different fine-grained characteristics, i.e.,
   intra-class variation and inter-class similarity, making the process
   challenging. Existing deep learning-based recognition methods fail to
   effectively address this recognition task under challenging
   requirements, leading to technical difficulties such as low accuracy and
   lack of generalization robustness. To address these requirements, this
   paper proposes PlantNet, a fine-grained network for plant recognition
   based on transfer learning and a bilinear convolutional neural network,
   which achieves high recognition accuracy in high-throughput phenotyping
   requirements. The network operates as follows. First, two deep feature
   extractors are constructed using transfer learning. The outer product of
   the different spatial locations corresponding to the two features is
   then calculated, and the bilinear convergence is computed for the
   different spatial locations. Finally, the fused bilinear vectors are
   normalized via maximum expectation to generate the network output.
   Experiments on a publicly available Arabidopsis dataset show that the
   proposed bilinear model performed better than related state-of-the-art
   methods. The interclass recognition accuracy of the four different
   species of Arabidopsis Sf-2, Cvi, Landsberg and Columbia are found to be
   98.48\%, 96.53\%, 96.79\% and 97.33\%, respectively, with an average
   accuracy of 97.25\%. Thus, the network has good generalization ability
   and robust performance, satisfying the needs of fine-grained plant
   recognition in agricultural production.},
Publisher = {SPRINGER},
Address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
Type = {Article},
Language = {English},
Affiliation = {Fan, XJ (Corresponding Author), Nanjing Forestry Univ, Coll Informat Sci \& Technol, Nanjing, Peoples R China.
   Yang, Ziying; He, Wenyan; Fan, Xijian, Nanjing Forestry Univ, Coll Informat Sci \& Technol, Nanjing, Peoples R China.
   Tjahjadi, Tardi, Univ Warwick, Sch Engn, Coventry CV4 7AL, W Midlands, England.},
DOI = {10.1007/s00500-021-06689-y},
EarlyAccessDate = {JAN 2022},
ISSN = {1432-7643},
EISSN = {1433-7479},
Keywords = {Fine-grained recognition; Convolutional neural network; Bilinear-CNN;
   Transfer learning},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications},
Author-Email = {xijian.fan@njfu.edu.cn},
Affiliations = {Nanjing Forestry University; University of Warwick},
ResearcherID-Numbers = {Fan, Xijian/GRR-2740-2022
   He, Wenyan/AAD-5964-2020},
ORCID-Numbers = {Fan, Xijian/0000-0002-7017-7667
   },
Funding-Acknowledgement = {National Natural Science Foundation of CHINA {[}61902187]; Jiangsu
   Province Innovative and Entrepreneurial Talent Project, Nanjing Forestry
   University High-level Start-up Foundation for Research; National College
   Students Innovation and Entrepreneurship Training Program
   {[}202010298123Y]},
Funding-Text = {The authors acknowledge the National Natural Science Foundation of CHINA
   (Grant No. 61902187), Jiangsu Province Innovative and Entrepreneurial
   Talent Project, Nanjing Forestry University High-level Start-up
   Foundation for Research, and National College Students Innovation and
   Entrepreneurship Training Program (Grant No. 202010298123Y).},
Cited-References = {Albawi S, 2017, I C ENG TECHNOL.
   Chen WR, 1997, SCIENCE, V278, P463, DOI 10.1126/science.278.5337.463.
   Cointault, 2008, AGR BIOSYST ENG SUST.
   Cointault F, 2008, NEW ZEAL J CROP HORT, V36, P117, DOI 10.1080/01140670809510227.
   Cointault F., 2012, TEXTURE COLOR FREQUE, P49.
   Donahue J, 2014, PR MACH LEARN RES, V32.
   Duan LF, 2015, CROP J, V3, P211, DOI 10.1016/j.cj.2015.03.002.
   Elphick CS, 2008, J APPL ECOL, V45, P1313, DOI 10.1111/j.1365-2664.2008.01545.x.
   Farnsworth EJ, 2013, BIOSCIENCE, V63, P891, DOI 10.1525/bio.2013.63.11.8.
   Gao WanLin, 2010, Research of Agricultural Modernization, V31, P257.
   Granier C, 2014, CURR OPIN PLANT BIOL, V18, P96, DOI 10.1016/j.pbi.2014.02.009.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Huang G., 2017, P 2017 IEEE C COMP V, P4700, DOI {[}DOI 10.1109/CVPR.2017.243, 10.1109/CVPR.2017.243].
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Kocher G, 2021, SOFT COMPUT, V25, P9731, DOI 10.1007/s00500-021-05893-0.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kumar G, 2020, J SUPERCOMPUT, V76, P8938, DOI 10.1007/s11227-020-03196-z.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105.
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170.
   Liu NA, 2016, NEUROCOMPUTING, V216, P460, DOI 10.1016/j.neucom.2016.08.005.
   Namin ST, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0333-4.
   O'Shea K., 2015, INTRO CONVOLUTIONAL.
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI {[}10.1109/ICCV.2017.74, 10.1007/s11263-019-01228-7].
   Siebner HR, 2009, NEUROSCIENCE, V164, P1, DOI 10.1016/j.neuroscience.2009.09.009.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Song KY, 2019, IEEE T CIRC SYST VID, V29, P2972, DOI 10.1109/TCSVT.2018.2875449.
   Thakur K, 2021, COMPUT ELECTR ENG, V92, DOI 10.1016/j.compeleceng.2021.107129.
   Thi Thanh-Nhan Nguyen, 2019, International Journal of Machine Learning and Computing, V9, P26, DOI 10.18178/ijmlc.2019.9.1.761.
   van Dyk DA, 2001, J COMPUT GRAPH STAT, V10, P1, DOI 10.1198/10618600152418584.
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436.
   Xu Z, 2017, IEEE T IMAGE PROCESS, V26, P135, DOI 10.1109/TIP.2016.2621661.
   Xu Z, 2015, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2015.290.
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9\_26.
   Ye QL, 2019, IEEE T NEUR NET LEAR, V30, P3818, DOI 10.1109/TNNLS.2019.2944869.
   Ye QL, 2018, IEEE T NEUR NET LEAR, V29, P4494, DOI 10.1109/TNNLS.2017.2749428.
   Yu F, 2019, NEUROCOMPUTING, V350, P108, DOI 10.1016/j.neucom.2019.03.053.
   Zhu HY, 2017, MULTIMED TOOLS APPL, V76, P4599, DOI 10.1007/s11042-016-3538-4.},
Number-of-Cited-References = {38},
Times-Cited = {5},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {19},
Journal-ISO = {Soft Comput.},
Doc-Delivery-Number = {4S0NH},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000739262100001},
OA = {Green Accepted},
DA = {2023-08-12},
}

@article{ WOS:000849283900001,
Author = {Shelke, Ankita and Mehendale, Ninad},
Title = {A CNN-based android application for plant leaf classification at remote
   locations},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2023},
Volume = {35},
Number = {3, SI},
Pages = {2601-2607},
Month = {JAN},
Abstract = {The Earth has witnessed the evolution of thousands of plant species in
   the kingdom named Plantae. Due to the diversity and subtle differences
   in each plant, it becomes difficult for a novice to identify a
   particular plant and to know the properties associated with it. We
   propose a classification model that can solve this issue by categorizing
   the input plant image. Our methodology can classify up to 79 different
   plant species found predominantly in Himachal Pradesh located in India.
   A Deep Learning-based model is used to carry out the classification. Our
   model is optimized to work efficiently without a live internet
   connection on smartphones and other devices with limited computational
   power. A total of 79 distinct classes were classified using the
   Convolution neural network DenseNet-161 model architecture with a
   testing accuracy of 97.3\%. The application works on any android
   platform and can classify the input plant image with an average latency
   of 1.98 s. Our application built on this model assists farmers and
   locals to get in-depth knowledge about the species including the local
   name, scientific name, description, and the care requirements by
   uploading or taking a picture of the plant leaf.},
Publisher = {SPRINGER LONDON LTD},
Address = {236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Mehendale, N (Corresponding Author), KJ Somaiya Coll Engn, Mumbai 400077, Maharashtra, India.
   Shelke, Ankita; Mehendale, Ninad, KJ Somaiya Coll Engn, Mumbai 400077, Maharashtra, India.},
DOI = {10.1007/s00521-022-07740-1},
EarlyAccessDate = {SEP 2022},
ISSN = {0941-0643},
EISSN = {1433-3058},
Keywords = {Plant classification; Deep learning; Flutter; Android application},
Keywords-Plus = {DISEASE DETECTION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {ninad@somaiya.edu},
Affiliations = {Somaiya Vidyavihar University; K J Somaiya College of Engineering},
ResearcherID-Numbers = {Mehendale, Ninad Dileep/Y-2455-2018},
ORCID-Numbers = {Mehendale, Ninad Dileep/0000-0003-3037-5076},
Cited-References = {Abdullahi HS, 2017, 2017 SEVENTH INTERNATIONAL CONFERENCE ON INNOVATIVE COMPUTING TECHNOLOGY (INTECH 2017), P155, DOI 10.1109/INTECH.2017.8102436.
   Adhikari S, 2018, PROC 1 KEC C ENG TEC, V1, P81.
   Aldam C., 2002, CURR SCI, V83, P797.
   Cap HQ, 2018, 2018 IEEE 14TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING \& ITS APPLICATIONS (CSPA 2018), P118, DOI 10.1109/CSPA.2018.8368697.
   Chung Y, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18030961.
   Cynthia S. T., 2019, 2019 INT C SUSTAINAB, P1, DOI {[}10.1109/STI47673.2019.9068092,0(STI)IEEE, DOI 10.1109/STI47673.2019.9068092,0(STI)IEEE].
   Deepalakshmi P, 2021, INT J INF SYST MODEL, V12, P1, DOI 10.4018/IJISMD.2021010101.
   Douarre C, 2019, COMPUT ELECTRON AGR, V165, DOI 10.1016/j.compag.2019.104967.
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009.
   Gao FF, 2020, COMPUT ELECTRON AGR, V176, DOI 10.1016/j.compag.2020.105634.
   Graf BL, 2015, COMPR REV FOOD SCI F, V14, P431, DOI 10.1111/1541-4337.12135.
   GRASSE P P, 1977, P297.
   Hartling S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061284.
   Li Y, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105803.
   Mukti IZ, 2019, INT CONF ELECTR ENG.
   Nazki Haseeb, 2019, {[}Smart Media Journal, 스마트미디어저널], V8, P46, DOI 10.30693/smj.2019.8.2.46.
   PAL NR, 1993, PATTERN RECOGN, V26, P1277, DOI 10.1016/0031-3203(93)90135-J.
   Rieseberg LH, 1997, ANNU REV ECOL SYST, V28, P359, DOI 10.1146/annurev.ecolsys.28.1.359.
   Willis K.J., 2014, EVOLUTION PLANTS.},
Number-of-Cited-References = {19},
Times-Cited = {3},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Neural Comput. Appl.},
Doc-Delivery-Number = {8B9NM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000849283900001},
DA = {2023-08-12},
}

@article{ WOS:000434467700009,
Author = {Fu, Li and Zheng, Yuhong and Zhang, Pengchong and Zhu, Jiangwei and
   Zhang, Haoyang and Zhang, Luxi and Su, Weitao},
Title = {Embedding leaf tissue in graphene ink to improve signals in
   electrochemistry-based chemotaxonomy},
Journal = {ELECTROCHEMISTRY COMMUNICATIONS},
Year = {2018},
Volume = {92},
Pages = {39-42},
Month = {JUL},
Abstract = {We propose a method for the electrochemical identification of plants by
   embedding leaf tissue into graphene deposited on a screen-printed
   electrode (SPE). The embedding process significantly enhanced the
   electrochemical signals, which made the SPE sufficiently sensitive to
   record information about electro-active compounds in plants. In this
   work, five Lycoris herbs have been used as examples to evaluate the
   feasibility of the proposed technique. Multidimensional pattern
   recognition was successfully established for plant identification. In
   addition, the recorded ``electrochemical fingerprints{''} provided
   valuable taxonomic information, demonstrating the enormous potential of
   the technique for plant chemotaxonomy.},
Publisher = {ELSEVIER SCIENCE INC},
Address = {360 PARK AVE SOUTH, NEW YORK, NY 10010-1710 USA},
Type = {Article},
Language = {English},
Affiliation = {Fu, L (Corresponding Author), Hangzhou Dianzi Univ, Coll Mat \& Environm Engn, Hangzhou 310018, Peoples R China.
   Zheng, YH (Corresponding Author), Mem Sun Yat Sen, Nanjing Bot Garden, Jiangsu Prov \& Chinese Acad Sci, Inst Bot, Nanjing 210014, Jiangsu, Peoples R China.
   Fu, Li; Zhang, Haoyang; Zhang, Luxi; Su, Weitao, Hangzhou Dianzi Univ, Coll Mat \& Environm Engn, Hangzhou 310018, Peoples R China.
   Zheng, Yuhong, Mem Sun Yat Sen, Nanjing Bot Garden, Jiangsu Prov \& Chinese Acad Sci, Inst Bot, Nanjing 210014, Jiangsu, Peoples R China.
   Zhang, Pengchong, Hangzhou Bot Garden, Hangzhou 310013, Zhejiang, Peoples R China.
   Zhu, Jiangwei, Nanjing Forestry Univ, Collaborat Innovat Ctr Sustainable Forestry South, Nanjing 210037, Jiangsu, Peoples R China.},
DOI = {10.1016/j.elecom.2018.05.018},
ISSN = {1388-2481},
EISSN = {1873-1902},
Keywords = {Chemotaxonomy; Plant identification; Graphene ink; Screen printed
   electrode; Multivariate chemometric analysis},
Keywords-Plus = {MICROEXTRACTION-ASSISTED VOLTAMMETRY; CYCLIC VOLTAMMETRY;
   AMARYLLIDACEAE; LYCORIS; MICROPARTICLES; PHYLOGENY; KARYOTYPE;
   EVOLUTION; PLANTS},
Research-Areas = {Electrochemistry},
Web-of-Science-Categories  = {Electrochemistry},
Author-Email = {fuli@hdu.edu.cn
   zhengyuhong@cnbg.net},
Affiliations = {Hangzhou Dianzi University; Chinese Academy of Sciences; Nanjing
   Forestry University},
ResearcherID-Numbers = {su, wei/GZM-3986-2022
   Fu, Li/AAH-4689-2020},
ORCID-Numbers = {Fu, Li/0000-0002-5957-7790},
Funding-Acknowledgement = {Research Foundation from Hangzhou Dianzi University {[}KYS205617071];
   Zhejiang Province Natural Science Foundation of China {[}LQ18E010001];
   Technology Development Project of Hangzhou West Lake Scenic Area
   Committee {[}2014-002]; Hangzhou Science and Technology Development
   Project {[}20152231E02]},
Funding-Text = {This work has been financially supported by Research Foundation from
   Hangzhou Dianzi University (KYS205617071); Zhejiang Province Natural
   Science Foundation of China (LQ18E010001); Technology Development
   Project of Hangzhou West Lake Scenic Area Committee (2014-002); Hangzhou
   Science and Technology Development Project (20152231E02).},
Cited-References = {Bik HM, 2017, PLOS BIOL, V15, DOI 10.1371/journal.pbio.2002231.
   Cebrian-Torrejon G, 2017, PHYTOCHEM ANALYSIS, V28, P171, DOI 10.1002/pca.2657.
   Chen J., 2006, FLORA CHINA.
   Domenech-Carbo A, 2017, ANAL METHODS-UK, V9, P2041, DOI {[}10.1039/C7AY00323D, 10.1039/c7ay00323d].
   Domenech-Carbo A, 2017, ELECTROANAL, V29, P643, DOI 10.1002/elan.201600588.
   Domenech-Carbo A, 2015, NEW J CHEM, V39, P7421, DOI 10.1039/c5nj01233c.
   Domenech-Caro A, 2015, FOOD CHEM, V172, P318, DOI 10.1016/j.foodchem.2014.09.066.
   Dominguez I, 2015, SENSOR ACTUAT B-CHEM, V210, P491, DOI 10.1016/j.snb.2015.01.009.
   Du FY, 2007, CHINESE J ANAL CHEM, V35, P1570.
   Fu L, 2018, ELECTRONICS-SWITZ, V7, DOI 10.3390/electronics7020015.
   Fu L, 2018, TALANTA, V180, P248, DOI 10.1016/j.talanta.2017.12.058.
   Halinski LP, 2017, PHYTOCHEMISTRY, V144, P87, DOI 10.1016/j.phytochem.2017.09.001.
   Herbert W., 1837, AMARYLLIDACEAE.
   Kilmartin PA, 2003, FOOD CHEM, V82, P501, DOI 10.1016/S0308-8146(03)00066-9.
   KURITA S, 1988, CYTOLOGIA, V53, P323, DOI 10.1508/cytologia.53.323.
   KURITA S, 1987, CYTOLOGIA, V52, P19, DOI 10.1508/cytologia.52.19.
   KURITA S, 1998, NATURAL HIST HIGANBA, P177.
   Labroo P, 2014, ANAL CHIM ACTA, V813, P90, DOI 10.1016/j.aca.2014.01.024.
   Li M, 2012, ANAL CHIM ACTA, V734, P31, DOI 10.1016/j.aca.2012.05.018.
   Masek A, 2014, FOOD CHEM, V148, P18, DOI 10.1016/j.foodchem.2013.10.003.
   Norstrom E, 2017, GEOCHIM COSMOCHIM AC, V219, P96, DOI 10.1016/j.gca.2017.09.029.
   Ortiz-Miranda AS, 2016, ANAL BIOANAL CHEM, V408, P4943, DOI 10.1007/s00216-016-9588-7.
   Park YunJum, 1998, Korean Journal of Horticultural Science \& Technology, V16, P242.
   Roh Mark S., 2002, Journal of the Korean Society for Horticultural Science, V43, P120.
   Ru QM, 2009, CHEM NAT COMPD+, V45, P474, DOI 10.1007/s10600-009-9404-0.
   Scampicchio M, 2005, ELECTROANAL, V17, P1215, DOI 10.1002/elan.200403236.
   Schneider JV, 2017, TAXON, V66, P855, DOI 10.12705/664.4.
   Shi SD, 2006, SCI HORTIC-AMSTERDAM, V110, P285, DOI 10.1016/j.scienta.2006.07.011.
   Shi SD, 2006, BIOCHEM GENET, V44, P198, DOI 10.1007/s10528-006-9023-4.
   Shi SD, 2014, BOT J LINN SOC, V176, P115, DOI 10.1111/boj.12198.
   Singh R., 2018, NATURAL PRODUCTS DRU, P119, DOI {[}DOI 10.1016/B978-0-08-102081-4.00006-X, 10.1016/B978-0-08-102081-4.00006-X].
   Suleimen EM, 2017, CHEM NAT COMPD+, V53, P169, DOI 10.1007/s10600-017-1940-4.
   Tae Kyoung Howan, 1996, Korean Journal of Plant Taxonomy, V26, P19.
   Traub H. P., 1949, AMARYLLIDACEAE TRIBE.
   {[}袁菊红 YUAN Juhong], 2007, {[}中草药, Chinese Traditional and Herbal Drugs], V38, P1555.},
Number-of-Cited-References = {35},
Times-Cited = {41},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {30},
Journal-ISO = {Electrochem. Commun.},
Doc-Delivery-Number = {GI6GN},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000434467700009},
DA = {2023-08-12},
}

@article{ WOS:000618077800002,
Author = {Fabijanska, Anna and Danek, Malgorzata and Barniak, Joanna},
Title = {Wood species automatic identification from wood core images with a
   residual convolutional neural network},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2021},
Volume = {181},
Month = {FEB},
Abstract = {This paper tackles the problem of automatic tree species identification
   from scanned images of wood cores. A convolutional neural network with
   residual connections is proposed to perform this task. The model is
   applied to consecutive image patches following the sliding window
   strategy to recognize a patch central pixel's membership. It then
   decides about the resulting tree species via a majority voting. The
   model's performance was assessed concerning a dataset of 312 wood core
   images representing 14 European tree species, including both conifer and
   angiosperm (ring-porous and diffuse-porous) wood. Two tasks were
   considered, including wood patch classification and wood core
   classification. In these tasks, the proposed model correctly recognized
   species of almost 93\% the wood image patches and 98.7\% of wood core
   images. It also outperformed the state-of-the-art convolutional neural
   network-based competitor by 9\% and 3\%, respectively. The influence of
   the model's parameters and training set-up on its performance is
   analyzed in the manuscript to ensure the highest recognition rates of
   wood species. The source code of the proposed method is released
   together with the corresponding image dataset to facilitate the
   reproduction of results.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Fabijanska, A (Corresponding Author), Lodz Univ Technol, Inst Appl Comp Sci, 18-22 Stefanowskiego Str, PL-90924 Lodz, Poland.
   Fabijanska, Anna, Lodz Univ Technol, Inst Appl Comp Sci, 18-22 Stefanowskiego Str, PL-90924 Lodz, Poland.
   Danek, Malgorzata, AGH Univ Sci \& Technol, Dept Environm Anal Mapping \& Econ Geol, Mickiewicza 30 Ave, PL-30059 Krakow, Poland.
   Barniak, Joanna, AGH Univ Sci \& Technol, Dept Gen Geol \& Geotourism, A Mickiewicza 30 Ave, PL-30059 Krakow, Poland.},
DOI = {10.1016/j.compag.2020.105941},
EarlyAccessDate = {JAN 2021},
Article-Number = {105941},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {Wood species identification; Texture classification; Deep learning;
   Convolutional neural network; Residual connections},
Keywords-Plus = {CLASSIFICATION; RECOGNITION},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {anna.fabijanska@p.lodz.pl},
Affiliations = {Lodz University of Technology; AGH University of Krakow; AGH University
   of Krakow},
ResearcherID-Numbers = {Danek, Małgorzata/R-9734-2018
   Fabijanska, Anna/M-8314-2013
   },
ORCID-Numbers = {Danek, Małgorzata/0000-0001-8101-7469
   Fabijanska, Anna/0000-0002-0249-7247
   Danek, Malgorzata/0000-0002-9829-5753
   Barniak, Joanna/0000-0001-7676-6668},
Funding-Acknowledgement = {Lodz University of Technology, Faculty of Electrical, Electronic,
   Computer and Control Engineering; AGH University of Science and
   Technology, Faculty of Geology, Geophysics and Environmental Protection},
Funding-Text = {This work was co-financed by the Lodz University of Technology, Faculty
   of Electrical, Electronic, Computer and Control Engineering as a part of
   the statutory project.; This work was co-financed by the AGH University
   of Science and Technology, Faculty of Geology, Geophysics and
   Environmental Protection as a part of statutory project.},
Cited-References = {Baas P, 2004, IAWA J, V25, P1, DOI 10.1163/22941932-90000349.
   Barmpoutis P, 2018, COMPUT ELECTRON AGR, V144, P241, DOI 10.1016/j.compag.2017.12.011.
   da Silva NR, 2017, ANN FOREST SCI, V74, DOI 10.1007/s13595-017-0619-0.
   Danek M, 2007, GEOCHRONOMETRIA, V26, P47, DOI 10.2478/v10003-007-0006-1.
   Filho P. L. Paula, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4178, DOI 10.1109/ICPR.2010.1015.
   Hafemann LG, 2014, INT C PATT RECOG, P1103, DOI 10.1109/ICPR.2014.199.
   He K, 2015, C COMPUTER VISION PA.
   Hu JF, 2019, WOOD SCI TECHNOL, V53, P505, DOI 10.1007/s00226-019-01086-z.
   Ibrahim I, 2018, EUR J WOOD WOOD PROD, V76, P345, DOI 10.1007/s00107-017-1163-1.
   Khairuddin ASM, 2011, SMART INNOV SYST TEC, V11, P305.
   Kobayashi K, 2017, J WOOD SCI, V63, P322, DOI 10.1007/s10086-017-1625-4.
   Krapiec M, 2007, GEOCHRONOMETRIA, V26, P53, DOI 10.2478/v10003-007-0003-4.
   Lens F., 2020, IAWA J INT ASS WOOD, V41, P1.
   Lopes DJV, 2020, FORESTS, V11, DOI 10.3390/f11030298.
   Marguerie D, 2007, J ARCHAEOL SCI, V34, P1417, DOI 10.1016/j.jas.2006.10.032.
   Martins J, 2013, MACH VISION APPL, V24, P567, DOI 10.1007/s00138-012-0417-5.
   Nadaraj M, 2008, INT J SIMUL SYST SCI, V9, P9.
   Paula PL, 2014, MACH VISION APPL, V25, P1019, DOI 10.1007/s00138-014-0592-7.
   Peralta-Medina E, 2012, GEOLOGY, V40, P219, DOI 10.1130/G32733.1.
   Ravindran P, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0292-9.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Scabini LFS, 2019, LECT NOTES COMPUT SC, V11752, P192, DOI 10.1007/978-3-030-30645-8\_18.
   Schweingruber F. H., 1978, Microscopic wood anatomy : structural variability of stems and twigs in recent and subfossil woods from Central Europe..
   Szychowska-Krapiec E., 2013, WICINA ARCHAEOLOGICA, V7, P593.
   Turhan K, 2013, TURK J AGRIC FOR, V37, P249, DOI 10.3906/tar-1205-47.
   Wheeler EA, 1989, IAWA B, V10, P219, DOI DOI 10.1163/22941932-90000496.
   Yadav AR, 2015, SADHANA-ACAD P ENG S, V40, P2287, DOI 10.1007/s12046-015-0441-z.
   Yadav AR, 2015, PROCEDIA COMPUT SCI, V57, P214, DOI 10.1016/j.procs.2015.07.435.
   Yusof R, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY \& INTERNET-BASED SYSTEMS (SITIS), P737, DOI 10.1109/SITIS.2013.120.
   Zamri MIP, 2016, COMPUT ELECTRON AGR, V124, P227, DOI 10.1016/j.compag.2016.04.004.
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1\_53.
   Zhao P, 2014, OPTIK, V125, P1144, DOI 10.1016/j.ijleo.2013.07.124.},
Number-of-Cited-References = {32},
Times-Cited = {16},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {30},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {QH2AA},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000618077800002},
DA = {2023-08-12},
}

@inproceedings{ WOS:000317076900026,
Author = {Hong, Soon-Won and Choi, Lynn},
Editor = {Djemal, K and Deriche, M and Puech, W and Ucan, ON},
Title = {Automatic Recognition of Flowers through Color and Edge Based Contour
   Detection},
Booktitle = {2012 3RD INTERNATIONAL CONFERENCE ON IMAGE PROCESSING THEORY, TOOLS AND
   APPLICATIONS},
Series = {International Conference on Image Processing Theory Tools and
   Applications},
Year = {2012},
Pages = {141-146},
Note = {3rd International Conference on Image Processing Theory, Tools and
   Applications (IPTA), Istanbul, TURKEY, OCT 15-18, 2012},
Organization = {IEEE; Univ Evry Val Essonne (UEVE); Istanbul Aydin Universitesi (IAU);
   Univ Evry Val Essonne, Inst Technologie (IUT); Informat Biol Integrat \&
   Complex Syst Lab (IBISC); Montpellier Lab Informat Robot \&
   Microelectron (LIRMM); Multidisciplinary Inst Res Syst Engn Mech \&
   Energet (PRISME); Natl Centre Sci Res (CNRS); European Assoc Signal
   Processing (EURASIP); IEEE France Sect},
Abstract = {Unlike simple images processed by the existing image-based search
   engines, flowers have wider and more irregular range of shapes and
   patterns. In this paper we present an automatic recognition system of
   flowers for smartphone users. After a user transmits a flower image to
   the server, the image processing and searching is performed only by the
   server, eliminating the user interaction from the recognition process.
   The server detects the contour of a flower image by using both
   color-based and edge-based contour detection. Then, we classify its
   color groups and contour shapes by using k-means clustering and history
   matching. After comparing the input image with the reference images
   stored on the server, the server sends the most similar image to the
   user. We also address the image recognition failure issue caused by the
   light and the camera angle by partial recognition and image recovery. We
   have obtained the success rate of 94.8 \% for 500 images from 100
   species.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Hong, SW (Corresponding Author), Korea Univ, Sch Elect Engn, Seoul, South Korea.
   Hong, Soon-Won; Choi, Lynn, Korea Univ, Sch Elect Engn, Seoul, South Korea.},
ISSN = {2154-512X},
ISBN = {978-1-4673-2585-1; 978-1-4673-2583-7},
Keywords = {flower recognition; contour detection; histogram matching; search
   engines; smartphones},
Research-Areas = {Engineering; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Imaging Science \& Photographic
   Technology},
Author-Email = {aldig@korea.ac.kr
   lchoi@korea.ac.kr},
Affiliations = {Korea University},
Cited-References = {{[}Anonymous], 2011, INT J ADV SCI TECHNO.
   {[}Anonymous], 2011, INT J ADV SCI TECHNO.
   Bardyn J. J., 1984, C REC FORM INT ART P, V1, P557.
   Barman Ana, 2010, INT J ADV SCI TECHNO, V15, P49.
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863.
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505.
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851.
   Foley, 1993, COMPUTER GRAPHICS PR.
   Gonzalez R.C., 2009, DIGITAL IMAGE PROCES, DOI {[}DOI 10.1117/1.3115362, 10.1117/1.3115362].
   KIM JH, 2009, INT INF TECHN APPL I, P580, DOI DOI 10.1109/IITA.2009.407.
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489.
   MacQueen J.B., 1967, 5 BERKELEY S MATH ST, P281, DOI DOI 10.1007/S11665-016-2173-6.
   Nilsback M.E., 2006, P 2006 IEEE COMPUTER, VVolume 2, P1447, DOI DOI 10.1109/CVPR.2006.42.
   Priya K. Jaya, 2011, INT J ADV SCI TECHNO, V36, P41.
   Saitoh T, 2004, INT C PATT RECOG, P27, DOI 10.1109/ICPR.2004.1333997.
   Schiele B., 1996, Computer Vision - ECCV `96. 4th Eurpean Conference on Computer Proceedings, P610, DOI 10.1007/BFb0015571.
   Sobel I., 1968, PATTERN CLASSIFICATI, P271.
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487.
   Velu C M, 2011, INT J ADV SCI TECHNO, V31, P67.
   Ye YH, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P723, DOI 10.1109/ISIMP.2004.1434166.
   Zou J, 2004, INT C PATT RECOG, P311, DOI 10.1109/ICPR.2004.1334185.},
Number-of-Cited-References = {21},
Times-Cited = {11},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BEK49},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000317076900026},
DA = {2023-08-12},
}

@article{ WOS:000467724700002,
Author = {Zhang, Liang and Zhang, Hongduo and Chen, Yedong and Dai, Sihui and Li,
   Xumeng and Imou, Kenji and Liu, Zhonghua and Li, Ming},
Title = {Real-time monitoring of optimum timing for harvesting fresh tea leaves
   based on machine vision},
Journal = {INTERNATIONAL JOURNAL OF AGRICULTURAL AND BIOLOGICAL ENGINEERING},
Year = {2019},
Volume = {12},
Number = {1},
Pages = {6-9},
Month = {JAN},
Abstract = {The harvesting time of fresh tea leaves has a significant impact on
   product yield and quality. The aim of this study was to propose a method
   for real-time monitoring of the optimum harvesting time for picking
   fresh tea leaves based on machine vision. Firstly, the shapes of fresh
   tea leaves were distinguished from RGB images of the tea-tree canopy
   after graying with the improved B-G algorithm, filtering with a median
   filter algorithm, binary processing with the Otsu algorithm, and noise
   reduction and edge smoothing using open and close operations. Then the
   leaf characteristics, such as leaf area index, average length, and leaf
   identification index, were calculated. Based on these, the Bayesian
   discriminant principle and method were used to construct a discriminant
   model for fresh tea-leaf collection status. When this method was applied
   to a RGB tea-tree canopy image acquired at 45 degrees shooting angle,
   the fresh tea-leaf recognition rate was 90.3\%, and the accuracy for
   fresh tea-leaf harvesting status was 98\% by cross validation. Hence,
   this method provides the basic conditions for future tea-plantation
   operation and management using information technology, automation, and
   intelligent systems.},
Publisher = {CHINESE ACAD AGRICULTURAL ENGINEERING},
Address = {RM 506, NO 41, MAIZIDIAN ST, CHAOYANG DISTRICT, BEIJING, 100125, PEOPLES
   R CHINA},
Type = {Article},
Language = {English},
Affiliation = {Li, M (Corresponding Author), Hunan Agr Univ, Coll Engn, Changsha 410128, Hunan, Peoples R China.
   Li, XM (Corresponding Author), Hunan Agr Univ, Coll Agr, Changsha 410128, Hunan, Peoples R China.
   Zhang, Liang; Zhang, Hongduo; Chen, Yedong; Li, Ming, Hunan Agr Univ, Coll Engn, Changsha 410128, Hunan, Peoples R China.
   Dai, Sihui; Liu, Zhonghua, Hunan Agr Univ, Coll Hort \& Landscape, Changsha 410128, Hunan, Peoples R China.
   Li, Xumeng, Hunan Agr Univ, Coll Agr, Changsha 410128, Hunan, Peoples R China.
   Imou, Kenji, Univ Tokyo, Grad Sch Agr \& Life Sci, Dept Biol \& Environm Engn, Tokyo 1130033, Japan.
   Li, Ming, Shandong Univ Technol, Sch Agr Engn \& Food Sci, Zibo 255000, Peoples R China.
   Zhang, Liang; Zhang, Hongduo; Li, Xumeng; Li, Ming, Hunan Agr Aviat Adv Technol Engn Res Ctr, Changsha 410129, Hunan, Peoples R China.},
DOI = {10.25165/j.ijabe.20191201.3418},
ISSN = {1934-6344},
EISSN = {1934-6352},
Keywords = {agricultural machinery; fresh tea leaves; machine vision; intelligent
   recognition; real-time monitoring},
Keywords-Plus = {COMPUTER VISION},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agricultural Engineering},
Author-Email = {18810987071@163.com
   1090275648@qq.com
   18016634@qq.com
   daibest@qq.com
   xm.li@hunau@edu.cn
   aimou@mail.ecc.u-tokyo.ac.jp
   larkin-liu@163.com
   liming@hunau.net},
Affiliations = {Hunan Agricultural University; Hunan Agricultural University; Hunan
   Agricultural University; University of Tokyo; Shandong University of
   Technology},
ResearcherID-Numbers = {, 孙超然/AAV-7054-2020},
Funding-Acknowledgement = {{[}2018YFD0200803];  {[}2017RS3061];  {[}2018GK2013];  {[}2017NK2382]; 
   {[}2017YFD0301507];  {[}2018JJ3227]},
Funding-Text = {This work was financially supported in part by Programs
   (2018YFD0200803), (2017RS3061), (2018GK2013), (2017NK2382),
   (2017YFD0301507) and (2018JJ3227).},
Cited-References = {Arco JE, 2015, EXPERT SYST APPL, V42, P3041, DOI 10.1016/j.eswa.2014.11.037.
   Benalia S, 2016, COMPUT ELECTRON AGR, V120, P17, DOI 10.1016/j.compag.2015.11.002.
   Bishop C.M., 2006, PATTERN RECOGN, V128.
   Bora D.J., 2016, INT J COMPUTER SCI E, V4, P156.
   Borah S, 2003, P SPIE INT SOC OPTIC, P468.
   Cai JianRong, 2000, Transactions of the Chinese Society of Agricultural Engineering, V16, P118.
   Chen Quansheng, 2006, Chinese Journal of Scientific Instrument, V27, P1704.
   Gejima Y., 2000, ASAE ANN INT M, P1095.
   Hu Lian, 2013, Transactions of the Chinese Society of Agricultural Engineering, V29, P12.
   Hu YG, 2015, INT J AGR BIOL ENG, V8, P50, DOI 10.3965/j.ijabe.20150805.1655.
   Hu YG, 2013, INT J AGR BIOL ENG, V6, P27, DOI 10.3965/j.ijabe.20130604.004.
   Ji S. M., 1995, Transactions of the Chinese Society of Agricultural Machinery, V26, P56.
   Kamarudin Nur Shazwani, 2015, Journal of Theoretical and Applied Information Technology, V71, P79.
   Li X L, 2009, T CSAM, V40, P119.
   Lin G, 1994, J TEA SCI, V14, P75.
   Liu Y, 2016, APPL OPTICS, V55, P462, DOI 10.1364/AO.55.000462.
   Lu YZ, 2015, INT J AGR BIOL ENG, V8, P170, DOI 10.3965/j.ijabe.20150805.1426.
   Sun DW, 2016, COMPUTER VISION TECHNOLOGY FOR FOOD QUALITY EVALUATION, 2ND EDITION, P1, DOI 10.1016/C2014-0-01718-2.
   {[}汤一平 Tang Yiping], 2016, {[}农业机械学报, Transactions of the Chinese Society for Agricultural Machinery], V47, P15.
   Tang Z, 2015, NEUROCOMPUTING, V168, P1011, DOI 10.1016/j.neucom.2015.05.024.
   Wang Jian, 2008, Journal of Tea Science, V28, P420.
   Wang Jian, 2011, Journal of Tea Science, V31, P72.
   Wei JiaJia, 2012, Journal of Tea Science, V32, P377.
   Wen KX, 2015, SEED SCI TECHNOL, V43, P62, DOI 10.15258/sst.2015.43.1.07.
   Wu XueMei, 2013, Journal of Tea Science, V33, P584.
   Zhao JW, 2009, APPL OPTICS, V48, P3557, DOI 10.1364/AO.48.003557.},
Number-of-Cited-References = {26},
Times-Cited = {15},
Usage-Count-Last-180-days = {8},
Usage-Count-Since-2013 = {72},
Journal-ISO = {Int. J. Agric. Biol. Eng.},
Doc-Delivery-Number = {HX9JW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000467724700002},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000851857500001,
Author = {Batchuluun, Ganbayar and Nam, Se Hyun and Park, Kang Ryoung},
Title = {Deep Learning-Based Plant-Image Classification Using a Small Training
   Dataset},
Journal = {MATHEMATICS},
Year = {2022},
Volume = {10},
Number = {17},
Month = {SEP},
Abstract = {Extensive research has been conducted on image augmentation,
   segmentation, detection, and classification based on plant images.
   Specifically, previous studies on plant image classification have used
   various plant datasets (fruits, vegetables, flowers, trees, etc., and
   their leaves). However, existing plant-based image datasets are
   generally small. Furthermore, there are limitations in the construction
   of large-scale datasets. Consequently, previous research on plant
   classification using small training datasets encountered difficulties in
   achieving high accuracy. However, research on plant image classification
   based on small training datasets is insufficient. Accordingly, this
   study performed classification by reducing the number of training images
   of plant-image datasets by 70\%, 50\%, 30\%, and 10\%, respectively.
   Then, the number of images was increased back through augmentation
   methods for training. This ultimately improved the plant-image
   classification performance. Based on the respective preliminary
   experimental results, this study proposed a plant-image classification
   convolutional neural network (PI-CNN) based on plant image augmentation
   using a plant-image generative adversarial network (PI-GAN). Our
   proposed method showed the higher classification accuracies compared to
   the state-of-the-art methods when the experiments were conducted using
   four open datasets of PlantVillage, PlantDoc, Fruits-360, and Plants.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Park, KR (Corresponding Author), Dongguk Univ, Div Elect \& Elect Engn, 30 Pildong Ro,1 Gil, Seoul 04620, South Korea.
   Batchuluun, Ganbayar; Nam, Se Hyun; Park, Kang Ryoung, Dongguk Univ, Div Elect \& Elect Engn, 30 Pildong Ro,1 Gil, Seoul 04620, South Korea.},
DOI = {10.3390/math10173091},
Article-Number = {3091},
EISSN = {2227-7390},
Keywords = {plant image classification; image augmentation; deep learning; PI-GAN;
   PI-CNN},
Research-Areas = {Mathematics},
Web-of-Science-Categories  = {Mathematics},
Author-Email = {parkgr@dongguk.edu},
Affiliations = {Dongguk University},
ORCID-Numbers = {NAM, SE HYUN/0000-0002-0181-8774},
Funding-Acknowledgement = {National Research Foundation of Korea (NRF) - Ministry of Science and
   ICT (MSIT) {[}NRF2022R1F1A1064291]; NRF - MSIT {[}NRF-2020R1A2C1006179,
   NRF-2021R1F1A1045587]},
Funding-Text = {This research was supported in part by the National Research Foundation
   of Korea (NRF) funded by the Ministry of Science and ICT (MSIT) through
   the Basic Science Research Program (NRF2022R1F1A1064291), in part by the
   NRF funded by the MSIT through the Basic Science Research Program
   (NRF-2021R1F1A1045587), and in part by the NRF funded by the MSIT
   through the Basic Science Research Program (NRF-2020R1A2C1006179).},
Cited-References = {Abdul Hamid N.N.A., 2019, INDONES J ELECT ENG, V14, P333, DOI {[}10.11591/ijeecs.v14.i1.pp333-339, DOI 10.11591/IJEECS.V14.I1.PP333-339].
   Bhattacharya D, 2020, ALGO INTELL SY, P229, DOI 10.1007/978-981-15-1100-4\_11.
   Bird JJ, 2022, SCI HORTIC-AMSTERDAM, V293, DOI 10.1016/j.scienta.2021.110684.
   Biswas B., 2019, COMPUTATIONAL INTELL, P105, DOI {[}10.1007/978-981-13-9042-5\_10, DOI 10.1007/978-981-13-9042-5\_10].
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097.
   Derczynski L, 2016, LREC 2016 - TENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P261.
   Desai S, 2020, IEEE WINT CONF APPL, P972, DOI 10.1109/WACV45572.2020.9093360.
   Feng ZP, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13091772.
   Franczyk Bogdan, 2020, Procedia Computer Science, V176, P1211, DOI 10.1016/j.procs.2020.09.117.
   Frid-Adar M, 2018, I S BIOMED IMAGING, P289, DOI 10.1109/ISBI.2018.8363576.
   Ghesquiere M., 2021, ADV COMPUTER VISION, P69, DOI {[}10.1007/978-3-030-71051-4\_5, DOI 10.1007/978-3-030-71051-4\_5].
   Ghosh S, 2020, PROCEEDINGS OF 2020 IEEE APPLIED SIGNAL PROCESSING CONFERENCE (ASPCON 2020), P163, DOI {[}10.1109/ASPCON49795.2020.9276669, 10.1109/aspcon49795.2020.9276669].
   github, PI GAN PI CNN MODELS.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hossain MS, 2019, IEEE T IND INFORM, V15, P1027, DOI 10.1109/TII.2018.2875149.
   Kader A., 2020, INT RES J ENG TECHNO, V7, P1516.
   kaggle, POPULAR IMAGE AUGMEN.
   kaggle, PLANTS DATASET.
   kaggle, PLANTVILLAGE DATASET.
   kaggle, FRUITS 360 DATASET.
   kaggle, FIDS30 DATASET.
   Keras, US.
   Kingma DP., 2015, 3 INT C LEARN REPR I.
   machinelearningmastery, IMAGE DATA AUGMENTAT.
   Muhathir M., 2020, J INFORM TELECOMMUN, V4, P1, DOI {[}10.31289/jite.v4i1.3860, DOI 10.31289/JITE.V4I1.3860].
   nvidia, NVIDIA GEFORCE GTX T.
   OpenCV, US.
   Pawara P, 2017, LECT NOTES COMPUT SC, V10617, P615, DOI 10.1007/978-3-319-70353-4\_52.
   peltarion, CATEGORICAL CROSS EN.
   Powers D. M., 2010, J MACH LEARN TECHNOL, P37.
   Python, US.
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767.
   Rudnik K, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9193971.
   Santos Thiago, 2019, Zenodo, DOI 10.5281/ZENODO.3361736.
   Savant S.P.., 2020, INT J SCI RES SCI EN, V7, P666, DOI {[}10.35940/ijeat.D9082.089620, DOI 10.35940/IJEAT.D9082.089620].
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74.
   Shahi TB, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0264586.
   Siddiqi R., 2020, P 11 INT C ADV INFOR, P1, DOI {[}10.1145/3406601.3406619, DOI 10.1145/3406601.3406619].
   Siddiqi R, 2019, ICDLT 2019: 2019 3RD INTERNATIONAL CONFERENCE ON DEEP LEARNING TECHNOLOGIES, P91, DOI 10.1145/3342999.3343002.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Singh A., 2021, P INT C EMERGING TRE, P89.
   Singh D, 2020, ACM INT CONF PR SER, P249, DOI 10.1145/3371158.3371196.
   Srivastava S., 2020, INT J ENG RES TECHNO, V9, P896.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Takahashi R, 2020, IEEE T CIRC SYST VID, V30, P2917, DOI 10.1109/TCSVT.2019.2935128.
   towardsdatascience, BINARY CROSS ENTROPY.
   Wang DF, 2021, COMPUT ELECTRON AGR, V190, DOI 10.1016/j.compag.2021.106468.
   Wang HF, 2020, Arxiv, DOI arXiv:1910.01279.
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319.},
Number-of-Cited-References = {49},
Times-Cited = {1},
Usage-Count-Last-180-days = {10},
Usage-Count-Since-2013 = {31},
Journal-ISO = {Mathematics},
Doc-Delivery-Number = {4K3MD},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000851857500001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:001017493800001,
Author = {Kang, Xiaoyan and Huang, Changping and Zhang, Lifu and Yang, Mi and
   Zhang, Ze and Lyu, Xin},
Title = {Assessing the severity of cotton Verticillium wilt disease from in situ
   canopy images and spectra using convolutional neural networks},
Journal = {CROP JOURNAL},
Year = {2023},
Volume = {11},
Number = {3},
Pages = {933-940},
Month = {JUN},
Abstract = {Verticillium wilt (VW) is a common soilborne disease of cotton. It
   occurs mainly in the seedling and boll -opening stages and severely
   impairs the yield and quality of the fiber. Rapid and accurate
   identification and evaluation of VW severity (VWS) forms the basis of
   field cotton VW control, which has great signif-icance to cotton
   production. Cotton VWS values are conventionally measured using in-field
   observations and laboratory test diagnoses, which require abundant time
   and professional expertise. Remote and prox-imal sensing using imagery
   and spectrometry have great potential for this purpose. In this study,
   we per-formed in situ investigations at three experimental sites in 2019
   and 2021 and collected VWS values, in situ images, and spectra of 361
   cotton canopies. To estimate cotton VWS values at the canopy scale, we
   developed two deep learning approaches that use in situ images and
   spectra, respectively. For the imagery-based method, given the high
   complexity of the in situ environment, we first transformed the task of
   healthy and diseased leaf recognition to the task of cotton field scene
   classification and then built a cotton field scenes (CFS) dataset with
   over 1000 images for each scene-unit type. We performed pre -trained
   convolutional neural networks (CNNs) training and validation using the
   CFS dataset and then used the networks after training to classify scene
   units for each canopy. The results showed that the DarkNet-19 model
   achieved satisfactory performance in CFS classification and VWS values
   estimation (R2 = 0.91, root-mean-square error (RMSE) = 6.35\%). For the
   spectroscopy-based method, we first designed a one-dimensional
   regression network (1D CNN) with four convolutional layers. After
   dimen-sionality reduction by sensitive-band selection and principal
   component analysis, we fitted the 1D CNN with varying numbers of
   principal components (PCs). The 1D CNN model with the top 20 PCs
   per-formed best (R2 = 0.93, RMSE = 5.77\%). These deep learning-driven
   approaches offer the potential of assessing crop disease severity from
   spatial and spectral perspectives.\& COPY; 2022 Crop Science Society of
   China and Institute of Crop Science, CAAS. Production and hosting by
   Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open
   access article under the CC BY-NC -ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
Publisher = {KEAI PUBLISHING LTD},
Address = {16 DONGHUANGCHENGGEN NORTH ST, Building 5, Room 411, BEIJING, DONGCHENG
   DISTRICT 100009, PEOPLES R CHINA},
Type = {Article},
Language = {English},
Affiliation = {Huang, CP (Corresponding Author), Chinese Acad Sci, Aerosp Informat Res Inst, Natl Engn Res Ctr Satellite Remote Sensing Applica, Beijing 100094, Peoples R China.
   Huang, CP (Corresponding Author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   Kang, Xiaoyan; Huang, Changping; Zhang, Lifu, Chinese Acad Sci, Aerosp Informat Res Inst, Natl Engn Res Ctr Satellite Remote Sensing Applica, Beijing 100094, Peoples R China.
   Huang, Changping, Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   Zhang, Lifu; Yang, Mi; Zhang, Ze; Lyu, Xin, Shihezi Univ, Coll Agr, Xinjiang Prod \& Construct Corps Oasis Ecoagr Key L, Shihezi 832003, Xinjiang, Peoples R China.},
DOI = {10.1016/j.cj.2022.12.002},
ISSN = {2095-5421},
EISSN = {2214-5141},
Keywords = {Canopy scale; Cotton verticillium wilt; Deep learning; Disease
   assessment; In situ imagery; In situ spectrometry},
Keywords-Plus = {XYLELLA-FASTIDIOSA; THERMAL IMAGERY; RESOLUTION},
Research-Areas = {Agriculture; Plant Sciences},
Web-of-Science-Categories  = {Agronomy; Plant Sciences},
Author-Email = {huangcp@aircas.ac.cn},
Affiliations = {Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Shihezi University},
ORCID-Numbers = {Kang, Xiaoyan/0000-0002-1203-8140},
Funding-Acknowledgement = {Key Research Program of Frontier Sciences, CAS {[}ZDBS-LY-DQC012];
   National Natural Science Foundation of China {[}41971321, 41830108];
   XPCC Science and Technology Project {[}2022CB002-01]; Open Fund of Key
   Laboratory of Oasis Eco-agriculture, XPCC {[}201801, 202003]; Youth
   Innovation Promotion Association, CAS {[}Y2021047]},
Funding-Text = {This research was funded by Key Research Program of Frontier Sciences,
   CAS (ZDBS-LY-DQC012) , the National Natural Science Foundation of China
   (41971321, 41830108) , XPCC Science and Technology Project
   (2022CB002-01) , and Open Fund of Key Laboratory of Oasis
   Eco-agriculture, XPCC (201801 and 202003) . Chang-ping Huang was
   supported by Youth Innovation Promotion Association, CAS (Y2021047) .},
Cited-References = {Loti NNA, 2021, J SCI FOOD AGR, V101, P3582, DOI 10.1002/jsfa.10987.
   Ainiwaer M, 2020, INT J REMOTE SENS, V41, P3346, DOI 10.1080/01431161.2019.1701723.
   Caldeira RF, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093169.
   Calderon R, 2013, REMOTE SENS ENVIRON, V139, P231, DOI 10.1016/j.rse.2013.07.031.
   Chen B, 2012, INT J REMOTE SENS, V33, P2706, DOI 10.1080/01431161.2011.619586.
   Chen P, 2020, COMPUT ELECTRON AGR, V176, DOI 10.1016/j.compag.2020.105612.
   Chen Q., 2020, J XINJIANG AGR U, V43, P261, DOI {[}10.3969/j.issn.1007-8614.2020.04.004, DOI 10.3969/J.ISSN.1007-8614.2020.04.004].
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Deng L, 2018, ISPRS J PHOTOGRAMM, V144, P298, DOI 10.1016/j.isprsjprs.2018.08.002.
   Elaraby A, 2022, CMC-COMPUT MATER CON, V71, P4019, DOI 10.32604/cmc.2022.022161.
   Feng L, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0476-y.
   Feng S., 2021, REMOTE SENS-BASEL, V13, P23.
   Gao P., 2019, THESIS SHIHEZI U SHI.
   Gomez-Rios A, 2019, EXPERT SYST APPL, V118, P315, DOI 10.1016/j.eswa.2018.10.010.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Huang G., 2017, P 2017 IEEE C COMP V, P4700, DOI {[}DOI 10.1109/CVPR.2017.243, 10.1109/CVPR.2017.243].
   Jain A, 2005, PATTERN RECOGN, V38, P2270, DOI 10.1016/j.patcog.2005.01.012.
   Jing Xia, 2010, Transactions of the Chinese Society of Agricultural Engineering, V26, P193.
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1.
   Kang XY, 2022, COMPUT ELECTRON AGR, V201, DOI 10.1016/j.compag.2022.107260.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Li B, 2020, ISPRS J PHOTOGRAMM, V162, P161, DOI 10.1016/j.isprsjprs.2020.02.013.
   Liu XX, 2021, CROP PROT, V140, DOI 10.1016/j.cropro.2020.105429.
   Meng J, 2018, PLANT SCI, V272, P235, DOI 10.1016/j.plantsci.2018.05.003.
   Montecchia JF, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91034-4.
   Morais CLM, 2019, BIOINFORMATICS, V35, P5257, DOI 10.1093/bioinformatics/btz421.
   Noon SK, 2021, J INTELL FUZZY SYST, V40, P12383, DOI 10.3233/JIFS-210516.
   Pan Q, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22166047.
   Paul A, 2015, GISCI REMOTE SENS, V52, P643, DOI 10.1080/15481603.2015.1075180.
   Poblete T, 2021, ISPRS J PHOTOGRAMM, V179, P133, DOI 10.1016/j.isprsjprs.2021.07.014.
   Poblete T, 2020, ISPRS J PHOTOGRAMM, V162, P27, DOI 10.1016/j.isprsjprs.2020.02.010.
   Qin XH, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105464.
   Rasanen A, 2020, GISCI REMOTE SENS, V57, P943, DOI 10.1080/15481603.2020.1829377.
   Saleem RM, 2021, SCI PROGRAMMING-NETH, V2021, DOI 10.1155/2021/8824601.
   Santos AF, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14010093.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Srivastava R. K., 2015, ADV NEURAL INFORM PR, P2377, DOI DOI 10.48550/ARXIV.1505.00387.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.
   {[}王汇涵 Wang Huihan], 2022, {[}农业工程学报, Transactions of the Chinese Society of Agricultural Engineering], V38, P205.
   {[}王姣 Wang Jiao], 2019, {[}地理与地理信息科学, Geography and Geo-information Science], V35, P46.
   Wang S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12182957.
   Wang TY, 2020, J APPL REMOTE SENS, V14, DOI 10.1117/1.JRS.14.034522.
   Xiao ZT, 2022, SIGNAL IMAGE VIDEO P, V16, P873, DOI 10.1007/s11760-021-02029-7.
   Xu K., 2020, REMOTE SENS-BASEL, V12, P16.
   Xu YJ, 2021, INNOVATION-AMSTERDAM, V2, DOI 10.1016/j.xinn.2021.100179.
   {[}闫靖昆 Yan Jingkun], 2021, {[}南京师大学报. 自然科学版, Journal of Nanjing Normal University. Natural Science], V44, P127.
   Yan TY, 2021, FRONT PLANT SCI, V12, DOI 10.3389/fpls.2021.604510.
   Yu K, 2018, FRONT PLANT SCI, V9, DOI 10.3389/fpls.2018.01195.
   Zhang X, 2019, SCI TOTAL ENVIRON, V650, P321, DOI 10.1016/j.scitotenv.2018.08.442.
   Zhang Y, 2022, J INTELL FUZZY SYST, V42, P2193, DOI 10.3233/JIFS-211514.
   Zhang Y, 2021, PLANT BIOTECHNOL J, V19, P2126, DOI 10.1111/pbi.13650.
   Zhao J, 2021, INT J AGR BIOL ENG, V14, P167, DOI 10.25165/j.ijabe.20211402.6023.
   Zhao X.H., 2022, FRONT PLANT SCI, V13, P13.
   Zheng XT, 2019, IEEE T GEOSCI REMOTE, V57, P4799, DOI 10.1109/TGRS.2019.2893115.
   Zhu C, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2114583119.},
Number-of-Cited-References = {56},
Times-Cited = {2},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {3},
Journal-ISO = {Crop J.},
Doc-Delivery-Number = {K6JW4},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:001017493800001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000349496300038,
Author = {Hussin, Nuril Aslina Che and Jamil, Nursuriati and Nordin, Sharifalillah
   and Awang, Khalil},
Title = {Geometrical-Invariant Grid-Based Colour Moment (GBCM) for Plant
   Identification},
Journal = {ADVANCED SCIENCE LETTERS},
Year = {2014},
Volume = {20},
Number = {10-12},
Pages = {1914-1917},
Month = {OCT-DEC},
Note = {2nd International Conference on Internet Services Technology and
   Information Engineering (ISTIE), Bali, INDONESIA, MAY 31-JUN 01, 2014},
Abstract = {In any object identification problem, feature extraction method is an
   important phase as it determines the identification's accuracy. This
   paper presents a colour feature extraction techniques known as Grid
   Based Color Moment (GBCM) to identify herbal plants of Malaysia. Thirty
   plant species images are collected from their natural habitats and
   captured under various time of the day. These plant images are then used
   as ground truth images. They are further rotated and scaled to produce
   another thirty test images. The extracted features of the test images
   are then identified by calculating their Euclidean Distance (ED) against
   the ground truth and achieved an accuracy rate of 96.7 percent.},
Publisher = {AMER SCIENTIFIC PUBLISHERS},
Address = {26650 THE OLD RD, STE 208, VALENCIA, CA 91381-0751 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Hussin, NAC (Corresponding Author), Univ Teknol MARA, Fac Comp \& Math Sci, Dept Comp Sci, Shah Alam 40450, Selangor, Malaysia.
   Hussin, Nuril Aslina Che; Jamil, Nursuriati; Nordin, Sharifalillah; Awang, Khalil, Univ Teknol MARA, Fac Comp \& Math Sci, Dept Comp Sci, Shah Alam 40450, Selangor, Malaysia.},
DOI = {10.1166/asl.2014.5643},
ISSN = {1936-6612},
EISSN = {1936-7317},
Keywords = {Plant Leaf Identification; Scale Invariant Feature Transform; Color
   Moment; Euclidean Distance},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Affiliations = {Universiti Teknologi MARA},
ResearcherID-Numbers = {Jamil, Nursuriati/AAV-9816-2020
   Nordin, Sharifalillah/ABB-6094-2021
   JAMIL, PROFESOR DR NURSURIATI/HLG-1516-2023
   JAMIL, NURSURIATI/D-1494-2012},
ORCID-Numbers = {JAMIL, NURSURIATI/0000-0003-4634-9833},
Cited-References = {Bama B.S., 2011, INDIAN J COMPUTER SC, V2, P202.
   Chen MY, 2010, PROC SPIE, V7704, DOI 10.1117/12.853465.
   Hiremath  P., 2011, INT J COMPUTER SCI S, V1, P44.
   Jamil N., 2006, GEOMETRIC MODELING I GEOMETRIC MODELING I, P171.
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235.
   Kadir A., 2012, INT J COMPUTER TREND, V2012, P225.
   Lowe D. G., 2001, IEEE COMP SOC C COMP, V1, P1682.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Shanwen Zhang, 2010, 2010 International Conference on Computer Application and System Modeling (ICCASM 2010), P521, DOI 10.1109/ICCASM.2010.5622528.
   Wu S. G., 2007, IEEE INT S SIGN PROC, V11.
   Ying D., 2010, P 2 INT C FUT COMP C, P437.
   Yu H, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P929, DOI 10.1109/ICIP.2002.1039125.
   Zhai CM, 2008, 2008 INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION, VOLS 1-4, P879, DOI 10.1109/ICINFA.2008.4608123.
   Zhang SW, 2009, LECT NOTES COMPUT SC, V5754, P948, DOI 10.1007/978-3-642-04070-2\_100.
   Zulkifli Z., THESIS U TEKNOLOGI M.},
Number-of-Cited-References = {15},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Adv. Sci. Lett.},
Doc-Delivery-Number = {CB2ZF},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S); Conference Proceedings Citation Index- Social Science &amp; Humanities (CPCI-SSH)},
Unique-ID = {WOS:000349496300038},
DA = {2023-08-12},
}

@article{ WOS:000519474500064,
Author = {Visu, P. and Sivakumar, Nagarajan and Kumaresan, P. and Babu, Sundaresan
   Yokesh and Ramesh, P. S.},
Title = {Removing leaf petioles and auto locating apex-base points using straight
   line interpolation and bisection},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2020},
Volume = {79},
Number = {7-8},
Pages = {5355-5369},
Month = {FEB},
Abstract = {Plants are one of the long lasting species on Earth and are used for
   various purposes such as medicine, food and organics. Apex point and
   base point, coined here, are very important points of leaf facilitating
   extraction of various leaf shape based features. The main challenge in
   automatic plant recognition system is automatically identifying
   apex-base points and removing petiole of the leaf. Here, we propose a
   novel methodology, using straight line interpolation and bisection, for
   automatically identifying leaf apex point and base point. The
   coordinates of apex point and base point are successfully retrieved and
   the methodology is rotation and scale invariant. Finally, a logistic
   regression is used for classification. The proposed methodology is
   tested in several datasets like Leaflia, flavia and the results are
   satisfactory.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Visu, P (Corresponding Author), Anna Univ, Dept IT, Velammal Engn Coll, Chennai, Tamil Nadu, India.
   Visu, P., Anna Univ, Dept IT, Velammal Engn Coll, Chennai, Tamil Nadu, India.
   Sivakumar, Nagarajan; Kumaresan, P., Vellore Inst Technol, Sch Informat Technol \& Engn, Vellore, Tamil Nadu, India.
   Babu, Sundaresan Yokesh, Vellore Inst Technol, Sch Comp Sci \& Engn, Vellore, Tamil Nadu, India.
   Ramesh, P. S., Vellore Inst Technol, CIIS, Vellore, Tamil Nadu, India.},
DOI = {10.1007/s11042-018-6579-z},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Apex point; Base point; Petiole; Straight line interpolation; Bisection;
   Pattern recognition; Plant recognition system},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {pandu.visu@gmail.com
   knsksiva@gmail.com
   pkumaresan@vit.ac.in
   yokeshbabu.s@vit.ac.in
   ramesh.ps@vit.ac.in},
Affiliations = {Anna University; Anna University Chennai; Velammal Engineering College;
   Vellore Institute of Technology (VIT); VIT Vellore; Vellore Institute of
   Technology (VIT); VIT Vellore; Vellore Institute of Technology (VIT);
   VIT Vellore},
ResearcherID-Numbers = {S, RAMESH P/AAA-6297-2019
   P, Visu/EVU-7748-2022
   P, Kumaresan/O-1053-2018},
ORCID-Numbers = {S, RAMESH P/0000-0001-5190-768X
   P, Visu/0000-0001-8020-1678
   P, Kumaresan/0000-0001-5563-8325},
Cited-References = {Bi W, 2016, INT C INTEL HUM MACH, P525, DOI 10.1109/IHMSC.2016.37.
   Chaki J, 2016, INT C MICR, P1.
   Hati S, 2013, INT J COMPUTER APPL, V62, P15, DOI {[}10.5120/10172-4897, DOI 10.5120/10172-4897].
   Jiazhi Pan, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P906, DOI 10.1109/CSSE.2008.918.
   Karpathy A, CS231N CONVOLUTIONAL.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lakshmi JK, 2009, ICDIP 2009: INTERNATIONAL CONFERENCE ON DIGITAL IMAGE PROCESSING, PROCEEDINGS, P260, DOI 10.1109/ICDIP.2009.21.
   Langner J, 2001, LEAVES RECOGNITION V.
   Mishra P. K., 2012, 2012 International Conference on Advances in Engineering, Science and Management (ICAESM), P68.
   Nandyal SS., 2013, INT J COMPUT VIS ROB, V3, P197, DOI {[}10.1504/IJCVR.2013.056040, DOI 10.1504/IJCVR.2013.056040].
   Prasad S., 2011, 2011 2nd International Conference on Computer and Communication Technology, P646, DOI 10.1109/ICCCT.2011.6075212.
   Price CA, 2011, PLANT PHYSIOL, V155, P236, DOI 10.1104/pp.110.162834.
   Sibi Chakaravarthy S, 2016, ADV SIGNAL PROCESSIN, P581.
   Suta L., 2012, 2012 IEEE International Conference on Intelligent Computer Communication and Processing (ICCP 2012). Proceedings, P181, DOI 10.1109/ICCP.2012.6356183.
   Valliammal N., 2011, HYBRID IMAGE SEGMENT, P1, DOI {[}DOI 10.1109/PACC.2011.5978883, DOI 10.1109/PACC2011.5978883].
   Wang XF, 2007, APPL MATH COMPUT.
   Wilf P, 1998, MANUAL LEAF ARCHITEC, P65.
   Winberg S, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1000, DOI 10.1109/ICIT.2013.6505808.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Zulkifli Z., 2011, Proceedings of the 2011 11th International Conference on Hybrid Intelligent Systems (HIS 2011), P430, DOI 10.1109/HIS.2011.6122144.},
Number-of-Cited-References = {20},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {KU1MT},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000519474500064},
DA = {2023-08-12},
}

@article{ WOS:000675744700001,
Author = {Wang, Xianfeng and Zhang, Chuanlei and Zhang, Shanwen},
Title = {Multiscale Convolutional Neural Networks with Attention for Plant
   Species Recognition},
Journal = {COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE},
Year = {2021},
Volume = {2021},
Month = {JUL 5},
Abstract = {Plant species recognition is a critical step in protecting plant
   diversity. Leaf-based plant species recognition research is important
   and challenging due to the large within-class difference and
   between-class similarity of leaves and the rich inconsistent leaves with
   different sizes, colors, shapes, textures, and venations. Most existing
   plant leaf recognition methods typically normalize all leaf images to
   the same size and then recognize them at one scale, which results in
   unsatisfactory performances. A novel multiscale convolutional neural
   network with attention (AMSCNN) model is constructed for plant species
   recognition. In AMSCNN, multiscale convolution is used to learn the
   low-frequency and high-frequency features of the input images, and an
   attention mechanism is utilized to capture rich contextual relationships
   for better feature extraction and improving network training. Extensive
   experiments on the plant leaf dataset demonstrate the remarkable
   performance of AMSCNN compared with the hand-crafted feature-based
   methods and deep-neural network-based methods. The maximum accuracy
   attained along with AMSCNN is 95.28\%.},
Publisher = {HINDAWI LTD},
Address = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Zhang, CL (Corresponding Author), Tianjin Univ Sci \& Technol, Coll Artificial Intelligence, Tianjin 300222, Peoples R China.
   Wang, Xianfeng; Zhang, Shanwen, Xijing Univ, Sch Informat Engn, Xian 710123, Shaanxi, Peoples R China.
   Zhang, Chuanlei, Tianjin Univ Sci \& Technol, Coll Artificial Intelligence, Tianjin 300222, Peoples R China.},
DOI = {10.1155/2021/5529905},
Article-Number = {5529905},
ISSN = {1687-5265},
EISSN = {1687-5273},
Research-Areas = {Mathematical \& Computational Biology; Neurosciences \& Neurology},
Web-of-Science-Categories  = {Mathematical \& Computational Biology; Neurosciences},
Author-Email = {wangxianfeng@xijing.edu.cn
   a17647@gmail.com
   wjdw716@163.com},
Affiliations = {Xijing University; Tianjin University Science \& Technology},
ORCID-Numbers = {zhang, shanwen/0000-0002-4534-5358},
Cited-References = {Abas M.A.H., 2018, INT J ENG TECHNOL, V7, P90, DOI {[}DOI 10.14419/IJET.V7I4.11.20781, 10.14419/ijet.v7i4.11.20781].
   ArunPriya C., 2012, INT J COMPUTER TECHN, V3, P1132.
   Bhardwaj A., 2013, INT J ENG TREND TECH, V4, P937.
   Du CB, 2017, IEEE ACCESS, V5, P15750, DOI 10.1109/ACCESS.2017.2735019.
   Handa A., 2015, INT J COMPUT APPL, V123, P20, DOI DOI 10.5120/IJCA2015905247.
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688.
   Jabal MFAB, 2013, J COMPUT SCI-NETH, V9, P1295, DOI {[}DOI 10.3844/J.CSSP.2013.1295.1304, DOI 10.3844/JCSSP.2013.1295.1304].
   Lee S.H, 2016, IEEE SYST J, V10, P59.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Li P., 2019, IMAGE GRAPHICS TECHN, P472.
   Lu ZL, 2000, VISION RES, V40, P173, DOI 10.1016/S0042-6989(99)00172-8.
   Peng YX, 2020, IEEE T IMAGE PROCESS, V29, P2728, DOI 10.1109/TIP.2019.2952085.
   Rasti R, 2018, IEEE T MED IMAGING, V37, P1024, DOI 10.1109/TMI.2017.2780115.
   Rezende Edmar, 2018, Information Technology - New Generations. 15th International Conference on Information Technology. Advances in Intelligent Systems and Computing (AISC 738), P51, DOI 10.1007/978-3-319-77028-4\_9.
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003.
   VijayaLakshmi B, 2016, COMPUT ELECTRON AGR, V125, P99, DOI 10.1016/j.compag.2016.04.033.
   Wang ZB, 2017, ARCH COMPUT METHOD E, V24, P637, DOI 10.1007/s11831-016-9181-4.
   Wikee S, 2011, FUNGAL DIVERS, V51, P43, DOI 10.1007/s13225-011-0146-5.
   Xiao QG, 2018, ECOL INFORM, V48, P117, DOI 10.1016/j.ecoinf.2018.09.001.
   Zhang SW, 2020, NEUROCOMPUTING, V408, P246, DOI 10.1016/j.neucom.2019.09.113.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhu HY, 2018, MULTIMED TOOLS APPL, V77, P29779, DOI 10.1007/s11042-017-5578-9.
   Zhu XL, 2018, COGN SYST RES, V52, P223, DOI 10.1016/j.cogsys.2018.06.008.
   Zhu YX, 2019, NEUROCOMPUTING, V365, P191, DOI 10.1016/j.neucom.2019.07.016.},
Number-of-Cited-References = {24},
Times-Cited = {3},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {16},
Journal-ISO = {Comput. Intell. Neurosci.},
Doc-Delivery-Number = {TM7RI},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000675744700001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000646354300058,
Author = {Pantano, Matteo and Kamps, Tobias and Pizzocaro, Solomon and Pantano,
   Giorgio and Corno, Matteo and Savaresi, Sergio},
Book-Group-Author = {IEEE},
Title = {Methodology for Plant Specific Cultivation through a Plant
   Identification pipeline},
Booktitle = {PROCEEDINGS OF 2020 IEEE INTERNATIONAL WORKSHOP ON METROLOGY FOR
   AGRICULTURE AND FORESTRY (METROAGRIFOR)},
Year = {2020},
Pages = {298-302},
Note = {3rd IEEE International Workshop on Metrology for Agriculture and
   Forestry (MetroAgriFor), ELECTR NETWORK, NOV 04-06, 2020},
Organization = {IEEE; Univ Trento; Minist Politiche Agricole Alimentari Forestali},
Abstract = {Agriculture needs to optimization for satisfying the rising demands of
   food due to world population growth. An approach to this problem is
   digitization of agriculture through IT tools by creating digital twins.
   However, in the digital twin creation the uniqueness of the plant is
   lost, therefore, plant based agricultural cultivation cannot be
   performed. Hence, this paper proposes a methodology to assign an
   identification marker to plants in a crop using an image analysis
   pipeline. To show the effectiveness of the algorithm the proposed method
   is evaluated on the Rovitis robotic platform and compared with the crop
   ontology. The outcome of this work can be used in robotic agricultural
   platforms to address plants singularly thus optimizing their
   cultivation.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Pantano, M (Corresponding Author), Siemens AG, Technol, Munich, Germany.
   Pantano, Matteo; Kamps, Tobias, Siemens AG, Technol, Munich, Germany.
   Pizzocaro, Solomon; Corno, Matteo; Savaresi, Sergio, Politecn Milan, Move Res Team, Milan, Italy.
   Pantano, Giorgio, Azienda Agr Giorgio Pantano, Candiana, Italy.},
ISBN = {978-1-7281-8783-9},
Keywords = {digital farming; plant specific cultivation; deep learning; autonomous
   robotics; precision viticulture; digital twin},
Research-Areas = {Automation \& Control Systems; Agriculture; Forestry},
Web-of-Science-Categories  = {Automation \& Control Systems; Agricultural Engineering; Forestry},
Affiliations = {Siemens AG; Siemens Germany; Polytechnic University of Milan},
ORCID-Numbers = {Pizzocaro, Solomon/0000-0003-1652-8523},
Cited-References = {Alves R, 2019, PUBLIC HEALTH NUTR, V22, P1971, DOI 10.1017/S136898001800410X.
   {[}Anonymous], 2018, ROVITIS 4 0 GRUPP OP.
   {[}Anonymous], 2013, FEASR PROGETTO DODIC.
   {[}Anonymous], 2018, ATP MAGAZIN, DOI DOI 10.17560/ATP.V60I10.2371.
   Azienda Agricola Giorgio Pantano, INN GRAP.
   Baweja C., 2018, ROTNIST.
   Bugmann G, 2011, AFRICON.
   Daglio G, 2019, IOP C SER EARTH ENV, V275, DOI 10.1088/1755-1315/275/1/012019.
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242.
   Kritzinger W, 2018, IFAC PAPERSONLINE, V51, P1016, DOI 10.1016/j.ifacol.2018.08.474.
   Kruize J. W., 2017, 7 AS AUSTR C PREC AG.
   LANCASHIRE PD, 1991, ANN APPL BIOL, V119, P561, DOI 10.1111/j.1744-7348.1991.tb04895.x.
   Lee WS, 2010, COMPUT ELECTRON AGR, V74, P2, DOI 10.1016/j.compag.2010.08.005.
   Lutz W, 2001, NATURE, V412, P543, DOI 10.1038/35087589.
   MEADOWS D H, 1972, P205.
   Mike S., 2012, MODELING SIMULATION.
   Negri E, 2017, PROCEDIA MANUF, V11, P939, DOI 10.1016/j.promfg.2017.07.198.
   Nuske S., 2011, 2011 IEEE RSJ INT C.
   O'Mahony N, 2020, ADV INTELL SYST COMP, V943, P128, DOI 10.1007/978-3-030-17795-9\_10.
   Primicerio J, 2017, EUR J REMOTE SENS, V50, P179, DOI 10.1080/22797254.2017.1308234.
   Profanter S, 2019, IEEE INT CONF INDUST, P955, DOI 10.1109/ICIT.2019.8755050.
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91.
   Roser M., 2013, LIFE EXPECTANCY.
   Tilman D, 2011, P NATL ACAD SCI USA, V108, P20260, DOI 10.1073/pnas.1116437108.},
Number-of-Cited-References = {24},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {6},
Doc-Delivery-Number = {BR3HF},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000646354300058},
DA = {2023-08-12},
}

@inproceedings{ WOS:000706493800029,
Author = {Yuan Liang and Li Yihao and Wang Yang and Mao Ailong and Chen Kun},
Book-Group-Author = {IEEE},
Title = {Detection and Identification of Sandwich Materials in Tobacco Packing
   Boxes Based on Three-channel Two-Dimensional Convolutional Neural
   Network},
Booktitle = {2021 4TH INTERNATIONAL CONFERENCE ON INTELLIGENT AUTONOMOUS SYSTEMS
   (ICOIAS 2021)},
Year = {2021},
Pages = {158-161},
Note = {4th International Conference on Intelligent Autonomous Systems (ICoIAS),
   Wuhan, PEOPLES R CHINA, MAY 14-16, 2021},
Organization = {Dalian Maritime Univ; Wuhan Univ Sci \& Technol; IEEE; Inst Engineers},
Abstract = {At present, the common problem in the tobacco factory is whether there
   is yellow cardboard sandwich material in the tobacco packaging box. The
   KUKA unpacking robot can not independently determine the shape of the
   cardboard, the central point position, and whether there is any,
   resulting in the problem of misoperation and repeated operation,
   resulting in the decline in the efficiency of tobacco unpacking. So this
   article with the method of image processing combined with convolution
   neural network puts forward a kind of complex tobacco packaging sandwich
   materials based on machine vision detection method, shape to calibrate
   the strawboards interlining, determine the center position, whether
   there is any problem and confirm the strawboards interlining, to realize
   accurate positioning KUKA robot clip strawboards sandwich material. The
   experiment shows that the efficiency of judgment is better than that of
   manual inspection, and the production efficiency of the factory is
   increased by 82\%, saving human resources to some extent.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Yuan, L (Corresponding Author), Zhengzhou Univ Light Ind, Zhengzhou, Peoples R China.
   Yuan Liang; Li Yihao, Zhengzhou Univ Light Ind, Zhengzhou, Peoples R China.
   Wang Yang; Mao Ailong, Tobacco Henan Ind Co Ltd, Golden Leaf Prod \& Mfg Ctr China, Zhengzhou, Peoples R China.
   Chen Kun, Shanghai Inst Spaceflight Control Technol, Shanghai, Peoples R China.},
DOI = {10.1109/ICoIAS53694.2021.00036},
ISBN = {978-1-6654-4195-7},
Keywords = {tobacco leaf recognition; machine vision; convolutional neural network},
Research-Areas = {Automation \& Control Systems; Computer Science},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Artificial Intelligence},
Author-Email = {yuanliangchn@163.com
   847790995@qq.com
   853574639@qq.com
   309187366@qq.com
   frank\_chan\_ck@163.com},
Affiliations = {Zhengzhou University of Light Industry; China National Tobacco
   Corporation},
ResearcherID-Numbers = {Wang, Yang/A-7106-2018},
ORCID-Numbers = {Wang, Yang/0000-0002-4562-8227},
Cited-References = {Jian Q, 2018, ANAL LETT.
   Wurschinger H., 2020, PROCEDIA CIRP, V90, P611, DOI 10.1016/j.procir.2020.01.121.
   Zhang SW, 2019, COGN SYST RES, V53, P31, DOI 10.1016/j.cogsys.2018.04.006.
   Zhang YN, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10155177.},
Number-of-Cited-References = {4},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BS2RE},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000706493800029},
DA = {2023-08-12},
}

@article{ WOS:000848606900001,
Author = {Li, Cheng and Li, Ming and Zhu, Xinghui and Chen, Yineng and Wu, Yanbin
   and Deng, Nan and Fang, Kui},
Title = {Identification Method of Grape Leaf Diseases Based on Improved CCT Model},
Journal = {INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE},
Year = {2022},
Volume = {36},
Number = {11},
Month = {SEP 15},
Abstract = {Grape is an important cash crop that is susceptible to diseases when
   growing, resulting in lower yield and quality. In recent years,
   transformers have achieved excellent performance in a variety of natural
   language processing and image recognition tasks through the
   self-attention mechanism. Therefore, this paper proposes a grape leaf
   disease recognition model named Dense Convolutional Transformer
   (DensCT). The compact convolutional transformer (CCT) is used as the
   backbone in this model, which improves the convolutional module of the
   original model by introducing densely connected modules, enhancing the
   transfer and reuse of features between networks. This also modifies the
   single-scale feature extraction method of the original model to
   multi-scale, which improves the feature extraction performance. Finally,
   the model was trained on two small-scale datasets from scratch, and the
   recognition accuracy of the final model on the test sets reached 89.19\%
   and 93.92\%. Compared with CCT, DenseNet121, ResNet50, MobileNetV3 and
   ViT, the recognition accuracy improved by 4.73\%, 3.38\%, 10.81\%,
   0.68\% and 18.24\% on the first dataset and 6.08\%, 5.41\%, 1.35\%,
   3.38\% and 12.84\% on the second dataset. The experimental results show
   that the proposed model can effectively identify grape leaf diseases,
   which can provide a reference for building disease leaf recognition
   models on small-scale datasets.},
Publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
Address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
Type = {Article},
Language = {English},
Affiliation = {Zhu, XH; Fang, K (Corresponding Author), Hunan Agr Univ, Coll Informat \& Intelligence, Changsha 410128, Peoples R China.
   Li, Cheng; Zhu, Xinghui; Chen, Yineng; Wu, Yanbin; Deng, Nan; Fang, Kui, Hunan Agr Univ, Coll Informat \& Intelligence, Changsha 410128, Peoples R China.
   Li, Ming, Hunan Agr Equipment Res Inst, Changsha 410129, Peoples R China.
   Wu, Yanbin, Changsha Commerce \& Tourism Coll, Network Secur \& Informat Technol Ctr, Changsha 410116, Peoples R China.},
DOI = {10.1142/S0218001422500379},
EarlyAccessDate = {AUG 2022},
ISSN = {0218-0014},
EISSN = {1793-6381},
Keywords = {Transformer; grape leaf diseases recognition; densely connected;
   self-attention},
Keywords-Plus = {INFORMATION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {zhuxh@hunau.edu.cn
   fk@hunau.edu.cn},
Affiliations = {Hunan Agricultural University},
ORCID-Numbers = {/0000-0003-2014-7133},
Funding-Acknowledgement = {Key Research and Development Program of Hunan Province {[}2020NK2033];
   National Natural Science Foundation of China {[}62072166]; Scientic
   Research Fund of Hunan Provincial Education Department, China {[}20A249,
   20A259]},
Funding-Text = {This work was supported in part by the Key Research and Development
   Program of Hunan Province (2020NK2033), the National Natural Science
   Foundation of China (62072166) and the Scientic Research Fund of Hunan
   Provincial Education Department, China (20A249 and 20A259).},
Cited-References = {Camargo A, 2009, COMPUT ELECTRON AGR, V66, P121, DOI 10.1016/j.compag.2009.01.003.
   Chao XF, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11104614.
   Chaudhary Archana, 2016, Information Processing in Agriculture, V3, P215, DOI 10.1016/j.inpa.2016.08.002.
   Cheng X, 2017, COMPUT ELECTRON AGR, V141, P351, DOI 10.1016/j.compag.2017.08.005.
   Cignoni P., 2008, PROC EUROGRAPH ITALI, P129, DOI DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136.
   d'Ascoli S, 2021, PR MACH LEARN RES, V139, DOI 10.1088/1742-5468/ac9830.
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.1412.11929.
   Dutot M, 2013, POSTHARVEST BIOL TEC, V85, P45, DOI 10.1016/j.postharvbio.2013.04.003.
   Hassan SM, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10121388.
   Hassani A., ARXIV210405704.
   Huang K, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su132111572.
   Thai HT, 2021, PROC INT CONF ADV, P33, DOI 10.1109/ATC52653.2021.9598303.
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
   Liang JH, 2020, IEEE ACCESS, V8, P147265, DOI 10.1109/ACCESS.2020.3015714.
   Lin HF, 2021, J FOOD MEAS CHARACT, V15, P4696, DOI 10.1007/s11694-021-01043-0.
   Lottes P, 2018, IEEE ROBOT AUTOM LET, V3, P2870, DOI 10.1109/LRA.2018.2846289.
   Miaomiao Ji, 2020, Information Processing in Agriculture, V7, P418, DOI 10.1016/j.inpa.2019.10.003.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Oyewola DO, 2021, PEERJ COMPUT SCI, V7, DOI 10.7717/peerj-cs.352.
   Patil AS, 2021, INT J AMBIENT ENERGY, V42, P1376, DOI 10.1080/01430750.2019.1608859.
   Reedha R., ARXIV.
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
   Shupei Wu, 2021, 2021 35th Symposium on Microelectronics Technology and Devices (SBMicro), P387, DOI 10.1109/IAECST54258.2021.9695688.
   Singh J., 2018, PROC INT C ISMAC COM, P1667.
   Szegedy C., 2015, P IEEE C COMP VIS PA.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358.
   Wu H., 2019, The Plant Phenome Journal, V2, DOI 10.2135/tppj2019.03.0006.
   Zhang JY, 2021, INT J DISTRIB SENS N, V17, DOI 10.1177/15501477211007407.
   Zhang KK, 2019, INT J AGRIC ENVIRON, V10, P98, DOI 10.4018/IJAEIS.2019040105.
   Zhang PC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2978, DOI 10.1109/ICCV48922.2021.00299.
   Zhenghua Zhang, 2021, 2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI), P153, DOI 10.1109/CISAI54367.2021.00036.},
Number-of-Cited-References = {34},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Int. J. Pattern Recognit. Artif. Intell.},
Doc-Delivery-Number = {4M5UH},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000848606900001},
DA = {2023-08-12},
}

@article{ WOS:000889791700001,
Author = {Li, Yang and Bao, Zhiyuan and Qi, Jiangtao},
Title = {Seedling maize counting method in complex backgrounds based on YOLOV5
   and Kalman filter tracking algorithm},
Journal = {FRONTIERS IN PLANT SCIENCE},
Year = {2022},
Volume = {13},
Month = {NOV 7},
Abstract = {Maize population density is one of the most essential factors in
   agricultural production systems and has a significant impact on maize
   yield and quality. Therefore, it is essential to estimate maize
   population density timely and accurately. In order to address the
   problems of the low efficiency of the manual counting method and the
   stability problem of traditional image processing methods in the field
   complex background environment, a deep-learning-based method for
   counting maize plants was proposed. Image datasets of the maize field
   were collected by a low-altitude UAV with a camera onboard firstly. Then
   a real-time detection model of maize plants was trained based on the
   object detection model YOLOV5. Finally, the tracking and counting method
   of maize plants was realized through Hungarian matching and Kalman
   filtering algorithms. The detection model developed in this study had an
   average precision mAP@0.5 of 90.66\% on the test dataset, demonstrating
   the effectiveness of the SE-YOLOV5m model for maize plant detection.
   Application of the model to maize plant count trials showed that maize
   plant count results from test videos collected at multiple locations
   were highly correlated with manual count results (R-2 = 0.92),
   illustrating the accuracy and validity of the counting method.
   Therefore, the maize plant identification and counting method proposed
   in this study can better achieve the detection and counting of maize
   plants in complex backgrounds and provides a research basis and
   theoretical basis for the rapid acquisition of maize plant population
   density.},
Publisher = {FRONTIERS MEDIA SA},
Address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Qi, JT (Corresponding Author), Jilin Univ, Key Lab Bion Engn, Minist Educ, Changchun, Peoples R China.
   Qi, JT (Corresponding Author), Jilin Univ, Coll Biol \& Agr Engn, Changchun, Peoples R China.
   Li, Yang; Bao, Zhiyuan; Qi, Jiangtao, Jilin Univ, Key Lab Bion Engn, Minist Educ, Changchun, Peoples R China.
   Li, Yang; Bao, Zhiyuan; Qi, Jiangtao, Jilin Univ, Coll Biol \& Agr Engn, Changchun, Peoples R China.
   Li, Yang, Chinese Acad Agr Sci, Tea Res Inst, Key Lab Tea Qual \& Safety Control, Minist Agr \& Rural Affairs, Hangzhou, Peoples R China.},
DOI = {10.3389/fpls.2022.1030962},
Article-Number = {1030962},
ISSN = {1664-462X},
Keywords = {object detection; YOLOv5; video tracking; maize plants; counting
   prediction},
Keywords-Plus = {PLANT-DENSITY; YIELD},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {qijiangtao@jlu.edu.cn},
Affiliations = {Jilin University; Jilin University; Chinese Academy of Agricultural
   Sciences; Tea Research Institute, CAAS; Ministry of Agriculture \& Rural
   Affairs},
ResearcherID-Numbers = {Qi, Jiangtao/AAL-6502-2020
   yang, li/AAP-3842-2021},
ORCID-Numbers = {Qi, Jiangtao/0000-0002-4644-6273
   yang, li/0000-0001-8145-3834},
Funding-Acknowledgement = {National Natural Science Foundation of China; Central Public-Interest
   Scientific Institution Basal Research Fund;  {[}31971783]; 
   {[}1610212022004]},
Funding-Text = {Funding This study was supported by the National Natural Science
   Foundation of China (31971783) and the Central Public-Interest
   Scientific Institution Basal Research Fund (1610212022004).},
Cited-References = {Adams C, 2019, FIELD CROP RES, V230, P11, DOI 10.1016/j.fcr.2018.10.005.
   Chapepa B, 2020, J COTTON RES, V3, DOI 10.1186/s42397-020-00059-z.
   Gao FF, 2022, COMPUT ELECTRON AGR, V197, DOI 10.1016/j.compag.2022.107000.
   Gene-Mola J, 2020, COMPUT ELECTRON AGR, V168, DOI 10.1016/j.compag.2019.105121.
   Glenn J., 2022, YOLOV5 GIT CODE.
   Hani N, 2020, J FIELD ROBOT, V37, P263, DOI 10.1002/rob.21902.
   Hu J., P IEEE CVF C COMP VI, P7132, DOI DOI 10.1109/CVPR.2018.00745.
   Hu Lian, 2013, Transactions of the Chinese Society of Agricultural Engineering, V29, P12.
   Jiang Y, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0528-3.
   Jin XL, 2019, PLANT PHENOMICS, V2019, DOI 10.34133/2019/4820305.
   Kalman R.E., 1960, NEW APPROACH LINEAR, V82, P35, DOI {[}DOI 10.1115/1.3662552, 10.1115/1.3662552].
   Koirala A, 2019, PRECIS AGRIC, V20, P1107, DOI 10.1007/s11119-019-09642-0.
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053.
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1\_48.
   Lin YD, 2022, COMPUT ELECTRON AGR, V197, DOI 10.1016/j.compag.2022.106938.
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913.
   Lv JD, 2019, SCI HORTIC-AMSTERDAM, V256, DOI 10.1016/j.scienta.2019.108615.
   Ndou V, 2021, EUPHYTICA, V217, DOI 10.1007/s10681-021-02918-5.
   Qi JT, 2022, COMPUT ELECTRON AGR, V194, DOI 10.1016/j.compag.2022.106780.
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767.
   Stein M, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111915.
   Tzutalin, 2015, LABELIMG GIT CODE.
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203.
   Wang ZL, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19122742.
   Yang GF, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.600854.
   Zhai LC, 2018, J INTEGR AGR, V17, P2235, DOI {[}10.1016/S2095-3119(18)61917-3, 10.1016/s2095-3119(18)61917-3].
   Zhang WL, 2022, HORTIC RES-ENGLAND, V9, DOI 10.1093/hr/uhac003.
   Zhao Y, 2021, PLANT PHENOMICS, V2021, DOI 10.34133/2021/9874650.
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI {[}10.1109/TNNLS.2018.2876865, 10.23977/icamcs.2018.001].
   Zhi XY, 2016, J INTEGR AGR, V15, P1469, DOI 10.1016/S2095-3119(15)61174-1.
   Zhou W, 2021, INT J CIRCUITS SYST, V15, P634, DOI {[}10.46300/9106.2021.15.70, DOI 10.46300/9106.2021.15.70].},
Number-of-Cited-References = {31},
Times-Cited = {1},
Usage-Count-Last-180-days = {32},
Usage-Count-Since-2013 = {47},
Journal-ISO = {Front. Plant Sci.},
Doc-Delivery-Number = {6N8HK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000889791700001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000580810400049,
Author = {Fan, Boyuan and Yang, Rutong and Wu, Weihong and Li, Linfang and Wang,
   Shu'an and Li, Ya and Wang, Peng and Gao, Lulu and Fu, Li and Zhu,
   Jiangwei and Karimi-Maleh, Hassan and Zheng, Yuhong},
Title = {Development of an electrochemical technology for ten Clematis spp
   varieties identification},
Journal = {INTERNATIONAL JOURNAL OF ELECTROCHEMICAL SCIENCE},
Year = {2020},
Volume = {15},
Number = {10},
Pages = {10212-10220},
Month = {OCT},
Abstract = {The identification of ornamental plants has always been a challenge. Due
   to the complexity of breeding development, ornamental plants often have
   very complex genetic relationships with native plants. In this work, we
   proposed the use of glassy carbon electrodes to perform voltammetric
   scanning of extracts of plant leaves. Ten specific varieties of Clematis
   were selected as research targets. We recorded the voltammograms of
   these Clematis varieties under different conditions and found that
   different extraction solvents and buffer solutions can present different
   profiles. Integrating these voltammograms can be used to quickly
   identify varieties of Clematis. In addition to scatter plots and 2D
   density maps we previously proposed, we propose a new pattern
   recognition method in this work.},
Publisher = {ESG},
Address = {BORIVOJA STEVANOVICA 25-7, BELGRADE, 11000, SERBIA},
Type = {Article},
Language = {English},
Affiliation = {Wu, WH; Fu, L (Corresponding Author), Hangzhou Dianzi Univ, Coll Mat \& Environm Engn, Hangzhou 310018, Peoples R China.
   Fan, Boyuan; Wu, Weihong; Fu, Li, Hangzhou Dianzi Univ, Coll Mat \& Environm Engn, Hangzhou 310018, Peoples R China.
   Yang, Rutong; Li, Linfang; Wang, Shu'an; Li, Ya; Wang, Peng; Gao, Lulu; Zheng, Yuhong, Jiangsu Prov \& Chinese Acad Sci, Inst Bot, Mem Sun Yat Sen, Nanjing Bot Garden, Nanjing 210014, Peoples R China.
   Zhu, Jiangwei, Nanjing Forestry Univ, Coinnovat Ctr Sustainable Forestry Southern China, Nanjing 210037, Peoples R China.
   Karimi-Maleh, Hassan, Univ Elect Sci \& Technol China, Sch Resources \& Enviroment, POB 611731,Xiyuan Ave, Chengdu, Peoples R China.
   Karimi-Maleh, Hassan, Univ Johannesburg, Dept Chem Sci, POB 17011,Doornfontein Campus, ZA-2028 Johannesburg, South Africa.},
DOI = {10.20964/2020.10.79},
ISSN = {1452-3981},
Keywords = {Electrochemical sensor; Voltammograms; Plant identification; Pattern
   recognition; Clematis spp},
Keywords-Plus = {ANTIOXIDANT ACTIVITY; CLASSIFICATION; PERFORMANCE; FLAVONOIDS;
   ALKALOIDS; PROFILE; LEAVES; PLANTS; ACIDS; L.},
Research-Areas = {Electrochemistry},
Web-of-Science-Categories  = {Electrochemistry},
Author-Email = {whwu@hdu.edu.cn
   fuli@hdu.edu.cn},
Affiliations = {Hangzhou Dianzi University; Chinese Academy of Sciences; Nanjing
   Forestry University; University of Electronic Science \& Technology of
   China; University of Johannesburg},
ResearcherID-Numbers = {Karimi-maleh, Hassan/N-1727-2019
   Fu, Li/AAH-4689-2020
   Zheng, Yu/GRJ-5808-2022},
ORCID-Numbers = {Karimi-maleh, Hassan/0000-0002-1027-481X
   Fu, Li/0000-0002-5957-7790
   },
Funding-Acknowledgement = {National Natural Science Foundation of China {[}31800603]},
Funding-Text = {This work was funded by National Natural Science Foundation of China
   (31800603).},
Cited-References = {AlChoubassi G, 2018, TRAC-TREND ANAL CHEM, V104, P77, DOI 10.1016/j.trac.2017.11.006.
   Arvand M, 2018, TALANTA, V176, P92, DOI 10.1016/j.talanta.2017.08.012.
   Bahmanzadegan A, 2019, NAT PROD RES, V33, P2376, DOI 10.1080/14786419.2018.1440232.
   Baran E, 2019, ARAB J CHEM, V12, P4303, DOI 10.1016/j.arabjc.2016.06.008.
   Cakal D, 2019, DYES PIGMENTS, V161, P411, DOI 10.1016/j.dyepig.2018.10.002.
   Chohra D, 2020, S AFR J BOT, V132, P164, DOI 10.1016/j.sajb.2020.04.026.
   Chou SR, 2020, ECOL INDIC, V110, DOI 10.1016/j.ecolind.2019.105867.
   Chubatova NV, 2018, WULFENIA, V25, P161.
   Di Marino D, 2019, CHEMELECTROCHEM, V6, P1434, DOI 10.1002/celc.201801676.
   Fu L, 2020, REV MEX ING QUIM, V19, P803, DOI 10.24275/rmiq/Alim869.
   Fu L, 2020, BIOSENS BIOELECTRON, V159, DOI 10.1016/j.bios.2020.112212.
   Fu L, 2020, FRONT CHEM, V8, DOI 10.3389/fchem.2020.00092.
   Fu L, 2020, SENSOR ACTUAT B-CHEM, V304, DOI 10.1016/j.snb.2019.127390.
   Fu L, 2019, SENSOR ACTUAT B-CHEM, V298, DOI 10.1016/j.snb.2019.126836.
   Fu L, 2019, BIOELECTROCHEMISTRY, V129, P199, DOI 10.1016/j.bioelechem.2019.06.001.
   Garcia-Carmona L, 2019, TRAC-TREND ANAL CHEM, V118, P29, DOI 10.1016/j.trac.2019.05.020.
   Guo LX, 2019, PHARMACOL RES, V149, DOI 10.1016/j.phrs.2019.104459.
   Irakli M, 2018, IND CROP PROD, V124, P382, DOI 10.1016/j.indcrop.2018.07.070.
   Kaji H, 2018, PROCEEDINGS OF THE 2018 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2018 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (UBICOMP/ISWC'18 ADJUNCT), P86, DOI 10.1145/3267305.3267586.
   Khanbabaeva O., 2020, J AGR SCI, V12, P214.
   Kishi-Kaboshi M, 2018, PLANT PHYSIOL BIOCH, V131, P47, DOI 10.1016/j.plaphy.2018.03.015.
   Kramer K, 2019, J FUNCT FOODS, V57, P112, DOI 10.1016/j.jff.2019.03.039.
   Li MY, 2020, PEERJ, V8, DOI 10.7717/peerj.8729.
   Liu ZB, 2020, J FOOD COMPOS ANAL, V86, DOI 10.1016/j.jfca.2019.103385.
   Lu YJ, 2020, INT J ELECTROCHEM SC, V15, P758, DOI 10.20964/2020.01.64.
   Leiva AM, 2018, ECOL ENG, V120, P116, DOI 10.1016/j.ecoleng.2018.05.023.
   Marin-Saez J, 2019, FOOD CHEM, V287, P265, DOI 10.1016/j.foodchem.2019.02.091.
   Marosz A, 2018, J PLANT NUTR, V41, P2606, DOI 10.1080/01904167.2018.1510518.
   Mostafa M., 2018, Bangladesh Journal of Scientific and Industrial Research, V53, P185, DOI 10.3329/bjsir.v53i3.38264.
   Muthusamy K, 2018, J BIOMOL STRUCT DYN, V36, P4197, DOI 10.1080/07391102.2017.1409653.
   Qiu L, 2018, J ASIAN NAT PROD RES, V20, P1038, DOI 10.1080/10286020.2017.1387780.
   Rabiza-Swider J, 2019, NOT BOT HORTI AGROBO, V47, P432, DOI 10.15835/nbha47211379.
   Saidi R, 2019, S AFR J BOT, V123, P208, DOI 10.1016/j.sajb.2019.03.010.
   Saidi R, 2018, J OLEO SCI, V67, P1483, DOI 10.5650/jos.ess18056.
   Serrano N, 2018, TALANTA, V189, P296, DOI 10.1016/j.talanta.2018.06.085.
   Sun Y, 2020, PHYTOCHEM LETT, V37, P95, DOI 10.1016/j.phytol.2020.03.010.
   Suprun EV, 2019, TRAC-TREND ANAL CHEM, V116, P44, DOI 10.1016/j.trac.2019.04.019.
   Svorc L, 2018, MICROCHEM J, V142, P297, DOI 10.1016/j.microc.2018.07.007.
   Toscano S, 2019, HORTICULTURAE, V5, DOI 10.3390/horticulturae5010006.
   Volochanskyi O, 2019, SPECTROCHIM ACTA A, V207, P143, DOI 10.1016/j.saa.2018.09.009.
   Xiang QH, 2019, MITOCHONDRIAL DNA B, V4, P834, DOI 10.1080/23802359.2019.1567290.
   Xing RM, 2019, SENSOR ACTUAT B-CHEM, V283, P35, DOI 10.1016/j.snb.2018.11.129.
   Xiong YangYang, 2019, Southwest China Journal of Agricultural Sciences, V32, P615.
   Xu YT, 2020, BIOELECTROCHEMISTRY, V133, DOI 10.1016/j.bioelechem.2020.107455.
   Xue JR, 2019, INT J AGR BIOL ENG, V12, P123, DOI 10.25165/j.ijabe.20191202.4637.
   Xue YN, 2019, DYES PIGMENTS, V161, P489, DOI 10.1016/j.dyepig.2018.10.001.
   Yang Y, 2019, FLORA, V253, P67, DOI 10.1016/j.flora.2019.03.008.
   Ye C, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-36266-7.
   Yous F, 2018, S AFR J BOT, V119, P390, DOI 10.1016/j.sajb.2018.09.036.
   Yu YH, 2020, SCI HORTIC-AMSTERDAM, V267, DOI 10.1016/j.scienta.2020.109327.
   Zeng P, 2018, INT J PHYTOREMEDIAT, V20, P311, DOI 10.1080/15226514.2017.1381939.
   Zhang MJ, 2020, CHEMISTRYSELECT, V5, P5035, DOI 10.1002/slct.202001100.
   Zhou JT, 2020, INT J ELECTROCHEM SC, V15, P5793, DOI 10.20964/2020.06.27.},
Number-of-Cited-References = {53},
Times-Cited = {6},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Int. J. Electrochem. Sci.},
Doc-Delivery-Number = {OE8YZ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000580810400049},
OA = {hybrid},
DA = {2023-08-12},
}

@inproceedings{ WOS:000271604900100,
Author = {Zhang, Shanwen and Chau, Kwok-Wing},
Editor = {Huang, DS and Jo, KH and Lee, HH and Kang, HJ and Bevilacqua, V},
Title = {Dimension Reduction Using Semi-Supervised Locally Linear Embedding for
   Plant Leaf Classification},
Booktitle = {EMERGING INTELLIGENT COMPUTING TECHNOLOGY AND APPLICATIONS, PROCEEDINGS},
Series = {Lecture Notes in Computer Science},
Year = {2009},
Volume = {5754},
Pages = {948-955},
Note = {5th International Conference on Intelligent Computing, Ulsan, SOUTH
   KOREA, SEP 16-19, 2009},
Organization = {IEEE Computat Intelligence Soc; Int Neural Network Soc; Natl Nat Sci Fdn
   China},
Abstract = {Plant has plenty use in foodstuff, medicine and industry, and is also
   vitally important for environmental protection. So, it is important and
   urgent to recognize and classify plant species. Plant classification
   based on leaf images is a basic research of botanical area and
   agricultural production. Due to the high nature complexity and high
   dimensionality of leaf image data, dimensional reduction algorithms are
   useful and necessary for such type of data analysis, since it can
   facilitate fast classifying plants, and understanding and managing plant
   leaf features. Supervised locally linear embedding (SLLE) is a powerful
   feature extraction method, which can yield very promising recognition
   results when coupled with some simple classifiers. In this paper, a
   semi-SLLE is proposed and is applied to plant classification based on
   leaf images. The experiment results show that the proposed algorithm
   performs very well on leaf image data which exhibits a manifold
   structure.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Zhang, SW (Corresponding Author), Chinese Acad Sci, Inst Intelligent Machine, POB 1130, Hefei 230031, Anhui, Peoples R China.
   Zhang, Shanwen, Chinese Acad Sci, Inst Intelligent Machine, Hefei 230031, Anhui, Peoples R China.
   Chau, Kwok-Wing, Hong Kong Polytech Univ, Dept Civil \& Struct Engn, Kowloon, Hong Kong, Peoples R China.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-642-04069-6},
Keywords = {Plant leaf image; Plant classification; locally linear embedding;
   Supervised locally linear embedding (SLLE); Semi-SLLE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory \& Methods},
Author-Email = {wjdw716@163.com},
Affiliations = {Chinese Academy of Sciences; Hong Kong Polytechnic University},
ResearcherID-Numbers = {Chau, Kwok-wing/E-5235-2011},
ORCID-Numbers = {Chau, Kwok-wing/0000-0001-6457-161X},
Cited-References = {de Ridder D., 2002, PH200201 DELFT U TEC.
   DESILVA V, 2002, 705712 NIPS.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Horn R. A., 1990, MATRIX ANAL.
   {[}黄鸿 HUANG Hong], 2008, {[}计算机科学, Computer Science], V35, P220.
   Knapp AK, 2002, SCIENCE, V298, P2202, DOI 10.1126/science.1076347.
   KOUROPTEVA O, 2002, P 1 INT C FUZZ SYST, P359.
   Lee D, 2003, PROF ENG, V16, P19.
   MIAO Z, 2006, ENG APPL ARTIFICIAL, V19.
   Pillati M., 2005, P 29 ANN C GERM CLAS, P15.
   PLOTZEDE RD, 2005, CANADA J BOT, V83.
   Pounds JA, 2004, NATURE, V427, P107, DOI 10.1038/427107a.
   SHI C, 2004, APBC162, V16, P1.
   Wang YH, 2005, BIOINFORMATICS, V21, P1530, DOI 10.1093/bioinformatics/bti192.
   Ye Y., 2004, P 2004 INT S INT MUL.},
Number-of-Cited-References = {15},
Times-Cited = {86},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {8},
Doc-Delivery-Number = {BLZ67},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000271604900100},
DA = {2023-08-12},
}

@inproceedings{ WOS:000426703300051,
Author = {Kiani, Ehsan and Al Shahadat, Mohamad and Sadikoglu, Fahreddin},
Editor = {Aliev, RA and Pedrycz, W and Jamshidi, M and Kacprzyk, J},
Title = {Child perception-based plant species identification},
Booktitle = {9TH INTERNATIONAL CONFERENCE ON THEORY AND APPLICATION OF SOFT
   COMPUTING, COMPUTING WITH WORDS AND PERCEPTION, ICSCCW 2017},
Series = {Procedia Computer Science},
Year = {2017},
Volume = {120},
Pages = {357-364},
Note = {9th International Conference on Theory and Application of Soft
   Computing, Computing with Words and Perception (ICSCCW), Budapest,
   HUNGARY, AUG 22-25, 2017},
Abstract = {Automation of mechanically pulling the weeds out of the crop row not
   only copes well with the new European high environmental standards but
   also removes the high cost of mere conventional alternative as high
   hand-weeding. The objective of this research work is to propose a
   similar distinguishing methodology of a weeding labour when
   discriminating the weeds species to choose and remove the undesired
   ones. The method is governed by a systematic analysis carried out on
   recognition method of an immaturely trained human brain. In other words,
   a number of children, who have never seen a maize agricultural field,
   were examined while recognizing a maize pattern using at most five
   sample images. The proposed method works mainly based on morphological
   operators for extraction of fundamental plant features in the image. The
   advantage of the proposed method is producing similar results to human
   labour which is approximate identification. This final decision was made
   by a fuzzy classifier which generate the degree of membership to either
   of weed or crop plant groups. Unlike the very popular research trend for
   object classification in the literature, our proposed methodology
   neither requires huge sample of images nor a high capacity processor.
   The accuracy of maize plant discrimination from typical Mediterranean
   weeds under extreme July sun illumination was observed as 95\% for a
   scene of a single plant and 85\% for a scene containing multiple
   objects. (c) 2018 The Authors. Published by Elsevier B.V.},
Publisher = {ELSEVIER SCIENCE BV},
Address = {SARA BURGERHARTSTRAAT 25, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Kiani, E (Corresponding Author), Near East Univ, Dept Mechatron Engn, POB 99138, Nicosia, Cyprus.
   Kiani, Ehsan; Al Shahadat, Mohamad; Sadikoglu, Fahreddin, Near East Univ, Dept Mechatron Engn, POB 99138, Nicosia, Cyprus.},
DOI = {10.1016/j.procs.2017.11.250},
ISSN = {1877-0509},
Keywords = {Weed/crop plant classification; fuzzy decision maker; machine vision;
   in-row weeding; approximate plant identification (API)},
Keywords-Plus = {COMPUTER VISION; CLASSIFICATION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods},
Author-Email = {ehskiani@gmail.com},
Affiliations = {Near East University},
Cited-References = {Astrand B, 2005, MECHATRONICS, V15, P251, DOI 10.1016/j.mechatronics.2004.05.005.
   Dyrmann M, 2016, BIOSYST ENG, V151, P72, DOI 10.1016/j.biosystemseng.2016.08.024.
   Frank Poulsen Engineering, 2014, MECH HOEING ROB.
   Garford, 2011, ROBOCROP INROW.
   Giselsson TM, 2013, SENSORS-BASEL, V13, P5585, DOI 10.3390/s130505585.
   Haralick R. M., 1992, COMPUTER ROBOT VISIO, V1.
   Arribas JI, 2011, COMPUT ELECTRON AGR, V78, P9, DOI 10.1016/j.compag.2011.05.007.
   Kataoka T, 2003, IEEE ASME INT C ADV, P1079, DOI 10.1109/aim.2003.1225492.
   Lee W. S., 1999, Precision Agriculture, V1, P95, DOI 10.1023/A:1009977903204.
   Midtiby HS, 2016, BIOSYST ENG, V146, P183, DOI 10.1016/j.biosystemseng.2016.01.012.
   Perez-Ruiz M, 2014, BIOSYST ENG, V126, P45, DOI 10.1016/j.biosystemseng.2014.07.009.
   Slaughter DC, 2008, COMPUT ELECTRON AGR, V61, P63, DOI 10.1016/j.compag.2007.05.008.
   Sujaritha M, 2017, COMPUT ELECTRON AGR, V134, P160, DOI 10.1016/j.compag.2017.01.008.
   Tellaeche A, 2008, COMPUT ELECTRON AGR, V60, P144, DOI 10.1016/j.compag.2007.07.008.
   Torres-Sospedra J, 2014, BIOSYST ENG, V123, P40, DOI 10.1016/j.biosystemseng.2014.05.005.
   Weis M., 2008, BORNIMER AGRARTECHNI, V69, P138.
   Zhang BH, 2014, FOOD RES INT, V62, P326, DOI 10.1016/j.foodres.2014.03.012.},
Number-of-Cited-References = {17},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BJ6IB},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000426703300051},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:001013148100004,
Author = {Yang, Chengzhuan and Fang, Lincong and Yu, Qian and Wei, Hui},
Title = {A Learning Robust and Discriminative Shape Descriptor for Plant Species
   Identification},
Journal = {IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS},
Year = {2023},
Volume = {20},
Number = {1},
Pages = {39-51},
Month = {JAN-FEB},
Abstract = {Plant identification based on leaf images is a widely concerned
   application field in artificial intelligence and botany. The key problem
   is extracting robust discriminative features from leaf images and
   assigning a measure of similarity. This study proposes an effective,
   robust shape descriptor to identify plant species from images of their
   leaves, which we call the high-level triangle shape descriptor (HTSD).
   First, we extract a leaf image's external contour and internal salient
   point information. We then use triangle features to describe the leaf
   contour, which we call the contour point based on triangle features
   (CPTFs). The internal information of the leaf image is based on salient
   point triangle features (SPTFs). The third step is to apply the Fisher
   vector to encode the two kinds of pointbased local triangle features
   into the HTSD. Finally, we employ the simple euclidean distance to
   calculate the dissimilarities between the HTSD characteristics of leaf
   images. We have extensively evaluated the proposed approach on several
   public leaf datasets successfully. Experimental results show that our
   method has superior recognition accuracy, outperforming current
   state-of-the-art shape-based and deep-learning plant identification
   approaches.},
Publisher = {IEEE COMPUTER SOC},
Address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
Type = {Article},
Language = {English},
Affiliation = {Yang, CZ (Corresponding Author), Zhejiang Normal Univ, Sch Math \& Comp Sci, Jinhua 321004, Zhejiang, Peoples R China.
   Yang, Chengzhuan, Zhejiang Normal Univ, Sch Math \& Comp Sci, Jinhua 321004, Zhejiang, Peoples R China.
   Fang, Lincong, Zhejiang Univ Finance \& Econ, Sch Informat, Hangzhou 310018, Peoples R China.
   Yu, Qian, Fudan Univ, Sch Comp Sci, Shanghai 200438, Peoples R China.
   Yu, Qian, Jiangsu Univ Technol, Sch Comp Engn, Changzhou 213001, Peoples R China.
   Wei, Hui, Fudan Univ, Lab Cognit Algorithm \& Model, Shanghai Key Lab Data Sci, Sch Comp Sci, Shanghai 201203, Peoples R China.},
DOI = {10.1109/TCBB.2022.3148463},
ISSN = {1545-5963},
EISSN = {1557-9964},
Keywords = {Plant species recognition; shape descriptor; fisher vector; triangle
   feature},
Keywords-Plus = {CLASSIFICATION; RECOGNITION; DISTANCE; ROTATION},
Research-Areas = {Biochemistry \& Molecular Biology; Computer Science; Mathematics},
Web-of-Science-Categories  = {Biochemical Research Methods; Computer Science, Interdisciplinary
   Applications; Mathematics, Interdisciplinary Applications; Statistics \&
   Probability},
Author-Email = {chengzhuan\_yang@163.com
   lincongfang@zufe.edu.cn
   yuqian@jsut.edu.cn
   weihui@fudan.edu.cn},
Affiliations = {Zhejiang Normal University; Zhejiang University of Finance \& Economics;
   Fudan University; Jiangsu University of Technology; Fudan University},
Funding-Acknowledgement = {National Nature Science Foundation of China {[}62106227, 61902159,
   61771146]; Zhejiang Provincial Natural Science Foundation of China
   {[}LQ19F020003]; Shanghai Science and Technology Development Funds
   {[}13dz2260200, 13511504300]},
Funding-Text = {This work was supported in part by the National Nature Science
   Foundation of China under Grants 62106227, 61902159, and 61771146, in
   part by the Zhejiang Provincial Natural Science Foundation of China
   under Grant LQ19F020003, and in part by Shanghai Science and Technology
   Development Funds under Grants 13dz2260200 and 13511504300.},
Cited-References = {Alamoudi S, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207386.
   {[}Anonymous], 2012, PROC ACM INT WORKSHO.
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
   Chen X, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105714.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Ke QH, 2014, PROC CVPR IEEE, P4146, DOI 10.1109/CVPR.2014.528.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Pawara P, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107528.
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1\_11.
   Porikli F, 2018, IEEE SIGNAL PROC MAG, V35, P17, DOI 10.1109/MSP.2017.2766286.
   Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x.
   Schutze Hinrich, 2008, INTRO INFORM RETRIEV, V39.
   Shah MP, 2017, IEEE IMAGE PROC, P860, DOI 10.1109/ICIP.2017.8296403.
   Shao Y, 2019, COMPUT ELECTRON AGR, V158, P102, DOI 10.1016/j.compag.2019.01.022.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Tan T, 2018, IEEE-ACM T AUDIO SPE, V26, P1393, DOI 10.1109/TASLP.2018.2825432.
   Tavakoli H, 2021, COMPUT ELECTRON AGR, V181, DOI 10.1016/j.compag.2020.105935.
   Wang B, 2022, IEEE ACM T COMPUT BI, V19, P1018, DOI 10.1109/TCBB.2020.3031280.
   Wang B, 2017, PROC CVPR IEEE, P2047, DOI 10.1109/CVPR.2017.221.
   Wang B, 2014, IEEE T IMAGE PROCESS, V23, P4101, DOI 10.1109/TIP.2014.2343457.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xu YX, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107558.
   Yang CZ, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107809.
   Yang CZ, 2019, IEEE ACCESS, V7, P178108, DOI 10.1109/ACCESS.2019.2958416.
   Yang CZ, 2019, SIGNAL PROCESS-IMAGE, V71, P110, DOI 10.1016/j.image.2018.11.004.
   Yang CZ, 2016, FRONT ARTIF INTEL AP, V285, P269, DOI 10.3233/978-1-61499-672-9-269.
   Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738.
   Yu XH, 2021, PATTERN RECOGN, V119, DOI 10.1016/j.patcog.2021.108067.
   Zhang HX, 2020, NEUROCOMPUTING, V378, P283, DOI 10.1016/j.neucom.2019.10.077.
   Zhang SW, 2020, NEUROCOMPUTING, V408, P246, DOI 10.1016/j.neucom.2019.09.113.
   Zhang SW, 2020, KNOWL-BASED SYST, V200, DOI 10.1016/j.knosys.2020.105998.
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004.
   Zhao YF, 2022, IEEE ACM T COMPUT BI, V19, P1817, DOI 10.1109/TCBB.2021.3056683.
   Zhao ZQ, 2015, LECT NOTES COMPUT SC, V9004, P348, DOI 10.1007/978-3-319-16808-1\_24.
   Zhiyu Liu, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9226, P115, DOI 10.1007/978-3-319-22186-1\_11.},
Number-of-Cited-References = {44},
Times-Cited = {3},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {6},
Journal-ISO = {IEEE-ACM Trans. Comput. Biol. Bioinform.},
Doc-Delivery-Number = {J9ZZ6},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:001013148100004},
DA = {2023-08-12},
}

@inproceedings{ WOS:000462745200018,
Author = {Chechlinski, Lukasz and Siemiatkowska, Barbara and Majewski, Michal},
Editor = {Szewczyk, R and Zielinski, C and Kaliczynska, M},
Title = {A System for Weeds and Crops Identification Based on Convolutional
   Neural Network},
Booktitle = {AUTOMATION 2018: ADVANCES IN AUTOMATION, ROBOTICS AND MEASUREMENT
   TECHNIQUES},
Series = {Advances in Intelligent Systems and Computing},
Year = {2018},
Volume = {743},
Pages = {193-202},
Note = {International Conference on Automation, Warsaw, POLAND, MAR 21-23, 2018},
Abstract = {This paper presents an early step towards an autonomous weeding system.
   The system is based on the Deep Convolutional Neural Network (Deep
   ConvNet, CNN). CNNs reached state-of-the-art results in many computer
   vision tasks. However, their effectiveness is strongly related to the
   network architecture, as well as quality and quantity of the training
   data, and the data collection is a time-consuming process. In this
   paper, we will present how to find the first approximation of the
   network architecture and the data quantity, based on two sequences of
   100 crop images. The obtained accuracy level equals to 96-98\%. The
   presented approach will be used to train and test the CNN on larger
   datasets in the future work.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Chechlinski, L (Corresponding Author), Warsaw Univ Technol, Warsaw, Poland.
   Chechlinski, Lukasz; Siemiatkowska, Barbara, Warsaw Univ Technol, Warsaw, Poland.
   Majewski, Michal, MCMS Warka Sp Zoo, Warka, Poland.},
DOI = {10.1007/978-3-319-77179-3\_18},
ISSN = {2194-5357},
EISSN = {2194-5365},
ISBN = {978-3-319-77179-3; 978-3-319-77178-6},
Keywords = {Automated weeding; ConvNet; Agricultural robotics},
Keywords-Plus = {PLANT-IDENTIFICATION; SHAPE; FEATURES; TEXTURE},
Research-Areas = {Automation \& Control Systems; Robotics},
Web-of-Science-Categories  = {Automation \& Control Systems; Robotics},
Author-Email = {l.chechlinski@mchtr.pw.edu.pl},
Affiliations = {Warsaw University of Technology},
ResearcherID-Numbers = {Siemiatkowska, Barbara/ACW-0934-2022
   },
ORCID-Numbers = {Chechlinski, Lukasz/0000-0002-4500-9741
   Siemiatkowska, Barbara/0000-0002-7691-1375},
Funding-Acknowledgement = {Polish NCBiR {[}POIR.01.01.01-00-0974/16]},
Funding-Text = {This work was founded by Polish NCBiR grant POIR.01.01.01-00-0974/16.},
Cited-References = {Aranda M. C., 2010, P ACM INT C IM VID R, P327, DOI {[}10.1145/1816041.1816089, DOI 10.1145/1816041.1816089].
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Bonnet P, 2016, MULTIMED TOOLS APPL, V75, P1647, DOI 10.1007/s11042-015-2607-4.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Cerutti G, 2013, IEEE IMAGE PROC, P1471, DOI 10.1109/ICIP.2013.6738302.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Lee Stefan, 2015, CORR.
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410.
   Mystkowska I., 2017, Progress in Plant Protection, V57, P21.
   Parekh R., 2012, INT J ADV ENG TECHNO, V2, P149.
   Priyankara HAC, 2015, 2015 MORATUWA ENGINEERING RESEARCH CONFERENCE (MERCON), P148, DOI 10.1109/MERCon.2015.7112336.
   Reyes AK, 2015, P WORK NOT CLEF 2015.
   Sardana V, 2017, CROP PROT, V95, P1, DOI 10.1016/j.cropro.2016.09.011.
   Sermanet P., 2013, ARXIV PREPRINT ARXIV, V1312, P6229.
   Simonyan K., 2014, 14091556V6 ARXIV, P1, DOI DOI 10.48550/ARXIV.1409.1556.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Tsolakidis DG, 2014, LECT NOTES ARTIF INT, V8445, P406, DOI 10.1007/978-3-319-07064-3\_33.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wu S. G., 2007, CORR.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.},
Number-of-Cited-References = {20},
Times-Cited = {4},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BM3WC},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000462745200018},
DA = {2023-08-12},
}

@article{ WOS:000075662800037,
Author = {Meyer, GE and Mehta, T and Kocher, M and Mortensen, DA and Samal, A},
Title = {Textural imaging and discriminant analysis for distinguishing weeds for
   spot spraying},
Journal = {TRANSACTIONS OF THE ASAE},
Year = {1998},
Volume = {41},
Number = {4},
Pages = {1189-1197},
Month = {JUL-AUG},
Abstract = {Advanced computer vision and statistical methods were employed for
   identifying living plants from soil/residue background for two species
   of grasses (Shattercane, Green Foxtail) and two broadleaf species
   (Velvetleaf; Red Root Pigweed) weeds. The excess green index method was
   used as a contrast enhancement for specifically identifying plant from
   soil regions. Excess green classified plant and soil regions correctly
   over the entire three-week observation period with high accuracies (99\%
   plus). Plant and soil binary images were derived from excess green
   images and provided edge boundaries. These boundaries were used with
   corresponding gray scale images to extract four classical textural
   features for plants and soil: angular second moment, inertia, entropy,
   and local homogeneity. These features were derived from the
   co-occurrence matrix. Stepwise and canonical discriminant analyses were
   used to test the classification performance of the texture and excess
   green features. Discrimination models of local homogeneity, inertia, and
   angular second moment were found to classify grass and broadleaf
   categories of plants, with classification accuracies of 93 and 85\%,
   respectively. Classification accuracies of individual species only
   ranged from 30 to 77\%. Soil classification accuracies were also high
   for textural feature algorithms (97\%). The time required to produce
   tokensets ranged from 15 to 20 s on a UNIX computer system. Additional
   time required for the system to reach a plant/soil classification ranged
   from 5 to 20 s. This translated into an overall system response time of
   20 to 30 s, with the preprocessing step constituting the major part of
   the system response time.},
Publisher = {AMER SOC AGRICULTURAL \& BIOLOGICAL ENGINEERS},
Address = {2950 NILES RD, ST JOSEPH, MI 49085-9659 USA},
Type = {Article},
Language = {English},
Affiliation = {Meyer, GE (Corresponding Author), Univ Nebraska, Dept Biol Syst Engn, 250 LW Chase Hall, Lincoln, NE 68583 USA.
   Univ Nebraska, Dept Comp Sci \& Engn, Lincoln, NE 68588 USA.},
ISSN = {0001-2351},
Keywords = {image processing; discriminant analysis; texture; color index},
Keywords-Plus = {PLANT-IDENTIFICATION; MACHINE VISION; MATRICES; SYSTEM; LEAF; SOIL},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agricultural Engineering},
Author-Email = {gem@engecsl.unl.edu},
Affiliations = {University of Nebraska System; University of Nebraska Lincoln},
ResearcherID-Numbers = {Kocher, Mininder S./K-1604-2019},
Cited-References = {{*}ALD CORP, 1993, ALD PHOT US MAN VER.
   BALLARD DH, 1982, COMPUTER VISION, pCH6.
   BARTELS PH, 1969, ACTA CYTOL, V13, P210.
   Chang YC, 1996, T ASAE, V39, P263, DOI 10.13031/2013.27506.
   DARLING EM, 1968, IEEE T SYST SCI CYB, VSSC4, P38, DOI 10.1109/TSSC.1968.300186.
   DAVIS LS, 1979, IEEE T PATTERN ANAL, V1, P251, DOI 10.1109/TPAMI.1979.4766921.
   FRANZ E, 1991, T ASAE, V34, P682.
   GONZALEZ RC, 1992, DIGITAL IMAGE PROCES, pCH1.
   GUYER DE, 1986, T ASAE, V29, P1500.
   GUYER DE, 1993, T ASAE, V36, P163.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   JULESZ B, 1975, SCI AM, V232, P34, DOI 10.1038/scientificamerican0475-34.
   MEHTA T, 1995, THESIS U NEBRASKA LI.
   MEYER GE, 1987, T ASAE, V30, P242.
   RAMAEKERS BP, 1996, THESIS U NEBRASKA LI.
   ROSENFELD A, 1970, PICTURE PROCESSING P, P562.
   {*}SAS I INC, 1990, SAS STAT US GUID VER, V1.
   SHEARER SA, 1990, T ASAE, V33, P2037.
   Shearer SA, 1996, T ASAE, V39, P1209, DOI 10.13031/2013.27614.
   TOMITO F, 1990, COMPUTER ANAL VISUAL, pCH1.
   WESZKA JS, 1976, IEEE T SYST MAN CYB, V6, P269, DOI 10.1109/TSMC.1976.5408777.
   WOEBBECKE DM, 1995, T ASAE, V38, P271, DOI 10.13031/2013.27839.
   WOEBBECKE DM, 1995, T ASAE, V38, P259, DOI 10.13031/2013.27838.
   Yonekawa S, 1996, T ASAE, V39, P1525, DOI 10.13031/2013.27647.
   ZAYAS I, 1990, T ASAE, V33, P1642, DOI 10.13031/2013.31521.},
Number-of-Cited-References = {26},
Times-Cited = {131},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {18},
Journal-ISO = {Trans. ASAE},
Doc-Delivery-Number = {115KE},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000075662800037},
DA = {2023-08-12},
}

@article{ WOS:000475871000100,
Author = {Kazerouni, Masoud Fathi and Saeed, Nazeer T. Mohammed and Kuhnert,
   Klaus-Dieter},
Title = {Fully-automatic natural plant recognition system using deep neural
   network for dynamic outdoor environments},
Journal = {SN APPLIED SCIENCES},
Year = {2019},
Volume = {1},
Number = {7},
Month = {JUL},
Abstract = {The leaf, as the vital part of plants, can be affected by physical and
   physiological factors which might lead to changes in its shape, color
   and size. These unique parts play an essential role in the design and
   implementation of plant recognition systems, as the shapes of leaves
   vary among different plants. Weather type and related factors, such as
   light intensity, humidity, temperature and wind-speed, may have effects
   on the number of leaves that grow on a plant, the amount of fresh and
   dried leaves, the deformation of leaves, color of leaves, positions of
   leaves on branches, etc. In addition, photographing in outdoor
   environments with different weather conditions has undesired impacts on
   images. For instance, light scattering changes severely when there are
   water droplets during photographing which influences the images captured
   in rainy weather. Despite the importance of the proposed factors and the
   relevant effects on the plants, leaves and images taken from plants, a
   plant recognition system should be independent from all environmental
   and non-environmental factors to be practically useful and applicable
   for identifying plant species in uncontrolled outdoor environments.
   Moreover, changes in the time of day (morning, noon and evening), the
   distance between camera and plant species as well as the angle and
   illumination of the object and environment when the images are taken
   should be considered in the process of plant recognition. For instance,
   in a windy weather condition, for an observer even from a short
   distance, it is challenging to distinguish all single leaves and
   identify the plant type from the shape of its leaf. Furthermore, the
   unstructured background of the images is another challenge which affects
   the images captured in fields and outdoor environments. These are only
   some difficulties for identification of plants in natural environments
   such as farms, forests, etc. A consideration of mentioned factors
   contributes to developing a novel, efficient and accurate system for
   plant recognition that can be used in various uncontrolled outdoor
   environments. In the real-life, the images of the plants captured with
   the presence of these factors are very challenging for recognizing plant
   species and it is a desire to develop an efficient and accurate system
   that can be used in various natural conditions and uncontrolled
   situations. This paper presents the development and implementation of a
   convolutional neural network for automatic plant recognition in
   uncontrolled outdoor environments. The deep network has been used to
   recognize four different natural plant species. The proposed system
   brings the opportunity and transformative potential of deep neural
   networks to the plant recognition field. This fully-automatic system is
   efficient and generalized for recognition of plants in outdoor
   environments like forests and farms, and its the accuracy is 99.5\%.
   Meanwhile, the final system has the functionality of being applied as a
   real-time one.},
Publisher = {SPRINGER INT PUBL AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Kazerouni, MF (Corresponding Author), Univ Siegen, Inst Real Time Learning Syst, Holderlinstr 3, D-57076 Siegen, Germany.
   Kazerouni, Masoud Fathi; Saeed, Nazeer T. Mohammed; Kuhnert, Klaus-Dieter, Univ Siegen, Inst Real Time Learning Syst, Holderlinstr 3, D-57076 Siegen, Germany.},
DOI = {10.1007/s42452-019-0785-9},
Article-Number = {756},
ISSN = {2523-3963},
EISSN = {2523-3971},
Keywords = {Deep learning; Convolutional neural networks; Natural plant recognition;
   Fully-automatic system; Dynamic environment},
Keywords-Plus = {CLASSIFICATION; MODEL},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {masoud.fathi@uni-siegen.de},
Affiliations = {Universitat Siegen},
Cited-References = {Abbasi S, 1997, LECT NOTES COMPUT SC, V1252, P284.
   {[}Anonymous], 1990, STAT PATTERN RECOGNI.
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364.
   Baghban A, 2019, ENG APPL COMP FLUID, V13, P26, DOI 10.1080/19942060.2018.1542345.
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
   Bell R.M., 2007, SIGKDD EXPLOR NEWSL, P75, DOI DOI 10.1145/1345448.1345465.
   Bernaschi M, 2011, P 12 INT RAD S SEPT, P315.
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324.
   Chen I. J., 2003, Business Process Management Journal, V9, P672, DOI 10.1108/14637150310496758.
   Cheng CH, 2002, EUR J OPER RES, V142, P174, DOI 10.1016/S0377-2217(01)00280-6.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Fathi Kazerouni M, 2015, ADV IMAGE VIDEO PROC, V3, P10.
   Fisher RA, 1938, ANN EUGENIC, V8, P376, DOI 10.1111/j.1469-1809.1938.tb02189.x.
   Gaber T, 2015, ADV INTELL SYST COMP, V368, P375, DOI 10.1007/978-3-319-19719-7\_33.
   Gwo CY, 2013, APPL PLANT SCI, V1, DOI 10.3732/apps.1200005.
   Hanley J.A., 2014, RECEIVER OPERATING C.
   Harris C, 1988, P 4 ALV VIS C, V15, P10.
   Hinton G. E., 2012, ARXIV, DOI DOI 10.9774/GLEAF.978-1-909493-38-4\_2.
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
   Kazerouni MF, 2017, COMPUT SCI RES NOTES, V2702, P81.
   Kazerouni MF., 2017, INT J COMPUT ELECT A, V10, P1497.
   Kazerouni MF, 2015, SIGNAL IMAGE PROCESS, V6, P1.
   Kebapci H, 2010, COMPUT J ADV ACCESS, V54, P1475.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lab L, 2017, CONVOLUTIONAL NEURAL.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   Lee CL, 2006, INT J IMAG SYST TECH, V16, P15, DOI 10.1002/ima.20063.
   Lowe DG., 1999, P 7 IEEE INT C COMPU, V2, P1150, DOI {[}10.1109/iccv.1999.790410, DOI 10.1109/ICCV.1999.790410].
   Manning C., 1999, FDN STAT NATURAL LAN.
   Moazenzadeh R, 2018, ENG APPL COMP FLUID, V12, P584, DOI 10.1080/19942060.2018.1482476.
   Nair V., 2010, ICML, P8, DOI DOI 10.5555/3104322.3104425.
   Provost F, 1998, MACH LEARN, V30, P127, DOI 10.1023/A:1007442505281.
   Quinlan J., 1993, C4 5 PROGRAMS MACHIN.
   Rosten E., 2006, EUR C COMP VIS, V1, P430, DOI DOI 10.1007/11744023\_34.
   Sabour S., 2017, ADV NEURAL INFORM PR, P3856.
   Sakai N, 1996, J FOOD ENG, V27, P397, DOI 10.1016/0260-8774(95)00022-4.
   Samadianfard S, 2019, ENG APPL COMP FLUID, V13, P142, DOI 10.1080/19942060.2018.1560364.
   Sasaki Y., 2007, TRUTH F MEASURE, V1, P1.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Szegedy C, 2014, Arxiv, DOI {[}arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594].
   Wu CL, 2011, J HYDROL, V399, P394, DOI 10.1016/j.jhydrol.2011.01.017.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yaseen ZM, 2019, J HYDROL, V569, P387, DOI 10.1016/j.jhydrol.2018.11.069.
   Yosinski J, 2015, UNDERSTANDING NEURAL.
   Zhang SW, 2009, LECT NOTES COMPUT SC, V5754, P948, DOI 10.1007/978-3-642-04070-2\_100.},
Number-of-Cited-References = {45},
Times-Cited = {9},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Journal-ISO = {SN Appl. Sci.},
Doc-Delivery-Number = {IJ4JT},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000475871000100},
OA = {Bronze},
DA = {2023-08-12},
}

@article{ WOS:000431747500016,
Author = {Riegler-Nurscher, Peter and Prankl, Johann and Bauer, Thomas and
   Strauss, Peter and Prank, Heinrich},
Title = {A machine learning approach for pixel wise classification of residue and
   vegetation cover under field conditions},
Journal = {BIOSYSTEMS ENGINEERING},
Year = {2018},
Volume = {169},
Pages = {188-198},
Month = {MAY},
Abstract = {Soil cover is a crucial factor for sustainable cultivation of arable
   land. A certain degree of residue and vegetation cover reduces erosion
   significantly and has positive effects on plant development. In order to
   accomplish these positive effects, it is necessary to measure and
   control the amount of soil cover on fields. Manual measurement methods
   are time consuming and/or subjective. Available image analysis methods
   often lack of generalisation and accuracy. Many approaches only focus on
   residue or on vegetation cover and do not consider different camera
   hardware. Recent advancements in machine learning techniques are
   promising to overcome these issues. The proposed method, the entangled
   random forest, a variant of a random decision forest, classifies
   individual pixels into soil, residue, living plants and stones. Simple
   and efficient pixel-wise comparisons to neighbouring pixels are
   integrated as decision-features into the random forest. To validate our
   method, the result of the automatic classification was compared with
   results of manual classifications from evaluators on image grid points.
   The classification of soil results in a regression equation between the
   results of the new introduced method and a manual image classification
   of y = 0.99x + 2.02 (R-2 = 0.93). Living plant classification results in
   a regression between both methods in y = 0.94x 0.70 with (R-2 = 0.98)
   and for dead residues in y = 1.04x 0.64 (R-2 = 0.84). It is possible to
   access a demo of the algorithm by using a web and a mobile application
   on https://soilcover.josephinum.at. (C) 2018 IAgrE. Published by
   Elsevier Ltd. All rights reserved.},
Publisher = {ACADEMIC PRESS INC ELSEVIER SCIENCE},
Address = {525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA},
Type = {Article},
Language = {English},
Affiliation = {Riegler-Nurscher, P (Corresponding Author), Josephinum Res, Rottenhauser Str 1, Wieselburg, Austria.
   Riegler-Nurscher, Peter; Prankl, Johann; Prank, Heinrich, Josephinum Res, Rottenhauser Str 1, Wieselburg, Austria.
   Bauer, Thomas; Strauss, Peter, Fed Agcy Water Management, Inst Land \& Water Management Res, Petzenkirchen, Austria.},
DOI = {10.1016/j.biosystemseng.2018.02.011},
ISSN = {1537-5110},
EISSN = {1537-5129},
Keywords = {20 image classification; Random forest; Soil cover estimation},
Keywords-Plus = {IMAGE-ANALYSIS; SOIL-EROSION; SEGMENTATION; PHOTOGRAPHY; TILLAGE},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agricultural Engineering; Agriculture, Multidisciplinary},
Author-Email = {p.riegler-nurscher@josephinum.at},
ResearcherID-Numbers = {Strauss, Peter/AAP-4084-2020},
ORCID-Numbers = {Strauss, Peter/0000-0002-8693-9304},
Funding-Acknowledgement = {Lower Austrian government; Austrian Research Promotion Agency; FWF
   Project of Bio-divERsA/FACCE-JPI {[}I-2043-B-25]},
Funding-Text = {The research leading to this work has received funding from the Lower
   Austrian government and the Austrian Research Promotion Agency.
   Furthermore this contribution is partly funded by the FWF Project
   I-2043-B-25 (Vinedivers) of Bio-divERsA/FACCE-JPI.},
Cited-References = {Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120.
   {[}Anonymous], 2013, DECISION FORESTCOM, DOI DOI 10.5555/2462584.
   Bai XD, 2014, BIOSYST ENG, V125, P80, DOI 10.1016/j.biosystemseng.2014.06.015.
   Bauer T, 2014, CATENA, V113, P363, DOI 10.1016/j.catena.2013.08.022.
   Behrens T, 2006, J AGRON CROP SCI, V192, P295, DOI 10.1111/j.1439-037X.2006.00211.x.
   Bennett LT, 2000, J RANGE MANAGE, V53, P634, DOI 10.2307/4003159.
   Blanco H., 2008, PRINCIPLES SOIL CONS.
   Blanco-Canqui H, 2008, SOIL SCI SOC AM J, V72, P693, DOI 10.2136/sssaj2007.0233.
   Bloschl G, 2016, HYDROL EARTH SYST SC, V20, P227, DOI 10.5194/hess-20-227-2016.
   Booth DT, 2005, ARID LAND RES MANAG, V19, P91, DOI 10.1080/15324980590916486.
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324.
   Bundesministerium fur Landwirtschaft Forstwirtschaft Umwelt und Wasserwirtschaft, 2014, BMLFUWLE118008923201.
   Campillo C, 2008, HORTSCIENCE, V43, P1780, DOI 10.21273/HORTSCI.43.6.1780.
   CARTER MR, 1994, SOIL TILL RES, V31, P289, DOI 10.1016/0167-1987(94)90037-X.
   CORAK SJ, 1993, J SOIL WATER CONSERV, V48, P70.
   Fageria NK, 2005, COMMUN SOIL SCI PLAN, V36, P2733, DOI 10.1080/00103620500303939.
   Garcia-Mateos G, 2015, AGR WATER MANAGE, V151, P158, DOI 10.1016/j.agwat.2014.08.010.
   Gislum R., 2016, CIGR AGENG C, P1.
   Guerrero JM, 2012, EXPERT SYST APPL, V39, P11149, DOI 10.1016/j.eswa.2012.03.040.
   Hamuda E, 2016, COMPUT ELECTRON AGR, V125, P184, DOI 10.1016/j.compag.2016.04.024.
   HARTWIG RO, 1978, J SOIL WATER CONSERV, V33, P90.
   Hosl R, 2016, CATENA, V137, P44, DOI 10.1016/j.catena.2015.08.009.
   Kirci M., 2014, 3 INT C AGR.
   Koller K, 2007, LANDTECHNIK, V62, P382.
   LAFLEN JM, 1981, J SOIL WATER CONSERV, V36, P341.
   Laliberte AS, 2010, J SPAT SCI, V55, P101, DOI 10.1080/14498596.2010.487853.
   Laliberte A. S., 2006, P ASPRS 2006 ANN C R.
   Luscier JD, 2006, FRONT ECOL ENVIRON, V4, P408, DOI 10.1890/1540-9295(2006)4{[}408:UDPAOI]2.0.CO;2.
   Marques MJ, 2007, SCI TOTAL ENVIRON, V378, P161, DOI 10.1016/j.scitotenv.2007.01.043.
   Mohammad AG, 2010, CATENA, V81, P97, DOI 10.1016/j.catena.2010.01.008.
   Montillo A, 2011, LECT NOTES COMPUT SC, V6801, P184, DOI 10.1007/978-3-642-22092-0\_16.
   MORRISON JE, 1993, J SOIL WATER CONSERV, V48, P478.
   Munsell color company, 1954, MUNSELL SOIL COLOR C.
   Obade V. de P., 2012, J ENV PROT, V3, P211, DOI DOI 10.4236/jep.2012.32026.
   Perez-Cabello F, 2012, J ARID ENVIRON, V76, P88, DOI 10.1016/j.jaridenv.2011.08.007.
   Pforte F, 2012, BIOSYST ENG, V112, P121, DOI 10.1016/j.biosystemseng.2012.03.005.
   Purcell LC, 2000, CROP SCI, V40, P834, DOI 10.2135/cropsci2000.403834x.
   Shotton J., 2010, IJCV 2010.
   Strauss P., 2003, P C 25 YEARS ASS ER, P545.
   Vosshenrich H. H., 2003, Landtechnik, V58, P92.
   Wischmeier W. H., 1978, U. S. Department of Agriculture, Agriculture Handbook.
   Wolf D, 2016, IEEE ROBOT AUTOM LET, V1, P49, DOI 10.1109/LRA.2015.2506118.},
Number-of-Cited-References = {42},
Times-Cited = {18},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {22},
Journal-ISO = {Biosyst. Eng.},
Doc-Delivery-Number = {GF2DC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000431747500016},
DA = {2023-08-12},
}

@inproceedings{ WOS:000371393800112,
Author = {Jassmann, Timothy J. and Tashakkori, Rahman and Parry, R. Mitchell},
Book-Group-Author = {IEEE},
Title = {Leaf Classification Utilizing a Convolutional Neural Network},
Booktitle = {IEEE SOUTHEASTCON 2015},
Series = {IEEE SoutheastCon-Proceedings},
Year = {2015},
Note = {IEEE Southeast Conference (IEEE SoutheastCon), Fort Lauderdale, FL, APR
   09-12, 2015},
Organization = {IEEE},
Abstract = {Tree identification is an important task in biological research. Since
   not all biologists are qualified for this task and experts are not
   accessible all the time, an application to assist in plant
   identification would be extremely useful. This paper describes an
   application classifies the type of a tree based on a picture of one of
   its leaves. The system developed for this research utilizes a
   convolutional neural network in an android mobile application to
   classify natural images of leaves.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Jassmann, TJ (Corresponding Author), Appalachian State Univ, Dept Comp Sci, Boone, NC 28608 USA.
   Jassmann, Timothy J.; Tashakkori, Rahman; Parry, R. Mitchell, Appalachian State Univ, Dept Comp Sci, Boone, NC 28608 USA.},
ISSN = {1091-0050},
ISBN = {978-1-4673-7300-5},
Keywords = {leaf classification; convolutional neural network; image processing},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \&
   Electronic},
Author-Email = {jassmanntj@appstate.edu
   tashakkorir@appstate.edu
   parryrm@appstate.edu},
Affiliations = {University of North Carolina; Appalachian State University},
Cited-References = {{[}Anonymous], 2003, P 7 INT C DOC AN REC.
   {[}Anonymous], 2014, PLANT ID 2012.
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123.
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295.
   Mehrotra K., 2013, P 4 INT WORKSH MULT.
   Ng A., 2013, UFLDL TUTORIAL.
   Shrestha B., 2010, CLASSIFICATION PLANT.},
Number-of-Cited-References = {7},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BE3YR},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000371393800112},
DA = {2023-08-12},
}

@article{ WOS:000414783300005,
Author = {Rzanny, Michael and Seeland, Marco and Waeldchen, Jana and Maeder,
   Patrick},
Title = {Acquiring and preprocessing leaf images for automated plant
   identification: understanding the tradeoff between effort and
   information gain},
Journal = {PLANT METHODS},
Year = {2017},
Volume = {13},
Month = {NOV 8},
Abstract = {Background: Automated species identification is a long term research
   subject. Contrary to flowers and fruits, leaves are available throughout
   most of the year. Offering margin and texture to characterize a species,
   they are the most studied organ for automated identification.
   Substantially matured machine learning techniques generate the need for
   more training data (aka leaf images). Researchers as well as enthusiasts
   miss guidance on how to acquire suitable training images in an efficient
   way.
   Methods: In this paper, we systematically study nine image types and
   three preprocessing strategies. Image types vary in terms of in-situ
   image recording conditions: perspective, illumination, and background,
   while the preprocessing strategies compare non-preprocessed, cropped,
   and segmented images to each other. Per image type-preprocessing
   combination, we also quantify the manual effort required for their
   implementation. We extract image features using a convolutional neural
   network, classify species using the resulting feature vectors and
   discuss classification accuracy in relation to the required effort per
   combination.
   Results: The most effective, non-destructive way to record herbaceous
   leaves is to take an image of the leaf's top side. We yield the highest
   classification accuracy using destructive back light images, i.e.,
   holding the plucked leaf against the sky for image acquisition. Cropping
   the image to the leaf's boundary substantially improves accuracy, while
   precise segmentation yields similar accuracy at a substantially higher
   effort. The permanent use or disuse of a flash light has negligible
   effects. Imaging the typically stronger textured backside of a leaf does
   not result in higher accuracy, but notably increases the acquisition
   cost.
   Conclusions: In conclusion, the way in which leaf images are acquired
   and preprocessed does have a substantial effect on the accuracy of the
   classifier trained on them. For the first time, this study provides a
   systematic guideline allowing researchers to spend available acquisition
   resources wisely while yielding the optimal classification accuracy.},
Publisher = {BMC},
Address = {CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Waldchen, J (Corresponding Author), Max Planck Inst Biogeochem, Dept Biogeochem Integrat, Hans Knoll Str 10, D-07745 Jena, Germany.
   Rzanny, Michael; Waeldchen, Jana, Max Planck Inst Biogeochem, Dept Biogeochem Integrat, Hans Knoll Str 10, D-07745 Jena, Germany.
   Seeland, Marco; Maeder, Patrick, Tech Univ Ilmenau, Inst Comp \& Syst Engn, Helmholtzpl 5, D-98693 Ilmenau, Germany.},
DOI = {10.1186/s13007-017-0245-8},
Article-Number = {97},
EISSN = {1746-4811},
Keywords = {Leaf image; Image acquisition; Preprocessing; Segmentation; Cropping;
   Background; Leaf side; Back light; Effort; CNN; Computer vision},
Keywords-Plus = {SYSTEM; LEAVES},
Research-Areas = {Biochemistry \& Molecular Biology; Plant Sciences},
Web-of-Science-Categories  = {Biochemical Research Methods; Plant Sciences},
Author-Email = {jwald@bgc-jena.mpg.de},
Affiliations = {Max Planck Society; Technische Universitat Ilmenau},
ResearcherID-Numbers = {Rzanny, Michael/GOH-1028-2022
   Seeland, Marco/H-1028-2011
   Mäder, Patrick/A-1848-2018},
ORCID-Numbers = {Rzanny, Michael/0000-0002-7232-5547
   Seeland, Marco/0000-0001-7204-3972
   Mäder, Patrick/0000-0001-6871-2707},
Funding-Acknowledgement = {German Ministry of Education and Research (BMBF) {[}01LC1319A,
   01LC1319B]; German Federal Ministry for the Environment; Nature
   Conservation, Building and Nuclear Safety (BMUB) {[}3514685C19];
   Stiftung Naturschutz Thuringen (SNT) {[}SNT-082-248-03/2014]; NVIDIA
   Corporation},
Funding-Text = {We are funded by the German Ministry of Education and Research (BMBF)
   Grants: 01LC1319A and 01LC1319B; the German Federal Ministry for the
   Environment, Nature Conservation, Building and Nuclear Safety (BMUB)
   Grant: 3514685C19; and the Stiftung Naturschutz Thuringen (SNT) Grant:
   SNT-082-248-03/2014. We would like to acknowledge the support of NVIDIA
   Corporation with the donation of a TitanX GPU used for this research. We
   thank Anke Bebber for carefully editing, which substantially improved
   our manuscript. We further thank Sandra Schau, Lars Lenkardt and Alice
   Deggelmann for assistance during the field recordings.},
Cited-References = {Aranda M. C., 2010, P ACM INT C IM VID R, P327, DOI {[}10.1145/1816041.1816089, DOI 10.1145/1816041.1816089].
   Austen GE, 2016, SCI REP-UK, V6, DOI 10.1038/srep33634.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Ceballos G, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1400253.
   Cerutti G, 2013, COMPUT VIS IMAGE UND, V117, P1482, DOI 10.1016/j.cviu.2013.07.003.
   Champ J, 2015, CLEF2015 WORKING NOT, V1391.
   Chatfield K., 2014, ARXIV PREPRINT ARXIV.
   Choi Sungbin, 2015, CLEF.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Farnsworth EJ, 2013, BIOSCIENCE, V63, P891, DOI 10.1525/bio.2013.63.11.8.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Goeau H., 2014, CLEF2014 WORKING NOT, P598.
   Grand-Brochier M, 2013, P 38 WORKSH GEOTH RE, P7.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Jia Y., 2014, P 22 ACM INT C MULT, P675.
   Joly A, 2016, MULTIMEDIA SYST, V22, P751, DOI 10.1007/s00530-015-0462-9.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Mader P., 2016, GERM COL WORKSH, P145.
   McGuinness K, 2010, PATTERN RECOGN, V43, P434, DOI 10.1016/j.patcog.2009.03.008.
   Meyer D., 2017, E1071 MISC FUNCTIONS.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Mzoughi O, 2013, IEEE INT CON MULTI.
   Peng B, 2013, PATTERN RECOGN, V46, P1020, DOI 10.1016/j.patcog.2012.09.015.
   R Core Team, 2020, R LANG ENV STAT COMP.
   Razavian SA, 2014, ARXIV14036382.
   Reyes AK, 2015, P WORK NOT CLEF 2015.
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720.
   Rzanny M, 2017, HARVARD DATAVERSE.
   Seeland M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170629.
   Soares JVB, 2013, MACH VISION APPL, V24, P1623, DOI 10.1007/s00138-013-0530-0.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wing MKCJ, 2016, CARET CLASSIFICATION.},
Number-of-Cited-References = {33},
Times-Cited = {35},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {18},
Journal-ISO = {Plant Methods},
Doc-Delivery-Number = {FM1ZR},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000414783300005},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000244836400005,
Author = {Okamoto, Hiroshi and Murata, Tetsuro and Kataoka, Takashi and Hata,
   Shun-Ichi},
Title = {Plant classification for weed detection using hyperspectral imaging with
   wavelet analysis},
Journal = {WEED BIOLOGY AND MANAGEMENT},
Year = {2007},
Volume = {7},
Number = {1},
Pages = {31-37},
Month = {MAR},
Abstract = {The goal of this study is to develop a new weed detection method that
   can be applied for automatic mechanical weed control. For successful
   weed detection, plants must be classified into crops and weeds according
   to their species. In this study, we employed a portable hyperspectral
   imaging system. The hyperspectral camera can capture landscape images
   that include crops, weeds, and the soil surface, and can provide more
   extensive information than conventional red, green, and blue (RGB)
   images. Although RGB images consist of red, green, and blue wavebands,
   the obtained hyperspectral images consist of 240 wavebands of spectral
   information. Hyperspectral imaging is expected to provide powerful
   technology for agricultural sensing. In the initial step of this study,
   the image pixels of the plants (crop or weeds) were segmented from the
   background soil surface using Euclidean distance as the discriminant
   function. In the next step, the image pixels of the crop (sugarbeet) and
   weeds (four species) were classified using the difference in the
   spectral characteristics of the plant species. In this process,
   classification variables were generated using wavelet transformation for
   data compression, noise reduction, and feature extraction, and then
   stepwise linear discriminant analysis was applied. The validation
   results indicate that the developed classification method has potential
   for practical use.},
Publisher = {BLACKWELL PUBLISHING},
Address = {9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Okamoto, H (Corresponding Author), Hokkaido Univ, Res Fac Agr, Kita Ku, Kita 9,Nishi 9, Sapporo, Hokkaido 0608589, Japan.
   Hokkaido Univ, Res Fac Agr, Kita Ku, Sapporo, Hokkaido 0608589, Japan.},
DOI = {10.1111/j.1445-6664.2006.00234.x},
ISSN = {1444-6162},
Keywords = {discriminant analysis; image processing; machine vision; weed control},
Keywords-Plus = {TEXTURE; VISION},
Research-Areas = {Agriculture; Plant Sciences},
Web-of-Science-Categories  = {Agronomy; Plant Sciences},
Author-Email = {hiro@bpe.agr.hokudai.ac.jp},
Affiliations = {Hokkaido University},
ResearcherID-Numbers = {Kataoka, Takashi/A-4503-2012},
Cited-References = {Astrand B, 2002, AUTON ROBOT, V13, P21, DOI 10.1023/A:1015674004201.
   Burks TF, 2000, T ASAE, V43, P1029, DOI 10.13031/2013.2971.
   Burks TF, 2000, T ASAE, V43, P441, DOI 10.13031/2013.2723.
   El-Faki MS, 2000, T ASAE, V43, P1001, DOI 10.13031/2013.2968.
   El-Faki MS, 2000, T ASAE, V43, P1969, DOI 10.13031/2013.3103.
   NOBLE SD, 2001, 2001 ASAE ANN INT M.
   Okamoto H., 2006, Agricultural Information Research, V15, P103, DOI 10.3173/air.15.103.
   Okamoto H., 2006, Agricultural Information Research, V15, P219, DOI 10.3173/air.15.219.
   SLAUGHTER DC, 2003, 2003 ASAE ANN INT M.
   TERAWAKI M, 2002, P AUT TECHN OFF ROAD, P129.
   YE X, 2006, 3 IFAC INT WORKSH BI, P165.},
Number-of-Cited-References = {11},
Times-Cited = {53},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {37},
Journal-ISO = {Weed Biol. Manag.},
Doc-Delivery-Number = {144ZZ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000244836400005},
DA = {2023-08-12},
}

@article{ WOS:000262825800028,
Author = {Tang, L. and Tian, L. F.},
Title = {PLANT IDENTIFICATION IN MOSAICKED CROP ROW IMAGES FOR AUTOMATIC EMERGED
   CORN PLANT SPACING MEASUREMENT},
Journal = {TRANSACTIONS OF THE ASABE},
Year = {2008},
Volume = {51},
Number = {6},
Pages = {2181-2191},
Month = {NOV-DEC},
Abstract = {Image processing algorithms for, individual corn plant and plant stein
   center identification were developed. These algorithms were applied to
   mosaicked crop row image for automatically measuring cot-it plant
   spacing at early growth stages. These algorithms utilized multiple
   sources of information for corn plant detection and plant center
   location estimation including plant color, plant morphological features,
   and the crop row centerline. The algorithm was tested over two 41 in
   (134.5 ft) long corn rows using video acquired two times in both
   directions. The system had a mean plant misidentification ratio of
   3.7\%. When compared with manual plant spacing measurements, the system
   achieved an overall spacing error (RMSE) of 1.7 cm and an overall R(2)
   of 0.96 between manual plant spacing measurement and the system
   estimates. The developed image processing algorithms were effective in
   automated corn plant spacing measurement at early growth stages.
   Interplant spacing errors were mainly due to crop damage and sampling
   platform vibration that caused mosaicking errors.},
Publisher = {AMER SOC AGRICULTURAL \& BIOLOGICAL ENGINEERS},
Address = {2950 NILES RD, ST JOSEPH, MI 49085-9659 USA},
Type = {Article},
Language = {English},
Affiliation = {Tang, L (Corresponding Author), Iowa State Univ, Dept Agr \& Biosyst Engn, 203 Davidson Hall, Ames, IA 50011 USA.
   Tang, L., Iowa State Univ, Dept Agr \& Biosyst Engn, Ames, IA 50011 USA.
   Tian, L. F., Univ Illinois, Dept Agr \& Biol Engn, Urbana, IL 61801 USA.},
ISSN = {0001-2351},
Keywords = {Corn plant spacing measurement; Image processing; Machine vision; Robust
   line fitting; Planters},
Keywords-Plus = {VISION; POPULATION},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agricultural Engineering},
Author-Email = {lietang@iastate.edu},
Affiliations = {Iowa State University; University of Illinois System; University of
   Illinois Urbana-Champaign},
Funding-Acknowledgement = {Hatch Act and State of Iowa funds {[}3612]; Illinois Council of Food and
   Agricultural Research (C-FAR) {[}1DACF 01-DS-3-1 AE]; University of
   Illinois; Deere Company},
Funding-Text = {This research of the Iowa Agriculture and Home Economics Experiment
   Station, Ames, Iowa (Project No. 3612) was supported by Hatch Act and
   State of Iowa funds. This research has been supported by the Illinois
   Council of Food and Agricultural Research (C-FAR, Project No. 1DACF
   01-DS-3-1 AE), the University of Illinois, and Deere \& Company.},
Cited-References = {Billingsley J, 1997, COMPUT ELECTRON AGR, V16, P147, DOI 10.1016/S0168-1699(96)00034-8.
   DAVIES ER, 1997, MACHINE VISION THEOR, P195.
   DUDANI SA, 1978, PATTERN RECOGN, V10, P145, DOI 10.1016/0031-3203(78)90023-7.
   Forsyth D.A., 2002, COMPUTER VISION MODE.
   Hough P.V.C., 1962, U.S. Patent, Patent No. 3,069,654.
   JIA JC, 1991, P SOC PHOTO-OPT INS, V1379, P246, DOI 10.1117/12.25095.
   KLETTE R, 1996, HDB IMAGE PROCESSING, P313.
   Marchant J. A., 1995, Real-Time Imaging, V1, P363, DOI 10.1006/rtim.1995.1036.
   MARCHANT JA, 1996, COMPUTERS ELECT AGR, V15, P559.
   MEER P, 1991, INT J COMPUT VISION, V6, P59, DOI 10.1007/BF00127126.
   Nielsen R. L., 2005, EFFECT PLANT SPACING.
   NIELSEN RL, 1995, J PROD AGRIC, V8, P391, DOI 10.2134/jpa1995.0391.
   PARKER JR, 1997, ALGORITHMS IMAGE PRO, P176.
   Press W. H., 1992, NUMERICAL RECIPES C, P656.
   REID JF, 1986, 863042 ASAE.
   ROSENFELD A, 1975, INFORM CONTROL, V29, P286, DOI 10.1016/S0019-9958(75)90448-9.
   Rosenfeld A., 1982, DIGITAL PICTURE PROC, VVolume 2.
   Shrestha DS, 2005, APPL ENG AGRIC, V21, P295.
   Shrestha DS, 2004, BIOSYST ENG, V89, P119, DOI 10.1016/j.biosystemseng.2004.06.007.
   Shrestha DS, 2003, T ASAE, V46, P559, DOI 10.13031/2013.12945.
   SHRESTHA DS, 2004, 043058 ASAE.
   SLAUGHTER DC, 1997, 971079 ASAE.
   Sogaard H. T., 1999, Precision agriculture `99, Part 1 and Part 2. Papers presented at the 2nd European Conference on Precision Agriculture, Odense, Denmark, 11-15 July 1999, P181.
   Steward B. L., 1998, Proceedings of SPIE, V3543, P266.
   Tang L, 2008, T ASABE, V51, P1079, DOI 10.13031/2013.24510.
   TRUCCO E, 1998, INTRO TECHNIQUES 3 D, P95.
   VANDERLIP R L, 1988, Applied Agricultural Research, V3, P116.},
Number-of-Cited-References = {27},
Times-Cited = {22},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {12},
Journal-ISO = {Trans. ASABE},
Doc-Delivery-Number = {399UW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000262825800028},
DA = {2023-08-12},
}

@article{ WOS:000870936900001,
Author = {Bilgili, Aysin and Bilgili, Ali Volkan and Tenekeci, Mehmet Emin and
   Karadag, Kerim},
Title = {Spectral characterization and classification of two different crown root
   rot and vascular wilt diseases (fusarium oxysporum f.sp. radicis
   lycopersici and fusarium solani) in tomato plants using different
   machine learning algorithms},
Journal = {EUROPEAN JOURNAL OF PLANT PATHOLOGY},
Year = {2023},
Volume = {165},
Number = {2},
Pages = {271-286},
Month = {FEB},
Abstract = {Fusarium oxysporum f.sp. radicis lycopersici (FORL) and Fusarium solani
   (F.S.) are common fungi responsible for crown root rot and vascular wilt
   diseases that highly impact the development of plants, causing
   significant yield losses. This study investigated changes in the
   hyperspectral reflectance of normal and Fusarium (FORL and
   F.S.)-infected tomato plants in a growth chamber at different disease
   stages (3, 10, 16, 23, 31 and 37 days after inoculation (DAI)) using a
   spectroradiometer as an alternative to traditional approaches for the
   early identification and classification of such diseases. Raw spectra,
   significant wavebands obtained with the RELIEF algorithm and various
   statistical features extracted from raw spectra were used to classify
   healthy and infected plants using three different classification
   algorithms (CAs): decision tree, cubic support vector machine and
   k-nearest neighbor models. At different stages of the disease, the
   spectral bands such as 508, 711, 540, 717, 536, 644 nm and 705, 1883,
   525, 518, 444, 522 nm were the most effective in distinguishing FORL and
   F.S.-inoculated plants from healthy plants, respectively. While FORL
   caused general stress in the plants, F.S. also had a negative
   physiological effect. All CAs proved highly successful in distinguishing
   healthy and diseased plants, with maximum classification accuracy
   achieved as early as 3 DAI. CAs using statistical parameters as input
   had higher accuracies than other CAs. Healthy and diseased plant
   classification was significantly different between the different CAs (p
   < 0.05), while DAI, pathogen type and inputs of the classification did
   not exhibit significant differences in classification (p > 0.05)
   according to ANOVA.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Bilgili, A (Corresponding Author), GAP Agr Res Inst GAPTAEM, Dept Plant Hlth, TR-63100 Sanliurfa, Turkey.
   Bilgili, Aysin, GAP Agr Res Inst GAPTAEM, Dept Plant Hlth, TR-63100 Sanliurfa, Turkey.
   Bilgili, Ali Volkan, Harran Univ, Agr Fac, Dept Soil Sci \& Plant Nutr, Osmanbey Campus, TR-63300 Sanliurfa, Turkey.
   Tenekeci, Mehmet Emin, Harran Univ, Fac Engn, Dept Comp Engn, Osmanbey Campus, TR-63300 Sanliurfa, Turkey.
   Karadag, Kerim, Harran Univ, Fac Engn, Dept Elect \& Elect Engn, Osmanbey Campus, TR-63300 Sanliurfa, Turkey.},
DOI = {10.1007/s10658-022-02605-8},
EarlyAccessDate = {OCT 2022},
ISSN = {0929-1873},
EISSN = {1573-8469},
Keywords = {Tomatoes; Fusarium spp; Machine learning algorithms; Visible
   near-infrared spectroradiometry; Classification; ANOVA},
Keywords-Plus = {POWDERY MILDEW; IDENTIFICATION; SYMPTOMS},
Research-Areas = {Agriculture; Plant Sciences},
Web-of-Science-Categories  = {Agronomy; Plant Sciences; Horticulture},
Author-Email = {aysin.bilgili@tatimorman.gov.tr},
Affiliations = {Harran University; Harran University; Harran University},
ResearcherID-Numbers = {Bilgili, Ayşin/ISS-0927-2023
   Tenekeci, Mehmet Emin/A-4156-2018
   Bilgili, Ali/ABG-5636-2020
   karadag, kerim/AAZ-9117-2020},
ORCID-Numbers = {Bilgili, Ayşin/0000-0003-0801-0484
   Bilgili, Ali/0000-0002-4727-8283
   },
Cited-References = {Abdulridha J, 2016, AGRICULTURE-BASEL, V6, DOI 10.3390/agriculture6040056.
   Ashourloo D, 2016, IEEE GEOSCI REMOTE S, V13, P851, DOI 10.1109/LGRS.2016.2550529.
   Aydemir O, 2013, INT J INNOV COMPUT I, V9, P1145.
   Bauriegel E, 2011, COMPUT ELECTRON AGR, V75, P304, DOI 10.1016/j.compag.2010.12.006.
   Bilgili A., 2017, DETERMINATION ROOT R.
   Bilgili A., 2019, 2 INT AGR C ABSTRACT, P14.
   Bilgili A.V., 2018, INT 7 PLANT PROTECTI, P115.
   Bravo C., 2003, REMOTE SENS ENVIRON, V48, P253.
   Marin-Ortiz JC, 2020, SAUDI J BIOL SCI, V27, P88, DOI 10.1016/j.sjbs.2019.05.007.
   Delalieux S, 2007, EUR J AGRON, V27, P130, DOI 10.1016/j.eja.2007.02.005.
   GIRALDO-BETANCOURT CRISTHIAN, 2020, rev.colomb.cienc.hortic., V14, P301, DOI 10.17584/rcch.2020v14i3.10938.
   Heim RHJ, 2018, PLANT PATHOL, V67, P1114, DOI 10.1111/ppa.12830.
   Hennessy A, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12010113.
   Herrmann I, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10030426.
   Junges AH, 2018, PHYTOPATHOL MEDITERR, V57, P399, DOI 10.14601/Phytopathol\_Mediterr-22862.
   Karadag K, 2020, SUSTAIN COMPUT-INFOR, V28, DOI 10.1016/j.suscom.2019.01.001.
   Lecoq H., 1991, TECHNIQUES INOCULATI.
   Lowe A, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0233-z.
   Mahlein AK, 2013, REMOTE SENS ENVIRON, V128, P21, DOI 10.1016/j.rse.2012.09.019.
   Mahlein AK, 2010, PRECIS AGRIC, V11, P413, DOI 10.1007/s11119-010-9180-7.
   Mahlein AK, 2012, PLANT METHODS, V8, DOI 10.1186/1746-4811-8-3.
   Mutlu N, 2015, THEOR APPL GENET, V128, P1791, DOI 10.1007/s00122-015-2547-4.
   Naidu RA, 2009, COMPUT ELECTRON AGR, V66, P38, DOI 10.1016/j.compag.2008.11.007.
   Odilbekov F, 2018, FRONT PLANT SCI, V9, DOI 10.3389/fpls.2018.00685.
   Pithan PA, 2021, INT J REMOTE SENS, V42, P5680, DOI 10.1080/01431161.2021.1929542.
   Ray SS, 2011, J INDIAN SOC REMOTE, V39, P161, DOI 10.1007/s12524-011-0094-2.
   Riggins JJ, 2011, SOUTH J APPL FOR, V35, P18, DOI 10.1093/sjaf/35.1.18.
   Sankaran S, 2010, COMPUT ELECTRON AGR, V72, P1, DOI 10.1016/j.compag.2010.02.007.
   Sterling A, 2020, EUR J PLANT PATHOL, V156, P1063, DOI 10.1007/s10658-020-01961-7.
   UPOV, 2013, TOM GUID COND TESTS.
   Yilmaz S., 2014, Turk Silahli Kuvvetleri,  Koruyucu Hekimlik Bulteni, V13, P421.
   Zhang JC, 2012, COMPUT ELECTRON AGR, V85, P13, DOI 10.1016/j.compag.2012.03.006.},
Number-of-Cited-References = {32},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {10},
Journal-ISO = {Eur. J. Plant Pathol.},
Doc-Delivery-Number = {8F6KD},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000870936900001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000373789200068,
Author = {Jamil, Nursuriati and Hussin, Nuril Aslina Che and Nordin, Sharifalillah
   and Awang, Khalil},
Editor = {Yussof, H and Miskon, MF},
Title = {Automatic Plant Identification: Is Shape the Key Feature?},
Booktitle = {2015 IEEE INTERNATIONAL SYMPOSIUM ON ROBOTICS AND INTELLIGENT SENSORS
   (IEEE IRIS2015)},
Series = {Procedia Computer Science},
Year = {2015},
Volume = {76},
Pages = {436-442},
Note = {IEEE International Symposium on Robotics and Intelligent Sensors (IEEE
   IRIS), Langkawi, MALAYSIA, OCT 18-20, 2015},
Organization = {IEEE; IEEE Robot \& Automat Soc, Malaysia Chapter; Ctr Excellence
   Humanoid Robots \& Bio Sensing},
Abstract = {Shape is the most popular feature used in plant leaf identification, be
   it manual or automatic plant identification. In this paper, a study is
   conducted to investigate the most contributing features among three
   low-level features for plant leaf identification. Intra- and inter-class
   identification are conducted using 455 herbal medicinal plant leaves,
   with 70\% allocated for training and 30\% for testing dataset. Shape
   feature is extracted using Scale Invariant Feature Transform (SIFT);
   colour is represented using colour moments; and Segmentation-Based
   Fractal Texture Analysis (SFTA) is utilized to describe texture feature.
   Intra-class analysis showed that fusion of texture and shape surpassed
   fusion of texture, shape and colour. Single texture feature
   identification also achieved highest identification rate compared to
   identification using colour or shape. Inter-class analysis further
   support texture to be the discriminative feature among the low-level
   features. Results demonstrate that single texture feature outperformed
   colour or shape feature achieving 92\% identification rate. Furthermore,
   fusion of all three features accomplished 94\% identification rate. (c)
   2015 The Authors. Published by Elsevier B.V.},
Publisher = {ELSEVIER SCIENCE BV},
Address = {SARA BURGERHARTSTRAAT 25, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Jamil, N (Corresponding Author), Univ Teknol MARA, Fac Comp \& Math Sci, Digital Image Audio \& Speech Technol Grp DIAST, Shah Alam 40450, Selangor, Malaysia.
   Jamil, Nursuriati; Hussin, Nuril Aslina Che; Nordin, Sharifalillah; Awang, Khalil, Univ Teknol MARA, Fac Comp \& Math Sci, Digital Image Audio \& Speech Technol Grp DIAST, Shah Alam 40450, Selangor, Malaysia.},
DOI = {10.1016/j.procs.2015.12.287},
ISSN = {1877-0509},
Keywords = {plant identification; feature extraction; SIFT; colour moment; fractal
   texture analysis},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Robotics},
Web-of-Science-Categories  = {Robotics},
Author-Email = {lizajamil@computer.org},
Affiliations = {Universiti Teknologi MARA},
ResearcherID-Numbers = {JAMIL, PROFESOR DR NURSURIATI/HLG-1516-2023
   JAMIL, NURSURIATI/D-1494-2012
   Nordin, Sharifalillah/ABB-6094-2021
   Jamil, Nursuriati/AAV-9816-2020},
ORCID-Numbers = {JAMIL, NURSURIATI/0000-0003-4634-9833
   },
Cited-References = {Abdul K., 2012, J THEORETICAL APPL I, V41, P83.
   {[}Anonymous], 1995, STORAGE RETRIEVAL IM, DOI {[}10.1117/12.205308, DOI 10.1117/12.205308].
   Arora A, CLEF ONL WORK NOT LA.
   Babatunde O., 2015, Journal of Agricultural Informatics, V6, P61.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   Chen MY, 2010, P SPIE DEFENSE SECUR, V7704.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Costa A. F., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P39, DOI 10.1109/SIBGRAPI.2012.15.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Liao PS, 2001, J INF SCI ENG, V17, P713.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   PETRY W, 1989, J AGRON CROP SCI, V163, P345, DOI 10.1111/j.1439-037X.1989.tb00777.x.
   Rashad M. Z., 2011, International Journal of Computer Science \& Information Technology, V3, P93, DOI 10.5121/ijcsit.2011.3407.
   Russin NAC, 2013, IEEE CONF OPEN SYST, P226, DOI 10.1109/ICOS.2013.6735079.
   Schneider C.K., 1912, HDB LAUBHOLZ KUNDE.
   Wang F, 2015, PLANT METHODS, V11, DOI 10.1186/s13007-015-0049-7.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yu H, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P929, DOI 10.1109/ICIP.2002.1039125.
   Zhao ZQ, 2015, NEUROCOMPUTING, V151, P1112, DOI 10.1016/j.neucom.2014.02.077.},
Number-of-Cited-References = {20},
Times-Cited = {22},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BE6AY},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000373789200068},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000787788000049,
Author = {Joly, Alexis and Goeau, Herve and Kahl, Stefan and Picek, Lukas and
   Lorieul, Titouan and Cole, Elijah and Deneu, Benjamin and Servajean,
   Maximilien and Durso, Andrew and Bolon, Isabelle and Glotin, Herve and
   Planque, Robert and Vellinga, Willem-Pier and Klinck, Holger and Denton,
   Tom and Eggel, Ivan and Bonnet, Pierre and Muller, Henning and Sulc,
   Milan},
Editor = {Hagen, M and Verberne, S and Macdonald, C and Seifert, C and Balog, K and Norvag, K and Setty, V},
Title = {LifeCLEF 2022 Teaser: An Evaluation of Machine-Learning Based Species
   Identification and Species Distribution Prediction},
Booktitle = {ADVANCES IN INFORMATION RETRIEVAL, PT II},
Series = {Lecture Notes in Computer Science},
Year = {2022},
Volume = {13186},
Pages = {390-399},
Note = {44th European Conference on Information Retrieval (ECIR), Stavanger,
   NORWAY, APR 10-14, 2022},
Organization = {Univ Stavanger; British Comp Soc, Informat Retrieval Specialist Grp;
   Amazon; Bloomberg; Cobrainer; Elsevier; Google; L3S Res Ctr;
   MediaFutures; Norwegian Univ Sci \& Technol; NorwAI; Schibsted; SIGIR;
   Signal AI; Spotify; Springer; Textkernel; Thomson Reuters; Vespa AI;
   Wayfair; Norwegian Univ Sci \& Technol, Dept Comp Sci},
Abstract = {Building accurate knowledge of the identity, the geographic distribution
   and the evolution of species is essential for the sustainable
   development of humanity, as well as for biodiversity conservation.
   However, the difficulty of identifying plants, animals and fungi is
   hindering the aggregation of new data and knowledge. Identifying and
   naming living organisms is almost impossible for the general public and
   is often difficult even for professionals and naturalists. Bridging this
   gap is a key step towards enabling effective biodiversity monitoring
   systems. The LifeCLEF campaign, presented in this paper, has been
   promoting and evaluating advances in this domain since 2011. The 2022
   edition proposes five data-oriented challenges related to the
   identification and prediction of biodiversity: (i) P1antCLEF: very
   large-scale plant identification, (ii) BirdCLEF: bird species
   recognition in audio soundscapes, (iii) GeoLifeCLEF: remote sensing
   based prediction of species, (iv) SnakeCLEF: Snake Species
   Identification in Medically Important scenarios, and (v) FungiCLEF:
   Fungi recognition from images and metadata.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Joly, A (Corresponding Author), Univ Montpellier, CNRS, LIRMM, INRIA, Montpellier, France.
   Joly, Alexis; Lorieul, Titouan; Deneu, Benjamin, Univ Montpellier, CNRS, LIRMM, INRIA, Montpellier, France.
   Goeau, Herve; Bonnet, Pierre, CIRAD, UMR AMAP, Montpellier, Occitanie, France.
   Glotin, Herve, Univ Toulon \& Var, Aix Marseille Univ, DYNI Team, LIS,CNRS, Marseille, France.
   Planque, Robert; Vellinga, Willem-Pier, Xeno Canto Fdn, Amsterdam, Netherlands.
   Eggel, Ivan; Muller, Henning, HES SO, Sierre, Switzerland.
   Kahl, Stefan; Klinck, Holger, Cornell Univ, KLYCCB, Cornell Lab Ornithol, Ithaca, NY USA.
   Servajean, Maximilien, Univ Paul Valery Montpellier, Univ Montpellier, CNRS, AMIS,LIRMM, Montpellier, France.
   Bolon, Isabelle, UNIGE, Dept Community Hlth \& Med, ISG, Geneva, Switzerland.
   Cole, Elijah, CALTECH, Dept Comp \& Math Sci, Pasadena, CA 91125 USA.
   Picek, Lukas, Univ West Bohemia, Dept Cybernet, FAV, Plzen, Czech Republic.
   Durso, Andrew, Florida Gulf Coast Univ, Dept Biol Sci, Ft Myers, FL USA.
   Denton, Tom, Google LLC, San Francisco, CA USA.
   Sulc, Milan, Czech Tech Univ, Dept Cybernet, FEE, Prague, Czech Republic.},
DOI = {10.1007/978-3-030-99739-7\_49},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-030-99739-7; 978-3-030-99738-0},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory \& Methods},
Author-Email = {alexis.joly@inria.fr},
Affiliations = {Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; Inria; CIRAD; Centre National de la Recherche Scientifique
   (CNRS); Institut de Recherche pour le Developpement (IRD); Universite de
   Montpellier; Centre National de la Recherche Scientifique (CNRS);
   UDICE-French Research Universities; Aix-Marseille Universite; Universite
   de Toulon; University of Applied Sciences \& Arts Western Switzerland;
   Cornell University; Centre National de la Recherche Scientifique (CNRS);
   Universite Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; CIRAD; University of Geneva; California Institute of
   Technology; University of West Bohemia Pilsen; State University System
   of Florida; Florida Gulf Coast University; Czech Technical University
   Prague},
ResearcherID-Numbers = {Picek, Lukáš/HLX-8615-2023
   Servajean, Maximilien/IQW-9683-2023
   Picek, Lukas/GXG-4988-2022
   Durso, Andrew/D-1657-2012
   Sonka, Milan/F-6227-2017
   Cole, Elijah/M-5428-2017},
ORCID-Numbers = {Picek, Lukáš/0000-0002-6041-9722
   Servajean, Maximilien/0000-0002-9426-2583
   Durso, Andrew/0000-0002-3008-7763
   Sonka, Milan/0000-0002-9613-9968
   Goeau, Herve/0000-0003-3296-3795
   Sulc, Milan/0000-0002-6321-0131
   Bonnet, Pierre/0000-0002-2828-4389
   Bolon, Isabelle/0000-0001-5940-2731
   Kahl, Stefan/0000-0002-2411-8877
   Deneu, Benjamin/0000-0003-0640-5706
   joly, alexis/0000-0002-2161-9940
   Cole, Elijah/0000-0001-6623-0966},
Funding-Acknowledgement = {European Union {[}863463]; DigitAG},
Funding-Text = {This project has received funding from the European Union's Horizon 2020
   research and innovation programme under grant agreement No. 863463
   (Cos4Cloud project), and the support of \#.DigitAG},
Cited-References = {Affouard A., 2017, P ICLR INT C LEARN R.
   {[}Anonymous], 2013, NIPS INT C NEUR INF.
   Bonnet P, 2018, MULTIMED SYST APPL, P131, DOI 10.1007/978-3-319-76445-0\_8.
   Cai JH, 2007, PROCEEDINGS OF THE 2007 INTERNATIONAL CONFERENCE ON INTELLIGENT SENSORS, SENSOR NETWORKS AND INFORMATION PROCESSING, P293.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Glotin H., 2013, P 1 WORKSH MACH LEAR.
   Goeau H., 2013, CLEF TASK OV 2013 CL.
   Goeau H., 2012, CLEF TASK OV 2012 CL.
   Goeau H., 2011, CLEF TASK OV 2011 CL.
   Joly Alexis, 2021, Experimental IR Meets Multilinguality, Multimodality, and Interaction: 12th International Conference of the CLEF Association, CLEF 2021, Proceedings. Lecture Notes in Computer Science, Information Systems and Applications, incl. Internet/Web, and HCI (12880), P371, DOI 10.1007/978-3-030-85251-1\_24.
   Joly Alexis, 2017, Experimental IR Meets Multilinguality, Multimodality, and Interaction. 8th International Conference of the CLEF Association, CLEF 2017. Proceedings: LNCS 10456, P255, DOI 10.1007/978-3-319-65813-1\_24.
   Joly Alexis, 2020, Experimental IR Meets Multilinguality, Multimodality, and Interaction. 11th International Conference of the CLEF Association, CLEF 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12260), P342, DOI 10.1007/978-3-030-58219-7\_23.
   Joly A., 2014, LNCS, P229, DOI DOI 10.1007/978-3-319-11382-1\_20.
   Joly A, 2019, LECT NOTES COMPUT SC, V11696, P387, DOI 10.1007/978-3-030-28577-7\_29.
   Joly A, 2016, LECT NOTES COMPUT SC, V9822, P286, DOI 10.1007/978-3-319-44564-9\_26.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Jones G.J., 2018, LNCS.
   Koh LP, 2004, SCIENCE, V305, P1632, DOI 10.1126/science.1101101.
   Lee DJ, 2004, PROC SPIE, V5606, P37, DOI 10.1117/12.571789.
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504.
   Picek L., 2022, P IEEE CVF WINT C AP.
   Picek L., 2020, CLEF TASK OV 2020 CL.
   Picek L., 2021, WORKING NOTES CLEF 2.
   Towsey M, 2012, BIOACOUSTICS, V21, P107, DOI 10.1080/09524622.2011.648753.
   Trifa VM, 2008, J ACOUST SOC AM, V123, P2424, DOI 10.1121/1.2839017.
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914.
   Villon S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-67573-7.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52.},
Number-of-Cited-References = {33},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BT0GX},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000787788000049},
OA = {Green Submitted},
DA = {2023-08-12},
}

@inproceedings{ WOS:000354608900030,
Author = {Khairuddin, Anis Salwa Mohd and Khalid, Marzuki and Yusof, Rubiyah},
Editor = {Tsihrintzis, GA and Virvou, M and Jain, LC and Howlett, RJ},
Title = {Using Two Stage Classification for Improved Tropical Wood Species
   Recognition System},
Booktitle = {INTELLIGENT INTERACTIVE MULTIMEDIA SYSTEMS AND SERVICES (IIMSS 2011)},
Series = {Smart Innovation Systems and Technologies},
Year = {2011},
Volume = {11},
Pages = {305-314},
Note = {4th International Conference on Intelligent Interactive Multimedia
   Systems and Services (IIMSS), Piraeus, GREECE, JUL 20-22, 2011},
Organization = {Dept Informat Univ Piraeus; Univ S Australia, Sch Elect \& Informat
   Engn; KES Int},
Abstract = {An automated wood recognition system is designed based on five stages:
   data acquisition, pre-processing images, feature extraction, pre
   classification and classification. The proposed system is able to
   identify 52 types of wood species based on wood features extracted using
   Basic Grey Level Aura Matrix (BGLAM) technique and statistical
   properties of pores distribution (SPPD) technique. The features obtained
   from both feature extractors are fused together and will determine the
   classification between the various wood species. In order to enhance the
   class separability, a pre-classification stage is developed which
   includes clustering and dimension reduction. K-means clustering is
   introduced to cluster the 52 wood species. As for dimension reduction,
   we proposed linear discriminant analysis (LDA) to solve linear data and
   kernel discriminant analysis/generalized singular value decomposition
   (KDA/GSVD) to solve nonlinearly structured data. For final
   classification, K-Nearest Neighbour (KNN) classifier is implemented to
   classify the wood species.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Khalid, M (Corresponding Author), Univ Teknol Malaysia, Ctr Artificial Intelligence \& Robot, Kuala Lumpur, Malaysia.
   Khalid, Marzuki; Yusof, Rubiyah, Univ Teknol Malaysia, Ctr Artificial Intelligence \& Robot, Kuala Lumpur, Malaysia.
   Khairuddin, Anis Salwa Mohd, Univ Malaya, Dept Elect Engn, Kuala Lumpur, Malaysia.},
ISSN = {2190-3018},
ISBN = {978-3-642-22158-3},
Keywords = {wood recognition; LDA; KDA/GSVD; K-means cluster; kNN classifier},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory \& Methods},
Author-Email = {anissalwa@um.edu.my
   marzuki.khalid@gmail.com
   rubiyah@ic.utm.my},
Affiliations = {Universiti Teknologi Malaysia; Universiti Malaya},
ResearcherID-Numbers = {KHAIRUDDIN, ANIS SALWA MOHD/B-5340-2010
   yusof, rubiyah/AAV-9212-2020},
ORCID-Numbers = {KHAIRUDDIN, ANIS SALWA MOHD/0000-0002-9873-4779
   },
Cited-References = {Chen W. - S., 2010, 2010 INT C WAV AN PA.
   Howland P., 2004, IEEE T PAM.
   Jiang Y., 2008, 2008 C IM SIGN PROC.
   Khairuddin U., 2011, ICIC EXPRESS LETT B, V2, P441.
   Khalid M, 2008, INT J SIMUL SYST SCI, V9, P9.
   Lu J., 2003, PATTERN RECOGNITION.
   PARK C, 2005, SIAM J MATRIX ANAL A.
   Qin XJ, 2004, PROC CVPR IEEE, P326.
   Wang K., 2007, 2 IEEE C IND EL APPL.
   Wang L., 2006, KERNEL UNCORRELATED.
   XiaoKai Z., 2008, P ICSP 2008 IEEE LOS.
   Ye F., 2009, 2009 INT S UB VIRT R.
   Yusof R, 2010, 2010 12TH INTERNATIONAL CONFERENCE ON COMPUTER MODELLING AND SIMULATION (UKSIM), P289, DOI 10.1109/UKSIM.2010.61.},
Number-of-Cited-References = {13},
Times-Cited = {5},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BC6YR},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000354608900030},
DA = {2023-08-12},
}

@inproceedings{ WOS:000539154300060,
Author = {Pavel, Monirul Islam and Kamruzzaman, Syed Mohammad and Hasan, Sadman
   Sakib and Sabuj, Saifur Rahman},
Book-Group-Author = {IEEE},
Title = {An IoT Based Plant Health Monitoring System Implementing Image
   Processing},
Booktitle = {2019 IEEE 4TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION
   SYSTEMS (ICCCS 2019)},
Year = {2019},
Pages = {299-303},
Note = {4th IEEE International Conference on Computer and Communication Systems
   (ICCCS), Singapore, SINGAPORE, FEB 23-25, 2019},
Organization = {IEEE},
Abstract = {The combination of internet of things (IoT) with environmental sensing
   and image processing device has opened a new era to monitor the health
   of plants. Classification of plant diseases in early stages using image
   processing and analyzing environmental sensing data not only helps
   farmers to get healthy plants but also maximize the production. To
   monitor and classify plant diseases IoT is essential to send images and
   give feedback on it. In this paper, a raspberry pi based IoT device is
   proposed which sends images of plants to classify diseases and updates
   environmental parameters like air temperature, humidity, soil moisture
   and pH in MySQL database in real-time. To segment the affected part of
   plant, k-mean cluster algorithm is used after performing preprocessing
   stage and converting into L{*}a{*}b color space. Multi-class support
   vector machine (SVM) is applied to categorize disease using fourteen
   types of features of color, texture and shape obtained when implementing
   gray level co-occurrence matrix where the system was able to classify
   with an accuracy of 97.33\%. Thus, classifying diseases and analyzing
   environment parameters help farms to monitor plant growth efficiently
   for better production.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Pavel, MI (Corresponding Author), BRAC Univ, Dept Comp Sci \& Engn, Dhaka 1212, Bangladesh.
   Pavel, Monirul Islam, BRAC Univ, Dept Comp Sci \& Engn, Dhaka 1212, Bangladesh.
   Kamruzzaman, Syed Mohammad; Hasan, Sadman Sakib; Sabuj, Saifur Rahman, BRAC Univ, Dept Elect \& Elect Engn, Dhaka 1212, Bangladesh.},
ISBN = {978-1-7281-1322-7},
Keywords = {Internet of things; Image processing; Multi-class svm; Plant disease
   classification; Environmental sensing},
Keywords-Plus = {MULTICLASS; DISEASE},
Research-Areas = {Computer Science; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications},
Author-Email = {monirul.islam.pavel@g.bracu.ac.bd
   i.m.kamruz@gmail.com
   sadman.s.hasan02@gmail.com
   s.r.sabuj@ieee.org},
Affiliations = {Bangladesh Rural Advancement Committee BRAC; BRAC University; Bangladesh
   Rural Advancement Committee BRAC; BRAC University},
ResearcherID-Numbers = {Sabuj, Saifur Rahman/Z-4030-2019
   Pavel, Monirul Islam/AGR-7835-2022
   },
ORCID-Numbers = {Sabuj, Saifur Rahman/0000-0002-7634-6994
   Pavel, Monirul Islam/0000-0001-9470-7725},
Cited-References = {Akhtar A, 2013, INT CONF FRONT INFO, P60, DOI 10.1109/FIT.2013.19.
   Al-Hiary H., 2011, INT J COMPUTER APPL, V17.
   Arivazhagan S., 2013, Agricultural Engineering International: CIGR Journal, V15, P211.
   Camargo A, 2009, COMPUT ELECTRON AGR, V66, P121, DOI 10.1016/j.compag.2009.01.003.
   Dhakate M, 2015, NAT CONF COMPUT VIS.
   Dong JW, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION, CYBERNETICS AND COMPUTATIONAL SOCIAL SYSTEMS (ICCSS), P53, DOI 10.1109/ICCSS.2017.8091383.
   Ghazali K. H., 2008, EUROPEAN J SCI RES, V20, P68.
   Ghosh L., 2012, J ENV SCI NATURAL RE.
   Guru D. S., 2011, P 4 ANN ACM BANG C C.
   Henn A., PLANT DOCTOR WATERIN.
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427.
   Ibrahim S. K., 2018, PROC EUR C SPACECRAF, P1.
   Kamruzzaman S. M., 2018, COMPUTATIONAL INTELL, P235.
   Liu DS, 2006, REMOTE SENS ENVIRON, V101, P167, DOI 10.1016/j.rse.2005.12.012.
   Martinez D, 2012, POSITIF, P23.
   Mokhtar U, 2015, 2015 11TH INTERNATIONAL COMPUTER ENGINEERING CONFERENCE (ICENCO), P246, DOI 10.1109/ICENCO.2015.7416356.
   Nandhini SA, 2018, WIRELESS PERS COMMUN, V102, P725, DOI 10.1007/s11277-017-5092-4.
   Peery J., DOES HUMIDITY INFLUE.
   Rocha A, 2014, IEEE T NEUR NET LEAR, V25, P289, DOI 10.1109/TNNLS.2013.2274735.
   Roger C, 1999, PLANT PATHOL, V48, P1, DOI 10.1046/j.1365-3059.1999.00312.x.
   Singh Vijai, 2017, Information Processing in Agriculture, V4, P41, DOI 10.1016/j.inpa.2016.10.005.
   Zhang SW, 2018, OPTIK, V157, P866, DOI 10.1016/j.ijleo.2017.11.190.},
Number-of-Cited-References = {22},
Times-Cited = {7},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BP1HA},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000539154300060},
DA = {2023-08-12},
}

@inproceedings{ WOS:000189420700608,
Author = {Qi, HN and Yang, JG},
Book-Group-Author = {IEEE},
Title = {Sawtooth feature extraction of leaf edge based on support vector machine},
Booktitle = {2003 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS
   1-5, PROCEEDINGS},
Year = {2003},
Pages = {3039-3044},
Note = {International Conference on Machine Learning and Cybernetics, Xian,
   PEOPLES R CHINA, NOV 02-05, 2003},
Organization = {IEEE Syst, Man \& Cybernet Tech Comm Cybernet; Hebei Univ; NW Polytech
   Univ},
Abstract = {The focus of Computer-Aided Plant-Identification (CAPI) is the stable
   features extraction of plant, such as sawtooth number of leaf edge for
   some species of plants. Because the shape of the sawteeth varies
   greatly, they cannot be depicted in rigid mathematic method. However, a
   trained SVM (Support Vector Machine) with good adaptability can be
   applied to classify sawtooth and nonsawtooth samples. The samples can be
   obtained by a rectangular sample window sliding along the edge of the
   leaf, and then be rotated to a standard pose for decreasing the
   complexity of identification. By avoiding repeated sampling and counting
   of the same sawtooth, the algorithm presented in the paper accomplishes
   automatic counting of the sawtooth number. The results of the experiment
   show that the SVM-based method works well.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Qi, HN (Corresponding Author), Zhejiang Univ, Inst Artificial Intelligence, Hangzhou 310027, Peoples R China.
   Zhejiang Univ, Inst Artificial Intelligence, Hangzhou 310027, Peoples R China.},
DOI = {10.1109/ICMLC.2003.1260099},
ISBN = {0-7803-7865-2},
Keywords = {SVM; feedforward neural network; sawtooth of leaf edge; CAPI},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Cybernetics},
Author-Email = {qihengnian@zju.edu.cn
   yangjg@zju.edu.cn},
Affiliations = {Zhejiang University},
Cited-References = {Hu RenYong, 2001, Journal of Zhejiang University (Agriculture and Life Sciences), V27, P419.
   HYERAN B, 2002, LNCS, V2388, P213.
   Scholkopf B., 2000, STAT LEARNING KERNEL.
   Sebe N, 2002, LECT NOTES COMPUT SC, V2383, P17.
   Vellido A, 1999, EXPERT SYST APPL, V17, P51, DOI 10.1016/S0957-4174(99)00016-0.
   XIAO JH, 2002, J WUYI U NATURE SCI, V16, P6.
   {[}余学军 Yu Xuejun], 2002, {[}中国图象图形学报. A, Journal of image and graphics], V7, P272.
   Zhang L, 2002, J COMPUT SCI TECH-CH, V17, P549, DOI 10.1007/BF02948823.
   Zhang XX, 2002, ADV WATER RESOUR, V25, P1, DOI 10.1016/S0309-1708(01)00047-1.
   ZHUANG YT, 2002, WEBBASED MULTIMEDIA, P38.},
Number-of-Cited-References = {10},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BY61C},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000189420700608},
DA = {2023-08-12},
}

@article{ WOS:000928903900002,
Author = {Retallack, Angus and Finlayson, Graeme and Ostendorf, Bertram and Lewis,
   Megan},
Title = {Using deep learning to detect an indicator arid shrub in
   ultra-high-resolution UAV imagery},
Journal = {ECOLOGICAL INDICATORS},
Year = {2022},
Volume = {145},
Month = {DEC},
Abstract = {Effective monitoring of arid and semi-arid rangelands around the world
   is essential to understand and combat degradation caused by
   anthropogenic use and facilitate effective management practices. Remote
   sensing tech-nologies provide ideal approaches for enhancing traditional
   on-ground monitoring. However, while broad-scale monitoring of
   vegetation in rangelands using satellites has been widely adopted, there
   has been far less uptake of remote sensing for measuring fine-scale
   indicators of ecosystem condition. This study demonstrates the
   feasibility of using ultra-high-resolution UAV (Uncrewed Aerial Vehicle)
   imagery and deep-learning-based object detection models to provide plant
   recognition and survey information relevant for operational monitoring
   programmes in arid and semi-arid ecosystems. Seven different object
   detectors using varying convolutional neural network (CNN) architectures
   are tested at three image resolutions to detect a widespread, dominant
   arid shrub species (pearl bluebush, Maireana sedifolia) that serves as a
   key indicator of overall site condition in southern Australian
   rangelands. To maximise the strength of statistical analysis, each
   method is trained on six different training datasets (each using 2,000
   to 3,000 training samples) at six widely dispersed sites. This results
   in 90 trained models, each validated at two sites. To test model
   generalisability, training and validation data was always sourced from
   separate vegetation monitoring sites.The influence of variability
   between sites on detection ac-curacy is also considered. The best
   performing models achieved F1 scores (overall accuracy) of around 75\%
   for pearl bluebush detection, a level of accuracy that provides useful
   monitoring information to land managers. Information extracted from UAV
   imagery using this approach relates directly to indicators of ecological
   condition measured in ground-based monitoring; including dominant plant
   species count, location and density. Continued development and eventual
   implementation of this method would provide objective
   conservation-relevant information at broad scales in a far reduced time
   and at a lower cost than is currently achievable using on-ground
   approaches.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Retallack, A (Corresponding Author), Univ Adelaide, Dept Ecol \& Evolutionary Biol, North Terrace Campus, Adelaide, SA 5005, Australia.
   Retallack, Angus; Finlayson, Graeme; Ostendorf, Bertram; Lewis, Megan, Univ Adelaide, Dept Ecol \& Evolutionary Biol, Adelaide, SA, Australia.
   Finlayson, Graeme, Bush Heritage Australia, POB 329,Flinders Lane, Melbourne, Vic 8009, Australia.
   Retallack, Angus, Univ Adelaide, Dept Ecol \& Evolutionary Biol, North Terrace Campus, Adelaide, SA 5005, Australia.},
DOI = {10.1016/j.ecolind.2022.109698},
EarlyAccessDate = {NOV 2022},
Article-Number = {109698},
ISSN = {1470-160X},
EISSN = {1872-7034},
Keywords = {Deep learning; UAV; Remote sensing; Arid ecology; Conservation;
   Rangelands},
Keywords-Plus = {UNMANNED AERIAL VEHICLE; SPECIES CLASSIFICATION; SPATIAL-RESOLUTION},
Research-Areas = {Biodiversity \& Conservation; Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Biodiversity Conservation; Environmental Sciences},
Author-Email = {angus.retallack@adelaide.edu.au},
Affiliations = {University of Adelaide; University of Adelaide},
ORCID-Numbers = {Retallack, Angus/0000-0002-1920-7728
   Ostendorf, Bertram/0000-0002-5868-3567},
Funding-Acknowledgement = {Thyne Reid Foundation},
Funding-Text = {Acknowledgements to Steven Humphris, Andrew Du (Australian Institute of
   Machine Learning) and Andrew Hennessey (The Plant Accelerator,
   Australian Plant Phenomics Facility) for sharing their knowledge of deep
   learning and providing strong expertise of the field and software for
   this research to be built on. To Kenneth Clarke (The Plant Accelerator,
   Australian Plant Phe-nomics Facility) and Andrew Du for providing their
   feedback on study design. To the Unmanned Research Aircraft Facility
   (URAF, The University of Adelaide) for supplying the UAV and ground
   control points used for data collection. To Ramesh Raja Segaran (URAF)
   and Dillon Campbell (URAF) for help designing a ground control and data
   collection protocol, and Dillon for assisting with orthomosaic
   processing. We acknowledge the support of Bush Heritage Australia staff
   and supporters for supporting this project. This project is being
   partially supported by the Thyne Reid Foundation. We acknowledge the
   Antakirinja Matu-Yankunytjatjara people as the Traditional Owners of Bon
   Bon.},
Cited-References = {AgiSoft, 2020, AG MET PROF VERS 1 6.
   Anderson K, 2013, FRONT ECOL ENVIRON, V11, P138, DOI 10.1890/120150.
   {[}Anonymous], 2016, BUSH HERITAGE AUSTR.
   {[}Anonymous], 2008, RANGELANDS 2008 TAKI.
   Ayhan B, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12152502.
   Baena S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0188714.
   Borowiec ML, 2022, METHODS ECOL EVOL, V13, P1640, DOI 10.1111/2041-210X.13901.
   Bureau of Meteorology, 2010, AV ANN SEAS MONTHL R.
   Chavda A, 2021, 2021 6TH INTERNATIONAL CONFERENCE FOR CONVERGENCE IN TECHNOLOGY (I2CT), DOI 10.1109/I2CT51068.2021.9418207.
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256.
   Deng XL, 2020, AGRIENGINEERING, V2, P294, DOI 10.3390/agriengineering2020019.
   DSITI, 2015, AUSSIEGRASS ENV CALC.
   Du L, 2022, AGRICULTURE-BASEL, V12, DOI 10.3390/agriculture12020248.
   ESRI, 2021, ARCGIS PRO VERS 2 8.
   Franklin SE, 2018, INT J REMOTE SENS, V39, P5236, DOI 10.1080/01431161.2017.1363442.
   Fraser RH, 2016, ARCT SCI, V2, P79, DOI 10.1139/as-2016-0008.
   Gallacher D, 2019, TASKS VEG SCI, V49, P91, DOI 10.1007/978-3-030-04417-6\_7.
   Guirado E, 2017, Arxiv, DOI arXiv:1706.00917.
   Guirado E, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9121220.
   Hay G.J., 2008, LECT NOTES GEOINFORM, P75, DOI {[}10.1007/978-3-540-77058-9\_4, DOI 10.1007/978-3-540-77058-9\_4].
   Held A., 2015, AUSCOVER GOOD PRACTI.
   Horning N, 2020, REMOTE SENS ECOL CON, V6, P487, DOI 10.1002/rse2.144.
   Huang HS, 2020, INT J REMOTE SENS, V41, P3446, DOI 10.1080/01431161.2019.1706112.
   James K., 2021, P ADV COMP 11 INT C, P545.
   James K, 2020, METHODS ECOL EVOL, V11, P1509, DOI 10.1111/2041-210X.13473.
   Karfs R., 2001, RESOURCE INVENTORY C.
   Kattenborn T, 2020, REMOTE SENS ECOL CON, V6, P472, DOI 10.1002/rse2.146.
   Kislov DE, 2021, REMOTE SENS ECOL CON, V7, P355, DOI 10.1002/rse2.194.
   Laliberte AS, 2011, REMOTE SENS-BASEL, V3, P2529, DOI 10.3390/rs3112529.
   Lamba A, 2019, CURR BIOL, V29, pR977, DOI 10.1016/j.cub.2019.08.016.
   Land \& Water Australia, 2009, NAT LAND WAT RES AUD.
   Lu B, 2018, GISCI REMOTE SENS, V55, P205, DOI 10.1080/15481603.2017.1408930.
   Lu B, 2017, ISPRS J PHOTOGRAMM, V128, P73, DOI 10.1016/j.isprsjprs.2017.03.011.
   Lu B, 2016, INT WORKS EARTH OB.
   Lussem U, 2020, PFG-J PHOTOGRAMM REM, V88, P407, DOI 10.1007/s41064-020-00117-w.
   McMichael AJ, 1999, BIOSCIENCE, V49, P205, DOI 10.2307/1313510.
   Messina G, 2021, DRONES-BASEL, V5, DOI 10.3390/drones5030061.
   Nezami S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12071070.
   NLWRA, 2001, RANG TRACK CHANG AUS.
   NOSS RF, 1990, CONSERV BIOL, V4, P355, DOI 10.1111/j.1523-1739.1990.tb00309.x.
   NVIS Technical Working Group, 2017, AUSTR VEG ATTR MAN N.
   Oldeland J, 2021, ENVIRON MONIT ASSESS, V193, DOI 10.1007/s10661-021-08852-2.
   Olsen A, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-38343-3.
   Queensland Department of Environment and Science, 2018, STAT LANDC TREES STU.
   Reid RS, 2014, ANNU REV ENV RESOUR, V39, P217, DOI 10.1146/annurev-environ-020713-163329.
   Sankey TT, 2019, RANGELAND ECOL MANAG, V72, P858, DOI 10.1016/j.rama.2019.04.002.
   Sasaki Y., 2007, TRUTH F MEASURE, V1, P1.
   TensorFlow, 2022, TRANSF LEARN FIN TUN.
   Theau JM, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0245784.
   Thomson ER, 2021, ENVIRON RES LETT, V16, DOI 10.1088/1748-9326/abf464.
   TUELLER PT, 1989, J RANGE MANAGE, V42, P442, DOI 10.2307/3899227.
   UN EMG, 2011, GLOB DRYL UN SYST WI.
   Sivakumar ANV, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12132136.
   Waddell P., 2010, INVENTORY CONDITION.
   Wilson L, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14061402.
   Zhang BS, 2018, COMPUT ELECTRON AGR, V150, P302, DOI 10.1016/j.compag.2018.05.010.
   Zhang C, 2020, ISPRS J PHOTOGRAMM, V169, P280, DOI 10.1016/j.isprsjprs.2020.09.025.
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI {[}10.1109/TNNLS.2018.2876865, 10.23977/icamcs.2018.001].
   Zhu WB, 2021, CHEMOMETR INTELL LAB, V211, DOI 10.1016/j.chemolab.2021.104269.},
Number-of-Cited-References = {59},
Times-Cited = {2},
Usage-Count-Last-180-days = {6},
Usage-Count-Since-2013 = {6},
Journal-ISO = {Ecol. Indic.},
Doc-Delivery-Number = {8S9PJ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000928903900002},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000431061400008,
Author = {Liu, Huajian and Lee, Sang-Heon and Chahl, Javaan Singh},
Title = {Registration of multispectral 3D points for plant inspection},
Journal = {PRECISION AGRICULTURE},
Year = {2018},
Volume = {19},
Number = {3},
Pages = {513-536},
Month = {JUN},
Abstract = {Machine vision technologies have shown advantages for efficient and
   accurate plant inspection in precision agriculture. Regarding the
   balance between accuracy of inspection and compactness for infield
   applications, multispectral imaging systems would be more suitable than
   RGB colour cameras or hyperspectral imaging systems. Multispectral image
   registration (MIR) is a key issue for multispectral imaging systems,
   however, this task is challenging. First of all, in many cases, two
   images needing registration do not have a one-to-one linear mapping in
   2D space and therefore they cannot be aligned in 2D images. Furthermore,
   the general MIR algorithms are limited to images with uniform intensity
   and are incapable of registering images with rich features. This study
   developed a machine vision system (MVS) and a MIR method which replaces
   2D-2D image registration by 3D-3D point cloud registration. The system
   can register 3D point clouds of ultraviolet (UV), blue, green, red and
   near-infrared (NIR) spectra in 3D space. It was found that the point
   clouds of general plants created by images of different spectral bands
   have a complementary property, and therefore a combined point cloud,
   called multispectral 3D point cloud, is denser than any cloud created by
   a single spectral band. Intensity information of each spectral band is
   available in a multispectral 3D point cloud and therefore image fusion
   and 3D morphological analysis can be conducted in the cloud. The MVS
   could be used as a sensor of a robotic system to fulfil on-the-go
   infield plant inspection tasks.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Liu, HJ (Corresponding Author), Univ South Australia, Sch Engn, Adelaide, SA, Australia.
   Liu, Huajian; Lee, Sang-Heon; Chahl, Javaan Singh, Univ South Australia, Sch Engn, Adelaide, SA, Australia.
   Chahl, Javaan Singh, Def Sci \& Technol Org, Joint \& Operat Anal Div, Canberra, ACT, Australia.},
DOI = {10.1007/s11119-017-9536-3},
ISSN = {1385-2256},
EISSN = {1573-1618},
Keywords = {Multispectral image registration; Multimodal image registration; 3D
   reconstruction; 3D point registration; Plant identification;
   Multispectral image processing; Computer vision; Machine vision; Image
   fusion},
Keywords-Plus = {FRUIT; CLASSIFICATION; IMAGES},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary},
Author-Email = {huajian.liu@mymail.unisa.edu.au},
Affiliations = {University of South Australia; Defence Science \& Technology},
ResearcherID-Numbers = {Lee, Sang-Heon/A-3064-2011
   },
ORCID-Numbers = {Lee, Sang-Heon/0000-0002-3655-7981
   Chahl, Javaan/0000-0001-6496-0543
   Liu, Huajian/0000-0002-9477-4132},
Funding-Acknowledgement = {Project Tyche, the trusted autonomy initiative of the Defence Science
   and Technology Group, Australian Department of Defence},
Funding-Text = {This research was supported by Project Tyche, the trusted autonomy
   initiative of the Defence Science and Technology Group, Australian
   Department of Defence.},
Cited-References = {{[}Anonymous], 2007, IEEE INT C COMPUTER, DOI DOI 10.1109/ICCV.2007.4409077.
   Baggio D. L., 2012, MASTERING OPENCV PRA, P121.
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
   Bradski G., 2008, LEARNING OPENCV, P405.
   Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637.
   Chaivivatrakul S, 2014, PRECIS AGRIC, V15, P662, DOI 10.1007/s11119-014-9361-x.
   Cubero S, 2014, PRECIS AGRIC, V15, P80, DOI 10.1007/s11119-013-9324-7.
   Fan X., 2011, THESIS.
   Firmenich D, 2011, IEEE IMAGE PROC, P181, DOI 10.1109/ICIP.2011.6115818.
   Haff RP, 2013, POSTHARVEST BIOL TEC, V86, P23, DOI 10.1016/j.postharvbio.2013.06.003.
   Haghighat MBA, 2011, COMPUT ELECTR ENG, V37, P789, DOI 10.1016/j.compeleceng.2011.04.016.
   Han JG, 2013, PATTERN RECOGN LETT, V34, P42, DOI 10.1016/j.patrec.2012.03.022.
   Harris C, 1988, P 4 ALV VIS C, V15, P10.
   Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2.
   Hasan M. H. R, 2012, THESIS.
   Hoel PG, 1943, ANN MATH STAT, V14, P155, DOI 10.1214/aoms/1177731457.
   Huajian L., 2014, AM J AGRIC BIOL SCI, V9, P174, DOI {[}10.3844/ajabssp.2014.174.193, DOI 10.3844/AJABSSP.2014.174.193].
   Kurtulmus F, 2014, PRECIS AGRIC, V15, P57, DOI 10.1007/s11119-013-9323-8.
   Lee D, 2009, PROC CVPR IEEE, P186, DOI 10.1109/CVPRW.2009.5206840.
   Li H, 2016, PRECIS AGRIC, V17, P678, DOI 10.1007/s11119-016-9443-z.
   Li P, 2012, INT J COMPUT VIS IMA, V2, P12, DOI {[}10.4018/ijcvip.2012100102, DOI 10.4018/IJCVIP.2012100102].
   Liu H., 2013, Journal of Computer Science, V9, P1803, DOI 10.3844/jcssp.2013.1803.1821.
   Liu H., 2013, SOC ENG AGR C INN AG, P95.
   Liu HJ, 2017, PRECIS AGRIC, V18, P635, DOI 10.1007/s11119-016-9473-6.
   Liu HJ, 2017, PRECIS AGRIC, V18, P667, DOI 10.1007/s11119-016-9472-7.
   Liu HJ, 2017, J OPT SOC AM A, V34, P523, DOI 10.1364/JOSAA.34.000523.
   Liu HJ, 2015, 2015 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING (IIH-MSP), P441, DOI 10.1109/IIH-MSP.2015.106.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Lucas B.D., 1981, P 7 INT JOINT C ARTI.
   MathWorks, 2017, COMP VIS SYST TOOLB.
   Nieuwenhuizen AT, 2007, PRECIS AGRIC, V8, P267, DOI 10.1007/s11119-007-9044-y.
   Nieuwenhuizen AT, 2010, PRECIS AGRIC, V11, P433, DOI 10.1007/s11119-009-9138-9.
   Peilin Li, 2011, 2011 Proceedings of IEEE International Conference on Computer Science and Automation Engineering (CSAE), P376, DOI 10.1109/CSAE.2011.5952491.
   Pluim JPW, 2003, IEEE T MED IMAGING, V22, P986, DOI 10.1109/TMI.2003.815867.
   Pollefeys M, 1999, THESIS.
   Qureshi WS, 2017, PRECIS AGRIC, V18, P224, DOI 10.1007/s11119-016-9458-5.
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275.
   Shen XY, 2014, LECT NOTES COMPUT SC, V8692, P309, DOI 10.1007/978-3-319-10593-2\_21.
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794.
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832.
   Wang J, 2011, BIOSYST ENG, V108, P345, DOI 10.1016/j.biosystemseng.2011.01.006.
   Woo J, 2015, IEEE T IMAGE PROCESS, V24, P757, DOI 10.1109/TIP.2014.2387019.
   Zhang Y, 2012, ISPRS J PHOTOGRAMM, V69, P65, DOI 10.1016/j.isprsjprs.2012.02.006.
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718.},
Number-of-Cited-References = {44},
Times-Cited = {12},
Usage-Count-Last-180-days = {7},
Usage-Count-Since-2013 = {76},
Journal-ISO = {Precis. Agric.},
Doc-Delivery-Number = {GE2RD},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000431061400008},
DA = {2023-08-12},
}

@article{ WOS:000814279700024,
Author = {Atique, Aneeq and Karim, Saira and Shahid, Saman and Alamgir, Zareen},
Title = {IDENTIFICATION OF PLANT SPECIES THROUGH LEAF VEIN MORPHOMETRIC AND DEEP
   LEARNING},
Journal = {PAKISTAN JOURNAL OF BOTANY},
Year = {2022},
Volume = {54},
Number = {6},
Pages = {2195-2202},
Month = {DEC},
Abstract = {Taxonomy language is challenging to comprehend and automated knowledge
   is required to identify the plant species. The study focused on
   developing an improved deep neural network: Residual Neural
   Network-ResNet \& and Densely Connected Convolution Network (DenseNet)
   for the plant identification with plant leaf vein architecture. There
   was a total of 44 species. Each species had 64 images, each of which was
   further divided into 52 images for the training data and 12 images for
   the test data. The Canny edge detection method was deployed to detect
   the vein architecture of the leaves. For ResNet and DenseNet, the 224 x
   224 binary image was used. The size of the feature maps in 4 dense
   blocks was: 56 x 56, 28 x 28, 14 x 14, and 7 x 7, respectively.
   MalayaKew (MK) data set was used for the experiment. There was a total
   of 44 classes and images were divided into the training set and the test
   set. The training set contained 2288 images, with each class having 52
   images. Test class contained 528 images, with each class having 12
   images. After preprocessing these images, they were fed to various
   networks of ResNet and DenseNet. Two algorithms, Stochastic gradient
   descent (SGD) and Adam optimization, were used in each network. Through
   SGD, the model ResNet, had 26, 34, 50, 101, and 152 layers. The best
   accuracy achieved was 89.24\% using 50 layers. DenseNet had 121, 169,
   and 201 layers. The best accuracy achieved was 94.20\% using 169 layers.
   In Adam optimizer, the ResNet model had 26, 34, 50, 101, and 152 layers.
   The best accuracy achieved was 89.50\% using 101 layers. DenseNet had
   121, 169, and 201 layers. The best accuracy achieved was 95.72\% using
   169 layers. Overall, the best performance was achieved using Adam
   optimizer using the DenseNet model with 169 layers and came out to be
   95.72\%. This also surpassed the accuracy that was achieved using D-leaf
   architecture. The proposed deep learning (DL) methods were very accurate
   in identifying plants.},
Publisher = {PAKISTAN BOTANICAL SOC},
Address = {DEPT OF BOTANY UNIV KARACHI, 32 KARACHI, PAKISTAN},
Type = {Article},
Language = {English},
Affiliation = {Shahid, S (Corresponding Author), Natl Univ Comp \& Emerging Sci NUCES, Dept Sci \& Humanities S\&H, Fast Lahore Campus, Peshawar, Pakistan.
   Atique, Aneeq; Karim, Saira; Alamgir, Zareen, Natl Univ Comp \& Emerging Sci NUCES, FAST Sch Comp FSC, FAST Lahore Campus, Peshawar, Pakistan.
   Shahid, Saman, Natl Univ Comp \& Emerging Sci NUCES, Dept Sci \& Humanities S\&H, Fast Lahore Campus, Peshawar, Pakistan.},
DOI = {10.30848/PJB2022-6(38)},
ISSN = {0556-3321},
EISSN = {2070-3368},
Keywords = {Residual neural network-resnet; Densely connected convolution
   network-densenet; Plant identification},
Keywords-Plus = {KEY},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {drshahidsaman@gmail.com},
ResearcherID-Numbers = {Shahid, Saman/AAJ-3172-2021
   },
ORCID-Numbers = {Shahid, Saman/0000-0001-5274-3031
   Karim, Saira/0000-0002-4285-8991},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Adinugroho S, 2018, 2018 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONIC ENGINEERING (ICEEE), P350, DOI 10.1109/ICEEE2.2018.8391360.
   Artemov IA, 2010, CONTEMP PROBL ECOL+, V3, P664, DOI 10.1134/S1995425510060093.
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005.
   Ceballos G, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1400253.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6474, P135, DOI 10.1007/978-3-642-17688-3\_14.
   Darwin C, 2009, ON THE ORIGIN OF SPECIES, P1, DOI 10.1017/CBO9780511694295.004.
   Elphick CS, 2008, J APPL ECOL, V45, P1313, DOI 10.1111/j.1365-2664.2008.01545.x.
   Farnsworth EJ, 2013, BIOSCIENCE, V63, P891, DOI 10.1525/bio.2013.63.11.8.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Govaerts R, 2001, TAXON, V50, P1085, DOI 10.2307/1224723.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hopkins GW, 2002, ANIM CONSERV, V5, P245, DOI 10.1017/S1367943002002299.
   Huang G., 2017, P 2017 IEEE C COMP V, P4700, DOI {[}DOI 10.1109/CVPR.2017.243, 10.1109/CVPR.2017.243].
   Jamil N, 2015, PROCEDIA COMPUT SCI, V76, P436, DOI 10.1016/j.procs.2015.12.287.
   Jiang HX, 2020, IEEE ACCESS, V8, P68828, DOI 10.1109/ACCESS.2020.2986946.
   Joly A, 2016, MULTIMEDIA SYST, V22, P751, DOI 10.1007/s00530-015-0462-9.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Larese Monica G., 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P447, DOI 10.1007/978-3-642-33275-3\_55.
   Liu NA, 2016, NEUROCOMPUTING, V216, P460, DOI 10.1016/j.neucom.2016.08.005.
   Lobanov, 2007, IDENTIFIERS BEETLES.
   Mary Sobha P.G., 2020, INT ADV COMPUTING C, P30.
   Mora C, 2011, PLOS BIOL, V9, DOI 10.1371/journal.pbio.1000606.
   Nagaraju M, 2020, INT J SYST ASSUR ENG, V11, P547, DOI 10.1007/s13198-020-00972-1.
   Noon SK, 2020, SUSTAIN COMPUT-INFOR, V28, DOI 10.1016/j.suscom.2020.100443.
   Pawara P, 2017, LECT NOTES COMPUT SC, V10617, P615, DOI 10.1007/978-3-319-70353-4\_52.
   Pawara P, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P479, DOI 10.5220/0006196204790486.
   Remagnino P., 2016, COMPUTATIONAL BOT ME.
   Sachar S., 2021, IOP Conference Series: Materials Science and Engineering, V1022, DOI 10.1088/1757-899X/1022/1/012086.
   SUN Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI DOI 10.1155/2017/7361042.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Xie GS, 2017, INT J COMPUT VISION, V124, P145, DOI 10.1007/s11263-017-1007-9.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.
   Zhang Y., 2021, HORTIC RES-ENGLAND, V8, P2.},
Number-of-Cited-References = {37},
Times-Cited = {0},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {9},
Journal-ISO = {Pak. J. Bot.},
Doc-Delivery-Number = {2H4PZ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000814279700024},
DA = {2023-08-12},
}

@inproceedings{ WOS:000329585000012,
Author = {Deepika, K. and Ruth, I. and Keerthana, S. and Bama, B. Sathya and
   Avvailakshmi, S. and Vidhya, A.},
Book-Group-Author = {IEEE},
Title = {Robust Plant Recognition Using Graph Cut based Flower Segmentation and
   PHOG based Feature Extraction},
Booktitle = {2012 INTERNATIONAL CONFERENCE ON MACHINE VISION AND IMAGE PROCESSING
   (MVIP)},
Year = {2012},
Pages = {44-47},
Note = {International Conference on Machine Vision and Image Processing (MVIP),
   Taipei, TAIWAN, DEC 14-15, 2012},
Organization = {IEEE Madras Sect; PSG Coll Technol, Dept Instrumentat \& Control Syst
   Engn},
Abstract = {This paper proposes an efficient computer-aided plant recognition method
   based on plant flower images using shape and texture features intended
   mainly for medical industry, botanical gardening and cosmetic industry.
   The target flower is segmented from the complex background using Graph
   cut segmentation. Shape and texture features are extracted for the
   segmented image. In the shape domain, a feature descriptor is developed
   using Pyramidal Histogram of Oriented Gradients (PHOG) that represents
   the image shape. It captures the distribution of intensity gradients or
   edge directions. Then in the texture domain, the feature descriptor is
   developed using Pyramidal Local Binary Pattern (PLBP). The relevant
   images are retrieved from the database by matching the concatenated
   histogram of the PHOG and PLBP feature descriptors for the given input
   image. Results on a database of 200 sample images belonging to different
   types of plants show an increased efficiency of 96\%.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Deepika, K (Corresponding Author), Thiagarajar Coll Engn, Madurai, Tamil Nadu, India.
   Deepika, K.; Ruth, I.; Keerthana, S.; Bama, B. Sathya; Avvailakshmi, S.; Vidhya, A., Thiagarajar Coll Engn, Madurai, Tamil Nadu, India.},
ISBN = {978-1-4673-2321-5},
Keywords = {Graph cut; Pyramidal Histogram of Oriented Gradients; Local Binary
   Pattern; Histogram matching},
Keywords-Plus = {CLASSIFICATION},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic},
Author-Email = {sbece@tce.edu},
Affiliations = {Thiagarajar College of Engineering},
ResearcherID-Numbers = {B, Sathya Bama/AAP-3200-2020
   B, Sathya/GWM-9986-2022
   },
ORCID-Numbers = {B, sathya bama/0000-0003-2670-4340},
Cited-References = {Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340.
   Kebapci H, 2011, COMPUT J, V54, P1475, DOI 10.1093/comjnl/bxq037.
   Lowe DG, 2008, INT J COMPUT VISION, V60, P91.
   Ojala T, 2000, LECT NOTES COMPUT SC, V1842, P404.
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.},
Number-of-Cited-References = {6},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BJQ10},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000329585000012},
DA = {2023-08-12},
}

@inproceedings{ WOS:000255865700035,
Author = {Seatovic, Dejan},
Editor = {Gasteratos, A and Vincze, M and Tsotsos, JK},
Title = {A segmentation approach in novel real time 3D plant recognition system},
Booktitle = {COMPUTER VISION SYSTEMS, PROCEEDINGS},
Series = {Lecture Notes in Computer Science},
Year = {2008},
Volume = {5008},
Pages = {363-372},
Note = {6th International Conference on Computer Vision Systems, Santorini,
   GREECE, MAY 12-15, 2008},
Abstract = {One of the most invasive and persistent kind of weed in agriculture is
   Rumex Obtusifolius L. also called ``Broad-leaved Dock{''}. The origin of
   the plant is Europe and northern Asia, but it has also been reported
   that this plant occurs in wide parts of Northern America. Eradication of
   this plant is labour-intensive and hence there is an interest in
   automatic weed control devices. Some vision. systems were proposed that
   allow to localize and map plants in the meadow. However, these systems
   were designed and implemented for off-line processing. This paper
   presents a segmentation approach that allows for real-time recognition
   and application of herbicides onto the plant leaves. Instead of
   processing the gray-scale or colour images, our approach relays on 3D
   point cloud analysis and processing. 3D data processing has several
   advantages over 2D image processing approaches when it comes to
   extraction and recognition of plants in their natural environment.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Seatovic, D (Corresponding Author), Zurich Univ Appl Sci, IMS Inst Mechatron Syst, POB 805, CH-8401 Winterthur, Switzerland.
   Zurich Univ Appl Sci, IMS Inst Mechatron Syst, CH-8401 Winterthur, Switzerland.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-540-79546-9},
Keywords = {precision farming; plant recognition; segmentation; real-time systems},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Computer Science, Theory \& Methods},
Author-Email = {dejan.seatovic@zhaw.ch},
Affiliations = {Zurich University of Applied Sciences},
ORCID-Numbers = {Seatovic, Dejan/0000-0002-4863-0365},
Cited-References = {BASANO L, 1988, J OPT SOC AM A, V5, P1170, DOI 10.1364/JOSAA.5.001170.
   BORKOWSKI A, 2004, D80539 DTSCH GEOD KO.
   GEBHARDT S, 2007, PRECISION FARMING, V1, P1.
   GEBHARDT S, 2006, PRECISION FARMING, V3, P165.
   GEBHARDT S, 2007, THESIS U BONN.
   Giannarou S, 2005, IEEE WRK SIG PRO SYS, P359, DOI 10.1109/SIPS.2005.1579893.
   Heath MD, 1997, IEEE T PATTERN ANAL, V19, P1338, DOI 10.1109/34.643893.
   Hsiao PY, 2005, 2005 Emerging Information Technology Conference (EITC), P38.
   Mangan AP, 1999, IEEE T VIS COMPUT GR, V5, P308, DOI 10.1109/2945.817348.
   Sarkar S., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P931, DOI 10.1109/ICPR.1990.118243.
   SEATOVIC D, 2007, RAAD 2007.
   Sun Y, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P825, DOI 10.1109/ICIP.2002.1039099.
   TSAI R, 1988, IEEE J, P323.
   Wunderlich W., 1993, Proceedings. Computers in Cardiology 1993 (Cat. No.93CH3384-5), P583, DOI 10.1109/CIC.1993.378335.},
Number-of-Cited-References = {14},
Times-Cited = {5},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BHS00},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000255865700035},
DA = {2023-08-12},
}

@article{ WOS:000599767200001,
Author = {Boho, David and Rzanny, Michael and Waeldchen, Jana and Nitsche, Fabian
   and Deggelmann, Alice and Wittich, Hans Christian and Seeland, Marco and
   Maeder, Patrick},
Title = {Flora Capture: a citizen science application for collecting structured
   plant observations},
Journal = {BMC BIOINFORMATICS},
Year = {2020},
Volume = {21},
Number = {1},
Month = {DEC 14},
Abstract = {Background Digital plant images are becoming increasingly important.
   First, given a large number of images deep learning algorithms can be
   trained to automatically identify plants. Second, structured image-based
   observations provide information about plant morphological
   characteristics. Finally in the course of digitalization, digital plant
   collections receive more and more interest in schools and universities.
   Results We developed a freely available mobile application called Flora
   Capture allowing users to collect series of plant images from predefined
   perspectives. These images, together with accompanying metadata, are
   transferred to a central project server where each observation is
   reviewed and validated by a team of botanical experts. Currently, more
   than 4800 plant species, naturally occurring in the Central European
   region, are covered by the application. More than 200,000 images,
   depicting more than 1700 plant species, have been collected by thousands
   of users since the initial app release in 2016. Conclusion Flora Capture
   allows experts, laymen and citizen scientists to collect a digital
   herbarium and share structured multi-modal observations of plants.
   Collected images contribute, e.g., to the training of plant
   identification algorithms, but also suit educational purposes.
   Additionally, presence records collected with each observation allow
   contribute to verifiable records of plant occurrences across the world.},
Publisher = {BMC},
Address = {CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Waldchen, J (Corresponding Author), Max Planck Inst Biogeochem, Dept Biogeochem Integrat, Hans Knoll Str 10, D-07745 Jena, Germany.
   Boho, David; Nitsche, Fabian; Wittich, Hans Christian; Seeland, Marco; Maeder, Patrick, Tech Univ Ilmenau, Inst Comp \& Syst Engn, Helmholtzpl 5, D-98693 Ilmenau, Germany.
   Rzanny, Michael; Waeldchen, Jana; Deggelmann, Alice, Max Planck Inst Biogeochem, Dept Biogeochem Integrat, Hans Knoll Str 10, D-07745 Jena, Germany.},
DOI = {10.1186/s12859-020-03920-9},
Article-Number = {576},
ISSN = {1471-2105},
Keywords = {Structured plant observations; Multi-organ plant identification; Mobile
   app; Citizen science; Digital plant collection; Digital herbariumn},
Research-Areas = {Biochemistry \& Molecular Biology; Biotechnology \& Applied
   Microbiology; Mathematical \& Computational Biology},
Web-of-Science-Categories  = {Biochemical Research Methods; Biotechnology \& Applied Microbiology;
   Mathematical \& Computational Biology},
Author-Email = {jwald@bgc-jena.mpg.de},
Affiliations = {Technische Universitat Ilmenau; Max Planck Society},
ResearcherID-Numbers = {Rzanny, Michael/GOH-1028-2022
   Mäder, Patrick/A-1848-2018},
ORCID-Numbers = {Rzanny, Michael/0000-0002-7232-5547
   Mäder, Patrick/0000-0001-6871-2707},
Funding-Acknowledgement = {Projekt DEAL; German Federal Ministry for the Environment, Nature
   Conservation, Building and Nuclear Safety (BMUB) {[}3514685C19,
   3519685A08, 3519685B08]; German Ministry of Education and Research
   (BMBF) {[}01LC1319, 01IS20062]; Stiftung Naturschutz Thuringen (SNT)
   {[}SNT-082-248-03/2014]; Thuringian Ministry for Environment, Energy and
   Nature Conservation Grant {[}68678]},
Funding-Text = {Open Access funding enabled and organized by Projekt DEAL. We are funded
   by the German Federal Ministry for the Environment, Nature Conservation,
   Building and Nuclear Safety (BMUB) Grants: 3514685C19, 3519685A08 and
   3519685B08; the German Ministry of Education and Research (BMBF) Grants:
   01LC1319, 01IS20062; the the Stiftung Naturschutz Thuringen (SNT) Grant:
   SNT-082-248-03/2014; and the Thuringian Ministry for Environment, Energy
   and Nature Conservation Grant: 68678. The funder had no role in the
   implementation of the app, the design of the study, analysis, and
   interpretation of data and in writing the manuscript.},
Cited-References = {Balmford A, 2002, SCIENCE, V295, P2367.
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256.
   De Moor T., 2019, CITIZ SCI THEORY PRA, V4, P38, DOI {[}10.5334/cstp.212, DOI 10.5334/CSTP.212].
   Dorward LJ, 2017, CONSERV LETT, V10, P160, DOI 10.1111/conl.12326.
   Dunker S, 2018, BMC ECOL, V18, DOI 10.1186/s12898-018-0209-5.
   Goeau H, 2018, BIODIVERS INF SCI ST, V2, P25637, DOI {[}10.3897/biss.2.25637, DOI 10.3897/BISS.2.25637].
   He AF, 2016, IEEE SYS MAN CYBERN, P2020, DOI 10.1109/SMC.2016.7844537.
   Heberling JM, 2018, APPL PLANT SCI, V6, DOI 10.1002/aps3.1193.
   Joly A, 2016, MULTIMEDIA SYST, V22, P751, DOI 10.1007/s00530-015-0462-9.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kelling S, 2019, BIOSCIENCE, V69, P170, DOI 10.1093/biosci/biz010.
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   McKinley DC, 2017, BIOL CONSERV, V208, P15, DOI 10.1016/j.biocon.2016.05.015.
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115.
   oho D, FLORA INCOGNITA AUTO.
   Pocock MJ, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172579.
   Qin HW, 2016, NEUROCOMPUTING, V187, P49, DOI 10.1016/j.neucom.2015.10.122.
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI {[}10.1162/NECO\_a\_00990, 10.1162/neco\_a\_00990].
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Rzanny M, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0462-4.
   Rzanny M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0245-8.
   Seeland M, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-018-2474-x.
   Terry JCD, 2020, METHODS ECOL EVOL, V11, P303, DOI 10.1111/2041-210X.13335.
   Valan M, 2019, SYST BIOL, V68, P876, DOI 10.1093/sysbio/syz014.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780.
   Willis, 2017, STATE WORLDS PLANTS.
   Wittich HC, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2201-7.},
Number-of-Cited-References = {30},
Times-Cited = {12},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {9},
Journal-ISO = {BMC Bioinformatics},
Doc-Delivery-Number = {PG5IB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000599767200001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000692841800001,
Author = {Paertel, Jaak and Paertel, Meelis and Waeldchen, Jana},
Title = {Plant image identification application demonstrates high accuracy in
   Northern Europe},
Journal = {AOB PLANTS},
Year = {2021},
Volume = {13},
Number = {4},
Month = {AUG 26},
Abstract = {Automated image-based plant identification has experienced rapid
   development and has been already used in research and nature management.
   However, there is a need for extensive studies on how accurately
   automatic plant identification works and which characteristics of
   observations and study species influence the results. We investigated
   the accuracy of the Flora Incognita application, a research-based tool
   for automated plant image identification. Our study was conducted in
   Estonia, Northern Europe. Photos originated from the Estonian national
   curated biodiversity observations database, originally without the
   intention to use them for automated identification (1496 photos, 542
   species) were examined. Flora Incognita was also directly tested in
   field conditions in various habitats, taking images of plant organs as
   guided by the application (998 observations, 1703 photos, 280 species).
   Identification accuracy was compared among species characteristics:
   plant family, growth forms and life forms, habitat type and regional
   frequency. We also analysed image characteristics (plant organs,
   background, number of species in focus), and the number of training
   images that were available for particular species to develop the
   automated identification algorithm. From database images 79.6 \% of
   species were correctly identified by Flora Incognita; in the field
   conditions species identification accuracy reached 85.3 \%. Overall, the
   correct genus was found for 89 \% and the correct plant family for 95 \%
   of the species. Accuracy varied among different plant families, life
   forms and growth forms. Rare and common species and species from
   different habitats were identified with equal accuracy. Images with
   reproductive organs or with only the target species in focus were
   identified with greater success. The number of training images per
   species was positively correlated with the identification success. Even
   though a high accuracy has been already achieved for Flora Incognita,
   allowing its usage for research and practices, our results can guide
   further improvements of this application and automated plant
   identification in general.},
Publisher = {OXFORD UNIV PRESS},
Address = {GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Partel, M (Corresponding Author), Univ Tartu, Inst Ecol \& Earth Sci, Lai 40, EE-51005 Tartu, Estonia.
   Paertel, Jaak, Hugo Treffner Gymnasium, Munga 12, EE-51007 Tartu, Estonia.
   Paertel, Meelis, Univ Tartu, Inst Ecol \& Earth Sci, Lai 40, EE-51005 Tartu, Estonia.
   Waeldchen, Jana, Max Planck Inst Biogeochem, D-07745 Jena, Germany.},
DOI = {10.1093/aobpla/plab050},
EarlyAccessDate = {JUL 2021},
Article-Number = {plab050},
ISSN = {2041-2851},
Keywords = {Artificial intelligence; automated plant species identification; citizen
   science; convolutional neural networks; deep learning; Estonian flora;
   Flora Incognita; identification application; plant identification},
Research-Areas = {Plant Sciences; Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Plant Sciences; Ecology},
Author-Email = {meelis.partel@ut.ee},
Affiliations = {University of Tartu; Tartu University Institute of Ecology \& Earth
   Sciences; Max Planck Society},
ResearcherID-Numbers = {Pärtel, Jaak/AAY-3094-2021
   Pärtel, Meelis/D-5493-2012},
ORCID-Numbers = {Pärtel, Jaak/0000-0002-7309-7064
   Pärtel, Meelis/0000-0002-5874-0138},
Funding-Acknowledgement = {German Federal Ministry for the Environment, Nature Conservation,
   Building and Nuclear Safety (BMUB) grants {[}3519685B08]; Estonian
   Research Council {[}PRG609]; European Regional Development Fund (Centre
   of Excellence EcolChange); Thuringian Ministry for Environment, Energy
   and Nature Conservation grant {[}0901-44-865214-1-1 8860/201]},
Funding-Text = {J.W.: German Federal Ministry for the Environment, Nature Conservation,
   Building and Nuclear Safety (BMUB) grants: 3519685B08; Thuringian
   Ministry for Environment, Energy and Nature Conservation grant:
   0901-44-865214-1-1 8860/201. M.P.: Estonian Research Council (PRG609)
   and European Regional Development Fund (Centre of Excellence
   EcolChange).},
Cited-References = {Abarenkov K, 2010, EVOL BIOINFORM, V6, P189, DOI 10.4137/EBO.S6271.
   Affouard A., 2017, P ICLR INT C LEARN R.
   {[}Anonymous], NATURE, DOI 10.1038/nature14539.
   {[}Anonymous], 2021, GALAXY A40 ENTERPRIS.
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01.
   Bilyk ZI, 2020, CEUR WORKSHOP PROCEE, V2731, P61.
   Boho D, 2020, BMC BIOINFORMATICS, V21, DOI 10.1186/s12859-020-03920-9.
   Bonnet P, 2020, ECOL SOLUT EVID, V1, DOI 10.1002/2688-8319.12023.
   Bonnet P, 2018, MULTIMED SYST APPL, P131, DOI 10.1007/978-3-319-76445-0\_8.
   Ceballos G, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1400253.
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256.
   Crocker E, 2020, PLANTS PEOPLE PLANET, V2, P47, DOI 10.1002/ppp3.41.
   Eesti NSV floora, 1959, FLOR EST SSR 1 11.
   Fox J., 2021, R COMPANION APPL REG, Vthird.
   GBIF.org, 2021, GBIF HOM PAG.
   Goeau H., 2016, WORKING NOTES CLEF 2, P428.
   Goeau H., 2014, P CLEF C SHEFF UK SE, P724.
   Goeau H, 2018, BIODIVERS INF SCI ST, V2, P25637, DOI {[}10.3897/biss.2.25637, DOI 10.3897/BISS.2.25637].
   Hawthorne W, 2006, PLANT IDENTIFICATION.
   Hopkins GW, 2002, ANIM CONSERV, V5, P245, DOI 10.1017/S1367943002002299.
   Info Flora, 2021, NAT DAT INF CTR SWIS.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Jones HG, 2020, AOB PLANTS, V12, DOI 10.1093/aobpla/plaa052.
   Jose SB, 2019, PLANTS PEOPLE PLANET, V1, P169, DOI 10.1002/ppp3.51.
   Kindt R., 2021, {*}{*}DATA OBJECT{*}{*}.
   Krall H., 2010, EESTI TAIMEDE MAARAJ.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Kuhn I, 2004, DIVERS DISTRIB, V10, P363, DOI 10.1111/j.1366-9516.2004.00106.x.
   Kukk T., 2020, ATLAS ESTONIAN FLORA.
   KUKK T, 1999, VASCULAR PLANT FLORA.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   LENTH RV, 2021, {*}{*}DATA OBJECT{*}{*}.
   Ludemann J., 2020, THESIS U LUNEBURG LU.
   Mader P, 2021, METHODS ECOL EVOL, V12, P1335, DOI 10.1111/2041-210X.13611.
   Mahecha MD, 2021, ECOGRAPHY, V44, P1131, DOI 10.1111/ecog.05492.
   Paal J, 1998, BIODIVERS CONSERV, V7, P1027, DOI 10.1023/A:1008857014648.
   Patefield W. M., 1981, APPL STAT, V30, P91, DOI {[}DOI 10.2307/2346669, 10.2307/2346669].
   Pearman PB, 2007, BIOL CONSERV, V138, P109, DOI 10.1016/j.biocon.2007.04.005.
   POWO, 2021, PLANTS WORLD.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Rzanny M, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0462-4.
   Rzanny M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0245-8.
   Schmidt M., 2019, PALMENGARTEN, V83, P138, DOI {[}10.21248/palmengarten.517, DOI 10.21248/PALMENGARTEN.517].
   Sulc M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0265-4.
   Terry JCD, 2020, METHODS ECOL EVOL, V11, P303, DOI 10.1111/2041-210X.13335.
   Thuiller W, 2008, PERSPECT PLANT ECOL, V9, P137, DOI 10.1016/j.ppees.2007.09.004.
   Valan M, 2019, SYST BIOL, V68, P876, DOI 10.1093/sysbio/syz014.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   WFO, 2021, WORLD FLOR ONL VERS.
   Wittich HC, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2201-7.},
Number-of-Cited-References = {51},
Times-Cited = {8},
Usage-Count-Last-180-days = {10},
Usage-Count-Since-2013 = {28},
Journal-ISO = {Aob Plants},
Doc-Delivery-Number = {UL7QQ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000692841800001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000705912400027,
Author = {Carley, Samantha S. and Price, Stanton R.},
Editor = {Pham, T and Solomon, L},
Title = {Analyzing a Human-in-the-Loop's Decisions for the Detection of Data
   Poisoning},
Booktitle = {ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR MULTI-DOMAIN OPERATIONS
   APPLICATIONS III},
Series = {Proceedings of SPIE},
Year = {2021},
Volume = {11746},
Note = {Conference on Artificial Intelligence and Machine Learning for
   Multi-Domain Operations Applications III, ELECTR NETWORK, APR 12-16,
   2021},
Organization = {SPIE},
Abstract = {Human-in-the-loop (HITL) is the process of combining the power of a
   machine or computer system and human intelligence to develop human-aware
   machine learning (ML) models. The HITL process creates a continuous
   feedback loop between human and machine, enabling the trained model to
   continuously improve as edge cases present themselves without the need
   to fine-tune the model from scratch. Several advantages of utilizing
   HITL ML systems are to avoid bias, ensure consistency and accuracy,
   improve efficiency, and provide transparency. However, adding human
   involvement also invites human mistakes. Occasionally, a HITL system may
   actually degrade the algorithm rather than improve it: mislabeling an
   object in an object detection algorithm, incorrectly scoring an
   algorithm's output, making misclicks and typos, and other human errors
   cause the HITL to make a mistake based on the facts presented to it.
   These errors being made by the user, intentional or not, can be
   considered a form of data poisoning. To understand the effects of a
   HITL's choices on an ML model, several pieces of information during the
   HITL process can be observed, i.e.,: the time taken by the user to
   provide input on an output or on a specific object class, as well as an
   evaluation of the consistency of submitted valid inputs, among other
   factors. Information extracted from the HITL's decision-making process
   can provide insights into whether poor choices are being made by the
   user (i.e., data poisoning) and identify where, when, and why these
   choices are being made. Many state-of-the-art models can be utilized for
   this work, such as ResNet-50, DarkNet-53, Xception, among others.
   However, for this work, we are less focused on the model being used and
   more focused on the procedure for tracking HITL performance to maximize
   model improvement. Nevertheless, this work will consider a pretrained
   model, though the approach will be model agnostic. The dataset used in
   this research is the publicly available ``Flowers Recognition{''}
   dataset available on Kaggle.(1)},
Publisher = {SPIE-INT SOC OPTICAL ENGINEERING},
Address = {1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Carley, SS (Corresponding Author), US Army Engn Res \& Dev Ctr ERDC, 3909 Halls Ferry Rd, Vicksburg, MS 39180 USA.
   Carley, Samantha S.; Price, Stanton R., US Army Engn Res \& Dev Ctr ERDC, 3909 Halls Ferry Rd, Vicksburg, MS 39180 USA.},
DOI = {10.1117/12.2586260},
Article-Number = {1174616},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-1-5106-4330-7},
Keywords = {Human-in-the-loop; machine learning; data poisoning; human-machine
   teaming},
Research-Areas = {Computer Science; Engineering; Optics},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic; Optics},
Author-Email = {samantha.s.carley@usace.army.mil
   stanton.r.price@usace.army.mil},
Affiliations = {United States Department of Defense; United States Army; U.S. Army Corps
   of Engineers; U.S. Army Engineer Research \& Development Center (ERDC)},
ResearcherID-Numbers = {Price, Stanton R./AAG-2313-2020},
ORCID-Numbers = {Price, Stanton R./0000-0003-2278-1015},
Funding-Acknowledgement = {U.S. Army Engineer Research and Development Center},
Funding-Text = {The experiments described and the resulting data presented herein were
   funded under the \textbackslash{}Development of Unconventional
   Countermeasures for Enhanced Survivability{''}program. These were
   managed and executed by the U.S. Army Engineer Research and Development
   Center. Permission was granted by the Director of the Geotechnical and
   Structures Laboratory to publish this information.},
Cited-References = {Anna C S Boden, HISTOPATHOLOGY.
   Brodley C, 1999, SIXTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-99)/ELEVENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE (IAAI-99), P760.
   Dong DX, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1723.
   Hadia X, 2020, PROC SPIE, V11413, DOI 10.1117/12.2556812.
   Holzinger Andreas, 2016, Brain Inform, V3, P119, DOI 10.1007/s40708-016-0042-6.
   Hurt JA, 2020, PROC SPIE, V11413, DOI 10.1117/12.2557609.
   ImageNet, IMAGENETORG.
   Li Jiwei, DIALOGUE LEARNING HU.
   Looney C. G, 1990 IEEE INT C SYST, P224.
   Mamaev A., FLOWERS RECOGNITION.
   Monarch R. M, HUMAN IN THE LOOP MA.
   Parton BS, 2006, J DEAF STUD DEAF EDU, V11, P94, DOI 10.1093/deafed/enj003.
   Peternel Luka, TEACHING ROBOTS COOP, V36, P123.
   Smith A, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P293, DOI 10.1145/3172944.3172965.
   Szegedy C., 2013, DEEP NEURAL NETWORKS.},
Number-of-Cited-References = {15},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BS2PQ},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000705912400027},
DA = {2023-08-12},
}

@article{ WOS:000256289100015,
Author = {Bruno, Odemir Martinez and Plotze, Rodrigo de Oliveira and Falvo,
   Mauricio and de Castro, Mario},
Title = {Fractal dimension applied to plant identification},
Journal = {INFORMATION SCIENCES},
Year = {2008},
Volume = {178},
Number = {12},
Pages = {2722-2733},
Month = {JUN 15},
Abstract = {This article discusses methods to identify plants by analysing leaf
   complexity based on estimating their fractal dimension. Leaves were
   analyzed according to the complexity of their internal and external
   shapes. A computational program was developed to process, analyze and
   extract the features of leaf images, thereby allowing for automatic
   plant identification. Results are presented from two experiments, the
   first to identify plant species from the Brazilian Atlantic forest and
   Brazilian Cerrado scrublands, using fifty leaf samples from ten
   different species, and the second to identify four different species
   from genus Passiflora, using twenty leaf samples for each class. A
   comparison is made of two methods to estimate fractal dimension
   (box-counting and multiscale Minkowski). The results are discussed to
   determine the best approach to analyze shape complexity based on the
   performance of the technique, when estimating fractal dimension and
   identifying plants. (C) 2008 Elsevier Inc. All rights reserved.},
Publisher = {ELSEVIER SCIENCE INC},
Address = {360 PARK AVE SOUTH, NEW YORK, NY 10010-1710 USA},
Type = {Article},
Language = {English},
Affiliation = {Bruno, OM (Corresponding Author), Univ Sao Paulo, ICMC, Av Trabalhador Saocarlense 400,Cx668, BR-13560970 Sao Carlos, SP, Brazil.
   Bruno, Odemir Martinez; Plotze, Rodrigo de Oliveira; Falvo, Mauricio; de Castro, Mario, Univ Sao Paulo, ICMC, BR-13560970 Sao Carlos, SP, Brazil.},
DOI = {10.1016/j.ins.2008.01.023},
ISSN = {0020-0255},
EISSN = {1872-6291},
Keywords = {image processing; fractal dimension; vegetal taxonomy; biometrics},
Keywords-Plus = {TEXTURE; IMAGES; SEGMENTATION; BONE},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Information Systems},
Author-Email = {bruno@icmc.usp.br
   roplotze@icmc.usp.br
   mfalvo@icmc.usp.br
   mcastro@icmc.usp.br},
Affiliations = {Universidade de Sao Paulo},
ResearcherID-Numbers = {Bruno, Odemir M/A-5279-2009
   Bruno, Odemir/AAU-7209-2020
   de Castro, Mário/E-2564-2011},
ORCID-Numbers = {Bruno, Odemir M/0000-0002-2945-1556
   Bruno, Odemir/0000-0002-2945-1556
   de Castro, Mário/0000-0001-8685-9470},
Cited-References = {{[}Anonymous], 2006, R LANG ENV STAT COMP.
   {[}Anonymous], 2000, SHAPE ANAL CLASSIFIC.
   {[}Anonymous], 1997, ALGORITHMS IMAGE PRO.
   Barbara D, 2003, DATA MIN KNOWL DISC, V7, P123, DOI 10.1023/A:1022493416690.
   Biswas MK, 1998, PATTERN RECOGN LETT, V19, P309, DOI 10.1016/S0167-8655(98)00002-6.
   Bohn S, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.061914.
   Bruno OM, 2004, MICROPROCESS MICROSY, V28, P107, DOI 10.1016/j.micpro.2004.01.001.
   Buczkowski S, 1998, PATTERN RECOGN, V31, P411, DOI 10.1016/S0031-3203(97)00054-X.
   Carlin M, 2000, PATTERN RECOGN LETT, V21, P1013, DOI 10.1016/S0167-8655(00)00061-1.
   CHAUDHURI BB, 1995, IEEE T PATTERN ANAL, V17, P72, DOI 10.1109/34.368149.
   Coelho R. C., 1996, Applied Signal Processing, V3, P163.
   Costa LD, 2002, NETWORK-COMP NEURAL, V13, P283, DOI 10.1088/0954-898X/13/3/303.
   Cuisenaire O, 1999, COMPUT VIS IMAGE UND, V76, P163, DOI 10.1006/cviu.1999.0783.
   Dougherty G, 2001, MED ENG PHYS, V23, P369, DOI 10.1016/S1350-4533(01)00057-1.
   Everitt B.S., 2001, APPL MULTIVARIATE AN.
   Gonzalez R. C., 2006, DIGITAL IMAGE PROCES.
   Judd W.S., 2008, PLANT SYSTEMATICS PH, DOI DOI 10.1016/j.jfca.2006.09.005.
   JUN Y, 2006, INFORM SCI, V176, P2042.
   Kurmann M. H, 1999, EVOLUTION PLANT ARCH.
   Lee CL, 2006, INT J IMAG SYST TECH, V16, P15, DOI 10.1002/ima.20063.
   Lee WL, 2005, INFORM SCIENCES, V175, P177, DOI 10.1016/j.ins.2005.01.007.
   MacDougal J.M., 1994, SYST BOT MONOGR, V41, P1, DOI {[}DOI 10.2307/25027834, 10.2307/25027834].
   Mande1brot B.B., 1983, FRACTAL GEOMETRY NAT, V51, P286.
   Mandelbrot B.B., 1977, FRACTALS FORM CHANCE.
   MARK DM, 1984, J INT ASS MATH GEOL, V16, P671, DOI 10.1007/BF01033029.
   Melin P, 2007, INFORM SCIENCES, V177, P1543, DOI 10.1016/j.ins.2006.07.022.
   Nam YY, 2005, LECT NOTES COMPUT SC, V3767, P876.
   Papoulis A., 1962, FOURIER INTEGRAL ITS.
   Pedrycz W, 2003, INFORM SCIENCES, V153, P199, DOI 10.1016/S0020-0255(03)00075-6.
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205.
   PFEIFER P, 1984, APPL SURF SCI, V18, P146, DOI 10.1016/0378-5963(84)90042-4.
   Plotze RD, 2005, CAN J BOT, V83, P287, DOI {[}10.1139/b05-002, 10.1139/B05-002].
   SCHROEDER M, 1991, CHAOS POWER LAWS MIN.
   SERRA J, 1988, IMAGE ANAL MATH MORP, V2.
   Taleb-Ahmed A, 2003, PATTERN RECOGN LETT, V24, P1971, DOI 10.1016/S0167-8655(03)00036-9.
   Tricot C., 1995, CURVES FRACTAL DIMEN.
   Tzionas P., 2005, 5 INT C TECHN AUT TH, P365.
   VANDERPLANK J, 2000, PASSIONFLOWERS.
   Voss R. F., 1988, SCI FRACTAL IMAGES, P21, DOI DOI 10.1007/978-1-4612-3784-6\_1.
   Wang XF, 2005, LECT NOTES COMPUT SC, V3644, P87.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.},
Number-of-Cited-References = {41},
Times-Cited = {165},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {46},
Journal-ISO = {Inf. Sci.},
Doc-Delivery-Number = {307AF},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000256289100015},
DA = {2023-08-12},
}

@article{ WOS:000209840100004,
Author = {Horaisova, Katerina and Kukal, Jaromir},
Title = {Could k-NN classifier be useful in tree leaves recognition?},
Journal = {ARCHIVES OF CONTROL SCIENCES},
Year = {2014},
Volume = {24},
Number = {2},
Pages = {177-192},
Month = {JUN},
Abstract = {This paper presents a method for affine invariant recognition of
   two-dimensional binary objects based on 2D Fourier power spectrum. Such
   function is translation invariant and their moments of second order
   enable construction of affine invariant spectrum except of the rotation
   effect. Harmonic analysis of samples on circular paths generates Fourier
   coefficients whose absolute values are affine invariant descriptors.
   Affine invariancy is approximately saved also for large digital binary
   images as demonstrated in the experimental part. The proposed method is
   tested on artificial data set first and consequently on a large set of
   2D binary digital images of tree leaves. High dimensionality of feature
   vectors is reduced via the kernel PCA technique with Gaussian kernel and
   the k-NN classifier is used for image classification. The results are
   summarized as k-NN classifier sensitivity after dimensionality
   reduction. The resulting descriptors after dimensionality reduction are
   able to distinguish real contours of tree leaves with acceptable
   classification error. The general methodology is directly applicable to
   any set of large binary images. All calculations were performed in the
   MATLAB environment.},
Publisher = {POLSKA AKAD NAUK, POLISH ACAD SCIENCES},
Address = {PL DEFILAD 1, WARSZAWA, 00-901, POLAND},
Type = {Article},
Language = {English},
Affiliation = {Horaisova, K (Corresponding Author), Czech Tech Univ, Fac Nucl Sci \& Phys Engn, Trojanova 13, Prague, Czech Republic.
   Horaisova, Katerina; Kukal, Jaromir, Czech Tech Univ, Fac Nucl Sci \& Phys Engn, Trojanova 13, Prague, Czech Republic.},
DOI = {10.2478/acsc-2014-0011},
ISSN = {2300-2611},
Keywords = {binary image; Fourier transform; affine invariance; harmonic analysis;
   pattern recognition; k-NN classifier},
Keywords-Plus = {IMAGE RETRIEVAL},
Research-Areas = {Automation \& Control Systems; Mathematics},
Web-of-Science-Categories  = {Automation \& Control Systems; Mathematics, Applied},
Author-Email = {katerina.horaisova@fjfi.cvut.cz},
Affiliations = {Czech Technical University Prague},
Cited-References = {ALTMAN DG, 1994, BRIT MED J, V308, P1552, DOI 10.1136/bmj.308.6943.1552.
   Antani S, 2004, STUD HEALTH TECHNOL, V107, P829.
   ANTANI S, 2002, P IND C COMP VIS GRA, P16.
   Bordogna G, 2007, LECT NOTES ARTIF INT, V4578, P370.
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Duda RO, 1973, PATTERN CLASSIFICATI.
   DUKKIPATI P, 2005, P 3 INT C COMP SCI I, P8.
   FLUSSER J, 1993, PATTERN RECOGN, V26, P167, DOI 10.1016/0031-3203(93)90098-H.
   Gonzales R.C., 2001, DIGITAL IMAGE PROCES, Vsecond.
   Ho J, 2011, COMPUT VIS IMAGE UND, V115, P50, DOI 10.1016/j.cviu.2010.07.007.
   Kim WY, 2000, SIGNAL PROCESS-IMAGE, V16, P95, DOI 10.1016/S0923-5965(00)00019-9.
   Long FH, 2003, SIG COM TEC, P1.
   Novotny P, 2013, BIOSYST ENG, V115, P444, DOI 10.1016/j.biosystemseng.2013.04.007.
   Sheta AF, 2012, INT CONF MULTIMED, P278, DOI 10.1109/ICMCS.2012.6320118.
   Sidiropoulos P, 2011, PATTERN RECOGN, V44, P739, DOI 10.1016/j.patcog.2010.09.014.
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972.
   Suk T, 2011, PATTERN RECOGN, V44, P2047, DOI 10.1016/j.patcog.2010.05.015.
   VELTKAMP R, 2008, STATE OF THE ART CON.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Yahiaoui I, 2006, LECT NOTES COMPUT SC, V4261, P357.
   Yang JW, 2012, INT J WAVELETS MULTI, V10, DOI 10.1142/S021969131250035X.
   Yang JW, 2012, MATH PROBL ENG, V2012, DOI 10.1155/2012/690262.
   Yang JW, 2012, MATH PROBL ENG, V2012, DOI 10.1155/2012/928161.},
Number-of-Cited-References = {24},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Arch. Control Sci.},
Doc-Delivery-Number = {V45TY},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000209840100004},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000389729200007,
Author = {Dyrmann, Mads and Karstoft, Henrik and Midtiby, Henrik Skov},
Title = {Plant species classification using deep convolutional neural network},
Journal = {BIOSYSTEMS ENGINEERING},
Year = {2016},
Volume = {151},
Pages = {72-80},
Month = {NOV},
Abstract = {Information on which weed species are present within agricultural fields
   is important for site specific weed management. This paper presents a
   method that is capable of recognising plant species in colour images by
   using a convolutional neural network. The network is built from scratch
   trained and tested on a total of 10,413 images containing 22 weed and
   crop species at early growth stages. These images originate from six
   different data sets, which have variations with respect to lighting,
   resolution, and soil type. This includes images taken under controlled
   conditions with regard to camera stabilisation and illumination, and
   images shot with hand-held mobile phones in fields with changing
   lighting conditions and different soil types. For these 22 species, the
   network is able to achieve a classification accuracy of 86.2\%. (C) 2016
   IAgrE. Published by Elsevier Ltd. All rights reserved.},
Publisher = {ACADEMIC PRESS INC ELSEVIER SCIENCE},
Address = {525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA},
Type = {Article},
Language = {English},
Affiliation = {Dyrmann, M (Corresponding Author), Univ Southern Denmark, Maersk Mc Kinney Moller Inst, Odense, Denmark.
   Dyrmann, Mads; Midtiby, Henrik Skov, Univ Southern Denmark, Maersk Mc Kinney Moller Inst, Odense, Denmark.
   Karstoft, Henrik, Aarhus Univ, Dept Engn, Aarhus, Denmark.},
DOI = {10.1016/j.biosystemseng.2016.08.024},
ISSN = {1537-5110},
EISSN = {1537-5129},
Keywords = {Plant classification; Deep learning; Convolutional Neural; Networks;
   Weed control},
Keywords-Plus = {WEED; IDENTIFICATION},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agricultural Engineering; Agriculture, Multidisciplinary},
Author-Email = {mady@mmmi.sdu.dk},
Affiliations = {University of Southern Denmark; Aarhus University},
ResearcherID-Numbers = {Midtiby, Henrik Skov/A-9246-2008
   },
ORCID-Numbers = {Midtiby, Henrik Skov/0000-0002-3310-5680
   Dyrmann, Mads/0000-0002-6510-1609},
Cited-References = {Aarhus University-Department of Agroecology \& SEGES, 2015, UKR.
   {[}Anonymous], 2001, BB MONOGRAPH.
   {[}Anonymous], 2015, LASAGNE 1 RELEASE.
   Astrand B, 2002, AUTON ROBOT, V13, P21, DOI 10.1023/A:1015674004201.
   Cao X., 2015, PRACTICAL THEORY DES.
   Dyrmann M., 2014, TECH REP.
   Giselsson T. M., 2010, REAL TIME CROPS WEED.
   Glorot Xavier, 2010, P 13 INT C ARTIFICIA, P249.
   Golzarian MR, 2011, PLANT METHODS, V7, DOI 10.1186/1746-4811-7-28.
   He K., 2015, DEEP RESIDUAL LEARNI, V7.
   Hodgson J. M., 1968, TECHNICAL B USDA, V171614.
   Io\&REG;e S., 2015, P INT C MACH LEARN P, P448.
   Jorgensen L. N., 2007, VURDERING PLANTEVOER, V115.
   Kazmi W. A., 2014, THESIS.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
   Minervini M, 2014, ECOL INFORM, V23, P35, DOI 10.1016/j.ecoinf.2013.07.004.
   Oerke EC, 2006, J AGR SCI-CAMBRIDGE, V144, P31, DOI 10.1017/S0021859605005708.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Scharr H, 2014, FZJ201403837 FORSCH.
   Simons KA, 2014, ADV ACC EDUC-TEACH, V15, P1, DOI 10.1108/S1085-462220140000015001.
   Slaughter DC, 2008, WEED TECHNOL, V22, P378, DOI 10.1614/WT-07-104.1.
   Sogaard HT, 2005, BIOSYST ENG, V91, P271, DOI 10.1016/j.biosystemseng.2005.04.011.
   WOEBBECKE DM, 1995, T ASAE, V38, P259, DOI 10.13031/2013.27838.},
Number-of-Cited-References = {23},
Times-Cited = {291},
Usage-Count-Last-180-days = {13},
Usage-Count-Since-2013 = {168},
Journal-ISO = {Biosyst. Eng.},
Doc-Delivery-Number = {EE6NJ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000389729200007},
DA = {2023-08-12},
}

@article{ WOS:000697023600003,
Author = {Desai, Vishwad and Savani, Vijay and Patel, Rutul},
Title = {Plant Classification based on Leaves using Artificial Neural Network},
Journal = {INTERNATIONAL JOURNAL OF INTEGRATED ENGINEERING},
Year = {2021},
Volume = {13},
Number = {6},
Pages = {39-49},
Abstract = {Manual methods to examine leaf for plant classification can be tedious,
   therefore, automation is desired. Existing methods try distinctive
   approaches to accomplish this task. Nowadays, Convolution Neural
   Networks (CNN) are widely used for such application which achieves
   higher accuracy. However, CNN's are computationally expensive and
   require extensive dataset for training. Other existing methods are far
   less resource expensive but they also have their shortcomings for
   example, some features cannot be processed accurately with automation,
   some necessary differentiators are left out. To overcome this, we have
   proposed a simple Artificial Neural Network (ANN) for automatic
   classification of plants based on their leaf features. Experimental
   results show that the proposed algorithm able to achieve an accuracy of
   96\% by incorporating only a single hidden layer of ANN. Hence, our
   approach is computationally efficient compared to existing CNN based
   methods.},
Publisher = {UNIV TUN HUSSEIN ONN MALAYSIA},
Address = {86400 PARIT RAJA, BATU PAHAT, JOHOR, 00000, MALAYSIA},
Type = {Article},
Language = {English},
Affiliation = {Savani, V (Corresponding Author), Nirma Univ, Inst Technol, Dept Elect \& Commun Engg, Ahmadabad 382481, Gujarat, India.
   Desai, Vishwad, Nirma Univ, Inst Technol, Alumani EC Dept, Ahmadabad 382481, Gujarat, India.
   Savani, Vijay; Patel, Rutul, Nirma Univ, Inst Technol, Dept Elect \& Commun Engg, Ahmadabad 382481, Gujarat, India.},
DOI = {10.30880/ijie.2021.13.06.003},
ISSN = {2229-838X},
Keywords = {Artificial neural network; feature extraction; dimensionality reduction;
   classification; plant},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Multidisciplinary},
Author-Email = {vijay.savani@nirmauni.ac.in},
Affiliations = {Nirma University; Nirma University},
ResearcherID-Numbers = {Patel, Rutul/HDM-5919-2022
   Savani, Vijay/D-2274-2018},
ORCID-Numbers = {Savani, Vijay/0000-0002-0874-5360},
Cited-References = {Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Abdul Kadir, 2014, Research Journal of Pharmaceutical, Biological and Chemical Sciences, V5, P1.
   {[}Anonymous], 2013, INT J ENG SCI TECHNO.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851.
   Dharwadkar S., 2017, 2017 2 IEEE INT C RE.
   Flavia, 2009, FLAVIA.
   Gwo CY, 2013, APPL PLANT SCI, V1, DOI 10.3732/apps.1200005.
   Kadir A., 2013, INT J COMPUT TRENDS, V1, P225, DOI DOI 10.48550/ARXIV.1401.4447.
   Kekre H., 2011, INT J ENG SCI TECHNO, V3, P8357.
   Lavania S., 2014, P 2014 IEEE INT C CO, P1, DOI DOI 10.1109/ICCIC.2014.7238345.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038.
   Singh K., 2010, INT J SIGNAL PROCESS, V3, P67.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yalcin H, 2016, INT CONF AGRO-GEOINF, P233.
   Zhang YC, 2018, ICMLC 2020: 2020 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING, P145, DOI 10.1145/3383972.3383975.},
Number-of-Cited-References = {18},
Times-Cited = {0},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {4},
Journal-ISO = {Int. J. Integr. Eng.},
Doc-Delivery-Number = {UR8WZ},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000697023600003},
OA = {Green Published, hybrid},
DA = {2023-08-12},
}

@inproceedings{ WOS:000430356400006,
Author = {Torkamani, Sahar and Lohweg, Volker},
Editor = {Leister, W and Nejad, MR and Hubner, M},
Title = {Shift-Invariant Motif Discovery in Image Processing},
Booktitle = {SEVENTH INTERNATIONAL CONFERENCE ON PERFORMANCE, SAFETY AND ROBUSTNESS
   IN COMPLEX SYSTEMS AND APPLICATIONS (PESARO 2017)},
Year = {2017},
Pages = {27-32},
Note = {7th International Conference on Performance, Safety and Robustness in
   Complex Systems and Applications (PESARO), Venice, ITALY, APR 23-27,
   2017},
Organization = {IARIA},
Abstract = {Nowadays, the boost of optical imaging technologies results in more data
   with a faster rate are being collected. Consequently, data and knowledge
   discovery science has become an attractive and a fast growing topic in
   several industry and research area. Motif discovery in image processing
   aims to tackle the problem of deriving structures or detecting
   regularities in image databases. Most of the motif discovery methods
   first convert images into time series and then attempt to find motifs in
   such data. This might lead to information loss and also the problem of
   inability to detect shifted and multi-scale image motifs of different
   size. Here, a method is proposed to find image motifs of different size
   in image datasets by applying images in original dimension without
   converting them to time series. Images are inspected by the Complex Quad
   Tree Wavelet Packet transform which provides broad frequency analysis of
   an image in various scales. Next, features are extracted from the
   wavelet coefficients. Finally, image motifs are detected by measuring
   the similarity of the features. The performance of the proposed method
   is demonstrated on a dataset with images from diverse applications, such
   as hand gesture, text recognition, leaf and plant identification.},
Publisher = {IARIA XPS PRESS},
Address = {PO BOX 7827, WILMINGTON, DE 19803 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Torkamani, S (Corresponding Author), Ostwestfalen Lippe Univ Appl Sci, inIT Inst Ind IT, Lemgo, Germany.
   Torkamani, Sahar; Lohweg, Volker, Ostwestfalen Lippe Univ Appl Sci, inIT Inst Ind IT, Lemgo, Germany.},
ISBN = {978-1-61208-549-4},
Keywords = {Motif discovery; Image processing; Wavelet transformation},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {sahar.torkamani@hs-owl.de
   volker.lohweg@hs-owl.de},
ResearcherID-Numbers = {Lohweg, Volker/AAF-2062-2019},
ORCID-Numbers = {Lohweg, Volker/0000-0002-3325-7887},
Funding-Acknowledgement = {International Graduate School of Intelligent Systems in Automation
   Technology (ISA); Institute of Industrial Information Technologies
   (inIT) of the Ostwestfalen-Lippe University of Applied Sciences},
Funding-Text = {This research is partly supported by the International Graduate School
   of Intelligent Systems in Automation Technology (ISA), which is run by
   scientists of the Faculty of Computer Science, Electrical Engineering
   and Mathematics and the Faculty of Mechanical Engineering of the
   University of Paderborn and the Institute of Industrial Information
   Technologies (inIT) of the Ostwestfalen-Lippe University of Applied
   Sciences.},
Cited-References = {Abdelnour AF, 2005, IEEE T SIGNAL PROCES, V53, P231, DOI 10.1109/TSP.2004.838959.
   Alpaydin E., 2010, INTRO MACHINE LEARNI, V2nd ed..
   {[}Anonymous], 1998, INTRO WAVELETS WAVEL.
   Bayer C., 2013, ETFA, P1.
   Chi LH, 2012, 2012 IEEE INTERNATIONAL CONFERENCE ON GRANULAR COMPUTING (GRC 2012), P72.
   Das MK, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-S7-S21.
   Deza MM, 2009, ENCY DISTANCES.
   En S, 2015, PROC INT CONF DOC, P606, DOI 10.1109/ICDAR.2015.7333833.
   Esling P, 2012, ACM COMPUT SURV, V45, DOI 10.1145/2379776.2379788.
   Gangwar S, 2015, 2015 SECOND INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING AND COMMUNICATION ENGINEERING ICACCE 2015, P34, DOI 10.1109/ICACCE.2015.12.
   Gayathri BM, 2016, 2016 IEEE INT C COMP, P1, DOI DOI 10.1109/ICCIC.2016.7919576.
   Grabocka J, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P392, DOI 10.1145/2623330.2623613.
   Hu WM, 2014, IEEE T IMAGE PROCESS, V23, P1513, DOI 10.1109/TIP.2014.2303639.
   Khan S. A., 2011, 2011 Proceedings of the IEEE 14th International Multitopic Conference (INMIC 2011), P25, DOI 10.1109/INMIC.2011.6151483.
   Nath SS, 2014, 2014 INTERNATIONAL CONFERENCE ON CONTROL, INSTRUMENTATION, COMMUNICATION AND COMPUTATIONAL TECHNOLOGIES (ICCICCT), P554, DOI 10.1109/ICCICCT.2014.6993023.
   Patel P, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P370, DOI 10.1109/ICDM.2002.1183925.
   Polyanin AD, 2007, HDB MATH ENG SCI.
   Rakthanmanon T., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P557, DOI 10.1109/ICDM.2011.102.
   Selesnick IW, 2005, IEEE SIGNAL PROC MAG, V22, P123, DOI 10.1109/MSP.2005.1550194.
   Silva P. F. B., 2014, UCI MACHINE LEARNING.
   Stegmann M.B., 2002, BRIEF INTRO STAT SHA.
   Torkamani S., 2015, SCHRIFTENREIHE I ANG, V54, P23.
   Torkamani S., 2015, WORKSH PROB GRAPH MO.
   Torkamani S., 2017, J WILEY INTERDISCIPL.
   Xi XP, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P249.
   Ye LX, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P947.
   Zaitoun NM, 2015, PROCEDIA COMPUT SCI, V65, P797, DOI 10.1016/j.procs.2015.09.027.
   Zhang K, 2015, LECT NOTES COMPUT SC, V9256, P312, DOI 10.1007/978-3-319-23192-1\_26.},
Number-of-Cited-References = {28},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BK0EG},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000430356400006},
DA = {2023-08-12},
}

@inproceedings{ WOS:A1996BH26N00466,
Author = {Shan, ZF and Kim, HM and Wang, FY},
Book-Group-Author = {IEEE},
Title = {Plant identification and performance optimization for neuro-fuzzy
   networks},
Booktitle = {INFORMATION INTELLIGENCE AND SYSTEMS, VOLS 1-4},
Year = {1996},
Pages = {2607-2612},
Note = {1996 IEEE International Conference on Systems, Man and Cybernetics -
   Information, Intelligence and Systems, BEIJING, PEOPLES R CHINA, OCT
   14-17, 1996},
Organization = {IEEE; Tsinghua Univ; IEEE SMC Soc; Natl Nat Sci Fdn China; Syst Engn Soc
   China; China Int Conf Ctr Sci \& Technol},
Abstract = {This paper discusses the structures and learning algorithms for
   identification and optimization with neuro-fuzzy networks (NFN). NFN are
   knowledge-based multilayer neural networks constructed by integrating
   three types of modular subnets for pattern recognition, fuzzy reasoning,
   and control synthesis, respectively. In this way, a NFN combines the
   reasoning procedure of fuzzy logic and learning capability of neural
   networks uniquely, thus is able to incorporate linguistic knowledge in
   the form of fuzzy rules in its network structure and then refine this
   knowledge through training and self learning. Simulation results are
   presented here to illustrate these ideas.},
Publisher = {INT ACADEMIC PUBL},
Address = {137 CHAONEI DAJIE, BEIJING 100704, PEOPLES R CHINA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Shan, ZF (Corresponding Author), UNIV ARIZONA,DEPT SYST \& IND ENGN,TUCSON,AZ 85721, USA.},
ISBN = {7-80003-381-3},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Cybernetics; Computer Science, Information Systems},
Number-of-Cited-References = {0},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BH26N},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:A1996BH26N00466},
DA = {2023-08-12},
}

@inproceedings{ WOS:000870333000019,
Author = {Joly, Alexis and Goeau, Herve and Kah, Stefan and Picek, LukaS and
   Lorieul, Titouan and Cole, Elijah and Deneu, Benjamin and Servajean,
   Maximilien and Durso, Andrew and Glotin, Herve and Planque, Robert and
   Vellinga, Willem-Pier and Navine, Amanda and Klinck, Holger and Denton,
   Tom and Eggel, Ivan and Bonnet, Pierre and Sulc, Milan and Hruz, Marek},
Editor = {Barron-Cedeno, A and DaSanMartino, G and Esposti, MD and Sebastiani, F and Macdonald, C and Pasi, G and Hanbury, A and Potthast, M and Faggioli, G and Ferro, N},
Title = {Overview of LifeCLEF 2022: An Evaluation of Machine-Learning Based
   Species Identification and Species Distribution Prediction},
Booktitle = {EXPERIMENTAL IR MEETS MULTILINGUALITY, MULTIMODALITY, AND INTERACTION
   (CLEF 2022)},
Series = {Lecture Notes in Computer Science},
Year = {2022},
Volume = {13390},
Pages = {257-285},
Note = {13th International Conference of the CLEF-Association (CLEF) -
   Experimental IR meets Multilinguality, Multimodality, and Interaction,
   Univ Bologna, Bologna, ITALY, SEP 05-08, 2022},
Organization = {Conf \& Labs Evaluat Forum Assoc},
Abstract = {Building accurate knowledge of the identity, the geographic distribution
   and the evolution of species is essential for the sustainable
   development of humanity, as well as for biodiversity conservation.
   However, the difficulty of identifying plants, animals and fungi is
   hindering the aggregation of new data and knowledge. Identifying and
   naming living organisms is almost impossible for the general public and
   is often difficult even for professionals and naturalists. Bridging this
   gap is a key step towards enabling effective biodiversity monitoring
   systems. The LifeCLEF campaign, presented in this paper, has been
   promoting and evaluating advances in this domain since 2011. The 2022
   edition proposes five data-oriented challenges related to the
   identification and prediction of biodiversity: (i) P1antCLEF: very
   large-scale plant identification, (ii) BirdCLEF: bird species
   recognition in audio soundscapes, (iii) GeoLifeCLEF: remote sensing
   based prediction of species, (iv) SnakeCLEF: snake species
   identification on a global scale, and (v) FungiCLEF: fungi recognition
   as an open set classification problem. This paper overviews the
   motivation, methodology and main outcomes of that five challenges.},
Publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
Address = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Joly, A (Corresponding Author), Univ Montpellier, CNRS, LIRMM, INRIA, Montpellier, France.
   Joly, A (Corresponding Author), CALTECH, Dept Comp \& Math Sci, Pasadena, CA 91125 USA.
   Joly, Alexis; Lorieul, Titouan; Deneu, Benjamin, Univ Montpellier, CNRS, LIRMM, INRIA, Montpellier, France.
   Goeau, Herve; Bonnet, Pierre, CIRAD, UMR AMAP, Montpellier, Occitanie, France.
   Glotin, Herve, Univ Toulon \& Var, Aix Marseille Univ, CNRS, LIS,DYNI Team, Marseille, France.
   Planque, Robert; Vellinga, Willem-Pier, Xeno Canto Fdn, Amsterdam, Netherlands.
   Eggel, Ivan, HES SO, Sierre, Switzerland.
   Kah, Stefan; Klinck, Holger, Cornell Univ, KLYCCB, Cornell Lab Ornithol, Ithaca, NY USA.
   Servajean, Maximilien, Univ Montpellier, Univ Paul Valery Montpellier, CNRS, AMI,LIRMM, Montpellier, France.
   Joly, Alexis; Goeau, Herve; Kah, Stefan; Picek, LukaS; Lorieul, Titouan; Cole, Elijah; Deneu, Benjamin; Servajean, Maximilien; Durso, Andrew; Glotin, Herve; Planque, Robert; Vellinga, Willem-Pier; Navine, Amanda; Klinck, Holger; Denton, Tom; Eggel, Ivan; Bonnet, Pierre; Sulc, Milan; Hruz, Marek, CALTECH, Dept Comp \& Math Sci, Pasadena, CA 91125 USA.
   Picek, LukaS; Cole, Elijah; Hruz, Marek, Univ West Bohemia, FAV, Dept Cybernet, Plzen, Czech Republic.
   Durso, Andrew, Florida Gulf Coast Univ, Dept Biol Sci, Ft Myers, FL USA.
   Denton, Tom, Google LLC, San Francisco, CA USA.
   Sulc, Milan, Rossumai Ai, Prague, Czech Republic.
   Navine, Amanda, Univ Hawaii, Listening Observ Hawaiian Ecosyst, Hilo, HI 96720 USA.},
DOI = {10.1007/978-3-031-13643-6\_19},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-031-13643-6; 978-3-031-13642-9},
Keywords-Plus = {RECOGNITION},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering},
Author-Email = {alexis.joly@inria.fr},
Affiliations = {Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; Inria; CIRAD; Centre National de la Recherche Scientifique
   (CNRS); Institut de Recherche pour le Developpement (IRD); Universite de
   Montpellier; Centre National de la Recherche Scientifique (CNRS);
   UDICE-French Research Universities; Aix-Marseille Universite; Universite
   de Toulon; University of Applied Sciences \& Arts Western Switzerland;
   Cornell University; Centre National de la Recherche Scientifique (CNRS);
   Universite Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; California Institute of Technology; University of West
   Bohemia Pilsen; State University System of Florida; Florida Gulf Coast
   University; University of Hawaii System; University Hawaii Hilo},
ResearcherID-Numbers = {Durso, Andrew/D-1657-2012
   Hruz, Marek/AGH-3677-2022
   Servajean, Maximilien/IQW-9683-2023
   Picek, Lukáš/HLX-8615-2023
   Picek, Lukas/GXG-4988-2022
   Cole, Elijah/M-5428-2017},
ORCID-Numbers = {Durso, Andrew/0000-0002-3008-7763
   Servajean, Maximilien/0000-0002-9426-2583
   Picek, Lukáš/0000-0002-6041-9722
   Kahl, Stefan/0000-0002-2411-8877
   joly, alexis/0000-0002-2161-9940
   Goeau, Herve/0000-0003-3296-3795
   Sulc, Milan/0000-0002-6321-0131
   Deneu, Benjamin/0000-0003-0640-5706
   Cole, Elijah/0000-0001-6623-0966},
Funding-Acknowledgement = {European Union {[}863463]},
Funding-Text = {This project has received funding from the European Union's Horizon 2020
   research and innovation programme under grant agreement No. 863463
   (Cos4Cloud project), and the support of \#DigitAG.},
Cited-References = {BAO H, 2022, ARXIV, DOI DOI 10.48550/ARXIV.2106.08254.
   Bolon I, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0229989.
   Bonnet P, 2018, MULTIMED SYST APPL, P131, DOI 10.1007/978-3-319-76445-0\_8.
   Cai JH, 2007, PROCEEDINGS OF THE 2007 INTERNATIONAL CONFERENCE ON INTELLIGENT SENSORS, SENSOR NETWORKS AND INFORMATION PROCESSING, P293.
   Carranza-Rojas J., 2022, WORKING NOTES CLEF 2.
   Chen T, 2020, PR MACH LEARN RES, V119.
   Chulif S., 2022, WORKING NOTES CLEF 2.
   Cole E, 2020, Arxiv, DOI arXiv:2004.04192.
   de Castaneda RR, 2019, LANCET DIGIT HEALTH, V1, pE202.
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482.
   Devlin J, 2019, Arxiv, DOI {[}arXiv:1810.04805, DOI 10.48550/ARXIV.1810.04805].
   Diao Q., 2022, ARXIV.
   Dosovitskiy A, 2021, Arxiv, DOI {[}arXiv:2010.11929, DOI 10.48550/ARXIV.2010.11929].
   Durso Andrew M, 2021, Toxicon X, V9-10, P100071, DOI 10.1016/j.toxcx.2021.100071.
   Durso AM, 2021, FRONT ARTIF INTELL, V4, DOI 10.3389/frai.2021.582110.
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442.
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   Glotin H, 2013, P NEUR INF PROC SCAL.
   Glotin H., 2013, P 1 WORKSH MACH LEAN.
   Goeau H., 2011, CLEF TASK OVERVIEW 2.
   Goeau H., 2018, CLEF TASK OVERVIEW 2.
   Goeau H., 2012, CLEF TASK OVERVIEW 2.
   Goeau H., 2022, WORKING NOTES CLEF 2.
   Grill T, 2017, EUR SIGNAL PR CONF, P1764, DOI 10.23919/EUSIPCO.2017.8081512.
   Hengl T, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0169748.
   Henkel C., 2021, CLEF WORKING NOTES 2.
   Hijmans RJ, 2005, INT J CLIMATOL, V25, P1965, DOI 10.1002/joc.1276.
   Homer C, 2015, PHOTOGRAMM ENG REM S, V81, P345, DOI 10.14358/PERS.81.5.345.
   Jiang J, 2022, WORKING NOTES CLEF 2.
   Jiankang Deng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P741, DOI 10.1007/978-3-030-58621-8\_43.
   Joly Alexis, 2021, Experimental IR Meets Multilinguality, Multimodality, and Interaction: 12th International Conference of the CLEF Association, CLEF 2021, Proceedings. Lecture Notes in Computer Science, Information Systems and Applications, incl. Internet/Web, and HCI (12880), P371, DOI 10.1007/978-3-030-85251-1\_24.
   Joly Alexis, 2017, Experimental IR Meets Multilinguality, Multimodality, and Interaction. 8th International Conference of the CLEF Association, CLEF 2017. Proceedings: LNCS 10456, P255, DOI 10.1007/978-3-319-65813-1\_24.
   Joly Alexis, 2020, Experimental IR Meets Multilinguality, Multimodality, and Interaction. 11th International Conference of the CLEF Association, CLEF 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12260), P342, DOI 10.1007/978-3-030-58219-7\_23.
   Joly A., 2014, LNCS, P229, DOI DOI 10.1007/978-3-319-11382-1\_20.
   Joly A, 2013, INT WORKSH MULT AN E, P2013.
   Joly A, 2019, LECT NOTES COMPUT SC, V11696, P387, DOI 10.1007/978-3-030-28577-7\_29.
   Joly A, 2018, LECT NOTES COMPUT SC, V11018, P247, DOI 10.1007/978-3-319-98932-7\_24.
   Joly A, 2016, LECT NOTES COMPUT SC, V9822, P286, DOI 10.1007/978-3-319-44564-9\_26.
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5\_46.
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006.
   Kahl S., 2019, CLEF TASK OVERVIEW 2.
   Kahl S, 2022, WORKING NOTES CLEF 2.
   Kahl S., 2021, WORKING NOTES CLEF 2.
   Kahl S., 2020, CLEF 2020 11 INT C C.
   Kahl S, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2021.101236.
   Karun A., 2022, WORKING NOTES CLEF 2.
   Kellenberger B., 2022, WORKING NOTES CLEF 2.
   Lasseck M., 2018, CLEF WORKING NOTES 2.
   Leblanc C., 2022, WORKING NOTES CLEF 2.
   Lee DJ, 2004, PROC SPIE, V5606, P37, DOI 10.1117/12.571789.
   Lee SH, 2018, IEEE T IMAGE PROCESS, V27, P4287, DOI 10.1109/TIP.2018.2836321.
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324.
   Liu H, 2019, PROC CVPR IEEE, P11939, DOI 10.1109/CVPR.2019.01222.
   Liu YH, 2019, Arxiv, DOI {[}arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692].
   Liu Z., 2022, ARXIV, DOI DOI 10.48550/ARXIV.2201.03545.
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986.
   Lorieul T., 2022, WORKING NOTES CLEF 2.
   Muhling M., 2020, CLEF WORKING NOTES, P1.
   Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504.
   Ong J.M., 2022, WORKING NOTES CLEF 2.
   Picek L., 2020, CLEF TASK OVERVIEW 2.
   Picek L., 2021, WORKING NOTES CLEF 2.
   Picek L., 2022, WORKING NOTES CLEF 2.
   Picek L, 2022, IEEE WINT CONF APPL, P3281, DOI 10.1109/WACV51458.2022.00334.
   Pitman NCA, 2021, NAT PLANTS, V7, P1010, DOI 10.1038/s41477-021-00974-2.
   Pravinkrishnan K., 2022, WORKING NOTES CLEF 2.
   REN J, 2020, ADV NEURAL INFORM PR, V33, P4175, DOI DOI 10.48550/ARXIV.2007.10740.
   Roberts DR, 2017, ECOGRAPHY, V40, P913, DOI 10.1111/ecog.02881.
   Roll U, 2017, NAT ECOL EVOL, V1, P1677, DOI 10.1038/s41559-017-0332-2.
   Seneviratne S.., 2021, WORKING NOTES CLEF 2.
   Shiu Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-57549-y.
   Tan MX, 2019, PR MACH LEARN RES, V97.
   Teng M., 2022, WORKING NOTES CLEF 2.
   Towsey M, 2012, BIOACOUSTICS, V21, P107, DOI 10.1080/09524622.2011.648753.
   Trifa VM, 2008, J ACOUST SOC AM, V123, P2424, DOI 10.1121/1.2839017.
   Uetz P, 2020, REPTILE DATABASE.
   van den Oord Aaron, 2018, ARXIV.
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914.
   Villon S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-67573-7.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.
   Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993.
   Wang JQ, 2021, PROC CVPR IEEE, P9690, DOI 10.1109/CVPR46437.2021.00957.
   Wood CM, 2021, METHODS ECOL EVOL, V12, P885, DOI 10.1111/2041-210X.13571.
   Xiong Z.., 2022, WORKING NOTES CLEF 2.
   Xu M., 2022, WORKING NOTES CLEF 2.
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52.
   Zhang X., 2022, WORKING NOTES CLEF 2.
   Zhong ZS, 2021, PROC CVPR IEEE, P16484, DOI 10.1109/CVPR46437.2021.01622.},
Number-of-Cited-References = {88},
Times-Cited = {1},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {3},
Doc-Delivery-Number = {BU0LY},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000870333000019},
OA = {Green Accepted, Green Submitted},
DA = {2023-08-12},
}

@article{ WOS:000410865400046,
Author = {Prasad, Shitala and Peddoju, Sateesh Kumar and Ghosh, Debashis},
Title = {An adaptive plant leaf mobile informatics using RSSC},
Journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
Year = {2017},
Volume = {76},
Number = {20},
Pages = {21339-21363},
Month = {OCT},
Abstract = {An automated plant biometric system is now an important step in
   preserving nature's biodiversity. This paper presents a novel Relative
   Sub-image Sparse Coefficient (RSSC) algorithm for mobile devices (MDs)
   representing plant leaves into a mathematically compact vector for its
   classification. The RSSC feature vector includes local Statistical
   Entropy Texture (SET) information inter-related to all the sub-images
   within a leaf. RSSC space is merged with Gray Level Co-occurrence Matrix
   (GLCM) feature to refine the outputs using best-Nearest Neighbor
   (best-NN), designed for MDs. The experiments were performed on three
   different types of leaf datasets: (i) Flavia, (ii) ICL and (iii)
   Diseased leaf datasets. The results proves our method more accurate and
   better compared to other existing plant identification systems. The
   proposed approach is also tolerant under shape distortion caused while
   capturing. The mobile machine learning system for leaf image informatics
   is deployed on Android devices which helps botanists, agriculturists and
   medical biologists to recognize ubiquitously the herbs and plant species
   anywhere-anytime.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Prasad, S (Corresponding Author), Indian Inst Technol Roorkee, Dept Comp Sci \& Engn, Roorkee, Uttarakhand, India.
   Prasad, Shitala; Peddoju, Sateesh Kumar, Indian Inst Technol Roorkee, Dept Comp Sci \& Engn, Roorkee, Uttarakhand, India.
   Ghosh, Debashis, Indian Inst Technol Roorkee, Dept Elect \& Commun Engn, Roorkee, Uttarakhand, India.},
DOI = {10.1007/s11042-016-4040-8},
ISSN = {1380-7501},
EISSN = {1573-7721},
Keywords = {Human mobile interaction (HMI); Best-NN; Leaf image informatics; Mobile
   vision (MV); Relative sub-image sparse coefficients (RSSC); Shape
   descriptor},
Keywords-Plus = {IDENTIFICATION; CLASSIFICATION; REPRESENTATION; RETRIEVAL},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods; Engineering,
   Electrical \& Electronic},
Author-Email = {shitala@ieee.org},
Affiliations = {Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Roorkee},
ResearcherID-Numbers = {Peddoju, Sateesh K/I-7249-2016
   Prasad, Shitala/AAI-8449-2020},
ORCID-Numbers = {Peddoju, Sateesh K/0000-0003-0202-3196
   },
Funding-Acknowledgement = {MHRD},
Funding-Text = {Authors would like to thank MHRD for financially supporting S. Prasad
   throughout his PhD work at IIT Roorkee.},
Cited-References = {Adamek T, 2004, IEEE T CIRC SYST VID, V14, P742, DOI 10.1109/TCSVT.2004.826776.
   Alajlan N, 2007, PATTERN RECOGN, V40, P1911, DOI 10.1016/j.patcog.2006.12.005.
   Backes AR, 2010, PATTERN RECOGN, V43, P685, DOI 10.1016/j.patcog.2009.07.017.
   Bai X, 2010, IEEE T PATTERN ANAL, V32, P861, DOI 10.1109/TPAMI.2009.85.
   Barrera M, 2012, JIP, V1, P4.
   C V N I, 2014, CISC VIS NETW IND GL, P1.
   Cho, 2013, MULTIMEDIA UBIQUITOU, VVolume 240, P699, DOI {[}10.1007/978-94-007-6738-6\_86, DOI 10.1007/978-94-007-6738-6\_86].
   Clark JY, 2009, BOT J LINN SOC, V159, P300, DOI 10.1111/j.1095-8339.2008.00891.x.
   Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073.
   Cope JS, 2010, LECT NOTES COMPUT SC, V6454, P669, DOI 10.1007/978-3-642-17274-8\_65.
   Deypir M, 2012, SCI IRAN, V19, P654, DOI 10.1016/j.scient.2011.09.020.
   Goeau H., 2011, IMAGECLEF 2011, P0.
   Goeau H., 2013, CLEF.
   Govaerts R, 2001, TAXON, V50, P1085, DOI 10.2307/1224723.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314.
   Hearn DJ, 2009, TAXON, V58, P934, DOI 10.1002/tax.583021.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lagar-Cavilla H. A., 2009, 2012 INT C DIG IM CO, P1, DOI DOI 10.1109/DICTA.2012.6411702.
   Lee CL, 2006, INT J IMAG SYST TECH, V16, P15, DOI 10.1002/ima.20063.
   Li Y, 2006, IEEE SYS MAN CYBERN, P3890, DOI 10.1109/ICSMC.2006.384738.
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Mouine S., 2013, P 3 ACM C INT C MULT, P127, DOI DOI 10.1145/2461466.2461489.
   Mullen RJ, 2008, LECT NOTES COMPUT SC, V5217, P251, DOI 10.1007/978-3-540-87527-7\_24.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Prasad S., 2011, 2011 2nd International Conference on Computer and Communication Technology, P646, DOI 10.1109/ICCCT.2011.6075212.
   Prasad S, IEEE POTENT IN PRESS.
   Prasad S., 2013, INT J ADV SCI TECHNO, V59, P41.
   Prasad S., 2011, P 2011 INT C COMM CO, P343, DOI {[}10.1145/1947940.1948012, DOI 10.1145/1947940.1948012].
   Prasad S, 2016, SIGNAL IMAGE VIDEO P, V10, P379, DOI 10.1007/s11760-015-0751-y.
   Prasad S, 2014, IEEE WCNC, P3314, DOI 10.1109/WCNC.2014.6953083.
   Prasad S, 2013, 2013 IEEE SECOND INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P405, DOI 10.1109/ICIIP.2013.6707624.
   Wang B, 2015, INFORM SCIENCES, V302, P132, DOI 10.1016/j.ins.2014.07.028.
   Wang B, 2014, IEEE T IMAGE PROCESS, V23, P4101, DOI 10.1109/TIP.2014.2343457.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Warren D., 1997, Sixth International Conference on Image Processing and its Applications (Conf. Publ. No.443), P497, DOI 10.1049/cp:19970943.
   White S, 2006, IEEE SYMPOSIUM ON 3D USER INTERFACES 2006, PROCEEDINGS, P119, DOI 10.1109/TRIDUI.2006.1618281.
   White S, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P291.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.
   Zhao ZQ, 2015, NEUROCOMPUTING, V151, P1112, DOI 10.1016/j.neucom.2014.02.077.
   Zulkifli Z., 2011, Proceedings of the 2011 11th International Conference on Hybrid Intelligent Systems (HIS 2011), P430, DOI 10.1109/HIS.2011.6122144.},
Number-of-Cited-References = {41},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {18},
Journal-ISO = {Multimed. Tools Appl.},
Doc-Delivery-Number = {FH0YB},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000410865400046},
DA = {2023-08-12},
}

@article{ WOS:000173582000010,
Author = {Billert, O and Singher, L},
Title = {Adaptive multiple filtering},
Journal = {OPTICAL ENGINEERING},
Year = {2002},
Volume = {41},
Number = {1},
Pages = {55-68},
Month = {JAN},
Abstract = {The method described represents an attractive compromise between the use
   of a single filter for a number of image distortions and one filter for
   each image or distortion of an image. The goal of this work is the
   generation of a number of filters, each of them being able to recognize
   a number of distortions. One of the main problems of the filter design
   is its high sensitivity to internal noise of the system, optical
   aberrations, etc. In this work, the most common case of image distortion
   invariance has been considered together with system noise invariance.
   The simulation results indicate the absence of a false alarm and good
   identification. The filter generation is based on a learning process in
   an electro-optical pattern recognition system. The genetic algorithm
   serves as an optimization method. A binary filter has been selected as
   the spatial filter. The comparison between a traditional matched spatial
   complex filter and this adaptive binary filter performance indicates the
   significant benefits of the latter. Statistical tools were used to
   estimate and compare the significance of the difference between the
   output average of a rejection and recognition image sets. (C) 2002
   society of Photo-Optical Instrumentation Engineers.},
Publisher = {SPIE-INT SOCIETY OPTICAL ENGINEERING},
Address = {1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98225 USA},
Type = {Article},
Language = {English},
Affiliation = {Billert, O (Corresponding Author), Technion Israel Inst Technol, IL-32000 Haifa, Israel.
   Technion Israel Inst Technol, IL-32000 Haifa, Israel.},
DOI = {10.1117/1.1425790},
ISSN = {0091-3286},
Keywords = {optical pattern recognition; multiple filters; genetic algorithm; plant
   classification},
Keywords-Plus = {INVARIANT PATTERN-RECOGNITION; SYNTHETIC DISCRIMINANT FUNCTIONS;
   PHASE-ONLY FILTERS; CHARACTER-RECOGNITION; OPTICAL-CORRELATION;
   ROTATION; SCALE; OPTIMIZATION; SHIFT},
Research-Areas = {Optics},
Web-of-Science-Categories  = {Optics},
Affiliations = {Technion Israel Institute of Technology},
Cited-References = {{[}Anonymous], APPL OPT.
   ARSENAULT HH, 1986, APPL OPTICS, V25, P3230, DOI 10.1364/AO.25.003230.
   BAHRI Z, 1988, J OPT SOC AM A, V5, P562, DOI 10.1364/JOSAA.5.000562.
   BRAUNECKER B, 1979, APPL OPTICS, V18, P2746, DOI 10.1364/AO.18.002746.
   CASASENT D, 1987, APPL OPTICS, V26, P2266, DOI 10.1364/AO.26.002266.
   CASASENT D, 1986, APPL OPTICS, V25, P2343, DOI 10.1364/AO.25.002343.
   CASASENT D, 1984, APPL OPTICS, V23, P1620, DOI 10.1364/AO.23.001620.
   CASASENT D, 1976, APPL OPTICS, V15, P1795, DOI 10.1364/AO.15.001795.
   CAULFIELD HJ, 1969, APPL OPTICS, V8, P354.
   DAVIS JA, 1993, APPL OPTICS, V32, P5095, DOI 10.1364/AO.32.005095.
   Hassebrook L., 1989, Proceedings of the SPIE - The International Society for Optical Engineering, V1053, P218.
   Hassebrook LG, 1997, OPT ENG, V36, P2710, DOI 10.1117/1.601519.
   HOLLAND JH, 1975, ADAPTATION NATURAL A.
   HONG RT, 1995, P SOC PHOTO-OPT INS, V2490, P88, DOI 10.1117/12.205809.
   HORNER JL, 1985, APPL OPTICS, V24, P609, DOI 10.1364/AO.24.000609.
   HORNER JL, 1984, APPL OPTICS, V23, P812, DOI 10.1364/AO.23.000812.
   HSU YN, 1984, APPL OPTICS, V23, P841, DOI 10.1364/AO.23.000841.
   HSU YN, 1982, APPL OPTICS, V21, P4016, DOI 10.1364/AO.21.004016.
   Jared D. A., 1986, Proceedings of the SPIE - The International Society for Optical Engineering, V638, P91, DOI 10.1117/12.964268.
   Javidi B, 1997, OPT ENG, V36, P2690, DOI 10.1117/1.601319.
   Jenkins E. L.  Jr., 1987, Proceedings of the SPIE - The International Society for Optical Engineering, V781, P140, DOI 10.1117/12.940545.
   KALMAN RR, 1986, APPL OPTICS, V25, P1032.
   KAMEMARU S, 1993, OPT ENG, V32, P26, DOI 10.1117/12.60071.
   Kumar BVKV, 1996, APPL OPTICS, V35, P1871, DOI 10.1364/AO.35.001871.
   KUMAR BVKV, 1986, J OPT SOC AM A, V3, P1579, DOI 10.1364/JOSAA.3.001579.
   MAHALANOBIS A, 1987, APPL OPTICS, V26, P3633, DOI 10.1364/AO.26.003633.
   MAHLAB U, 1992, APPL OPTICS, V31, P1117, DOI 10.1364/AO.31.001117.
   MENDLOVIC D, 1988, OPT COMMUN, V67, P172, DOI 10.1016/0030-4018(88)90374-4.
   MERSEREAU K, 1986, APPL OPTICS, V25, P2338, DOI 10.1364/AO.25.002338.
   Mirkin I, 1997, P SOC PHOTO-OPT INS, V3159, P300, DOI 10.1117/12.279454.
   Mitchell M., 1995, INTRO GENETIC ALGORI.
   MUI JK, 1980, IEEE T PATTERN ANAL, V2, P429, DOI 10.1109/TPAMI.1980.6592364.
   ROSEN J, 1989, APPL OPTICS, V28, P240, DOI 10.1364/AO.28.000240.
   ROSEN J, 1987, APPL OPTICS, V26, P2315, DOI 10.1364/AO.26.002315.
   ROSEN J, 1988, APPL OPTICS, V27, P2895, DOI 10.1364/AO.27.002895.
   SCHILS GF, 1986, J OPT SOC AM A, V3, P1433, DOI 10.1364/JOSAA.3.001433.
   SCHILS GF, 1988, J OPT SOC AM A, V5, P1309, DOI 10.1364/JOSAA.5.001309.
   Singher L, 1997, OPT ENG, V36, P922, DOI 10.1117/1.601258.
   SWEENEY DW, 1987, APPL OPTICS, V26, P3458, DOI 10.1364/AO.26.003458.
   VANDERLUGT A, 1964, IEEE T INFORM THEORY, V10, P139.
   Walsh T. R., 1989, Proceedings of the SPIE - The International Society for Optical Engineering, V1098, P240, DOI 10.1117/12.960443.
   WALSH TR, 1990, OPT ENG, V29, P1052, DOI 10.1117/12.55694.
   WALSH TR, 1989, P SOC PHOTO-OPT INS, V1151, P203.
   YE AQ, 1994, APPL OPTICS, V33, P8226, DOI 10.1364/AO.33.008226.},
Number-of-Cited-References = {44},
Times-Cited = {7},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Opt. Eng.},
Doc-Delivery-Number = {516XL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000173582000010},
DA = {2023-08-12},
}

@article{ WOS:000660980400014,
Author = {Abbas, Sawaid and Peng, Qian and Wong, Man Sing and Li, Zhilin and Wang,
   Jicheng and Ng, Kathy Tze Kwun and Kwok, Coco Yin Tung and Hui, Karena
   Ka Wai},
Title = {Characterizing and classifying urban tree species using bi-monthly
   terrestrial hyperspectral images in Hong Kong},
Journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
Year = {2021},
Volume = {177},
Pages = {204-216},
Month = {AUG},
Abstract = {Urban trees exhibit a wide range of ecosystem services that have long
   been unveiled and increasingly reported. The ability to map tree species
   and analyze tree health conditions would become vividly essential.
   Remote sensing techniques, especially hyperspectral imaging, are being
   evolved for species identification and vegetation monitoring from
   spectral reponse patterns. In this study, a hyperspectral library for
   urban tree species in Hong Kong was established comprising 75 urban
   trees belonging to 19 species. 450 bi-monthly images were acquired by a
   terrestrial hyperspectral camera (SPECIM-IQ) from November 2018 to
   October 2019. A Deep Neural Network classification model was developed
   to identify tree species from the hyperspectral imagery with an overall
   accuracy ranging from 85\% to 96\% among different seasons.
   Representative spectral reflectance curves of healthy and unhealthy
   conditions for each species were extracted and analyzed. The
   hyperspectral phenology models were developed to achieve high accuracy
   and optimization of data acquisition. The bi-monthly canopy signatures
   and vegetation indices revealed different seasonality patterns of
   evergreen and deciduous species in Hong Kong. We explored the utility of
   terrestrial hyperspectral remote sensing and Deep Neural Network for
   urban tree species identification and characterizing. This provides a
   unique baseline to understand hyperspectral characteristics and
   seasonality of urban tree species in Hong Kong that can also contribute
   to hyperspectral imaging and database development elsewhere in the
   world.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Wong, MS (Corresponding Author), Hong Kong Polytech Univ, Dept Land Surveying \& Geoinformat, Hong Kong, Peoples R China.
   Abbas, Sawaid; Peng, Qian; Wong, Man Sing; Li, Zhilin; Kwok, Coco Yin Tung; Hui, Karena Ka Wai, Hong Kong Polytech Univ, Dept Land Surveying \& Geoinformat, Hong Kong, Peoples R China.
   Wong, Man Sing, Hong Kong Polytech Univ, Res Inst Sustainable Urban Dev, Hong Kong, Peoples R China.
   Li, Zhilin, Southwest Jiaotong Univ, Fac Geosci \& Environm Engn, Chengdu, Peoples R China.
   Li, Zhilin, Southwest Jiaotong Univ, State Prov Joint Engn Lab Spatial Informat Techno, Chengdu, Peoples R China.
   Wang, Jicheng, Sichuan Normal Univ, Minist Educ Land Resources Evaluat \& Monitoring S, Key Lab, Chengdu, Peoples R China.
   Ng, Kathy Tze Kwun, HKSAR Govt, Landscape Div, Highways Dept, Hong Kong, Peoples R China.},
DOI = {10.1016/j.isprsjprs.2021.05.003},
EarlyAccessDate = {MAY 2021},
ISSN = {0924-2716},
EISSN = {1872-8235},
Keywords = {Urban tree; Hyperspectral library; Tree species; Seasonality; Deep
   learning; SPECIM-IQ},
Keywords-Plus = {CHLOROPHYLL FLUORESCENCE; IMAGING SPECTROSCOPY; ECOSYSTEM SERVICES;
   FOREST; CLASSIFICATION; VEGETATION; LIDAR; REFLECTANCE; RESOLUTION; RED},
Research-Areas = {Physical Geography; Geology; Remote Sensing; Imaging Science \&
   Photographic Technology},
Web-of-Science-Categories  = {Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science \& Photographic Technology},
Author-Email = {sabbas@polyu.edu.hk
   qian.peng@connect.polyu.hk
   Ls.charles@polyu.edu.hk
   dean.ge@swjtu.edu.cn
   wangjicheng123@sicnu.edu.cn
   cla.lsc@hyd.gov.hk
   yt-coco.kwok@connect.polyu.hk
   karena.kw.hui@polyu.edu.hk},
Affiliations = {Hong Kong Polytechnic University; Hong Kong Polytechnic University;
   Southwest Jiaotong University; Southwest Jiaotong University; Sichuan
   Normal University},
ResearcherID-Numbers = {Li, Zhilin/D-8750-2019
   Abbas, Sawaid/Y-4432-2019
   },
ORCID-Numbers = {Li, Zhilin/0000-0003-1507-323X
   Peng, Qian/0000-0002-0278-035X
   WANG, Jicheng/0000-0003-2744-7059},
Funding-Acknowledgement = {Highways Department under the project ``Feasibility Study on Setting Up
   Spectral Library for Common Tree Species in HK{''}; Research Institute
   for Sustainable Urban Development, the Hong Kong Polytechnic University
   {[}1-BBWD]; PolyU (UGC) funding grant {[}1-ZVUU]},
Funding-Text = {The Highways Department supported this project under the project
   ``Feasibility Study on Setting Up Spectral Library for Common Tree
   Species in HK{''}. M.S. Wong thanks the support from the Research
   Institute for Sustainable Urban Development, the Hong Kong Polytechnic
   University with a project id: 1-BBWD. S. Abbas would also like to thank
   the support from PolyU (UGC) funding grant (1-ZVUU).},
Cited-References = {Aasen H, 2015, ISPRS J PHOTOGRAMM, V108, P245, DOI 10.1016/j.isprsjprs.2015.08.002.
   Abbas S, 2021, SCI TOTAL ENVIRON, V752, DOI 10.1016/j.scitotenv.2020.141760.
   Alonzo M, 2014, REMOTE SENS ENVIRON, V148, P70, DOI 10.1016/j.rse.2014.03.018.
   {[}Anonymous], 2008, ENVIRON SCI-TOKYO, DOI DOI 10.1080/15693430802055524.
   Arasumani M., 2021, TESTING EFFICACY HYP.
   Asner GP, 2016, GLOB ECOL CONSERV, V8, P212, DOI 10.1016/j.gecco.2016.09.010.
   Asner GP, 2015, REMOTE SENS-BASEL, V7, P3526, DOI 10.3390/rs70403526.
   Asner GP, 2015, REMOTE SENS ENVIRON, V158, P15, DOI 10.1016/j.rse.2014.11.011.
   Asner GP, 2014, NEW PHYTOL, V204, P127, DOI 10.1111/nph.12895.
   Awad MM, 2018, J FORESTRY RES, V29, P1395, DOI 10.1007/s11676-017-0528-y.
   Baldridge AM, 2009, REMOTE SENS ENVIRON, V113, P711, DOI 10.1016/j.rse.2008.11.007.
   Ballanti L, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8060445.
   Behmann J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020441.
   Ben-Dor E, 2002, INT J REMOTE SENS, V23, P1043, DOI 10.1080/01431160010006962.
   Bojinski S, 2003, COMPUT GEOSCI-UK, V29, P27, DOI 10.1016/S0098-3004(02)00107-3.
   Bolund P, 1999, ECOL ECON, V29, P293, DOI 10.1016/S0921-8009(99)00013-0.
   Buddenbaum H, 2005, INT J REMOTE SENS, V26, P5453, DOI 10.1080/01431160500285076.
   Caughlin TT, 2016, ECOL APPL, V26, P2367, DOI 10.1002/eap.1436.
   Cho H, 2014, KOREAN J REMOTE SENS, V30, P25, DOI 10.7780/kjrs.2014.30.1.3.
   Cho MA, 2006, REMOTE SENS ENVIRON, V101, P181, DOI 10.1016/j.rse.2005.12.011.
   Clark ML, 2020, ISPRS J PHOTOGRAMM, V159, P26, DOI 10.1016/j.isprsjprs.2019.11.007.
   Cochrane MA, 2000, INT J REMOTE SENS, V21, P2075, DOI 10.1080/01431160050021303.
   Cotrozzi L, 2017, TREE PHYSIOL, V37, P1582, DOI 10.1093/treephys/tpx106.
   Dadon A, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11232800.
   Dalponte M, 2012, REMOTE SENS ENVIRON, V123, P258, DOI 10.1016/j.rse.2012.03.013.
   De'Ath G, 2002, ECOLOGY, V83, P1105, DOI 10.2307/3071917.
   Degerickx J, 2018, INT J APPL EARTH OBS, V73, P26, DOI 10.1016/j.jag.2018.05.021.
   Delegido J, 2014, ECOL INDIC, V40, P34, DOI 10.1016/j.ecolind.2014.01.002.
   EcoSIS N., 2014, ECOLOGICAL SPECTRAL, V2021.
   Escobedo FJ, 2011, ENVIRON POLLUT, V159, P2078, DOI 10.1016/j.envpol.2011.01.010.
   Escobedo FJ, 2009, LANDSCAPE URBAN PLAN, V90, P102, DOI 10.1016/j.landurbplan.2008.10.021.
   Fagan ME, 2015, REMOTE SENS-BASEL, V7, P5660, DOI 10.3390/rs70505660.
   Ferreira MP, 2019, ISPRS J PHOTOGRAMM, V149, P119, DOI 10.1016/j.isprsjprs.2019.01.019.
   Ferreira MP, 2016, REMOTE SENS ENVIRON, V179, P66, DOI 10.1016/j.rse.2016.03.021.
   Gamon JA, 1997, OECOLOGIA, V112, P492, DOI 10.1007/s004420050337.
   GAMON JA, 1992, REMOTE SENS ENVIRON, V41, P35, DOI 10.1016/0034-4257(92)90059-S.
   Ghosh G, 2012, J INDIAN SOC REMOTE, V40, P129, DOI 10.1007/s12524-011-0143-x.
   Gomez-Baggethun Erik, 2013, P175.
   Gong P, 1997, REMOTE SENS ENVIRON, V62, P189, DOI 10.1016/S0034-4257(97)00094-1.
   Goswami S., 2015, DEV WEB BASED VEGETA.
   Goswami S, 2011, MONITORING ECOSYSTEM.
   Halme E, 2019, INT J APPL EARTH OBS, V83, DOI 10.1016/j.jag.2019.101942.
   Hartling S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061284.
   Im J, 2005, REMOTE SENS ENVIRON, V99, P326, DOI 10.1016/j.rse.2005.09.008.
   Irteza SM, 2021, EARTH SYST ENVIRON, V5, P127, DOI 10.1007/s41748-020-00175-5.
   Jensen RR, 2012, GEOCARTO INT, V27, P443, DOI 10.1080/10106049.2011.638989.
   Katkovsky LV, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111698.
   Kothari S, 2018, PHOTOSYNTHETICA, V56, P455, DOI 10.1007/s11099-018-0777-9.
   KRAUSE GH, 1991, ANNU REV PLANT PHYS, V42, P313, DOI 10.1146/annurev.pp.42.060191.001525.
   Laurin GV, 2016, REMOTE SENS ENVIRON, V176, P163, DOI 10.1016/j.rse.2016.01.017.
   Laurin GV, 2014, ISPRS J PHOTOGRAMM, V89, P49, DOI 10.1016/j.isprsjprs.2014.01.001.
   Leckie DG, 2005, CAN J REMOTE SENS, V31, P175, DOI 10.5589/m05-004.
   Lee J, 2016, IEEE J-STARS, V9, P2554, DOI 10.1109/JSTARS.2016.2569408.
   Lin CS, 2018, ISPRS J PHOTOGRAMM, V142, P174, DOI 10.1016/j.isprsjprs.2018.05.022.
   Liu LX, 2017, REMOTE SENS ENVIRON, V200, P170, DOI 10.1016/j.rse.2017.08.010.
   Maselli F, 2004, REMOTE SENS ENVIRON, V89, P423, DOI 10.1016/j.rse.2003.10.020.
   Migliavacca M, 2017, NEW PHYTOL, V214, P1078, DOI 10.1111/nph.14437.
   Miyoshi GT, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12020244.
   Modzelewska A., FORESTRY INT J FORES, P1.
   Nijland W, 2014, AGR FOREST METEOROL, V184, P98, DOI 10.1016/j.agrformet.2013.09.007.
   Nowak David J., 2008, Arboriculture \& Urban Forestry, V34, P347.
   Osco LP, 2020, ISPRS J PHOTOGRAMM, V160, P97, DOI 10.1016/j.isprsjprs.2019.12.010.
   Paz-Kagan T, 2017, ECOL APPL, V27, P1466, DOI 10.1002/eap.1540.
   Pettorelli N, 2005, TRENDS ECOL EVOL, V20, P503, DOI 10.1016/j.tree.2005.05.011.
   Rossini M, 2015, GEOPHYS RES LETT, V42, P1632, DOI 10.1002/2014GL062943.
   Schaepman M.E., 1998, CALIBRATION FIELD SP, V31, P146.
   Schiefer F, 2020, ISPRS J PHOTOGRAMM, V170, P205, DOI 10.1016/j.isprsjprs.2020.10.015.
   Shen Q, 2021, ENVIRON RES LETT, V16, DOI 10.1088/1748-9326/abd2f1.
   Shi YF, 2021, INT J APPL EARTH OBS, V98, DOI 10.1016/j.jag.2021.102311.
   Thenkabail P., 2019, GLOBAL HYPERSPECTRAL.
   Thenkabail P.S., 2018, BIOPHYSICAL BIOCH CH.
   Townsend PA, 2003, IEEE T GEOSCI REMOTE, V41, P1347, DOI 10.1109/TGRS.2003.813205.
   Tratalos J, 2007, LANDSCAPE URBAN PLAN, V83, P308, DOI 10.1016/j.landurbplan.2007.05.003.
   Trier OD, 2018, EUR J REMOTE SENS, V51, P336, DOI 10.1080/22797254.2018.1434424.
   TUCKER CJ, 1986, INT J REMOTE SENS, V7, P1395, DOI 10.1080/01431168608948944.
   TUCKER CJ, 1979, REMOTE SENS ENVIRON, V8, P127, DOI 10.1016/0034-4257(79)90013-0.
   Wan H., REMOTE SENS-BASEL, V13, P1.
   Wu J, 2016, SCIENCE, V351, P972, DOI 10.1126/science.aad5068.
   Wu SB, 2021, ISPRS J PHOTOGRAMM, V171, P36, DOI 10.1016/j.isprsjprs.2020.10.017.
   Xue JR, 2017, J SENSORS, V2017, DOI 10.1155/2017/1353691.
   Yan WY, 2020, ISPRS J PHOTOGRAMM, V169, P152, DOI 10.1016/j.isprsjprs.2020.09.001.
   Zarco-Tejada PJ, 2001, IEEE T GEOSCI REMOTE, V39, P1491, DOI 10.1109/36.934080.
   Zarco-Tejada PJ, 2000, REMOTE SENS ENVIRON, V74, P596, DOI 10.1016/S0034-4257(00)00149-8.
   Zhang N, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12193188.
   Zhang XF, 2013, INT J APPL EARTH OBS, V21, P506, DOI 10.1016/j.jag.2012.07.003.},
Number-of-Cited-References = {85},
Times-Cited = {17},
Usage-Count-Last-180-days = {11},
Usage-Count-Since-2013 = {74},
Journal-ISO = {ISPRS-J. Photogramm. Remote Sens.},
Doc-Delivery-Number = {SR3ZD},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000660980400014},
DA = {2023-08-12},
}

@inproceedings{ WOS:000464435000195,
Author = {Liu, Wei and Li, Zhifang and Lin, Jialun and Liang, Danning},
Editor = {Wang, G and Fox, G and Martinez, G and Hill, R and Mueller, P},
Title = {The PSO-SVM-based Method of the Recognition of Plant Leaves},
Booktitle = {2017 15TH IEEE INTERNATIONAL SYMPOSIUM ON PARALLEL AND DISTRIBUTED
   PROCESSING WITH APPLICATIONS AND 2017 16TH IEEE INTERNATIONAL CONFERENCE
   ON UBIQUITOUS COMPUTING AND COMMUNICATIONS (ISPA/IUCC 2017)},
Series = {IEEE International Symposium on Parallel and Distributed Processing with
   Applications},
Year = {2017},
Pages = {1350-1355},
Note = {15th IEEE International Symposium on Parallel and Distributed Processing
   with Applications (ISPA) / 16th IEEE International Conference on
   Ubiquitous Computing and Communications (IUCC), Guangzhou, PEOPLES R
   CHINA, DEC 12-15, 2017},
Organization = {IEEE; IEEE Comp Soc; Guangzhou Univ; Cent S Univ},
Abstract = {Images of plant leaves are studied in the current paper, as a way to
   make recognition and classification of the plant species. First of all,
   the images are processed before extracting the shape features of the
   leaves. The leaves are then classified using the SVM (Support Vector
   Machine). After that, the SVM model's parameters are optimized using the
   PSO algorithm (Particle Swarm Optimization), which is also applied to
   design and optimize the classification algorithm as well as to make
   recognition and classification of the leaves. Finally, the Matlab
   simulation results prove that our method is valid.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Liu, W (Corresponding Author), Hainan Med Univ, Dept Med Informat, Haikou, Hainan, Peoples R China.
   Liu, Wei; Li, Zhifang; Lin, Jialun; Liang, Danning, Hainan Med Univ, Dept Med Informat, Haikou, Hainan, Peoples R China.},
DOI = {10.1109/ISPA/IUCC.2017.00205},
ISSN = {2158-9178},
ISBN = {978-1-5386-3790-6},
Keywords = {Plant recognition; Feature extraction; SVM; PSO},
Research-Areas = {Computer Science; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Hardware \& Architecture; Computer Science,
   Information Systems; Computer Science, Interdisciplinary Applications;
   Telecommunications},
Author-Email = {liuwei-1120@qq.com
   631802562@qq.com
   19143212@qq.com
   2268339763@qq.com},
Affiliations = {Hainan Medical University},
Cited-References = {{[}Anonymous], IAENG INT J COMPUTER.
   {[}Anonymous], COMPUTER ENG APPL.
   {[}Anonymous], IEEE J SELECTED TOPI.
   {[}Anonymous], INT C HYBR INT SYST.
   {[}Anonymous], PLANT LEAF DIS RECOG.
   {[}Anonymous], INT C COMP ENG TECHN.
   Chang C.-C., 2011, ACM T INTEL SYST TEC, V2, P1, DOI DOI 10.1145/1961189.1961199.
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199.
   Ghasab MAJ, 2015, EXPERT SYST APPL, V42, P2361, DOI 10.1016/j.eswa.2014.11.011.
   Maheswari A., 2014, Research Journal of Pharmaceutical, Biological and Chemical Sciences, V5, P415.
   Man QK, 2008, COMM COM INF SC, V15, P192.
   Ze-Xue Li, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9226, P145, DOI 10.1007/978-3-319-22186-1\_14.},
Number-of-Cited-References = {12},
Times-Cited = {2},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BM4YH},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000464435000195},
DA = {2023-08-12},
}

@article{ WOS:000816026000001,
Author = {De Bortoli, Luca and Marsi, Stefano and Marinello, Francesco and
   Carrato, Sergio and Ramponi, Giovanni and Gallina, Paolo},
Title = {Structure from Linear Motion (SfLM): An On-the-Go Canopy Profiling
   System Based on Off-the-Shelf RGB Cameras for Effective Sprayers Control},
Journal = {AGRONOMY-BASEL},
Year = {2022},
Volume = {12},
Number = {6},
Month = {JUN},
Abstract = {Phytosanitary treatment is one of the most critical operations in
   vineyard management. Ideally, the spraying system should treat only the
   canopy, avoiding drift, leakage and wasting of product where leaves are
   not present: variable rate distribution can be a successful approach,
   allowing the minimization of losses and improving economic as well as
   environmental performances. The target of this paper is to realize a
   smart control system to spray phytosanitary treatment just on the
   leaves, optimizing the overall costs/benefits ratio. Four different
   optical-based systems for leaf recognition are analyzed, and their
   performances are compared using a synthetic vineyard model. In the
   paper, we consider the usage of three well-established methods (infrared
   barriers, LIDAR 2-D and stereoscopic cameras), and we compare them with
   an innovative low-cost real-time solution based on a suitable computer
   vision algorithm that uses a simple monocular camera as input. The
   proposed algorithm, analyzing the sequence of input frames and
   exploiting the parallax property, estimates the depth map and eventually
   reconstructs the profile of the vineyard's row to be treated. Finally,
   the performances obtained by the new method are evaluated and compared
   with those of the other methods on a well-controlled artificial
   environment resembling an actual vineyard setup while traveling at
   standard tractor forward speed.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Gallina, P (Corresponding Author), Univ Trieste, Dept Engn \& Architecture, Appl Mech Machinery, I-34127 Trieste, Italy.
   De Bortoli, Luca; Marsi, Stefano; Carrato, Sergio; Ramponi, Giovanni, Univ Trieste, Dept Engn \& Architecture, Image Proc Lab IPL, Via A Valerio 10, I-34127 Trieste, Italy.
   Marinello, Francesco, Univ Padua, Dept Land Environm Agr \& Forestry TESAF, Viale Univ 16, I-35122 Padua, Italy.
   Gallina, Paolo, Univ Trieste, Dept Engn \& Architecture, Appl Mech Machinery, I-34127 Trieste, Italy.},
DOI = {10.3390/agronomy12061276},
Article-Number = {1276},
EISSN = {2073-4395},
Keywords = {precision agriculture; automated orchard or vineyard treatments; machine
   vision; innovative sprayer; FDA foliage detector algorithm; depth
   analysis; monocular vision},
Keywords-Plus = {SENSORS; VISION},
Research-Areas = {Agriculture; Plant Sciences},
Web-of-Science-Categories  = {Agronomy; Plant Sciences},
Author-Email = {luca.debortoli@dia.units.it
   marsi@units.it
   francesco.marinello@unipd.it
   carrato@units.it
   ramponi@units.it
   pgallina@units.it},
Affiliations = {University of Trieste; University of Padua; University of Trieste},
ResearcherID-Numbers = {Marinello, Francesco/H-5619-2019},
ORCID-Numbers = {Marinello, Francesco/0000-0002-3283-5665},
Funding-Acknowledgement = {POR-FESR},
Funding-Text = {This research was funded by the grant POR-FESR 2014-2020, Activity 1.3.a
   ``New intelligent machines and systems for treatment on foliar
   apparatus, irrigation and sowing{''}.},
Cited-References = {{[}Anonymous], LIDAR ROBOTIS LDS 01.
   {[}Anonymous], RASPBERRY PI 4.
   {[}Anonymous], ODROID XU4.
   {[}Anonymous], RGB D CAMERA INTEL D.
   Arno J, 2013, PRECIS AGRIC, V14, P290, DOI 10.1007/s11119-012-9295-0.
   Benet G, 2002, ROBOT AUTON SYST, V40, P255, DOI 10.1016/S0921-8890(02)00271-3.
   Berenstein R, 2010, INTEL SERV ROBOT, V3, P233, DOI 10.1007/s11370-010-0078-z.
   Braun T., 2010, P 1 COMM VEH TECHN S.
   Cross JV, 2001, CROP PROT, V20, P13, DOI 10.1016/S0261-2194(00)00046-6.
   Dey D., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P329, DOI 10.1109/WACV.2012.6163017.
   Diskin Y, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.2.023003.
   eeshop, IR RECIVER CHQ1838.
   Fofi D, 2004, BBA LIB, V5303, P90, DOI 10.1117/12.525369.
   Gil E, 2013, SENSORS-BASEL, V13, P516, DOI 10.3390/s130100516.
   Hocevar M, 2010, J PLANT DIS PROTECT, V117, P71.
   Kazmi W, 2014, ISPRS J PHOTOGRAMM, V88, P128, DOI 10.1016/j.isprsjprs.2013.11.012.
   Kinetics, US.
   Lai Q, 2018, AUTOMATION AGR SECUR, DOI 10.5772/intechopen.73622.
   licor, LI COR LAI 2200C PLA.
   Lu JH, 1997, IEEE T CIRC SYST VID, V7, P429, DOI 10.1109/76.564122.
   Marinello F, 2007, MEAS SCI TECHNOL, V18, P1404, DOI 10.1088/0957-0233/18/5/028.
   Marinello F., 2017, ADV ANIMAL BIOSCIENC, V8, P525, DOI {[}10.1017/S2040470017001042, DOI 10.1017/S2040470017001042].
   Marinello F, 2017, ENG RUR DEVELOP, P730, DOI 10.22616/ERDev2017.16.N147.
   McCarthy CL, 2010, INTEL SERV ROBOT, V3, P209, DOI 10.1007/s11370-010-0075-2.
   Nobis M, 2005, AGR FOREST METEOROL, V128, P243, DOI 10.1016/j.agrformet.2004.10.002.
   Pagliai A, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14051145.
   Rosell JR, 2012, COMPUT ELECTRON AGR, V81, P124, DOI 10.1016/j.compag.2011.09.007.
   Saiz-Rubio V, 2012, SPAN J AGRIC RES, V10, P596, DOI 10.5424/sjar/2012103-508-11.
   Sarri D, 2017, COMPUT ELECTRON AGR, V142, P248, DOI 10.1016/j.compag.2017.09.018.
   Toews RB, 2012, ERWERBS-OBSTBAU, V54, P49, DOI 10.1007/s10341-012-0161-z.
   Val L, 2007, P 6 EUROPEAN C PRECI.
   Vidoni R, 2018, PROCEEDINGS OF THE ASME INTERNATIONAL MECHANICAL ENGINEERING CONGRESS AND EXPOSITION, 2017, VOL 4A.
   Xiao K, 2017, COMPUT ELECTRON AGR, V133, P30, DOI {[}1, 10.1016/j.compag.2016.12.002].
   Narvaez FY, 2017, IEEE-ASME T MECH, V22, P2428, DOI 10.1109/TMECH.2017.2760866.},
Number-of-Cited-References = {34},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {7},
Journal-ISO = {Agronomy-Basel},
Doc-Delivery-Number = {2K0HI},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000816026000001},
OA = {gold, Green Published},
DA = {2023-08-12},
}

@article{ WOS:000632597500007,
Author = {Xia, Yixi and Yabuki, Nobuyoshi and Fukuda, Tomohiro},
Title = {Development of a system for assessing the quality of urban street-level
   greenery using street view images and deep learning},
Journal = {URBAN FORESTRY \& URBAN GREENING},
Year = {2021},
Volume = {59},
Month = {APR},
Abstract = {Street greenery has long played a vital role in the quality of urban
   landscapes and is closely related to people's physical and mental
   health. Also, the level of street greenery is an important indicator of
   urban environmental quality. However, despite extensive research into
   environmental assessment methods for urban greenery, plant
   identification and greenery index calculations are still mostly done
   manually. In this research, we developed a method based on semantic
   segmentation processing of street view images to calculate the Green
   View Index of urban streets, and the Panoramic View Green View Index
   (PVGVI) is proposed for measuring the visible street-level greenery. We
   validated the results by comparison with those of manual inspection and
   the Pyramid Scene Parsing Network method. The vegetation detection rate
   of our method is very close to the ground truth value, which means it
   can distinguish almost all of the vegetation information from the street
   view images, and based on it we can calculate the PVGVI which is
   reliable. In addition, we conducted a case study of street-level
   greenery using the PVGVI and confirmed that this method can better
   visualize urban street-level greenery. The proposed method is scalable
   and automatable, and it contributes to the growing trend of integrating
   large freely available street view image datasets with semantic
   segmentation to inform urban planners.},
Publisher = {ELSEVIER GMBH},
Address = {HACKERBRUCKE 6, 80335 MUNICH, GERMANY},
Type = {Article},
Language = {English},
Affiliation = {Xia, YX (Corresponding Author), Osaka Univ, Grad Sch Engn, Div Sustainable Energy \& Environm Engn, 2-1 Yamadaoka, Suita, Osaka 5650871, Japan.
   Xia, Yixi; Yabuki, Nobuyoshi; Fukuda, Tomohiro, Osaka Univ, Grad Sch Engn, Div Sustainable Energy \& Environm Engn, 2-1 Yamadaoka, Suita, Osaka 5650871, Japan.},
DOI = {10.1016/j.ufug.2021.126995},
EarlyAccessDate = {FEB 2021},
Article-Number = {126995},
ISSN = {1618-8667},
EISSN = {1610-8167},
Keywords = {Deep learning; Green View Index (GVI); Image segmentation; Panoramic
   View Green View Index (PVGVI); Street view images; Urban green space},
Keywords-Plus = {SPATIAL-DISTRIBUTION; TREES; NEIGHBORHOODS},
Research-Areas = {Plant Sciences; Environmental Sciences \& Ecology; Forestry; Urban
   Studies},
Web-of-Science-Categories  = {Plant Sciences; Environmental Studies; Forestry; Urban Studies},
Author-Email = {yixi@it.see.eng.osaka-u.ac.jp},
Affiliations = {Osaka University},
ResearcherID-Numbers = {Fukuda, Tomohiro/J-6490-2016
   },
ORCID-Numbers = {Fukuda, Tomohiro/0000-0002-4271-4445
   Yabuki, Nobuyoshi/0000-0002-2944-4540},
Cited-References = {Aoki Y., 1985, Landscape Research, V10, P9, DOI 10.1080/01426398508706131.
   Aoki Y., 1991, LANDSCAPE RES, V16, P3, DOI DOI 10.1080/01426399108706344.
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615.
   Bain L., 2012, LIVING STREETS STRAT.
   Barbierato E., 2019, EARTH OBSERVATION AD, V187.
   Blanco H, 2009, PROG PLANN, V71, P153, DOI 10.1016/j.progress.2009.03.001.
   Branson S, 2018, ISPRS J PHOTOGRAMM, V135, P13, DOI 10.1016/j.isprsjprs.2017.11.008.
   Cai BY, 2018, IEEE INT CONGR BIG, P49, DOI 10.1109/BigDataCongress.2018.00014.
   Chen J, 2013, INT ARCH PHOTOGRAMM, V40-4-W3, P47, DOI 10.5194/isprsarchives-XL-4-W3-47-2013.
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2\_49.
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709.
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350.
   Coutts C, 2008, ENVIRON PLANN B, V35, P552, DOI 10.1068/b3406.
   Gebru T, 2017, P NATL ACAD SCI USA, V114, P13108, DOI 10.1073/pnas.1700035114.
   Giles-Corti B, 2003, AM J HEALTH PROMOT, V18, P93, DOI 10.4278/0890-1171-18.1.93.
   Glaeser EL, 2018, ECON INQ, V56, P114, DOI 10.1111/ecin.12364.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Hidalgo C.A., 2015, URBAN EC REGIONAL ST.
   Kang J, 2018, ISPRS J PHOTOGRAMM, V145, P44, DOI 10.1016/j.isprsjprs.2018.02.006.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Landry SM, 2009, ENVIRON PLANN A, V41, P2651, DOI 10.1068/a41236.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lee ACK, 2011, J PUBLIC HEALTH-UK, V33, P212, DOI 10.1093/pubmed/fdq068.
   Leslie E, 2010, LANDSCAPE URBAN PLAN, V95, P28, DOI 10.1016/j.landurbplan.2009.11.002.
   Li X., 2019, USING GOOGLE STREET.
   Li XJ, 2018, URBAN FOR URBAN GREE, V31, P109, DOI 10.1016/j.ufug.2018.02.013.
   Li XJ, 2018, LANDSCAPE URBAN PLAN, V169, P81, DOI 10.1016/j.landurbplan.2017.08.011.
   Li XJ, 2015, URBAN FOR URBAN GREE, V14, P751, DOI 10.1016/j.ufug.2015.07.006.
   Li XJ, 2015, URBAN FOR URBAN GREE, V14, P675, DOI 10.1016/j.ufug.2015.06.006.
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
   Louv R., 2008, Last child in the woods: saving our children from nature-deficit disorder.
   MacFaden SW, 2012, J APPL REMOTE SENS, V6, DOI 10.1117/1.JRS.6.063567.
   Mavrogianni A, 2014, BUILD ENVIRON, V78, P183, DOI 10.1016/j.buildenv.2014.04.008.
   Naik N, 2014, IEEE COMPUT SOC CONF, P793, DOI 10.1109/CVPRW.2014.121.
   Rundle AG, 2011, AM J PREV MED, V40, P94, DOI 10.1016/j.amepre.2010.09.034.
   Schroeder H. W., 1983, Journal of Arboriculture, V9, P237.
   Seiferling I, 2017, LANDSCAPE URBAN PLAN, V165, P93, DOI 10.1016/j.landurbplan.2017.05.010.
   Shet V., 2014, GO BACK TIME STREET.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308.
   Urbina M, 2015, 2015 IEEE THIRTY FIFTH CENTRAL AMERICAN AND PANAMA CONVENTION (CONCAPAN XXXV).
   Wales, 2016, TREE COVER WALESTOWN.
   Wegner JD, 2016, PROC CVPR IEEE, P6014, DOI 10.1109/CVPR.2016.647.
   Wolf KL, 2005, J FOREST, V103, P396.
   Yang J, 2009, LANDSCAPE URBAN PLAN, V91, P97, DOI 10.1016/j.landurbplan.2008.12.004.
   Yu XY, 2019, FORESTS, V10, DOI 10.3390/f10010003.
   Zhang WX, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18082484.
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660.},
Number-of-Cited-References = {49},
Times-Cited = {43},
Usage-Count-Last-180-days = {59},
Usage-Count-Since-2013 = {205},
Journal-ISO = {Urban For. Urban Green.},
Doc-Delivery-Number = {RC1XC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
Unique-ID = {WOS:000632597500007},
DA = {2023-08-12},
}

@article{ WOS:000464312100001,
Author = {Sadeghi-Tehran, Pouria and Angelov, Plamen and Virlet, Nicolas and
   Hawkesford, Malcolm J.},
Title = {Scalable Database Indexing and Fast Image Retrieval Based on Deep
   Learning and Hierarchically Nested Structure Applied to Remote Sensing
   and Plant Biology},
Journal = {JOURNAL OF IMAGING},
Year = {2019},
Volume = {5},
Number = {3},
Month = {MAR 1},
Abstract = {Digitalisation has opened a wealth of new data opportunities by
   revolutionizing how images are captured. Although the cost of data
   generation is no longer a major concern, the data management and
   processing have become a bottleneck. Any successful visual trait system
   requires automated data structuring and a data retrieval model to
   manage, search, and retrieve unstructured and complex image data. This
   paper investigates a highly scalable and computationally efficient image
   retrieval system for real-time content-based searching through
   large-scale image repositories in the domain of remote sensing and plant
   biology. Images are processed independently without considering any
   relevant context between sub-sets of images. We utilize a deep
   Convolutional Neural Network (CNN) model as a feature extractor to
   derive deep feature representations from the imaging data. In addition,
   we propose an effective scheme to optimize data structure that can
   facilitate faster querying at search time based on the hierarchically
   nested structure and recursive similarity measurements. A thorough
   series of tests were carried out for plant identification and
   high-resolution remote sensing data to evaluate the accuracy and the
   computational efficiency of the proposed approach against other
   content-based image retrieval (CBIR) techniques, such as the bag of
   visual words (BOVW) and multiple feature fusion techniques. The results
   demonstrate that the proposed scheme is effective and considerably
   faster than conventional indexing structures.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Sadeghi-Tehran, P (Corresponding Author), Rothamsted Res, Dept Plant Sci, Harpenden AL5 2JQ, Herts, England.
   Sadeghi-Tehran, Pouria; Virlet, Nicolas; Hawkesford, Malcolm J., Rothamsted Res, Dept Plant Sci, Harpenden AL5 2JQ, Herts, England.
   Angelov, Plamen, Univ Lancaster, InfoLab21, Sch Comp \& Commun, Lancaster LA1 4WA, England.},
DOI = {10.3390/jimaging5030033},
Article-Number = {33},
EISSN = {2313-433X},
Keywords = {content-based image retrieval; deep convolutional neural networks;
   information retrieval; data indexing; recursive similarity measurement;
   deep learning; bag of visual words; remote sensing},
Keywords-Plus = {FIELD PHENOTYPING PLATFORM; NEURAL-NETWORKS; FEATURES; COLOR},
Research-Areas = {Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Imaging Science \& Photographic Technology},
Author-Email = {pouria.sadeghi-tehran@rothamsted.ac.uk
   p.angelov@lancaster.ac.uk
   nicolas.virlet@rothamsted.ac.uk
   malcolm.hawkesford@rothamsted.ac.uk},
Affiliations = {UK Research \& Innovation (UKRI); Biotechnology and Biological Sciences
   Research Council (BBSRC); Rothamsted Research; N8 Research Partnership;
   Lancaster University},
ResearcherID-Numbers = {Hawkesford, Malcolm John/AAE-1522-2022
   Angelov, Plamen/AAE-8284-2019
   },
ORCID-Numbers = {Angelov, Plamen/0000-0002-5770-934X
   Sadeghi-Tehran, Pouria/0000-0003-0352-227X
   Hawkesford, Malcolm/0000-0001-8759-3969
   Virlet, Nicolas/0000-0001-6030-4282},
Funding-Acknowledgement = {Biotechnology and Biological Sciences Research Council (BBSRC) of the UK
   as part of the Designing Future Wheat {[}BBS/E/C/000I0220]; BBSRC
   {[}BBS/E/C/000I0220] Funding Source: UKRI},
Funding-Text = {Rothamsted Research receives support from the Biotechnology and
   Biological Sciences Research Council (BBSRC) of the UK as part of the
   Designing Future Wheat (BBS/E/C/000I0220) project.},
Cited-References = {Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736.
   Ahonen T., 2009, P 16 SCAND C IM AN S.
   Alzu'bi A, 2015, J VIS COMMUN IMAGE R, V32, P20, DOI 10.1016/j.jvcir.2015.07.012.
   Angelov P., 2012, U.S. Patent, Patent No. {[}US9390265B2, 9390265].
   Angelov P, 2017, INT J INTELL SYST, V32, P82, DOI 10.1002/int.21837.
   Angelov P, 2015, PROCEDIA COMPUT SCI, V53, P1, DOI 10.1016/j.procs.2015.07.273.
   Angelov P, 2011, INT J INTELL SYST, V26, P189, DOI 10.1002/int.20462.
   {[}Anonymous], 2006, IEEE COMPUTER SOC C.
   {[}Anonymous], 2010, P ACM INT C MULTIMED.
   {[}Anonymous], 2002, EVOLVING RULE BASED.
   {[}Anonymous], 2017, RECENT ADV CONTENT B.
   Bartolini I, 2018, MULTIMEDIA SYST, V24, P459, DOI 10.1007/s00530-017-0567-4.
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006.
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833.
   Busemeyer L, 2013, SENSORS-BASEL, V13, P2830, DOI 10.3390/s130302830.
   Cai J., 2014, P INT C MULT RETR AC, P407.
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598.
   Chih-Fong Tsai, 2012, ISRN Artificial Intelligence, DOI 10.5402/2012/376804.
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248.
   Distasi R, 2000, J VISUAL LANG COMPUT, V11, P369, DOI 10.1006/jvlc.2000.0167.
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0\_26.
   Guo YM, 2012, PATTERN RECOGN, V45, P3834, DOI 10.1016/j.patcog.2012.04.003.
   Han S., 2015, P 2015 IEEE INT C IM.
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0\_38.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412.
   Jegou H., 2010, P 2010 IEEE COMP SOC.
   Jiang F, 2016, NEUROCOMPUTING, V175, P146, DOI 10.1016/j.neucom.2015.10.044.
   Jiang K, 2015, PROC CVPR IEEE, P4933, DOI 10.1109/CVPR.2015.7299127.
   Joe Yue-Hei Ng, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P53, DOI 10.1109/CVPRW.2015.7301272.
   Johnson R., 2015, P 29 C NEUR INF PROC.
   Kirchgessner N, 2017, FUNCT PLANT BIOL, V44, P154, DOI 10.1071/FP16165.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947.
   Larson RR, 2010, J AM SOC INF SCI TEC, V61, P852, DOI 10.1002/asi.21234.
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542.
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005.
   Li YS, 2016, IEEE GEOSCI REMOTE S, V13, P157, DOI 10.1109/LGRS.2015.2503142.
   Lin CH, 2009, IMAGE VISION COMPUT, V27, P658, DOI 10.1016/j.imavis.2008.07.004.
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227.
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489.
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
   Mezaris V., 2003, P 2003 INT C IM PROC.
   Mohedano E, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P327, DOI 10.1145/2911996.2912061.
   Nikkam Pushpalatha Shrikant, 2016, International Journal of Computer Vision and Image Processing, V6, P54, DOI 10.4018/IJCVIP.2016070104.
   Nister D, 2006, P IEEE COMP SOC C CO, V2, P2161.
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2.
   Olivas E. S., 2009, HDB RES MACHINE LEAR.
   Perronnin F., 2010, P 2010 IEEE COMP SOC.
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131.
   Romero A, 2016, IEEE T GEOSCI REMOTE, V54, P1349, DOI 10.1109/TGRS.2015.2478379.
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003.
   Shen Y., 2014, P 23 ACM INT C INF K.
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556.
   Singh S. M., 2012, INT J COMPUT SCI ISS, V9.
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972.
   Sun SY, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818708.
   Tang JH, 2015, IEEE T IMAGE PROCESS, V24, P2827, DOI 10.1109/TIP.2015.2421443.
   Tzelepi M, 2018, SIGNAL PROCESS-IMAGE, V63, P30, DOI 10.1016/j.image.2018.01.007.
   Tzelepi M, 2018, NEUROCOMPUTING, V275, P2467, DOI 10.1016/j.neucom.2017.11.022.
   Virlet N, 2017, FUNCT PLANT BIOL, V44, P143, DOI 10.1071/FP16163.
   Wan J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P157, DOI 10.1145/2647868.2654948.
   WANG JJ, 2010, PROC CVPR IEEE, P3360, DOI DOI 10.1109/CVPR.2010.5540018.
   Wang XY, 2014, MULTIMED TOOLS APPL, V68, P545, DOI 10.1007/s11042-012-1055-7.
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566.
   Yang L, 2014, BIOINFORMATICS, V30, P996, DOI 10.1093/bioinformatics/btt623.
   Yang Y., 2010, ADV GEOGRAPHIC INFOR, P270, DOI DOI 10.1145/1869790.1869829.
   You J, 2009, PROCEEDINGS OF THE 8TH IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS, P188, DOI 10.1109/COGINF.2009.5250753.
   Yu H, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P929, DOI 10.1109/ICIP.2002.1039125.
   Yu H, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8030259.
   Yue J, 2011, MATH COMPUT MODEL, V54, P1121, DOI 10.1016/j.mcm.2010.11.044.
   Zhang C., 2014, CIT J COMPUT INF TEC, V22, P1, DOI DOI 10.2498/cit.1002256.
   Zhang Jian, 2018, ARXIV180202904.},
Number-of-Cited-References = {75},
Times-Cited = {16},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {8},
Journal-ISO = {J. Imaging},
Doc-Delivery-Number = {HT1GM},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000464312100001},
OA = {Green Submitted, Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000230484500122,
Author = {Fu, H and Chi, ZR and Feng, DG and Song, JT},
Book-Group-Author = {IEEE},
Title = {Machine learning techniques for ontology-based leaf classification},
Booktitle = {2004 8TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION, ROBOTICS AND
   VISION, VOLS 1-3},
Series = {International Conference on Control Automation Robotics and Vision},
Year = {2004},
Pages = {681-686},
Note = {8th International Conference on Control, Automation, Robotics and Vision
   (ICARCV 2004), Kunming, PEOPLES R CHINA, DEC 06-09, 2004},
Organization = {Nanyang Technol Univ, Sch Elect \& Elect Engn; Republic Singapore;
   Nanjing Univ Sci \& Technol, Peoples Republic China; IEEE Robotics \&
   Automation Soc; IEEE Syst, Man \& Cybernet Soc; IEEE Control Syst Soc;
   IEE; Natl Nat Sci Fdn China; Singapore Tech Engn Ltd; Lee Fdn},
Abstract = {Leaf classification, indexing as well as retrieval is an important part
   of a computerized plant identification system. In this paper, an
   integrated approach for an ontology-based leaf classification system is
   proposed, wherein machine teaming techniques play a crucial role for the
   automatization of the system. For the leaf contour classification, a
   sealed CCD code system is proposed to categorize the basic shape and
   margin type of a leaf by using the similar taxonomy principle adopted by
   the botanists. Then a trained neural network is employed to recognize
   the detailed tooth patterns. The measurement on an unlobed leaf is also
   conducted automatically according to the method used in botany. For the
   leaf vein recognition, the vein texture is extracted by employing an
   efficient combined thresholding and neural network approach so as to
   obtain more vein details of a leaf Compared with the past studies, the
   proposed method integrates low-level features of an image and the
   specific knowledge in the domain (ontology) of botany, and therefore
   provides a more practical system for users to comprehend and handle.
   Primary experiments have shown promising results and proven the
   feasibility of the proposed system.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Chi, ZR (Corresponding Author), Hong Kong Polytech Univ, Ctr Multimedia Signal Proc, Dept Elect \& Informat Engn, Kowloon, Hong Kong, Peoples R China.
   Hong Kong Polytech Univ, Ctr Multimedia Signal Proc, Dept Elect \& Informat Engn, Kowloon, Hong Kong, Peoples R China.},
ISSN = {2474-2953},
ISBN = {0-7803-8653-1},
Research-Areas = {Automation \& Control Systems; Computer Science; Robotics},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Artificial
   Intelligence; Robotics},
Author-Email = {enzheru@polyu.edu.hk},
Affiliations = {Hong Kong Polytechnic University},
ORCID-Numbers = {Feng, Dagan/0000-0002-3381-214X
   fu, hong/0000-0003-2246-7552},
Cited-References = {ABBASI S, 1997, INT C SCAL SPAC THEO, P284.
   {[}Anonymous], 2001, ONTOLOGY DEV.
   ASH A., 1999, MANUAL LEAF ARCHITEC.
   Chi ZR, 2003, SIG COM TEC, P432.
   Fu H, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS \& SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P208.
   Im C, 1998, INT C PATT RECOG, P1171, DOI 10.1109/ICPR.1998.711904.
   NIELSEN H, 1996, ACTA HORT ISHS, V406, P153.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.
   Wang ZY, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOL 1 \& 2, P372, DOI 10.1109/FUZZ.2002.1005019.
   Wang ZY, 2000, LECT NOTES COMPUT SC, V1929, P477.},
Number-of-Cited-References = {10},
Times-Cited = {10},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BCP26},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000230484500122},
DA = {2023-08-12},
}

@article{ WOS:000496827100140,
Author = {Fricker, Geoffrey A. and Ventura, Jonathan D. and Wolf, Jeffrey A. and
   North, Malcolm P. and Davis, Frank W. and Franklin, Janet},
Title = {A Convolutional Neural Network Classifier Identifies Tree Species in
   Mixed-Conifer Forest from Hyperspectral Imagery},
Journal = {REMOTE SENSING},
Year = {2019},
Volume = {11},
Number = {19},
Month = {OCT},
Abstract = {In this study, we automate tree species classification and mapping using
   field-based training data, high spatial resolution airborne
   hyperspectral imagery, and a convolutional neural network classifier
   (CNN). We tested our methods by identifying seven dominant trees species
   as well as dead standing trees in a mixed-conifer forest in the Southern
   Sierra Nevada Mountains, CA (USA) using training, validation, and
   testing datasets composed of spatially-explicit transects and plots
   sampled across a single strip of imaging spectroscopy. We also used a
   three-band `Red-Green-Blue' pseudo true-color subset of the
   hyperspectral imagery strip to test the classification accuracy of a CNN
   model without the additional non-visible spectral data provided in the
   hyperspectral imagery. Our classifier is pixel-based rather than object
   based, although we use three-dimensional structural information from
   airborne Light Detection and Ranging (LiDAR) to identify trees (points >
   5 m above the ground) and the classifier was applied to image pixels
   that were thus identified as tree crowns. By training a CNN classifier
   using field data and hyperspectral imagery, we were able to accurately
   identify tree species and predict their distribution, as well as the
   distribution of tree mortality, across the landscape. Using a window
   size of 15 pixels and eight hidden convolutional layers, a CNN model
   classified the correct species of 713 individual trees from
   hyperspectral imagery with an average F-score of 0.87 and F-scores
   ranging from 0.67-0.95 depending on species. The CNN classification
   model performance increased from a combined F-score of 0.64 for the
   Red-Green-Blue model to a combined F-score of 0.87 for the hyperspectral
   model. The hyperspectral CNN model captures the species composition
   changes across 700 meters (1935 to 2630 m) of elevation from a
   lower-elevation mixed oak conifer forest to a higher-elevation
   fir-dominated coniferous forest. High resolution tree species maps can
   support forest ecosystem monitoring and management, and identifying dead
   trees aids landscape assessment of forest mortality resulting from
   drought, insects and pathogens. We publicly provide our code to apply
   deep learning classifiers to tree species identification from geospatial
   imagery and field training data.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Fricker, GA (Corresponding Author), Calif Polytech State Univ San Luis Obispo, Dept Social Sci, San Luis Obispo, CA 93407 USA.
   Fricker, GA (Corresponding Author), Univ Calif Riverside, Dept Bot \& Plant Sci, Riverside, CA 92521 USA.
   Fricker, Geoffrey A., Calif Polytech State Univ San Luis Obispo, Dept Social Sci, San Luis Obispo, CA 93407 USA.
   Fricker, Geoffrey A.; Franklin, Janet, Univ Calif Riverside, Dept Bot \& Plant Sci, Riverside, CA 92521 USA.
   Ventura, Jonathan D., Calif Polytech State Univ San Luis Obispo, Dept Comp Sci \& Software Engn, San Luis Obispo, CA 93407 USA.
   Wolf, Jeffrey A., Amazon Corp, Amazon Web Serv, Seattle, WA 98109 USA.
   North, Malcolm P., US Forest Serv, PSW Res Stn, Mammoth Lakes, CA 93546 USA.
   Davis, Frank W., Univ Calif Santa Barbara, Bren Sch Environm Sci \& Management, Santa Barbara, CA 93106 USA.},
DOI = {10.3390/rs11192326},
Article-Number = {2326},
EISSN = {2072-4292},
Keywords = {deep learning; species distribution modeling; convolutional neural
   networks; hyperspectral imagery},
Keywords-Plus = {LIDAR DATA; CROWN DELINEATION; TROPICAL FORESTS; EO-1 HYPERION;
   INVENTORY; BIOMASS; RECOGNITION; SCALE; PLANT; LEAF},
Research-Areas = {Environmental Sciences \& Ecology; Geology; Remote Sensing; Imaging
   Science \& Photographic Technology},
Web-of-Science-Categories  = {Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science \& Photographic Technology},
Author-Email = {africker@calpoly.edu
   jventu09@calpoly.edu
   jawolf@amazon.com
   mnorth@ucdavis.edu
   fwd@bren.ucsb.edu
   jfrankl@ucr.edu},
Affiliations = {California State University System; California Polytechnic State
   University San Luis Obispo; University of California System; University
   of California Riverside; California State University System; California
   Polytechnic State University San Luis Obispo; Amazon.com; United States
   Department of Agriculture (USDA); United States Forest Service;
   University of California System; University of California Santa Barbara},
ResearcherID-Numbers = {North, Malcolm/AAW-8897-2020
   Franklin, Janet/G-6538-2013
   },
ORCID-Numbers = {North, Malcolm/0000-0002-9090-784X
   Fricker, Geoffrey/0000-0001-6310-3427
   Franklin, Janet/0000-0003-0314-4598
   Davis, Frank/0000-0002-4643-5718},
Funding-Acknowledgement = {U.S. National Science Foundation {[}EF-1065864, EF-1550653, EF-1065826,
   EF-1550640]; Joint Fire Sciences Program {[}15-1-07-6]},
Funding-Text = {Funding was provided by the U.S. National Science Foundation
   (EF-1065864, -1550653, -1065826 and -1550640) and the Joint Fire
   Sciences Program (15-1-07-6).},
Cited-References = {Allen CD, 2015, ECOSPHERE, V6, DOI 10.1890/ES15-00203.1.
   Allen CD, 2010, FOREST ECOL MANAG, V259, P660, DOI 10.1016/j.foreco.2009.09.001.
   Alonzo M, 2014, REMOTE SENS ENVIRON, V148, P70, DOI 10.1016/j.rse.2014.03.018.
   Asner GP, 2007, J APPL REMOTE SENS, V1, DOI 10.1117/1.2794018.
   Asner GP, 2009, FRONT ECOL ENVIRON, V7, P269, DOI 10.1890/070152.
   Asner GP, 2008, REMOTE SENS ENVIRON, V112, P3958, DOI 10.1016/j.rse.2008.07.003.
   Asner GP, 2008, P NATL ACAD SCI USA, V105, P4519, DOI 10.1073/pnas.0710811105.
   Asner GP, 2016, P NATL ACAD SCI USA, V113, pE249, DOI 10.1073/pnas.1523397113.
   Asner GP, 2012, REMOTE SENS ENVIRON, V124, P454, DOI 10.1016/j.rse.2012.06.012.
   Ballanti L, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8060445.
   Barry S, 2006, J APPL ECOL, V43, P413, DOI 10.1111/j.1365-2664.2006.01136.x.
   Bauriegel E, 2011, COMPUT ELECTRON AGR, V75, P304, DOI 10.1016/j.compag.2010.12.006.
   Bergstra J., 2011, NIPS 2011 BIGLEARNIN, P1.
   Brell Maximilian, 2017, IEEE Transactions on Geoscience and Remote Sensing, V55, P2799, DOI 10.1109/TGRS.2017.2654516.
   Cartus O, 2012, REMOTE SENS ENVIRON, V124, P466, DOI 10.1016/j.rse.2012.05.029.
   Castelluccio M., 2015, ARXIV150800092, DOI DOI 10.1016/S1872-2032(08)60029-3.
   Chang CI, 2004, IEEE T GEOSCI REMOTE, V42, P608, DOI 10.1109/TGRS.2003.819189.
   Chen GY, 2011, IEEE T GEOSCI REMOTE, V49, P973, DOI 10.1109/TGRS.2010.2075937.
   Chen YS, 2016, IEEE T GEOSCI REMOTE, V54, P6232, DOI 10.1109/TGRS.2016.2584107.
   Cheng G, 2018, IEEE T GEOSCI REMOTE, V56, P2811, DOI 10.1109/TGRS.2017.2783902.
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023.
   Clark ML, 2005, REMOTE SENS ENVIRON, V96, P375, DOI 10.1016/j.rse.2005.03.009.
   Colgan MS, 2012, REMOTE SENS-BASEL, V4, P3462, DOI 10.3390/rs4113462.
   Condit R, 2000, SCIENCE, V288, P1414, DOI 10.1126/science.288.5470.1414.
   Conroy MJ, 1996, ECOL APPL, V6, P763, DOI 10.2307/2269481.
   Cudahy TJ, 2001, INT GEOSCI REMOTE SE, P314, DOI 10.1109/IGARSS.2001.976142.
   Culvenor DS, 2003, REMOTE SENSING OF FOREST ENVIRONMENTS: CONCEPTS AND CASE STUDIES, P255.
   Dalponte M, 2015, ISPRS J PHOTOGRAMM, V110, P77, DOI 10.1016/j.isprsjprs.2015.10.010.
   Dalponte M, 2014, REMOTE SENS ENVIRON, V140, P306, DOI 10.1016/j.rse.2013.09.006.
   Dalponte M, 2012, REMOTE SENS ENVIRON, V123, P258, DOI 10.1016/j.rse.2012.03.013.
   Das AJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0069917.
   Doughty CE, 2011, OECOLOGIA, V165, P289, DOI 10.1007/s00442-010-1800-4.
   Duncanson LI, 2014, REMOTE SENS ENVIRON, V154, P378, DOI 10.1016/j.rse.2013.07.044.
   Edwards TC, 2006, ECOL MODEL, V199, P132, DOI 10.1016/j.ecolmodel.2006.05.016.
   Evans D. L., 1992, Southern Journal of Applied Forestry, V16, P67.
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013.
   Feret JB, 2013, IEEE T GEOSCI REMOTE, V51, P73, DOI 10.1109/TGRS.2012.2199323.
   Ferraz A, 2016, REMOTE SENS ENVIRON, V183, P318, DOI 10.1016/j.rse.2016.05.028.
   Ferreira MP, 2016, REMOTE SENS ENVIRON, V179, P66, DOI 10.1016/j.rse.2016.03.021.
   Fielding AH, 1997, ENVIRON CONSERV, V24, P38, DOI 10.1017/S0376892997000088.
   Garrido-Novell C, 2012, J FOOD ENG, V113, P281, DOI 10.1016/j.jfoodeng.2012.05.038.
   Ghosh A, 2014, INT J APPL EARTH OBS, V26, P49, DOI 10.1016/j.jag.2013.05.017.
   Gong P, 1997, REMOTE SENS ENVIRON, V62, P189, DOI 10.1016/S0034-4257(97)00094-1.
   Graves SJ, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8020161.
   Hawes JE, 2012, FOREST ECOL MANAG, V281, P163, DOI 10.1016/j.foreco.2012.06.023.
   He HS, 1999, ECOLOGY, V80, P81, DOI 10.1890/0012-9658(1999)080{[}0081:SEASSO]2.0.CO;2.
   Holmgren J, 2004, REMOTE SENS ENVIRON, V90, P415, DOI 10.1016/S0034-4257(03)00140-8.
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680.
   Hu W, 2015, J SENSORS, V2015, DOI 10.1155/2015/258619.
   Hyyppa J, 2008, INT J REMOTE SENS, V29, P1339, DOI 10.1080/01431160701736489.
   Immitzer M, 2012, REMOTE SENS-BASEL, V4, P2661, DOI 10.3390/rs4092661.
   Kampe T., 2013, 005 NEON TM.
   Kampe TU, 2010, J APPL REMOTE SENS, V4, DOI 10.1117/1.3361375.
   Ke YH, 2010, REMOTE SENS ENVIRON, V114, P1141, DOI 10.1016/j.rse.2010.01.002.
   Kim Y, 2014, ARXIV14085882, V1408, P5882, DOI 10.3115/v1/D14-1181.
   Korpela I, 2011, REMOTE SENS ENVIRON, V115, P2062, DOI 10.1016/j.rse.2011.04.008.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Kruse FA, 2003, IEEE T GEOSCI REMOTE, V41, P1388, DOI 10.1109/TGRS.2003.812908.
   Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195.
   Laybros A, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11070789.
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539.
   Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839.
   Lefsky MA, 2001, CAN J FOREST RES, V31, P78, DOI 10.1139/cjfr-31-1-78.
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
   Lucas R, 2010, IEEE J-STARS, V3, P576, DOI 10.1109/JSTARS.2010.2086436.
   Makantasis K, 2015, INT GEOSCI REMOTE SE, P4959, DOI 10.1109/IGARSS.2015.7326945.
   Marrs J, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11070819.
   Martin ME, 1998, REMOTE SENS ENVIRON, V65, P249, DOI 10.1016/S0034-4257(98)00035-2.
   Maschler J, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081218.
   Meier U, 2011, PROC INT CONF DOC, P1250, DOI 10.1109/ICDAR.2011.252.
   Meyer MD, 2007, PLANT SOIL, V294, P113, DOI 10.1007/s11104-007-9235-3.
   Mou LC, 2017, IEEE T GEOSCI REMOTE, V55, P3639, DOI 10.1109/TGRS.2016.2636241.
   Nagendra H, 2001, INT J REMOTE SENS, V22, P2377, DOI 10.1080/01431160117096.
   Nevalainen O, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9030185.
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115.
   North M., 2002, General Technical Report - Pacific Southwest Research Station, USDA Forest Service.
   North M, 2016, ECOSYSTEMS OF CALIFORNIA, P553.
   Okamoto H, 2007, WEED BIOL MANAG, V7, P31, DOI 10.1111/j.1445-6664.2006.00234.x.
   Pearlman JS, 2003, IEEE T GEOSCI REMOTE, V41, P1160, DOI 10.1109/TGRS.2003.815018.
   Peet R.K., 1974, Annual Rev Ecol Syst, V5, P285, DOI 10.1146/annurev.es.05.110174.001441.
   Penatti Otavio A. B., 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P44, DOI 10.1109/CVPRW.2015.7301382.
   Pengra BW, 2007, REMOTE SENS ENVIRON, V108, P74, DOI 10.1016/j.rse.2006.11.002.
   Popescu SC, 2007, BIOMASS BIOENERG, V31, P646, DOI 10.1016/j.biombioe.2007.06.022.
   Pu RL, 2009, INT J REMOTE SENS, V30, P2759, DOI 10.1080/01431160802555820.
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4\_28.
   Schimel DS, 2013, FRONT ECOL ENVIRON, V11, P129, DOI 10.1890/120111.
   Shen X, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111180.
   Sothe C, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111338.
   Stehman SV, 1997, REMOTE SENS ENVIRON, V62, P77, DOI 10.1016/S0034-4257(97)00083-7.
   Tomppo E, 2008, REMOTE SENS ENVIRON, V112, P1982, DOI 10.1016/j.rse.2007.03.032.
   Tuominen S, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10050714.
   Underwood EC, 2006, ENVIRON MONIT ASSESS, V121, P47, DOI 10.1007/s10661-005-9106-4.
   Urban DL, 2000, LANDSCAPE ECOL, V15, P603, DOI 10.1023/A:1008183331604.
   Van Horn G, 2017, ARXIV170706642.
   Wettle M, 2004, REMOTE SENS ENVIRON, V93, P188, DOI 10.1016/j.rse.2004.07.014.
   WOODCOCK CE, 1987, REMOTE SENS ENVIRON, V21, P311, DOI 10.1016/0034-4257(87)90015-0.
   Wulder M, 1998, PROG PHYS GEOG, V22, P449, DOI 10.1177/030913339802200402.
   Yu F., 2015, 1511 ARXIV.
   Zhang CY, 2012, PHOTOGRAMM ENG REM S, V78, P1079, DOI 10.14358/PERS.78.10.1079.
   Zhang Q, 2018, IEEE T GEOSCI REMOTE, V56, P4274, DOI 10.1109/TGRS.2018.2810208.},
Number-of-Cited-References = {101},
Times-Cited = {86},
Usage-Count-Last-180-days = {21},
Usage-Count-Since-2013 = {111},
Journal-ISO = {Remote Sens.},
Doc-Delivery-Number = {JN3VH},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000496827100140},
OA = {gold, Green Published},
DA = {2023-08-12},
}

@article{ WOS:000403520500011,
Author = {de Mesquita Sa Junior, Jarbas Joaci and Cortez, Paulo Cesar and Kolb,
   Rosana Marta and Backes, Andre Ricardo},
Title = {Plant species identification using shortest paths in graphs applied to
   color images of palisade parenchyma},
Journal = {ECOLOGICAL INFORMATICS},
Year = {2017},
Volume = {39},
Pages = {119-122},
Month = {MAY},
Abstract = {Aiming to identify plants, we propose to evaluate the color texture of
   the palisade parenchyma, from microscopic images of leaf cross-sections,
   using a graph based approach. Our texture analysis approach models the
   image texture as a graph and uses measurements computed from the
   shortest paths between specific vertices to provide a feasible texture
   signature. For a more consistent evaluation, we compared our approach to
   different methods for color texture analysis in a texture classification
   experiment. The results obtained indicate that our approach is the most
   suitable for this histological analysis as it surpassed all the other
   texture approaches using Linear Discriminant Analysis, and obtained the
   second best accuracy using 1-Nearest Neighbor. These results also
   corroborate the feasibility of using both histological (as the palisade
   parenchyma) and computer analysis for identification and delimitation of
   plant taxa.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Backes, AR (Corresponding Author), Univ Fed Uberlandia, Fac Comp, Av Joao Naves de Avila 2121, BR-38408100 Uberlandia, MG, Brazil.
   de Mesquita Sa Junior, Jarbas Joaci, Univ Fed Ceara, Curso Engn Comp, Campus Sobral,Rua Estanislau Frota S-N, BR-62010560 Sobral, Ceara, Brazil.
   Cortez, Paulo Cesar, Univ Fed Ceara, Dept Engn Teleinformat, Campus Pici S-N,Bloco 725,Caixa Postal 6007, BR-60455970 Fortaleza, Ceara, Brazil.
   Kolb, Rosana Marta, Univ Estadual Paulista, Dept Ciencias Biol, Fac Ciencias \& Letras, Av Dom Antonio 2100, BR-19806900 Assis, SP, Brazil.
   Backes, Andre Ricardo, Univ Fed Uberlandia, Fac Comp, Av Joao Naves de Avila 2121, BR-38408100 Uberlandia, MG, Brazil.},
DOI = {10.1016/j.ecoinf.2017.04.006},
ISSN = {1574-9541},
EISSN = {1878-0512},
Keywords = {Plant identification; Color texture; Palisade parenchyma; Shortest
   paths; Graphs},
Keywords-Plus = {TEXTURE CLASSIFICATION; COMPUTER VISION},
Research-Areas = {Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Ecology},
Author-Email = {jarbas\_joaci@yahoo.com.br
   cortez@lesc.ufc.br
   rosanakolb@hotmail.com
   backes@ufu.br},
Affiliations = {Universidade Federal do Ceara; Universidade Federal do Ceara;
   Universidade Estadual Paulista; Universidade Federal de Uberlandia},
ResearcherID-Numbers = {Cortez, Paulo César P. C./E-5980-2015
   Kolb, Rosana M./D-3592-2012
   },
ORCID-Numbers = {Kolb, Rosana M./0000-0003-3841-5597
   Backes, Andre/0000-0002-7486-4253},
Funding-Acknowledgement = {CNPq (National Council for Scientific and Technological Development,
   Brazil) {[}302416/2015-3]; FAPEMIG (Foundation to the Support of
   Research in Minas Gerais) {[}APQ-03437-15]; PROPP-UFU},
Funding-Text = {Andre R. Backes gratefully acknowledges the financial support of CNPq
   (National Council for Scientific and Technological Development, Brazil)
   (Grant \#302416/2015-3), FAPEMIG (Foundation to the Support of Research
   in Minas Gerais) (Grant \#APQ-03437-15) and PROPP-UFU.},
Cited-References = {{[}Anonymous], 1990, STAT PATTERN RECOGNI.
   Backes AR, 2012, PATTERN RECOGN, V45, P1984, DOI 10.1016/j.patcog.2011.11.009.
   Bianconi F, 2011, J ELECTRON IMAGING, V20, DOI 10.1117/1.3651210.
   Bianconi F, 2009, PATTERN RECOGN LETT, V30, P765, DOI 10.1016/j.patrec.2009.02.006.
   Casanova D, 2009, INT J IMAG SYST TECH, V19, P236, DOI 10.1002/ima.20201.
   Sa JJD, 2014, IEEE T IMAGE PROCESS, V23, P3751, DOI 10.1109/TIP.2014.2333655.
   Sa JJD, 2013, ECOL INFORM, V15, P34, DOI 10.1016/j.ecoinf.2013.02.007.
   Dijkstra E.W., 1959, NUMERISCHE MATH, V1, P269, DOI DOI 10.1007/BF01386390.
   Drimbarean A, 2001, PATTERN RECOGN LETT, V22, P1161, DOI 10.1016/S0167-8655(01)00058-7.
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957.
   Hoang M. A., 2002, P WORKSH TEXT AN MAC, P73.
   Hoang MA, 2005, SIGNAL PROCESS, V85, P265, DOI 10.1016/j.sigpro.2004.10.009.
   Kaplan LM, 1999, IEEE T IMAGE PROCESS, V8, P1572, DOI 10.1109/83.799885.
   Liu GH, 2011, PATTERN RECOGN, V44, P2123, DOI 10.1016/j.patcog.2011.02.003.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Palm C, 2004, PATTERN RECOGN, V37, P965, DOI 10.1016/j.patcog.2003.09.010.
   Paschos G, 2003, PATTERN RECOGN LETT, V24, P309, DOI 10.1016/S0167-8655(02)00244-1.
   Plotze RD, 2005, CAN J BOT, V83, P287, DOI {[}10.1139/b05-002, 10.1139/B05-002].
   Porebski A., 2008, 2008 1 WORKSHOPS IMA, P1, DOI DOI 10.1109/IPTA.2008.4743780.
   Sa JJD, 2011, BOTANY, V89, P467, DOI {[}10.1139/B11-038, 10.1139/b11-038].},
Number-of-Cited-References = {20},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Journal-ISO = {Ecol. Inform.},
Doc-Delivery-Number = {EX8SM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000403520500011},
OA = {Green Published},
DA = {2023-08-12},
}

@inproceedings{ WOS:000252265104053,
Author = {Komi, Pauli J. and Jackson, Mike R. and Parkin, Rob M.},
Book-Group-Author = {IEEE},
Title = {Plant classification combining colour and spectral cameras for weed
   control purposes},
Booktitle = {2007 IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS,
   PROCEEDINGS, VOLS 1-8},
Year = {2007},
Pages = {2039-2042},
Note = {IEEE International Symposium on Industrial Electronics, Vigo, SPAIN, JUN
   04-07, 2007},
Organization = {IEEE},
Abstract = {Weed plant detection and classification is a difficult task for any
   computer vision system. Previous studies show promising results with
   either colour camera or spectral imaging solution S. However, typical
   colour camera solutions have found it hard to deal with overlapping
   leaves,and spectral solutions often lack in the spatial resolution
   required for accurate leaf level detection. In this paper a novel system
   for weed detection and classification is presented using both low-cost
   RGB (Red, Green, Blue) colour and spectral (400 - 1000 nm) cameras
   combining the strengths of these individual technologies. The system
   presented performs accurate leaf level classification and is capable of
   identification at 97.6\% with non-overlapping full leaves in laboratory
   under controlled lighting conditions. Plant leaf samples from 6
   different plant types were used. With dedicated hardware and optimized
   software the system should be capable of at least 5 km/h real-time
   operation in field conditions.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Komi, PJ (Corresponding Author), Univ Loughborough, Mechatron Res Ctr, Loughborough, Leics, England.
   Komi, Pauli J.; Jackson, Mike R.; Parkin, Rob M., Univ Loughborough, Mechatron Res Ctr, Loughborough, Leics, England.},
DOI = {10.1109/ISIE.2007.4374921},
ISBN = {978-1-4244-0754-5},
Research-Areas = {Automation \& Control Systems; Computer Science; Engineering; Robotics;
   Telecommunications},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Cybernetics; Computer
   Science, Theory \& Methods; Engineering, Electrical \& Electronic;
   Robotics; Telecommunications},
Affiliations = {Loughborough University},
Cited-References = {Astrand B, 2002, AUTON ROBOT, V13, P21, DOI 10.1023/A:1015674004201.
   Hemming J, 2001, J AGR ENG RES, V78, P233, DOI 10.1006/jaer.2000.0639.
   Pedersen SM, 2006, PRECIS AGRIC, V7, P295, DOI 10.1007/s11119-006-9014-9.
   Thenkabail PS, 2004, REMOTE SENS ENVIRON, V91, P354, DOI 10.1016/j.rse.2004.03.013.
   VANDEV D, 2004, P 10 INT SUMM C PROB, V16, P291.
   Vrindts E., 2002, Precision Agriculture, V3, P63, DOI 10.1023/A:1013326304427.},
Number-of-Cited-References = {6},
Times-Cited = {5},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {4},
Doc-Delivery-Number = {BHD17},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000252265104053},
OA = {Green Published},
DA = {2023-08-12},
}

@article{ WOS:000353350300009,
Author = {Chaki, Jyotismita and Parekh, Ranjan and Bhattacharya, Samar},
Title = {Plant leaf recognition using texture and shape features with neural
   classifiers},
Journal = {PATTERN RECOGNITION LETTERS},
Year = {2015},
Volume = {58},
Pages = {61-68},
Month = {JUN 1},
Abstract = {This paper proposes a novel methodology of characterizing and
   recognizing plant leaves using a combination of texture and shape
   features. Texture of the leaf is modeled using Gabor filter and gray
   level co-occurrence matrix (GLCM) while shape of the leaf is captured
   using a set of curvelet transform coefficients together with invariant
   moments. Since these features are in general sensitive to the
   orientation and scaling of the leaf image, a pre-processing stage prior
   to feature extraction is applied to make corrections for varying
   translation, rotation and scaling factors. Efficacy of the proposed
   methods is studied by using two neural classifiers: a neuro-fuzzy
   controller (NFC) and a feed-forward back-propagation multi-layered
   perceptron (MLP) to discriminate between 31 classes of leaves. The
   features have been applied individually as well as in combination to
   investigate how recognition accuracies can be improved. Experimental
   results demonstrate that the proposed approach is effective in
   recognizing leaves with varying texture, shape, size and orientations to
   an acceptable degree. (C) 2015 Elsevier B.V. All rights reserved.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Chaki, J (Corresponding Author), Jadavpur Univ, Sch Educ Technol, Kolkata, India.
   Chaki, Jyotismita; Parekh, Ranjan; Bhattacharya, Samar, Jadavpur Univ, Sch Educ Technol, Kolkata, India.},
DOI = {10.1016/j.patrec.2015.02.010},
ISSN = {0167-8655},
EISSN = {1872-7344},
Keywords = {Gabor filter; Gray level en occurrence matrix; Curvelet transform;
   Invariant moments; Neuro-fuzzy controller; Feed-forward
   back-propagation; multi-layered perception},
Keywords-Plus = {COLOR},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {jyotismita.c@gmail.com},
Affiliations = {Jadavpur University},
ResearcherID-Numbers = {Chaki, Jyotismita/T-4882-2019},
ORCID-Numbers = {Chaki, Jyotismita/0000-0003-1804-8590},
Cited-References = {Abbasi S, 1997, LECT NOTES COMPUT SC, V1252, P284.
   {[}Anonymous], INF CONTROL.
   ArunPriya C, 2012, P INT C PATT REC INF.
   Bama B. S., 2011, IND J COMP SCI ENG, V2, P202.
   Beghin T, 2010, LECT NOTES COMPUT SC, V6475, P345, DOI 10.1007/978-3-642-17691-3\_32.
   Caglayan A, 2013, LECT NOTES COMPUT SC, V8157, P161, DOI 10.1007/978-3-642-41184-7\_17.
   Candes E.J., 2000, CURVES SURFACE FITTI, P1.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Du JX, 2013, NEUROCOMPUTING, V116, P150, DOI 10.1016/j.neucom.2012.03.028.
   Du JX, 2011, LECT NOTES COMPUT SC, V6838, P364, DOI 10.1007/978-3-642-24728-6\_49.
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Jiazhi Pan, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P906, DOI 10.1109/CSSE.2008.918.
   Kadir Abdul, 2012, Journal of Theoretical and Applied Information Technology, V41, P82.
   Kadir A., 2011, INT J COMPUT APPL, V29, P15, DOI DOI 10.5120/3592-4981.
   Kebapci H, 2011, COMPUT J, V54, P1475, DOI 10.1093/comjnl/bxq037.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Perez AJ, 2000, COMPUT ELECTRON AGR, V25, P197, DOI 10.1016/S0168-1699(99)00068-X.
   Prasad S., 2011, 2011 2nd International Conference on Computer and Communication Technology, P646, DOI 10.1109/ICCCT.2011.6075212.
   Sakai N, 1996, J FOOD ENG, V27, P397, DOI 10.1016/0260-8774(95)00022-4.
   Valliammal N., 2012, J ADV RES ARTIF INTE, V1, P37.
   Wang QP, 2010, LECT NOTES ARTIF INT, V6216, P240, DOI 10.1007/978-3-642-14932-0\_30.
   Wang X, 2014, DIGIT SIGNAL PROCESS, V34, P101, DOI 10.1016/j.dsp.2014.08.005.
   Wang Z., 2000, INT C ADV VIS INF SY, P477.
   Wang ZY, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOL 1 \& 2, P372, DOI 10.1109/FUZZ.2002.1005019.
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120.
   Yang L.-W., 2012, LNCS, V7389, P393, DOI {[}10.1007/978-3-642-31588-6\_51, DOI 10.1007/978-3-642-31588-6\_51].},
Number-of-Cited-References = {27},
Times-Cited = {125},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {57},
Journal-ISO = {Pattern Recognit. Lett.},
Doc-Delivery-Number = {CG5RA},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000353350300009},
DA = {2023-08-12},
}

@article{ WOS:000798015600011,
Author = {Bonnet, Pierre and Joly, Alexis and Faton, Jean-Michel and Brown, Susan
   and Kimiti, David and Deneu, Benjamin and Servajean, Maximilien and
   Affouard, Antoine and Lombardo, Jean-Christophe and Mary, Laura and
   Vignau, Christel and Munoz, Francois},
Title = {How citizen scientists contribute to monitor protected areas thanks to
   automatic plant identification tools},
Journal = {ECOLOGICAL SOLUTIONS AND EVIDENCE},
Year = {2020},
Volume = {1},
Number = {2},
Month = {DEC},
Abstract = {1. Successful monitoring and management of plant resources worldwide
   needs the involvement of civil society to support natural reserve
   managers. Because it is difficult to correctly and quickly identify
   plant species for non-specialists, the development of recent techniques
   based on automatic visual identification should facilitate and increase
   public engagement in citizen science initiatives. 2. Automatic
   identification platforms are new to most citizen scientists and land
   managers. Pl@ntNet is such a platform, available since 2013 on web and
   mobile environments, and now included in several workflows such as
   invasive alien species management, endemic species monitoring,
   educational activities and eco-tourism practices. The successful
   development of such platforms needs to identify their strengths and
   weaknesses in order to improve and facilitate their use in all aspects
   of ecosystem management. 3. Here we present two Pl@ntNet citizen science
   initiatives used by conservation practitioners in Europe (France) and
   Africa (Kenya). We discuss various perspectives, including benefits and
   limitations. Based on the experiences of field managers, we formulate
   several recommendations for future initiatives. The recommendations are
   aimed at a diverse group of conservation managers and citizen science
   practitioners.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Bonnet, P (Corresponding Author), CIRAD, UMR AMAP, F-34398 Montpellier, Occitanie, France.
   Bonnet, Pierre, CIRAD, UMR AMAP, F-34398 Montpellier, Occitanie, France.
   Bonnet, Pierre; Deneu, Benjamin; Affouard, Antoine, Univ Montpellier, CNRS, AMAP, CIRAD,INRAE,IRD, Montpellier, Occitanie, France.
   Joly, Alexis; Deneu, Benjamin; Affouard, Antoine; Lombardo, Jean-Christophe, ZENITH Team, INRIA Sophia Antipolis, UMR 5506, LIRMM,CC 477, Montpellier, Occitanie, France.
   Faton, Jean-Michel, Reserve Nat Natl Ramieres Val Drome, Allex, France.
   Brown, Susan, Lewa House Pimbi Ltd, Isiolo, Kenya.
   Kimiti, David, Lewa Wildlife Conservancy, Isiolo, Kenya.
   Servajean, Maximilien, Univ Montpellier, CNRS, UMR 5506, LIRMM, Montpellier, Occitanie, France.
   Servajean, Maximilien, Paule Valery Univ Montpellier 3, AMIS, Montpellier, Occitanie, France.
   Mary, Laura; Vignau, Christel, Tela Bot, Montpellier, Occitanie, France.
   Munoz, Francois, Univ Grenoble Alpes, Lab Ecol Alpine, Grenoble, France.},
DOI = {10.1002/2688-8319.12023},
Article-Number = {e12023},
EISSN = {2688-8319},
Keywords = {artificial intelligence; automatic plant identification; citizen
   science; deep learning technologies; opportunistic data; plant
   biodiversity monitoring; public engagement; volunteers; intelligence
   artificielle; identification automatique des plantes; science citoyenne;
   technologies d'apprentissage profond; donnees opportunistes;
   surveillance de la biodiversite vegetale; engagement du public;
   volontaires},
Keywords-Plus = {SCIENCE; RELIABILITY},
Research-Areas = {Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Ecology},
Author-Email = {pierre.bonnet@cirad.fr},
Affiliations = {CIRAD; Centre National de la Recherche Scientifique (CNRS); Institut de
   Recherche pour le Developpement (IRD); Universite de Montpellier; Centre
   National de la Recherche Scientifique (CNRS); CIRAD; INRAE; Institut de
   Recherche pour le Developpement (IRD); Universite de Montpellier; Centre
   National de la Recherche Scientifique (CNRS); Universite Paul-Valery;
   Universite Perpignan Via Domitia; Universite de Montpellier; Centre
   National de la Recherche Scientifique (CNRS); Universite Paul-Valery;
   Universite Perpignan Via Domitia; Universite de Montpellier; CIRAD;
   UDICE-French Research Universities; Communaute Universite Grenoble
   Alpes; Universite Grenoble Alpes (UGA); Centre National de la Recherche
   Scientifique (CNRS); Universite de Savoie},
ResearcherID-Numbers = {Bonnet, Pierre/AAG-6819-2020
   Servajean, Maximilien/IQW-9683-2023
   },
ORCID-Numbers = {Bonnet, Pierre/0000-0002-2828-4389
   Servajean, Maximilien/0000-0002-9426-2583
   Deneu, Benjamin/0000-0003-0640-5706},
Funding-Acknowledgement = {European Union's Horizon 2020 Research and Innovation Programme
   {[}863463]; Agropolis Fondation},
Funding-Text = {European Union's Horizon 2020 Research and Innovation Programme,
   Grant/Award Number: No. 863463 (Cos4Cloud project); Agropolis Fondation,
   Grant/Award Number: Pl@ntNet flagship program; \#DigitAG, Grant/Award
   Number: PhDgrant of Benjamin Deneu},
Cited-References = {Affouard A., 2017, P ICLR INT C LEARN R.
   Anhalt-Depies C, 2019, BIOL CONSERV, V238, DOI 10.1016/j.biocon.2019.108195.
   Botella C, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0232078.
   Botella C, 2018, APPL PLANT SCI, V6, DOI 10.1002/aps3.1029.
   Ceccaroni L., 2019, CITIZ SCI THEORY PRA, V4, P29, DOI DOI 10.5334/CSTP.241.
   Chandler M, 2017, BIOL CONSERV, V213, P280, DOI 10.1016/j.biocon.2016.09.004.
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256.
   Constant N, 2017, JCOM-J SCI COMMUN, V16, DOI 10.22323/2.16040203.
   Goeau H., 2018, CEUR WORKSHOP P, P1.
   Goeau H., 2014, P INT C MULT RETR, P527.
   Goeau H., 2011, CEUR WORKSHOP P.
   Goeau H., 2014, CLEF2014 WORKING NOT, P598.
   Goeau H., 2019, CLEF 2019 C LABS EVA.
   Goeau H., 2015, CEUR WORKSHOP P WORK.
   Goeau H., 2017, CLEF 2017 C LABS EVA, P1.
   GOEAU H., 2013, ACM INT C MULT BARC, V21, P423, DOI DOI 10.1145/2502081.2502251.
   Goeau H., 2013, CEUR WORKSHOP P.
   Goeau H., 2016, WORKING NOTES CLEF 2, P428.
   Goeau H., 2012, CEUR WORKSHOP P.
   Groom Q, 2017, J APPL ECOL, V54, P612, DOI 10.1111/1365-2664.12767.
   Jennett C, 2016, JCOM-J SCI COMMUN, V15, DOI 10.22323/2.15030205.
   Johnson BA, 2020, GLOB ECOL CONSERV, V21, DOI 10.1016/j.gecco.2019.e00812.
   Johnston Alison, 2018, Methods in Ecology and Evolution, V9, P88.
   Joly A, 2019, LECT NOTES COMPUT SC, V11696, P387, DOI 10.1007/978-3-030-28577-7\_29.
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436.
   Loos J, 2015, J NAT CONSERV, V26, P45, DOI 10.1016/j.jnc.2015.05.001.
   Newman G, 2011, ECOL INFORM, V6, P217, DOI 10.1016/j.ecoinf.2011.03.002.
   Pocock MJO, 2018, ADV ECOL RES, V59, P169, DOI 10.1016/bs.aecr.2018.06.003.
   Preece J, 2016, INT J HUM-COMPUT INT, V32, P585, DOI 10.1080/10447318.2016.1194153.
   Ratnieks FLW, 2016, METHODS ECOL EVOL, V7, P1226, DOI 10.1111/2041-210X.12581.
   Sharma N, 2019, PEERJ, V6, DOI 10.7717/peerj.5965.
   Steger C, 2017, J APPL ECOL, V54, P2053, DOI 10.1111/1365-2664.12921.
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075.},
Number-of-Cited-References = {33},
Times-Cited = {15},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {2},
Journal-ISO = {Ecol. Solut. Evid.},
Doc-Delivery-Number = {VL1SC},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000798015600011},
OA = {gold, Green Published},
DA = {2023-08-12},
}

@article{ WOS:000718325900005,
Author = {Zheng, Yuhong and Li, Xiaolong and Han, Fugui and Fu, Li and Sun, Jinbo},
Title = {Identification of foliage plants Heuchera based on electrochemical
   profile of active molecules},
Journal = {INTERNATIONAL JOURNAL OF ELECTROCHEMICAL SCIENCE},
Year = {2021},
Volume = {16},
Number = {11},
Month = {NOV},
Abstract = {An electrochemical profile of plant tissue can reflect the genetic
   differences among different plants. In this work, the electrochemical
   profiles were used to identify 12 Heuchera cultivars. Leaf extracts of
   Heuchera `Rio', Heuchera `Tapestry', Heuchera `Caramel', Heuchera
   `Paris', Heuchera Heuchera `Cherry Cola', Heuchera Tiramisu', Heuchera
   `Obsidian', Heuchera Midnight Ruffles', Heuchera `Electra', Heuchera
   `Georgia Peach' and Heuchera `Red Wine' were prepared for recording the
   electrochemical profiles. These electrochemical profiles were used to
   construct different pattern recognition strategies and to identify
   cultivars. The identification of patterns was more accurate than that of
   individual electrochemical profiles.},
Publisher = {ESG},
Address = {BORIVOJA STEVANOVICA 25-7, BELGRADE, 11000, SERBIA},
Type = {Article},
Language = {English},
Affiliation = {Zheng, YH (Corresponding Author), Jiangsu Prov \& Chinese Acad Sci, Inst Bot, Nanjing Bot Garden Mem Sun Yat Sen, Nanjing 210014, Peoples R China.
   Fu, L (Corresponding Author), Hangzhou Dianzi Univ, Coll Mat \& Environm Engn, Key Lab Novel Mat Sensor Zhejiang Prov, Hangzhou 310018, Peoples R China.
   Zheng, Yuhong; Han, Fugui, Jiangsu Prov \& Chinese Acad Sci, Inst Bot, Nanjing Bot Garden Mem Sun Yat Sen, Nanjing 210014, Peoples R China.
   Li, Xiaolong; Fu, Li, Hangzhou Dianzi Univ, Coll Mat \& Environm Engn, Key Lab Novel Mat Sensor Zhejiang Prov, Hangzhou 310018, Peoples R China.
   Sun, Jinbo, Shuyang Jindi Landscaping Engn Co Ltd, Shuyang 223600, Peoples R China.},
DOI = {10.20964/2021.11.44},
Article-Number = {211136},
ISSN = {1452-3981},
Keywords = {Electroanalysis; Leaf extract; Plant identification; Heuchera;
   Biometrics},
Research-Areas = {Electrochemistry},
Web-of-Science-Categories  = {Electrochemistry},
Author-Email = {zhengyuhong@cnbg.net
   fuli@hdu.edu.cn},
Affiliations = {Chinese Academy of Sciences; Hangzhou Dianzi University},
ResearcherID-Numbers = {Zheng, Yu/GRJ-5808-2022
   Li, xiaolong/GRS-9148-2022
   li, xiao/HKV-8405-2023
   li, xiao/HJP-5134-2023
   li, xiao/GSN-6181-2022
   Fu, Li/AAH-4689-2020},
ORCID-Numbers = {Fu, Li/0000-0002-5957-7790},
Funding-Acknowledgement = {Special Grant for Science and Technology in North Jiangsu
   {[}SZ-SQ2019007]},
Funding-Text = {This work has been financially supported by Special Grant for Science
   and Technology in North Jiangsu (SZ-SQ2019007).},
Cited-References = {{[}Anonymous], 2017, PLANT VAR STUD PROT, V13, P55.
   Brilli F, 2018, TRENDS PLANT SCI, V23, P507, DOI 10.1016/j.tplants.2018.03.004.
   Czuchaj P., 2017, Nauka Przyroda Technologie, V11, P325, DOI 10.17306/j.npt.00221.
   De Keyser E, 2019, SCI HORTIC-AMSTERDAM, V253, P270, DOI 10.1016/j.scienta.2019.04.006.
   Duan RS, 2021, FRONT CHEM, V9, DOI 10.3389/fchem.2021.689735.
   Elmer W, 2020, J PHYTOPATHOL, V168, P56, DOI 10.1111/jph.12867.
   Fu L, 2021, BIOELECTROCHEMISTRY, V140, DOI 10.1016/j.bioelechem.2021.107829.
   Fu L, 2020, BIOSENS BIOELECTRON, V159, DOI 10.1016/j.bios.2020.112212.
   Fu L, 2019, BIOELECTROCHEMISTRY, V129, P199, DOI 10.1016/j.bioelechem.2019.06.001.
   Fu L, 2019, J ELECTROANAL CHEM, V841, P142, DOI 10.1016/j.jelechem.2019.04.046.
   Fu L, 2018, BIOSENS BIOELECTRON, V120, P102, DOI 10.1016/j.bios.2018.08.052.
   Fu L, 2018, TALANTA, V180, P248, DOI 10.1016/j.talanta.2017.12.058.
   Garcia LF, 2016, PREP BIOCHEM BIOTECH, V46, P850, DOI 10.1080/10826068.2016.1155060.
   Hariram M, 2018, ARCH ENVIRON CON TOX, V74, P56, DOI 10.1007/s00244-017-0446-1.
   Hassan A, 2017, BIOMED ENVIRON SCI, V30, P846, DOI 10.3967/bes2017.114.
   Hovhannisyan V, 2017, AGRIBUSINESS, V33, P226, DOI 10.1002/agr.21488.
   Karimi-Maleh H, 2021, BIOSENS BIOELECTRON, V184, DOI 10.1016/j.bios.2021.113252.
   Karimi-Maleh H, 2021, J CLEAN PROD, V291, DOI 10.1016/j.jclepro.2021.125880.
   Karimi-Maleh H, 2021, IND ENG CHEM RES, V60, P816, DOI 10.1021/acs.iecr.0c04698.
   Karimi-Maleh H, 2020, J MOL LIQ, V310, DOI 10.1016/j.molliq.2020.113185.
   Kim J, 2017, ADV HEALTHC MATER, V6, DOI 10.1002/adhm.201700770.
   Li J, 2021, FRONT CHEM, V9, DOI 10.3389/fchem.2021.680593.
   Lubell-Brand JD, 2020, HORTSCIENCE, V55, P2045, DOI 10.21273/HORTSCI15410-20.
   Mitra S, 2019, MAT SCI ENG C-MATER, V103, DOI 10.1016/j.msec.2019.109802.
   Oh YA, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16050796.
   Punja ZK, 2018, CAN J PLANT PATHOL, V40, P514, DOI 10.1080/07060661.2018.1535467.
   Rolland-Lagan AG, 2009, PLANT J, V57, P195, DOI 10.1111/j.1365-313X.2008.03678.x.
   Suhaimi S, 2015, INT J ELECTROCHEM SC, V10, P2859.
   Wang QiaoLiang, 2019, Journal of Henan Agricultural Sciences, V48, P137.
   Wang YW, 2021, FRONT CHEM, V9, DOI 10.3389/fchem.2021.721008.
   Wu WH, 2020, INT J ELECTROCHEM SC, V15, P10093, DOI 10.20964/2020.10.69.
   Wu WH, 2020, SENSOR MATER, V32, P2941, DOI 10.18494/SAM.2020.2972.
   Wu Z, 2021, FRONT CHEM, V9, DOI 10.3389/fchem.2021.670074.
   Xie YingZan, 2018, Southwest China Journal of Agricultural Sciences, V31, P2104.
   Xu YT, 2020, BIOELECTROCHEMISTRY, V133, DOI 10.1016/j.bioelechem.2020.107455.
   Xu ZL, 2021, FRONT CHEM, V9, DOI 10.3389/fchem.2021.733371.
   Yue YZ, 2021, FRONT CHEM, V9, DOI 10.3389/fchem.2021.709487.
   Zhang MJ, 2020, CHEMISTRYSELECT, V5, P5035, DOI 10.1002/slct.202001100.
   Zhang X, 2020, REV MEX ING QUIM, V19, P281, DOI 10.24275/rmiq.Bio1750.
   Zhao HQ, 2017, HORTSCIENCE, V52, P622, DOI {[}10.21273/HORTSCI11340-16, 10.21273/hortsci11340-16].
   Zhou JT, 2020, ANAL LETT, V53, P2517, DOI 10.1080/00032719.2020.1746327.
   Zhu Ya, 2018, Acta Agriculturae Jiangxi, V30, P67.},
Number-of-Cited-References = {42},
Times-Cited = {12},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {8},
Journal-ISO = {Int. J. Electrochem. Sci.},
Doc-Delivery-Number = {WX0WQ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000718325900005},
OA = {hybrid},
DA = {2023-08-12},
}

@article{ WOS:000285877300009,
Author = {Rossatto, Davi Rodrigo and Casanova, Dalcimar and Kolb, Rosana Marta and
   Bruno, Odemir Martinez},
Title = {Fractal analysis of leaf-texture properties as a tool for taxonomic and
   identification purposes: a case study with species from Neotropical
   Melastomataceae (Miconieae tribe)},
Journal = {PLANT SYSTEMATICS AND EVOLUTION},
Year = {2011},
Volume = {291},
Number = {1-2},
Pages = {103-116},
Month = {JAN},
Abstract = {Melastomataceae is a common and dominant family in Neotropical
   vegetation, with high species diversity which leads to a large variation
   in some morphological structures. Despite this, some species of
   Melastomataceae are very similar in their external leaf morphology,
   leading to difficulties in their identification without the presence of
   reproductive organs. Here we have proposed and tested a computer-aided
   texture-based approach used to correctly identify and distinguish leaves
   of some species of Melastomataceae that occur in a region of Neotropical
   savanna in Southeastern Brazil, also comparing it with other previously
   proposed approaches. The results demonstrated that our approach may
   clearly separate the studied species, analyzing the patterns of leaf
   texture (both adaxial and abaxial surfaces), and achieving better
   accuracy (100\%) than other methods. Our work has suggested that leaf
   texture properties can be used as a new characteristic for
   identification, and as an additional source of information in taxonomic
   and systematic studies. As the method may be supervised by experts, it
   is also suitable for discrimination of species with high morphological
   plasticity, improving the automated discrimination task. This approach
   can be very useful for identification of species in the absence of
   reproductive material, and is a rapid and powerful tool for plant
   identification.},
Publisher = {SPRINGER WIEN},
Address = {SACHSENPLATZ 4-6, PO BOX 89, A-1201 WIEN, AUSTRIA},
Type = {Article},
Language = {English},
Affiliation = {Bruno, OM (Corresponding Author), Univ Sao Paulo, Inst Fis Sao Carlos, Av Trabalhador Sao Carlense 400, BR-13560970 Sao Carlos, SP, Brazil.
   Casanova, Dalcimar; Bruno, Odemir Martinez, Univ Sao Paulo, Inst Fis Sao Carlos, BR-13560970 Sao Carlos, SP, Brazil.
   Rossatto, Davi Rodrigo, Univ Brasilia, Lab Fisiol Vegetal, Dept Bot, BR-70904970 Brasilia, DF, Brazil.
   Kolb, Rosana Marta, UNESP, Fac Ciencias \& Letras, Dept Ciencias Biol, BR-19806900 Assis, SP, Brazil.},
DOI = {10.1007/s00606-010-0366-2},
ISSN = {0378-2697},
EISSN = {2199-6881},
Keywords = {Leaf texture; Fractal analysis; Volumetric fractal; Melastomataceae;
   Plant identification; Computer vision},
Keywords-Plus = {RETRIEVAL SCHEME; SHAPE; CLASSIFICATION; DISCRIMINATION; PLASTICITY;
   PHYLOGENY; IMAGES},
Research-Areas = {Plant Sciences; Evolutionary Biology},
Web-of-Science-Categories  = {Plant Sciences; Evolutionary Biology},
Author-Email = {drrossatto@gmail.com
   dalcimar@gmail.com
   rosanakolb@hotmail.com
   bruno@ifsc.usp.br},
Affiliations = {Universidade de Sao Paulo; Universidade de Brasilia; Universidade
   Estadual Paulista},
ResearcherID-Numbers = {Rossatto, Davi Rodrigo/A-3521-2011
   Bruno, Odemir/AAU-7209-2020
   Kolb, Rosana M./D-3592-2012
   Bruno, Odemir M/A-5279-2009
   Casanova, Dalcimar/J-1588-2012
   },
ORCID-Numbers = {Rossatto, Davi Rodrigo/0000-0001-9510-8345
   Bruno, Odemir/0000-0002-2945-1556
   Kolb, Rosana M./0000-0003-3841-5597
   Bruno, Odemir M/0000-0002-2945-1556
   Casanova, Dalcimar/0000-0002-1905-4602},
Funding-Acknowledgement = {CNPq (National Council for Scientific and Technological Development,
   Brazil) {[}306628/2007-4, 484474/2007-3]; FAPESP (Sao Paulo Research
   Foundation, Brazil)},
Funding-Text = {The authors acknowledge Assis Ecological Station and Instituto Florestal
   for permission to collect the leaves of the studied species, and Dr
   Renato Goldenberg for helping with plant identification. Odemir M. Bruno
   gratefully acknowledges the financial support of CNPq (National Council
   for Scientific and Technological Development, Brazil) (Grant
   \#306628/2007-4 and \#484474/2007-3). Dalcimar Casanova acknowledges
   support from FAPESP (Sao Paulo Research Foundation, Brazil)
   (2008/57313-2) for his PhD grant.},
Cited-References = {Abbasi S, 1997, LECT NOTES COMPUT SC, V1252, P284.
   {[}Anonymous], 2000, INTEGRATIVE PLANT AN.
   {[}Anonymous], 1987, LEAF VENATION PATTER.
   {[}Anonymous], 1998, HERBARIUM HDB.
   ASH A., 1999, MANUAL LEAF ARCHITEC.
   Backes AR, 2009, PATTERN RECOGN, V42, P54, DOI 10.1016/j.patcog.2008.07.006.
   Backes AR, 2009, INT J PATTERN RECOGN, V23, P1145, DOI 10.1142/S0218001409007508.
   BAILEY I. W., 1951, PHYTOMORPHOLOGY {[}DELHI], V1, P67.
   Beaulieu JM, 2008, NEW PHYTOL, V179, P975, DOI 10.1111/j.1469-8137.2008.02528.x.
   Blanco Mario A., 2006, Selbyana, V27, P83.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Camargo EA, 2008, THESIS PARANA FEDERA.
   Castro-Esau KL, 2004, REMOTE SENS ENVIRON, V90, P353, DOI 10.1016/j.rse.2004.01.013.
   DALLWITZ MJ, 1974, SYST ZOOL, V23, P50, DOI 10.2307/2412239.
   Dean M, 2008, PLANT SYST EVOL, V273, P97, DOI 10.1007/s00606-008-0029-8.
   DeWolf G., 1968, ARNOLDIA, V28, P69.
   DURIGAN G., 2004, PLANTAS CERRADO PAUL.
   Endress PK, 2002, BOT REV, V68, P545, DOI 10.1663/0006-8101(2002)068{[}0545:MAASIT]2.0.CO;2.
   FERSON S, 1985, SYST ZOOL, V34, P59, DOI 10.2307/2413345.
   Goldenberg Renato, 2004, Acta Bot. Bras., V18, P927, DOI 10.1590/S0102-33062004000400024.
   Goldenberg Renato, 2008, Harvard Papers in Botany, V13, P223, DOI 10.3100/1043-4534-13.2.223.
   Goldenberg R, 2008, INT J PLANT SCI, V169, P963, DOI 10.1086/589697.
   Gratani L, 2006, TREES-STRUCT FUNCT, V20, P549, DOI 10.1007/s00468-006-0070-6.
   Hetherington AM, 2003, NATURE, V424, P901, DOI 10.1038/nature01843.
   Hlwatika CNM, 2002, ANN BOT-LONDON, V89, P109, DOI 10.1093/aob/mcf011.
   Holgren PK, 1992, PLANT SPECIALISTS IN.
   Judd W.S., 2008, PLANT SYSTEMATICS PH, V3rd.
   Kaplan LM, 1999, IEEE T IMAGE PROCESS, V8, P1572, DOI 10.1109/83.799885.
   Lee CL, 2006, INT J IMAG SYST TECH, V16, P15, DOI 10.1002/ima.20063.
   Leenhouts P.W., 1968, GUIDE PRACTICE HERBA.
   Mancuso S, 2002, VITIS, V41, P137.
   Martin CV, 2008, CLADISTICS, V24, P315, DOI 10.1111/j.1096-0031.2007.00185.x.
   McLachlan G.J., 1992, DISCRIMINANT ANAL ST, DOI DOI 10.1002/0471725293.
   Mendonca R.C., 2008, CHECKLIST, P421.
   Miao ZJ, 2000, PATTERN RECOGN LETT, V21, P169, DOI 10.1016/S0167-8655(99)00144-0.
   Michelangeli FA, 2004, TAXON, V53, P279, DOI 10.2307/4135608.
   Mitchell T.M., 1997, MACH LEARN, V1.
   Mugnai S, 2008, PLANT SYST EVOL, V270, P95, DOI 10.1007/s00606-007-0601-7.
   Nam Y, 2008, COMPUT VIS IMAGE UND, V110, P245, DOI 10.1016/j.cviu.2007.08.002.
   Nam YY, 2005, LECT NOTES COMPUT SC, V3767, P876.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Pankhurst RJ, 1978, BIOL IDENTIFICATION.
   Pearce DW, 2006, TREE PHYSIOL, V26, P211, DOI 10.1093/treephys/26.2.211.
   Persson HA, 2001, MOL ECOL, V10, P1385, DOI 10.1046/j.1365-294X.2001.01280.x.
   Ramos E, 2009, ECOL INFORM, V4, P177, DOI 10.1016/j.ecoinf.2009.06.003.
   Ramos VS, 2008, ARVORES FLORESTA EST.
   Rossatto DR, 2009, AUST J BOT, V57, P439, DOI 10.1071/BT09045.
   SALATINO A, 1986, Revista Brasileira de Botanica, V9, P117.
   Slaughter DC, 2008, COMPUT ELECTRON AGR, V61, P63, DOI 10.1016/j.compag.2007.05.008.
   Souza M. L. D. R., 2009, FLORA FANEROGAMICA E, V6, P32.
   Wang XF, 2005, LECT NOTES COMPUT SC, V3644, P87.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wang Z, 2003, IEE P-VIS IMAGE SIGN, V150, P34, DOI 10.1049/ip-vis:20030160.},
Number-of-Cited-References = {53},
Times-Cited = {31},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {17},
Journal-ISO = {Plant Syst. Evol.},
Doc-Delivery-Number = {702EL},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000285877300009},
DA = {2023-08-12},
}

@inproceedings{ WOS:000428694800030,
Author = {Akila, I. S. and Sivakumar, A. and Swaminathan, S.},
Book-Group-Author = {IEEE},
Title = {Automation in Plant Growth Monitoring using High-Precision Image
   Classification and Virtual Height Measurement Techniques},
Booktitle = {2017 INTERNATIONAL CONFERENCE ON INNOVATIONS IN INFORMATION, EMBEDDED
   AND COMMUNICATION SYSTEMS (ICIIECS)},
Year = {2017},
Note = {International Conference on Innovations in Information, Embedded and
   Communication Systems (ICIIECS), Karpagam Coll Engn, Coimbatore, INDIA,
   MAR 17-18, 2017},
Organization = {IEEE Madras Sect; IEEE Computat Intelligence Soc},
Abstract = {Agriculture is the backbone of our Indian economy and almost 80
   percentage of our population depend on it, but it is becoming a
   strenuous practice due to irregular weather pattern and over usage of
   underground water. So there is a need for automatic monitoring and
   advisory system for increase in productivity. Image processing is a
   powerful technique that has now been widely used in various fields for
   solving complex problems and its application in the field of agriculture
   has been proved a useful contribution. Thus the proposed work aims at
   extracting plant color or texture using image processing technique and
   the resultant image is subjected to virtual height measurement scheme.
   Then plant height is verified for normal growth in comparison with the
   plant growth chart. If plant growth is not well, an advisory system may
   be created which can alert the user.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Akila, IS (Corresponding Author), Coimbatore Inst Technol, Dept Elect \& Commun Engn, Coimbatore, Tamil Nadu, India.
   Akila, I. S.; Sivakumar, A.; Swaminathan, S., Coimbatore Inst Technol, Dept Elect \& Commun Engn, Coimbatore, Tamil Nadu, India.},
ISBN = {978-1-5090-3294-5},
Keywords = {Conditional Random Field Temporal Search (CRFT); color extraction; plant
   identification; virtual height measurement; agriculture},
Research-Areas = {Engineering; Telecommunications},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic; Telecommunications},
Affiliations = {Coimbatore Institute of Technology},
ResearcherID-Numbers = {S, Akila I/AAV-5028-2020},
ORCID-Numbers = {S, Akila I/0000-0001-5240-4670},
Cited-References = {Georgescu Florin-Andrei, 2016, IEEE T GEOSCIENCE RE, V13.
   Gupta S., 2012, INT J ENG RES TECHNO, V1.
   Hernandez-Hernandez JL, 2016, COMPUT ELECTRON AGR, V122, P124, DOI 10.1016/j.compag.2016.01.020.
   Lee Joong, 2007, ELSEVIER MAGAZIN DEC, P17.
   Sarangi S, 2016, COMPUT ELECTRON AGR, V122, P200, DOI 10.1016/j.compag.2016.01.009.
   Shih HC, 2016, IEEE T IMAGE PROCESS, V25, P4665, DOI 10.1109/TIP.2016.2586658.
   Zhao J, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2577886.},
Number-of-Cited-References = {7},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {2},
Doc-Delivery-Number = {BJ8QY},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000428694800030},
DA = {2023-08-12},
}

@article{ WOS:000563929200047,
Author = {Ahmed, Ali and Hussein, Sherif E.},
Title = {Leaf identification using radial basis function neural networks and SSA
   based support vector machine},
Journal = {PLOS ONE},
Year = {2020},
Volume = {15},
Number = {8},
Month = {AUG 19},
Abstract = {In this research, an efficient scheme to identify leaf types is
   proposed. In that scheme, the leaf boundary points are fitted in a
   continuous contour using Radial Basis Function Neural Networks (RBFNN)
   to calculate the centroid of the leaf shape. Afterwards, the distances
   between predetermined points and the centroid were computed and
   normalized. In addition, the time complexity of the features' extraction
   algorithm was calculated. The merit of this scheme is objects'
   independence to translation, rotation and scaling. Moreover, different
   classification techniques were evaluated against the leaf shape
   features. Those techniques included two of the most commonly used
   classification methods; RBFNN and SVM that were evaluated and compared
   with other researches that used complex features extraction algorithms
   with much higher dimensionality. Furthermore, a third classification
   method with an optimization technique for the SVM using Salp Swarm
   Algorithm (SSA) was utilized showing a significant improvement over
   RBFNN and SVM.},
Publisher = {PUBLIC LIBRARY SCIENCE},
Address = {1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA},
Type = {Article},
Language = {English},
Affiliation = {Hussein, SE (Corresponding Author), Mansoura Univ, Fac Engn, Comp Engn \& Syst Dept, Mansoura, Egypt.
   Ahmed, Ali, Menoufia Univ, Fac Comp \& Informat, IT Dept, Menoufia, Egypt.
   Hussein, Sherif E., Mansoura Univ, Fac Engn, Comp Engn \& Syst Dept, Mansoura, Egypt.},
DOI = {10.1371/journal.pone.0237645},
Article-Number = {e0237645},
ISSN = {1932-6203},
Keywords-Plus = {SHAPE-FEATURES; PLANT-IDENTIFICATION; RECOGNITION; CLASSIFICATION;
   ALGORITHM; IMAGES},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {sherif\_hussein@mans.edu.eg},
Affiliations = {Egyptian Knowledge Bank (EKB); Menofia University; Egyptian Knowledge
   Bank (EKB); Mansoura University},
ResearcherID-Numbers = {Ahmed, Ali/AFO-2834-2022
   ELMITWALLI, SHERIF ELSAYED HUSSEIN/I-7955-2016},
ORCID-Numbers = {ELMITWALLI, SHERIF ELSAYED HUSSEIN/0000-0002-5394-4385},
Cited-References = {Abbassi R, 2019, ENERG CONVERS MANAGE, V179, P362, DOI 10.1016/j.enconman.2018.10.069.
   Al-Zoubi AM, 2018, KNOWL-BASED SYST, V153, P91, DOI 10.1016/j.knosys.2018.04.025.
   Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010.
   CHANG CC, 1991, PATTERN RECOGN, V24, P1053, DOI 10.1016/0031-3203(91)90121-K.
   Deng W, 2019, SOFT COMPUT, V23, P2445, DOI 10.1007/s00500-017-2940-9.
   Dua D., 2019, UCI MACHINE LEARNING.
   Fachri Yanuar Rudi F., 2019, MAT SCI ENG, V536, P012150.
   Fotopoulou F, 2013, PATTERN ANAL APPL, V16, P381, DOI 10.1007/s10044-011-0254-6.
   Frank A, 2010, UCI MACHINE LEARNING.
   Gao LY, 2017, GENOM PROTEOM BIOINF, V15, P389, DOI 10.1016/j.gpb.2017.08.002.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Ibrahim A, 2018, ADV INTELL SYST COMP, V723, P42, DOI 10.1007/978-3-319-74690-6\_5.
   Jin TS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0139482.
   Kalyoncu C, 2015, COMPUT VIS IMAGE UND, V133, P102, DOI 10.1016/j.cviu.2014.11.001.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   Khmag A, 2017, IEEE ST CONF RES DEV, P467, DOI 10.1109/SCORED.2017.8305438.
   Larese MG, 2014, PATTERN RECOGN, V47, P158, DOI 10.1016/j.patcog.2013.06.012.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Miller S, 2019, ECON LETT, V175, P28, DOI 10.1016/j.econlet.2018.12.001.
   Mirjalili S, 2019, STUD COMPUT INTELL, V780, P105, DOI 10.1007/978-3-319-93025-1\_8.
   Mirjalili S, 2017, ADV ENG SOFTW, V114, P163, DOI 10.1016/j.advengsoft.2017.07.002.
   Moawad NM, 2019, ISA T, V87, P200, DOI 10.1016/j.isatra.2018.11.021.
   Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095.
   Olsen A, 2015, 2015 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P441.
   Sokic E, 2016, SIGNAL PROCESS-IMAGE, V40, P82, DOI 10.1016/j.image.2015.11.002.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z.
   Wang H, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0204714.
   Wang S, 2009, INTERNATIONAL JOINT CONFERENCE ON COMPUTATIONAL SCIENCES AND OPTIMIZATION, VOL 1, PROCEEDINGS, P274, DOI 10.1109/CSO.2009.201.
   Wang XF, 2005, LECT NOTES COMPUT SC, V3644, P87.
   Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108.
   Wu MH, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0205002.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.
   Zareapoor M, 2018, PATTERN RECOGN LETT, V115, P4, DOI 10.1016/j.patrec.2017.09.018.
   Zulkifli Z., 2011, Proceedings of the 2011 11th International Conference on Hybrid Intelligent Systems (HIS 2011), P430, DOI 10.1109/HIS.2011.6122144.},
Number-of-Cited-References = {35},
Times-Cited = {8},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {8},
Journal-ISO = {PLoS One},
Doc-Delivery-Number = {NG4BK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000563929200047},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000223391800006,
Author = {Kavdir, I},
Title = {Discrimination of sunflower, weed and soil by artificial neural networks},
Journal = {COMPUTERS AND ELECTRONICS IN AGRICULTURE},
Year = {2004},
Volume = {44},
Number = {2},
Pages = {153-160},
Month = {AUG},
Abstract = {Selective application of herbicide to weeds at an early stage in crop
   growth is an important aspect of site-specific management of field
   crops, both economically and environmentally. This paper describes the
   application of a neural network classifier to differentiate between 2
   and 3 weeks old sunflower plants and common cocklebur weeds of similar
   size, shape and colour. Colour images were obtained by a digital camera,
   in natural sunlight. A specific objective was to minimise the subsequent
   image processing operations needed to enhance the images and to extract
   the features needed by a back propagation neural network classifier.
   Neural network structures with different numbers of hidden layers and
   neurons in them were tested to find the optimal classifier. The maximum
   number of correctly recognised images in distinguishing weeds from
   sunflower plants was 71 (out of 86), while it was 82 and 74 in
   separating sunflower and weed images from bare soil images,
   respectively. (C) 2004 Elsevier B.V. All rights reserved.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Kavdir, I (Corresponding Author), Canakkale Onsekiz Mart Univ, Coll Agr, Dept Agr Machinery, TR-17020 Canakkale, Turkey.
   Canakkale Onsekiz Mart Univ, Coll Agr, Dept Agr Machinery, TR-17020 Canakkale, Turkey.},
DOI = {10.1016/j.compag.2004.03.006},
ISSN = {0168-1699},
EISSN = {1872-7107},
Keywords = {weed recognition; plant recognition; artificial neural networks; image
   processing; site-specific applications; sunflower},
Keywords-Plus = {DESIGN},
Research-Areas = {Agriculture; Computer Science},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications},
Author-Email = {kavdiris@comu.edu.tr},
Affiliations = {Canakkale Onsekiz Mart University},
Cited-References = {BILLER RH, 1996, REDUCED INPUT HERBIC.
   Burks TF, 2000, T ASAE, V43, P1029, DOI 10.13031/2013.2971.
   EBERHART RC, 1990, NEURAL NETWORK PC TO.
   Hassoun M., 1995, FUNDAMENTALS ARTIFIC.
   Moshou D, 2001, COMPUT ELECTRON AGR, V31, P5, DOI 10.1016/S0168-1699(00)00170-8.
   {*}SAS I, 1999, SAS STAT US GUID, V2.
   Tian L, 1999, T ASAE, V42, P893, DOI 10.13031/2013.13269.
   VRINDTS E, 1997, OPTICAL DISCRIMINATI, P537.
   Wang N, 2001, T ASAE, V44, P409, DOI 10.13031/2013.4673.},
Number-of-Cited-References = {9},
Times-Cited = {33},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {10},
Journal-ISO = {Comput. Electron. Agric.},
Doc-Delivery-Number = {847KR},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000223391800006},
DA = {2023-08-12},
}

@article{ WOS:000850966600001,
Author = {Kesler, Selami and Karakan, Abdil and Oguz, Yuksel},
Title = {Real-Time Strawberry Plant Classification and Efficiency Increase with
   Hybrid System Deep Learning: Microcontroller and Mobile Application},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2022},
Volume = {12},
Number = {17},
Month = {SEP},
Abstract = {The strawberry plant has three life stages: seedling, blooming, and
   crop. It needs different acclimatization conditions in these life
   stages. A dataset consisting of 10,000 photographs of the strawberry
   plant was prepared. Using this dataset, classification in convolutional
   neural networks was performed in Matrix Laboratory (MATLAB). Nine
   different algorithms were used in this process. They were realized in
   ResNet101 architecture, and the highest accuracy rate was 99.8\%. A
   low-resolution camera was used while growing strawberry plants in the
   application greenhouse. Every day at 10:00, a picture of the strawberry
   plant was taken. The captured image was processed in ResNet101
   architecture. The result of the detection process appeared on the
   computer screen and was sent to the microcontroller via a USB
   connection. The microcontroller adjusted air-conditioning in the
   greenhouse according to the state of the strawberry plant. For this, it
   decided based on the data received from the temperature, humidity, wind
   direction, and wind speed sensors outside the greenhouse and the
   temperature, humidity, and soil moisture sensors inside the greenhouse.
   In addition, all data from the sensors and the life stage of the plant
   were displayed with a mobile application. The mobile application also
   provided the possibility for manual control. In the study, the
   greenhouse was divided into two. Strawberries were grown with the hybrid
   system on one side of the greenhouse and a normal system on the other
   side of the greenhouse. This study achieved 9.75\% more crop, had a
   4.75\% earlier crop yield, and required 8.59\% less irrigation in
   strawberry plants grown using the hybrid system.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Karakan, A (Corresponding Author), Afyon Kocatepe Univ, Dazkiri Vocat Sch, Elect Dept, TR-03204 Afyon, Turkey.
   Kesler, Selami, Pamukkale Univ, Fac Engn, Elect \& Elect Engn, TR-20160 Denizli, Turkey.
   Karakan, Abdil, Afyon Kocatepe Univ, Dazkiri Vocat Sch, Elect Dept, TR-03204 Afyon, Turkey.
   Oguz, Yuksel, Afyon Kocatepe Univ, Fac Technol, Elect \& Elect Engn, TR-03204 Afyon, Turkey.},
DOI = {10.3390/app12178860},
Article-Number = {8860},
EISSN = {2076-3417},
Keywords = {deep learning; convolutional neural networks; MATLAB; hybrid system;
   mobile application; productivity},
Keywords-Plus = {DISEASES},
Research-Areas = {Chemistry; Engineering; Materials Science; Physics},
Web-of-Science-Categories  = {Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied},
Author-Email = {akarakan@aku.edu.tr},
Affiliations = {Pamukkale University; Afyon Kocatepe University; Afyon Kocatepe
   University},
ResearcherID-Numbers = {KARAKAN, Abdil/GRY-6081-2022
   KESLER, Selami/A-8819-2018
   kesler, selami/HKF-1779-2023},
ORCID-Numbers = {KARAKAN, Abdil/0000-0003-1651-7568
   KESLER, Selami/0000-0002-7027-1426
   kesler, selami/0000-0002-7027-1426},
Cited-References = {Alkan A, 2021, TURK J AGRIC FOR, V45, P717, DOI 10.3906/tar-2007-105.
   Anh PTQ, 2022, POSTHARVEST BIOL TEC, V190, DOI 10.1016/j.postharvbio.2022.111956.
   Ashwinkumar S, 2022, MATER TODAY-PROC, V51, P480, DOI 10.1016/j.matpr.2021.05.584.
   Brahimi M, 2018, HUM-COMPUT INT-SPRIN, P93, DOI 10.1007/978-3-319-90403-0\_6.
   Chen SY, 2022, SPECTROCHIM ACTA A, V279, DOI 10.1016/j.saa.2022.121418.
   Daobilige S., 2021, BIOSYST ENG, V204, P198.
   DeChant C, 2017, PHYTOPATHOLOGY, V107, P1426, DOI 10.1094/PHYTO-11-16-0417-R.
   Espinosa AR, 2020, RENEW ENERG, V162, P249, DOI 10.1016/j.renene.2020.07.154.
   Fujita E, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P989, DOI {[}10.1109/ICMLA.2016.56, 10.1109/ICMLA.2016.0178].
   Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Khandelwal Ines, 2019, Computational Intelligence: Theories, Applications and Future DirectionsVolume II. ICCI-2017. Advances in Intelligent Systems and Computing (AISC 799), P295, DOI 10.1007/978-981-13-1135-2\_23.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   Lu Y, 2017, NEUROCOMPUTING, V267, P378, DOI 10.1016/j.neucom.2017.06.023.
   Maeda-Gutierrez V, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10041245.
   Mcilwaine B, 2021, INT J APPL EARTH OBS, V97, DOI 10.1016/j.jag.2020.102279.
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419.
   Nachtigall LG, 2016, PROC INT C TOOLS ART, P472, DOI {[}10.1109/ICTAI.2016.0078, 10.1109/ICTAI.2016.75].
   Paymode A. S., 2022, Artificial Intelligence in Agriculture, V6, P23, DOI 10.1016/j.aiia.2021.12.002.
   Rangarajan AK, 2018, PROCEDIA COMPUT SCI, V133, P1040, DOI 10.1016/j.procs.2018.07.070.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Sabzi S, 2021, CHEMOMETR INTELL LAB, V217, DOI 10.1016/j.chemolab.2021.104404.
   Shrimali S., 2021, PROCEDIA COMPUT SCI, V191, P469, DOI {[}10.1016/j.procs.2021.07.059, DOI 10.1016/J.PROCS.2021.07.059].
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801.
   Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.48550/ARXIV.1409.4842.
   Tang Z, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105735.
   Vasconez JP, 2020, COMPUT ELECTRON AGR, V173, DOI 10.1016/j.compag.2020.105348.
   Waheed A, 2020, COMPUT ELECTRON AGR, V175, DOI 10.1016/j.compag.2020.105456.
   Wang YB, 2021, EXPERT SYST APPL, V178, DOI 10.1016/j.eswa.2021.114770.
   Weng Z, 2022, COMPUT ELECTRON AGR, V196, DOI 10.1016/j.compag.2022.106871.
   Xiao K., 2022, SMART AGR TECHNOL, V2, P100060, DOI {[}10.1016/j.atech.2022.100060, DOI 10.1016/J.ATECH.2022.100060].
   Yuan PS, 2022, COMPUT ELECTRON AGR, V194, DOI 10.1016/j.compag.2021.106679.
   Zeiler M.D., 2014, VISUALIZING UNDERSTA, P818.},
Number-of-Cited-References = {33},
Times-Cited = {1},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {3},
Journal-ISO = {Appl. Sci.-Basel},
Doc-Delivery-Number = {4J0MN},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000850966600001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000482109500065,
Author = {Joly, Alexis and Bonnet, Pierre and Affouard, Antoine and Lombardo,
   Jean-Christophe and Goeau, Herve},
Book-Group-Author = {ACM},
Title = {Pl@ntNet - My Business},
Booktitle = {PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17)},
Year = {2017},
Pages = {551-555},
Note = {25th ACM International Conference on Multimedia (MM), Comp Hist Museum,
   Mountain View, CA, OCT 23-27, 2017},
Organization = {Assoc Comp Machinery; ACM SIGMM},
Abstract = {Pl@ntNet is a world-scale participatory platform and information system
   dedicated to the monitoring of plant biodiversity through image-based
   plant identification. Nowadays, the mobile front-end of Pl@ntNet has
   been downloaded by more than 4 millions users in about 170 countries and
   an active community of contributors produce and revise new observations
   everyday. This paper presents a business proposal allowing enterprises
   or organizations to set up their own private collaborative workflow
   within Pl@ntNet information system. The main added value is to allow
   them working on their own business object (e.g. plant disease
   diagnostic, deficiency measurements, railway lines maintenance, etc.)
   and with their own community of contributors and end-users (employees,
   sales representatives, clients, observers network, etc.). This business
   idea answers to a growing demand in agriculture and environmental
   economics. Actors in these domains begin to know that machine learning
   techniques are mature enough but the lack of training data and of
   efficient tools to collect them is a major breakthrough. A collaborative
   platform like Pl@ntNet extended with the technical innovations presented
   in this paper is the ideal tool to bridge this gap. It will initiate a
   powerful positive feedback loop boosting the production of training data
   while improving the work of the employees.},
Publisher = {ASSOC COMPUTING MACHINERY},
Address = {1515 BROADWAY, NEW YORK, NY 10036-9998 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Joly, A (Corresponding Author), INRIA, LIRMM, Montpellier, France.
   Joly, Alexis; Affouard, Antoine; Lombardo, Jean-Christophe, INRIA, LIRMM, Montpellier, France.
   Bonnet, Pierre; Goeau, Herve, Cirad, AMAP, Montpellier, France.},
DOI = {10.1145/3123266.3129312},
ISBN = {978-1-4503-4906-2},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Theory \& Methods; Engineering, Electrical \&
   Electronic},
Affiliations = {CHARMEU; Universite de Montpellier; Centre National de la Recherche
   Scientifique (CNRS); Universite Paul-Valery; Universite Perpignan Via
   Domitia; Inria; CHARMEU; Universite de Montpellier; CIRAD},
ResearcherID-Numbers = {joly, alexis/AAV-3101-2021
   Bonnet, Pierre/AAG-6819-2020
   },
ORCID-Numbers = {joly, alexis/0000-0002-2161-9940
   Bonnet, Pierre/0000-0002-2828-4389
   Goeau, Herve/0000-0003-3296-3795},
Cited-References = {{[}Anonymous], 2013, PALNT LIST.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Ganin Y, 2016, J MACH LEARN RES, V17.
   Joly A, 2016, MULTIMEDIA SYST, V22, P751, DOI 10.1007/s00530-015-0462-9.
   Ravi Sachin, 2016, PROC INT C LEARN REP.
   Silvertown J, 2015, ZOOKEYS, P125, DOI 10.3897/zookeys.480.8803.
   Sullivan BL, 2014, BIOL CONSERV, V169, P31, DOI 10.1016/j.biocon.2013.11.003.
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594.},
Number-of-Cited-References = {8},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BN4KU},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000482109500065},
OA = {Green Published},
DA = {2023-08-12},
}

@article{ WOS:000738881100001,
Author = {Halstead, Michael and Ahmadi, Alireza and Smitt, Claus and Schmittmann,
   Oliver and McCool, Chris},
Title = {Crop Agnostic Monitoring Driven by Deep Learning},
Journal = {FRONTIERS IN PLANT SCIENCE},
Year = {2021},
Volume = {12},
Month = {DEC 20},
Abstract = {Farmers require diverse and complex information to make agronomical
   decisions about crop management including intervention tasks. Generally,
   this information is gathered by farmers traversing their fields or
   glasshouses which is often a time consuming and potentially expensive
   process. In recent years, robotic platforms have gained significant
   traction due to advances in artificial intelligence. However, these
   platforms are usually tied to one setting (such as arable farmland), or
   algorithms are designed for a single platform. This creates a
   significant gap between available technology and farmer requirements. We
   propose a novel field agnostic monitoring technique that is able to
   operate on two different robots, in arable farmland or a glasshouse
   (horticultural setting). Instance segmentation forms the backbone of
   this approach from which object location and class, object area, and
   yield information can be obtained. In arable farmland, our segmentation
   network is able to estimate crop and weed at a species level and in a
   glasshouse we are able to estimate the sweet pepper and their ripeness.
   For yield information, we introduce a novel matching criterion that
   removes the pixel-wise constraints of previous versions. This approach
   is able to accurately estimate the number of fruit (sweet pepper) in a
   glasshouse with a normalized absolute error of 4.7\% and an R-2 of 0.901
   with the visual ground truth. When applied to cluttered arable farmland
   scenes it improves on the prior approach by 50\%. Finally, a qualitative
   analysis shows the validity of this agnostic monitoring algorithm by
   supplying decision enabling information to the farmer such as the impact
   of a low level weeding intervention scheme.},
Publisher = {FRONTIERS MEDIA SA},
Address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Halstead, M (Corresponding Author), Univ Bonn, Inst Agr Engn, Agr Robot, Bonn, Germany.
   Halstead, Michael; Ahmadi, Alireza; Smitt, Claus; Schmittmann, Oliver; McCool, Chris, Univ Bonn, Inst Agr Engn, Agr Robot, Bonn, Germany.},
DOI = {10.3389/fpls.2021.786702},
Article-Number = {786702},
ISSN = {1664-462X},
Keywords = {plant classification; artificial intelligence; deep learning;
   convolutional neural network; image segmentation; field plant
   observation},
Keywords-Plus = {ROW WEED-CONTROL},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {michael.halstead@uni-bonn.de},
Affiliations = {University of Bonn},
ORCID-Numbers = {Halstead, Michael/0000-0001-7185-9304},
Funding-Acknowledgement = {Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under
   Germany's Excellence Strategy {[}EXC-2070 - 390732324 - PhenoRob]},
Funding-Text = {This work has partially been funded by the Deutsche
   Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's
   Excellence Strategy, EXC-2070 - 390732324 PhenoRob.},
Cited-References = {ABARES, 2018, FARM FINANCIAL PERFO.
   Adeux G, 2019, NAT SUSTAIN, V2, P1018, DOI 10.1038/s41893-019-0415-y.
   Ahmadi A., 2021, INT C INTELLIGENT RO.
   Ahmadi A., 2021, PROC DAGM GERMAN C P.
   Arad B, 2020, J FIELD ROBOT, V37, P1027, DOI 10.1002/rob.21937.
   Bakker T, 2010, J TERRAMECHANICS, V47, P63, DOI 10.1016/j.jterra.2009.06.002.
   Bargoti S, 2017, J FIELD ROBOT, V34, P1039, DOI 10.1002/rob.21699.
   Bawden O, 2017, J FIELD ROBOT, V34, P1179, DOI 10.1002/rob.21727.
   Blaix C, 2018, WEED RES, V58, P151, DOI 10.1111/wre.12303.
   Denman S, 2015, PATTERN RECOGN LETT, V68, P306, DOI 10.1016/j.patrec.2015.06.015.
   Grimstad L, 2017, IFAC PAPERSONLINE, V50, P4588, DOI 10.1016/j.ifacol.2017.08.1005.
   Halstead M, 2020, P DIGITAL IMAGE COMP.
   Halstead M, 2018, IEEE ROBOT AUTOM LET, V3, P2995, DOI 10.1109/LRA.2018.2849514.
   He K., 2018, P IEEE INT C COMPUTE.
   Hemming S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19081807.
   Hung C, 2013, IEEE INT C INT ROBOT, P5314, DOI 10.1109/IROS.2013.6697125.
   Jayaraman D, 2016, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2016.418.
   Kirk R, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20010275.
   Koirala A, 2019, PRECIS AGRIC, V20, P1107, DOI 10.1007/s11119-019-09642-0.
   Lehnert C, 2017, IEEE ROBOT AUTOM LET, V2, P872, DOI 10.1109/LRA.2017.2655622.
   Lopez A, 2021, INT J APPL EARTH OBS, V97, DOI 10.1016/j.jag.2020.102274.
   Luling Nils, 2021, 2021 IEEE International Conference on Robotics and Automation (ICRA), P2331, DOI 10.1109/ICRA48506.2021.9561792.
   LWK-Rheinland, 2020, ANBAUEMPFEHLUNGEN LA.
   McCool C, 2016, IEEE INT CONF ROBOT, P2506, DOI 10.1109/ICRA.2016.7487405.
   Meier U., 2018, GROWTH STAGES MONOAN, P204.
   Nuske S, 2011, IEEE INT C INT ROBOT.
   Nuske S, 2014, J FIELD ROBOT, V31, P837, DOI 10.1002/rob.21541.
   Peruzzi A, 2017, J AGRIC ENG, V48, P57, DOI 10.4081/jae.2017.583.
   Raven PH, 2021, P NATL ACAD SCI USA, V118, DOI 10.1073/pnas.2002548117.
   Redmon J, ARXIV.
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91.
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031.
   Sa I, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16081222.
   Slaughter DC, 2008, COMPUT ELECTRON AGR, V61, P63, DOI 10.1016/j.compag.2007.05.008.
   Smitt Claus, 2021, 2021 IEEE International Conference on Robotics and Automation (ICRA), P2324, DOI 10.1109/ICRA48506.2021.9562047.
   Stein M, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111915.
   Tian YN, 2019, COMPUT ELECTRON AGR, V157, P417, DOI 10.1016/j.compag.2019.01.012.
   Utstumo T, 2018, COMPUT ELECTRON AGR, V154, P36, DOI 10.1016/j.compag.2018.08.043.
   Wan SH, 2020, COMPUT NETW, V168, DOI 10.1016/j.comnet.2019.107036.
   Wang Q. a., 2013, EXPT ROBOTICS, P745, DOI {[}10.1007/978-3-319-00065-7\_50, DOI 10.1007/978-3-319-00065-7\_50].
   Wang XL, 2019, PROC CVPR IEEE, P2561, DOI 10.1109/CVPR.2019.00267.
   Yamamoto K, 2014, SENSORS-BASEL, V14, P12191, DOI 10.3390/s140712191.
   Zabawa L, 2019, IEEE COMPUT SOC CONF, P2571, DOI 10.1109/CVPRW.2019.00313.
   Zhang L, 2019, IEEE ACCESS, V7, P56028, DOI 10.1109/ACCESS.2019.2899940.},
Number-of-Cited-References = {44},
Times-Cited = {5},
Usage-Count-Last-180-days = {1},
Usage-Count-Since-2013 = {10},
Journal-ISO = {Front. Plant Sci.},
Doc-Delivery-Number = {YB2XG},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000738881100001},
OA = {gold, Green Published},
DA = {2023-08-12},
}

@article{ WOS:000326264900004,
Author = {Mattila, Heta and Valli, Pertti and Pahikkala, Tapio and Teuhola, Jukka
   and Nevalainen, Olli S. and Tyystjarvi, Esa},
Title = {Comparison of chlorophyll fluorescence curves and texture analysis for
   automatic plant identification},
Journal = {PRECISION AGRICULTURE},
Year = {2013},
Volume = {14},
Number = {6},
Pages = {621-636},
Month = {DEC},
Abstract = {With automatic plant identification methods, the amount of herbicides
   used in agriculture can be reduced when herbicides are sprayed only on
   weeds. In the present study, leaves of oat (Avena sativa) and dandelion
   (Taraxacum officinale, TAROF) were arranged so that there was overlap
   between the species, imaged with a pulse amplitude modulation
   fluorescence camera and photographed with a digital color camera. The
   fluorescence induction curves from each pixel were parameterized to
   obtain a set of features and from color photographs, texture features
   were calculated. A support vector algorithm that also performed feature
   selection was used for pattern recognition of both data sets.
   Fluorescence-based identification worked well with oat leaves, producing
   92.2 \% of correctly identified pixels, whereas the texture-based method
   often mis-identified the central vein of a TAROF leaf as oat,
   identifying correctly only 66.5 \% of oat pixels. With TAROF that shows
   a clear dicot-type texture, the texture method was slightly better (96.4
   \% correctly identified pixels) than the fluorescence method (94.6 \%).
   In fluorescence-based identification, the accuracy varied between entire
   TAROF leaves, probably reflecting the genetic variability of TAROF. The
   results suggest that the accuracy of identification could be improved by
   combining two identification methods.},
Publisher = {SPRINGER},
Address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Tyystjarvi, E (Corresponding Author), Univ Turku, Dept Biochem \& Food Chem, FIN-20014 Turku, Finland.
   Mattila, Heta; Tyystjarvi, Esa, Univ Turku, Dept Biochem \& Food Chem, FIN-20014 Turku, Finland.
   Valli, Pertti; Pahikkala, Tapio; Teuhola, Jukka; Nevalainen, Olli S., Univ Turku, Dept Informat Technol, FIN-20014 Turku, Finland.},
DOI = {10.1007/s11119-013-9320-y},
ISSN = {1385-2256},
EISSN = {1573-1618},
Keywords = {Automatic plant identification; Chlorophyll a fluorescence; Fluorescence
   fingerprinting; Leaf texture; Support vector machine},
Keywords-Plus = {SUGAR-BEET; WEED MANAGEMENT; IMAGE-ANALYSIS; REAL-TIME; CLASSIFICATION;
   DISCRIMINATION; CROPS; COLOR; SELECTION; FEATURES},
Research-Areas = {Agriculture},
Web-of-Science-Categories  = {Agriculture, Multidisciplinary},
Author-Email = {esatyy@utu.fi},
Affiliations = {University of Turku; University of Turku},
ResearcherID-Numbers = {Tyystjärvi, Esa/B-2360-2015
   Pahikkala, Tapio/H-9659-2012
   },
ORCID-Numbers = {Tyystjärvi, Esa/0000-0001-6808-7470
   Pahikkala, Tapio/0000-0003-4183-2455
   Mattila, Heta/0000-0002-5071-9721},
Funding-Acknowledgement = {Academy of Finland; Nordic Energy Research (AquaFeed project)},
Funding-Text = {Ladislav Nedbal and Julie Olejnickova are warmly thanked for help with
   the Fluor-Cam. ET and HM were financially supported by Academy of
   Finland and by Nordic Energy Research (AquaFeed project).},
Cited-References = {Aitkenhead MJ, 2003, COMPUT ELECTRON AGR, V39, P157, DOI 10.1016/S0168-1699(03)00076-0.
   {[}Anonymous], 2003, NOTICES AMS.
   Baker NR, 2004, J EXP BOT, V55, P1607, DOI 10.1093/jxb/erh196.
   Berger S, 2007, J EXP BOT, V58, P797, DOI 10.1093/jxb/erl208.
   Borkowski W, 2004, INT J MOD PHYS C, V15, P1171, DOI 10.1142/S0129183104006601.
   Borregaard T, 2000, J AGR ENG RES, V75, P389, DOI 10.1006/jaer.1999.0519.
   Brown RB, 2005, WEED SCI, V53, P252, DOI 10.1614/WS-04-068R1.
   Burgos-Artizzu XP, 2011, COMPUT ELECTRON AGR, V75, P337, DOI 10.1016/j.compag.2010.12.011.
   Codrea MC, 2010, PHOTOSYNTH RES, V105, P273, DOI 10.1007/s11120-010-9578-0.
   Gebhardt S, 2007, PRECIS AGRIC, V8, P1, DOI 10.1007/s11119-006-9024-7.
   Gerhards R, 2003, WEED RES, V43, P385, DOI 10.1046/j.1365-3180.2003.00349.x.
   Hutto KC, 2006, WEED SCI, V54, P335.
   Keranen Mika, 2003, Precision Agriculture, V4, P53, DOI 10.1023/A:1021863005378.
   Keranen M, 2009, HARMFUL ALGAE, V8, P817, DOI 10.1016/j.hal.2007.12.023.
   LANCASHIRE PD, 1991, ANN APPL BIOL, V119, P561, DOI 10.1111/j.1744-7348.1991.tb04895.x.
   Lin FY, 2008, COMM COM INF SC, V15, P432.
   Loghavi M, 2008, COMPUT ELECTRON AGR, V63, P112, DOI 10.1016/j.compag.2008.01.020.
   Longchamps L, 2010, PRECIS AGRIC, V11, P181, DOI 10.1007/s11119-009-9126-0.
   Luschei EC, 2001, WEED SCI, V49, P536, DOI 10.1614/0043-1745(2001)049{[}0536:IACOFW]2.0.CO;2.
   Matous K, 2006, PHOTOSYNTH RES, V90, P243, DOI 10.1007/s11120-006-9120-6.
   Maxwell K, 2000, J EXP BOT, V51, P659, DOI 10.1093/jexbot/51.345.659.
   Midtiby HS, 2011, COMPUT ELECTRON AGR, V77, P35, DOI 10.1016/j.compag.2011.03.006.
   Mishra A, 2009, J FLUORESC, V19, P905, DOI 10.1007/s10895-009-0491-x.
   Moshou D, 2001, COMPUT ELECTRON AGR, V31, P5, DOI 10.1016/S0168-1699(00)00170-8.
   Nieuwenhuizen AT, 2007, PRECIS AGRIC, V8, P267, DOI 10.1007/s11119-007-9044-y.
   Norremark M, 2007, COMPUT ELECTRON AGR, V56, P130, DOI 10.1016/j.compag.2007.01.006.
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623.
   Pahikkala T., 2010, P 9 INT C MACH LEARN.
   Pahikkala T, 2012, ALGORITHM MOL BIOL, V7, DOI 10.1186/1748-7188-7-11.
   Panneton B, 2011, APPL SPECTROSC, V65, P10, DOI 10.1366/10-06100.
   Pennebaker W., 1993, JPEG STILL IMAGE DAT.
   Persson M, 2008, BIOSYST ENG, V100, P484, DOI 10.1016/j.biosystemseng.2008.05.003.
   Pietikainen M, 2000, PATTERN RECOGN, V33, P43, DOI 10.1016/S0031-3203(99)00032-1.
   Piron A, 2008, COMPUT ELECTRON AGR, V62, P141, DOI 10.1016/j.compag.2007.12.007.
   Rew LJ, 1996, WEED RES, V36, P283, DOI 10.1111/j.1365-3180.1996.tb01658.x.
   Rossatto DR, 2011, PLANT SYST EVOL, V291, P103, DOI 10.1007/s00606-010-0366-2.
   Rumpf T, 2012, COMPUT ELECTRON AGR, V80, P89, DOI 10.1016/j.compag.2011.10.018.
   Sainz-Costa N, 2011, SENSORS-BASEL, V11, P7095, DOI 10.3390/s110707095.
   Silverman B. W., 2018, DENSITY ESTIMATION S, DOI 10.1201/9781315140919.
   Slaughter DC, 2008, COMPUT ELECTRON AGR, V61, P63, DOI 10.1016/j.compag.2007.05.008.
   Slaughter DC, 2008, WEED TECHNOL, V22, P378, DOI 10.1614/WT-07-104.1.
   Sogaard HT, 2005, BIOSYST ENG, V91, P271, DOI 10.1016/j.biosystemseng.2005.04.011.
   Soille P, 2000, IMAGE VISION COMPUT, V18, P1025, DOI 10.1016/S0262-8856(00)00043-3.
   Tyystjarvi E, 1999, BIOPHYS J, V77, P1159, DOI 10.1016/S0006-3495(99)76967-5.
   Tyystjarvi E, 2011, PRECIS AGRIC, V12, P546, DOI 10.1007/s11119-010-9201-6.
   Wang N, 2007, BIOSYST ENG, V98, P276, DOI 10.1016/j.biosystemseng.2007.08.007.
   Wiles LJ, 2009, PRECIS AGRIC, V10, P277, DOI 10.1007/s11119-008-9097-6.
   WOEBBECKE DM, 1995, T ASAE, V38, P259, DOI 10.13031/2013.27838.
   Zhang Y, 2011, BIOSYST ENG, V110, P330, DOI 10.1016/j.biosystemseng.2011.09.006.
   Zhang Y., 2009, 096365 ASABE.},
Number-of-Cited-References = {50},
Times-Cited = {5},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {53},
Journal-ISO = {Precis. Agric.},
Doc-Delivery-Number = {242TK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000326264900004},
DA = {2023-08-12},
}

@article{ WOS:000474617200001,
Author = {Uemura, Takuya and Arimura, Gen-Ichiro},
Title = {Current opinions about herbivore-associated molecular patterns and plant
   intracellular signaling},
Journal = {PLANT SIGNALING \& BEHAVIOR},
Year = {2019},
Volume = {14},
Number = {9},
Month = {SEP 2},
Abstract = {Elicitor-associated compounds included in oral secretions of herbivorous
   arthropods, defined as herbivore-associated molecular patterns (HAMPs),
   induce defense responses in plants. Recognition of HAMPs by the host
   plants triggers the activation of downstream intracellular and
   intercellular signaling, resulting in the production of defensive
   secondary metabolites and volatile emissions to defend against herbivore
   attack. Thus far, several chemical classes of HAMPs, e.g., fatty
   acid-amino acid conjugates, peptides, enzymes, and oligosaccharides,
   have been characterized from not only plant-chewing arthropod herbivores
   but also plant-sucking arthropod herbivores. Here, we introduce the
   latest insights about HAMPs and the HAMPs-induced defense signaling
   network in host plants.},
Publisher = {TAYLOR \& FRANCIS INC},
Address = {530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA},
Type = {Article},
Language = {English},
Affiliation = {Arimura, GI (Corresponding Author), Tokyo Univ Sci, Fac Ind Sci \& Technol, Dept Biol Sci \& Technol, Katsushika Ku, 6-3-1 Niijuku, Tokyo 1258585, Japan.
   Uemura, Takuya; Arimura, Gen-Ichiro, Tokyo Univ Sci, Fac Ind Sci \& Technol, Dept Biol Sci \& Technol, Tokyo, Japan.},
DOI = {10.1080/15592324.2019.1633887},
EarlyAccessDate = {JUN 2019},
ISSN = {1559-2316},
EISSN = {1559-2324},
Keywords = {Elicitor; herbivore-associated molecular patterns (HAMPs); pattern
   recognition receptors (PRRs)},
Keywords-Plus = {DEFENSE RESPONSES; EARLY EVENTS; ELICITOR; PROTEIN},
Research-Areas = {Biochemistry \& Molecular Biology; Plant Sciences},
Web-of-Science-Categories  = {Biochemistry \& Molecular Biology; Plant Sciences},
Author-Email = {garimura@rs.tus.ac.jp},
Affiliations = {Tokyo University of Science},
Funding-Acknowledgement = {MEXT {[}18H04630, 18H04786]; Grants-in-Aid for Scientific Research
   {[}18H04786, 18H04630] Funding Source: KAKEN},
Funding-Text = {This work was financially supported in part by MEXT Grants-in-Aid for
   Scientific Research on Innovative Areas {[}18H04630 and 18H04786].},
Cited-References = {Alborn HT, 2007, P NATL ACAD SCI USA, V104, P12976, DOI 10.1073/pnas.0705947104.
   Alborn HT, 1997, SCIENCE, V276, P945, DOI 10.1126/science.276.5314.945.
   Ali MRM, 2019, PLANT PHYSIOL, V179, P1273, DOI 10.1104/pp.18.00715.
   Bricchi I, 2013, PLANT J, V73, P14, DOI 10.1111/j.1365-313X.2012.05103.x.
   Gilardoni PA, 2011, PLANT CELL, V23, P3512, DOI 10.1105/tpc.111.088229.
   Guo HJ, 2013, INSECT BIOCHEM MOLEC, V43, P849, DOI 10.1016/j.ibmb.2013.06.005.
   Hilker M, 2006, J CHEM ECOL, V32, P1379, DOI 10.1007/s10886-006-9057-4.
   Hu LF, 2018, NEW PHYTOL, V219, P1097, DOI 10.1111/nph.15247.
   Iida J, 2019, NEW PHYTOL, V224, P875, DOI 10.1111/nph.15813.
   Kazan K, 2012, TRENDS PLANT SCI, V17, P22, DOI 10.1016/j.tplants.2011.10.006.
   Liu YQ, 2015, NAT BIOTECHNOL, V33, P301, DOI 10.1038/nbt.3069.
   Louis J, 2013, NEW PHYTOL, V199, P66, DOI 10.1111/nph.12308.
   Maffei ME, 2007, TRENDS PLANT SCI, V12, P310, DOI 10.1016/j.tplants.2007.06.001.
   MATTIACCI L, 1995, P NATL ACAD SCI USA, V92, P2036, DOI 10.1073/pnas.92.6.2036.
   Musser RO, 2002, NATURE, V416, P599, DOI 10.1038/416599a.
   Schmelz EA, 2006, P NATL ACAD SCI USA, V103, P8894, DOI 10.1073/pnas.0602328103.
   Schmelz EA, 2009, P NATL ACAD SCI USA, V106, P653, DOI 10.1073/pnas.0811861106.
   Shangguan XX, 2018, PLANT PHYSIOL, V176, P552, DOI 10.1104/pp.17.00755.
   Spiteller D, 2001, TETRAHEDRON LETT, V42, P1483, DOI 10.1016/S0040-4039(00)02290-5.
   Yan C, 2018, MOL CELL, V70, P136, DOI 10.1016/j.molcel.2018.03.013.
   Yoshinaga N, 2007, J CHEM ECOL, V33, P1376, DOI 10.1007/s10886-007-9321-2.
   Yoshinaga N, 2010, J CHEM ECOL, V36, P319, DOI 10.1007/s10886-010-9764-8.},
Number-of-Cited-References = {22},
Times-Cited = {8},
Usage-Count-Last-180-days = {9},
Usage-Count-Since-2013 = {40},
Journal-ISO = {Plant Signal. Behav.},
Doc-Delivery-Number = {JA2RK},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000474617200001},
OA = {Bronze, Green Published},
DA = {2023-08-12},
}

@article{ WOS:000434130700009,
Author = {Li, Ji and Tang, Lie},
Title = {Crop recognition under weedy conditions based on 3D imaging for robotic
   weed control},
Journal = {JOURNAL OF FIELD ROBOTICS},
Year = {2018},
Volume = {35},
Number = {4},
Pages = {596-611},
Month = {JUN},
Abstract = {A 3D time-of-flight camera was applied to develop a crop plant
   recognition system for broccoli and green bean plants under weedy
   conditions. The developed system overcame the previously unsolved
   problems caused by occluded canopy and illumination variation. An
   efficient noise filter was developed to remove the sparse noise points
   in 3D point cloud space. Both 2D and 3D features including the gradient
   of amplitude and depth image, surface curvature, amplitude percentile
   index, normal direction, and neighbor point count in 3D space were
   extracted and found effective for recognizing these two types of plants.
   Separate segmentation algorithms were developed for each of the broccoli
   and green bean plant in accordance with their 3D geometry and 2D
   amplitude characteristics. Under the experimental condition where the
   crops were heavily infested by various types of weed plants, detection
   rates over 88.3\% and 91.2\% were achieved for broccoli and green bean
   plant leaves, respectively. Additionally, the crop plants were segmented
   out with nearly complete shape. Moreover, the algorithms were
   computationally optimized, resulting in an image processing speed of
   over 30 frames per second.},
Publisher = {WILEY},
Address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
Type = {Article},
Language = {English},
Affiliation = {Tang, L (Corresponding Author), Iowa State Univ, Agr \& Biosyst Engn, Ames, IA 50011 USA.
   Li, Ji; Tang, Lie, Iowa State Univ, Agr \& Biosyst Engn, Ames, IA 50011 USA.},
DOI = {10.1002/rob.21763},
ISSN = {1556-4959},
EISSN = {1556-4967},
Keywords = {3D point cloud; machine vision; plant recognition; robotic weed control},
Keywords-Plus = {PLANT; SYSTEMS},
Research-Areas = {Robotics},
Web-of-Science-Categories  = {Robotics},
Author-Email = {lietang@iastate.edu},
Affiliations = {Iowa State University},
Cited-References = {Astrand B, 2005, MECHATRONICS, V15, P251, DOI 10.1016/j.mechatronics.2004.05.005.
   Chutia D, 2016, T GIS, V20, P463, DOI 10.1111/tgis.12164.
   Du JX, 2007, APPL MATH COMPUT, V185, P883, DOI 10.1016/j.amc.2006.07.072.
   Ehsani MR, 2004, T ASAE, V47, P909, DOI 10.13031/2013.16088.
   Fennimore SA, 2016, WEED TECHNOL, V30, P823, DOI 10.1614/WT-D-16-00070.1.
   Franco C, 2017, PRECIS AGRIC, V18, P366, DOI 10.1007/s11119-017-9520-y.
   Furbank RT, 2011, TRENDS PLANT SCI, V16, P635, DOI 10.1016/j.tplants.2011.09.005.
   Griepentrog H. W., 2005, Precision Agriculture, V6, P157, DOI 10.1007/s11119-005-1032-5.
   Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003.
   Jeschke P, 2016, PEST MANAG SCI, V72, P433, DOI 10.1002/ps.4190.
   Jin J, 2009, J FIELD ROBOT, V26, P591, DOI 10.1002/rob.20293.
   Jones H.G., 2010, REMOTE SENSING VEGET, P271.
   KAPUR JN, 1985, COMPUT VISION GRAPH, V29, P273, DOI 10.1016/0734-189X(85)90125-2.
   Kise M, 2005, BIOSYST ENG, V90, P357, DOI 10.1016/j.biosystemseng.2004.12.008.
   KITTLER J, 1986, PATTERN RECOGN, V19, P41, DOI 10.1016/0031-3203(86)90030-0.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Liebman M., 2001, ECOLOGICAL MANAGEMEN.
   Nagasaka Y, 2004, COMPUT ELECTRON AGR, V43, P223, DOI 10.1016/j.compag.2004.01.005.
   Nakarmi AD, 2012, COMPUT ELECTRON AGR, V82, P23, DOI 10.1016/j.compag.2011.12.011.
   Noh H, 2005, T ASAE, V48, P393, DOI 10.13031/2013.17933.
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
   Slaughter DC, 2008, COMPUT ELECTRON AGR, V61, P63, DOI 10.1016/j.compag.2007.05.008.
   Sogaard HT, 2003, COMPUT ELECTRON AGR, V38, P141, DOI 10.1016/S0168-1699(02)00140-0.
   THOMPSON JF, 1991, CROP PROT, V10, P254, DOI 10.1016/0261-2194(91)90002-9.
   Vazquez-Arellano M, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16050618.
   Weiss U, 2011, ROBOT AUTON SYST, V59, P265, DOI 10.1016/j.robot.2011.02.011.
   Xiang R, 2014, COMPUT ELECTRON AGR, V106, P75, DOI 10.1016/j.compag.2014.05.006.
   Zhang Y, 2012, ISPRS J PHOTOGRAMM, V69, P65, DOI 10.1016/j.isprsjprs.2012.02.006.
   {[}No title captured].
   {[}No title captured].
   {[}No title captured].
   {[}No title captured].
   {[}No title captured].
   {[}No title captured].
   {[}No title captured].
   {[}No title captured].
   {[}No title captured].
   {[}No title captured].},
Number-of-Cited-References = {38},
Times-Cited = {14},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {43},
Journal-ISO = {J. Field Robot.},
Doc-Delivery-Number = {GI1KZ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000434130700009},
DA = {2023-08-12},
}

@inproceedings{ WOS:000251420100048,
Author = {Ericson, Stefan and Astrand, Bjorn},
Editor = {Schilling, K},
Title = {Algorithms for visual odometry in outdoor field environment},
Booktitle = {PROCEEDINGS OF THE 13TH IASTED INTERNATIONAL CONFERENCE ON ROBOTICS AND
   APPLICATIONS/PROCEEDINGS OF THE IASTED INTERNATIONAL CONFERENCE ON
   TELEMATICS},
Series = {IASTED International Conference on Robotics and Applications},
Year = {2007},
Pages = {287+},
Note = {13th IASTED International Conference on Robotics and Applications/IASTED
   International Conference on Telematics, Julius Maximilians Univ
   Wurzburg, Wurzburg, GERMANY, AUG 29-31, 2007},
Organization = {Int Assoc Sci \& Technol Dev; IASTED TCR; IASTED TCT},
Abstract = {In this paper different algorithms for visual odometry are evaluated for
   navigating an agricultural weeding robot in outdoor field environment.
   Today there is an encoder wheel that keeps track of the weeding tools
   position relative the camera, but the system suffers from wheel slippage
   and errors caused by the uneven terrain. To overcome these difficulties
   the aim is to replace the encoders with visual odometry using the plant
   recognition camera. Four different optical flow algorithms are tested on
   four different surfaces, indoor carpet, outdoor asphalt, grass and soil.
   The tests are performed on an experimental platform. The result shows
   that the errors consist mainly of dropouts caused by overriding maximum
   speed, and of calibration error due to uneven ground. The number of
   dropouts can be reduced by limiting the maximum speed and detection of
   missing frames. The calibration problem can be solved using stereo
   cameras. This gives a height measurement and the calibration will be
   given by camera mounting. The algorithm using normalized
   cross-correlation shows the best result concerning number of dropouts,
   accuracy and calculation time.},
Publisher = {ACTA PRESS ANAHEIM},
Address = {PO BOX 5124, ANAHEIM, CA 92814-5124 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Ericson, S (Corresponding Author), Univ Skovde, Box 408, S-54128 Skovde, Sweden.
   Ericson, Stefan, Univ Skovde, Box 408, S-54128 Skovde, Sweden.
   Astrand, Bjorn, Halmstad Univ, S-30118 Halmstad, Sweden.},
ISSN = {1027-264X},
ISBN = {978-0-88986-685-0},
Keywords = {agricultural applications; computer vision; visual odometry; and optical
   flow},
Research-Areas = {Robotics; Imaging Science \& Photographic Technology; Telecommunications},
Web-of-Science-Categories  = {Robotics; Imaging Science \& Photographic Technology; Telecommunications},
Author-Email = {stefan.ericson@his.se
   bjom.astrand@ide.hh.se},
Affiliations = {University of Skovde; Halmstad University},
Cited-References = {{[}Anonymous], 2002, DIGITAL IMAGE PROCES.
   Astrand B, 2002, AUTON ROBOT, V13, P21, DOI 10.1023/A:1015674004201.
   Barron J. L., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P236, DOI 10.1109/CVPR.1992.223269.
   Campbell J, 2005, IEEE INT CONF ROBOT, P3421.
   DERICHE R, 1993, INT J COMPUT VISION, V10, P101, DOI 10.1007/BF01420733.
   Dunbabin M, 2004, IEEE INT CONF ROBOT, P7, DOI 10.1109/ROBOT.2004.1307121.
   GALVIN B, 1998, P BRIT MACH VIS C, P195.
   Harris C, 1988, P 4 ALV VIS C, V15, P10.
   Helmick DM, 2004, AEROSP CONF PROC, P772.
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410.
   Lucas B., 1981, ITERATIVE IMAGE REGI, P674.
   MILELLA A, 2006, IEEE INT C COMP VIS.
   Nister D, 2004, PROC CVPR IEEE, P652.
   Nister D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P199.
   Se S, 2001, IEEE INT CONF ROBOT, P2051, DOI 10.1109/ROBOT.2001.932909.
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794.},
Number-of-Cited-References = {16},
Times-Cited = {2},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {1},
Doc-Delivery-Number = {BGZ05},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000251420100048},
DA = {2023-08-12},
}

@inproceedings{ WOS:000426330002143,
Author = {Prasad, Shitala and Singh, Pankaj P.},
Book-Group-Author = {IEEE},
Title = {Medicinal Plant Leaf Information Extraction Using Deep Features},
Booktitle = {TENCON 2017 - 2017 IEEE REGION 10 CONFERENCE},
Series = {TENCON IEEE Region 10 Conference Proceedings},
Year = {2017},
Pages = {2722-2726},
Note = {IEEE Region 10 Conference (TENCON), MALAYSIA, NOV 05-08, 2017},
Organization = {IEEE; IEEE Region 10},
Abstract = {In today's digital world of ubiquitous and Internet of thinks, medicinal
   plant identification is a challenging but very useful task in computer
   vision (CV) helping agro-community to recognize the unknown species more
   rapidly. The tchnological improvements in feature representation deep
   convolutional neural network (DCNN) is promisingly used in several
   applications like object recognitions, natural language processing and
   computer graphics. In this paper, we propose a knowledge transfer from
   object identification to plant species identification where the raw
   plant leaf image is represented into deep features. These deep features
   are experimentally proved to out-perform the state-of-the-art in plant
   species recognition. These paper presents a new and efficient technique
   for leaf acquisition. Secondly, the image is transformed to device
   independent l alpha beta color space that is further used to compute
   VGG-16 feature map. This feature map is re-projected to PCA subspace to
   optimize the performance for species recognition. To prove the
   robustness, the paper uses two different types of plant leaf datasets.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Prasad, S (Corresponding Author), Nanyang Technol Univ, Biometr \& Forens Lab, Singapore, Singapore.
   Prasad, Shitala, Nanyang Technol Univ, Biometr \& Forens Lab, Singapore, Singapore.
   Singh, Pankaj P., CIT Kokrajahr, Comp Sci \& Engn, Kokrajhar, Assam, India.},
ISSN = {2159-3442},
ISBN = {978-1-5090-1134-6},
Keywords-Plus = {IDENTIFICATION; REPRESENTATION; RETRIEVAL; SPACE},
Research-Areas = {Engineering},
Web-of-Science-Categories  = {Engineering, Electrical \& Electronic},
Author-Email = {shitala@ieee.org
   pankajp.singh@cit.ac.in},
Affiliations = {Nanyang Technological University \& National Institute of Education
   (NIE) Singapore; Nanyang Technological University},
ResearcherID-Numbers = {Prasad, Shitala/AAI-8449-2020
   Singh, Pankaj Pratap/N-3527-2015},
ORCID-Numbers = {Singh, Pankaj Pratap/0000-0003-4079-4485},
Cited-References = {Adamek T, 2004, IEEE T CIRC SYST VID, V14, P742, DOI 10.1109/TCSVT.2004.826776.
   Alajlan N, 2007, PATTERN RECOGN, V40, P1911, DOI 10.1016/j.patcog.2006.12.005.
   Chen SL, 2010, PLOS ONE, V5, DOI {[}10.1371/journal.pone.0008613, 10.1371/journal.pone.0015633].
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   Fiel S., 2010, P 16 COMP VIS WINT W, P67.
   Goeau H., 2013, CLEF.
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI {[}10.1016/j.patcog.2017.05.015, 10.14722/ndss.2017.23457].
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41.
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60.
   MCGUIRE RG, 1992, HORTSCIENCE, V27, P1254, DOI 10.21273/HORTSCI.27.12.1254.
   MCLAREN K, 1976, J SOC DYERS COLOUR, V92, P338.
   Neto J. C., 2005, OPTICS E 2005.
   Neto JC, 2006, COMPUT ELECTRON AGR, V51, P66, DOI 10.1016/j.compag.2005.11.002.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Prasad S., 2011, 2011 2nd International Conference on Computer and Communication Technology, P646, DOI 10.1109/ICCCT.2011.6075212.
   Prasad S., 2015, IEEE POTENT IN PRESS.
   Prasad S., 2011, P 2011 INT C COMM CO, P343, DOI {[}10.1145/1947940.1948012, DOI 10.1145/1947940.1948012].
   Prasad S, 2017, ADV COMPU INTELL ROB, P15, DOI 10.4018/978-1-5225-2545-5.ch002.
   Prasad S, 2017, MULTIMED TOOLS APPL, V76, P6915, DOI 10.1007/s11042-016-3309-2.
   Ross HA, 2008, SYST BIOL, V57, P216, DOI 10.1080/10635150802032990.
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y.
   Simonyan K., 2014, 14091556V6 ARXIV, P1, DOI DOI 10.48550/ARXIV.1409.1556.
   Vedaldi A., P ACM INT C MULT.
   Wang B, 2015, INFORM SCIENCES, V302, P132, DOI 10.1016/j.ins.2014.07.028.
   Zhang LK, 2016, COMPUT ELECTRON AGR, V127, P184, DOI 10.1016/j.compag.2016.06.017.
   Zhou SF, 2007, DRUG DISCOV TODAY, V12, P664, DOI 10.1016/j.drudis.2007.06.004.},
Number-of-Cited-References = {28},
Times-Cited = {11},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BJ5UT},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000426330002143},
DA = {2023-08-12},
}

@article{ WOS:000881617300001,
Author = {Karadeniz, Alper Talha and Celik, Yuksel and Basaran, Erdal},
Title = {Classification of walnut varieties obtained from walnut leaf images by
   the recommended residual block based CNN model},
Journal = {EUROPEAN FOOD RESEARCH AND TECHNOLOGY},
Year = {2023},
Volume = {249},
Number = {3},
Pages = {727-738},
Month = {MAR},
Abstract = {Walnuts are widely used, although they come in a variety of types and
   qualities. It is essential to choose the correct walnut variety with the
   necessary ecological characteristics to continue the production of
   walnut fruit, which has positive benefits on human health. Because
   planting a walnut garden is expensive and the harvesting process takes a
   while. However, since the colour and feel of walnut leaves are so
   similar, it can be challenging to tell them apart. Experts must devote a
   significant amount of time to differentiating walnut kinds, and
   morphological tests should be conducted. There are different studies in
   the literature for walnut variety differentiation. Nevertheless, those
   are studies conducted with the classification of a small number of
   walnut varieties or laboratory experiments. With the advancement of
   technology, deep learning techniques based on computers are now
   routinely utilized for leaf recognition. These technologies enable
   significant reductions in error rates, time saves, and cost. With a
   total of 1751 leaf pictures collected from 18 species of walnuts, a
   special walnut dataset was constructed for this study in order to
   identify walnut types from walnut leaves. To automatically classify the
   provided dataset, images are trained with residual block-based
   convolutional neural network architectures. Following the discovery of
   each image's deep features, the Atom Search Optimization algorithm was
   used to choose the most distinctive characteristics. Support vector
   machines (SVM) were used to classify walnut species with the new feature
   set created. The experimental studies of the proposed model based on
   Residual block and Atom Search optimization successfully categorised the
   walnut dataset with an accuracy rating of 87.42\%.},
Publisher = {SPRINGER},
Address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
Type = {Article},
Language = {English},
Affiliation = {Karadeniz, AT (Corresponding Author), Trabzon Univ, Digital Transformat \& Software Dept, Trabzon, Turkey.
   Karadeniz, Alper Talha, Trabzon Univ, Digital Transformat \& Software Dept, Trabzon, Turkey.
   Celik, Yuksel, Karabuk Univ, Fac Engn, Dept Comp Engn, Karabuk, Turkey.
   Basaran, Erdal, Ibrahim Cecen Univ Agri, Dept Comp Technol, Agri, Turkey.},
DOI = {10.1007/s00217-022-04168-8},
EarlyAccessDate = {NOV 2022},
ISSN = {1438-2377},
EISSN = {1438-2385},
Keywords = {Walnut dataset; Residual block; Atom search optimization; Optimization
   based feature selection; Support vector machines},
Keywords-Plus = {ATOM SEARCH OPTIMIZATION},
Research-Areas = {Food Science \& Technology},
Web-of-Science-Categories  = {Food Science \& Technology},
Author-Email = {alperkaradeniz@trabzon.edu.tr
   yukselcelik@karabuk.edu.tr
   ebasaran@agri.edu.tr},
Affiliations = {Trabzon University; Karabuk University; Agri Ibrahim Cecen University},
ResearcherID-Numbers = {CELIK, Yuksel/AAG-4712-2019
   },
ORCID-Numbers = {CELIK, Yuksel/0000-0002-7117-9736
   BASARAN, Erdal/0000-0001-8569-2998
   Karadeniz, Alper Talha/0000-0003-4165-3932},
Cited-References = {Abu Al-Haija Qasem, 2020, 2020 International Conference on Computational Science and Computational Intelligence (CSCI), P1586, DOI 10.1109/CSCI51800.2020.00293.
   Anagnostis A, 2021, COMPUT ELECTRON AGR, V182, DOI 10.1016/j.compag.2021.105998.
   Anagnostis A, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020469.
   Basaran E, 2022, SIGNAL IMAGE VIDEO P, V16, P1821, DOI 10.1007/s11760-022-02141-2.
   Beikmohammadi A, 2022, EXPERT SYST APPL, V202, DOI 10.1016/j.eswa.2022.117470.
   Celik Y, 2022, SIGNAL IMAGE VIDEO P, V16, P1135, DOI 10.1007/s11760-021-02094-y.
   Dobrescu A, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.00141.
   Dong YB, 2019, REMOTE SENS LETT, V10, P1095, DOI 10.1080/2150704X.2019.1650982.
   Erdal E., 2019, DICLE NIVERSITESI M, V10, P841.
   Esteki M, 2017, CHEMOMETR INTELL LAB, V171, P251, DOI 10.1016/j.chemolab.2017.10.014.
   Firildik K., 2019, ANATOL J COMPUT SCI, V4, P88.
   Moreno-Torres JG, 2012, IEEE T NEUR NET LEAR, V23, P1304, DOI 10.1109/TNNLS.2012.2199516.
   Hammadi WQ, 2022, PROCEEDING OF THE 2ND 2022 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND SOFTWARE ENGINEERING (CSASE 2022), P218, DOI 10.1109/CSASE51777.2022.9759610.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Japkowicz N., 2011, EVALUATING LEARNING.
   Jun L. U., 2017, 2 INT C TEST MEAS CO, P276.
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016.
   KARADENIZ, 2011, ORDU UNIVERSITESI BI, V1, P65.
   Khaire UM, 2022, J KING SAUD UNIV-COM, V34, P1060, DOI 10.1016/j.jksuci.2019.06.012.
   Khalesi S., 2012, Modern Applied Science, V6, P43.
   Khan MA, 2021, INTELL AUTOM SOFT CO, V30, P771, DOI 10.32604/iasc.2021.018039.
   Kulkarni K, 2013, ACTA CRYSTALLOGR D, V69, P2236, DOI 10.1107/S0907444913018593.
   Ozbay FA, ADIYAMAN UNIVERSITES, V9, P88.
   OZDET B, ULUDAG UNIVERSITESI, V27, P135.
   Refaeilzadeh P., 2009, ENCY DATABASE SYST, V5, P532, DOI {[}DOI 10.1007/978-1-4899-7993-3\_565-2, 10.1007/978-0-387-39940-9\_565].
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002.
   Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1609/AAAI.V31I1.11231.
   Tan JW, 2020, IEEE ACM T COMPUT BI, V17, P82, DOI 10.1109/TCBB.2018.2848653.
   Togacar M, 2019, FIRAT NIVERSITESI MU, V31, P223.
   Yang B, 2020, MICROPROCESSORS, DOI DOI 10.1016/J.MICPRO.2020.103398.
   Yu Han Liu, 2018, Journal of Physics: Conference Series, V1087, DOI 10.1088/1742-6596/1087/6/062032.
   Zhang YP, 2021, HORTIC RES-ENGLAND, V8, DOI 10.1038/s41438-021-00608-w.
   Zhao WG, 2019, KNOWL-BASED SYST, V163, P283, DOI 10.1016/j.knosys.2018.08.030.
   Zhao WG, 2019, FUTURE GENER COMP SY, V91, P601, DOI 10.1016/j.future.2018.05.037.},
Number-of-Cited-References = {34},
Times-Cited = {0},
Usage-Count-Last-180-days = {9},
Usage-Count-Since-2013 = {19},
Journal-ISO = {Eur. Food Res. Technol.},
Doc-Delivery-Number = {9P3UE},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000881617300001},
DA = {2023-08-12},
}

@inproceedings{ WOS:000668692000068,
Author = {Chiuyari Veramendi, Wilbur N. and Cruvinel, Paulo E.},
Book-Group-Author = {IEEE},
Title = {Algorithm For the Countering Maize Plants Based On UAV, Digital Image
   Processing and Semantic Modeling},
Booktitle = {2021 IEEE 15TH INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING (ICSC
   2021)},
Series = {IEEE International Conference on Semantic Computing},
Year = {2021},
Pages = {393-397},
Note = {15th IEEE International Conference on Semantic Computing (ICSC), ELECTR
   NETWORK, JAN 27-29, 2021},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {With the need to increase agricultural production and to avoid loss,
   this paper presents the development of a new method for counting plants
   of maize in an agricultural field using spectral images obtained by an
   UAV, as well as digital processing and semantic modeling techniques. The
   method is based on the use of the Circular Hough Transform (CHT) in
   conjunction with the techniques of Backmapping, neighborhood analysis,
   and a classification of patterns. Both the supper vector machines (SVM)
   and the neural networks (NN) methods have been evaluated for the
   classification procedure. Besides, using a computational environment for
   simulation, previous results have been obtained, i.e., showing not only
   the usefulness of the direct measures but also an automatic way for the
   plants identification, counting and height determination of the planted
   maize. Also, the establishment of a friendly interface has been carried
   out, which allows the monitoring of the phenological phases involved in
   the stages of the maize cultivation.},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Veramendi, WNC (Corresponding Author), Embrapa Instrumentat, Sao Carlos, SP, Brazil.
   Veramendi, WNC (Corresponding Author), Univ Fed Sao Carlos, Post Grad Program Comp Sci, Sao Carlos, SP, Brazil.
   Chiuyari Veramendi, Wilbur N.; Cruvinel, Paulo E., Embrapa Instrumentat, Sao Carlos, SP, Brazil.
   Chiuyari Veramendi, Wilbur N.; Cruvinel, Paulo E., Univ Fed Sao Carlos, Post Grad Program Comp Sci, Sao Carlos, SP, Brazil.},
DOI = {10.1109/ICSC50631.2021.00072},
ISSN = {2325-6516},
ISBN = {978-1-7281-8899-7},
Keywords = {Image Processing; Semantic Decision Making; Hough Transform; UAVs; Risk
   Management; Pest Control},
Keywords-Plus = {NETWORK},
Research-Areas = {Computer Science},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence},
Author-Email = {wilbur.chiuyari@estudante.ufscar.br
   paulo.cruvinel@embrapa.br},
Affiliations = {Empresa Brasileira de Pesquisa Agropecuaria (EMBRAPA); Universidade
   Federal de Sao Carlos},
Funding-Acknowledgement = {Sao Paulo Research Foundation (FAPESP) {[}17/19350-2]; National Council
   for Scientific and Technological Development (CNPq); Fundacao de Amparo
   a Pesquisa do Estado de Sao Paulo (FAPESP) {[}17/19350-2] Funding
   Source: FAPESP},
Funding-Text = {This research was partially supported by the Sao Paulo Research
   Foundation (FAPESP, Process No. 17/19350-2), and the National Council
   for Scientific and Technological Development (CNPq - Process No.). We
   thank the institutional support received from the Brazilian Corporation
   for Agricultural Research (Embrapa) and the Computer Science Department
   of the Federal University of Sao Carlos (UFSCar).},
Cited-References = {Ahmad I, 2018, J INDIAN SOC REMOTE, V46, P1701, DOI 10.1007/s12524-018-0825-8.
   Arroyo JA, 2017, 2017 IEEE MEXICAN HUMANITARIAN TECHNOLOGY CONFERENCE (MHTC), P137, DOI 10.1109/MHTC.2017.8006410.
   Bah MD, 2020, IEEE ACCESS, V8, P5189, DOI 10.1109/ACCESS.2019.2960873.
   Chen Yayong, 2019, Sensors (Basel), V19, DOI 10.3390/s19245558.
   Dubbini M., 2017, Agricultural Engineering International: CIGR Journal, V19, P87.
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242.
   Fan Z, 2018, IEEE J-STARS, V11, P876, DOI 10.1109/JSTARS.2018.2793849.
   Food and Agriculture Organization, 2020, The state of food security and nutrition in the world 2020: transforming food systems for affordable healthy diets, DOI 10.4060/ca9692en.
   Freire T. L, 2020, MILHO ANALISE MENSAL.
   Garcia-Martinez H, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10040469.
   Gnadinger F, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9060544.
   Hall O, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2030022.
   Hasituya, 2020, INT J REMOTE SENS, V41, P7757, DOI 10.1080/01431161.2020.1763510.
   Hough P. V. C., 1959, MACHINE ANAL BUBBLE, V73, P554.
   Kestur R, 2018, J INDIAN SOC REMOTE, V46, P991, DOI 10.1007/s12524-018-0756-4.
   Kitano B.T., 2019, IEEE GEOSCI REMOTE S, P1, DOI {[}DOI 10.1109/LGRS.2019.2930549, 10.1109/LGRS.2019.2930549].
   Koc-San D, 2018, COMPUT ELECTRON AGR, V150, P289, DOI 10.1016/j.compag.2018.05.001.
   Kumar J. P., 2019, Information Processing in Agriculture, V6, P233.
   Laborde D, 2020, SCIENCE, V369, P500, DOI 10.1126/science.abc4765.
   Li H, 2016, PRECIS AGRIC, V17, P678, DOI 10.1007/s11119-016-9443-z.
   Liu SY, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.00739.
   Lu B, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12162659.
   Marsujitullah, 2019, J PHYS CONF SER, V1198, DOI 10.1088/1742-6596/1198/9/092001.
   Mukhopadhyay P, 2015, PATTERN RECOGN, V48, P993, DOI 10.1016/j.patcog.2014.08.027.
   Natu A.S., 2016, INT J RECENT INNOVAT, V4, P563.
   Parlange R., 2018, 10 INT MICROAIR VEHI, P283.
   Perez-Liva M., 2016, 2016 Global Medical Engineering Physics Exchanges/Pan-American Health Care Exchanges (GMEPE/PAHCE), P1, DOI 10.1109/GMEPE-PAHCE.2016.7504651.
   Popp J., 2013, Food and Nutrition Sciences, V4, P8.
   Qiao L, 2019, IFAC PAPERSONLINE, V52, P330, DOI 10.1016/j.ifacol.2019.12.561.
   Su J., 2019, UNMANNED SYST, V08.
   Su JY, 2020, UNMANNED SYST, V8, P71, DOI 10.1142/S2301385020500053.
   Wang YR, 2019, INT J REMOTE SENS, V40, P7356, DOI 10.1080/01431161.2018.1513669.},
Number-of-Cited-References = {32},
Times-Cited = {0},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BR7OX},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000668692000068},
DA = {2023-08-12},
}

@article{ WOS:000733318100002,
Author = {Wilf, Peter and Wing, Scott L. and Meyer, Herbert W. and Rose, Jacob A.
   and Saha, Rohit and Serre, Thomas and Cuneo, N. Ruben and Donovan,
   Michael P. and Erwin, Diane M. and Gandolfo, Maria A. and Gonzalez-Akre,
   Erika and Herrera, Fabiany and Hu, Shusheng and Iglesias, Ari and
   Johnson, Kirk R. and Karim, Talia S. and Zou, Xiaoyu},
Title = {An image dataset of cleared, x-rayed, and fossil leaves vetted to plant
   family for human and machine learning},
Journal = {PHYTOKEYS},
Year = {2021},
Number = {187},
Pages = {93-128},
Month = {DEC 16},
Abstract = {Leaves are the most abundant and visible plant organ, both in the modern
   world and the fossil record. Identifying foliage to the correct plant
   family based on leaf architecture is a fundamental botanical skill that
   is also critical for isolated fossil leaves, which often, especially in
   the Cenozoic, represent extinct genera and species from extant families.
   Resources focused on leaf identification are remarkably scarce; however,
   the situation has improved due to the recent proliferation of digitized
   herbarium material, live plant identification applications, and online
   collections of cleared and fossil leaf images. Nevertheless, the need
   remains for a specialized image dataset for comparative leaf
   architecture. We address this gap by assembling an open-access database
   of 30,252 images of vouchered leaf specimens vetted to family level,
   primarily of angiosperms, including 26,176 images of cleared and x-rayed
   leaves representing 354 families and 4,076 of fossil leaves from 48
   families. The images maintain original resolution, have user-friendly
   filenames, and are vetted using APG and modern paleobotanical standards.
   The cleared and x-rayed leaves include the Jack A. Wolfe and Leo J.
   Hickey contributions to the National Cleared Leaf Collection and a
   collection of high-resolution scanned x-ray negatives, housed in the
   Division of Paleobotany, Department of Paleobiology, Smithsonian
   National Museum of Natural History, Washington D.C.; and the Daniel I.
   Axelrod Cleared Leaf Collection, housed at the University of California
   Museum of Paleontology, Berkeley. The fossil images include a sampling
   of Late Cretaceous to Eocene paleobotanical sites from the Western
   Hemisphere held at numerous institutions, especially from Florissant
   Fossil Beds National Monument (late Eocene, Colorado), as well as
   several other localities from the Late Cretaceous to Eocene of the
   Western USA and the early Paleogene of Colombia and southern Argentina.
   The dataset facilitates new research and education opportunities in
   paleobotany, comparative leaf architecture, systematics, and machine
   learning.},
Publisher = {PENSOFT PUBLISHERS},
Address = {12 PROF GEORGI ZLATARSKI ST, SOFIA, 1700, BULGARIA},
Type = {Article},
Language = {English},
Affiliation = {Wilf, P (Corresponding Author), Penn State Univ, Dept Geosci, University Pk, PA 16802 USA.
   Wilf, P (Corresponding Author), Penn State Univ, Earth \& Environm Syst Inst, University Pk, PA 16802 USA.
   Wilf, Peter; Zou, Xiaoyu, Penn State Univ, Dept Geosci, University Pk, PA 16802 USA.
   Wilf, Peter; Zou, Xiaoyu, Penn State Univ, Earth \& Environm Syst Inst, University Pk, PA 16802 USA.
   Wing, Scott L.; Johnson, Kirk R., Smithsonian Inst, Dept Paleobiolop, Washington, DC 20013 USA.
   Meyer, Herbert W., Natl Pk Serv, Florissant Fossil Beds Natl Monument, Florissant, CO 80816 USA.
   Rose, Jacob A., Brown Univ, Sch Engn, Providence, RI 02912 USA.
   Saha, Rohit; Serre, Thomas, Brown Univ, Carney Inst Brain Sci, Dept Cognit Linguist \& Psychol Sci, Providence, RI 02912 USA.
   Cuneo, N. Ruben, CONICET Museo Paleontol Egidio Feruglio, RA-9100 Trelew, Chubut, Argentina.
   Donovan, Michael P., Cleveland Museum Nat Hist, Dept Paleobot \& Paleoecol, Cleveland, OH 44106 USA.
   Erwin, Diane M., Univ Calif Berkeley, Museum Paleontol, Berkeley, CA 94720 USA.
   Gandolfo, Maria A., Cornell Univ, Sch Integrat Plant Sci, LH Bailey Hortorium, Plant Biol Sect, Ithaca, NY 14853 USA.
   Gonzalez-Akre, Erika, Natl Zool Pk, Smithsonian Conservat Biol Inst, Conservat Ecol Ctr, Front Royal, VA 22630 USA.
   Herrera, Fabiany, Field Museum Nat Hist, Negaunee Integrat Res Ctr, Chicago, IL 60605 USA.
   Hu, Shusheng, Yale Univ, Peabody Museum Nat Hist, Div Paleobot, New Haven, CT 06520 USA.
   Iglesias, Ari, CONICET UNComa, Inst Invest Biodiversidad \& Ambiente INIBIOMA, RA-8400 San Carlos De Bariloche, Rio Negro, Argentina.
   Karim, Talia S., Univ Colorado, Museum Nat Hist, Boulder, CO 80503 USA.},
DOI = {10.3897/phytokeys.187.72350},
ISSN = {1314-2011},
EISSN = {1314-2003},
Keywords = {Angiosperms; cleared leaves; data science; fossil leaves; leaf
   architecture; paleobotany},
Keywords-Plus = {GREEN RIVER FORMATION; NORTH-AMERICA; LATE PALEOCENE; LEAF ARCHITECTURE;
   MIDDLE EOCENE; NORDENSKIOLDIA TROCHODENDRACEAE; INSECT ASSOCIATIONS;
   CERREJON FORMATION; WASHINGTON-STATE; FLORISSANT FLORA},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {pwilf@psu.edu},
Affiliations = {Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Pennsylvania State University -
   University Park; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Pennsylvania State University; Pennsylvania State University -
   University Park; Smithsonian Institution; United States Department of
   the Interior; Brown University; Brown University; Consejo Nacional de
   Investigaciones Cientificas y Tecnicas (CONICET); Cleveland Museum of
   Natural History; University of California System; University of
   California Berkeley; Cornell University; Smithsonian Institution;
   Smithsonian National Zoological Park \& Conservation Biology Institute;
   Field Museum of Natural History (Chicago); Yale University; Consejo
   Nacional de Investigaciones Cientificas y Tecnicas (CONICET);
   Universidad Nacional del Comahue; University of Colorado System;
   University of Colorado Boulder},
ResearcherID-Numbers = {Wing, Scott L/IYJ-5681-2023
   },
ORCID-Numbers = {Wing, Scott L/0000-0002-2954-8905
   Wilf, Peter/0000-0001-6813-1937},
Funding-Acknowledgement = {NSF {[}EAR-1925755, EAR-1925481, EAR-1925552, DEB-1556666, DEB-1556136];
   National Park Service (HWM)},
Funding-Text = {Funding for this work came from NSF grants EAR-1925755, EAR-1925481, and
   EAR-1925552 (PW, TS, MAG, and others); DEB-1556666 and DEB-1556136 (PW,
   MAG, and others); and the National Park Service (HWM).},
Cited-References = {{[}Anonymous], 2003, FOSSILFLORISSANT.
   {[}Anonymous], 1999, FLORA RESERVA DUCKE.
   {[}Anonymous], U CALIF PUBL GEOL SC.
   AXELROD DI, 1986, ANN MO BOT GARD, V73, P565, DOI 10.2307/2399194.
   Bakker FT, 2020, PEERJ, V8, DOI 10.7717/peerj.8225.
   Belongie R., 2021, ARXIV210513808.
   Blonder B, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2844.
   Bonnet P, 2018, MULTIMED SYST APPL, P131, DOI 10.1007/978-3-319-76445-0\_8.
   Borsch T, 2020, TAXON, V69, P1311, DOI 10.1002/tax.12373.
   Brea M, 2008, ALCHERINGA, V32, P427, DOI 10.1080/03115510802417695.
   Byng JW, 2016, BOT J LINN SOC, V181, P1, DOI 10.1111/boj.12385.
   CALL VB, 1994, REV PALAEOBOT PALYNO, V80, P305, DOI 10.1016/0034-6667(94)90007-8.
   Cariglino B., 2007, THESIS PENNSYLVANIA.
   Carpenter RJ, 2014, PALAEONTOL ELECTRON, V17.
   Carranza-Rojas J, 2018, MULTIMED SYST APPL, P151, DOI 10.1007/978-3-319-76445-0\_9.
   Carvalho MR, 2021, INT J PLANT SCI, V182, P401, DOI 10.1086/714053.
   Carvalho MR, 2021, SCIENCE, V372, P63, DOI 10.1126/science.abf1969.
   Carvalho MR, 2011, AM J BOT, V98, P1337, DOI 10.3732/ajb.1000539.
   Christophel D. C., 1996, Leaf and cuticle atlas of Australian leafy Lauraceae..
   Christophel D.C., 1993, LEAF ATLAS AUSTR TRO.
   Clyde WC, 2014, GEOL SOC AM BULL, V126, P289, DOI 10.1130/B30915.1.
   Cockerell TDA, 1908, B AM MUS NAT HIST, V24, P71.
   CRANE PR, 1991, AM J BOT, V78, P1311, DOI 10.2307/2445271.
   Das A, 2014, PLANT METHODS, V10, DOI 10.1186/1746-4811-10-8.
   Denk T, 2005, CAN J BOT, V83, P1663, DOI 10.1139/b05-122.
   DeVore ML, 2007, PLANT SYST EVOL, V266, P45, DOI 10.1007/s00606-007-0540-3.
   DeVore ML, 2005, CAN J EARTH SCI, V42, P205, DOI 10.1139/E04-072.
   DILCHER DL, 1974, BOT REV, V40, P1, DOI 10.1007/BF02860067.
   Dilcher DL., 1971, PALAEOBOTANIST, V20, P7.
   Donovan MP, 2017, NAT ECOL EVOL, V1, DOI 10.1038/s41559-016-0012.
   Donovan MP, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0103542.
   Doria G, 2008, AM J BOT, V95, P954, DOI 10.3732/ajb.2007216.
   Doweld AB, 2016, PHYTOTAXA, V273, P191, DOI 10.11646/phytotaxa.273.3.6.
   Doyle JA, 2007, COUR FOR SEKENBG, V258, P21.
   Ellis B., 2009, MANUAL LEAF ARCHITEC, V1.
   Evanoff Emmett, 2001, Proceedings of the Denver Museum of Nature \& Science, V4, P1.
   Flynn S, 2019, INT J PLANT SCI, V180, P464, DOI 10.1086/703526.
   FOSTER AS, 1952, AM J BOT, V39, P752, DOI 10.2307/2438624.
   Fuller DQ, 2005, BOT REV, V71, P295, DOI 10.1663/0006-8101(2005)071{[}0295:SALAOT]2.0.CO;2.
   GANDOLFO MA, 1988, B TORREY BOT CLUB, V115, P83, DOI 10.2307/2996138.
   GANDOLFO MA, 1992, B TORREY BOT CLUB, V119, P152, DOI 10.2307/2997028.
   Gandolfo MA, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0021084.
   Garcia-Gutierrez E, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11391.
   Gemmill CEC, 1997, PALAIOS, V12, P439, DOI 10.2307/3515382.
   Gentry A., 1993, FIELD GUIDE FAMILIES.
   Gomez-Navarro C, 2009, AM J BOT, V96, P1300, DOI 10.3732/ajb.0800378.
   Gonzalez CC, 2007, BOT REV, V73, P235, DOI 10.1663/0006-8101(2007)73{[}235:ROTPMR]2.0.CO;2.
   Green WA, 2014, APPL PLANT SCI, V2, DOI 10.3732/apps.1400006.
   Greenwood DR, 2016, CAN J EARTH SCI, V53, P548, DOI 10.1139/cjes-2015-0177.
   Herendeen PS, 2019, INT J PLANT SCI, V180, P220, DOI 10.1086/701468.
   Hermsen EJ, 2013, BOT REV, V79, P1, DOI 10.1007/s12229-012-9114-3.
   Hermsen EJ, 2012, AM J BOT, V99, P1356, DOI 10.3732/ajb.1200025.
   Herrera F, 2019, AUST SYST BOT, V32, P385, DOI 10.1071/SB19001.
   Herrera FA, 2008, AM J BOT, V95, P1569, DOI 10.3732/ajb.0800172.
   HICKEY L. J., 1974, ANATOMY DICOTYLEDONS, V1, P25.
   HICKEY LJ, 1975, ANN MO BOT GARD, V62, P538, DOI 10.2307/2395267.
   HICKEY LJ, 1973, AM J BOT, V60, P17, DOI 10.2307/2441319.
   Hickey LJ., 1977, GEOL SOC AM MEM, V150, P1.
   HILL R S, 1982, Palaeontographica Abteilung B Palaeophytologie, V181, P44.
   Hussein BR, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21134549.
   Hussein BR, 2020, LECT NOTES ELECTR EN, V603, P321, DOI 10.1007/978-981-15-0058-9\_31.
   Iglesias A, 2007, GEOLOGY, V35, P947, DOI 10.1130/G23889A.1.
   Iglesias A, 2021, PALAEONTOL ELECTRON, V24, DOI 10.26879/1124.
   Jia H, 2014, INT J PLANT SCI, V175, P601, DOI 10.1086/675693.
   Johnson K.R., 1996, PROC DENVER MUS NAT, V3, P1.
   Johnson K. R., 2002, GEOLOGICAL SOC AM SP, V361, P329, DOI DOI 10.1130/0-8137-2361-2.329.
   JOHNSON KR, 1989, NATURE, V340, P708, DOI 10.1038/340708a0.
   Johnson KR, 1995, GREEN RIVER FORMATIO, P121.
   JONES JH, 1986, ANN MO BOT GARD, V73, P228, DOI 10.2307/2399112.
   JR GU, 2007, COURIER FORSCHUNGSIN, V258, P11.
   Jud NA, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0248369.
   Jud NA, 2018, AM J BOT, V105, P927, DOI 10.1002/ajb2.1092.
   Jud NA, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0176164.
   Keller, 2004, IDENTIFICATION TROPI, DOI {[}10.1007/978-3-0348-7905-7, DOI 10.1007/978-3-0348-7905-7].
   Kellner A, 2012, INT J PLANT SCI, V173, P239, DOI 10.1086/663965.
   Kindt R, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11388.
   Klucking EP, 1986, LEAF VENATION PATTER, V1.
   Knight CL, 2013, PALAEONTOL ELECTRON, V16.
   Krogh D, 1998, U CALIFORNIA MEMORIA, P8.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Leopold EB, 2008, GEOL SOC AM SPEC PAP, V435, P53.
   Lesquereux L., 1883, CONTRIBUTIONS FOSSIL.
   Lesquereux L, 1873, 6 ANN REPORT US GEOL, P317, DOI 10.3133/70038930.
   MACGINITIE H D, 1974, University of California Publications in Geological Sciences, V108, P1.
   MacGINITIE HARRY D., 1953, CARNEGIE INST WASHINGTON PUBL, V599, P1.
   MANCHESTER S.R., 1989, EVOLUTION SYSTEMATIC, V2, P221.
   MANCHESTER SR, 1987, BOT GAZ, V148, P263, DOI 10.1086/337654.
   Manchester SR, 1996, INT J PLANT SCI, V157, P644, DOI 10.1086/297386.
   Manchester SR, 2004, CAN J BOT, V82, P1509, DOI {[}10.1139/b04-112, 10.1139/B04-112].
   Manchester SR, 1999, INT J PLANT SCI, V160, P188, DOI 10.1086/314114.
   Manchester SR, 2002, INT J PLANT SCI, V163, P725, DOI 10.1086/341513.
   Manchester SR, 2002, SYST BOT, V27, P368.
   MANCHESTER SR, 1983, AM J BOT, V70, P1147, DOI 10.2307/2443285.
   Manchester SR, 1998, REV PALAEOBOT PALYNO, V102, P153, DOI 10.1016/S0034-6667(97)00056-0.
   Manchester SR, 2001, INT J PLANT SCI, V162, P985, DOI 10.1086/320783.
   Manchester SR, 1997, AM J BOT, V84, P649, DOI 10.2307/2445902.
   MANCHESTER SR, 1989, AM J BOT, V76, P256, DOI 10.2307/2444668.
   MANCHESTER SR, 1986, BOT GAZ, V147, P200, DOI 10.1086/337587.
   Manchester SR., 2001, P DENVER MUSEUM NATU, V4, P137.
   Manchester SR., 1987, MONOGR SYST BOT MISS, V21, P1.
   Manchester SR, 2006, INT J PLANT SCI, V167, P897, DOI 10.1086/503918.
   Manchester SR, 2018, INT J PLANT SCI, V179, P663, DOI 10.1086/699282.
   Manchester Steven R., 2014, Sbornik Narodniho Muzea v Praze Rada B Prirodni Vedy, V70, P153, DOI 10.14446/AMNP.2014.153.
   Manchester SR, 2009, INT J PLANT SCI, V170, P132, DOI 10.1086/593040.
   Martínez-Millán Marcela, 2005, Rev. Mex. Biodiv., V76, P137.
   McClain AM, 2001, AM J BOT, V88, P1316, DOI 10.2307/3558343.
   MCIVER E.E., 1993, PALAEONTOGR CANAD, V10, P1.
   Merkhofer L, 2015, AM J BOT, V102, P1160, DOI 10.3732/ajb.1500159.
   Meyer, 2012, SAVED TIME FIGHT EST.
   Meyer HW, 2008, GEOL SOC AM SPEC PAP, V435, P159, DOI 10.1130/2008.2435(11).
   Meyer HW, 1997, U CALIF PUBL GEOL SC, V141, P1.
   Peppe DJ, 2011, NEW PHYTOL, V190, P724, DOI 10.1111/j.1469-8137.2010.03615.x.
   Pigg KB, 2001, INT J PLANT SCI, V162, P1187, DOI 10.1086/321927.
   Premoli AC, 1996, BOT J LINN SOC, V121, P25, DOI 10.1111/j.1095-8339.1996.tb00743.x.
   Royer DL, 2007, PALEOBIOLOGY, V33, P574, DOI 10.1666/07001.1.
   Schorn HE, 1998, PALEOBIOS, V18, P21.
   Smith ME, 2008, GEOL SOC AM BULL, V120, P54, DOI 10.1130/B26073.1.
   Stiles E, 2020, PALEOBIOLOGY, V46, P445, DOI 10.1017/pab.2020.45.
   TAYLOR DW, 1992, PLANT SYST EVOL, V180, P137, DOI 10.1007/BF00941148.
   TODZIA CA, 1991, ANN MO BOT GARD, V78, P476, DOI 10.2307/2399575.
   Traiser C, 2018, PALAEONTOL ELECTRON, V21, DOI 10.26879/773.
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914.
   Vasco A, 2014, APPL PLANT SCI, V2, DOI 10.3732/apps.1400038.
   Veatch SW, 2008, GEOL SOC AM SPEC PAP, V435, P1.
   VON ETTINGSHAUSEN C. R., 1861, BLATT SKELETE DIKOTY.
   von Ettingshausen CF, 1858, NERVATION BOMBACEEN.
   Wang Q, 2013, AM J BOT, V100, P422, DOI 10.3732/ajb.1200415.
   Weaver WN, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11367.
   Wilf P, 2005, AM NAT, V165, P634, DOI 10.1086/430055.
   Wilf P, 2000, GEOL SOC AM BULL, V112, P292, DOI 10.1130/0016-7606(2000)112<0292:LPEECC>2.3.CO;2.
   Wilf P, 2005, P NATL ACAD SCI USA, V102, P8944, DOI 10.1073/pnas.0500516102.
   Wilf P, 2003, SCIENCE, V300, P122, DOI 10.1126/science.1080475.
   Wilf P, 1999, SCIENCE, V284, P2153, DOI 10.1126/science.284.5423.2153.
   Wilf P, 2004, PALEOBIOLOGY, V30, P347, DOI 10.1666/0094-8373(2004)030<0347:LPEATE>2.0.CO;2.
   Wilf P, 1998, PALAIOS, V13, P514, DOI 10.2307/3515344.
   Wilf P, 2001, P NATL ACAD SCI USA, V98, P6221, DOI 10.1073/pnas.111069498.
   Wilf P., 2008, PALEONTOL SOC PAP, V14, P319, DOI DOI 10.1017/S1089332600001741.
   Wilf P, 2006, SCIENCE, V313, P1112, DOI 10.1126/science.1129569.
   Wilf P, 2019, SCIENCE, V364, P972, DOI 10.1126/science.aaw5139.
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113.
   Wilf P, 2013, ANNU REV EARTH PL SC, V41, P561, DOI 10.1146/annurev-earth-050212-124217.
   Wing SL, 2014, B PEABODY MUS NAT HI, V55, P69, DOI 10.3374/014.055.0201.
   Wing SL, 2009, P NATL ACAD SCI USA, V106, P18627, DOI 10.1073/pnas.0905130106.
   WING SL, 1992, AM J BOT, V79, P1320, DOI 10.2307/2445060.
   Wing SL, 1998, LATE PALEOCENE-EARLY EOCENE CLIMATIC AND BIOTIC EVENTS IN THE MARINE AND TERRESTRIAL RECORDS, P380.
   WING SL, 1984, AM J BOT, V71, P388, DOI 10.2307/2443497.
   Winkler IS, 2010, J PALEONTOL, V84, P935, DOI 10.1666/09-163.1.
   WOLFE J A, 1987, Journal of the Faculty of Science Hokkaido University Series IV Geology and Mineralogy, V22, P1.
   Wolfe J.A., 1987, USGS BULL, V1597, P1.
   Wolfe JA, 1990, US GEOLOGICAL SURVEY, V1923, P1, DOI DOI 10.3133/B1923.
   Xu H, 2021, NEW PHYTOL, V229, P631, DOI 10.1111/nph.16923.},
Number-of-Cited-References = {151},
Times-Cited = {8},
Usage-Count-Last-180-days = {4},
Usage-Count-Since-2013 = {9},
Journal-ISO = {PhytoKeys},
Doc-Delivery-Number = {XT0WQ},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000733318100002},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000228690800008,
Author = {Du, P and Zhao, HJ and Zhang, B and Zheng, LF},
Editor = {Nasrabadi, NM and Rizvi, SA},
Title = {Independent component analysis for hyperspectral imagery plant
   classification},
Booktitle = {Applications of Neural Networks and Machine Learning in Image Processing
   IX},
Series = {PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)},
Year = {2005},
Volume = {5673},
Pages = {71-81},
Note = {Conference on Applications of Neural Networks and Machine Learning in
   Image Processing IX, San Jose, CA, JAN 19-20, 2005},
Organization = {Soc Imaging Sci \& Technol; SPIE},
Abstract = {In land investigation, it is often required to extract plant and
   vegetation information from land covers, especially when plants are
   sparsely dispersed. To avoid the expensive ground survey, hyperspectral
   remote sensing image is adopted due to its narrow spectral bandwidth and
   high spectral resolution. However conventional unsupervised
   classification techniques often suffer from requiring priori as input
   parameter and sensitiveness to interference. This paper proposes an
   Independent Component Analysis (ICA) based unsupervised classification
   algorithm. ICA is a technique that stems out from the Blind Source
   Separation. In hyperspectral data processing, ICA projects data vectors
   to the space where the items of the vectors are mutually statistically
   independent, and therefore is capable of extracting various kinds of
   plant information. So as to strengthen the contrast of the resulted
   independent components, histogram adjustment and mathematical morphology
   post-processing procedure are appended after ICA decomposition. Through
   real hyperspectral data experiments, our algorithm has been verified to
   have better performance for classification than K-means and ISODATA.
   Besides, computation efficiency and noise robustness have also been
   improved by a noise filtering preprocessing procedure.},
Publisher = {SPIE-INT SOC OPTICAL ENGINEERING},
Address = {1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Du, P (Corresponding Author), Beihang Univ, Beijing, Peoples R China.
   Beihang Univ, Beijing, Peoples R China.},
DOI = {10.1117/12.584356},
ISSN = {0277-786X},
ISBN = {0-8194-5646-2},
Keywords = {hyperspectral remote sensing; feature extraction; plant classification;
   independent component analysis},
Research-Areas = {Computer Science; Imaging Science \& Photographic Technology},
Web-of-Science-Categories  = {Computer Science, Artificial Intelligence; Imaging Science \&
   Photographic Technology},
Affiliations = {Beihang University},
Cited-References = {{[}Anonymous], 1998, INDEPENDENT COMPONEN.
   CHERIYADAT A, 2003, GEOSC REM SENS S 200, V6, P3420.
   Duda D., 1973, PATTERN CLASSIFICATI.
   GREEN AA, 1988, IEEE T GEOSCI REMOTE, V26, P65, DOI 10.1109/36.3001.
   HUIJIE DZ, 2005, J BEIJING U AERONAUT.
   Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483.
   Hyvarinen A., 1999, Neural Computing Surveys, V2.
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504.
   Jain AK, 1988, ALGORITHMS CLUSTERIN.
   WOOD J, 1990, MASTER DRAWINGS, V28, P3.},
Number-of-Cited-References = {10},
Times-Cited = {1},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {5},
Doc-Delivery-Number = {BCC99},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000228690800008},
DA = {2023-08-12},
}

@article{ WOS:000975166500001,
Author = {Kopec, Dominik and Zakrzewska, Agata and Halladin-Dabrowska, Anna and
   Wylazlowska, Justyna and Slawik, Lukasz},
Title = {The essence of acquisition time of airborne hyperspectral and on-ground
   reference data for classification of highly invasive annual vine
   Echinocystis lobata (Michx.) Torr. \& A. Gray},
Journal = {GISCIENCE \& REMOTE SENSING},
Year = {2023},
Volume = {60},
Number = {1},
Month = {DEC 31},
Abstract = {Invasive alien species are one of the biggest threats to biodiversity
   today. Identifying their locations are mandatory parts of the strategies
   being developed to control them. Remote sensing along with machine
   learning are already proven and effective tools for monitoring invasive
   species, especially trees, shrubs, and tall perennials. However, annual
   vine species are particularly difficult to map using remote sensing
   because of their dynamic plant growth and the movement of shoots during
   the growing season. Therefore, the phenological phase in which the data
   is acquired, and the synchronization of airborne data acquisition with
   on-ground reference data, may be key factors for correct plant
   classification. This research aimed to answer the following questions:
   (i) What is the impact of acquiring synchronized on-ground data and
   hyperspectral data in different phases of plants' phenological
   development on the annual vine IAPS (Invasive Alien Plant Species)
   classification results? (ii) How does the lack of synchronization while
   obtaining hyperspectral and on-ground data collection impact annual vine
   IAPS mapping results? (iii) Does multitemporal image fusion improve the
   results of annual vine IAPS classification? For this purpose, research
   was carried out on Echinocystis lobata, an annual vine species
   considered highly invasive in Europe. The obtained results indicate that
   the phenological phase in which the data is acquired has a very strong
   influence on the quality of the classification result. The period of
   flowering (summer) with the greatest coverage of the area with shoots
   was optimal for the classification of Echinocystis lobata with F1
   classification accuracy of 0.87 +/- 0.04. The accuracy of
   classifications was significantly less for spring (F1 = 0.64 +/- 0.04)
   and autumn (F1 = 0.75 +/- 0.05). Obtaining on-ground reference data that
   is mismatched temporally with hyperspectral data causes a decrease in
   the accuracy of the result up to 0.08 (from F1 = 0.64 to 0.56) in
   relation to data obtained synchronously. In the multitemporal image
   fusion method, using hyperspectral data linked from different phases of
   plants' development to classify an image had a minimal improvement in
   classification accuracy compared to classifications trained on images
   from one phenological stage. The main conclusion is that mapping an
   annual vine using remote sensing and machine learning is possible and
   highly effective, provided the remote sensing and on-ground data are
   obtained in strict synchronization and the appropriate phenological
   phase. For the most efficient classification results, a single data
   acquisition per year is enough, even in the case of annual vine IAPS.
   Further research is needed to explore the possibility of mapping
   Echinocystis lobata using, i.e. multispectral or hyperspectral satellite
   data (e.g. EnMAP).},
Publisher = {TAYLOR \& FRANCIS LTD},
Address = {2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Kopec, D (Corresponding Author), Univ Lodz, Fac Biol \& Environm Protect, Dept Biogeog Paleoecol \& Nat Conservat, Lodz, Poland.
   Kopec, Dominik; Zakrzewska, Agata, Univ Lodz, Fac Biol \& Environm Protect, Dept Biogeog Paleoecol \& Nat Conservat, Lodz, Poland.
   Kopec, Dominik; Halladin-Dabrowska, Anna; Wylazlowska, Justyna; Slawik, Lukasz, MGGP Aero sp Z o o, Dept Remote Sensing, Tarnow, Poland.},
DOI = {10.1080/15481603.2023.2204682},
Article-Number = {2204682},
ISSN = {1548-1603},
EISSN = {1943-7226},
Keywords = {plant species classification; multitemporal data fusion; random forest;
   time series; plant phenology; machine learning},
Keywords-Plus = {RIPARIAN BUFFER STRIPS; MULTISPECTRAL IMAGERY; ACCURACY ASSESSMENT;
   CHLOROPHYLL CONTENT; FEATURE-SELECTION; PLANTS; IDENTIFICATION;
   REFLECTANCE; SATELLITE; PATCHES},
Research-Areas = {Physical Geography; Remote Sensing},
Web-of-Science-Categories  = {Geography, Physical; Remote Sensing},
Author-Email = {dominik.kopec@biol.uni.lodz.pl},
Affiliations = {University of Lodz},
ORCID-Numbers = {Kopec, Dominik/0000-0003-0831-2992},
Funding-Acknowledgement = {Polish National Centre for Research and Development (NCBR); MGGP Aero
   under the program ``Natural Environment, Agriculture and Forestry{''}
   BIOSTRATEG II: The innovative approach supporting monitoring of
   non-forest Natura 2000 habitats, using remote sensing methods (HabitARS)
   {[}DZP/BIOSTRATEG-II/390/2015]},
Funding-Text = {The work was co-financed by the Polish National Centre for Research and
   Development (NCBR) and MGGP Aero under the program ``Natural
   Environment, Agriculture and Forestry{''} BIOSTRATEG II: The innovative
   approach supporting monitoring of non-forest Natura 2000 habitats, using
   remote sensing methods (HabitARS); project number
   DZP/BIOSTRATEG-II/390/2015. The consortium leader is MGGP Aero. The
   project partners include University of Lodz, University of Warsaw,
   Warsaw University of Life Sciences, Institute of Technology and Life
   Sciences, University of Silesia in Katowice, and Warsaw University of
   Technology.},
Cited-References = {Aldakheel YY, 1997, INT J REMOTE SENS, V18, P3683, DOI 10.1080/014311697216883.
   Andrew ME, 2008, REMOTE SENS ENVIRON, V112, P4301, DOI 10.1016/j.rse.2008.07.016.
   {[}Anonymous], 2004, 20 C INT SOC PHOTOGR.
   Botta-Dukat Z., 2008, MOST IMPORTANT INVAS.
   Bradley BA, 2014, BIOL INVASIONS, V16, P1411, DOI 10.1007/s10530-013-0578-9.
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324.
   Chance CM, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01528.
   Chen Li, 2014, Journal of Zhejiang A\&F University, V31, P185.
   Cheng YB, 2007, J APPL REMOTE SENS, V1, DOI 10.1117/1.2749266.
   Cooksey D., 1997, Rangelands, V19, P20.
   Dai J, 2020, REMOTE SENS ENVIRON, V250, DOI 10.1016/j.rse.2020.112037.
   Datt B, 1999, J PLANT PHYSIOL, V154, P30, DOI 10.1016/S0176-1617(99)80314-9.
   Dudley KL, 2015, REMOTE SENS ENVIRON, V167, P121, DOI 10.1016/j.rse.2015.05.004.
   European Commission, 2014, REG EU NO 1143 2014.
   European Commission Green Infrastructure (GI)-Enhancing Europe's Natural Capital, 2013, COMM COMM EUR PARL C, P11.
   Evangelista PH, 2009, REMOTE SENS-BASEL, V1, P519, DOI 10.3390/rs1030519.
   Forysiak J, 2012, ACTA GEOGRAPHICA LOD, V99, P1.
   Gholizadeh H, 2022, REMOTE SENS ENVIRON, V271, DOI 10.1016/j.rse.2022.112887.
   Gibson KD, 2004, WEED TECHNOL, V18, P742, DOI 10.1614/WT-03-170R1.
   Gitelson AA, 1997, INT J REMOTE SENS, V18, P2691, DOI 10.1080/014311697217558.
   Grasic M, 2019, WATER-SUI, V11, DOI 10.3390/w11112395.
   Gray CJ, 2008, WEED TECHNOL, V22, P713, DOI 10.1614/WT-07-116.1.
   GREEN AA, 1988, IEEE T GEOSCI REMOTE, V26, P65, DOI 10.1109/36.3001.
   Grundy A., 2004, Q REV BIOL, V79, P314, DOI {[}10.1086/425794, DOI 10.1086/425794].
   Guanter L, 2015, REMOTE SENS-BASEL, V7, P8830, DOI 10.3390/rs70708830.
   Hamouz P, 2008, J PLANT DIS PROTECT, P167.
   Hesketh M, 2012, REMOTE SENS ENVIRON, V118, P73, DOI 10.1016/j.rse.2011.11.005.
   Hestir EL, 2008, REMOTE SENS ENVIRON, V112, P4034, DOI 10.1016/j.rse.2008.01.022.
   Huang CY, 2009, SENSORS-BASEL, V9, P4869, DOI 10.3390/s90604869.
   Hulme P. E., 2009, THEOR MATH PHYS SER, DOI {[}10.1007/978-1-4020-8280-1, DOI 10.1007/978-1-4020-8280-1].
   Hulme PE, 2009, SCIENCE, V324, P40, DOI 10.1126/science.1171111.
   Hunt ER, 2007, J APPL REMOTE SENS, V1, DOI 10.1117/1.2536275.
   Hunt ER, 2004, WEED SCI, V52, P492, DOI 10.1614/WS-03-132R.
   de Castro AI, 2012, PRECIS AGRIC, V13, P302, DOI 10.1007/s11119-011-9247-0.
   Khare S, 2019, ECOL INDIC, V106, DOI 10.1016/j.ecolind.2019.105520.
   Khare S, 2018, GEOCARTO INT, V33, P681, DOI 10.1080/10106049.2017.1289562.
   Kolaczkowska E., 2016, METHODS CONTROL ALIE, P37.
   Kopec D, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19132871.
   Kopec D, 2014, FOREST ECOL MANAG, V314, P120, DOI 10.1016/j.foreco.2013.11.033.
   Kostrakiewicz-Gieralt K, 2022, FORESTS, V13, DOI 10.3390/f13020256.
   Labonte J, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12060922.
   Liang WW, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12040609.
   Lopez-Granados F, 2006, WEED SCI, V54, P346.
   Marcinkowska-Ochtyra A, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11192264.
   MARSHALL TR, 1994, J AQUAT PLANT MANAGE, V32, P61.
   Maxwell AE, 2018, INT J REMOTE SENS, V39, P2784, DOI 10.1080/01431161.2018.1433343.
   McCracken DI, 2012, J ENVIRON QUAL, V41, P355, DOI 10.2134/jeq2010.0532.
   Meerdink SK, 2019, REMOTE SENS ENVIRON, V232, DOI 10.1016/j.rse.2019.111308.
   Michez A, 2016, INT J APPL EARTH OBS, V44, P88, DOI 10.1016/j.jag.2015.06.014.
   Mullerova J, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.00887.
   Mullerova J, 2016, INT ARCH PHOTOGRAMM, V41, P903, DOI 10.5194/isprsarchives-XLI-B7-903-2016.
   Niphadkar M, 2016, INT J REMOTE SENS, V37, P3074, DOI 10.1080/01431161.2016.1193795.
   Noujdina NV, 2008, WEED SCI, V56, P173, DOI 10.1614/WS-07-009.1.
   Pejchar L, 2009, TRENDS ECOL EVOL, V24, P497, DOI 10.1016/j.tree.2009.03.016.
   Pena-Barragan JM, 2007, WEED RES, V47, P164, DOI 10.1111/j.1365-3180.2007.00553.x.
   Peters F., 2016, ANAL PHENOLOGIC DYNA, DOI {[}10.18297/etd/2437, DOI 10.18297/ETD/2437].
   Protopopova V. V., 2015, Biodiversity: Research and Conservation, V39, P7.
   Protopopova V. V., 2014, BIODIVERSITY RES CON, V35, P31, DOI {[}DOI 10.2478/BIORC-2014-0018, 10.2478/biorc-2014-0018].
   Pysek P, 2010, ANNU REV ENV RESOUR, V35, P25, DOI 10.1146/annurev-environ-033009-095548.
   Rai PK, 2020, ECOL INDIC, V111, DOI 10.1016/j.ecolind.2019.106020.
   Richardson DM, 2012, NEW PHYTOL, V196, P383, DOI 10.1111/j.1469-8137.2012.04292.x.
   Richter R., 2016, ATCOR 4 USER GUIDE.
   Rocchini D, 2015, PROG PHYS GEOG, V39, P283, DOI 10.1177/0309133315574659.
   Roelfsema CM, 2014, REMOTE SENS ENVIRON, V150, P172, DOI 10.1016/j.rse.2014.05.001.
   Rudnicki WR, 2006, LECT NOTES ARTIF INT, V4259, P557.
   Sabat-Tomala A, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14010064.
   Schlapfer D, 2002, INT J REMOTE SENS, V23, P2609, DOI 10.1080/01431160110115825.
   SILVERTOWN J, 1985, J ECOL, V73, P841, DOI 10.2307/2260151.
   Singh KK, 2018, ISPRS J PHOTOGRAMM, V142, P151, DOI 10.1016/j.isprsjprs.2018.05.023.
   Singh N, 2009, INT J REMOTE SENS, V30, P3441, DOI 10.1080/01431160802562222.
   Slawik L, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11080970.
   Somers B, 2013, REMOTE SENS ENVIRON, V136, P14, DOI 10.1016/j.rse.2013.04.006.
   Somers B, 2012, REMOTE SENS-BASEL, V4, P2510, DOI 10.3390/rs4092510.
   Somodi I, 2012, BIOL CONSERV, V150, P59, DOI 10.1016/j.biocon.2012.02.014.
   STANKoviC V, 2022, BOT SERB, V46, P197, DOI 10.2298/BOTSERB2202197S.
   Stehman SV, 2003, ENVIRON ECOL STAT, V10, P301, DOI 10.1023/A:1025138423071.
   Stehman SV, 1998, REMOTE SENS ENVIRON, V64, P331, DOI 10.1016/S0034-4257(98)00010-8.
   Stutter MI, 2012, J ENVIRON QUAL, V41, P297, DOI 10.2134/jeq2011.0439.
   Tesfamichael SG, 2018, GISCI REMOTE SENS, V55, P417, DOI 10.1080/15481603.2017.1396658.
   Tokarska-Guzik B., 2022, CABI COMPENDIUM CABI, DOI {[}10.1079/cabicompendium.113998, DOI 10.1079/CABICOMPENDIUM.113998].
   Underwood E, 2003, REMOTE SENS ENVIRON, V86, P150, DOI 10.1016/S0034-4257(03)00096-8.
   Ustrnul Z., 2021, CLIMATE POLAND 2021.
   Vila Montserrat, 2009, V3, P265.
   Vuolo F, 2018, INT J APPL EARTH OBS, V72, P122, DOI 10.1016/j.jag.2018.06.007.
   Walsh SJ, 2018, SOC ECOL INTERACT GA, P143, DOI 10.1007/978-3-319-67177-2\_8.
   Weber Ewald, 2017, P1.
   Weisberg PJ, 2021, REMOTE SENS ENVIRON, V263, DOI 10.1016/j.rse.2021.112568.
   Wu YG, 2006, BIOL INVASIONS, V8, P1483, DOI 10.1007/s10530-005-5840-3.
   Wulder MA, 2006, INT J REMOTE SENS, V27, P663, DOI 10.1080/01431160500185284.
   Zajac A., 2011, Biodiversity: Research and Conservation, V23, P43.
   Zhou R, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13234910.},
Number-of-Cited-References = {91},
Times-Cited = {0},
Usage-Count-Last-180-days = {14},
Usage-Count-Since-2013 = {14},
Journal-ISO = {GISci. Remote Sens.},
Doc-Delivery-Number = {E4HJ5},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000975166500001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000909804400001,
Author = {Batchuluun, Ganbayar and Nam, Se Hyun and Park, Chanhum and Park, Kang
   Ryoung},
Title = {Super-Resolution Reconstruction-Based Plant Image Classification Using
   Thermal and Visible-Light Images},
Journal = {MATHEMATICS},
Year = {2023},
Volume = {11},
Number = {1},
Month = {JAN},
Abstract = {Few studies have been conducted on thermal plant images. This is because
   of the difficulty in extracting and analyzing various color-related
   patterns and features from the plant image obtained using a thermal
   camera, which does not provide color information. In addition, the
   thermal camera is sensitive to the surrounding temperature and humidity.
   However, the thermal camera enables the extraction of invisible patterns
   in the plant by providing external and internal heat information.
   Therefore, this study proposed a novel plant classification method based
   on both the thermal and visible-light plant images to exploit the
   strengths of both types of cameras. To the best of our knowledge, this
   study is the first to perform super-resolution reconstruction using
   visible-light and thermal plant images. Furthermore, a method to improve
   the classification performance through generative adversarial network
   (GAN)-based super-resolution reconstruction was proposed. Through the
   experiments using a self-collected dataset of thermal and visible-light
   images, our method shows higher accuracies than the state-of-the-art
   methods.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Park, KR (Corresponding Author), Dongguk Univ, Div Elect \& Elect Engn, 30 Pildong Ro,1 Gil, Seoul 04620, South Korea.
   Batchuluun, Ganbayar; Nam, Se Hyun; Park, Chanhum; Park, Kang Ryoung, Dongguk Univ, Div Elect \& Elect Engn, 30 Pildong Ro,1 Gil, Seoul 04620, South Korea.},
DOI = {10.3390/math11010076},
Article-Number = {76},
EISSN = {2227-7390},
Keywords = {plant image; classification; deep learning; super-resolution
   reconstruction},
Research-Areas = {Mathematics},
Web-of-Science-Categories  = {Mathematics},
Author-Email = {parkgr@dongguk.edu},
Affiliations = {Dongguk University},
ORCID-Numbers = {NAM, SE HYUN/0000-0002-0181-8774},
Funding-Acknowledgement = {National Research Foundation of Korea (NRF) - Ministry of Science and
   ICT (MSIT) through the Basic Science Research Program
   {[}NRF-2022R1F1A1064291]; NRF - MSIT through the Basic Science Research
   Program {[}NRF-2021R1F1A1045587]; MSIT, Korea, under the ITRC
   (Information Technology Research Center) support program
   {[}IITP-2022-2020-0-01789]},
Funding-Text = {This research was supported in part by the National Research Foundation
   of Korea (NRF) funded by the Ministry of Science and ICT (MSIT) through
   the Basic Science Research Program (NRF-2022R1F1A1064291), in part by
   the NRF funded by the MSIT through the Basic Science Research Program
   (NRF-2021R1F1A1045587), and in part by the MSIT, Korea, under the ITRC
   (Information Technology Research Center) support program
   (IITP-2022-2020-0-01789) supervised by the IITP (Institute for
   Information \& Communications Technology Planning \& Evaluation).},
Cited-References = {{[}Anonymous], FLIR TAU 2.
   {[}Anonymous], THERVISDB.
   {[}Anonymous], ANAL VARIANCE.
   {[}Anonymous], LOGITECH C270 HD WEB.
   {[}Anonymous], PLANTSR PLANTMC.
   {[}Anonymous], PLANTVILLAGE DATASET.
   {[}Anonymous], CATEGORICAL CROSS EN.
   {[}Anonymous], CROSS ENTROPY LOSS.
   Ashwinkumar S, 2022, MATER TODAY-PROC, V51, P480, DOI 10.1016/j.matpr.2021.05.584.
   Batchuluun G, 2022, J KING SAUD UNIV-COM, V34, P10474, DOI 10.1016/j.jksuci.2022.11.003.
   Batchuluun G, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10214053.
   Batchuluun G, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10173091.
   Chakraborty Aditya, 2021, Proceedings of 5th International Conference on Computing Methodologies and Communication (ICCMC 2021), P1624, DOI 10.1109/ICCMC51019.2021.9418042.
   Chollet F, KERAS CALIFORNIA US.
   Chompookham Thipwimon, 2021, ICIC Express Letters, V15, P553, DOI 10.24507/icicel.15.06.553.
   Derczynski L, 2016, LREC 2016 - TENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P261.
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622.
   Cap QH, 2019, Arxiv, DOI arXiv:1911.11341.
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522.
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711.
   Kingma D., 2014, AIP CONF P, DOI DOI 10.1063/1.4902458.
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386.
   OpenCV, US.
   Powers D. M., 2010, J MACH LEARN TECHNOL, P37.
   Python, US.
   Raza SEA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0123262.
   Singh D, 2020, ACM INT CONF PR SER, P249, DOI 10.1145/3371158.3371196.
   Taniguchi H., 2018, P 23 OPT COMM C, P1, DOI {[}10.1109/OECC.2018.8730055, DOI 10.1109/OECC.2018.8730055, DOI 10.1109/AIPR.2018.8707385].
   TensorFlow, US.
   Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243.
   Wang DF, 2021, COMPUT ELECTRON AGR, V190, DOI 10.1016/j.compag.2021.106468.
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861.
   Yamamoto K, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17112557.
   Yilma G, 2021, TURK J ELECTR ENG CO, V29, P2869, DOI 10.3906/elk-2105-115.},
Number-of-Cited-References = {34},
Times-Cited = {1},
Usage-Count-Last-180-days = {5},
Usage-Count-Since-2013 = {5},
Journal-ISO = {Mathematics},
Doc-Delivery-Number = {7R0YC},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000909804400001},
OA = {gold, Green Published},
DA = {2023-08-12},
}

@article{ WOS:000623186200001,
Author = {Mahajan, Shubham and Raina, Akshay and Gao, Xiao-Zhi and Pandit, Amit
   Kant},
Title = {Plant Recognition Using Morphological Feature Extraction and Transfer
   Learning over SVM and AdaBoost},
Journal = {SYMMETRY-BASEL},
Year = {2021},
Volume = {13},
Number = {2},
Month = {FEB},
Abstract = {Plant species recognition from visual data has always been a challenging
   task for Artificial Intelligence (AI) researchers, due to a number of
   complications in the task, such as the enormous data to be processed due
   to vast number of floral species. There are many sources from a plant
   that can be used as feature aspects for an AI-based model, but features
   related to parts like leaves are considered as more significant for the
   task, primarily due to easy accessibility, than other parts like
   flowers, stems, etc. With this notion, we propose a plant species
   recognition model based on morphological features extracted from
   corresponding leaves' images using the support vector machine (SVM) with
   adaptive boosting technique. This proposed framework includes the
   pre-processing, extraction of features and classification into one of
   the species. Various morphological features like centroid, major axis
   length, minor axis length, solidity, perimeter, and orientation are
   extracted from the digital images of various categories of leaves. In
   addition to this, transfer learning, as suggested by some previous
   studies, has also been used in the feature extraction process. Various
   classifiers like the kNN, decision trees, and multilayer perceptron
   (with and without AdaBoost) are employed on the opensource dataset,
   FLAVIA, to certify our study in its robustness, in contrast to other
   classifier frameworks. With this, our study also signifies the
   additional advantage of 10-fold cross validation over other dataset
   partitioning strategies, thereby achieving a precision rate of 95.85\%.},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Mahajan, S (Corresponding Author), Shri Mata Vaishno Devi Univ, Sch Elect \& Commun, Katra 182320, India.
   Mahajan, Shubham; Pandit, Amit Kant, Shri Mata Vaishno Devi Univ, Sch Elect \& Commun, Katra 182320, India.
   Raina, Akshay, Shri Mata Vaishno Devi Univ, Sch Elect Engn, Katra 182320, India.
   Gao, Xiao-Zhi, Univ Eastern Finland, Sch Comp, Kuopio 70210, Finland.},
DOI = {10.3390/sym13020356},
Article-Number = {356},
EISSN = {2073-8994},
Keywords = {plant species recognition; SVM; AdaBoost; non-separable data; feature
   extraction; feature selection; transfer learning},
Research-Areas = {Science \& Technology - Other Topics},
Web-of-Science-Categories  = {Multidisciplinary Sciences},
Author-Email = {19dec001@smvdu.ac.in
   18bee003@smvdu.ac.in
   xiao.z.gao@uef.fi
   amit.pandit@smvdu.com},
Affiliations = {Shri Mata Vaishno Devi University; Shri Mata Vaishno Devi University;
   University of Eastern Finland},
ResearcherID-Numbers = {MAHAJAN, SHUBHAM/AAY-6389-2020
   },
ORCID-Numbers = {MAHAJAN, SHUBHAM/0000-0003-0385-3933
   Raina, Akshay/0000-0002-8623-5470},
Cited-References = {{[}Anonymous], 2010, INT J COMP APPL.
   {[}Anonymous], 2007, PROC INT C MACH LEAR, DOI DOI 10.1007/978-3-642-02326-2\_51.
   {[}Anonymous], 2018, BOOSTING ALGORITHMS.
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384.
   Azlah MAF, 2019, COMPUTERS, V8, DOI 10.3390/computers8040077.
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411.
   Duan KB, 2005, LECT NOTES COMPUT SC, V3541, P278.
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504.
   Govindaraj D., 2016, 160405242 ARXIV.
   Hastie T, 1998, ADV NEUR IN, V10, P507.
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
   Jeon WS, 2017, INT J FUZZY LOG INTE, V17, P26, DOI 10.5391/IJFIS.2017.17.1.26.
   Kadir A., 2014, INT J ADV SCI TECHNO, V44, P113.
   Kalpana D.B., 2011, INT J COMPUT APPL, V28, P18, DOI {[}10.5120/3408-4754, DOI 10.5120/3408-4754].
   Kim TH, 2012, LECT NOTES COMPUT SC, V7202, P122, DOI 10.1007/978-3-642-31919-8\_16.
   Kumar M, 2019, IEEE ACCESS, V7, P163912, DOI 10.1109/ACCESS.2019.2952176.
   Kumar S., 2012, INDIAN J COMPUTER SC, V3, P436.
   Li XC, 2008, ENG APPL ARTIF INTEL, V21, P785, DOI 10.1016/j.engappai.2007.07.001.
   Pankaja K, 2018, P 2018 INT C INV RES, P1190, DOI {[}10.1109/ICIRCA.2018.8597184, DOI 10.1109/ICIRCA.2018.8597184].
   Sambhaji E. S., 2014, ASIAN J, V2, P10.
   Sun RC, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020466.
   Sun Y, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7361042.
   Yang KL, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10111721.
   Yosinski J, 2014, ADV NEUR IN, V27.
   Zhang YN, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10155177.
   Zhu J, 2009, STAT INTERFACE, V2, P349.},
Number-of-Cited-References = {26},
Times-Cited = {16},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {8},
Journal-ISO = {Symmetry-Basel},
Doc-Delivery-Number = {QO5MM},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000623186200001},
OA = {gold},
DA = {2023-08-12},
}

@article{ WOS:000716045600001,
Author = {Zheng, Yuhong and Wang, Da and Li, Xiaolong and Wang, Ziyang and Zhou,
   Qingwei and Fu, Li and Yin, Yunlong and Creech, David},
Title = {Biometric Identification of Taxodium spp. and Their Hybrid Progenies by
   Electrochemical Fingerprints},
Journal = {BIOSENSORS-BASEL},
Year = {2021},
Volume = {11},
Number = {10},
Month = {OCT},
Abstract = {The use of electrochemical fingerprints for plant identification is an
   emerging application in biosensors. In this work, Taxodium ascendens, T.
   distichum, T. mucronatum, and 18 of their hybrid progenies were
   collected for this purpose. This is the first attempt to use
   electrochemical fingerprinting for the identification of plant hybrid
   progeny. Electrochemical fingerprinting in the leaves of Taxodium spp.
   was recorded under two conditions. The results showed that the
   electrochemical fingerprints of each species and progeny possessed very
   suitable reproducibility. These electrochemical fingerprints represent
   the electrochemical behavior of electrochemically active substances in
   leaf tissues under specific conditions. Since these species and
   progenies are very closely related to each other, it is challenging to
   identify them directly using a particular electrochemical
   fingerprinting. Therefore, electrochemical fingerprints measured under
   different conditions were used to perform pattern recognition. We can
   identify different species and progenies by locating the features in
   different pattern maps. We also performed a phylogenetic study with data
   from electrochemical fingerprinting. The results proved that the
   electrochemical classification results and the relationship between them
   are closely related.</p>},
Publisher = {MDPI},
Address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Zheng, YH (Corresponding Author), Jiangsu Prov \& Chinese Acad Sci, Jiangsu Engn Res Ctr Taxodium Rich Germplasm Inno, Inst Bot, Mem Sun Yat Sen,Nanjing Bot Garden, Nanjing 210014, Peoples R China.
   Fu, L (Corresponding Author), Hangzhou Dianzi Univ, Coll Mat \& Environm Engn, Hangzhou 310018, Peoples R China.
   Zheng, Yuhong; Wang, Ziyang; Yin, Yunlong, Jiangsu Prov \& Chinese Acad Sci, Jiangsu Engn Res Ctr Taxodium Rich Germplasm Inno, Inst Bot, Mem Sun Yat Sen,Nanjing Bot Garden, Nanjing 210014, Peoples R China.
   Wang, Da; Li, Xiaolong; Zhou, Qingwei; Fu, Li, Hangzhou Dianzi Univ, Coll Mat \& Environm Engn, Hangzhou 310018, Peoples R China.
   Creech, David, Stephen F Austin State Univ, Arthur Temple Coll Forestry \& Agr, Nacogdoches, TX 75962 USA.},
DOI = {10.3390/bios11100403},
Article-Number = {403},
EISSN = {2079-6374},
Keywords = {electroanalysis; <p>Taxodium spp.\& nbsp;</p>; plant identification;
   fingerprints; biometrics},
Keywords-Plus = {MICROEXTRACTION-ASSISTED VOLTAMMETRY; ZHONGSHANSHAN; OXIDATION;
   AUTHENTICATION; EXTRACT; YELLOW; LEAVES; TEA},
Research-Areas = {Chemistry; Science \& Technology - Other Topics; Instruments \&
   Instrumentation},
Web-of-Science-Categories  = {Chemistry, Analytical; Nanoscience \& Nanotechnology; Instruments \&
   Instrumentation},
Author-Email = {zhengyuhong@cnbg.net
   wangda@hdu.edu.cn
   lxlr@hdu.edu.cn
   wangziyang@cnbg.net
   zhouqw@hdu.edu.cn
   fuli@hdu.edu.cn
   ylyin@cnbg.net
   dcreech@sfasu.edu},
Affiliations = {Chinese Academy of Sciences; Hangzhou Dianzi University},
ResearcherID-Numbers = {Fu, Li/AAH-4689-2020
   li, xiao/HKV-8405-2023
   Zheng, Yu/GRJ-5808-2022
   li, xiao/GSN-6181-2022
   Li, xiaolong/GRS-9148-2022
   li, xiao/HJP-5134-2023},
ORCID-Numbers = {Fu, Li/0000-0002-5957-7790
   },
Funding-Acknowledgement = {Jiangsu Provincial Long-term Scientific Research Base for Taxodium Rich
   {[}LYKJ{[}2021]05]; Jiangsu Provincial Policy Guidance Program
   {[}BX2020010]},
Funding-Text = {FundingThis research was financially supported by Jiangsu Provincial
   Long-term Scientific Research Base for Taxodium Rich. Breeding and
   Cultivation (LYKJ{[}2021]05) and Jiangsu Provincial Policy Guidance
   Program (BX2020010).},
Cited-References = {Abdelsalam NR, 2019, IND CROP PROD, V139, DOI 10.1016/j.indcrop.2019.111515.
   Canovas R, 2021, CHEMOSENSORS, V9, DOI 10.3390/chemosensors9070187.
   {[}陈云鹏 Chen Yunpeng], 2002, {[}复旦学报. 自然科学版, Journal of Fudan University.Natural Sciences], V41, P641.
   Chou SR, 2020, ECOL INDIC, V110, DOI 10.1016/j.ecolind.2019.105867.
   Domenech-Carbo A, 2015, RSC ADV, V5, P61006, DOI 10.1039/c5ra11336a.
   Dominguez I, 2015, SENSOR ACTUAT B-CHEM, V210, P491, DOI 10.1016/j.snb.2015.01.009.
   Duan H, 2020, BMC GENOMICS, V21, DOI 10.1186/s12864-020-6532-1.
   Fan BY, 2021, BIOSENSORS-BASEL, V11, DOI 10.3390/bios11050155.
   Feng ZW, 2013, MOL BIOL REP, V40, P1861, DOI 10.1007/s11033-012-2227-2.
   Fu L, 2019, SENSOR ACTUAT B-CHEM, V298, DOI 10.1016/j.snb.2019.126836.
   Fu L, 2018, BIOSENS BIOELECTRON, V120, P102, DOI 10.1016/j.bios.2018.08.052.
   Fu L, 2018, J ELECTROANAL CHEM, V817, P128, DOI 10.1016/j.jelechem.2018.04.009.
   Fu L, 2018, TALANTA, V180, P248, DOI 10.1016/j.talanta.2017.12.058.
   Hua JF, 2017, FLORA, V231, P29, DOI 10.1016/j.flora.2017.04.007.
   Iniesta J, 2001, ELECTROCHIM ACTA, V46, P3573, DOI 10.1016/S0013-4686(01)00630-2.
   Irakli M, 2018, IND CROP PROD, V124, P382, DOI 10.1016/j.indcrop.2018.07.070.
   Liu ZB, 2020, J FOOD COMPOS ANAL, V86, DOI 10.1016/j.jfca.2019.103385.
   Marin-Saez J, 2019, FOOD CHEM, V287, P265, DOI 10.1016/j.foodchem.2019.02.091.
   Martini M, 2015, ANAL METHODS-UK, V7, P5740, DOI {[}10.1039/c5ay01145k, 10.1039/C5AY01145K].
   Niinemets U, 2015, NEW PHYTOL, V205, P973, DOI 10.1111/nph.13096.
   Ramsay A, 2016, J FOOD COMPOS ANAL, V47, P16, DOI 10.1016/j.jfca.2015.12.004.
   Rolland-Lagan AG, 2009, PLANT J, V57, P195, DOI 10.1111/j.1365-313X.2008.03678.x.
   Ryu J, 2018, J COASTAL RES, P381, DOI 10.2112/SI85-077.1.
   SHARIFIAN H, 1986, J ELECTROCHEM SOC, V133, P921, DOI 10.1149/1.2108763.
   Shi HB, 2021, J FOOD MEAS CHARACT, V15, P4711, DOI 10.1007/s11694-021-01030-5.
   Shumyantseva VV, 2020, MENDELEEV COMMUN, V30, P299, DOI 10.1016/j.mencom.2020.05.012.
   Sun Y, 2021, MICROCHEM J, V169, DOI 10.1016/j.microc.2021.106516.
   Wang WL, 2020, CHEM COMMUN, V56, P3329, DOI 10.1039/c9cc09433d.
   Wang ZQ, 2017, TREES-STRUCT FUNCT, V31, P1519, DOI 10.1007/s00468-017-1566-y.
   Xu YT, 2020, BIOELECTROCHEMISTRY, V133, DOI 10.1016/j.bioelechem.2020.107455.
   Yang J, 2017, BIOMED PHARMACOTHER, V96, P1199, DOI 10.1016/j.biopha.2017.11.098.
   Yu CG, 2016, PLANT PHYSIOL BIOCH, V100, P156, DOI 10.1016/j.plaphy.2016.01.009.
   Zhang MJ, 2020, CHEMISTRYSELECT, V5, P5035, DOI 10.1002/slct.202001100.
   Zheng YH, 2020, INT J ELECTROCHEM SC, V15, P9622, DOI 10.20964/2020.10.54.
   Zheng YH, 2018, INORG NANO-MET CHEM, V48, P449, DOI 10.1080/24701556.2019.1569687.
   Zheng YH, 2015, INT J ELECTROCHEM SC, V10, P3530.},
Number-of-Cited-References = {36},
Times-Cited = {31},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {19},
Journal-ISO = {Biosensors-Basel},
Doc-Delivery-Number = {WT7ML},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000716045600001},
OA = {Green Published, gold},
DA = {2023-08-12},
}

@article{ WOS:000673963800001,
Author = {Bakhshipour, Adel},
Title = {Cascading Feature Filtering and Boosting Algorithm for Plant Type
   Classification Based on Image Features},
Journal = {IEEE ACCESS},
Year = {2021},
Volume = {9},
Pages = {82021-82030},
Abstract = {Crop and weeds identification is of important steps towards the
   development of efficient automotive weed control systems. The higher the
   accuracy of plant detection and classification, the higher the
   performance of the weeding machine. In this study, the capability of two
   popular boosting methods including Adaboost.M1 and LogitBoost algorithms
   was evaluated to enhance the plant classification performance of four
   classifiers, namely Multi-Layer Perceptron (MLP), k-Nearest Neighbors
   (kNN), Random Forest (RF), and Support Vector Machine (SVM). Four
   feature filtering techniques including Correlation-based Feature
   Selection (CFS), Information Gain (IG), Gain Ratio (GR), and OneR were
   applied to the image-extracted features and 10 of the most significant
   features were selected and fed into single and boosted classifiers. The
   RF model trained by IG selected features (IG-RF) was the most
   appropriate classifier among the evaluated models whether in single or
   boosted modes. It was also found that boosting of IG-RF by using
   Adaboost.M1 and LogitBoost algorithms improved the classification
   accuracy. Regarding the performance values, the LogitBoost-IG-RF
   structure, which provided a classification accuracy of 99.58\%, a kappa
   (k) of 0.9948, and a Root Mean Squared Error (RMSE) of 0.0688 on
   training dataset, was selected as the most appropriate classifier for
   plant discrimination in peanut fields. The accuracy, k, and RMSE
   criteria of this combination on test dataset were 95.00\%, 0.9375, and
   0.1591, respectively. It was concluded that combination of boosting
   algorithms and feature selection methods can promote plant type
   discrimination accuracy, which is a crucial factor in the development of
   precision weed control systems.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Article},
Language = {English},
Affiliation = {Bakhshipour, A (Corresponding Author), Univ Guilan, Fac Agr Sci, Dept Biosyst Engn, Rasht 4199613776, Iran.
   Bakhshipour, Adel, Univ Guilan, Fac Agr Sci, Dept Biosyst Engn, Rasht 4199613776, Iran.},
DOI = {10.1109/ACCESS.2021.3086269},
ISSN = {2169-3536},
Keywords = {Ensemble learning; feature selection; image processing; plant
   identification; precision agriculture},
Keywords-Plus = {POTENTIAL YIELD LOSS; ARTIFICIAL NEURAL-NETWORKS; MACHINE VISION; WEED
   INTERFERENCE; FEATURE-SELECTION; UNITED-STATES; IDENTIFICATION; SHAPES;
   SYSTEM; CROPS},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {abakhshipour@guilan.ac.ir},
Affiliations = {University of Guilan},
ResearcherID-Numbers = {Bakhshipour, Adel/AAX-6326-2020},
ORCID-Numbers = {Bakhshipour, Adel/0000-0002-0292-8713},
Funding-Acknowledgement = {University of Guilan},
Funding-Text = {The author would like to thank the University of Guilan for supporting
   this research.},
Cited-References = {Abdollahpouri A., 2019, INT J INF COMMUN TEC, V11, P57.
   Adhitya Y, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10111642.
   Ajdadi FR, 2016, SOIL TILL RES, V162, P8, DOI 10.1016/j.still.2016.04.012.
   Cortes EA, 2007, INT ADV ECON RES, V13, P301, DOI 10.1007/s11294-007-9090-2.
   Are A., 2011, INT AGROPHYS, V25, P1.
   Aware A. A., 2015, INT J INNOV ADV COMP, V4.
   Bakhshipour A, 2020, PLANT METHODS, V16, DOI 10.1186/s13007-020-00695-1.
   Bakhshipour A, 2018, COMPUT ELECTRON AGR, V145, P153, DOI 10.1016/j.compag.2017.12.032.
   Bakhshipour A, 2017, BIOSYST ENG, V157, P1, DOI 10.1016/j.biosystemseng.2017.02.002.
   Bijanzadeh E, 2010, AUST J CROP SCI, V4, P402.
   Bin Ghazali KH, 2012, PHYSCS PROC, V25, P2116, DOI 10.1016/j.phpro.2012.03.358.
   Cavallo DP, 2019, COMPUT ELECTRON AGR, V156, P558, DOI 10.1016/j.compag.2018.12.019.
   Chen YJ, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21010212.
   Chen YA, 2021, 35TH INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN 2021), P311, DOI 10.1109/ICOIN50884.2021.9333852.
   Chowdhury S, 2015, EXPERT SYST APPL, V42, P5047, DOI 10.1016/j.eswa.2015.02.047.
   Dagdia Z. C., 2020, FUND INFORM, V2020, P1.
   Darshan H. K., 2019, J ADV INFORM TECHNOL, V10, P14, DOI DOI 10.12720/JAIT.10.1.14-19.
   Dille JA, 2020, WEED TECHNOL, V34, P624, DOI 10.1017/wet.2020.12.
   Elnemr HA, 2017, 2017 INTL CONF ON ADVANCED CONTROL CIRCUITS SYSTEMS (ACCS) SYSTEMS \& 2017 INTL CONF ON NEW PARADIGMS IN ELECTRONICS \& INFORMATION TECHNOLOGY (PEIT), P91, DOI 10.1109/ACCS-PEIT.2017.8303025.
   Faisal Ahmed, 2011, World Applied Sciences Journal, V12, P432.
   Fan X, 2017, P INT COMP SOFTW APP, P822, DOI 10.1109/COMPSAC.2017.49.
   Fawakherji M., 2020, INT J ROBOT COMPUT, V2, P39, DOI DOI 10.35708/RC1869-126258.
   Forero M. G., 2018, P IB C PATT REC MADR, P117.
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504.
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223.
   Galloway M.M., 1975, COMPUT VISION GRAPH, V4, P172, DOI {[}10.1016/S0146-664X(75)80008-6, DOI 10.1016/S0146-664X(75)80008-6].
   Ghojogh B., ARXIV190502845.
   Hare A. T., 2019, Peanut Science, V46, P182.
   HOLTE RC, 1993, MACH LEARN, V11, P63, DOI 10.1023/A:1022631118932.
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692.
   Jafari A., 2012, INT J INF SCI MANAGE, V4, P1.
   Kadavi PR, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081252.
   Kamath R, 2020, INT J AGRON, V2020, DOI 10.1155/2020/6474536.
   Kamath R, 2020, INT J AGR BIOL ENG, V13, P191, DOI 10.25165/j.ijabe.20201301.4920.
   Kazmi W, 2015, COMPUT ELECTRON AGR, V118, P290, DOI 10.1016/j.compag.2015.08.023.
   Khalid S, 2014, 2014 SCIENCE AND INFORMATION CONFERENCE (SAI), P372, DOI 10.1109/SAI.2014.6918213.
   Khojastehnazhand M, 2020, J FOOD ENG, V271, DOI 10.1016/j.jfoodeng.2019.109864.
   Khurana, ADV ELECTROMECHANICA, V2021, P671.
   Lalabadi HM, 2020, AQUACULT ENG, V90, DOI 10.1016/j.aquaeng.2020.102076.
   Li LH, 2021, IEEE SENS J, V21, P17447, DOI 10.1109/JSEN.2020.3045501.
   Lin FF, 2017, SUSTAINABILITY-BASEL, V9, DOI 10.3390/su9081335.
   Liu B., 2020, CURR ROBOT REP, V1, P19, DOI DOI 10.1007/S43154-020-00001-W.
   Ma JX, 2018, IEEE ENG MED BIO, P4945, DOI 10.1109/EMBC.2018.8513169.
   Naeem S, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11020263.
   Narassiguin A, 2016, PATTERN ANAL APPL, V19, P1093, DOI 10.1007/s10044-016-0553-z.
   Omeer AA, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2020.101181.
   Park BE, 2016, HEALTHC INFORM RES, V22, P299, DOI 10.4258/hir.2016.22.4.299.
   Payman SH, 2018, QUAL ASSUR SAF CROP, V10, P103, DOI 10.3920/QAS2017.1109.
   Pazoki AR, 2014, J ANIM PLANT SCI-PAK, V24, P336.
   Priyatna Ryan Dhika, 2018, IOP Conference Series: Materials Science and Engineering, V420, DOI 10.1088/1757-899X/420/1/012133.
   QIN XL, 2017, P IEEE C COMP VIS PA, P1.
   Que YH, 2016, I C FIELD PROG LOGIC, DOI 10.1109/FPL.2016.7577302.
   Rahimi-Ajdadi F, 2018, MEASUREMENT, V121, P179, DOI 10.1016/j.measurement.2018.02.060.
   Ravichandran S., 2016, TRAINING, V3, P1.
   Rhouma MB, 2017, COMPUT ELECTRON AGR, V142, P326, DOI 10.1016/j.compag.2017.08.029.
   Rojas, 2017, ING INVEST, V37, P68.
   Sabzi S, 2020, HELIYON, V6, DOI 10.1016/j.heliyon.2020.e03685.
   Sabzi S, 2018, COMPUT IND, V98, P80, DOI 10.1016/j.compind.2018.03.001.
   Sabzi S, 2017, APPL SOFT COMPUT, V56, P107, DOI 10.1016/j.asoc.2017.03.006.
   Saha D., 2017, P INT C RES AD CONV, P136.
   Shahin M, 2016, ESEM'16: PROCEEDINGS OF THE 10TH ACM/IEEE INTERNATIONAL SYMPOSIUM ON EMPIRICAL SOFTWARE ENGINEERING AND MEASUREMENT, DOI 10.1145/2961111.2962587.
   Shouche SP, 2001, COMPUT ELECTRON AGR, V33, P55, DOI 10.1016/S0168-1699(01)00174-0.
   Siddiqi MH, 2014, J INF SCI ENG, V30, P1227.
   Soltani N, 2018, WEED TECHNOL, V32, P749, DOI 10.1017/wet.2018.88.
   Soltani N, 2018, WEED TECHNOL, V32, P342, DOI 10.1017/wet.2017.116.
   Tang JL, 2016, COMPUT ELECTRON AGR, V122, P103, DOI 10.1016/j.compag.2015.12.016.
   Tang JL, 2018, SOFT COMPUT, V22, P7649, DOI 10.1007/s00500-018-3125-x.
   Santos SGTD, 2020, ARTIF INTELL REV, V53, P1293, DOI 10.1007/s10462-019-09696-6.
   Thanoon Mohammad A., 2019, 2019 1st AL-Noor International Conference for Science and Technology (NICST). Proceedings, P32, DOI 10.1109/NICST49484.2019.9043805.
   Tiwari AK, 2015, 2015 11TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC), P691, DOI 10.1109/ICNC.2015.7378074.
   Voorhoeve, 2018, MACHINE LEARNING CRO.
   Wang AC, 2019, COMPUT ELECTRON AGR, V158, P226, DOI 10.1016/j.compag.2019.02.005.
   Wang W, 2018, 2018 9TH IEEE INTERNATIONAL CONFERENCE ON BIG KNOWLEDGE (ICBK), P221, DOI 10.1109/ICBK.2018.00037.
   Wang XZ, 2019, IEEE ACCESS, V7, P151525, DOI 10.1109/ACCESS.2019.2948095.
   Wei X., 2021, GRAY LEVEL RUN LENGT.
   Xing C, 2016, PROC CVPR IEEE, P4489, DOI 10.1109/CVPR.2016.486.
   Yariyan P, 2020, WATER RESOUR MANAG, V34, P3037, DOI 10.1007/s11269-020-02603-7.
   Yildirim Pinar, 2015, International Journal of Machine Learning and Computing, V5, P258, DOI 10.7763/IJMLC.2015.V5.517.
   Yuan WA, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50480-x.
   Zhang SW, 2019, FRONT COMP SCI-SWITZ, V1, DOI 10.3389/fcomp.2019.00004.
   Zheng Y, 2017, COMPUT ELECTRON AGR, V141, P215, DOI 10.1016/j.compag.2017.07.028.
   Zhu PF, 2015, PATTERN RECOGN, V48, P438, DOI 10.1016/j.patcog.2014.08.006.},
Number-of-Cited-References = {82},
Times-Cited = {8},
Usage-Count-Last-180-days = {3},
Usage-Count-Since-2013 = {14},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {TK1YX},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000673963800001},
OA = {gold},
DA = {2023-08-12},
}

@inproceedings{ WOS:000246237603149,
Author = {Aoyama, Tomoo and Akashi, Ryo and Umeno, Hidenori and Nagshima, Umpei},
Book-Group-Author = {IEEE},
Title = {An approach to plant identification technology: Development of lignin
   pyrolysis gas chromatography and pattern recognition},
Booktitle = {2006 SICE-ICASE International Joint Conference, Vols 1-13},
Year = {2006},
Pages = {3613-3618},
Note = {SICE-ICASE International Joint Conference, Busan, SOUTH KOREA, OCT
   18-21, 2006},
Organization = {SICE; ICASE},
Abstract = {We discovered a technique to extract a characteristic spectrum pattern
   from Lignin pyrolysis gas chromatography. The Lignin is a kind of
   methoxyphenol polymer that is often found among cell walls of plants.
   Using the spectrum, we can determine the species from fragments of 1mg
   samples. The technique is useful for criminal investigation. The
   determination of species depends on distances calculated by species and
   a database of the spectra. We use the distances and can calculate
   family-locations in Plantae. The location shows the evolution/end of
   families. We evaluated 88 kinds of plants and discussed the end of
   ginkgo family and the branch of monocotyledon and a relation between
   C3/C4 metabolisms of Gramineae (family of the rice).},
Publisher = {IEEE},
Address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
Type = {Proceedings Paper},
Language = {English},
ISBN = {978-89-950038-4-8},
Keywords = {lignin; pyrolysis; gas chromatography; species-location; ginkgo; Oryza
   sativa},
Research-Areas = {Automation \& Control Systems; Computer Science; Engineering; Robotics},
Web-of-Science-Categories  = {Automation \& Control Systems; Computer Science, Artificial
   Intelligence; Engineering, Electrical \& Electronic; Robotics},
Cited-References = {AOYAMA T, 2006, Patent No. 2006146328.
   Graham SW, 2000, AM J BOT, V87, P1712, DOI 10.2307/2656749.
   IWATSUKI K, 30 LECT PLANTS FUNGI.
   IWATSUKI K, 2000, PHYLOGENETIC BOT, V1.
   Kuroda KI, 2000, J ANAL APPL PYROL, V56, P79, DOI 10.1016/S0165-2370(00)00085-1.
   NISHIOKA H, 2006, JAPANESE J FORENSIC, V11, P53.
   STEWART WN, 1993, PALEOBOTAN EVOLUTION.
   UCHIYAMA T, IN PRESS JAPANESE J.},
Number-of-Cited-References = {8},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Doc-Delivery-Number = {BGE04},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000246237603149},
DA = {2023-08-12},
}

@inproceedings{ WOS:000467096700011,
Author = {Qu, Yuan and Chen, Yixiang and Chen, Wenjie},
Editor = {Bi, Y and Chen, G and Deng, Q and Wang, Y},
Title = {Co-design and Implementation of Image Recognition Based on ARM and FPGA},
Booktitle = {EMBEDDED SYSTEMS TECHNOLOGY, ESTC 2017},
Series = {Communications in Computer and Information Science},
Year = {2018},
Volume = {857},
Pages = {141-153},
Note = {15th National Conference on Embedded Systems Technology (ESTC) -
   Embedded Systems and Intelligent Computing, Shenyang, PEOPLES R CHINA,
   NOV 17-19, 2017},
Organization = {CCF Embedded Syst Specialized Comm; NE Univ, Sch Comp Sci \& Engn;
   Neusoft},
Abstract = {With the development of the Internet of things, the image recognition
   system is widely required in many fields. It has very high requirement
   in real-time, but usually it has high complexity and large data. So the
   real-time, which improved by hardware acceleration, is the key of image
   recognition system. Traditional processors have the disadvantages of low
   flexibility and configurability for prototype of embedded system. The
   family of Xilinx Zynq7000 processors integrate dual-core ARM Cortext-A9
   and low-power FPGA. It can improve the operating efficiency and
   dynamical configurability of developing image applications. It also can
   reduce the power consumption of image processing. In this paper, we
   present an ARM and FPGA Co-design architecture of image recognition
   system based on Zynq7000 processor. Then we validate this architecture
   by the leaf recognition system. This architecture is based on module
   designed at system-level and modeled at algorithm-level. After
   determining the algorithm option, we partite the ARM and FPGA of modules
   depending on algorithm simulation, and then implement them separately.
   Finally, ARM and FPGA modules are interconnected by interface or driver.
   When the joint debugging is completed, prototype development of the
   embedded application is finished. As the experiment shown, FPGA and ARM
   codesign is 1.84 times faster than the pure ARM.},
Publisher = {SPRINGER-VERLAG BERLIN},
Address = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
Type = {Proceedings Paper},
Language = {English},
Affiliation = {Chen, YX (Corresponding Author), East China Normal Univ, Sch Comp Sci \& Software Engn, MOE Res Ctr Software Hardware Codesign Engn, Shanghai 200062, Peoples R China.
   Qu, Yuan; Chen, Yixiang; Chen, Wenjie, East China Normal Univ, Sch Comp Sci \& Software Engn, MOE Res Ctr Software Hardware Codesign Engn, Shanghai 200062, Peoples R China.},
DOI = {10.1007/978-981-13-1026-3\_11},
ISSN = {1865-0929},
EISSN = {1865-0937},
ISBN = {978-981-13-1026-3; 978-981-13-1025-6},
Keywords = {Co-design and implementation; ARM plus FPGA; Image recognition},
Research-Areas = {Computer Science; Engineering},
Web-of-Science-Categories  = {Computer Science, Hardware \& Architecture; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic},
Author-Email = {yxchen@sei.ecnu.edu.cn},
Affiliations = {East China Normal University},
Funding-Acknowledgement = {Natural Science Foundation of Shanghai {[}15ZR1410000]},
Funding-Text = {This work is partially sponsored by Natural Science Foundation of
   Shanghai (15ZR1410000).},
Cited-References = {Ameur R. B., 2017, COMPUTATIONAL INTELL, P1.
   Delligatti L., 2014, BRIEF GUIDE SYSTEMS.
   Drozdenko Benjamin, 2016, 2016 IEEE Conference on Computer Communications: Workshops (INFOCOM WKSHPS), P682, DOI 10.1109/INFCOMW.2016.7562163.
   Li L., 2016, COMPUT TECHNOL DEV.
   Liao Y. - P., 2010, 2010 INT S COMP COMM.
   Liu H., 2015, COMPUT MODERNIZATION, V240, P1.
   Lu J., 2013, EMBEDDED SYSTEM HARD.
   Uluturk C., 2012, INT S INN INT SYST A, P1, DOI DOI 10.1109/INISTA.2012.6247030.
   Wang Z., 2014, COMPUT ENG APPL.
   Yu-Ping Liao, 2010, 2010 International Symposium on Computer, Communication, Control and Automation (3CA), P334, DOI 10.1109/3CA.2010.5533816.
   Zhang J., 2015, IMAGE MULTIMEDIA.
   Zhou Z., 2016, MACHINE LEARNING.},
Number-of-Cited-References = {12},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {7},
Doc-Delivery-Number = {BM6NI},
Web-of-Science-Index = {Conference Proceedings Citation Index - Science (CPCI-S)},
Unique-ID = {WOS:000467096700011},
DA = {2023-08-12},
}

@article{ WOS:000400678700016,
Author = {Ustyuzhanin, Anton and Dammer, Karl-Heinz and Giebel, Antje and
   Weltzien, Cornelia and Schirrmann, Michael},
Title = {Discrimination of Common Ragweed (Ambrosia artemisiifolia) and Mugwort
   (Artemisia vulgaris) Based on Bag of Visual Words Model},
Journal = {WEED TECHNOLOGY},
Year = {2017},
Volume = {31},
Number = {2},
Pages = {310-319},
Month = {MAR-APR},
Abstract = {Common ragweed is a plant species causing allergic and asthmatic
   symptoms in humans. To control its propagation, an early identification
   system is needed. However, due to its similar appearance with mugwort,
   proper differentiation between these two weed species is important.
   Therefore, we propose a method to discriminate common ragweed and
   mugwort leaves based on digital images using bag of visual words (BoVW).
   BoVW is an object-based image classification that has gained acceptance
   in many areas of science. We compared speeded-up robust features (SURF)
   and grid sampling for keypoint selection. The image vocabulary was built
   using K-means clustering. The image classifier was trained using support
   vector machines. To check the robustness of the classifier, specific
   model runs were conducted with and without damaged leaves in the
   trainings dataset. The results showed that the BoVW model allows the
   discrimination between common ragweed and mugwort leaves with high
   accuracy. Based on SURF keypoints with 50\% of 788 images in total as
   training data, we achieved a 100\% correct recognition of the two plant
   species. The grid sampling resulted in slightly less recognition
   accuracy (98 to 99\%). In addition, the classification based on SURF was
   up to 31 times faster.},
Publisher = {CAMBRIDGE UNIV PRESS},
Address = {32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA},
Type = {Article},
Language = {English},
Affiliation = {Ustyuzhanin, A (Corresponding Author), Leibniz Inst Agr Engn \& Bioecon, Dept Engn Crop Prod, Max Eyth Allee 100, D-14469 Potsdam, Germany.
   Ustyuzhanin, Anton; Dammer, Karl-Heinz; Giebel, Antje; Weltzien, Cornelia; Schirrmann, Michael, Leibniz Inst Agr Engn \& Bioecon, Dept Engn Crop Prod, Max Eyth Allee 100, D-14469 Potsdam, Germany.},
DOI = {10.1614/WT-D-16-00068.1},
ISSN = {0890-037X},
EISSN = {1550-2740},
Keywords = {Weed species; digital images; machine learning; speeded-up robust
   features; plant recognition},
Keywords-Plus = {WEED IDENTIFICATION; CLASSIFICATION; POPULATIONS; EXPRESSION},
Research-Areas = {Agriculture; Plant Sciences},
Web-of-Science-Categories  = {Agronomy; Plant Sciences},
Author-Email = {austyuzhanin@atb-potsdam.de},
Affiliations = {Leibniz Institut fur Agrartechnik und Biookonomie (ATB)},
ResearcherID-Numbers = {Ustyuzhanin, Anton/AAN-4565-2020},
ORCID-Numbers = {Ustyuzhanin, Anton/0000-0003-4185-8102},
Cited-References = {A Fakhri A. Nasir, 2014, Modern Applied Science, V8, P121.
   Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003.
   Auda Y, 2002, REV FR ALLERGOL, V42, P533, DOI 10.1016/S0335-7457(02)00178-8.
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
   Brandes D., 2006, TUEXENIA, V27, P167.
   Bromuri S, 2014, J BIOMED INFORM, V51, P165, DOI 10.1016/j.jbi.2014.05.010.
   Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678.
   Dammer KH, 2013, WEED RES, V53, P146, DOI 10.1111/wre.12006.
   Dietterich TG, 1998, NEURAL COMPUT, V10, P1895, DOI 10.1162/089976698300017197.
   Faraki M, 2015, IET COMPUT VIS, V9, P331, DOI 10.1049/iet-cvi.2014.0018.
   Fumanal B, 2008, WEED RES, V48, P349, DOI 10.1111/j.1365-3180.2008.00627.x.
   Guo W, 2015, PLANT METHODS, V11, DOI 10.1186/s13007-015-0047-9.
   Hamaoui-Laguel L, 2015, NAT CLIM CHANGE, V5, P766, DOI {[}10.1038/nclimate2652, 10.1038/NCLIMATE2652].
   Hemming J, 2001, J AGR ENG RES, V78, P233, DOI 10.1006/jaer.2000.0639.
   Hernandez-Serna A, 2014, PEERJ, V2, DOI 10.7717/peerj.563.
   Jaric S, 2011, ARCH BIOL SCI, V63, P1181, DOI 10.2298/ABS1104181J.
   Ji SW, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-119.
   Jung A., 2006, 4th International Plant Protection Symposium at Debrecen University and 11th Trans-Tisza Plant Protection Forum, 18-19 October, 2006, Debrecen, Hungary, P153.
   Kaden NN, 1979, ETIMOLOGICHESKIY SLO.
   Larsen ABL, 2014, IEEE T MED IMAGING, V33, P1573, DOI 10.1109/TMI.2014.2318434.
   Leiblein M., 2008, TREFFPUNKT BIOL VIEL, V8, P97.
   Leiblein-Wild MC, 2014, BIOL INVASIONS, V16, P2003, DOI 10.1007/s10530-014-0644-y.
   Li ZS, 2011, IEICE T FUND ELECTR, VE94A, P533, DOI 10.1587/transfun.E94.A.533.
   Lozano-Vega G, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.5.053025.
   Ludcke HJ, 2009, ALLERGOLOGIE, V32, P402.
   Makra L, 2015, APPL ECOL ENV RES, V13, P489.
   McCarthy C, 2012, P 34 ANN C, P3.
   Mukti FA, 2015, J MED IMAG HEALTH IN, V5, P1009, DOI 10.1166/jmihi.2015.1491.
   Ngom R, 2014, IEEE J-STARS, V7, P126, DOI 10.1109/JSTARS.2013.2254469.
   Oyallon E, 2015, IMAGE PROCESS ON LIN, V5, P176, DOI 10.5201/ipol.2015.69.
   Ren Y, 2016, SIGNAL IMAGE VIDEO P, V10, P471, DOI 10.1007/s11760-015-0763-7.
   Silc U., 2009, Hacquetia, V8, P41, DOI 10.2478/v10028-009-0003-1.
   Sirbu C., 2008, ACTA HORIT BOT BUCUR, V35, P60.
   Storkey J, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088156.
   Sun H, 2012, IEEE GEOSCI REMOTE S, V9, P109, DOI 10.1109/LGRS.2011.2161569.
   Swain KC, 2011, BIOSYST ENG, V110, P450, DOI 10.1016/j.biosystemseng.2011.09.011.
   Ustyuzhanin A, 2015, GESUNDE PFLANZ, V67, P165, DOI 10.1007/s10343-015-0352-2.
   Verschwele A., 2014, Julius-Kuhn-Archiv, P21.
   Vetrivel A, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8030231.
   Vukovic Ivana, 2007, Agriculturae Conspectus Scientificus, V72, P103.
   Wang J, 2013, BIOMED SIGNAL PROCES, V8, P634, DOI 10.1016/j.bspc.2013.06.004.
   Weisa M., 2007, Precision agriculture `07. Papers presented at the 6th European Conference on Precision Agriculture, Skiathos, Greece, 3-6 June, 2007, P537.
   Williams JS, 2004, Proceedings of the Fifty-Eighth Annual Conference of the Southeastern Association of Fish and Wildlife Agencies, P1.
   Yang J., 2007, P INT WORKSHOP MULTI, P197, DOI DOI 10.1145/1290082.1290111.
   Yang S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0116500.
   Yanikoglu B, 2014, MACH VISION APPL, V25, P1369, DOI 10.1007/s00138-014-0612-7.},
Number-of-Cited-References = {46},
Times-Cited = {3},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {13},
Journal-ISO = {Weed Technol.},
Doc-Delivery-Number = {EU0DG},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000400678700016},
DA = {2023-08-12},
}

@article{ WOS:000340851600004,
Author = {Joly, Alexis and Goeau, Herve and Bonnet, Pierre and Bakic, Vera and
   Barbe, Julien and Selmi, Souheil and Yahiaoui, Itheri and Carre,
   Jennifer and Mouysset, Elise and Molino, Jean-Francois and Boujemaa,
   Nozha and Barthelemy, Daniel},
Title = {Interactive plant identification based on social image data},
Journal = {ECOLOGICAL INFORMATICS},
Year = {2014},
Volume = {23},
Number = {SI},
Pages = {22-34},
Month = {SEP},
Abstract = {Speeding up the collection and integration of raw botanical observation
   data is a crucial step towards a sustainable development of agriculture
   and the conservation of biodiversity. Initiated in the context of a
   citizen sciences project, the main contribution of this paper is an
   innovative collaborative workflow focused on image-based plant
   identification as a mean to enlist new contributors and facilitate
   access to botanical data. Since 2010, hundreds of thousands of
   geo-tagged and dated plant photographs were collected and revised by
   hundreds of novice, amateur and expert botanists of a specialized social
   network. An image-based identification tool - available as both a web
   and a mobile application - is synchronized with that growing data and
   allows any user to query or enrich the system with new observations. An
   important originality is that it works with up to five different organs
   contrarily to previous approaches that mainly relied on the leaf. This
   allows querying the system at any period of the year and with
   complementary images composing a plant observation. Extensive
   experiments of the visual search engine as well as system-oriented and
   user-oriented evaluations of the application show that it is already
   very helpful to determine a plant among hundreds or thousands of
   species. At the time of writing, the whole framework covers about half
   of the plant species living in France (2200 species), which already
   makes it the widest existing automated identification tool (with its
   imperfections). (C) 2013 Elsevier B.V. All rights reserved.},
Publisher = {ELSEVIER},
Address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
Type = {Article},
Language = {English},
Affiliation = {Joly, A (Corresponding Author), INRIA, LIRMM, ZENITH Team, F-34090 Montpellier, France.
   Joly, Alexis; Goeau, Herve; Bakic, Vera; Selmi, Souheil, INRIA, LIRMM, ZENITH Team, F-34090 Montpellier, France.
   Bonnet, Pierre, CIRAD, UMR AMAP, F-34398 Montpellier, France.
   Barbe, Julien, INRA, UMR AMAP, F-34398 Montpellier, France.
   Yahiaoui, Itheri, Univ Reims, CReSTIC, F-51100 Reims, France.
   Carre, Jennifer; Mouysset, Elise, Tela Botan, F-34000 Montpellier, France.
   Molino, Jean-Francois, IRD, UMR AMAP, F-34398 Montpellier, France.
   Boujemaa, Nozha, INRIA Saclay, F-92120 Palaiseau, France.
   Barthelemy, Daniel, BIOS, CIRAD, F-34398 Montpellier, France.
   Barthelemy, Daniel, INRA, UMR AMAP, F-34398 Montpellier, France.},
DOI = {10.1016/j.ecoinf.2013.07.006},
ISSN = {1574-9541},
EISSN = {1878-0512},
Keywords = {Plant; Identification; Images; Visual; Retrieval; Social network;
   Collaborative; Crowdsourcing; Citizen science; Multi-organ; Leaf;
   Flower; Bark; Fruit; Ecology; Surveillance; Monitoring; Multimedia;
   Computer vision; Botanist},
Research-Areas = {Environmental Sciences \& Ecology},
Web-of-Science-Categories  = {Ecology},
Author-Email = {alexis.joly@inria.fr},
Affiliations = {Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; Inria; CIRAD; Centre National de la Recherche Scientifique
   (CNRS); Institut de Recherche pour le Developpement (IRD); Universite de
   Montpellier; CIRAD; Centre National de la Recherche Scientifique (CNRS);
   Institut de Recherche pour le Developpement (IRD); Universite de
   Montpellier; INRAE; Universite de Reims Champagne-Ardenne; CIRAD; Centre
   National de la Recherche Scientifique (CNRS); Institut de Recherche pour
   le Developpement (IRD); Universite de Montpellier; CIRAD; CIRAD; Centre
   National de la Recherche Scientifique (CNRS); Institut de Recherche pour
   le Developpement (IRD); Universite de Montpellier; INRAE},
ResearcherID-Numbers = {Molino, Jean-François/C-5011-2009
   joly, alexis/AAV-3101-2021
   Bonnet, Pierre/AAG-6819-2020
   },
ORCID-Numbers = {Molino, Jean-François/0000-0001-8853-7133
   joly, alexis/0000-0002-2161-9940
   Bonnet, Pierre/0000-0002-2828-4389
   barthelemy, Daniel/0000-0003-3187-2517
   Goeau, Herve/0000-0003-3296-3795},
Funding-Acknowledgement = {Agropolis fundation},
Funding-Text = {This work was funded by the Agropolis fundation as its first flagship
   project Pl@ntNet.<SUP>11</SUP> We also would like to thank the authors
   of Photoflora<SUP>12</SUP> database and all other volunteers who
   gratefully spent their time in collecting pictures.},
Cited-References = {Agarwal G, 2006, TAXON, V55, P597, DOI 10.2307/25065637.
   {[}Anonymous], 2008, P 16 ACM INT C MULT.
   Backes AR, 2009, PATTERN RECOGN, V42, P54, DOI 10.1016/j.patcog.2008.07.006.
   Backes AR, 2009, INT J PATTERN RECOGN, V23, P1145, DOI 10.1142/S0218001409007508.
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
   Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8\_9.
   Bruno OM, 2008, INFORM SCIENCES, V178, P2722, DOI 10.1016/j.ins.2008.01.023.
   Cerutti Guillaume, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P202.
   Chang E, 2012, P IEEE, V100, P2580, DOI 10.1109/JPROC.2012.2204110.
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172.
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692.
   Goeau H., 2012, P 1 ACM INT WORKSH M, P41.
   Goeau H., 2012, CLEF WORKING NOTES.
   Goeau H., 2011, CLEF WORKING NOTES.
   Goeau H., 2011, P 19 INT C MULT SCOT, DOI {[}10.1145/2072298.2072472, DOI 10.1145/2072298.2072472].
   Gouet V, 2001, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P30, DOI 10.1109/IVL.2001.990853.
   Grozea C., 2012, CLEF WORKING NOTES.
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2\_24.
   Joly A., 2007, P 6 ACM INT C IM VID, P573.
   Joly A., 2009, P 17 ACM INT C MULT, P581, DOI DOI 10.1145/1631272.1631361.
   Joly A, 2011, PROC CVPR IEEE, P873, DOI 10.1109/CVPR.2011.5995709.
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3\_36.
   Le Q.V., 2012, ICML.
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748.
   Mouine S., 2012, ACM INT C MULT RETR.
   Neto JC, 2006, COMPUT ELECTRON AGR, V50, P121, DOI 10.1016/j.compag.2005.09.004.
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS \& IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47.
   Otsu N., 1979, T SYSTEMS MAN CYBERN.
   Romero E, 2007, LECT NOTES COMPUT SC, V4668, P209.
   Sanchez J, 2011, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2011.5995504.
   Sarmiento C., 2011, IAWA J.
   Soderkvist O., 2001, COMPUTER VISION CLAS.
   Sorokin A., 2008, PROC CVPR IEEE, P1, DOI DOI 10.1109/CVPRW.2008.4562953.
   Spampinato C., 2012, ACM C MULT, P1507.
   Yahiaoui I., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P254, DOI 10.1109/ICME.2012.130.
   Yahiaoui I., 2012, LEAF SHAPE DESCRIPTO, P254.
   Yahiaoui I, 2006, LECT NOTES COMPUT SC, V4261, P357.},
Number-of-Cited-References = {38},
Times-Cited = {72},
Usage-Count-Last-180-days = {2},
Usage-Count-Since-2013 = {65},
Journal-ISO = {Ecol. Inform.},
Doc-Delivery-Number = {AN8JX},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000340851600004},
DA = {2023-08-12},
}

@article{ WOS:000965055100001,
Author = {Li, Jia-Le and Su, Wen-Hao and Zhang, He-Yi and Peng, Yankun},
Title = {A real-time smart sensing system for automatic localization and
   recognition of vegetable plants for weed control},
Journal = {FRONTIERS IN PLANT SCIENCE},
Year = {2023},
Volume = {14},
Month = {MAR 27},
Abstract = {Tomato is a globally grown vegetable crop with high economic and
   nutritional values. Tomato production is being threatened by weeds. This
   effect is more pronounced in the early stages of tomato plant growth.
   Thus weed management in the early stages of tomato plant growth is very
   critical. The increasing labor cost of manual weeding and the negative
   impact on human health and the environment caused by the overuse of
   herbicides are driving the development of smart weeders. The core task
   that needs to be addressed in developing a smart weeder is to accurately
   distinguish vegetable crops from weeds in real time. In this study, a
   new approach is proposed to locate tomato and pakchoi plants in real
   time based on an integrated sensing system consisting of camera and
   color mark sensors. The selection scheme of reference, color, area, and
   category of plant labels for sensor identification was examined. The
   impact of the number of sensors and the size of the signal tolerance
   region on the system recognition accuracy was also evaluated. The
   experimental results demonstrated that the color mark sensor using the
   main stem of tomato as the reference exhibited higher performance than
   that of pakchoi in identifying the plant labels. The scheme of applying
   white topical markers on the lower main stem of the tomato plant is
   optimal. The effectiveness of the six sensors used by the system to
   detect plant labels was demonstrated. The computer vision algorithm
   proposed in this study was specially developed for the sensing system,
   yielding the highest overall accuracy of 95.19\% for tomato and pakchoi
   localization. The proposed sensor-based system is highly accurate and
   reliable for automatic localization of vegetable plants for weed control
   in real time.},
Publisher = {FRONTIERS MEDIA SA},
Address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
Type = {Article},
Language = {English},
Affiliation = {Su, WH (Corresponding Author), China Agr Univ, Coll Engn, Beijing, Peoples R China.
   Li, Jia-Le; Su, Wen-Hao; Zhang, He-Yi; Peng, Yankun, China Agr Univ, Coll Engn, Beijing, Peoples R China.},
DOI = {10.3389/fpls.2023.1133969},
Article-Number = {1133969},
ISSN = {1664-462X},
Keywords = {crop signalling; computer vision; plant identification; automated
   weeding; precision agriculture},
Keywords-Plus = {SUPPORT VECTOR MACHINE; CLASSIFICATION; DISCRIMINATION; SEGMENTATION;
   VISION; IMAGES; CROPS},
Research-Areas = {Plant Sciences},
Web-of-Science-Categories  = {Plant Sciences},
Author-Email = {wenhao.su@cau.edu.cn},
Affiliations = {China Agricultural University},
ResearcherID-Numbers = {Su, Wen-Hao/Q-2431-2019},
ORCID-Numbers = {Su, Wen-Hao/0000-0003-1745-4722},
Funding-Acknowledgement = {National Natural Science Foundation of China {[}32101610]},
Funding-Text = {Funding This research was funded by National Natural Science Foundation
   of China, grant number 32101610.},
Cited-References = {Ahmed F, 2012, CROP PROT, V40, P98, DOI 10.1016/j.cropro.2012.04.024.
   Bakhshipour A, 2018, COMPUT ELECTRON AGR, V145, P153, DOI 10.1016/j.compag.2017.12.032.
   Bayer-Krucsay D. D., 1937, LIGHT FILTER.
   Blasco J, 2002, BIOSYST ENG, V83, P149, DOI 10.1006/bioe.2002.0109.
   Borregaard T, 2000, J AGR ENG RES, V75, P389, DOI 10.1006/jaer.1999.0519.
   Brunetti G, 2019, SCI HORTIC-AMSTERDAM, V252, P342, DOI 10.1016/j.scienta.2019.04.002.
   Chaudhary P, 2018, J FOOD SCI TECH MYS, V55, P2833, DOI 10.1007/s13197-018-3221-z.
   Fennimore SA, 2016, WEED TECHNOL, V30, P823, DOI 10.1614/WT-D-16-00070.1.
   Ferreira AD, 2017, COMPUT ELECTRON AGR, V143, P314, DOI 10.1016/j.compag.2017.10.027.
   Garcia-Santillan ID, 2018, BIOSYST ENG, V166, P28, DOI 10.1016/j.biosystemseng.2017.11.003.
   Hall David, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5223, DOI 10.1109/ICRA.2017.7989612.
   Hamuda E, 2016, COMPUT ELECTRON AGR, V125, P184, DOI 10.1016/j.compag.2016.04.024.
   Huang W., 2015, COMPUT SCI INF TECHN, V3, P70, DOI DOI 10.13189/CSIT.2015.030303.
   Ibaraki Y., 2014, PLANT IMAGE ANAL FUN.
   Johansen K, 2020, FRONT ARTIF INTELL, V3, DOI 10.3389/frai.2020.00028.
   Johansen K, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.00370.
   Lamm RD, 2002, T ASAE, V45, P231.
   Lee W. S., 1999, Precision Agriculture, V1, P95, DOI 10.1023/A:1009977903204.
   Li Y, 2022, COMPUT ELECTRON AGR, V196, DOI 10.1016/j.compag.2022.106880.
   Lin FF, 2017, SUSTAINABILITY-BASEL, V9, DOI 10.3390/su9081335.
   Lottes P, 2017, J FIELD ROBOT, V34, P1160, DOI 10.1002/rob.21675.
   Ma X, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0215676.
   Manh AG, 2001, J AGR ENG RES, V80, P139, DOI 10.1006/jaer.2001.0725.
   Milioto A, 2018, IEEE INT CONF ROBOT, P2229.
   Nguyen T. T., 2017, 2017 ASABE ANN INT M, V1.
   Perez-Ortiz M, 2016, EXPERT SYST APPL, V47, P85, DOI 10.1016/j.eswa.2015.10.043.
   Qasem J.R., 2019, SEED DORMANCY GERMIN, DOI DOI 10.5772/INTECHOPEN.88015.
   Raja R., 2019, 2019 ASABE ANN INT M, V1.
   Raja R, 2020, BIOSYST ENG, V194, P152, DOI 10.1016/j.biosystemseng.2020.03.022.
   Raja R, 2020, BIOSYST ENG, V192, P257, DOI 10.1016/j.biosystemseng.2020.02.002.
   Raja R, 2019, BIOSYST ENG, V187, P278, DOI 10.1016/j.biosystemseng.2019.09.011.
   Rehman TU, 2019, COMPUT ELECTRON AGR, V162, P1, DOI 10.1016/j.compag.2019.03.023.
   Rodrigo MA, 2014, CHEM REV, V114, P8720, DOI 10.1021/cr500077e.
   Rojas MA, 2017, CAN J PLANT SCI, V97, P411, DOI 10.1139/cjps-2016-0078.
   Saxe D, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P379, DOI 10.1109/AFGR.1996.557295.
   Siogkas GK, 2006, CIRCUITS AND SYSTEMS FOR SIGNAL PROCESSING , INFORMATION AND COMMUNICATION TECHNOLOGIES, AND POWER SOURCES AND SYSTEMS, VOL 1 AND 2, PROCEEDINGS, P537, DOI 10.1109/MELCON.2006.1653157.
   Slaughter DC, 2008, COMPUT ELECTRON AGR, V61, P63, DOI 10.1016/j.compag.2007.05.008.
   Su W.-H., 2020, CHALLENGES, V11, DOI {[}10.3390/challe11020023, DOI 10.3390/CHALLE11020023].
   Su WH, 2020, BIOSYST ENG, V193, P62, DOI 10.1016/j.biosystemseng.2020.02.011.
   Su WH, 2020, COMPUT ELECTRON AGR, V168, DOI 10.1016/j.compag.2019.105155.
   Subeesh A., 2022, Artificial Intelligence in Agriculture, V6, P47, DOI 10.1016/j.aiia.2022.01.002.
   Tang JL, 2018, SOFT COMPUT, V22, P7649, DOI 10.1007/s00500-018-3125-x.
   Vrindts E., 2002, Precision Agriculture, V3, P63, DOI 10.1023/A:1013326304427.
   Vuong V. L., 2017, 2017 ASABE ANN INT M, V1.
   Wang AC, 2019, COMPUT ELECTRON AGR, V158, P226, DOI 10.1016/j.compag.2019.02.005.
   Westwood JH, 2018, WEED SCI, V66, P275, DOI 10.1017/wsc.2017.78.
   Yang P., 2008, CHI 2019 P 2019 CHI, V30, P70.
   Yang YC, 2009, 2009 SYMPOSIUM ON PHOTONICS AND OPTOELECTRONICS (SOPO 2009), P909.},
Number-of-Cited-References = {48},
Times-Cited = {0},
Usage-Count-Last-180-days = {16},
Usage-Count-Since-2013 = {16},
Journal-ISO = {Front. Plant Sci.},
Doc-Delivery-Number = {C9LR4},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000965055100001},
OA = {Green Published, gold},
DA = {2023-08-12},
}
