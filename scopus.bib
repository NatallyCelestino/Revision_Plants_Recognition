Scopus
EXPORT DATE: 12 August 2023

@ARTICLE{Park2022,
	author = {Park, Geonha and Lee, Yun-Gyo and Yoon, Ye-Seul and Ahn, Ji-Young and Lee, Jei-Wan and Jang, Young-Pyo},
	title = {Machine Learning-Based Species Classification Methods Using DART-TOF-MS Data for Five Coniferous Wood Species},
	year = {2022},
	journal = {Forests},
	volume = {13},
	number = {10},
	doi = {10.3390/f13101688},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140708403&doi=10.3390%2ff13101688&partnerID=40&md5=0dd19292814201d632ab0f90753b64b5},
	affiliations = {Department of Life and Nanopharmaceutical Sciences, Graduate School, Kyung Hee University, Seoul, 02447, South Korea; Department of Biomedical and Pharmaceutical Sciences, Graduate School, Kyung Hee University, Seoul, 02447, South Korea; Department of Basic Pharmaceutical Science, College of Pharmacy, Kyung Hee University, Seoul, 02447, South Korea; Department of Forest Bioresources, National Institute of Forest Science, Suwon, 16631, South Korea; Department of Integrated Drug Development and Natural Products, Graduate School, Kyung Hee University, Seoul, 02447, South Korea},
	abstract = {Various problems worldwide are caused by illegal production and distribution of timber, such as deception about timber species and origin and illegal logging. Numerous studies on wood tracking are being conducted around the world to demonstrate the legitimacy of timber. Tree species identification is the most basic element of wood tracking research because the quality of wood varies greatly from species to species and is consistent with the botanical origin of commercially distributed wood. Although many recent studies have combined machine learning-based classification methods with various analytical methods to identify tree species, it is unclear which classification model is most effective. The purpose of this work is to examine and compare the performance of three supervised machine learning classification models, support vector machine (SVM), random forest (RF), and artificial neural network (ANN), in identifying five conifer species and propose an optimal model. Using direct analysis in real-time ionization combined with time-of-flight mass spectrometry (DART-TOF-MS), metabolic fingerprints of 250 individual specimens representing five species were collected three times. When the machine learning models were applied to classify the wood species, ANN outperformed SVM and RF. All three models showed 100% prediction accuracy for genus classification. For species classification, the ANN model had the highest prediction accuracy of 98.22%. The RF model had an accuracy of 94.22%, and the SVM had the lowest accuracy of 92.89%. These findings demonstrate the practicality of authenticating wood species by combining DART-TOF-MS with machine learning, and they indicate that ANN is the best model for wood species identification. © 2022 by the authors.},
	author_keywords = {artificial neural network (ANN); classification method; DART-TOF-MS; machine learning; random forest (RF); support vector machine (SVM); wood species classification},
	keywords = {Accuracy; Analysis; Classification; Forestry; Machinery; Models; Neural Networks; Wood Species; Crime; Decision trees; Forestry; Inductively coupled plasma; Learning systems; Mass spectrometry; Neural networks; Random forests; Timber; Artificial neural network; Classification methods; DART-TOF-MS; Machine-learning; Random forest; Random forests; Species classification; Support vector machine; Support vectors machine; Wood species classification; accuracy assessment; artificial neural network; classification; coniferous tree; mass spectrometry; support vector machine; timber; wood quality; Support vector machines},
	correspondence_address = {Y.-P. Jang; Department of Life and Nanopharmaceutical Sciences, Graduate School, Kyung Hee University, Seoul, 02447, South Korea; email: ypjang@khu.ac.kr},
	publisher = {MDPI},
	issn = {19994907},
	language = {English},
	abbrev_source_title = {Forests},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Yang202210581,
	author = {Yang, Ziying and He, Wenyan and Fan, Xijian and Tjahjadi, Tardi},
	title = {PlantNet: transfer learning-based fine-grained network for high-throughput plants recognition},
	year = {2022},
	journal = {Soft Computing},
	volume = {26},
	number = {20},
	pages = {10581 – 10590},
	doi = {10.1007/s00500-021-06689-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122315574&doi=10.1007%2fs00500-021-06689-y&partnerID=40&md5=1da8ef3ffc1a92ebf83430f472b1c549},
	affiliations = {College of Information Science and Technology, Nanjing Forestry University, Nanjing, China; School of Engineering, University of Warwick, Coventry, CV4 7AL, United Kingdom},
	abstract = {In high-throughput phenotyping, recognizing individual plant categories is a vital support process for plant breeding. However, different plant categories have different fine-grained characteristics, i.e., intra-class variation and inter-class similarity, making the process challenging. Existing deep learning-based recognition methods fail to effectively address this recognition task under challenging requirements, leading to technical difficulties such as low accuracy and lack of generalization robustness. To address these requirements, this paper proposes PlantNet, a fine-grained network for plant recognition based on transfer learning and a bilinear convolutional neural network, which achieves high recognition accuracy in high-throughput phenotyping requirements. The network operates as follows. First, two deep feature extractors are constructed using transfer learning. The outer product of the different spatial locations corresponding to the two features is then calculated, and the bilinear convergence is computed for the different spatial locations. Finally, the fused bilinear vectors are normalized via maximum expectation to generate the network output. Experiments on a publicly available Arabidopsis dataset show that the proposed bilinear model performed better than related state-of-the-art methods. The interclass recognition accuracy of the four different species of Arabidopsis Sf-2, Cvi, Landsberg and Columbia are found to be 98.48%, 96.53%, 96.79% and 97.33%, respectively, with an average accuracy of 97.25%. Thus, the network has good generalization ability and robust performance, satisfying the needs of fine-grained plant recognition in agricultural production. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
	author_keywords = {Bilinear-CNN; Convolutional neural network; Fine-grained recognition; Transfer learning},
	keywords = {Agriculture; Convolution; Deep learning; Transfer learning; Bilinear-CNN; Convolutional neural network; Fine grained; Fine-grained recognition; High-throughput phenotyping; Inter class; Plant recognition; Recognition accuracy; Spatial location; Transfer learning; Convolutional neural networks},
	correspondence_address = {X. Fan; College of Information Science and Technology, Nanjing Forestry University, Nanjing, China; email: xijian.fan@njfu.edu.cn},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {14327643},
	language = {English},
	abbrev_source_title = {Soft Comput.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Green Open Access}
}

@ARTICLE{Thanikkal20231189,
	author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M.T.},
	title = {An Efficient Mobile Application for Identification of Immunity Boosting Medicinal Plants using Shape Descriptor Algorithm},
	year = {2023},
	journal = {Wireless Personal Communications},
	volume = {131},
	number = {2},
	pages = {1189 – 1205},
	doi = {10.1007/s11277-023-10476-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153232716&doi=10.1007%2fs11277-023-10476-3&partnerID=40&md5=0e30004bbf51d95fbdb839821ba31740},
	affiliations = {Department of Computer Science and Engineering, Amity School of Engineering and Technology, Amity University Uttar Pradesh, U.P., Noida, 201313, India; Department of Electronics and Communication Engineering, Amity School of Engineering and Technology, Amity University Uttar Pradesh, U.P., Noida, 201313, India; Department of Botany, St. Thomas College, Kerala, Thrissur, India},
	abstract = {In the Covid-19 pandemic situation, the world is looking for immunity-boosting techniques for fighting against coronavirus. Every plant is medicine in one or another way, but Ayurveda explains the uses of plant-based medicines and immunity boosters for specific requirements of the human body. To help Ayurveda, botanists are trying to identify more species of medicinal immunity-boosting plants by evaluating the characteristics of the leaf. For a normal person, detecting immunity-boosting plants is a difficult task. Deep learning networks provide highly accurate results in image processing. In the medicinal plant analysis, many leaves are like each other. So, the direct analysis of leaf images using the deep learning network causes many issues for medicinal plant identification. Hence, keeping the requirement of a method at large to help all human beings, the proposed leaf shape descriptor with the deep learning-based mobile application is developed for the identification of immunity-boosting medicinal plants using a smartphone. SDAMPI algorithm explained numerical descriptor generation for closed shapes. This mobile application achieved 96%accuracy for the 64 × 64 sized images. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Bigram; COVID-19; Deep learning; Medicinal plants; Mobile app; Shape descriptor},
	keywords = {Deep learning; Image processing; Learning systems; Mobile computing; Plants (botany); Based medicines; Bigrams; Coronaviruses; Deep learning; Immunity boosters; Learning network; Medicinal plants; Mobile app; Mobile applications; Shape descriptors; Coronavirus; COVID-19},
	correspondence_address = {A.K. Dubey; Department of Electronics and Communication Engineering, Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida, U.P., 201313, India; email: dubey1ak@gmail.com},
	publisher = {Springer},
	issn = {09296212},
	coden = {WPCOF},
	language = {English},
	abbrev_source_title = {Wireless Pers Commun},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Bronze Open Access}
}

@ARTICLE{Lee2023,
	author = {Lee, Chin Poo and Lim, Kian Ming and Song, Yu Xuan and Alqahtani, Ali},
	title = {Plant-CNN-ViT: Plant Classification with Ensemble of Convolutional Neural Networks and Vision Transformer},
	year = {2023},
	journal = {Plants},
	volume = {12},
	number = {14},
	doi = {10.3390/plants12142642},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166225061&doi=10.3390%2fplants12142642&partnerID=40&md5=343fe7b92422f96a2b6a73bb65950742},
	affiliations = {Faculty of Information Science and Technology, Multimedia University, Melaka, 75450, Malaysia; Department of Computer Science, King Khalid University, Abha, 61421, Saudi Arabia; Center for Artificial Intelligence (CAI), King Khalid University, Abha, 61421, Saudi Arabia},
	abstract = {Plant leaf classification involves identifying and categorizing plant species based on leaf characteristics, such as patterns, shapes, textures, and veins. In recent years, research has been conducted to improve the accuracy of plant classification using machine learning techniques. This involves training models on large datasets of plant images and using them to identify different plant species. However, these models are limited by their reliance on large amounts of training data, which can be difficult to obtain for many plant species. To overcome this challenge, this paper proposes a Plant-CNN-ViT ensemble model that combines the strengths of four pre-trained models: Vision Transformer, ResNet-50, DenseNet-201, and Xception. Vision Transformer utilizes self-attention to capture dependencies and focus on important leaf features. ResNet-50 introduces residual connections, aiding in efficient training and hierarchical feature extraction. DenseNet-201 employs dense connections, facilitating information flow and capturing intricate leaf patterns. Xception uses separable convolutions, reducing the computational cost while capturing fine-grained details in leaf images. The proposed Plant-CNN-ViT was evaluated on four plant leaf datasets and achieved remarkable accuracy of 100.00%, 100.00%, 100.00%, and 99.83% on the Flavia dataset, Folio Leaf dataset, Swedish Leaf dataset, and MalayaKew Leaf dataset, respectively. © 2023 by the authors.},
	author_keywords = {convolutional neural network; deep learning; plant classification; plant leaf classification; Vision Transformer},
	correspondence_address = {K.M. Lim; Faculty of Information Science and Technology, Multimedia University, Melaka, 75450, Malaysia; email: kmlim@mmu.edu.my},
	publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
	issn = {22237747},
	language = {English},
	abbrev_source_title = {Plants},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Thanikkal2023448,
	author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M.T.},
	title = {A novel edge detection method for medicinal plant's leaf features extraction},
	year = {2023},
	journal = {International Journal of System Assurance Engineering and Management},
	volume = {14},
	number = {1},
	pages = {448 – 458},
	doi = {10.1007/s13198-022-01814-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143438676&doi=10.1007%2fs13198-022-01814-y&partnerID=40&md5=153c419a59e521dfa95749f42a6c8690},
	affiliations = {Department of Computer Science and Engineering, Amity School of Engineering and Technology, Amity University Uttar Pradesh, U.P, Noida, 201313, India; Department of Electronics and Communication Engineering, Amity School of Engineering and Technology, Amity University Uttar Pradesh, U.P, Noida, 201313, India; Department of Botany, St. Thomas College, Kerala, Thrissur, India},
	abstract = {Morphological features-based leaf identification algorithms provide highly accurate results. But it is required a single-lined edge extraction algorithm for morphological feature generation. Existing edge extraction algorithms have heavy calculations and higher iteration steps to extract edges. The simplicity of the edge detection algorithm helps to reduce the complexity of the image feature extraction process. In this paper, a fast and straightforward novel edge detection algorithm is introduced in the spatial domain. In a single iteration over all the pixels of the image, our algorithm can achieve a better result than existing edge detection techniques. Also, this paper provides a novel algorithm for leaf shape, vein, apex, and base feature extraction techniques using the edge detection algorithm that can be utilized further for the classification and identification of medicinal plant species or any other plant species too. The performance measure of the proposed edge detection algorithm for leaf features is better as compared to the existing edge detection algorithms. This edge detection algorithm achieved 92% of accuracy and a PSNR rate of 10.88 dB with the time complexity of O(n*m), where n is the height and m is the width of the given image. The importance of medicinal plant identification and existing leaf identification techniques are also discussed in this paper. © 2022, The Author(s) under exclusive licence to The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.},
	author_keywords = {Edge detection; Image processing; Leaves classification; Morphological features; Plant identification},
	keywords = {Extraction; Feature extraction; Image classification; Plants (botany); Signal detection; Detection algorithm; Edge extraction; Extraction algorithms; Images processing; Leaf classification; Leaf identification; Medicinal plants; Morphological features; Plant identification; Plant species; Iterative methods},
	correspondence_address = {A.K. Dubey; Department of Electronics and Communication Engineering, Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida, U.P, 201313, India; email: dubey1ak@gmail.com},
	publisher = {Springer},
	issn = {09756809},
	language = {English},
	abbrev_source_title = {Intl. J. Syst. Assur. Eng. Manage.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Mahurkar2023429,
	author = {Mahurkar, Dipak Pralhad and Patidar, Hemant},
	title = {Particular Leaf Contour-Based Feature Extraction Technique to Identify the Species when the Leaf is shrouded},
	year = {2023},
	journal = {International Journal of Computer Information Systems and Industrial Management Applications},
	volume = {15},
	number = {2023},
	pages = {429 – 437},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166046828&partnerID=40&md5=bcbf39b3369ec81a83cdda81db59a969},
	affiliations = {Electronics and Communication Engineering, Oriental University, Sanwer Road,Jakhiya, Indore, India},
	abstract = {A critical and challenging in pattern recognition is the identification of plant species from obstructed leaf photographs. The biggest issue at that time is to accurately identify the species of leaf when all of the leaves are identical in appearance and obscured. Shape is one of the most important visual elements and is also recognized as a fundamental attribute for conveying the content of pictures. Since it can be difficult to gauge how similar distinct forms are to one another, as well as describe the content of shapes. The two main categories of shape descriptors are region-based and contour-based shape descriptors (CBSD) techniques. Region-based approaches use the complete area of an item for shape description as opposed to contour-based approaches that only use the information contained in an image's contour. In this study, we presented a shape description approach called Particular Contour-Based Shape Descriptors (PCBSD) for the identification of the plant leaves since the CBSD recovered the low level visual properties of the pictures. This method successfully captures the local and global characteristics of a leaf shape while preserving the translation, rotation, and scaling similarity transformations. This method is also quite compact and has a low processing complexity. To evaluate our experiments, we utilized Flavia datasets of typical plant leaves. We show that our technique created the best complete leaf match when high occlusion (around 50% occlusion) occurs. We may say that our method exceeds prior state-of-the-art shape-based plant leaf recognition algorithms and it generates accuracy of 76%.Picture processing methods are used to separate the leaf-based characteristics from the leaf image. Eventually, using machine learning methods, then leaf identification was accomplished. © MIR Labs, www.mirlabs.net/ijcisim/index.html},
	author_keywords = {Contour based; Feature extraction; K-Nearest Neighbor Classifier; Leaf Classification; Plant Species Identification},
	correspondence_address = {D.P. Mahurkar; Electronics and Communication Engineering, Oriental University, Indore, Sanwer Road,Jakhiya, India; email: dipsmahu11@gmail.com},
	publisher = {Machine Intelligence Research (MIR) Labs},
	issn = {21507988},
	language = {English},
	abbrev_source_title = {Int.  J.  Comput.  Inf.  Sys. Ind.  Manage.  Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Batchuluun2023,
	author = {Batchuluun, Ganbayar and Nam, Se Hyun and Park, Chanhum and Park, Kang Ryoung},
	title = {Super-Resolution Reconstruction-Based Plant Image Classification Using Thermal and Visible-Light Images},
	year = {2023},
	journal = {Mathematics},
	volume = {11},
	number = {1},
	doi = {10.3390/math11010076},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145934329&doi=10.3390%2fmath11010076&partnerID=40&md5=9ed8541815c5e9d3dbb420e56002e121},
	affiliations = {Division of Electronics and Electrical Engineering, Dongguk University, 30 Pildong-ro, 1-gil, Jung-gu, Seoul, 04620, South Korea},
	abstract = {Few studies have been conducted on thermal plant images. This is because of the difficulty in extracting and analyzing various color-related patterns and features from the plant image obtained using a thermal camera, which does not provide color information. In addition, the thermal camera is sensitive to the surrounding temperature and humidity. However, the thermal camera enables the extraction of invisible patterns in the plant by providing external and internal heat information. Therefore, this study proposed a novel plant classification method based on both the thermal and visible-light plant images to exploit the strengths of both types of cameras. To the best of our knowledge, this study is the first to perform super-resolution reconstruction using visible-light and thermal plant images. Furthermore, a method to improve the classification performance through generative adversarial network (GAN)-based super-resolution reconstruction was proposed. Through the experiments using a self-collected dataset of thermal and visible-light images, our method shows higher accuracies than the state-of-the-art methods. © 2022 by the authors.},
	author_keywords = {classification; deep learning; plant image; super-resolution reconstruction},
	correspondence_address = {K.R. Park; Division of Electronics and Electrical Engineering, Dongguk University, Seoul, 30 Pildong-ro, 1-gil, Jung-gu, 04620, South Korea; email: parkgr@dongguk.edu},
	publisher = {MDPI},
	issn = {22277390},
	language = {English},
	abbrev_source_title = {Mathematics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Shah2023,
	author = {Shah, Sabab Ali and Lakho, Ghulam Mustafa and Keerio, Hareef Ahmed and Sattar, Muhammad Nouman and Hussain, Gulzar and Mehdi, Mujahid and Vistro, Rahim Bux and Mahmoud, Eman A. and Elansary, Hosam O.},
	title = {Application of Drone Surveillance for Advance Agriculture Monitoring by Android Application Using Convolution Neural Network},
	year = {2023},
	journal = {Agronomy},
	volume = {13},
	number = {7},
	doi = {10.3390/agronomy13071764},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165986568&doi=10.3390%2fagronomy13071764&partnerID=40&md5=72f9cdab5acfbadaf5b1ba45a334b482},
	affiliations = {Research Institute of Engineering and Technology, Hanyang University, Ansan, 15588, South Korea; Faculty of Architecture and Town Planning, Aror University of Art, Architecture, Design and Heritage, Sukkur, 6500, Pakistan; Department of Computer Engineering, Sun Moon University, Asan, 31461, South Korea; Department of Environmental Engineering, Quaid-e-Awam University of Engineering, Science and Technology, Nawabshah, 67210, Pakistan; Department of Civil Engineering, National University of Technology, Islamabad, 44000, Pakistan; Faculty of Design, Aror University of Art, Architecture, Design and Heritage, Sukkur, 6500, Pakistan; Department of Irrigation and Drainage, Faculty of Agricultural Engineering, Sindh Agriculture University, Tandojam, 70060, Pakistan; Department of Food Industries, Faculty of Agriculture, Damietta University, Damietta, 34511, Egypt; Department of Plant Production, College of Food Agriculture Sciences, King Saud University, P.O. Box 2460, Riyadh, 11451, Saudi Arabia},
	abstract = {Plant diseases are a significant threat to global food security, impacting crop yields and economic growth. Accurate identification of plant diseases is crucial to minimize crop loses and optimize plant health. Traditionally, plant classification is performed manually, relying on the expertise of the classifier. However, recent advancements in deep learning techniques have enabled the creation of efficient crop classification systems using computer technology. In this context, this paper proposes an automatic plant identification process based on a synthetic neural network with the ability to detect images of plant leaves. The trained model EfficientNet-B3 was used to achieve a high success rate of 98.80% in identifying the corresponding combination of plant and disease. To make the system user-friendly, an Android application and website were developed, which allowed farmers and users to easily detect diseases from the leaves. In addition, the paper discusses the transfer method for studying various plant diseases, and images were captured using a drone or a smartphone camera. The ultimate goal is to create a user-friendly leaf disease product that can work with mobile and drone cameras. The proposed system provides a powerful tool for rapid and efficient plant disease identification, which can aid farmers of all levels of experience in making informed decisions about the use of chemical pesticides and optimizing plant health. © 2023 by the authors.},
	author_keywords = {drone; EfficientNet-B3; leaf disease product; plant disease identification},
	correspondence_address = {S.A. Shah; Research Institute of Engineering and Technology, Hanyang University, Ansan, 15588, South Korea; email: sayedsabab@hanyang.ac.kr; H.O. Elansary; Department of Plant Production, College of Food Agriculture Sciences, King Saud University, Riyadh, P.O. Box 2460, 11451, Saudi Arabia; email: helansary@ksu.edu.sa},
	publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
	issn = {20734395},
	language = {English},
	abbrev_source_title = {Agronomy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Arunkumar2023,
	author = {Arunkumar, K. and Leninisha, S.},
	title = {An effective identification between various plant species using shape descriptors and image processing technique},
	year = {2023},
	journal = {Proceedings of 2023 International Conference on Signal Processing, Computation, Electronics, Power and Telecommunication, IConSCEPT 2023},
	doi = {10.1109/IConSCEPT57958.2023.10170691},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166366726&doi=10.1109%2fIConSCEPT57958.2023.10170691&partnerID=40&md5=6f3d620320a045a822f3bc5f78e08aa5},
	affiliations = {SCOPE, VIT, Chennai Campus, Chennai, India},
	abstract = {A modern agricultural sector requires accurate crop identification and classification. A new computer vision system is presented here that successfully discriminates between various plant species in real time under uncontrolled lighting. Features are vital for image classification and shape descriptors are mainly considered in this study. This system consists of image processing delivering results in real-time and a pixel calculator with more accuracy. Using these components together results in an efficient, reliable system for achieving excellent results in many different situations. Tested on several leaf species taken from the UCI repository. The system successfully detects an average of 87% under different variety of species. Additionally, the system has shown to produce acceptable results even under extremely challenging conditions, such as disease infected leaf or irregular shape leaf. The leaf boundaries was determined and evaluated through Harris corner algorithm. Compared to other high-cost methods, it was observed high species classification and lower testing time for our approach. The researchers also discussed challenges and solutions related to leaf classification, including identifying different leaves, classes of leaf shapes, lighting conditions, and stages of growth.  © 2023 IEEE.},
	author_keywords = {Crop identification; Deep learning; Image processing; Plant classification; Precision agriculture},
	keywords = {Crops; Deep learning; Lighting; Plants (botany); Precision agriculture; Crop identification; Deep learning; Image processing technique; Images processing; Leaf shape; Plant classification; Plant species; Precision Agriculture; Real- time; Shape descriptors; Image classification},
	correspondence_address = {K. Arunkumar; SCOPE, VIT, Chennai, Chennai Campus, India; email: arunkumar.kathirvel2020@vitstudent.ac.in},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835031212-6},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Signal Process., Comput., Electron., Power Telecommun., IConSCEPT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2023 International Conference on Signal Processing, Computation, Electronics, Power and Telecommunication, IConSCEPT 2023; Conference date: 25 May 2023 through 26 May 2023; Conference code: 190623}
}

@ARTICLE{Roslan2023136,
	author = {Roslan, Noor Aini Mohd and Diah, Norizan Mat and Ibrahim, Zaidah and Munarko, Yuda and Minarno, Agus Eko},
	title = {Automatic plant recognition using convolutional neural network on Malaysian medicinal herbs: the value of data augmentation},
	year = {2023},
	journal = {International Journal of Advances in Intelligent Informatics},
	volume = {9},
	number = {1},
	pages = {136 – 147},
	doi = {10.26555/ijain.v9i1.1076},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153604549&doi=10.26555%2fijain.v9i1.1076&partnerID=40&md5=1738e6877dcdb04ba44672e115d1ce18},
	affiliations = {School of Computing Sciences, College of Computing, Informatics and Media Universiti Teknologi MARA, Selangor, Malaysia; Universitas Muhammadiyah Malang, Informatics, Malang, Indonesia},
	abstract = {Herbs are an important nutritional source for humans since they provide a variety of nutrients. Indigenous people have employed herbs, in particular, as traditional medicines since ancient times. Malaysia has hundreds of plant species; herb detection may be difficult due to the variety of herb species and their shape and color similarities. Furthermore, there is a scarcity of support datasets for detecting these plants. The main objective of this paper is to investigate the performance of convolutional neural network (CNN) on Malaysian medicinal herbs datasets, real data and augmented data. Malaysian medical herbs data were obtained from Taman Herba Pulau Pinang, Malaysia, and ten kinds of native herbs were chosen. Both datasets were evaluated using the CNN model developed throughout the research. Overall, herbs real data obtained an average accuracy of 75%, whereas herbs augmented data achieved an average accuracy of 88%. Based on these findings, herbs augmented data surpassed herbs actual data in terms of accuracy after undergoing the augmentation technique. © 2023, Universitas Ahmad Dahlan. All rights reserved.},
	author_keywords = {Convolutional neural network (CNN); Data augmentation; Deep learning; Malaysian medicinal herbs},
	correspondence_address = {N.A.M. Roslan; School of Computing Sciences, College of Computing, Informatics and Media Universiti Teknologi MARA, Selangor, Malaysia; email: noorainimohdroslan@gmail.com},
	publisher = {Universitas Ahmad Dahlan},
	issn = {24426571},
	language = {English},
	abbrev_source_title = {Int. J. Adv. Intell. Inform.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Kopeć2023,
	author = {Kopeć, Dominik and Zakrzewska, Agata and Halladin-Dąbrowska, Anna and Wylazłowska, Justyna and Sławik, Łukasz},
	title = {The essence of acquisition time of airborne hyperspectral and on-ground reference data for classification of highly invasive annual vine Echinocystis lobata (Michx.) Torr. & A. Gray},
	year = {2023},
	journal = {GIScience and Remote Sensing},
	volume = {60},
	number = {1},
	doi = {10.1080/15481603.2023.2204682},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153333407&doi=10.1080%2f15481603.2023.2204682&partnerID=40&md5=e23c34e3b83d2387ae12f5ff27124d88},
	affiliations = {Department of Biogeography, Paleoecology, and Nature Conservation, Faculty of Biology and Environmental Protection, University of Lodz, Łódź, Poland; Department of Remote Sensing, MGGP Aero sp. z o.o, Tarnów, Poland},
	abstract = {Invasive alien species are one of the biggest threats to biodiversity today. Identifying their locations are mandatory parts of the strategies being developed to control them. Remote sensing along with machine learning are already proven and effective tools for monitoring invasive species, especially trees, shrubs, and tall perennials. However, annual vine species are particularly difficult to map using remote sensing because of their dynamic plant growth and the movement of shoots during the growing season. Therefore, the phenological phase in which the data is acquired, and the synchronization of airborne data acquisition with on-ground reference data, may be key factors for correct plant classification. This research aimed to answer the following questions: (i) What is the impact of acquiring synchronized on-ground data and hyperspectral data in different phases of plants’ phenological development on the annual vine IAPS (Invasive Alien Plant Species) classification results? (ii) How does the lack of synchronization while obtaining hyperspectral and on-ground data collection impact annual vine IAPS mapping results? (iii) Does multitemporal image fusion improve the results of annual vine IAPS classification? For this purpose, research was carried out on Echinocystis lobata, an annual vine species considered highly invasive in Europe. The obtained results indicate that the phenological phase in which the data is acquired has a very strong influence on the quality of the classification result. The period of flowering (summer) with the greatest coverage of the area with shoots was optimal for the classification of Echinocystis lobata with F1 classification accuracy of 0.87 ± 0.04. The accuracy of classifications was significantly less for spring (F1 = 0.64 ± 0.04) and autumn (F1 = 0.75 ± 0.05). Obtaining on-ground reference data that is mismatched temporally with hyperspectral data causes a decrease in the accuracy of the result up to 0.08 (from F1 = 0.64 to 0.56) in relation to data obtained synchronously. In the multitemporal image fusion method, using hyperspectral data linked from different phases of plants’ development to classify an image had a minimal improvement in classification accuracy compared to classifications trained on images from one phenological stage. The main conclusion is that mapping an annual vine using remote sensing and machine learning is possible and highly effective, provided the remote sensing and on-ground data are obtained in strict synchronization and the appropriate phenological phase. For the most efficient classification results, a single data acquisition per year is enough, even in the case of annual vine IAPS. Further research is needed to explore the possibility of mapping Echinocystis lobata using, i.e. multispectral or hyperspectral satellite data (e.g. EnMAP). © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.},
	author_keywords = {machine learning; multitemporal data fusion; plant phenology; plant species classification; random forest; time series},
	keywords = {airborne sensing; classification; data acquisition; invasive species; machine learning; monitoring; phenology; remote sensing; satellite data; time series; vegetation cover; vegetation mapping; vine},
	correspondence_address = {D. Kopeć; Department of Biogeography, Paleoecology, and Nature Conservation, Faculty of Biology and Environmental Protection, University of Lodz, Łódź, Poland; email: dominik.kopec@biol.uni.lodz.pl},
	publisher = {Taylor and Francis Ltd.},
	issn = {15481603},
	language = {English},
	abbrev_source_title = {GISci. Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Azadnia2022,
	author = {Azadnia, Rahim and Al-Amidi, Mohammed Maitham and Mohammadi, Hamed and Cifci, Mehmet Akif and Daryab, Avat and Cavallo, Eugenio},
	title = {An AI Based Approach for Medicinal Plant Identification Using Deep CNN Based on Global Average Pooling},
	year = {2022},
	journal = {Agronomy},
	volume = {12},
	number = {11},
	doi = {10.3390/agronomy12112723},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141741679&doi=10.3390%2fagronomy12112723&partnerID=40&md5=6751897281aad243b7c51227368782ad},
	affiliations = {Department of Biosystems Engineering, University of Tehran, Karaj, 3158777871, Iran; Information Technology, Al-Mustaqbal University College, Babylon, 51001, Iraq; Department of Industrial Engineering and Management Systems, University of Central Florida, Orlando, 32816, FL, United States; Department of Computer Engineering, Bandirma Onyedi Eylul University, Balikesir, 10200, Turkey; Department of Agricultural, Karaj Branch, Islamic Azad University, Karaj, 3158777871, Iran; Institute of Sciences and Technologies for Sustainable Energy and Mobility (STEMS), National Research Council (CNR) of Italy, Torino, 10129, Italy},
	abstract = {Medicinal plants have always been studied and considered due to their high importance for preserving human health. However, identifying medicinal plants is very time-consuming, tedious and requires an experienced specialist. Hence, a vision-based system can support researchers and ordinary people in recognising herb plants quickly and accurately. Thus, this study proposes an intelligent vision-based system to identify herb plants by developing an automatic Convolutional Neural Network (CNN). The proposed Deep Learning (DL) model consists of a CNN block for feature extraction and a classifier block for classifying the extracted features. The classifier block includes a Global Average Pooling (GAP) layer, a dense layer, a dropout layer, and a softmax layer. The solution has been tested on 3 levels of definitions (64 × 64, 128 × 128 and 256 × 256 pixel) of images for leaf recognition of five different medicinal plants. As a result, the vision-based system achieved more than 99.3% accuracy for all the image definitions. Hence, the proposed method effectively identifies medicinal plants in real-time and is capable of replacing traditional methods. © 2022 by the authors.},
	author_keywords = {Convolutional Neural Network (CNN); Global Average Pooling (GAP); identification; image processing; medicinal plant},
	correspondence_address = {E. Cavallo; Institute of Sciences and Technologies for Sustainable Energy and Mobility (STEMS), National Research Council (CNR) of Italy, Torino, 10129, Italy; email: eugenio.cavallo@cnr.it},
	publisher = {MDPI},
	issn = {20734395},
	language = {English},
	abbrev_source_title = {Agronomy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access}
}

@ARTICLE{Beloiu2023,
	author = {Beloiu, Mirela and Heinzmann, Lucca and Rehush, Nataliia and Gessler, Arthur and Griess, Verena C.},
	title = {Individual Tree-Crown Detection and Species Identification in Heterogeneous Forests Using Aerial RGB Imagery and Deep Learning},
	year = {2023},
	journal = {Remote Sensing},
	volume = {15},
	number = {5},
	doi = {10.3390/rs15051463},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149931270&doi=10.3390%2frs15051463&partnerID=40&md5=a007e871500621dda5c6b775d912a295},
	affiliations = {Department of Environmental Systems Science, Institute of Terrestrial Ecosystems, ETH Zurich, Zurich, 8092, Switzerland; Forest Resources and Management, Swiss Federal Institute for Forest, Snow and Landscape WSL, Birmensdorf, 8903, Switzerland; Forest Dynamics, Swiss Federal Institute for Forest, Snow and Landscape Research WSL, Birmensdorf, 8903, Switzerland},
	abstract = {Automatic identification and mapping of tree species is an essential task in forestry and conservation. However, applications that can geolocate individual trees and identify their species in heterogeneous forests on a large scale are lacking. Here, we assessed the potential of the Convolutional Neural Network algorithm, Faster R-CNN, which is an efficient end-to-end object detection approach, combined with open-source aerial RGB imagery for the identification and geolocation of tree species in the upper canopy layer of heterogeneous temperate forests. We studied four tree species, i.e., Norway spruce (Picea abies (L.) H. Karst.), silver fir (Abies alba Mill.), Scots pine (Pinus sylvestris L.), and European beech (Fagus sylvatica L.), growing in heterogeneous temperate forests. To fully explore the potential of the approach for tree species identification, we trained single-species and multi-species models. For the single-species models, the average detection accuracy (F1 score) was 0.76. Picea abies was detected with the highest accuracy, with an average F1 of 0.86, followed by A. alba (F1 = 0.84), F. sylvatica (F1 = 0.75), and Pinus sylvestris (F1 = 0.59). Detection accuracy increased in multi-species models for Pinus sylvestris (F1 = 0.92), while it remained the same or decreased slightly for the other species. Model performance was more influenced by site conditions, such as forest stand structure, and less by illumination. Moreover, the misidentification of tree species decreased as the number of species included in the models increased. In conclusion, the presented method can accurately map the location of four individual tree species in heterogeneous forests and may serve as a basis for future inventories and targeted management actions to support more resilient forests. © 2023 by the authors.},
	author_keywords = {conifer and deciduous species; Convolutional Neural Network (CNN); forest monitoring; temperate forest; tree species geolocation},
	keywords = {Antennas; Automation; Conservation; Convolution; Convolutional neural networks; Deep learning; Multilayer neural networks; Object detection; Plants (botany); Conifer species; Convolutional neural network; Deciduous species; Forest monitoring; Geolocations; Individual tree; Temperate forests; Tree species; Tree species geolocation; Forestry},
	correspondence_address = {M. Beloiu; Department of Environmental Systems Science, Institute of Terrestrial Ecosystems, ETH Zurich, Zurich, 8092, Switzerland; email: mirela.beloiu@usys.ethz.ch},
	publisher = {MDPI},
	issn = {20724292},
	language = {English},
	abbrev_source_title = {Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Picek2022,
	author = {Picek, Lukáš and Šulc, Milan and Patel, Yash and Matas, Jiří},
	title = {Plant recognition by AI: Deep neural nets, transformers, and kNN in deep embeddings},
	year = {2022},
	journal = {Frontiers in Plant Science},
	volume = {13},
	doi = {10.3389/fpls.2022.787527},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139567145&doi=10.3389%2ffpls.2022.787527&partnerID=40&md5=ac9c8af5656f3a7ea639a21044262f04},
	affiliations = {Department of Cybernetics, Faculty of Applied Sciences, University of West Bohemia, Pilsen, Czech Republic; Visual Recognition Group, Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic},
	abstract = {The article reviews and benchmarks machine learning methods for automatic image-based plant species recognition and proposes a novel retrieval-based method for recognition by nearest neighbor classification in a deep embedding space. The image retrieval method relies on a model trained via the Recall@k surrogate loss. State-of-the-art approaches to image classification, based on Convolutional Neural Networks (CNN) and Vision Transformers (ViT), are benchmarked and compared with the proposed image retrieval-based method. The impact of performance-enhancing techniques, e.g., class prior adaptation, image augmentations, learning rate scheduling, and loss functions, is studied. The evaluation is carried out on the PlantCLEF 2017, the ExpertLifeCLEF 2018, and the iNaturalist 2018 Datasets—the largest publicly available datasets for plant recognition. The evaluation of CNN and ViT classifiers shows a gradual improvement in classification accuracy. The current state-of-the-art Vision Transformer model, ViT-Large/16, achieves 91.15% and 83.54% accuracy on the PlantCLEF 2017 and ExpertLifeCLEF 2018 test sets, respectively; the best CNN model (ResNeSt-269e) error rate dropped by 22.91% and 28.34%. Apart from that, additional tricks increased the performance for the ViT-Base/32 by 3.72% on ExpertLifeCLEF 2018 and by 4.67% on PlantCLEF 2017. The retrieval approach achieved superior performance in all measured scenarios with accuracy margins of 0.28%, 4.13%, and 10.25% on ExpertLifeCLEF 2018, PlantCLEF 2017, and iNat2018–Plantae, respectively. Copyright © 2022 Picek, Šulc, Patel and Matas.},
	author_keywords = {classification; computer vision; fine-grained; machine learning; plant; recognition; species; species recognition},
	correspondence_address = {L. Picek; Department of Cybernetics, Faculty of Applied Sciences, University of West Bohemia, Pilsen, Czech Republic; email: lukaspicek@gmail.com},
	publisher = {Frontiers Media S.A.},
	issn = {1664462X},
	language = {English},
	abbrev_source_title = {Front. Plant Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Lv2023,
	author = {Lv, Zhimin and Zhang, Zhibin},
	title = {Research on plant leaf recognition method based on multi-feature fusion in different partition blocks},
	year = {2023},
	journal = {Digital Signal Processing: A Review Journal},
	volume = {134},
	doi = {10.1016/j.dsp.2023.103907},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146902712&doi=10.1016%2fj.dsp.2023.103907&partnerID=40&md5=53ac785cb199e288a26c982d8a8a3e62},
	affiliations = {School of Computer science, Inner Mongolia University, Hohhot, 010021, China; 3Key Laboratory of Wireless Networks and Mobile Computing, School of Computer Science, Inner Mongolia University, Hohhot, 010021, China},
	abstract = {As an indispensable organism in nature, plants play an essential role in the ecological equilibrium. Ecological plant protection is also receiving increasing attention. In today's digital era, one of the important research topics is recognizing plants by using imaging equipment. In this study, we propose a multi-feature fusion approach based on the Local Binary Pattern (LBP) feature for plant leaf recognition by using the partition block strategy. By comparing the original LBP and Multiscale Block Local Binary Pattern (MB-LBP), we present an improved LBP feature descriptor. It extends the range of feature extraction, considering the effect that the multi-neighbourhood pixels have on the central pixels during the LBP encoding process and the central pixels are represented by double coding values. Therefore, it can extract more detailed information about plant leaves. Moreover, considering the leaf boundary and shape information extraction and the illumination variations, the Histogram of Oriented Gradient (HOG) feature and the colour feature are fused with the improved LBP feature descriptor. After dimensionality reduction by Principal Component Analysis (PCA), a mixed feature vector of plant leaves is used as input to an Extreme Learning Machine (ELM) for identifying plant leaves in two publicly available datasets, the Flavia dataset and the Swedish dataset. Experimental results show that our algorithm outperforms the conventional algorithms, with the recognition accuracy of 99.30% on the Flavia dataset where 4 × 4 partition blocks are used for the improved LBP feature and 2 × 2 partition blocks for the colour feature, 99.52% on the Swedish dataset where 4 × 4 partition blocks are used for the improved LBP feature and 2 × 2 partition blocks for the colour feature, respectively. © 2023 The Author(s)},
	author_keywords = {LBP; Leaf recognition; Multi-feature fusion; Partition block},
	keywords = {Color; Ecology; Learning systems; Pixels; Plants (botany); Principal component analysis; Color features; Feature descriptors; Improved local binary patterns; Leaf recognition; Local binary patterns; Multi-feature fusion; Partition block; Pattern features; Plant leaves; Swedishs; Local binary pattern},
	correspondence_address = {Z. Zhang; School of Computer science, Inner Mongolia University, Hohhot, 010021, China; email: cszhibin@imu.edu.cn},
	publisher = {Elsevier Inc.},
	issn = {10512004},
	coden = {DSPRE},
	language = {English},
	abbrev_source_title = {Digital Signal Process Rev J},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Yordanov2023,
	author = {Yordanov, Momchil and d’Andrimont, Raphaël and Martinez-Sanchez, Laura and Lemoine, Guido and Fasbender, Dominique and van der Velde, Marijn},
	title = {Crop Identification Using Deep Learning on LUCAS Crop Cover Photos},
	year = {2023},
	journal = {Sensors},
	volume = {23},
	number = {14},
	doi = {10.3390/s23146298},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166002838&doi=10.3390%2fs23146298&partnerID=40&md5=54b02a758f75aed437c1bd2f0531d5ca},
	affiliations = {SEIDOR Consulting S.L, Barcelona, 08500, Spain; European Commission, Joint Research Centre (JRC), Ispra, 21027, Italy; Walloon Institute of Evaluation, Foresight and Statistics (IWEPS), Namur, 5001, Belgium},
	abstract = {Massive and high-quality in situ data are essential for Earth-observation-based agricultural monitoring. However, field surveying requires considerable organizational effort and money. Using computer vision to recognize crop types on geo-tagged photos could be a game changer allowing for the provision of timely and accurate crop-specific information. This study presents the first use of the largest multi-year set of labelled close-up in situ photos systematically collected across the European Union from the Land Use Cover Area frame Survey (LUCAS). Benefiting from this unique in situ dataset, this study aims to benchmark and test computer vision models to recognize major crops on close-up photos statistically distributed spatially and through time between 2006 and 2018 in a practical agricultural policy relevant context. The methodology makes use of crop calendars from various sources to ascertain the mature stage of the crop, of an extensive paradigm for the hyper-parameterization of MobileNet from random parameter initialization, and of various techniques from information theory in order to carry out more accurate post-processing filtering on results. The work has produced a dataset of 169,460 images of mature crops for the 12 classes, out of which 15,876 were manually selected as representing a clean sample without any foreign objects or unfavorable conditions. The best-performing model achieved a macro F1 (M-F1) of 0.75 on an imbalanced test dataset of 8642 photos. Using metrics from information theory, namely the equivalence reference probability, resulted in an increase of 6%. The most unfavorable conditions for taking such images, across all crop classes, were found to be too early or late in the season. The proposed methodology shows the possibility of using minimal auxiliary data outside the images themselves in order to achieve an M-F1 of 0.82 for labelling between 12 major European crops. © 2023 by the authors.},
	author_keywords = {agriculture; computer vision; data valorization; deep learning; image classification algorithms; mapping from imagery; plant recognition},
	keywords = {Computer games; Computer vision; Deep learning; Image classification; Information filtering; Land use; Statistical tests; Condition; Cover areas; Crop identification; Data valorization; Deep learning; Image classification algorithms; Land use/cover; Mapping from imagery; Plant recognition; Valorisation; agriculture; article; classification algorithm; computer vision; crop; deep learning; European Union; filtration; human; imagery; information science; land use; nonhuman; probability; season; valorization; Crops},
	correspondence_address = {M. Yordanov; SEIDOR Consulting S.L, Barcelona, 08500, Spain; email: momtchil.iordanov@ext.ec.europa.eu; M. van der Velde; European Commission, Joint Research Centre (JRC), Ispra, 21027, Italy; email: marijn.van-der-velde@ec.europa.eu},
	publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
	issn = {14248220},
	pmid = {37514593},
	language = {English},
	abbrev_source_title = {Sensors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Kayhan2022,
	author = {Kayhan, Gökhan},
	title = {Comparison of the performance of different learning algorithms in leaf feature extraction and recognition using convolution neural network},
	year = {2022},
	journal = {Concurrency and Computation: Practice and Experience},
	volume = {34},
	number = {26},
	doi = {10.1002/cpe.7294},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136513380&doi=10.1002%2fcpe.7294&partnerID=40&md5=c747a39f09a26f0bde9c3b4c48cb0817},
	affiliations = {Department of Computer Engineering, Ondokuz Mayıs University, Samsun, Turkey},
	abstract = {Plant identification with computer systems has been developed with image processing tools and has helped researchers to identify unknown plant species with high accuracy. In this study, the leaves of five different plants were classified according to their shapes using deep learning. A database was created with leaf images of mint, echinacea, St. John's wort, melissa, and thyme plants. Images in this database were classified with a convolution neural network (CNN). For this classification, 70% training and 30% testing were randomly selected in the database. The parameters of the CNN layer consist of a set of (Formula presented.) learnable filters. In the CNN, 10 (Formula presented.) kernel matrices with stride [1 1] were used. A rectified linear unit was chosen as the activation function. Maximum pooling was performed using a (Formula presented.) filter with stride [2 2]. In this classification, five fully connected layers were created. Using CNN, the performance of different learning algorithms was compared. It was observed that CNN achieved more successful results than traditional attribute methods. © 2022 John Wiley & Sons, Ltd.},
	author_keywords = {classification; convolution neural network; feature extraction; image processing},
	keywords = {Classification (of information); Convolution; Deep learning; Extraction; Feature extraction; Image classification; Learning algorithms; Plants (botany); Classifieds; Convolution neural network; Feature extraction and recognition; Features extraction; High-accuracy; Image processing tools; Images processing; Performance; Plant identification; Plant species; Database systems},
	correspondence_address = {G. Kayhan; Department of Computer Engineering, Ondokuz Mayıs University, Samsun, Turkey; email: gkayhan@omu.edu.tr},
	publisher = {John Wiley and Sons Ltd},
	issn = {15320626},
	coden = {CCPEB},
	language = {English},
	abbrev_source_title = {Concurr. Comput. Pract. Exper.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Nasir2023,
	author = {Nasir, Fazal and Haris, Muhammad and Khan, Bilawal and Tufail, Muhammad and Khan, Muhammad Tahir and Dong, Zhang},
	title = {Real-Time Plant Recognition and Crop Row Navigation for Autonomous Precision Agricultural Sprayer Robot},
	year = {2023},
	journal = {2023 International Conference on Robotics and Automation in Industry, ICRAI 2023},
	doi = {10.1109/ICRAI57502.2023.10089591},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153582841&doi=10.1109%2fICRAI57502.2023.10089591&partnerID=40&md5=22e48fe50a6dc460f6fab5fdcc612ca5},
	affiliations = {National Center of Robotics and Automation, Advanced Robotics and Automation Lab, Peshawar, Pakistan; University of Engineering and Technology, Department of Mechatronics Engineering, Peshawar, Pakistan; Institute of Automation, Qilu University of Technology (Shandong Academy of Sciences), Jinan, China},
	abstract = {This paper presents a vision-based selective spraying technique for an autonomous agricultural sprayer robot. In traditional methods, excessive chemical spraying cause deleterious effects on human health, environment and becomes uneconomical. In order to reduce the agrochemical wastage encounters in a broadcast spraying, a selective plant spraying method is used for crop chemical treatment. The sensor-based approach assisted with YOLOv7 model is deployed on a custom designed robot for recognizing and localizing the lettuce plants in field. The PID-based pressure controller is designed that minimizes the undesirable fluctuations cause by the opening/closing of solenoid-valve-nozzles (SVNs) during spraying. Thus the nozzle's spraying quality is maintained by keeping the pressure constant to a desired value. A visual servoing scheme for row tracking is presented that uses the detected plant's spatial features. The robustness of the visual-based navigation is validated in the real field experiments.  © 2023 IEEE.},
	author_keywords = {Agricultural robotics; Deep learning; Precision agriculture; Pressure control; Robot Navigation; Row detection},
	keywords = {Agricultural robots; Deep learning; Precision agriculture; Pressure control; Solenoid valves; Spray nozzles; Visual servoing; Agricultural robotics; Crop rows; Deep learning; Plant recognition; Precision Agriculture; Real- time; Robot navigation; Row detection; Spraying techniques; Vision based; Crops},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166546472-7},
	language = {English},
	abbrev_source_title = {Int. Conf. Robot. Autom. Ind., ICRAI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2023 International Conference on Robotics and Automation in Industry, ICRAI 2023; Conference date: 3 March 2023 through 5 March 2023; Conference code: 187834}
}

@ARTICLE{Chulif20235963,
	author = {Chulif, Sophia and Lee, Sue Han and Chang, Yang Loong and Chai, Kok Chin},
	title = {A machine learning approach for cross-domain plant identification using herbarium specimens},
	year = {2023},
	journal = {Neural Computing and Applications},
	volume = {35},
	number = {8},
	pages = {5963 – 5985},
	doi = {10.1007/s00521-022-07951-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142027802&doi=10.1007%2fs00521-022-07951-6&partnerID=40&md5=9a3b2f5af38024202a7149beb5a2922a},
	affiliations = {Faculty of Engineering, Computing and Science, Swinburne University of Technology Sarawak Campus, Kuching, Malaysia; Department of Artificial Intelligence, NEUON AI SDN. BHD., Kota Samarahan, Malaysia; Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia},
	abstract = {The preservation of plant specimens in herbaria has been carried out for centuries in efforts to study and confirm plant taxa. With the increasing collection of herbaria made available digitally, it is practical to use herbarium specimens for the automation of plant identification. They are also substantially more accessible and less expensive to obtain compared to field images. In fact, in remote and inaccessible habitats, field images of rare plant species are still immensely lacking. As a result, rare plant species identification is challenging due to the deficiency of training data. To address this problem, we investigate a cross-domain adaptation approach that allows knowledge transfer from a model learned from herbarium specimens to field images. We propose a model called Herbarium–Field Triplet Loss Network (HFTL network) to learn the mapping between herbarium and field domains. Specifically, the model is trained to maximize the embedding distance of different plant species and minimize the embedding distance of the same plant species given herbarium–field pairs. This paper presents the implementation and performance of the HFTL network to assess the herbarium–field similarity of plants. It corresponds to the cross-domain plant identification challenge in PlantCLEF 2020 and PlantCLEF 2021. Despite the lack of field images, our results show that the network can generalize and identify rare species. Our proposed HFTL network achieved a mean reciprocal rank score of 0.108 and 0.158 on the test set related to the species with few training field photographs in PlantCLEF 2020 and PlantCLEF 2021, respectively. © 2022, The Author(s).},
	author_keywords = {Computer vision; Convolutional neural networks; Herbarium; Plant identification; Triplet loss},
	keywords = {Convolutional neural networks; Embeddings; Knowledge management; Machine learning; Plants (botany); Convolutional neural network; Cross-domain; Embeddings; Field images; Herbarium; Herbarium specimens; Loss networks; Plant identification; Plant species; Triplet loss; Computer vision},
	correspondence_address = {S. Chulif; Faculty of Engineering, Computing and Science, Kuching, Swinburne University of Technology Sarawak Campus, Malaysia; email: schulif@swinburne.edu.my},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {09410643},
	language = {English},
	abbrev_source_title = {Neural Comput. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Liu20235961,
	author = {Liu, Qiang and Wang, Shubo and He, Xiongkui and Liu, Yajia},
	title = {Pear Flower Recognition Based on YOLO v5s Target Detection Model in Complex Orchard Scenes},
	year = {2023},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {845 LNEE},
	pages = {5961 – 5970},
	doi = {10.1007/978-981-19-6613-2_576},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151117814&doi=10.1007%2f978-981-19-6613-2_576&partnerID=40&md5=223d78ea3ffa52f256918af96e07ee31},
	affiliations = {Centre for Chemicals Application Technology, China Agricultural University, Beijing, 100193, China; College of Agricultural Unmanned System, China Agricultural University, Beijing, 100193, China; College of Science, China Agricultural University, Beijing, 100193, China; College of Engineering, China Agricultural University, Beijing, 100083, China},
	abstract = {Aiming at the problems of low pollination accuracy, low efficiency and waste of financial and material resources in traditional pollination methods such as bee pollination and artificial pollination, this paper proposes a pollination method based on machine vision to accurately identify pear flowers through the YOLO v5 deep learning framework identifies pear blossoms in complex situations such as different light dense and backgrounds. The working environment required by YOLO v5 is built based on Pytorch. In order to improve the quality of data set and recognition accuracy, this paper collects pear flowers in various environments, and completes the information labeling through labelImg. Then the neural network parameters are adjusted to complete the training of the model. The target detection experiment of pear flower is carried out by using the trained weight file. The results show that this method can accurately identify pear flowers, and the recognition accuracy is above 85%. In comparison, the recognition accuracy of pear flowers is higher by the YOLO v5 method. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Accurate recognition; Deep learning; Pear flower dataset; Target detection; YOLO v5},
	keywords = {Complex networks; Fruits; Accurate recognition; Deep learning; Detection models; Financial resources; Flower recognition; Pear flower dataset; Pear flowers; Recognition accuracy; Targets detection; YOLO v5; Deep learning},
	correspondence_address = {X. He; Centre for Chemicals Application Technology, China Agricultural University, Beijing, 100193, China; email: 734453131@qq.com},
	editor = {Yan L. and Duan H. and Deng Y. and Yan L.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981196612-5},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: International Conference on Guidance, Navigation and Control, ICGNC 2022; Conference date: 5 August 2022 through 7 August 2022; Conference code: 292199}
}

@CONFERENCE{Varma202314,
	author = {Varma, Watan Kishor and Kumar, Vipin},
	title = {Analysis of Crop Leaf Image Classification using Deep Learning Models over Novel Dataset},
	year = {2023},
	journal = {Proceedings of the 10th International Conference on Signal Processing and Integrated Networks, SPIN 2023},
	pages = {14 – 19},
	doi = {10.1109/SPIN57001.2023.10116960},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160002907&doi=10.1109%2fSPIN57001.2023.10116960&partnerID=40&md5=1fe415153c94fd1d732b2b9fc4f9683b},
	affiliations = {Mahatma Gandhi Central University Motihari, Dept. of Computer Science and It, Bihar, India},
	abstract = {Plants are an essential component of our life. They provide nourishment for us, medicine, oxygen, air to breathe, and other requirements that make life worthwhile. If adequate precautions are not followed in this area, they can seriously affect plants, impacting the output's productivity, quantity, and quality [1]. We know that the primary component of a plant that defines its range and varieties is its leaf. Stm, due to the plant's nature, which exhibits several different pattern variations, leaf identification is a challenging and complex task. Distinguishing between different types of leaves and crop plants of similar size is a difficult task. Precise plant identification is beyond the average person's abilities as it requires specialist expertise. Therefore, this work is being attempted using Deep Learning (DL) models to solve these crop plant identification problems. The author collected and successfully classified 570t i RGB highresolution photos of agricultural plant leaves organized into 21 categories in this research. Deep Learning (DL) models have been used in a comparative assessment of the classification performance of classifiers, with the maximum test accuracy being 92.53\% on original data and 94.3(i% on augmented data.  © 2023 IEEE.},
	author_keywords = {Crop plants leaf; Deep learning; Image classification; Image processing; Machine learning; RGB image},
	keywords = {Classification (of information); Crops; Deep learning; Learning systems; Plants (botany); Crop plant leaf; Crop plants; Deep learning; Images classification; Images processing; Learning models; Machine-learning; Plant identification; Plant leaves; RGB images; Image classification},
	correspondence_address = {W.K. Varma; Mahatma Gandhi Central University Motihari, Dept. of Computer Science and It, Bihar, India; email: watanmca@gmail.com},
	editor = {Pandey M.K. and Rai J.K. and Kumar P. and Dubey A.K. and Shukla A.K.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549099-3},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Signal Process. Integr. Networks, SPIN},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 10th International Conference on Signal Processing and Integrated Networks, SPIN 2023; Conference date: 23 March 2023 through 24 March 2023; Conference code: 188530}
}

@ARTICLE{Lin2023,
	author = {Lin, Haoran and Liu, Xiaoyang and Han, Zemin and Cui, Hongxia and Dian, Yuanyong},
	title = {Identification of Tree Species in Forest Communities at Different Altitudes Based on Multi-Source Aerial Remote Sensing Data},
	year = {2023},
	journal = {Applied Sciences (Switzerland)},
	volume = {13},
	number = {8},
	doi = {10.3390/app13084911},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156118134&doi=10.3390%2fapp13084911&partnerID=40&md5=61ab58f100aa68c5e6259469a69e08fc},
	affiliations = {College of Horticulture and Forestry Sciences, Huazhong Agricultural University, Wuhan, 430070, China; Hubei Forestry Investigation and Planning Institute, Wuhan, 430079, China; Hubei Academy of Forestry, Wuhan, 430075, China; Hubei Engineering Technology Research Centre for Forestry Information, Huazhong Agricultural University, Wuhan, 430070, China; Key Laboratory of Urban Agriculture in Central China, Ministry of Agriculture, Wuhan, 430070, China},
	abstract = {The accurate identification of forest tree species is important for forest resource management and investigation. Using single remote sensing data for tree species identification cannot quantify both vertical and horizontal structural characteristics of tree species, so the classification accuracy is limited. Therefore, this study explores the application value of combining airborne high-resolution multispectral imagery and LiDAR data to classify tree species in study areas of different altitudes. Three study areas with different altitudes in Muyu Town, Shennongjia Forest Area were selected. Based on the object-oriented method for image segmentation, multi-source remote sensing feature extraction was performed. The recursive feature elimination algorithm was used to filter out the feature variables that were optimal for classifying tree species in each altitude study area. Four machine learning algorithms, SVM, KNN, RF, and XGBoost, were combined to classify tree species at each altitude and evaluate the accuracy. The results show that the diversity of tree layers decreased with the altitude in the different study areas. The texture features and height features extracted from LiDAR data responded better to the forest community structure in the different study areas. Coniferous species showed better classification than broad-leaved species within the same study areas. The XGBoost classification algorithm showed the highest accuracy of 87.63% (kappa coefficient of 0.85), 88.24% (kappa coefficient of 0.86), and 84.03% (kappa coefficient of 0.81) for the three altitude study areas, respectively. The combination of multi-source remote sensing numbers with the feature filtering algorithm and the XGBoost algorithm enabled accurate forest tree species classification. © 2023 by the authors.},
	author_keywords = {different altitudes; LiDAR; machine learning; multispectral image; tree species classification},
	correspondence_address = {Y. Dian; College of Horticulture and Forestry Sciences, Huazhong Agricultural University, Wuhan, 430070, China; email: dianyuanyong@mail.hzau.edu.cn},
	publisher = {MDPI},
	issn = {20763417},
	language = {English},
	abbrev_source_title = {Appl. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Zhang2022657,
	author = {Zhang, Kaidi and Wang, Binjun and Tong, Xin and Liu, Keke},
	title = {Fire detection using vision transformer on power plant},
	year = {2022},
	journal = {Energy Reports},
	volume = {8},
	pages = {657 – 664},
	doi = {10.1016/j.egyr.2022.05.224},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131950161&doi=10.1016%2fj.egyr.2022.05.224&partnerID=40&md5=54f25c50db10cebaa8c8711c9d75f580},
	affiliations = {School of information and Cyber security, People's Public Security University of China, Beijing, 100038, China; Yunnan university of Finance and Economics, Wuhua District, Kunming, 650000, China},
	abstract = {The importance of power plant safety is increasing in the era of gradual technological development. When a fire occurs in the power plant, it will cause huge material losses, social unrest, and even casualties. The paper studies the common methods and models of fire warning, and introduces several model recognition techniques based on flames or smoke. Improved an automated power plant identification system based on the vision transformer, and proved the advantages of the technology through comparative analysis. © 2022},
	author_keywords = {Deep learning; Fire detection; Power plant; Vision transformer},
	keywords = {Fire detectors; Fires; Smoke; Comparative analyzes; Deep learning; Fire detection; Material loss; Model recognition; Plant identification systems; Plant safety; Technological development; Vision transformer; Deep learning},
	correspondence_address = {B. Wang; School of information and Cyber security, People's Public Security University of China, Beijing, 100038, China; email: wangbinjun@ppsuc.edu.cn},
	publisher = {Elsevier Ltd},
	issn = {23524847},
	language = {English},
	abbrev_source_title = {Energy Rep.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@CONFERENCE{Rekha2023345,
	author = {Rekha, V. and Reddy, L. Venkateswara and Chaudhari, Sachin Vasant and Gopi, Arepalli and Nithiya, C. and Ahamed, Shaik Khaleel},
	title = {Automated Deep Learning with Wavelet Neural Network based Rice Plant Classification},
	year = {2023},
	journal = {IDCIoT 2023 - International Conference on Intelligent Data Communication Technologies and Internet of Things, Proceedings},
	pages = {345 – 350},
	doi = {10.1109/IDCIoT56793.2023.10053487},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149756677&doi=10.1109%2fIDCIoT56793.2023.10053487&partnerID=40&md5=6019f28aa30d646b50601cb5081e550f},
	affiliations = {Agurchand Manmull Jain College, Tamil Nadu, Chennai, India; Department of Computer Science and Engineering, Kg Reddy College of Engineering and Technology, Telangana, Hyderabad, India; Department of Electronics and Computer Engineering, Sanjivani College of Engineering, Kopargaon, India; Department of Computer Science Engineering, Koneru Lakshmaiah Education Foundation, Andhra Pradesh, Guntur, India; Department of Ece, R.M.K. College of Engineering and Technology, Puduvoyal, India; Department of Cse, Methodist College of Engineering and Technology, Telangana, Hyderabad, India},
	abstract = {In the agricultural sector, a disease that occurs in plants is primarily responsible for the decrease in production and results in massive financial loss. Rice is considered the crucial food crop in Asian nations and is affected by distinct types of diseases. Due to the arrival of deep learning (DL) and computer vision methods, rice plant ailments will be identified and diminish the problem of the agriculturalists to save the crops. Currently, computer aided diagnosis (CAD) methods become accessible to observe pests and crop diseases with the help of plant images. An automatic rice disease prognosis technique could present details regarding preventing and controlling rice ailments for decreasing the monetary loss, decline the insecticide residue, and increase the number and quality of yields. In order to attain this method, authors are advised to advance effective image processing methods for noticing plant diseases. Therefore, this paper presents an Automated Deep Learning with Wavelet Neural Network Based Rice Plant Classification model named ADLWNN model. The proposed ADLWNN model focuses on the effectual recognition and categorization of rice plant images. The proposed ADLWNN model primarily exploits convolutional neural network (CNN) model to extract features from the input rice plant images. Moreover, manta ray optimization algorithm (MRFO) algorithm is applied as a hyperparameter optimizer. Besides, the WNN model is employed for the robust recognition and categorization of rice plant images. The simulation analysis of the ADLWNN model is tested using a set of rice plant images and the results indicated as 98.17% better outcomes for the ADLWNN model over other techniques.  © 2023 IEEE.},
	author_keywords = {Convolution neural network; Disease diagnosis; Image classification; Machine learning; Rice plant images},
	keywords = {Computer aided diagnosis; Convolution; Convolutional neural networks; Crops; Deep learning; Learning systems; Losses; Quality control; Agricultural sector; Convolution neural network; Disease diagnosis; Images classification; Machine-learning; Network-based; Neural-networks; Plant classification; Rice plant image; Rice plants; Image classification},
	correspondence_address = {V. Rekha; Agurchand Manmull Jain College, Chennai, Tamil Nadu, India; email: rekhaonmail@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166547451-1},
	language = {English},
	abbrev_source_title = {IDCIoT - Int. Conf. Intell. Data Commun. Technol. Internet Things, Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2023 International Conference on Intelligent Data Communication Technologies and Internet of Things, IDCIoT 2023; Conference date: 5 January 2023 through 7 January 2023; Conference code: 187004}
}

@ARTICLE{Chen2023,
	author = {Chen, Caiyan and Jing, Linhai and Li, Hui and Tang, Yunwei and Chen, Fulong},
	title = {Individual Tree Species Identification Based on a Combination of Deep Learning and Traditional Features},
	year = {2023},
	journal = {Remote Sensing},
	volume = {15},
	number = {9},
	doi = {10.3390/rs15092301},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159267236&doi=10.3390%2frs15092301&partnerID=40&md5=2bae58f17256dece36ce19dbc0827308},
	affiliations = {Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; International Research Center of Big Data for Sustainable Development Goals, Beijing, 100094, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Poyang Lake Wetland and Watershed Research, Ministry of Education &, School of Geography and Environment, Jiangxi Normal University, Nanchang, 330022, China},
	abstract = {Accurate identification of individual tree species (ITS) is crucial to forest management. However, current ITS identification methods are mainly based on traditional image features or deep learning. Traditional image features are more interpretative, but the generalization and robustness of such methods are inferior. In contrast, deep learning based approaches are more generalizable, but the extracted features are not interpreted; moreover, the methods can hardly be applied to limited sample sets. In this study, to further improve ITS identification, typical spectral and texture image features were weighted to assist deep learning models for ITS identification. To validate the hybrid models, two experiments were conducted; one on the dense forests of the Huangshan Mountains, Anhui Province and one on the Gaofeng forest farm, Guangxi Province, China. The experimental results demonstrated that with the addition of image features, different deep learning ITS identification models, such as DenseNet, AlexNet, U-Net, and LeNet, with different limited sample sizes (480, 420, 360), were all enhanced in both study areas. For example, the accuracy of DenseNet model with a sample size of 480 were improved to 87.67% from 85.41% in Huangshan. This hybrid model can effectively improve ITS identification accuracy, especially for UAV aerial imagery or limited sample sets, providing the possibility to classify ITS accurately in sample-poor areas. © 2023 by the authors.},
	author_keywords = {deep learning; individual tree species identification; remote sensing; spectral feature; texture feature},
	keywords = {Aerial photography; Antennas; Classification (of information); Deep learning; Image enhancement; Learning systems; Remote sensing; Sampling; Textures; Deep learning; Image features; Individual tree; Individual tree species identification; Remote-sensing; Sample sets; Spectral feature; Texture features; Tree species; Tree species identifications; Forestry},
	correspondence_address = {L. Jing; International Research Center of Big Data for Sustainable Development Goals, Beijing, 100094, China; email: jinglh@aircas.ac.cn},
	publisher = {MDPI},
	issn = {20724292},
	language = {English},
	abbrev_source_title = {Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Devi2023,
	author = {Devi, M. Shyamala and Aruna, R. and Rajeswari, D. Raja and Manogna, R. Sai},
	title = {Conv2D Xception Adadelta Gradient Descent Learning Rate Deep learning Optimizer for Plant Species Classification},
	year = {2023},
	journal = {2023 3rd International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies, ICAECT 2023},
	doi = {10.1109/ICAECT57570.2023.10117710},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160453958&doi=10.1109%2fICAECT57570.2023.10117710&partnerID=40&md5=5b705fabd2ff552ceac6d33fd167df89},
	affiliations = {R&D Institute of Science and Technology, Computer Science & Engineering, Vel Tech Rangarajan Dr. Sagunthala, Tamilnadu, Chennai, India},
	abstract = {Almost every element of living beings is undergoing extraordinary transformation as a result of technological advances. While many areas of agriculture are leveraging cutting-edge technological innovations, farming remains far behind in adopting these advancements. Ayurvedic medicine and other plant-based medical systems employ the classification of plants by their leaflets for a variety of purposes, including ecosystem, farming, disease diagnosis, rare plant maintenance, and Herbal remedies. Due to the utilization of biological nomenclature, standard methods of plant recognition and classification are difficult, time-consuming, and aggravating for non-experts. For learners who are seeking to learn about species, this poses a real challenge. Based on this consequence, this paper recommends Conv2D Xception Adadelta Gradient Descent (CXAGD) Deep Learning to classify the plant species based on the leaflet feature structure. The proposed CXAGD model uses Leaf classification dataset from KAGGLE with 4500 plant leaflets of various species. One input, a dense average pooling layer, and an output layer were used in the development of the proposed Conv2D Adadelta Gradient Deep CNN model. The CXAGD model was constructed using 36 depth wise separable convolutional layers and a maximum pooling layer for each convolution and trained with learning rate of Adadelta optimizer. The CXAGD built with Xception network with the Adadelta optimizer with the dropout of 0.5, weight decay of L2 regularization. The leaf dataset subjected to the training and testing data division and the training data applied to CXAGD model and also applied to other CNN models for comparing the efficiency of the proposed CXAGD model. Python was used for programming, with a batch size of 64 and 30 training epochs, on a Geforce Tesla V100 Graphics card server. Implementation outcomes projects that proposed CXAGD model exhibits accuracy of 97.85%, Precision of 97.42%, Recall of 97.75% and FScore of 97.76% when compared with existing CNN models.  © 2023 IEEE.},
	author_keywords = {accuracy; CNN; Convolution; deep learning; pooling},
	keywords = {Classification (of information); Computer aided diagnosis; Convolutional neural networks; Deep learning; Gradient methods; Learning algorithms; Learning systems; Optimization; Python; Statistical tests; Accuracy; CNN models; Deep learning; Gradient descent learning rates; Gradient-descent; Optimizers; Plant species; Pooling; Species classification; Training data; Convolution},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549400-7},
	language = {English},
	abbrev_source_title = {Int. Conf. Adv. Electr., Comput., Commun. Sustain. Technol., ICAECT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies, ICAECT 2023; Conference date: 5 January 2023 through 6 January 2023; Conference code: 188686}
}

@ARTICLE{Yudaputra2023,
	author = {Yudaputra, Angga and Yuswandi, Ade Yusuf and Witono, Joko Ridho and Cropper, Wendell P. and Usmadi, Didi},
	title = {Tree species identification in ex situ conservation areas using WorldView-2 Satellite Data and Machine Learning Methods: a case study in the Bogor Botanic Garden},
	year = {2023},
	journal = {Tropical Ecology},
	doi = {10.1007/s42965-023-00308-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162042708&doi=10.1007%2fs42965-023-00308-7&partnerID=40&md5=b73f92a5fe3c9be85ed94863cb3663f6},
	affiliations = {Research Center for Plant Conservation, Botanic Gardens, and Forestry, National Research and Innovation Agency, KST Soekarno Jalan Raya Jakarta-Bogor Km 46, West Java, Cibinong, 16911, Indonesia; Directorate of Scientific Collection Management, National Research and Innovation Agency, KST Soekarno Jalan Raya Jakarta-Bogor Km. 46, West Java, Cibinong, 16911, Indonesia; Research Center for Biosystematics and Evolution, National Research and Innovation Agency, KST Soekarno Jalan Raya Jakarta-Bogor Km. 46, West Java, Cibinong, 16911, Indonesia; School of Forest, Fisheries and Geomatics Sciences, University of Florida, Gainesville, 32611-0410, FL, United States; Research Center for Ecology and Ethnobiology, National Research and Innovation Agency, KST Soekarno Jalan Raya Jakarta-Bogor Km 46, West Java, Cibinong, 16911, Indonesia},
	abstract = {Spatially-explicit data on the species composition of forest plants can be an important tool for forest management and conservation. One specific application of these data is for identifying tropical tree species through machine learning techniques to classify satellite remote sensing images. This study aims to examine the ability to use Worldview-2 high-resolution data with various machine learning methods to identify tree species in the Bogor Botanic Garden. Eighteen species from 11 families were selected as samples representing an ecologically and taxonomically diverse data set. Using aggregated image variables, each tree species was found to have different reflectance, texture, and spectral vegetation index variable values. Cluster analysis showed that the 18 tree species could be separated into three clusters that partly reflected taxonomic relationships. Four machine learning algorithms (Support Vector Machine (SVM), Random Forest (RF), K-nearest neighbor (KNN), and Bayesian) were used to predict the species identity of pixels in the image data. A multicollinearity test using a Variance Inflation Factor method reduced the predictor variables from 54 to 9. The highest accuracy (0.96) was observed using SVM, followed by RF (0.91), KNN (0.86), and Bayesian (0.74). The implementation of high-resolution satellite imagery and machine learning for species identification in tropical ex situ plant conservation areas, such as botanic gardens is reported here for the first time. © 2023, International Society for Tropical Ecology.},
	author_keywords = {Classification; Conservation; Machine learning algorithm; Modeling ecology; Remote sensing; Tropical tree; Urban forest},
	correspondence_address = {D. Usmadi; Research Center for Ecology and Ethnobiology, National Research and Innovation Agency, Cibinong, KST Soekarno Jalan Raya Jakarta-Bogor Km 46, West Java, 16911, Indonesia; email: didi020@brin.go.id},
	publisher = {Springer},
	issn = {05643295},
	coden = {ISTEB},
	language = {English},
	abbrev_source_title = {Trop. Ecol.},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wang20234217,
	author = {Wang, Zhaobin and Cui, Jing and Zhu, Ying},
	title = {Review of plant leaf recognition},
	year = {2023},
	journal = {Artificial Intelligence Review},
	volume = {56},
	number = {5},
	pages = {4217 – 4253},
	doi = {10.1007/s10462-022-10278-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138742158&doi=10.1007%2fs10462-022-10278-2&partnerID=40&md5=cb66004d00385a902d28f3161b3cfa81},
	affiliations = {School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Key Laboratory of Microbial Resources Exploitation and Application of Gansu Province, Institute of Biology, Gansu Academy of Sciences, Lanzhou, 730000, China},
	abstract = {Plants can be seen everywhere in daily life and are closely connected with our lives. The recognition and classification of plants are of great significance to ecological and environmental protection. Traditional plant identification methods are complex, and experts cannot classify multiple plant species quickly. More and more researchers pay attention to image processing and pattern recognition and use them to identify and classify plant leaves quickly. Based on this, this paper summarizes and classifies the methods of plant leaf recognition in recent years. First, we analyze these studies and classify them using different features and classifiers, such as shape, texture, color features, support vector machines, K nearest neighbors, convolutional neural networks, and so on. Secondly, compare the recognition results of plant leaf recognition methods under different datasets. Finally, the recognition of plant leaves is summarized, and future research and development have prospected. © 2022, The Author(s), under exclusive licence to Springer Nature B.V.},
	author_keywords = {Classification; Feature extraction; Plant recognition},
	keywords = {Convolutional neural networks; Image processing; Nearest neighbor search; Plants (botany); Support vector machines; Textures; Daily lives; Features extraction; Identification method; Image patterns; Images processing; Leaf recognition; Plant identification; Plant leaves; Plant recognition; Plant species; Classification (of information)},
	correspondence_address = {Z. Wang; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; email: wangzhb@lzu.edu.cn},
	publisher = {Springer Nature},
	issn = {02692821},
	coden = {AIRVE},
	language = {English},
	abbrev_source_title = {Artif Intell Rev},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Karadeniz2023727,
	author = {Karadeniz, Alper Talha and Çelik, Yüksel and Başaran, Erdal},
	title = {Classification of walnut varieties obtained from walnut leaf images by the recommended residual block based CNN model},
	year = {2023},
	journal = {European Food Research and Technology},
	volume = {249},
	number = {3},
	pages = {727 – 738},
	doi = {10.1007/s00217-022-04168-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141663845&doi=10.1007%2fs00217-022-04168-8&partnerID=40&md5=f886a48297d6a18f1dad6712768daacd},
	affiliations = {Digital Transformation and Software Department, Trabzon University, Trabzon, Turkey; Department of Computer Engineering, Faculty of Engineering, Karabuk University, Karabuk, Turkey; Department of Computer Technology, Agri Ibrahim Cecen University, Agri, Turkey},
	abstract = {Walnuts are widely used, although they come in a variety of types and qualities. It is essential to choose the correct walnut variety with the necessary ecological characteristics to continue the production of walnut fruit, which has positive benefits on human health. Because planting a walnut garden is expensive and the harvesting process takes a while. However, since the colour and feel of walnut leaves are so similar, it can be challenging to tell them apart. Experts must devote a significant amount of time to differentiating walnut kinds, and morphological tests should be conducted. There are different studies in the literature for walnut variety differentiation. Nevertheless, those are studies conducted with the classification of a small number of walnut varieties or laboratory experiments. With the advancement of technology, deep learning techniques based on computers are now routinely utilized for leaf recognition. These technologies enable significant reductions in error rates, time saves, and cost. With a total of 1751 leaf pictures collected from 18 species of walnuts, a special walnut dataset was constructed for this study in order to identify walnut types from walnut leaves. To automatically classify the provided dataset, images are trained with residual block-based convolutional neural network architectures. Following the discovery of each image's deep features, the Atom Search Optimization algorithm was used to choose the most distinctive characteristics. Support vector machines (SVM) were used to classify walnut species with the new feature set created. The experimental studies of the proposed model based on Residual block and Atom Search optimization successfully categorised the walnut dataset with an accuracy rating of 87.42%. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
	author_keywords = {Atom search optimization; Optimization based feature selection; Residual block; Support vector machines; Walnut dataset},
	keywords = {Atoms; Classification (of information); Convolutional neural networks; Deep learning; Network architecture; Atom search optimization; Block based; Features selection; Leaf images; Optimisations; Optimization based feature selection; Residual block; Search optimization; Support vectors machine; Walnut dataset; Support vector machines},
	correspondence_address = {A.T. Karadeniz; Digital Transformation and Software Department, Trabzon University, Trabzon, Turkey; email: alperkaradeniz@trabzon.edu.tr},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {14382377},
	language = {English},
	abbrev_source_title = {Eur. Food Res. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Shelke20232601,
	author = {Shelke, Ankita and Mehendale, Ninad},
	title = {A CNN-based android application for plant leaf classification at remote locations},
	year = {2023},
	journal = {Neural Computing and Applications},
	volume = {35},
	number = {3},
	pages = {2601 – 2607},
	doi = {10.1007/s00521-022-07740-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137331543&doi=10.1007%2fs00521-022-07740-1&partnerID=40&md5=1ffd7e12d473d9186a7af2ee87d3ee2f},
	affiliations = {K. J. Somaiya College of Engineering, Vidyavihar, Mumbai, 400077, India},
	abstract = {The Earth has witnessed the evolution of thousands of plant species in the kingdom named Plantae. Due to the diversity and subtle differences in each plant, it becomes difficult for a novice to identify a particular plant and to know the properties associated with it. We propose a classification model that can solve this issue by categorizing the input plant image. Our methodology can classify up to 79 different plant species found predominantly in Himachal Pradesh located in India. A Deep Learning-based model is used to carry out the classification. Our model is optimized to work efficiently without a live internet connection on smartphones and other devices with limited computational power. A total of 79 distinct classes were classified using the Convolution neural network DenseNet-161 model architecture with a testing accuracy of 97.3%. The application works on any android platform and can classify the input plant image with an average latency of 1.98 s. Our application built on this model assists farmers and locals to get in-depth knowledge about the species including the local name, scientific name, description, and the care requirements by uploading or taking a picture of the plant leaf. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
	author_keywords = {Android application; Deep learning; Flutter; Plant classification},
	keywords = {Deep learning; Android applications; Classification models; Deep learning; Flutter; Learning Based Models; Plant classification; Plant leaf classifications; Plant species; Property; Remote location; Android (operating system)},
	correspondence_address = {N. Mehendale; K. J. Somaiya College of Engineering, Vidyavihar, Mumbai, 400077, India; email: ninad@somaiya.edu},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {09410643},
	language = {English},
	abbrev_source_title = {Neural Comput. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Thomkaew202391,
	author = {Thomkaew, Jiraporn and Intakosum, Sarun},
	title = {Plant Species Classification Using Leaf Edge Feature Combination with Morphological Transformations and SIFT Key Point},
	year = {2023},
	journal = {Journal of Image and Graphics(United Kingdom)},
	volume = {11},
	number = {1},
	pages = {91 – 97},
	doi = {10.18178/joig.11.1.91-97},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150269705&doi=10.18178%2fjoig.11.1.91-97&partnerID=40&md5=7ccd03f0fb2723287a7449b2bbc6c5db},
	affiliations = {Department of Computer Science, Faculty of Science, King Mongkut’s Institute of Technology Ladkrabang, Bangkok, Thailand},
	abstract = {This paper presents a new approach to plant classification by using leaf edge feature combination with Morphological Transformations and defining key points on leaf edge with SIFT. There are three steps in the process. Image preprocessing, feature extraction, and image classification. In the image preprocessing step, image noise is removed with Morphological Transformations and leaf edge detect with Canny Edge Detection. The leaf edge is identified with SIFT, and the plant leaf feature was extracted by CNN according to the proposed method. The plant leaves are then classified by random forest. Experiments were performed on the PlantVillage dataset of 10 classes, 5 classes of healthy leaves, and 5 classes of diseased leaves. The results showed that the proposed method was able to classify plant species more accurately than using features based on leaf shape and texture. The proposed method has an accuracy of 95.62%. © 2023 by the authors.},
	author_keywords = {leaf edge; morphological transformations; plant species classification; random forest; SIFT},
	correspondence_address = {J. Thomkaew; Department of Computer Science, Faculty of Science, King Mongkut’s Institute of Technology Ladkrabang, Bangkok, Thailand; email: jiraporn.th@rmutsv.ac.th},
	publisher = {University of Portsmouth},
	issn = {23013699},
	language = {English},
	abbrev_source_title = {J. Imag. Gr.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Devi2023,
	author = {Devi, R.Manjula and Sangeetha, M. and Sagana, C. and Savitha, S. and Hemalatha, P. and Janani, N. and Maamathi, K.},
	title = {Plant type classification based on leaves using Fusion based Support Vector Machine},
	year = {2023},
	journal = {2023 International Conference on Computer Communication and Informatics, ICCCI 2023},
	doi = {10.1109/ICCCI56745.2023.10128587},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163087558&doi=10.1109%2fICCCI56745.2023.10128587&partnerID=40&md5=6214c151549ba2727a51e3ad98ca615e},
	affiliations = {Kongu Engineering College, Perundurai, Department of CSD, Erode, India; Department of CSE, Kongu Engineering College, Erode, Perundurai, India},
	abstract = {In general, trees and other plants are recognized by their leaves. Many of them are industrial crops that are also used in the production of medicines. The identification of leaves is essential due to the increased automation occurring in both the commercial and medical sectors. Typically, physical or genetic traits are used to identify leaves. However, classifying the many leaf cultivars is becoming more and more complicated due to the large number of morphological variations among them. Since there have been several evolutionary changes over the preceding few decades, there are currently more varieties of a certain leaf type. Thus it takes time to manually sort and identify these leaves. Here in this project three leaves are chosen and their respective features are extracted using Image Processing techniques and the SVM (Support Vector Machine) classifier algorithm is used to categorize the leaves according to their shape, colour, and texture. © 2023 IEEE.},
	author_keywords = {Leaf Recognition; Support Vector Machine (SVM)},
	keywords = {Crops; Image processing; Plants (botany); Textures; Evolutionary changes; Genetic traits; Image processing technique; Industrial crops; Leaf recognition; Morphological variation; Plant types; Support vector machine; Support vectors machine; Type classifications; Support vector machines},
	correspondence_address = {R.M. Devi; Kongu Engineering College, Perundurai, Department of CSD, Erode, India; email: rmanjuladevi.gem@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835034821-7},
	language = {English},
	abbrev_source_title = {Int. Conf. Comput. Commun. Informatics, ICCCI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2023 International Conference on Computer Communication and Informatics, ICCCI 2023; Conference date: 23 January 2023 through 25 January 2023; Conference code: 188967}
}

@ARTICLE{Yuan20231037,
	author = {Yuan, Chuangchuang and Liu, Tonghai and Song, Shuang and Gao, Fangyu and Zhang, Rui},
	title = {Research on Plant Species Identification Based on Improved Convolutional Neural Network},
	year = {2023},
	journal = {Phyton-International Journal of Experimental Botany},
	volume = {92},
	number = {4},
	pages = {1037 – 1058},
	doi = {10.32604/phyton.2023.025343},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146244357&doi=10.32604%2fphyton.2023.025343&partnerID=40&md5=06634593771ec5cf1bacf56d44e87b18},
	affiliations = {College of Computer and Information Engineering, Tianjin Agricultural University, Tianjin, 300392, China},
	abstract = {Plant species recognition is an important research area in image recognition in recent years. However, the existing plant species recognition methods have low recognition accuracy and do not meet professional requirements in terms of recognition accuracy. Therefore, ShuffleNetV2 was improved by combining the current hot concern mechanism, convolution kernel size adjustment, convolution tailoring, and CSP technology to improve the accuracy and reduce the amount of computation in this study. Six convolutional neural network models with sufficient trainable parameters were designed for differentiation learning. The SGD algorithm is used to optimize the train-ing process to avoid overfitting or falling into the local optimum. In this paper, a conventional plant image dataset TJAU10 collected by cell phones in a natural context was constructed, containing 3000 images of 10 plant species on the campus of Tianjin Agricultural University. Finally, the improved model is compared with the baseline ver-sion of the model, which achieves better results in terms of improving accuracy and reducing the computational effort. The recognition accuracy tested on the TJAU10 dataset reaches up to 98.3%, and the recognition precision reaches up to 93.6%, which is 5.1% better than the original model and reduces the computational effort by about 31% compared with the original model. In addition, the experimental results were evaluated using metrics such as the confusion matrix, which can meet the requirements of professionals for the accurate identification of plant species. © 2023, Tech Science Press. All rights reserved.},
	author_keywords = {convolutional neural network; Deep learning; model improvement; plant identification},
	correspondence_address = {T. Liu; College of Computer and Information Engineering, Tianjin Agricultural University, Tianjin, 300392, China; email: tonghai_1227@126.com},
	publisher = {Tech Science Press},
	issn = {00319457},
	coden = {PHYBA},
	language = {English},
	abbrev_source_title = {Phyton-International Journal of Experimental Botany},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Fan2022,
	author = {Fan, Li and Fröhlich, Katja and Melzer, Eric and Pruitt, Rory N. and Albert, Isabell and Zhang, Lisha and Joe, Anna and Hua, Chenlei and Song, Yanyue and Albert, Markus and Kim, Sang-Tae and Weigel, Detlef and Zipfel, Cyril and Chae, Eunyoung and Gust, Andrea A. and Nürnberger, Thorsten},
	title = {Genotyping-by-sequencing-based identification of Arabidopsis pattern recognition receptor RLP32 recognizing proteobacterial translation initiation factor IF1},
	year = {2022},
	journal = {Nature Communications},
	volume = {13},
	number = {1},
	doi = {10.1038/s41467-022-28887-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126202834&doi=10.1038%2fs41467-022-28887-4&partnerID=40&md5=58a0a1ec2b5308b64e4a4a9223e83019},
	affiliations = {Center of Plant Molecular Biology (ZMBP), University of Tübingen, Tübingen, Germany; BioChem agrar, Labor für biologische und chemische Analytik GmbH, Machern, Germany; Department of Biology, University of Erlangen-Nürnberg, Erlangen, Germany; Department of Molecular Biology, Max Planck Institute for Developmental Biology, Tübingen, Germany; Department of Medical & Biological Sciences, The Catholic University of Korea, Bucheon-si, South Korea; Institute of Plant and Microbial Biology, Zürich-Basel Plant Science Center, University of Zürich, Zürich, Switzerland; Department of Biological Sciences, National University of Singapore, Singapore, Singapore; Department of Biochemistry, University of Johannesburg, Johannesburg, South Africa},
	abstract = {Activation of plant pattern-triggered immunity (PTI) relies on the recognition of microbe-derived structures, termed patterns, through plant-encoded surface-resident pattern recognition receptors (PRRs). We show that proteobacterial translation initiation factor 1 (IF1) triggers PTI in Arabidopsis thaliana and related Brassicaceae species. Unlike for most other immunogenic patterns, IF1 elicitor activity cannot be assigned to a small peptide epitope, suggesting that tertiary fold features are required for IF1 receptor activation. We have deployed natural variation in IF1 sensitivity to identify Arabidopsis leucine-rich repeat (LRR) receptor-like protein 32 (RLP32) as IF1 receptor using a restriction site-associated DNA sequencing approach. RLP32 confers IF1 sensitivity to rlp32 mutants, IF1-insensitive Arabidopsis accessions and IF1-insensitive Nicotiana benthamiana, binds IF1 specifically and forms complexes with LRR receptor kinases SOBIR1 and BAK1 to mediate signaling. Similar to other PRRs, RLP32 confers resistance to Pseudomonas syringae, highlighting an unexpectedly complex array of bacterial pattern sensors within a single plant species. © 2022, The Author(s).},
	keywords = {Arabidopsis; Arabidopsis Proteins; Genotype; Plant Diseases; Plant Immunity; Proteobacteria; Pseudomonas syringae; Receptors, Pattern Recognition; alkaline phosphatase; cold shock protein; elongation factor Tu; epitope; ethylene; flagellin; initiation factor 1; leucine; leucine rich repeat protein; luciferase; membrane protein; mitogen activated protein kinase; pathogen associated molecular pattern; pattern recognition receptor; pattern recognition receptor efr; pattern recognition receptor fls2; peptide fragment; plant extract; proline; proteinase; reactive oxygen metabolite; receptor like protein 32; rse receptor; sclerotinia culture filtrate elicitor 1; streptavidin; unclassified drug; Arabidopsis protein; pattern recognition receptor; genotype; herb; identification method; immunity; pattern recognition; peptide; Agrobacterium tumefaciens; alpha helix; amino acid sequence; Arabidopsis thaliana; Arabis; Article; autoimmunity; bacterial growth; bacterial infection; biological variation; biotinylation; Brassica oleracea; Brassica rapa; Brassicaceae; Capsella; cell density; chemoluminescence; chlorophyll content; chloroplast; controlled study; cross linking; DNA flanking region; DNA sequencing; EC50; enzyme activity; Escherichia coli; gas chromatography; gene knockout; gene sequence; genetic polymorphism; genetic transcription; genotyping; high performance liquid chromatography; histochemistry; illumina sequencing; immunoblotting; immunogenicity; immunoprecipitation; inoculation; ion exchange chromatography; isoelectric point; Komagataella pastoris; limit of detection; liquid chromatography-mass spectrometry; luminescence; Lysobacter; mass spectrometry; molecular interaction; molecular weight; mutant; Nicotiana benthamiana; Nicotiana tabacum; nonhuman; open reading frame; phytopathogen; plant defense; plant gene; plant genetics; plant growth; plant identification; plant immunity; plant leaf; plant pattern triggered immunity; polyacrylamide gel electrophoresis; promoter region; protein expression; protein family; protein motif; protein phosphorylation; protein purification; protein secondary structure; protein tertiary structure; Proteobacteria; proteomics; Pseudomonas syringae; quantitative trait locus; Ralstonia solanacearum; restriction site; restriction site associated DNA sequencing; rlp32 sobir1 bak1 complex; sequencing based identification; site directed mutagenesis; Solanum; species diversity; structural homology; transgenic plant; transposon; ultraviolet spectroscopy; Arabidopsis; genetics; genotype; metabolism; microbiology; plant disease; Proteobacteria},
	correspondence_address = {A.A. Gust; Center of Plant Molecular Biology (ZMBP), University of Tübingen, Tübingen, Germany; email: andrea.gust@zmbp.uni-tuebingen.de; T. Nürnberger; Center of Plant Molecular Biology (ZMBP), University of Tübingen, Tübingen, Germany; email: nuernberger@uni-tuebingen.de; E. Chae; Department of Molecular Biology, Max Planck Institute for Developmental Biology, Tübingen, Germany; email: dbsce@nus.edu.sg},
	publisher = {Nature Research},
	issn = {20411723},
	pmid = {35277499},
	language = {English},
	abbrev_source_title = {Nat. Commun.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Zhong2022,
	author = {Zhong, Xianping and Ban, Heng},
	title = {Pre-trained network-based transfer learning: A small-sample machine learning approach to nuclear power plant classification problem},
	year = {2022},
	journal = {Annals of Nuclear Energy},
	volume = {175},
	doi = {10.1016/j.anucene.2022.109201},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131216707&doi=10.1016%2fj.anucene.2022.109201&partnerID=40&md5=5b822026e49f6cb2cba9416513013d5e},
	affiliations = {Department of Mechanical Engineering and Materials Science, University of Pittsburgh, 3700 O'Hara Street, Pittsburgh, 15213, PA, United States},
	abstract = {Some research topics belonging to classification problems in the nuclear industry, such as fault diagnosis and accident identification, can be solved by feature extraction and subsequent application of statistical machine learning classifiers. Recently, deep neural network-based methods with automatic feature extraction and high accuracy have gained wide attention. They usually require large-scale training data, however, plant fault or accident data are scarce or difficult to obtain. This paper proposes a convolutional network (CNN)-based transfer learning method to solve this problem. The network's shallow layer is derived from a pre-trained CNN based on the ImageNet database to automatically extract features, and the deep layer is customized to match the classification problem. Data in non-image formats are converted to image formats and subsequently used to train the network. Case studies of rotating machines fault diagnosis show that the proposed method requires only limited training data to achieve high accuracy. © 2022 Elsevier Ltd},
	author_keywords = {Classification problem; ImageNet; Nuclear power plant; Pre-trained convolutional network; Transfer learning},
	keywords = {Computer aided diagnosis; Convolution; Convolutional neural networks; Deep neural networks; Extraction; Failure analysis; Fault detection; Feature extraction; Image classification; Nuclear energy; Nuclear fuels; Nuclear industry; Nuclear power plants; Nuclear reactor accidents; Classification problem; Convolutional networks; High-accuracy; Image format; Imagenet; Machine learning approaches; Network-based; Pre-trained convolutional network; Small samples; Transfer learning; Classification (of information)},
	correspondence_address = {X. Zhong; Department of Mechanical Engineering and Materials Science, University of Pittsburgh, 3700 O'Hara Street, Pittsburgh, 15213, United States; email: xianping.zhong@pitt.edu},
	publisher = {Elsevier Ltd},
	issn = {03064549},
	coden = {ANEND},
	language = {English},
	abbrev_source_title = {Ann Nucl Energy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@ARTICLE{Nasir2023,
	author = {Nasir, Fazal E. and Tufail, Muhammad and Haris, Muhammad and Iqbal, Jamshed and Khan, Said and Khan, Muhammad Tahir},
	title = {Precision agricultural robotic sprayer with real-time Tobacco recognition and spraying system based on deep learning},
	year = {2023},
	journal = {PLoS ONE},
	volume = {18},
	number = {3 MARCH},
	doi = {10.1371/journal.pone.0283801},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151395575&doi=10.1371%2fjournal.pone.0283801&partnerID=40&md5=08a3ce5ccbe9e157495b8ba5f06b884c},
	affiliations = {Advanced Robotics and Automation Laboratory, National Centre of Robotics and Automation (NCRA), Peshawar, Pakistan; Department of Mechatronics Engineering, University of Engineering and Technology, Peshawar, Pakistan; School of Computer Science, Faculty of Sciences and Engineering, University of Hull, Hull, United Kingdom; Department of Mechanical Engineering, College of Engineering, University of Bahrain, Isa Town, Bahrain},
	abstract = {Precision agricultural techniques try to prevent either an excessive or inadequate application of agrochemicals during pesticide application. In recent years, it has become popular to combine traditional agricultural practices with artificial intelligence algorithms. This research presents a case study of variable-rate targeted spraying using deep learning for tobacco plant recognition and identification in a real tobacco field. An extensive comparison of the detection performance of six YOLO-based models for the tobacco crop has been performed based on experimentation in tobacco fields. An F1-score of 87.2% and a frame per second rate of 67 were achieved using the YOLOv5n model trained on actual field data. Additionally, a novel disturbance-based pressure and flow control method has been introduced to address the issue of unwanted pressure fluctuations that are typically associated with bangbang control. The quality of spray achieved by attenuation of these disturbances has been evaluated both qualitatively and quantitatively using three different spraying case studies: broadcast, and selective spraying at 20 psi pressure; and variable-rate spraying at pressure varying from 15-120 psi. As compared to the broadcast spraying, the selective and variable rate spray methods have achieved up to 60% reduction of agrochemicals. © 2023 Nasir et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	keywords = {Artificial Intelligence; Deep Learning; Pesticides; Robotic Surgical Procedures; Tobacco; agricultural chemical; pesticide; agricultural land; Article; comparative study; deep learning; field experiment; flow; nonhuman; pesticide spraying; precision agriculture; pressure; qualitative analysis; quantitative analysis; tobacco; artificial intelligence; robot assisted surgery; tobacco},
	correspondence_address = {J. Iqbal; School of Computer Science, Faculty of Sciences and Engineering, University of Hull, Hull, United Kingdom; email: j.iqbal@hull.ac.uk},
	publisher = {Public Library of Science},
	issn = {19326203},
	coden = {POLNC},
	pmid = {37000803},
	language = {English},
	abbrev_source_title = {PLoS ONE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Keerthika2023902,
	author = {Keerthika, P. and Devi, R. Manjula and Prasad, S.J. Suji and Venkatesan, R. and Gunasekaran, Hemalatha and Sudha, K.},
	title = {Plant Classification based on Grey Wolf Optimizer based Support Vector Machine (GOS) Algorithm},
	year = {2023},
	journal = {Proceedings - 7th International Conference on Computing Methodologies and Communication, ICCMC 2023},
	pages = {902 – 906},
	doi = {10.1109/ICCMC56507.2023.10083535},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153568553&doi=10.1109%2fICCMC56507.2023.10083535&partnerID=40&md5=b4a51bc2c6f97a1b8816e308dc07c67b},
	affiliations = {Department of CSE, Vellore Institute of Technology, Tamilnadu, Vellore, India; Department of CSE, Kongu Engineering College, Tamilnadu, Perundurai, India; Department of EIE, Kongu Engineering College, Tamilnadu, Perundurai, India; Department of CSE, Karunya Institute of Technology and Sciences, Tamilnadu, Coimbatore, India; IT Department, University of Technology and Applied Sciences, Oman; Department of CSE, Muthayammal Engineering College, Tamilnadu, Rasipuram, India},
	abstract = {Leaves are the primary identifying feature of trees and other plants. Many of these plants are used in the pharmaceutical industry as industrial crops. Growing automation in industries including commerce and medicine has made accurate leaf identification crucial. Leaves are typically classified according to morphological or genetic characteristics. As a result of their numerous physical differences, however, it is becoming increasingly difficult to categorize the diverse leaf cultivars that exist. Several evolutionary shifts over the past several decades have resulted in an increase in the number of variants of a certain leaf type. To manually sift and identify these leaves is a laborious process. A novel hybrid GOS algorithm is proposed in this study for detecting leaves based on their shape, color, and texture. Three types of leaves (apple, cucumber, and mango) are used as examples, and features for each are extracted using Image Processing techniques, before being optimized with the Grey Wolf Optimizer and finally classified with the SVM (Support Vector Machine) classifier algorithm. Experimental results show that the proposed GOS work improves upon the SVM classifier, with a classification accuracy of 96.83 percent. © 2023 IEEE.},
	author_keywords = {Grey Wolf Optimizer; Plant Classification; Support Vector Machine (SVM)},
	keywords = {Crops; Cultivation; Image processing; Industrial plants; Plants (botany); Textures; Classifieds; Gray wolf optimizer; Gray wolves; Industrial crops; Optimizers; Pharmaceutical industry; Plant classification; Support vector machine; Support vector machine classifiers; Support vectors machine; Support vector machines},
	correspondence_address = {R. Venkatesan; Department of CSE, Karunya Institute of Technology and Sciences, Coimbatore, Tamilnadu, India; email: rlvenkei2000@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166546408-6},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Comput. Methodol. Commun., ICCMC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 7th International Conference on Computing Methodologies and Communication, ICCMC 2023; Conference date: 23 February 2023 through 25 February 2023; Conference code: 187717}
}

@ARTICLE{Zhang2022,
	author = {Zhang, Ruolei and Zhu, Yijun and Ge, Zhangshangjie and Mu, Hongbo and Qi, Dawei and Ni, Haiming},
	title = {Transfer Learning for Leaf Small Dataset Using Improved ResNet50 Network with Mixed Activation Functions},
	year = {2022},
	journal = {Forests},
	volume = {13},
	number = {12},
	doi = {10.3390/f13122072},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144627540&doi=10.3390%2ff13122072&partnerID=40&md5=51de167f46f309ab243194096cb14c4a},
	affiliations = {College of Science, Northeast Forestry University, Hexing Road 26, Harbin, 150040, China},
	abstract = {Taxonomic studies of leaves are one of the most effective means of correctly identifying plant species. In this paper, mixed activation function is used to improve the ResNet50 network in order to further improve the accuracy of leaf recognition. Firstly, leaf images of 15 common tree species in northern China were collected from the Urban Forestry Demonstration Base of Northeast Forestry University (45°43′–45°44′ N, 126°37′–126°38′ E, forest type was artificial forest), and a small leaf dataset was established. After that, seven commonly used activation functions were selected to improve the ResNet50 network structure, and the improved network was applied to the transfer learning research of the leaf small dataset. On this basis, five activation functions with better performance were selected for the study of mixed activation functions in deep learning. Two of these five activation functions are arbitrarily selected for combination, and a total of twenty combinations are obtained. Further, the first activation function was used in each combination to replace the first ReLU function after all addition operations in the ResNet50 network residual block structure, and another activation function was used to replace the other position ReLU functions. The experimental results show that in the transfer learning of the leaf small dataset using the ResNet50 deep residual network, the appropriate combination of mixed activation functions can increase the performance of the improved network to a certain extent. Among them, the ELU-Swish1 combination has the most significant improvement effect on the network performance, whose final effective validation accuracy reaches 98.17%. Furthermore, the comparison with GoogLeNet and VGG-16 also demonstrates the excellent performance of the improved ELU-Swish1 ResNet50 (ES-ResNet50) network architecture. Finally, tests on the other two small leaf datasets, Flavia and Swedish, also demonstrate the performance improvement of ES-ResNet50. The validation accuracy of the improved ES-Resnet 50 algorithm on these two datasets reaches 99.30% and 99.39%, respectively. All these experiments prove that the recognition performance of leaf transfer learning using the ES-ResNet50 network is indeed improved, which may be caused by the complementarity of the e-exponential gradient of ELU and Swish1 activation functions in the negative region. © 2022 by the authors.},
	author_keywords = {deep learning; leaves recognition; mixed activation functions; neural network; residual block},
	keywords = {Accuracy; Activation; Forestry; Leaves; Performance; Swedish; Transfer; Upgrading; China; Chemical activation; Deep learning; Network architecture; Plants (botany); Timber; Activation functions; Deep learning; Leaf recognition; Mixed activation function; Neural-networks; Performance; Residual block; Small data set; Taxonomic study; Transfer learning; artificial neural network; data set; leaf; machine learning; pattern recognition; taxonomy; Forestry},
	correspondence_address = {H. Ni; College of Science, Northeast Forestry University, Harbin, Hexing Road 26, 150040, China; email: nihaiming2013@nefu.edu.cn},
	publisher = {MDPI},
	issn = {19994907},
	language = {English},
	abbrev_source_title = {Forests},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Li202371,
	author = {Li, Dongfang and Li, Boliao and Long, Sifang and Feng, Huaiqu and Xi, Te and Kang, Shuo and Wang, Jun},
	title = {Rice seedling row detection based on morphological anchor points of rice stems},
	year = {2023},
	journal = {Biosystems Engineering},
	volume = {226},
	pages = {71 – 85},
	doi = {10.1016/j.biosystemseng.2022.12.012},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146344758&doi=10.1016%2fj.biosystemseng.2022.12.012&partnerID=40&md5=129e55aad65cf314da69f38a9e4b99a6},
	affiliations = {Department of Biosystems Engineering, Zhejiang University, 866 Yuhangtang Road, Hangzhou, 310058, China},
	abstract = {Field management in rice seedling paddies through vision navigation assistance is an effective measure for rice production automation and yield promotion. Rice row detection lays the foundation for vision navigation while also presenting challenges. One crucial issue hindering further improvement in rice row detection performance is the irregular growth morphology of rice plants. Rice leaves' divergent growth postures and uneven orientations make it challenging to accurately determine the actual growth site of rice seedlings. In this study, rice stems, which maintain a more stable growth posture than the leaves, were used as the primary recognition objects to eliminate the interference caused by rice leaves, thereby promoting rice row detection accuracy. Transformer-based semantic segmentation models were adopted to identify triangular morphological masks on the stem of individual rice seedlings. The anchor points representing the ground-breaking position of rice seedlings were then calculated from the predicted stem masks. A dynamic search direction-based clustering algorithm was presented to group the sparsely distributed anchor points into rows and complete the row fitting simultaneously. The proposed stem-recognition-based method achieved an excellent rice row detection performance with up to 92.93% detection accuracy under complex natural conditions, which was far superior to the 16.36% obtained by the leaf-recognition-based approach. © 2022 IAgrE},
	author_keywords = {Autonomous navigation; Machine vision; Rice row detection; Row clustering; Stem segmentation; Transformer},
	keywords = {Clustering algorithms; Navigation; Plants (botany); Semantics; Anchor point; Autonomous navigation; Clusterings; Machine-vision; Rice row detection; Rice seedlings; Row clustering; Stem segmentation; Transformer; Vision navigation; Computer vision},
	correspondence_address = {J. Wang; Department of Biosystems Engineering, Zhejiang University, Hangzhou, 866 Yuhangtang Road, 310058, China; email: jwang@zju.edu.cn},
	publisher = {Academic Press},
	issn = {15375110},
	coden = {BEINB},
	language = {English},
	abbrev_source_title = {Biosyst. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Korpela2023,
	author = {Korpela, Ilkka and Polvivaara, Antti and Hovi, Aarne and Junttila, Samuli and Holopainen, Markus},
	title = {Influence of phenology on waveform features in deciduous and coniferous trees in airborne LiDAR},
	year = {2023},
	journal = {Remote Sensing of Environment},
	volume = {293},
	doi = {10.1016/j.rse.2023.113618},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158865038&doi=10.1016%2fj.rse.2023.113618&partnerID=40&md5=f543d8fdcb575d171a2b6df91ac6956b},
	affiliations = {University of Helsinki, Faculty of Agriculture and Forestry, Department of Forest Sciences, P.O.Box 24, FI-00014, Finland; Aalto University, School of Engineering, Department of Built Environment, P.O.Box 14100, Aalto, FI-00076, Finland; University of Eastern Finland, Faculty of Science and Forestry, School of Forest Sciences, P.O. Box 111, Joensuu, FI-80101, Finland; Department of Remote Sensing and Photogrammetry, Finnish Geospatial Research Institute (FGI), National Land Survey of Finland (NLS), Geodeetinrinne 2, Masala, FI-02430, Finland},
	abstract = {Information on forest structure is vital for sustainable forest management. Currently, airborne LiDAR remote sensing has been well established as an effective tool to characterize the structure of canopies and forest inventory variables. Radiometry and geometry are highly intertwined in LiDAR remote sensing of forest vegetation and phenology influences the geometric-optical properties of deciduous and evergreen trees causing seasonal variation in LiDAR observations. This variation may be considered as a nuisance or exploited in for example tree species identification. Airborne LiDAR data are also influenced by sensor functioning, acquisition settings, scan geometry and the atmosphere. Reliable estimation of subtle phenological effects calls for data in which the impact of the external factors is minimal. We experimented with such data and explored LIDAR waveforms (WFs) in boreal trees in winter, early summer and late summer. Our objectives were to i) assess the match of the multitemporal LiDAR data for observing true changes in vegetation; ii) quantify the influence of phenology in deciduous and evergreen trees; iii) study the effect of varying scan zenith angle (SZA) and canopy age on WF features in different phenostates; iv) assess the temporal feature correlation in individual living and dead standing trees. A WF-recording pulsed LiDAR sensor unit operating at the wavelength of 1550 nm was used in repeated acquisitions. WF attributes such as energy, peak amplitude and echo width were derived for each pulse and were localized vertically to crown, understory and ground components. Silver and downy birch, black alder, European aspen, Siberian larch, Scots pine, Norway spruce and dead standing spruce formed our strata. Results showed that phenology caused more variation in WF features of deciduous trees compared to evergreen conifers. Deciduous trees displayed substantial between-species variation that was linked with differences in branching pattern, leaf orientation and bark reflectance. Pine displayed a possible winter-early summer anomaly in canopy backscattering that may be linked with changes in foliage clumping or with the role of stamens in early summer trees. Trees displayed positive temporal correlation in WF features and correlations were the strongest in evergreen and deciduous conifers and decreased with time. SZA had minor influence on WF features whereas age exercised a strong effect on many features with parallel variation between species and phenostates. Structural changes following death, i.e. ‘aging’ changed the geometric WF features of dead standing trees. Our results provide new insights for enhancing tree species identification by using WF LiDAR and for LiDAR time-series analysis of vegetation. © 2023 The Author(s)},
	author_keywords = {1550 nm; Branching structure; Change detection; Leaf orientation; Mortality; Radiometric match; Seasonality; Species identification; Time-series},
	keywords = {Backscattering; Feature extraction; Forestry; Information management; Optical properties; Optical radar; Optical remote sensing; Radiometry; Vegetation mapping; 1550 nm; Branching structures; Change detection; Leaf orientation; Mortality; Radiometric match; Radiometrics; Seasonality; Species identification; Times series; coniferous tree; deciduous tree; detection method; forest inventory; forest management; leaf; lidar; mortality; phenology; radiometric method; remote sensing; seasonality; time series analysis; waveform analysis; zenith angle; Geometry},
	correspondence_address = {I. Korpela; University of Helsinki, Faculty of Agriculture and Forestry, Department of Forest Sciences, P.O.Box 24, FI-00014, Finland; email: ilkka.korpela@helsinki.fi},
	publisher = {Elsevier Inc.},
	issn = {00344257},
	coden = {RSEEA},
	language = {English},
	abbrev_source_title = {Remote Sens. Environ.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Naali2023569,
	author = {Naali, F. and Alipour-Fard, T. and Arefi, H.},
	title = {EVALUATION OF DIFFERENT PARAMETERS FOR PLANT CLASSIFICATION BY PRE-TRAINED DEEP LEARNING MODELS WITH BIGEARTHNET DATASET},
	year = {2023},
	journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	volume = {10},
	number = {4/W1-2022},
	pages = {569 – 574},
	doi = {10.5194/isprs-annals-X-4-W1-2022-569-2023},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146943546&doi=10.5194%2fisprs-annals-X-4-W1-2022-569-2023&partnerID=40&md5=7eec0601b4e06f310a1dd6e16f06c0c5},
	affiliations = {School of Surveying and Geospatial Engineering, University of Tehran, Tehran, 1417414418, Iran; Institute for Spatial Information and Surveying Technology, Mainz University of Applied Sciences, Germany},
	abstract = {Vegetation monitoring and mapping are essential for a diverse range of environmental problems such as forest management, food resources, and climate change assessment. Several methods have been developed to classify different vegetation types based on remote sensing (RS) data. Land use classification has been revolutionized with the advent of neural networks. Various vegetation types were classified using multispectral Sentinel-2 satellite images due to their high spatial resolution and spectral information. Deep Convolutional Neural Network is considered a promising method for classifying remote sensing images with high spatial resolution due to its powerful feature extraction capabilities. However, large labeled datasets are required for better classification performance, so we have used pre-trained ResNet networks with 152 layers, 50 layers, and 101 layers trained on Big Earth Net (BEN). In order to obtain the best network performance and evaluate the sensitivity of the parameters in this study, we have performed two experiments: 1) the effect of different patch sizes and 2) increasing the number of images. The results demonstrate that ResNet 152 shows the highest accuracy with patches of 120 × 120 pixels, with an accuracy of 76.62%, and ResNet 50 is the best with an accuracy of 76.2% since the process of this network does not take much time. © Author(s) 2023. CC BY 4.0 License.},
	author_keywords = {Convolution Neural Network; Crop Classification; Deep Learning; ResNet; Sentinel-2},
	keywords = {Classification (of information); Climate change; Convolution; Deep neural networks; Image classification; Image resolution; Land use; Large dataset; Learning systems; Multilayer neural networks; Network layers; Remote sensing; Satellite imagery; Vegetation mapping; Convolution neural network; Crop classification; Deep learning; High spatial resolution; Learning models; Plant classification; Resnet; Sentinel-2; Vegetation monitoring; Vegetation type; Forestry},
	correspondence_address = {F. Naali; School of Surveying and Geospatial Engineering, University of Tehran, Tehran, 1417414418, Iran; email: f.naali@ut.ac.ir},
	editor = {Delavar M.R. and Ali Abbaspour R. and Farzaneh S.},
	publisher = {Copernicus Publications},
	issn = {21949042},
	language = {English},
	abbrev_source_title = {ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: Joint 6th Sensors and Models in Photogrammetry and Remote Sensing, SMPR 2023 and 4th Geospatial Information Research, GIResearch 2022 Conferences; Conference date: 19 February 2023 through 22 February 2023; Conference code: 186055; All Open Access, Gold Open Access}
}

@ARTICLE{Kadiwal2023273,
	author = {Kadiwal, Shashank M. and Hegde, Venkatesh and Shrivathsa, N.V. and Gowrishankar, S. and Srinivasa, A.H. and Veena, A.},
	title = {A Survey of Different Identification and Classification Methods for Medicinal Plants},
	year = {2023},
	journal = {Lecture Notes in Networks and Systems},
	volume = {563},
	pages = {273 – 291},
	doi = {10.1007/978-981-19-7402-1_20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151160213&doi=10.1007%2f978-981-19-7402-1_20&partnerID=40&md5=82710c3a2e178f8c6aaa13d090811032},
	affiliations = {Department of Computer Science and Engineering, Dr. Ambedkar Institute of Technology, Karnataka, Bengaluru, 560056, India},
	abstract = {Pharmaceutical firms are increasingly resorting to therapeutic plants since they are less costly and have fewer negative effects than existing medications. Based on these findings, some people are concerned about automated medicinal plant recognition. There are several techniques to creating a reliable classification system that can categorize medicinal plants in real time. The study includes the usefulness and reliability of many image processing algorithms, machine learning, and deep learning algorithms for plant classification based on the leaf images used in the recent years with their benefits and drawbacks. The effectiveness of these algorithms in recognizing leaf images based on plant characteristics like shape, grain, texture, and combination of many aspects is evaluated. Our paper looks at both publicly available and self-captured leaf datasets for automated plant identification, and it concludes with a summary of current research and areas for improvement. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Computer vision; Convolutional neural network (CNN); Deep learning; Leaf patterns; Machine learning; Medicinal plants; Multi-classification; Support vector machine (SVM)},
	keywords = {Computer vision; Convolutional neural networks; Deep learning; Learning algorithms; Learning systems; Plants (botany); Textures; Convolutional neural network; Deep learning; Leaf images; Leaf pattern; Machine-learning; Medicinal plants; Multi-classification; Support vector machine; Support vectors machine; Support vector machines},
	correspondence_address = {S.M. Kadiwal; Department of Computer Science and Engineering, Dr. Ambedkar Institute of Technology, Bengaluru, Karnataka, 560056, India; email: shashankkadiwal2000@gmail.com},
	editor = {Smys S. and Kamel K.A. and Palanisamy R.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {23673370},
	isbn = {978-981197401-4},
	language = {English},
	abbrev_source_title = {Lect. Notes Networks Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference on Inventive Computation and Information Technologies, ICICIT 2022; Conference date: 25 August 2022 through 26 August 2022; Conference code: 291489}
}

@ARTICLE{Bhusnurmath2023407,
	author = {Bhusnurmath, Rohini A. and Doddamani, Shaila},
	title = {Bark Texture Classification Using Deep Transfer Learning},
	year = {2023},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {14078 LNAI},
	pages = {407 – 420},
	doi = {10.1007/978-3-031-36402-0_38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164910451&doi=10.1007%2f978-3-031-36402-0_38&partnerID=40&md5=e9aec0282e37a252f5f6413c61b05221},
	affiliations = {Department of Computer Science, Karnataka State Akkamahadevi Women’s University, Karnataka, Vijayapura, 586108, India},
	abstract = {Tasks that are involved in forest are conservation, disease diagnostics, plant production, and tree species identification is crucial. There has been a disagreement over whether the leaves, fruits, flowers, or bark of the tree should be utilized to distinguish between species. Research has shown that bark is crucial because it persists during seasonal changes and gives trees their distinctive identities through structural variances. Using bark texture image to identify tree species is a difficult topic that could be helpful for many forestry-related jobs. Although recent developments of deep learning performed the outstanding outcome on common eyesight problems. This research presents a deep learning-based strategy for classifying 50 different categories of trees based upon the texture of their bark for that taken the dataset called BarkVN-50. This BarkVN-50 dataset is the largest values of trees bark that have been taken into account for bark classification thus far. Here in present experiment investigated the use feature extraction from the basic CNN model and pre-trained (Transfer learning techniques) models like VGG16, MobileNet and also compared the results of the all three models with their computation time and accuracies of each models. It is observed from results that pre-trained model work more efficient with high accuracy. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author_keywords = {CNN; Deep learning; MobileNet; Pattern recognition; Pre-trained model; Species Identification; Texture classification; VGG16},
	keywords = {Classification (of information); Conservation; Deep learning; Diagnosis; Learning systems; Pattern recognition; Plants (botany); Textures; Deep learning; Disease diagnostics; Mobilenet; Plant production; Pre-trained model; Species identification; Texture classification; Transfer learning; Tree species identifications; VGG16; Forestry},
	correspondence_address = {S. Doddamani; Department of Computer Science, Karnataka State Akkamahadevi Women’s University, Vijayapura, Karnataka, 586108, India; email: dodamanishaila@gmail.com},
	editor = {Morusupalli R. and Dandibhotla T.S. and Atluri V.V. and Komati V.R. and Windridge D. and Lingras P.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303136401-3},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 16th International Conference on Multi-disciplinary Trends in Artificial Intelligence, MIWAI 2023; Conference date: 21 July 2023 through 22 July 2023; Conference code: 296989}
}

@ARTICLE{Das2023167,
	author = {Das, Arnab and Siva Sai Kumar, B. and Shiva Shankar Reddy, S. and Naveen Reddy, S. and Peeyush, K.P.},
	title = {Ayurvedic Medicinal Plant Identification System Using Embedded Image Processing Techniques},
	year = {2023},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {977},
	pages = {167 – 184},
	doi = {10.1007/978-981-19-7753-4_14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151144207&doi=10.1007%2f978-981-19-7753-4_14&partnerID=40&md5=a296c7a7d88c09045c71effa08ab574d},
	affiliations = {Department of Electronics and Communication Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India},
	abstract = {Plants can be classified based on various classification methods such as cell, genetic and serum etc. It's difficult for an individual to explore the various classification methods and it's practically not feasible as it demands good knowledge in plant taxonomy and long-term time investment. Due to the shortage of experienced and qualified taxonomists in identification and classification of medicinal plants, with the help of different image processing algorithms and computer vision, the above difference can be bridged. The main objective is to develop a Deep Learning and Machine Learning based model to identify and classify plants based on various features, which is done with the help of Gabor filter and Gray Level Co-occurrence Matrices (GLCM) and using classifiers such as Random Forest (RF), and Light Gradient Boosting Machine (LGBM), and made a comparative analysis which resulted an accuracy of 95.5% with LGBM and GLCM filter and used to develop a standalone device that clicks a picture and identifies the medicinal plant. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Classification; Convolutional neural network; Feature extraction; Gabor filter; Gray level co-occurrence matrix; Image processing; Light gradient boosting machine; Machine learning; Medicinal plants; Random forest classifier; Segmentation; Shannon entropy},
	keywords = {Adaptive boosting; Classification (of information); Convolutional neural networks; Deep learning; Image segmentation; Investments; Learning systems; Plants (botany); Convolutional neural network; Features extraction; Gradient boosting; Gray-level co-occurrence matrix; Grey-level co-occurrence matrixes; Images processing; Light gradient boosting machine; Light gradients; Machine-learning; Medicinal plants; Random forest classifier; Segmentation; Shannon's entropy; Gabor filters},
	correspondence_address = {K.P. Peeyush; Department of Electronics and Communication Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; email: kp_peeyush@cb.amrita.edu},
	editor = {Bindhu V. and Tavares J.M. and Vuppalapati C.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981197752-7},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference on Communication, Computing and Electronics Systems, ICCCES 2022; Conference date: 15 September 2022 through 16 September 2022; Conference code: 291999}
}

@ARTICLE{Wu2022,
	author = {Wu, Zhao and Jiang, Feng and Cao, Rui},
	title = {Research on recognition method of leaf diseases of woody fruit plants based on transfer learning},
	year = {2022},
	journal = {Scientific Reports},
	volume = {12},
	number = {1},
	doi = {10.1038/s41598-022-18337-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137781514&doi=10.1038%2fs41598-022-18337-y&partnerID=40&md5=dbf613b67f3b49e58c6361c999fee06b},
	affiliations = {School of Computer and Information Engineering, Central South University of Forestry and Technology, Changsha, China},
	abstract = {Fruit leaf diseases have a significant impact on the later development and maturity of fruits, so rapid and accurate identification of fruit leaf diseases plays an important role in the development of fruit production. In this paper, the leaf disease data set of 6 kinds of fruits is divided into 25 categories according to the species—the type of the disease—the severity, and we propose an improved model based on ResNet101 to identify woody fruit plant leaf diseases, in which a global average pooling layer is used to reduce model training parameters, layer normalization, dropout and L2 regularization are used to prevent model overfitting, SENet attention mechanism is used to improve the model's ability to extract features. At the same time, transfer learning is used to reduce training time and training parameters. Experimental results show that the overall accuracy of woody fruit plant leaf recognition based on this model can reach 85.90%. Compared with the classic ResNet network, the accuracy is increased by 1.20%, and the model parameters are reduced by 98.14%. Therefore, the model proposed in this paper provides a better solution for the identification of leaf diseases of woody fruit plants and has a higher accuracy rate. © 2022, The Author(s).},
	keywords = {Fruit; Machine Learning; Plant Leaves; Plants; Wood; fruit; machine learning; plant; plant leaf; wood},
	correspondence_address = {F. Jiang; School of Computer and Information Engineering, Central South University of Forestry and Technology, Changsha, China; email: jf09mail@126.com},
	publisher = {Nature Research},
	issn = {20452322},
	pmid = {36100617},
	language = {English},
	abbrev_source_title = {Sci. Rep.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Zhang2022,
	author = {Zhang, Jian-Lin and Su, Wen-Hao and Zhang, He-Yi and Peng, Yankun},
	title = {SE-YOLOv5x: An Optimized Model Based on Transfer Learning and Visual Attention Mechanism for Identifying and Localizing Weeds and Vegetables},
	year = {2022},
	journal = {Agronomy},
	volume = {12},
	number = {9},
	doi = {10.3390/agronomy12092061},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138560512&doi=10.3390%2fagronomy12092061&partnerID=40&md5=7c6c8c8f9f5e92757fef9fe83554019d},
	affiliations = {College of Engineering, China Agricultural University, Beijing, Haidian, 100083, China},
	abstract = {Weeds in the field affect the normal growth of lettuce crops by competing with them for resources such as water and sunlight. The increasing costs of weed management and limited herbicide choices are threatening the profitability, yield, and quality of lettuce. The application of intelligent weeding robots is an alternative to control intra-row weeds. The prerequisite for automatic weeding is accurate differentiation and rapid localization of different plants. In this study, a squeeze-and-excitation (SE) network combined with You Only Look Once v5 (SE-YOLOv5x) is proposed for weed-crop classification and lettuce localization in the field. Compared with models including classical support vector machines (SVM), YOLOv5x, single-shot multibox detector (SSD), and faster-RCNN, the SE-YOLOv5x exhibited the highest performance in weed and lettuce plant identifications, with precision, recall, mean average precision (mAP), and F1-score values of 97.6%, 95.6%, 97.1%, and 97.3%, respectively. Based on plant morphological characteristics, the SE-YOLOv5x model detected the location of lettuce stem emerging points in the field with an accuracy of 97.14%. This study demonstrates the capability of SE-YOLOv5x for the classification of lettuce and weeds and the localization of lettuce, which provides theoretical and technical support for automated weed control. © 2022 by the authors.},
	author_keywords = {attention mechanism; deep learning; machine learning; SVM; transfer learning; weed identification; YOLOv5},
	correspondence_address = {W.-H. Su; College of Engineering, China Agricultural University, Haidian, Beijing, 100083, China; email: wenhao.su@cau.edu.cn},
	publisher = {MDPI},
	issn = {20734395},
	language = {English},
	abbrev_source_title = {Agronomy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access}
}

@ARTICLE{Haq2023369,
	author = {Haq, Mohd Anul and Ahmed, Ahsan and Gyani, Jayadev},
	title = {Implementation of CNN for Plant Identification using UAV Imagery},
	year = {2023},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {14},
	number = {4},
	pages = {369 – 378},
	doi = {10.14569/IJACSA.2023.0140441},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158167400&doi=10.14569%2fIJACSA.2023.0140441&partnerID=40&md5=b15aed072a53b06acdb9a8882ce88038},
	affiliations = {Department of Computer Science-College of Computer and Information Sciences, Majmaah University, Al-Majmaah, 11952, Saudi Arabia; Department of Information Technology-College of Computer and Information Sciences, Majmaah University, Al-Majmaah, 11952, Saudi Arabia},
	abstract = {Plants are the world's most significant resource since they are the only natural source of oxygen. Additionally, plants are considered crucial since they are the major source of energy for humanity and have nutritional, therapeutic, and other benefits. Image identification has become more prominent in this technology-driven world, where many innovations are happening in this sphere. Image processing techniques are increasingly being used by researchers to identify plants. The capacity of Convolutional Neural Networks (CNN) to transfer weights learned with huge standard datasets to tasks with smaller collections or more particular data has improved over time. Several applications are made for image identification using deep learning, and Machine Learning (ML) algorithms. Plant image identification is a prominent part of such. The plant image dataset of about 300 images collected by mobile phone and camera from different places in the natural scenes with nine species of different plants are deployed for training. A five-layered convolution neural network (CNN) is applied for large-scale plant classification in a natural environment. The proposed work claims a higher accuracy in plant identification based on experimental data. The model achieves the utmost recognition rate of 96% NU108 dataset and UAV images of NU101 have achieved an accuracy of 97.8%. © 2023, International Journal of Advanced Computer Science and Applications. All Rights Reserved.},
	author_keywords = {Convolutional Neural Networks (CNN); Machine Learning (ML) algorithms; plant image dataset; plant image identification},
	keywords = {Convolution; Deep learning; Image processing; Learning algorithms; Multilayer neural networks; Unmanned aerial vehicles (UAV); Convolutional neural network; Image datasets; Image identification; Machine learning algorithms; Natural sources; Plant identification; Plant image dataset; Plant image identification; Sources of energy; Convolutional neural networks},
	publisher = {Science and Information Organization},
	issn = {2158107X},
	language = {English},
	abbrev_source_title = {Intl. J. Adv.  Comput. Sci. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Thanikkal202327905,
	author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M.T.},
	title = {Deep - Morpho Algorithm (DMA) for medicinal leaves features extraction},
	year = {2023},
	journal = {Multimedia Tools and Applications},
	volume = {82},
	number = {18},
	pages = {27905 – 27925},
	doi = {10.1007/s11042-023-14567-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148630727&doi=10.1007%2fs11042-023-14567-y&partnerID=40&md5=0cf31b6c271146f770ee858782bd3682},
	affiliations = {Department of Computer Science and Engineering, Amity School of Engineering and Technology, Amity University Uttar Pradesh, UP, Noida, 201313, India; Department of Electronics and Communication Engineering, Amity School of Engineering and Technology, Amity University Uttar Pradesh, UP, Noida, 201313, India; Department of Botany, St. Thomas College, Kerala, Thrissur, India},
	abstract = {Presently, for the identification and classification of images, various deep learning techniques are being used. In these techniques, the whole image is considered to produce similar feature sets for many images. As a result, this mechanism loses many of its features at the final stage. Therefore, to analyze and identify medicinal leaves through an artificial eye of botanists, it was emphasized that the leaf image features should remain preserved till the final stage of classification for better accuracy. The existing plant identification approaches are trained using the leaf images. So leaf features are lost in the different stages of the convolution process and the same feature values are generated for similar type leaf images. This raises ambiguity in the results and affects the accuracy of leaf image identification. But here, in this proposed deep learning-based plant leaves morphological feature recognition system, leaf morphological features are used to train the system. Morphological features are identified to recognize a plant leaf. Here, morphological features of medicinal plant leaves, venation, shapes, apices, and bases are extracted and analyzed to predict the image class. So, the leaf features remain persevered until the final stage. The proposed feature recognition analysis improves the accuracy of the leaf identification method. In this, more than 300 leaves from 18 different plant families are collected and trained to build the deep learning classifier and achieve 96% accuracy. The performance evaluation was also conducted over “Flavia”, “Swedish” and “Leaf” data set and obtained 91%, 87% and 91% accuracy. The performance of image classification and feature preservation algorithms with less computational power are indicating the potential applicability of the proposed Deep - Morpho Algorithm (DMA) in medicinal plants and leaves identification. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Deep learning; Feature recognition; Leaf morphology; Medicinal plant; Plant recognition},
	keywords = {Classification (of information); Image analysis; Image classification; Learning systems; Morphology; Plants (botany); Deep learning; Features recognition; Image features; Leaf images; Leaf morphology; Medicinal plants; Morphological features; Plant identification; Plant leaves; Plant recognition; Deep learning},
	correspondence_address = {A.K. Dubey; Department of Electronics and Communication Engineering, Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida, UP, 201313, India; email: dubey1ak@gmail.com},
	publisher = {Springer},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Batchuluun2022,
	author = {Batchuluun, Ganbayar and Nam, Se Hyun and Park, Kang Ryoung},
	title = {Deep Learning-Based Plant Classification Using Nonaligned Thermal and Visible Light Images},
	year = {2022},
	journal = {Mathematics},
	volume = {10},
	number = {21},
	doi = {10.3390/math10214053},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141837035&doi=10.3390%2fmath10214053&partnerID=40&md5=91ff99c0d80f95cb099e5068b43dd698},
	affiliations = {Division of Electronics and Electrical Engineering, Dongguk University, 30 Pildong-ro, 1-gil, Jung-gu, Seoul, 04620, South Korea},
	abstract = {There have been various studies conducted on plant images. Machine learning algorithms are usually used in visible light image-based studies, whereas, in thermal image-based studies, acquired thermal images tend to be analyzed with a naked eye visual examination. However, visible light cameras are sensitive to light, and cannot be used in environments with low illumination. Although thermal cameras are not susceptible to these drawbacks, they are sensitive to atmospheric temperature and humidity. Moreover, in previous thermal camera-based studies, time-consuming manual analyses were performed. Therefore, in this study, we conducted a novel study by simultaneously using thermal images and corresponding visible light images of plants to solve these problems. The proposed network extracted features from each thermal image and corresponding visible light image of plants through residual block-based branch networks, and combined the features to increase the accuracy of the multiclass classification. Additionally, a new database was built in this study by acquiring thermal images and corresponding visible light images of various plants. © 2022 by the authors.},
	author_keywords = {deep learning; image classification; plant image; thermal image; visible light image},
	correspondence_address = {K.R. Park; Division of Electronics and Electrical Engineering, Dongguk University, Seoul, 30 Pildong-ro, 1-gil, Jung-gu, 04620, South Korea; email: parkgr@dongguk.edu},
	publisher = {MDPI},
	issn = {22277390},
	language = {English},
	abbrev_source_title = {Mathematics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Weyler20233310,
	author = {Weyler, Jan and Labe, Thomas and Magistri, Federico and Behley, Jens and Stachniss, Cyrill},
	title = {Towards Domain Generalization in Crop and Weed Segmentation for Precision Farming Robots},
	year = {2023},
	journal = {IEEE Robotics and Automation Letters},
	volume = {8},
	number = {6},
	pages = {3310 – 3317},
	doi = {10.1109/LRA.2023.3262417},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151567749&doi=10.1109%2fLRA.2023.3262417&partnerID=40&md5=96aa0ecc1408bbcaf82be5d98baa1623},
	affiliations = {University of Bonn, Bonn, 53115, Germany; University of Oxford, Department of Engineering Science, Oxford, OX1 4BH, United Kingdom; Lamarr Institute for Machine Learning and Artificial Intelligence, Bonn, 53115, Germany},
	abstract = {Precision farming robots offer the potential to reduce the amount of used agrochemicals through targeted interventions and thus are a promising step towards sustainable agriculture. A prerequisite for such systems is a robust plant classification system that can identify crops and weeds in various agricultural fields. Most vision-based systems train convolutional neural networks (CNNs) on a given dataset, i.e., the source domain, to perform semantic segmentation of images. However, deploying these models on unseen fields, i.e., in the target domain, often shows a low generalization capability. Enhancing the generalization capability of CNNs is critical to increasing their performance on target domains with different operational conditions. In this letter, we present a domain generalized semantic segmentation approach for robust crop and weed detection by effectively extending and diversifying the source domain to achieve high performance across different agricultural field conditions. We propose to leverage unlabeled images captured from various agricultural fields during training in a two-step framework. First, we suggest a method to automatically compute sparse annotations and use them to present the model more plant varieties and growth stages to enhance its generalization capability. Among others, we exploit unlabeled images from fields containing crops sown in rows. Second, we propose a style transfer method that renders the source domain images in the style of images from various fields to achieve increased diversification. We conduct extensive experiments and show that we achieve superior performance in crop-weed segmentation across various fields compared to state-of-the-art methods.  © 2016 IEEE.},
	author_keywords = {deep learning for visual perception; Robotics and automation in agriculture and forestry; semantic scene understanding},
	keywords = {Agricultural chemicals; Agricultural robots; Deep learning; Farms; Neural networks; Semantic Segmentation; Semantics; Weed control; Agricultural fields; Annotation; Deep learning for visual perception; Farming; Robotic and automation in agriculture and forestry; Scene understanding; Semantic scene understanding; Semantic segmentation; Vegetation mapping; Visual perception; Crops},
	correspondence_address = {C. Stachniss; University of Bonn, Bonn, 53115, Germany; email: cyrill.stachniss@igg.uni-bonn.de},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {23773766},
	language = {English},
	abbrev_source_title = {IEEE Robot. Autom.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Sonia2023,
	author = {Sonia, S. V. Evangelin and Dhanush, N. and Dhanush Ram, S.},
	title = {Medicinal Plants Classification by VisualCharacteristics of Leaves Using CNN},
	year = {2023},
	journal = {2023 2nd International Conference on Electrical, Electronics, Information and Communication Technologies, ICEEICT 2023},
	doi = {10.1109/ICEEICT56924.2023.10157410},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165769895&doi=10.1109%2fICEEICT56924.2023.10157410&partnerID=40&md5=55f2737fb21c237259d718ef7ba4db77},
	affiliations = {Karunya Institute of Technology and Sciences, Tamil Nadu, India},
	abstract = {Identification of medicinal plants correctly is crucial in Ayurvedic medicine. By their visual characteristics and scent, plants are recognized by human experts. Negative effects could come from incorrectly identifying therapeutic herbs. Using visual morphological traits like the size, color, and texture of the leaves and flowers, it is possible to automate the identification of plants. In this study, we demonstrate the use of machine learning and image processing to identify uncommon medicinal plants with high accuracy. The leaves and petals of rare medicinal plants used in Ayurvedic medicine were scanned and turned into a database for this study. Images were captured of both the anterior and posterior sides of leaves and flowers. According to their distinct feature combination, leaves are categorized. The success rates of identifying plants have been found to be as high as 98% in testing. © 2023 IEEE.},
	author_keywords = {ayurveda; classification; convolutional Neural Networks; feature extraction; Identification of plants; medicine; morphological traits},
	keywords = {Image processing; Plants (botany); Textures; Ayurveda; Ayurvedic medicine; Color and textures; Convolutional neural network; Features extraction; Human expert; Identification of plant; Medicinal plants; Morphological traits; Plant classification; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835039763-5},
	language = {English},
	abbrev_source_title = {Int. Conf. Electr., Electron., Inf. Commun. Technol., ICEEICT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2nd International Conference on Electrical, Electronics, Information and Communication Technologies, ICEEICT 2023; Conference date: 5 April 2023 through 7 April 2023; Conference code: 190032}
}

@ARTICLE{Minowa2022,
	author = {Minowa, Yasushi and Shigematsu, Koharu and Takahara, Hikaru},
	title = {A Deep Learning-Based Model for Tree Species Identification Using Pollen Grain Images},
	year = {2022},
	journal = {Applied Sciences (Switzerland)},
	volume = {12},
	number = {24},
	doi = {10.3390/app122412626},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144834485&doi=10.3390%2fapp122412626&partnerID=40&md5=48f779fd327aa50f2eb548274a393256},
	affiliations = {Graduate School of Life and Environmental Sciences, Kyoto Prefectural University, Kyoto, 6068522, Japan; Faculty of Life and Environmental Sciences, Kyoto Prefectural University, Kyoto, 6068522, Japan},
	abstract = {The objective of this study was to develop a deep learning-based tree species identification model using pollen grain images taken with a camera mounted on an optical microscope. From five focal points, we took photographs of pollen collected from tree species widely distributed in the Japanese archipelago, and we used these to produce pollen images. We used Caffe as the deep learning framework and AlexNet and GoogLeNet as the deep learning algorithms. We constructed four learning models that combined two learning patterns, one for focal point images with data augmentation, for which the training and test data were the same, and the other without data augmentation, for which they were not the same. The performance of the proposed model was evaluated according to the MCC and F score. The most accurate classification model was based on the GoogLeNet algorithm, with data augmentation after 200 epochs. Tree species identification accuracy varied depending on the focal point, even for the same pollen grain, and images focusing on the pollen surface tended to be more accurately classified than those focusing on the pollen outline and membrane structure. Castanea crenata, Fraxinus sieboldiana, and Quercus crispula pollen grains were classified with the highest accuracy, whereas Gamblea innovans, Carpinus tschonoskii, Cornus controversa, Fagus japonica, Quercus serrata, and Quercus sessilifolia showed the lowest classification accuracy. Future studies should consider application to fossil pollen in sediments and state-of-the-art deep learning algorithms. © 2022 by the authors.},
	author_keywords = {AlexNet; Caffe; deep learning; F score; focal point; GoogLeNet; MCC; pollen grain images; tree species identification},
	correspondence_address = {Y. Minowa; Graduate School of Life and Environmental Sciences, Kyoto Prefectural University, Kyoto, 6068522, Japan; email: sharmy@uf.kpu.ac.jp},
	publisher = {MDPI},
	issn = {20763417},
	language = {English},
	abbrev_source_title = {Appl. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Blesslin Elizabeth20231772,
	author = {Blesslin Elizabeth, C.P. and Baulkani, S.},
	title = {Novel Network for Medicinal Leaves Identification},
	year = {2023},
	journal = {IETE Journal of Research},
	volume = {69},
	number = {4},
	pages = {1772 – 1782},
	doi = {10.1080/03772063.2021.2016504},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122453273&doi=10.1080%2f03772063.2021.2016504&partnerID=40&md5=179aac4c5f540d9e4abaff060775e09f},
	affiliations = {Department of Electronics and Communication Engineering, Arunachala College of Engineering for Women, Manavilai, Tamil Nadu, Kanyakumari, 629 203, India; Department of Electronics and Communication Engineering, Government College of Engineering, Tamil Nadu, Tirunelveli, 627 007, India},
	abstract = {From the ancient days, plant leaves are used to cure various infectious diseases. Even today, herbal leaves are preferred by medicinal experts for treating cancer, asthma, heart problems, etc. The recognition of these herbal plants is based on the visual perception of villagers. There are many kinds of species that seem to be very similar in color and shape. There is a high probability of human error in the identification of such plants. It is inevitable to correctly identify the species of plants to treat the patients. Therefore, a smart plant classification system is essentially required to eliminate human error. This research work develops a hybrid system that is based on deep convolutional neural networks. The system is named as AousethNet which is a modification of AlexNet by replacing its classifier namely, SoftMax with the Majority vote classifier. It is trained to predict the plant species from a huge number of leaf samples from four datasets namely Mendeley, D-Leaf, Flavia, and Folio. Typically, the performance of AousethNet with Mendeley dataset attained an accuracy of 99.89%, precision 98.61%, and very less recognition time of 0.087 s/image. This system is found to have good feature extraction and strong discrimination ability compared with the original version of AlexNet. © 2023 IETE.},
	author_keywords = {AlexNet; AousethNet; Deep convolutional neural networks; Majority vote classifier; Medicinal leaves identification; SoftMax},
	keywords = {Convolution; Convolutional neural networks; Deep neural networks; Diseases; Errors; Plants (botany); Alexnet; Aousethnet; Convolutional neural network; Deep convolutional neural network; Human errors; Leaf identification; Majority vote classifier; Majority voter; Medicinal leaf identification; Softmax; Hybrid systems},
	correspondence_address = {C.P. Blesslin Elizabeth; Department of Electronics and Communication Engineering, Arunachala College of Engineering for Women, Kanyakumari, Manavilai, Tamil Nadu, 629 203, India; email: bless_christo@yahoo.co.in},
	publisher = {Taylor and Francis Ltd.},
	issn = {03772063},
	language = {English},
	abbrev_source_title = {IETE J Res},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Zhang2023,
	author = {Zhang, Xuechen and Wu, Zhengmin and Cao, Chengmao and Luo, Kun and Qin, Kuan and Huang, Yangyang and Cao, Jie},
	title = {Design and operation of a deep-learning-based fresh tea-leaf sorting robot},
	year = {2023},
	journal = {Computers and Electronics in Agriculture},
	volume = {206},
	doi = {10.1016/j.compag.2023.107664},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146852789&doi=10.1016%2fj.compag.2023.107664&partnerID=40&md5=6829d62b09692bb4c510613a44987c43},
	affiliations = {College of Engineering, Anhui Agricultural University, Hefei, 230036, China; School of Tea and Food Sand Technology, Anhui Agricultural University, Hefei, 230036, China; State Key Laboratory of Tea Plant Biology and Utilization, Key Laboratory of Tea Biology and Tea Processing of Ministry of Agriculture and Rural Affairs, International Joint Research Laboratory of Tea Chemistry and Health Effects of Ministry of Education, Anhui, Hefei, 230036, China},
	abstract = {Tea is one of the most popular beverages worldwide. It is rich in substances, such as tea polyphenols and polysaccharides, that are closely related to human health. However, fresh tea leaves picked by machines are neither neat nor uniform. In this study, we apply a deep learning algorithm and the DELTA parallel robotic arm for high-precision and fast sorting of machine-picked fresh tea leaves. First, a convolutional neural network algorithm and a regional segmentation method are used to achieve fast identification and localization of machine-picked fresh tea leaves. Second, the actual running time model of the stepper motor at different pulse frequencies was established by the interpolation fitting method. The running times of the parallel mechanical arm associated with S-curve type acceleration and deceleration motions were calculated accurately using this model, and the sorting point of tea fresh leaves was then determined. A multi-objective, continuous sorting model of machine-picked fresh tea leaves is constructed to verify and optimize the model effects. The results of the robot's arm running-time test show that the running time from the end of the robotic arm's application to the fresh tea leaf sorting plane is in the range 500–1800 ms. The experimental results of fresh tea leaf recognition and classification show that after 150 iterations, the recognition accuracy of the validation set can reach 99.82%. Finally, the average sorting accuracy of the four experiments reach 89%, with the highest sorting accuracy reaching up to 92%. These results demonstrate that the proposed method leads to an excellent sorting effect pertaining to machine-picked fresh tea leaves. This approach could be easily applied to any agricultural product sorting application. © 2023 Elsevier B.V.},
	author_keywords = {Convolutional neural network; DELTA parallel robotic arm; Fresh tea leaf; Sorting robot},
	keywords = {Acceleration; Agricultural products; Agricultural robots; Convolution; Deep learning; Learning algorithms; Machine design; Robotic arms; Stepping motors; Convolutional neural network; DELTA parallel robotic arm; Design and operations; Fresh tea leaf; Parallel robotics; Running time; Sorting accuracies; Sorting robot; Tea polyphenols; Tea polysaccharides; algorithm; artificial neural network; design method; interpolation; machine learning; operations technology; robotics; segmentation; Convolutional neural networks},
	correspondence_address = {Z. Wu; School of Tea and Food Sand Technology, Anhui Agricultural University, Hefei, 230036, China; email: wzmin@ahau.edu.cn},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Thanikkal2022702,
	author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M.T.},
	title = {Deep Learning based Aquatic and Semi Aquatic Plants Morphological Features Extraction and Classification},
	year = {2022},
	journal = {International Journal of Performability Engineering},
	volume = {18},
	number = {10},
	pages = {702 – 709},
	doi = {10.23940/ijpe.22.10.p3.702-709},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143878159&doi=10.23940%2fijpe.22.10.p3.702-709&partnerID=40&md5=936a6f4984748b7ca6d122d8e0070c20},
	affiliations = {Amity School of Engineering and Technology, Amity University Uttar Pradesh, Uttar Pradesh, Noida, 201303, India; Department of Botany, St. Thomas College, Kerala, Thrissur, 680001, India},
	abstract = {In Ayurveda, the ancient medicinal plant identification system is based on the morphological comparison of leaf, fruit, flower, root, stem etc. Botanists use morphometrics for aquatic and semi-aquatic medicinal plants classification. However, deep learning networks provide the highest image classification result in digital image processing. Existing deep learning algorithms generate feature maps for pixel-wise image classification. In the feature map of deep learning output, most of the morphological features are missing. This issue leads to the Catastrophic forgetting issue of deep learning. To generate a traditional morphological feature-based medicinal plant identification system, we are introducing morphometrics and morphological feature-based deep learning networks for aquatic and semi-aquatic plant classification. This article contains: (a) A detailed morphological features database of aquatic and semi-aquatic medicinal plants, (b) a summary of the importance of the morphological features-based leaf classification, (c) a morphological features extraction algorithm and (d) the morphological features-based deep learning approach for aquatic and semi-aquatic plant classification. This human brain-like procedure achieved 97% classification accuracy and reduced the Catastrophic forgetting issue of continual learning.  © 2022 Totem Publisher, Inc.},
	author_keywords = {aquatic plants; deep learning; image processing; medicinal plants; morphometric},
	correspondence_address = {A.K. Dubey; Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida, Uttar Pradesh, 201303, India; email: dubey1ak@gmail.com},
	publisher = {Totem Publishers Ltd},
	issn = {09731318},
	language = {English},
	abbrev_source_title = {Int. J. Perform. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Sun2023,
	author = {Sun, Xiaobo and Xu, Lin and Zhou, Yufeng and Shi, Yongjun},
	title = {Leaves and Twigs Image Recognition Based on Deep Learning and Combined Classifier Algorithms},
	year = {2023},
	journal = {Forests},
	volume = {14},
	number = {6},
	doi = {10.3390/f14061083},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164141316&doi=10.3390%2ff14061083&partnerID=40&md5=e2faae81d438fa3a81d871ed26a1df29},
	affiliations = {State Key Laboratory of Subtropical Silviculture, Zhejiang A&F University, Hangzhou, 311300, China; Key Laboratory of Carbon Cycling in Forest Ecosystems and Carbon Sequestration of Zhejiang Province, Zhejiang A&F University, Hangzhou, 311300, China; School of Environmental and Resources Science, Zhejiang A&F University, Hangzhou, 311300, China},
	abstract = {In recent years, the automatic recognition of tree species based on images taken by digital cameras has been widely applied. However, many problems still exist, such as insufficient tree species image acquisition, uneven distribution of image categories, and low recognition accuracy. Tree leaves can be used to differentiate and classify tree species due to their cognitive signatures in color, vein texture, shape contour, and edge serration. Moreover, the way the leaves are arranged on the twigs has strong characteristics. In this study, we first built an image dataset of 21 tree species based on the features of the twigs and leaves. The tree species feature dataset was divided into the training set and test set, with a ratio of 8:2. Feature extraction was performed after training the convolutional neural network (CNN) using the k-fold cross-validation (K-Fold–CV) method, and tree species classification was performed with classifiers. To improve the accuracy of tree species identification, we combined three improved CNN models with three classifiers. Evaluation indicators show that the overall accuracy of the designed composite model was 1.76% to 9.57% higher than other CNN models. Furthermore, in the MixNet XL CNN model, combined with the K-nearest neighbors (KNN) classifier, the highest overall accuracy rate was obtained at 99.86%. In the experiment, the Grad-CAM heatmap was used to analyze the distribution of feature regions that play a key role in classification decisions. Observation of the Grad-CAM heatmap illustrated that the main observation area of SE-ResNet50 was the most accurately positioned, and was mainly concentrated in the interior of small twigs and leaflets. Our research showed that modifying the training method and classification module of the CNN model and combining it with traditional classifiers to form a composite model can effectively improve the accuracy of tree species recognition. © 2023 by the authors.},
	author_keywords = {attention mechanism; convolutional neural network (CNN); support vector machine (SVM); tree species recognition},
	keywords = {Accuracy; Classification; Classifiers; Education; Leaves; Statistical Analysis; Trees; Twigs; Convolution; Convolutional neural networks; Deep learning; Image classification; Image recognition; Nearest neighbor search; Neural network models; Statistical tests; Textures; Attention mechanisms; Convolutional neural network; Neural network model; Overall accuracies; Species recognition; Support vector machine; Support vectors machine; Tree species; Tree species recognition; algorithm; artificial neural network; data set; identification method; image classification; leaf; support vector machine; Support vector machines},
	correspondence_address = {Y. Shi; State Key Laboratory of Subtropical Silviculture, Zhejiang A&F University, Hangzhou, 311300, China; email: 19940009@zafu.edu.cn},
	publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
	issn = {19994907},
	language = {English},
	abbrev_source_title = {Forests},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Xiao2023,
	author = {Xiao, Qi and Zhou, Zhenzeng and Shen, Zijie and Chen, Jiandan and Gu, Chunchuan and Li, Lihua and Chen, Fengnong and Liu, Hongying},
	title = {Electrochemical fingerprinting combined with machine learning algorithm for closely related medicinal plant identification},
	year = {2023},
	journal = {Sensors and Actuators B: Chemical},
	volume = {375},
	doi = {10.1016/j.snb.2022.132922},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141320997&doi=10.1016%2fj.snb.2022.132922&partnerID=40&md5=8b7199ee9220defd0c3b0f19fdac9bd9},
	affiliations = {College of Automation, Hangzhou Dianzi University, Hangzhou, 310018, China; Shulan (Hangzhou) Hospital Affiliated to Zhejiang Shuren University Shulan International Medical College, Hangzhou, 310000, China},
	abstract = {Medicinal plants have been widely used in the treatment of various diseases for human health. We developed a novel method for the identification of closely related medicinal plants using a machine learning (ML)-based electrochemical fingerprinting platform. Firstly, the system featured a bare glassy carbon electrode capable of recording the voltammetric response of active components in medicinal plants as electrochemical fingerprints. Subsequently, different algorithms and various datasets were employed to analyze the correlation between the above electrochemical fingerprint data and the medicinal plant species. As a proof-of-concept, 6 species of Anoectochilus roxburghii (A. roxburghii) were selected as the verification samples. The electrochemical fingerprints of the samples were measured by differential pulse voltammetry in two buffer solutions. Thereafter, four powerful ML algorithms were utilized for the identification of A. roxburghii with different datasets. The results showed that the accuracy of identifying species reached 94.4 % by the nonlinear support vector machines based on the slope data of electrochemical responses in two buffer solutions, evidencing the successful discrimination of closely related medical plants by this method. Additionally, ML combined with electrochemical fingerprinting approaches had the advantages of being rapid, affordable, and straightforward, which provided potential applications in pharmaceutical research and plant taxonomy. © 2022 Elsevier B.V.},
	author_keywords = {Anoectochilus roxburghii; Electrochemical fingerprinting; Machine learning; Medicinal plant identification},
	keywords = {Glass membrane electrodes; Learning systems; Palmprint recognition; Plants (botany); Support vector machines; Voltammetry; Anoectochilus roxburghii; Buffer solutions; Electrochemical fingerprinting; Electrochemicals; Human health; Machine learning algorithms; Machine-learning; Medicinal plant identification; Medicinal plants; Plant identification; Learning algorithms},
	correspondence_address = {L. Li; College of Automation, Hangzhou Dianzi University, Hangzhou, 310018, China; email: lilh@hdu.edu.cn},
	publisher = {Elsevier B.V.},
	issn = {09254005},
	coden = {SABCE},
	language = {English},
	abbrev_source_title = {Sens Actuators, B Chem},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Bilgili2023271,
	author = {Bilgili, Ayşin and Bilgili, Ali Volkan and Tenekeci, Mehmet Emin and Karadağ, Kerim},
	title = {Spectral characterization and classification of two different crown root rot and vascular wilt diseases (fusarium oxysporum f.sp. radicis lycopersici and fusarium solani) in tomato plants using different machine learning algorithms},
	year = {2023},
	journal = {European Journal of Plant Pathology},
	volume = {165},
	number = {2},
	pages = {271 – 286},
	doi = {10.1007/s10658-022-02605-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140409197&doi=10.1007%2fs10658-022-02605-8&partnerID=40&md5=146e0a97710576c4964d4b1a039debd3},
	affiliations = {Department of Plant Health, GAP Agricultural Research Institute (GAPTAEM), Şanlıurfa, 63100, Turkey; Department of Soil Science and Plant Nutrition, Agriculture Faculty, Harran University, Osmanbey Campus, Şanlıurfa, 63300, Turkey; Department of Computer Engineering, Faculty of Engineering, Harran University, Osmanbey Campus, Şanlıurfa, 63300, Turkey; Department of Electrical and Electronics Engineering, Faculty of Engineering, Harran University, Osmanbey Campus, Şanlıurfa, 63300, Turkey},
	abstract = {Fusarium oxysporum f.sp. radicis lycopersici (FORL) and Fusarium solani (F.S.) are common fungi responsible for crown root rot and vascular wilt diseases that highly impact the development of plants, causing significant yield losses. This study investigated changes in the hyperspectral reflectance of normal and Fusarium (FORL and F.S.)-infected tomato plants in a growth chamber at different disease stages (3, 10, 16, 23, 31 and 37 days after inoculation (DAI)) using a spectroradiometer as an alternative to traditional approaches for the early identification and classification of such diseases. Raw spectra, significant wavebands obtained with the RELIEF algorithm and various statistical features extracted from raw spectra were used to classify healthy and infected plants using three different classification algorithms (CAs): decision tree, cubic support vector machine and k-nearest neighbor models. At different stages of the disease, the spectral bands such as 508, 711, 540, 717, 536, 644 nm and 705, 1883, 525, 518, 444, 522 nm were the most effective in distinguishing FORL and F.S.-inoculated plants from healthy plants, respectively. While FORL caused general stress in the plants, F.S. also had a negative physiological effect. All CAs proved highly successful in distinguishing healthy and diseased plants, with maximum classification accuracy achieved as early as 3 DAI. CAs using statistical parameters as input had higher accuracies than other CAs. Healthy and diseased plant classification was significantly different between the different CAs (p < 0.05), while DAI, pathogen type and inputs of the classification did not exhibit significant differences in classification (p > 0.05) according to ANOVA. © 2022, Koninklijke Nederlandse Planteziektenkundige Vereniging.},
	author_keywords = {ANOVA; Classification; Fusarium spp; Machine learning algorithms; Tomatoes; Visible near-infrared spectroradiometry},
	keywords = {algorithm; disease; inoculation; machine learning; pathogen; reflectance; root rot; soil classification; support vector machine; wilt},
	correspondence_address = {A. Bilgili; Department of Plant Health, GAP Agricultural Research Institute (GAPTAEM), Şanlıurfa, 63100, Turkey; email: aysin.bilgili@tarimorman.gov.tr},
	publisher = {Institute for Ionics},
	issn = {09291873},
	coden = {EPLPE},
	language = {English},
	abbrev_source_title = {Eur. J. Plant Pathol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Pushpa2023,
	author = {Pushpa, B.R. and Hemanth Kumar, M. and Shobha Rani, N.},
	title = {Exploring the Capability of NASNet Model for Plant Classification on GRASP-125 Dataset},
	year = {2023},
	journal = {2023 4th International Conference for Emerging Technology, INCET 2023},
	doi = {10.1109/INCET57972.2023.10169961},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166373980&doi=10.1109%2fINCET57972.2023.10169961&partnerID=40&md5=a2b275c337dc56bbaa56d54227e84e3c},
	affiliations = {Mysuru Campus Amrita Vishwa Vidyapeetham, Dept. of Computer Science School of Computing, India},
	abstract = {Plant classification and identification, which has strong ties to both computer science and taxonomy, is becoming an increasingly dynamic area of research within the field of computer recognition. Researchers have utilized different techniques including image processing, pattern recognition, and machine learning to develop systems capable of accurately identifying and classifying plants. Access to large image databases and advances in image processing and artificial intelligence have led to the development of highly accurate plant identification systems. These systems have the potential to greatly reduce the amount of time and proficiency required for plant identification, which are only achievable by skilled taxonomists. In this study, we experimented with the Greek vAScular Plants (GRASP) dataset, consisting of images across 125 different plant species native to the mountains belonging Oiti and Parnassus in central Greece, to perform automatic plant identification using a deep learning approach namely NASNet. These plant species, which include both rare and aesthetically pleasing varieties, are often found along popular hiking routes in the region. Our method for plant classification utilizes deep learning techniques on the GRASP-125 dataset, with the goal of improving the accuracy and efficiency of plant identification. © 2023 IEEE.},
	author_keywords = {computer vision; deep learning; GRASP-125 dataset; NASNet; plant classification},
	keywords = {Classification (of information); Deep learning; Learning systems; Classification and identifications; Deep learning; Greek vascular plant-125 dataset; Images processing; Machine-learning; NASNet; Plant classification; Plant identification; Plant species; Vascular plant; Computer vision},
	correspondence_address = {B.R. Pushpa; Mysuru Campus Amrita Vishwa Vidyapeetham, Dept. of Computer Science School of Computing, India; email: preeths1@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835033575-0},
	language = {English},
	abbrev_source_title = {Int. Conf. Emerg. Technol., INCET},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference for Emerging Technology, INCET 2023; Conference date: 26 May 2023 through 28 May 2023; Conference code: 190624}
}

@ARTICLE{Chanyal202278,
	author = {Chanyal, Himanshu and Yadav, Rakesh Kumar and Saini, Dilip Kumar J.},
	title = {Classification of Medicinal Plants Leaves Using Deep Learning Technique: A Review},
	year = {2022},
	journal = {International Journal of Intelligent Systems and Applications in Engineering},
	volume = {10},
	number = {4},
	pages = {78 – 87},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153406913&partnerID=40&md5=ff587390579fd12bcbe934b4f854a6d2},
	affiliations = {Department of Computer Science & Engineering, IFTM University, U.P, Moradabad, India; Department of Computer Science & Engineering, Himalayan School of Science & Technology, Swami Rama Himalayan University, Uttarakhand, Dehradun, India},
	abstract = {Pharmaceutical companies are increasingly using medicinal plants since they are less costly and have less adverse effects than current drugs. As a result, a lot of academics are very interested in studying automatic medicinal plant classification. A powerful classifier that can accurately categorize therapeutic plants in real time must be created. This article reviews the effectiveness and predictability of many machine learning and deep learning algorithms deployed in recent years to categorize plants using pictures of their leaves. This study contains image processing techniques for some classifiers that are used to recognize leaves and extract important leaf characteristics. Early plant disease identification is essential because plant diseases have an impact on the growth of their specific species. There are several Machine learning models that are used to identify and classify the signs of plant diseases, but recent advancements in Deep Learning, a subset of ML, seem to offer tremendous promise for improved accuracy. The ML and DL models used to categorize different plant leaves are thoroughly reviewed in this article. © 2022, Ismail Saritas. All rights reserved.},
	author_keywords = {Deep learning; Leaf pattern recognition; Machine learning; Medicinal plants classification; Plant disease},
	correspondence_address = {D.K.J. Saini; Department of Computer Science & Engineering, Himalayan School of Science & Technology, Swami Rama Himalayan University, Dehradun, Uttarakhand, India; email: dilipsaini@email.com},
	publisher = {Ismail Saritas},
	issn = {21476799},
	language = {English},
	abbrev_source_title = {Internat. J. Intel. Syst. Appl. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kim2022,
	author = {Kim, Tae Kyung and Hong, Jeonghyun and Ryu, Daun and Kim, Sukyung and Byeon, Si Yeon and Huh, Woojin and Kim, Kunhyo and Baek, Gyu Heon and Kim, Hyun Seok},
	title = {Identifying and extracting bark key features of 42 tree species using convolutional neural networks and class activation mapping},
	year = {2022},
	journal = {Scientific Reports},
	volume = {12},
	number = {1},
	doi = {10.1038/s41598-022-08571-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126737959&doi=10.1038%2fs41598-022-08571-9&partnerID=40&md5=b8a1fd129df422a165c21cc3be76f2cf},
	affiliations = {Department of Agriculture, Forestry and Bioresources, Seoul National University, Seoul, 08826, South Korea; Department of Forest Sciences, Seoul National University, Seoul, 08826, South Korea; Interdisciplinary Program in Agricultural and Forest Meteorology, Seoul National University, Seoul, 08826, South Korea; National Center for Agrometeorology, Seoul, 08826, South Korea; Research Institute of Agricultural and Life Sciences, Seoul National University, Seoul, 08826, South Korea},
	abstract = {The significance of automatic plant identification has already been recognized by academia and industry. There were several attempts to utilize leaves and flowers for identification; however, bark also could be beneficial, especially for trees, due to its consistency throughout the seasons and its easy accessibility, even in high crown conditions. Previous studies regarding bark identification have mostly contributed quantitatively to increasing classification accuracy. However, ever since computer vision algorithms surpassed the identification ability of humans, an open question arises as to how machines successfully interpret and unravel the complicated patterns of barks. Here, we trained two convolutional neural networks (CNNs) with distinct architectures using a large-scale bark image dataset and applied class activation mapping (CAM) aggregation to investigate diagnostic keys for identifying each species. CNNs could identify the barks of 42 species with > 90% accuracy, and the overall accuracies showed a small difference between the two models. Diagnostic keys matched with salient shapes, which were also easily recognized by human eyes, and were typified as blisters, horizontal and vertical stripes, lenticels of various shapes, and vertical crevices and clefts. The two models exhibited disparate quality in the diagnostic features: the old and less complex model showed more general and well-matching patterns, while the better-performing model with much deeper layers indicated local patterns less relevant to barks. CNNs were also capable of predicting untrained species by 41.98% and 48.67% within the correct genus and family, respectively. Our methodologies and findings are potentially applicable to identify and visualize crucial traits of other plant organs. © 2022, The Author(s).},
	keywords = {Algorithms; Humans; Neural Networks, Computer; Plant Bark; Trees; Vision, Ocular; algorithm; bark; human; tree; vision},
	correspondence_address = {H.S. Kim; Department of Agriculture, Forestry and Bioresources, Seoul National University, Seoul, 08826, South Korea; email: cameroncrazies@snu.ac.kr},
	publisher = {Nature Research},
	issn = {20452322},
	pmid = {35306532},
	language = {English},
	abbrev_source_title = {Sci. Rep.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Kansara2023417,
	author = {Kansara, Meera and Parikh, Ajay},
	title = {Unravel the Outlier Detection for Indian Ayurvedic Plant Organ Image Dataset},
	year = {2023},
	journal = {Lecture Notes in Networks and Systems},
	volume = {421},
	pages = {417 – 426},
	doi = {10.1007/978-981-19-1142-2_33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135059840&doi=10.1007%2f978-981-19-1142-2_33&partnerID=40&md5=3d8c2dd0ec783fa8310cb839ae844907},
	affiliations = {Gujarat Vidyapith, Ahmedabad, India},
	abstract = {Image-based outlier detection has been a fundamental research problem for machine learning and computer vision researchers. This paper unravels the outlier detection process for the data preparation framework of the Indian Ayurvedic plant organ image dataset. While creating dataset the outlier images might get introduce due to human or device errors. Identification and rectification of such outlier images are crucial part for creating clean dataset. This paper evaluated and compared four well-known and state-of-the-art outlier detection algorithms, namely Isolation Forest, Local Outlier Factor, Histogram-Based Outlier Score, and One-Class Support Vector Machine for detecting the outliers from the dataset of Indian Ayurvedic plant organ images. For this experiment dataset containing 690 images of “Centella asiatica” was used and augmented to generate more image samples. In total, 21 morphological, geometric, color, and texture features have been extracted from each plant organ image. The experiment shows the isolation forest giving superior results with 91% accuracy, at the same time Histogram-Based Outlier Score proves to be the fastest in execution time. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Image dataset; Image processing; Indian Ayurvedic plant identification; Machine learning; Outlier detection},
	correspondence_address = {M. Kansara; Gujarat Vidyapith, Ahmedabad, India; email: meeraj.kansara@gmail.com},
	editor = {Singh P.K. and Wierzchoń S.T. and Tanwar S. and Rodrigues J.J.P.C. and Rodrigues J.J.P.C. and Ganzha M.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {23673370},
	isbn = {978-981191141-5},
	language = {English},
	abbrev_source_title = {Lect. Notes Networks Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Conference on Computing, Communications, and Cyber-Security, IC4S 2021; Conference date: 30 October 2021 through 31 October 2021; Conference code: 280259}
}

@ARTICLE{Cui2023,
	author = {Cui, Zhelin and Li, Xinran and Li, Tao and Li, Mingyang},
	title = {Improvement and Assessment of Convolutional Neural Network for Tree Species Identification Based on Bark Characteristics},
	year = {2023},
	journal = {Forests},
	volume = {14},
	number = {7},
	doi = {10.3390/f14071292},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166215060&doi=10.3390%2ff14071292&partnerID=40&md5=cf8f3327c5cf370d16a77a8643d26c66},
	affiliations = {Co-Innovation Center for Sustainable Forestry in Southern China, Nanjing Forestry University, Nanjing, 210037, China},
	abstract = {Efficient tree species identification is of great importance in forest inventory and management. As the textural properties of tree barks vary less notably as a result of seasonal change than other tree organs, they are more suitable for the identification of tree species using deep learning models. In this study, we adopted the ConvNeXt convolutional neural network to identify 33 tree species using the BarkNetV2 dataset, compared the classification accuracy values of different tree species, and performed visual analysis of the network’s visual features. The results show the following trends: (1) the pre-trained network weights exhibit up to 97.61% classification accuracy for the test set, indicating that the network has high accuracy; (2) the classification accuracy values of more than half of the tree species can reach 98%, while the confidence level of correct identification (probability ratio of true labels) of tree species images is relatively high; and (3) there is a strong correlation between the network’s visual attractiveness and the tree bark’s biological characteristics, which share similarities with humans’ organization of tree species. The method suggested in this study has the potential to increase the efficiency of tree species identification in forest resources surveys and is of considerable value in forest management. © 2023 by the authors.},
	author_keywords = {bark image; convolutional neural network; tree species identification; visual attractiveness},
	keywords = {Accuracy; Characteristics; Classification; Forest Management; Forestry; Species Identification; Surveys; Trees; Classification (of information); Convolution; Deep learning; Forestry; Image enhancement; Bark image; Classification accuracy; Convolutional neural network; Forest inventory; Seasonal changes; Textural properties; Tree barks; Tree species; Tree species identifications; Visual attractiveness; Convolutional neural networks},
	correspondence_address = {M. Li; Co-Innovation Center for Sustainable Forestry in Southern China, Nanjing Forestry University, Nanjing, 210037, China; email: lmy196727@njfu.edu.cn},
	publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
	issn = {19994907},
	language = {English},
	abbrev_source_title = {Forests},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Valdez2023533,
	author = {Valdez, Daryl B. and Aliac, Chris Jordan G. and Feliscuzo, Larmie S.},
	title = {Detecting Medicinal Plants Using YOLOv5: A Mobile Vision Approach},
	year = {2023},
	journal = {6th International Conference on Inventive Computation Technologies, ICICT 2023 - Proceedings},
	pages = {533 – 538},
	doi = {10.1109/ICICT57646.2023.10134246},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163484767&doi=10.1109%2fICICT57646.2023.10134246&partnerID=40&md5=a729441fac44a02bada8bd6714eccf49},
	affiliations = {College of Computer Studies, Cebu Institute of Technology University, Cebu City, Philippines},
	abstract = {The use of medicinal plants for treating ailments has been a practice since ancient times. With the advent of computing and technology, the field of automated plant species identification has rapidly advanced. While there have been many studies on classifying plant species using leaves, the automatic identification using other plant organs remains unexplored. In this paper, we aim to investigate the precise and automated detection of flowers from medicinal plants in their natural environment. To achieve this, a new medicinal plant object detection dataset is presented comprising of four classes of medicinal plant species commonly found in the Philippines. Then, trained a YOLOv5-based model that detects medicinal plant flowers with a mean average precision of 83%. And, a mobile application was developed with the integrated model. Overall, the findings demonstrate the feasibility and acceptability of real-time automated detection and identification of medicinal plant species. This research has significant implications for the future of traditional medicine and the preservation of medicinal plant species. © 2023 IEEE.},
	author_keywords = {Deep Learning; Mobile App; Object Detection; Plant Identification; Plants Dataset},
	keywords = {Automation; Deep learning; E-learning; Object recognition; Plants (botany); Automated detection; Deep learning; Medicinal plants; Mobile app; Mobile vision; Objects detection; Plant dataset; Plant identification; Plant species; Plant species identification; Object detection},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835039849-6},
	language = {English},
	abbrev_source_title = {Int. Conf. Inven. Comput. Technol., ICICT - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 6th International Conference on Inventive Computation Technologies, ICICT 2023; Conference date: 26 April 2023 through 28 April 2023; Conference code: 189117}
}

@CONFERENCE{Pushpa2023,
	author = {Pushpa, B.R. and Srinag, R. and Shobha Rani, N.},
	title = {Classification of Plant Species based Seedlings and Weedlings in Low Lightening Conditions using Deep Convolution Neural Network},
	year = {2023},
	journal = {2023 4th International Conference for Emerging Technology, INCET 2023},
	doi = {10.1109/INCET57972.2023.10170644},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166380832&doi=10.1109%2fINCET57972.2023.10170644&partnerID=40&md5=bcf1acbec39188eae8870e25182903c1},
	affiliations = {Amrita Vishwa Vidyapeetham, School of Computing Mysuru Campus, Dept. of Computer Science, India},
	abstract = {Smart farming techniques involve the use of plant identification and classification. Deep learning can be particularly useful for classifying low-light images because it can impulsively learn features from the data that can be relevant for classification. This is especially important in low light conditions where the image may be noisy or contain artefacts that are not relevant to the task. In the experiment, the plant seedlings and weedlings dataset consisting of low light images are subjected to a deep-learning model. Low-light images tend to have poor image quality due to the limited amount of available light. This results in a very low signal-to-noise ratio, making extracting beneficial information from the images extremely ambiguous. In the proposed work, a deep learning XceptionNet model is utilized to perform classification of plants using seedlings and weedlings that provides performance yielding an accuracy of 94.13% with 25 epochs. © 2023 IEEE.},
	author_keywords = {agriculture; classification; deep learning; segmentation; weeds},
	keywords = {Classification (of information); Deep learning; Seed; Condition; Convolution neural network; Deep learning; Learn+; Low-light images; Plant classification; Plant identification; Plant species; Segmentation; Weed; Signal to noise ratio},
	correspondence_address = {B.R. Pushpa; Amrita Vishwa Vidyapeetham, School of Computing Mysuru Campus, Dept. of Computer Science, India; email: preeths1@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835033575-0},
	language = {English},
	abbrev_source_title = {Int. Conf. Emerg. Technol., INCET},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference for Emerging Technology, INCET 2023; Conference date: 26 May 2023 through 28 May 2023; Conference code: 190624}
}

@ARTICLE{He2022,
	author = {He, Jie and Sun, Yongke and Yu, Chunjiang and Cao, Yong and Zhao, Youjie and Du, Guanben},
	title = {An Improved Wood Recognition Method Based on the One-Class Algorithm},
	year = {2022},
	journal = {Forests},
	volume = {13},
	number = {9},
	doi = {10.3390/f13091350},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138492458&doi=10.3390%2ff13091350&partnerID=40&md5=1d6dc3ed4a9f5cc4b36bf151eef64b6a},
	affiliations = {School of Big Data and Intelligent Engineering, Southwest Forestry University, Kunming, 650224, China; International Engineering and Technology Institute, Hong Kong; Yunnan Provincial Key Laboratory of Wood Adhesives and Glued Products, Southwest Forestry University, Kunming, 650224, China},
	abstract = {Wood recognition is necessary for work in the wood trade activities. The advantage of the one-class wood classification method is more generalization, and it only needs positive samples and does not need negative samples in the training phase, so it is suitable for rare wood species inspection. This paper proposed an improved method based on the one-class support vector machine (OCSVM) for wood species recognition. It uses cross-section images acquired with a magnifying glass, which uses a pre-trained VGG16 model for feature extraction, a normal distribution test for key features filtering, and OCSVM to determine the wood species. The results showed that the approach achieved a mean recall of 0.842 for both positive and negative samples, which indicates this method has good performance for wood recognition. In a negative public dataset, the negative recall reached as high as 0.989, which showed that this method has good generalization. © 2022 by the authors.},
	author_keywords = {one-class classification; transfer learning; wood recognition},
	keywords = {Classification; Commerce; Glass; Inspection; Samples; Test Methods; Wood Species; Normal distribution; Wood; Classification methods; Generalisation; Negative samples; One-class Classification; Recognition methods; Support vectors machine; Training phasis; Transfer learning; Wood recognition; Wood trade; algorithm; classification; data set; performance assessment; wood quality; Support vector machines},
	correspondence_address = {Y. Sun; School of Big Data and Intelligent Engineering, Southwest Forestry University, Kunming, 650224, China; email: sunyongke@swfu.edu.cn; Y. Cao; International Engineering and Technology Institute, Hong Kong; email: cn_caoyong@126.com},
	publisher = {MDPI},
	issn = {19994907},
	language = {English},
	abbrev_source_title = {Forests},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Mulla2023278,
	author = {Mulla, Rais Allauddin and Pawar, Mahendra Eknath and Banait, Satish S. and Ajani, Samir N. and Borawake, Madhuri Pravin and Hundekari, Sheela},
	title = {Design and Implementation of Deep Learning Method for Disease Identification in Plant Leaf},
	year = {2023},
	journal = {International Journal on Recent and Innovation Trends in Computing and Communication},
	volume = {11},
	pages = {278 – 285},
	doi = {10.17762/ijritcc.v11i2s.6147},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160041055&doi=10.17762%2fijritcc.v11i2s.6147&partnerID=40&md5=f3b802beed49fe8601649cd1e0ba15fa},
	affiliations = {Vasantdada Patil pratishthan's college of engineering Sion, India; Department of Computer Engineering, K.K. Wagh Institute of Engineering Education and Research, Nashik. SPPU-Pune, India; Department of Computer Engineering, St. Vincent Pallotti College of Engineering and Technology, Maharashtra, Nagpur, India; Department of Computer Engineering, Pune District Education Association's College Of Engineering,, Manjari (Bk"), Maharashtra, Pune, 412307, India; MIT ADT University, Loni kalbhor, Pune, India},
	abstract = {In the whole agriculture plays a very important in country’s economic condition specially in Indian agriculture has a crucial role for raising the Indian economic structure and its level. India’s frequent changing climatic situation, various bacterial disease is much normal that drastically decreases the productivity of crop productivity. Most of the researcher is moving towards into this topic to find the early detection technique to identify the disease in small green leaves plants. A single, micro bacterial infectious disease can destroy all the agricultural small green leaves plants get damaged overnight and hence must be prevented and cured as earliest as possible so that agriculture production. In this research work, we had tried to developed a green small green leaves plants bacterial disease early detection system based on the deep learning network system which will detect the disease at very earlier state of symptoms observed. Deep learning technique is has various algorithms to detect the earliest stage of any of the procedural processing of any bacterial infections or disease. This paper consists of investigations and analysis of latest deep learning techniques. Initially we will explore the deep learning architecture, its various source of data and different types of image processing method that can be used for processing the images captured of leaf for data processing. Different DL architectures with various data visualization’s tools has recently developed to determine symptoms and classifications of different type of plant-based disease. We had observed some issue that was un identified in previous research work during our literature survey and their technique to resolve that issue in order to handle the functional auto-detection system for identifying the certain plant disease in the field where massive growth of green small green leaves plants production is mostly done. Recently various enhancement has been done in techniques in CNN (convolution neural network) that generates much accurate images classification of any object. Our research work is based on deep learning network that will observe and identifies the symptoms generated in leaflet of plant and identifies the type of bacterial infection in progress in that with the help of plant classification stated in the plant dataset. Our research work represents the implementation DCGAN and Hybrid Net Model using Deep learning algorithm for early-stage identification of green plant leaves disease in various environmental condition. Our result obtained shows that it has DCGAN accuracy 96.90% when compared withHybrid Net model disease detection methodologies. © 2023 Auricle Global Society of Education and Research. All rights reserved.},
	author_keywords = {Early stage-disease; Image Processing; Machine Learning Model},
	publisher = {Auricle Global Society of Education and Research},
	issn = {23218169},
	language = {English},
	abbrev_source_title = {Int. J. Recent. Innov. Trend. Comput. Commun.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Bronze Open Access}
}

@ARTICLE{Beikmohammadi2022,
	author = {Beikmohammadi, Ali and Faez, Karim and Motallebi, Ali},
	title = {SWP-LeafNET: A novel multistage approach for plant leaf identification based on deep CNN},
	year = {2022},
	journal = {Expert Systems with Applications},
	volume = {202},
	doi = {10.1016/j.eswa.2022.117470},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129864654&doi=10.1016%2fj.eswa.2022.117470&partnerID=40&md5=0e4f5fa2833e91629d56c393e828cb7e},
	affiliations = {Department of Computer and Systems Sciences, Stockholm University, Stockholm, Sweden; Department of Electrical Engineering, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran},
	abstract = {Modern scientific and technological advances allow botanists to use computer vision-based approaches for plant identification tasks. These approaches have their own challenges. Leaf classification is a computer-vision task performed for the automated identification of plant species, a serious challenge due to variations in leaf morphology, including its size, texture, shape, and venation. Researchers have recently become more inclined toward deep learning-based methods rather than conventional feature-based methods due to the popularity and successful implementation of deep learning methods in image analysis, object recognition, and speech recognition. In this paper, to have an interpretable and reliable system, a botanist's behavior is modeled in leaf identification by proposing a highly-efficient method of maximum behavioral resemblance developed through three deep learning-based models. Different layers of the three models are visualized to ensure that the botanist's behavior is modeled accurately. The first and second models are designed from scratch. Regarding the third model, the pre-trained architecture MobileNetV2 is employed along with the transfer-learning technique. The proposed method is evaluated on two well-known datasets: Flavia and MalayaKew. According to a comparative analysis, the suggested approach is more accurate than hand-crafted feature extraction methods and other deep learning techniques in terms of 99.67% and 99.81% accuracy. Unlike conventional techniques that have their own specific complexities and depend on datasets, the proposed method requires no hand-crafted feature extraction. Also, it increases accuracy as compared with other deep learning techniques. Moreover, SWP-LeafNET is distributable and considerably faster than other methods because of using shallower models with fewer parameters asynchronously. © 2022 The Author(s)},
	author_keywords = {Convolutional neural network; Deep learning; Plant leaf recognition; SWP-LeafNET},
	keywords = {Convolutional neural networks; Deep learning; Extraction; Feature extraction; Object recognition; Plants (botany); Speech recognition; Textures; Convolutional neural network; Deep learning; Leaf identification; Leaf recognition; Learning techniques; Multistage approach; Plant leaf recognition; Plant leaves; Scientific advances; SWP-LeafNET; Computer vision},
	correspondence_address = {A. Beikmohammadi; Department of Computer and Systems Sciences, Stockholm University, Stockholm, Sweden; email: beikmohammadi@dsv.su.se},
	publisher = {Elsevier Ltd},
	issn = {09574174},
	coden = {ESAPE},
	language = {English},
	abbrev_source_title = {Expert Sys Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Cao2023,
	author = {Cao, Jie and Wu, Zhengmin and Zhang, Xuechen and Luo, Kun and Zhao, Bo and Sun, Changying},
	title = {Sorting of Fresh Tea Leaf Using Deep Learning and Air Blowing},
	year = {2023},
	journal = {Applied Sciences (Switzerland)},
	volume = {13},
	number = {6},
	doi = {10.3390/app13063551},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152013167&doi=10.3390%2fapp13063551&partnerID=40&md5=11bb818d9854304b982ea3899e730a74},
	affiliations = {School of Tea and Food Science & Technology, Anhui Agricultural University, Hefei, 230036, China; State Key Laboratory of Tea Plant Biology and Utilization, Hefei, 230036, China; School of Engineering, Anhui Agricultural University, Hefei, 230036, China},
	abstract = {The sorting of machine-picked fresh tea leaves after mechanized harvesting remains a challenge because of the complex morphological characteristics and physicochemical properties of fresh tea leaves. First, the recognition results of four types of models, namely, YOLOv5, YOLOv3, Fast RCNN, and SSD, were compared. It was found that YOLOv5, with guaranteed recognition accuracy, had a recognition speed of 4.7 ms/frame (about four times that of the second ranked YOLOv3). Therefore, this study presents a novel fresh tea leaf sorting system that provides rapid and high-precision multi-channel sorting for four grades of tea leaves using a tea leaf recognition model based on the You Only Look Once (YOLOv5) deep learning model. Subsequently, a raw dataset, consisting of 6400 target images of different grades and different moisture contents, was used to evaluate three different optimization methods. Among these, the Stochastic Gradient Descent (SGD) optimization method was found to provide the best model training results with an average recognition accuracy of 98.2%. In addition, the recognition efficacy of the recognition model was found to be positively correlated with the gradient coverage of tea’s moisture content in the training set. Theoretical analysis was then conducted, along with the experimental investigation of the air-blowing force on the fresh tea leaves in the sorting process, with 30° determined to be the optimal air-blowing angle. Finally, the overall results showed that the construction of the full moisture content training set enabled a model recognition accuracy of up to 88.8%, a recall of 88.4%, a recognition speed of 4.7 ms/frame, and an overall sorting accuracy of 85.4%. This result is promising for multi-channel sorting of fresh tea leaf grades in complex situations, and as such provides a strong basis for the application of tea leaf sorting equipment. © 2023 by the authors.},
	author_keywords = {air blowing; moisture content; sorting; tea; YOLOv5s},
	correspondence_address = {Z. Wu; School of Tea and Food Science & Technology, Anhui Agricultural University, Hefei, 230036, China; email: wzmin@ahau.edu.cn},
	publisher = {MDPI},
	issn = {20763417},
	language = {English},
	abbrev_source_title = {Appl. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Ríos-Toledo2023114,
	author = {Ríos-Toledo, German and Pérez-Patricio, Madaín and Cundapí-López, Luis Ángel and Camas-Anzueto, J.L. and Morales-Navarro, N.A. and Osuna-Coutiño, J. A. de Jesús},
	title = {Plant Stress Recognition Using Deep Learning and 3D Reconstruction},
	year = {2023},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {13902 LNCS},
	pages = {114 – 124},
	doi = {10.1007/978-3-031-33783-3_11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164270432&doi=10.1007%2f978-3-031-33783-3_11&partnerID=40&md5=d242497da84ff08b64c187a40fced770},
	affiliations = {Instituto Tecnológico de Tuxtla Gutiérrez (ITTG), Tuxtla Gutiérrez, Mexico},
	abstract = {Plant stress recognition consists of Identification, Classification, Quantification, and Prediction (ICQP) in crop stress. There are several approaches to plant stress identification. However, most of these approaches are based on the use of expert employees or invasive techniques. In general, expert employees have a good performance on different plants, but this alternative requires sufficient staff in order to guarantee quality crops. On the other hand, invasive techniques need the dismemberment of the leaves. To address this problem, an alternative is to process an image seeking to interpret patterns of the images where the plant geometry may be observed, thus removing the qualified labor dependency or the crop dismemberment, but adding the challenge of having to interpret images ambiguities correctly. Motivated by the latter, we propose a new approach for plant stress recognition using deep learning and 3D reconstruction. This strategy combines the abstraction power of deep learning and the visual patterns of plant geometry. For that, our methodology has three steps. First, the plant recognition step provides the segmentation, location, and delimitation of the crop. Second, we propose a leaf detection analysis to classify and locate the boundaries between the different leaves. Finally, we use a depth sensor and the pinhole camera model to extract a 3D reconstruction. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author_keywords = {Deep Learning; Plant Stress Recognition; Visual Pattern},
	keywords = {3D modeling; Crops; Deep learning; Pattern recognition; Personnel; Pinhole cameras; Plants (botany); Three dimensional computer graphics; 3D reconstruction; Deep learning; Invasive techniques; New approaches; Performance; Plant stress; Plant stress recognition; Power; Stress recognition; Visual pattern; Image reconstruction},
	correspondence_address = {J.A.J. Osuna-Coutiño; Instituto Tecnológico de Tuxtla Gutiérrez (ITTG), Tuxtla Gutiérrez, Mexico; email: juan.oc@tuxtla.tecnm.mx},
	editor = {Rodríguez-González A.Y. and Pérez-Espinosa H. and Martínez-Trinidad J.F. and Carrasco-Ochoa J.A. and Olvera-López J.A.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303133782-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 15th Mexican Conference on Pattern Recognition, MCPR 2023; Conference date: 21 June 2023 through 24 June 2023; Conference code: 296369}
}

@CONFERENCE{Indira20233438,
	author = {Indira, D.N.V.S.L.S. and Goddu, Jyothi and Indraja, Baisani and Challa, Vijaya Madhavi Lakshmi and Manasa, Bezawada},
	title = {A review on fruit recognition and feature evaluation using CNN},
	year = {2023},
	journal = {Materials Today: Proceedings},
	volume = {80},
	pages = {3438 – 3443},
	doi = {10.1016/j.matpr.2021.07.267},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153573898&doi=10.1016%2fj.matpr.2021.07.267&partnerID=40&md5=5f5d5ba24da5a4b5f6cc6da2978722e3},
	affiliations = {Associate Professor, Department of Information Technology, Gudlavalleru Engineering College, Gudlavalleru, India; Assistant Professor, IT Department, Vignan's institute of information technology, Visakhapatnam, India; Assistant Professor, IT Department, Vallurupalli Nageswara Rao Vignana Jyothi Institute of Engineering &Technology, Hyderabad, India; Assistant Professor, Department of CSE, RVR & JC College of Engineering, India},
	abstract = {The first thing addressed in this paper is Plant fruit recognition and its feature extraction, it plays vital role in Agriculture. The point is to construct an exact, quick and solid framework utilizing CNN realities. Automatic fruit detection may reduce human efforts (counting number of fruits and manual identification). The second thing addressed in this paper is plant classification to avoid similarities in many plant fruits. Fruit classification may help fruit sellers to identify and to differentiate various kind of fruits having same similarities. The proposed framework has applied convolutional Neural Net (CNN) to the undertakings of distinguishing natural fruit pictures. In any case, deep learning has been shown as of late to be an extremely incredible picture identification procedure, and CNN is a best in class way to deal with deep learning. © 2021},
	author_keywords = {Convolutional Neural Network; Feature Evaluation; Fruit; Plant; Recognition},
	keywords = {Convolution; Convolutional neural networks; Deep learning; Convolutional neural network; Feature evaluation; Features extraction; Fruit recognition; Identification procedure; Manual identification; Picture identification; Plant; Plant classification; Recognition; Fruits},
	correspondence_address = {D.N.V.S.L.S. Indira; Associate Professor, Department of Information Technology, Gudlavalleru Engineering College, Gudlavalleru, India; email: indiragamini@gmail.com},
	publisher = {Elsevier Ltd},
	issn = {22147853},
	language = {English},
	abbrev_source_title = {Mater. Today Proc.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Yuan2023,
	author = {Yuan, Jiayu and Wu, Zhiwei and Li, Shun and Kang, Ping and Zhu, Shihao},
	title = {Multi-Feature-Based Identification of Subtropical Evergreen Tree Species Using Gaofen-2 Imagery and Algorithm Comparison},
	year = {2023},
	journal = {Forests},
	volume = {14},
	number = {2},
	doi = {10.3390/f14020292},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149031134&doi=10.3390%2ff14020292&partnerID=40&md5=1612d884751676a13ff3d0f8976b6f78},
	affiliations = {Key Laboratory of Poyang Lake Wetland and Watershed Research, Ministry of Education, Jiangxi Normal University, Nanchang, 330022, China; Key Laboratory of Natural Disaster Monitoring, Early Warning and Assessment of Jiangxi Province, Jiangxi Normal University, Nanchang, 330022, China; School of Geography and Environment, Jiangxi Normal University, Nanchang, 330022, China},
	abstract = {The species and distribution of trees in a forest are critical to the understanding of forest ecosystem processes and the development of forest management strategies. Subtropical forest landscapes feature a complex canopy structure and high stand density. Studies on the effects of classification algorithms on the remote sensing-based identification of tree species are few. GF-2 is the first satellite in China with sub-meter accuracy which has the high resolution and short replay cycle. Here, we considered three representative tree types (Masson pine, Chinese fir, and broadleaved evergreen trees) in the southern subtropical evergreen broadleaved forest region of China as research objects. We quantitatively compared the effects of five machine learning algorithms, including the backpropagation neural network, k-nearest neighbour, polytomous logistic regression, random forest (RF) and support vector machine (SVM), and four features (vegetation index, band reflectance, textural features, and topographic factors) on tree species identification using Gaofen-2 panchromatic and multispectral remote sensing images and field survey data. All five classification algorithms could effectively identify major tree species in subtropical forest areas (overall accuracy [OA] > 87.40%, kappa coefficient > 81.08%). The SVM model exhibited the best identification ability (OA = 90.27%, kappa coefficient = 85.37%), followed by RF (OA = 88.90%, Kappa coefficient = 83.30%). The combination of band reflectance, vegetation index, and the topographic factor performed exhibited the best, followed by the combination of band reflectance, vegetation index, textural feature, and topographic factor. In addition, we find that the classifier constructed by a single feature is not as effective as the combination of multiple feature factors. The addition of topographic factors can significantly improve the ability of tree species identification. According to the results of the five classifiers, the separability of the three tree species was good. The producer’s accuracy and user’s accuracy of Masson pine were more than 90%, and the evergreen broad-leaved tree and Chinese fir were more than 80%. The commission errors and omission errors of the three tree species were evergreen broadleaved tree > Chinese fir > Masson pine. The variable importance assessment results showed that the normalized difference greenness index, altitude, and the modified soil-adjusted vegetation index were the key variables. The results of this study used GF-2 to accurately identify the main tree species of subtropical evergreen forests in China, which can help forest managers to regularly monitor tree species composition and provide theoretical support for forest managers to formulate policies, monitor sustainable plans for wood mining, and forest conservation and management measures. © 2023 by the authors.},
	author_keywords = {machine learning; multi-feature combination; remote sensing identification; subtropical forest; tree species identification},
	keywords = {Ecosystems; Forestry; Neural Networks; Reflection; Remote Sensing; Tropics; China; Classification (of information); Ecosystems; Forestry; Learning algorithms; Linear regression; Nearest neighbor search; Neural networks; Reflection; Support vector machines; Tropics; Vegetation mapping; Feature combination; Machine-learning; Multi-feature combination; Multifeatures; Remote sensing identification; Remote-sensing; Subtropical forests; Tree species; Tree species identifications; Vegetation index; algorithm; comparative study; evergreen tree; machine learning; remote sensing; satellite imagery; subtropical region; Remote sensing},
	correspondence_address = {Z. Wu; Key Laboratory of Poyang Lake Wetland and Watershed Research, Ministry of Education, Jiangxi Normal University, Nanchang, 330022, China; email: wuzhiwei@jxnu.edu.cn},
	publisher = {MDPI},
	issn = {19994907},
	language = {English},
	abbrev_source_title = {Forests},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Meghana2023,
	author = {Meghana, M. and Radhika, S. and Sheeja Kumari, V.},
	title = {Anomaly Detection for Vertical Plant Wall System using Novel Support Vector Machine in Comparison with Artificial Neural Network for improving accuracy},
	year = {2023},
	journal = {Proceedings of 8th IEEE International Conference on Science, Technology, Engineering and Mathematics, ICONSTEM 2023},
	doi = {10.1109/ICONSTEM56934.2023.10142657},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163148093&doi=10.1109%2fICONSTEM56934.2023.10142657&partnerID=40&md5=6416550936f7943a5e670fb5df18aa48},
	affiliations = {Saveetha University, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Department of Computer Science and Engineering, Tamilnadu, Chennai, 602105, India},
	abstract = {Aim: New Support Vector Machine and Artificial Neural Network are used to construct a plant wall system for indoor climate management. Materials and Methods: To detect indoor climatic changes in vertical plant wall systems, a Machine Learning Method that compares Artificial Neural Network and Innovative Support Vector Machine was created. The G Power calculator yielded 534 samples. 1068 samples and 80% Pretest Power. Results: Innovative Support vector machine had the highest accuracy in indoor changes at 68.42% and the lowest mean error at 58.58%. Groups are statistically insignificant. Conclusion: New Support Vector Machine technique outperforms Artificial Neural Network in indoor control.  © 2023 IEEE.},
	author_keywords = {Anomaly Detection; Artificial Neural Network; Indoor Climate Control; Machine Learning; Novel Support Vector Machine; Plant Recognition System},
	keywords = {Anomaly detection; Learning systems; Neural networks; Vectors; Anomaly detection; Indoor climate control; Machine-learning; Novel support vector machine; Plant recognition; Plant recognition system; Power; Recognition systems; Support vectors machine; Wall systems; Support vector machines},
	correspondence_address = {M. Meghana; Saveetha University, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Department of Computer Science and Engineering, Chennai, Tamilnadu, 602105, India; email: meghanam18@saveetha.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835034779-1},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Sci., Technol., Eng. Math., ICONSTEM},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 8th IEEE International Conference on Science, Technology, Engineering and Mathematics, ICONSTEM 2023; Conference date: 6 April 2023 through 7 April 2023; Conference code: 189244}
}

@ARTICLE{Liu2023,
	author = {Liu, Huaipeng},
	title = {Classification of tree species using UAV-based multi-spectral and multi-seasonal images: a multi-feature-based approach},
	year = {2023},
	journal = {New Forests},
	doi = {10.1007/s11056-023-09974-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151479174&doi=10.1007%2fs11056-023-09974-w&partnerID=40&md5=c7a8871e0acc6c8570f457e6e954839e},
	affiliations = {School of Land and Tourism, Luoyang Normal University, Henan Province, Luoyang, 471934, China},
	abstract = {Exploration of the effectiveness of multi-type features and multi-seasonal data of remote sensing images and selection of an optimal feature set from all extracted features are popular research topics in tree species classification. Eight typical image feature sets, namely, spectral band, digital surface model (DSM), texture (TEX), tassel cap transformation (TC), hue, saturation and value colour space (HSV), principal component analysis, minimum noise fraction (MNF) and spectral index (SI), were extracted in this study from images of four seasons acquired using the RedEdge-MX sensor, and maximum likelihood and random forest classifiers were used to categorise 32 typical urban tree species. Experimental results revealed the following: (1) the tree species recognition accuracy determined using the texture set (87.89%) was higher than that determined using other types of feature sets; (2) the optimal feature set containing 20 features comprised 4 DSMs, 11 TEXs, 2 TCs, 1 HSV (S), 1 SI and 1 MNF, and the classification accuracy determined using the set of features was 89.53% and (3) the classification accuracy for tree species identification determined using multi-seasonal spectral data was higher than that determined using individual seasonal data. The major contribution of this study to relevant literature is that it proves that urban greening tree species can be accurately identified using multiple features and seasonal images acquired through UAV-based sensors. The multi-feature-based approach also performs substantially well in practical applications for mapping tree species in a general urban environment considering the effects of a heterogeneous environment on tree species classification and comprehensive image processing and classification methods. © 2023, The Author(s), under exclusive licence to Springer Nature B.V.},
	author_keywords = {Driving factors; Maximum likelihood classification; Multi-season images multi-type features; Random forest; RedEdge-MX data; Tree species recognition},
	keywords = {Classification (of information); Data mining; Forestry; Image acquisition; Image classification; Maximum likelihood; Principal component analysis; Remote sensing; Unmanned aerial vehicles (UAV); Driving factors; Feature based approaches; Maximum-likelihood classification; Multi-season image multi-type feature; Multifeatures; Random forests; Rededge-MX data; Species recognition; Tree species; Tree species recognition; Textures},
	correspondence_address = {H. Liu; School of Land and Tourism, Luoyang Normal University, Luoyang, Henan Province, 471934, China; email: gatestudy@163.com},
	publisher = {Springer Science and Business Media B.V.},
	issn = {01694286},
	coden = {NEFOE},
	language = {English},
	abbrev_source_title = {New For.},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Joly2023568,
	author = {Joly, Alexis and Goëau, Hervé and Kahl, Stefan and Picek, Lukáš and Botella, Christophe and Marcos, Diego and Šulc, Milan and Hrúz, Marek and Lorieul, Titouan and Moussi, Sara Si and Servajean, Maximilien and Kellenberger, Benjamin and Cole, Elijah and Durso, Andrew and Glotin, Hervé and Planqué, Robert and Vellinga, Willem-Pier and Klinck, Holger and Denton, Tom and Eggel, Ivan and Bonnet, Pierre and Müller, Henning},
	title = {LifeCLEF 2023 Teaser: Species Identification and Prediction Challenges},
	year = {2023},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {13982 LNCS},
	pages = {568 – 576},
	doi = {10.1007/978-3-031-28241-6_65},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151051641&doi=10.1007%2f978-3-031-28241-6_65&partnerID=40&md5=3b3a5d764f7e36843e8299cc968d057a},
	affiliations = {Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France; CIRAD, UMR AMAP, Occitanie, Montpellier, France; Univ. Toulon, Aix Marseille Univ., CNRS, LIS, DYNI Team, Marseille, France; Xeno-canto Foundation, The Hague, Netherlands; HES-SO, Sierre, Switzerland; KLYCCB, Cornell Lab of Ornithology, Cornell University, Ithaca, United States; LIRMM, AMI, Univ Paul Valéry Montpellier, Univ Montpellier, CNRS, Montpellier, France; Department of Computing and Mathematical Sciences, Caltech, Pasadena, United States; Department of Cybernetics, FAV, University of West Bohemia, Pilsen, Czech Republic; Department of Biological Sciences, Florida Gulf Coast University, Fort Myers, United States; Google LLC, San Francisco, United States; Rossum.ai, Prague, Czech Republic; Listening Observatory for Hawaiian Ecosystems, Univ. of Hawai’i at Hilo, Hilo, United States; Centre for Invasion Biology, Stellenbosch University, Stellenbosch, South Africa; Department of Ecology and Evolutionary Biology, Yale University, New Haven, United States; Inria, TETIS, Univ Montpellier, Montpellier, France; Mansfield, United States},
	abstract = {Building accurate knowledge of the identity, the geographic distribution and the evolution of species is essential for the sustainable development of humanity, as well as for biodiversity conservation. However, the difficulty of identifying plants, animals and fungi is hindering the aggregation of new data and knowledge. Identifying and naming living organisms is almost impossible for the general public and is often difficult, even for professionals and naturalists. Bridging this gap is a key step towards enabling effective biodiversity monitoring systems. The LifeCLEF campaign, presented in this paper, has been promoting and evaluating advances in this domain since 2011. The 2023 edition proposes five data-oriented challenges related to the identification and prediction of biodiversity: (i) PlantCLEF: very large-scale plant identification from images, (ii) BirdCLEF: bird species recognition in audio soundscapes, (iii) GeoLifeCLEF: remote sensing based prediction of species, (iv) SnakeCLEF: snake recognition in medically important scenarios, and (v) FungiCLEF: fungi recognition beyond 0–1 cost. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author_keywords = {AI; Biodiversity; Bird identification; Fungi identification; Machine learning; Plant identification; Snake identification; Species distribution model; Species identification; Species prediction},
	keywords = {Biodiversity; Birds; Conservation; Fungi; Geographical distribution; Machine learning; Remote sensing; Biodiversity conservation; Bird identification; Fungus identification; Geographics; Machine-learning; Plant identification; Snake identifications; Species distribution modeling; Species identification; Species prediction; Forecasting},
	correspondence_address = {A. Joly; Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France; email: alexis.joly@inria.fr},
	editor = {Kamps J. and Goeuriot L. and Crestani F. and Maistro M. and Joho H. and Davis B. and Gurrin C. and Caputo A. and Kruschwitz U.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303128240-9},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 45th European Conference on Information Retrieval, ECIR 2023; Conference date: 2 April 2023 through 6 April 2023; Conference code: 292029}
}

@CONFERENCE{Sarkar20236,
	author = {Sarkar, Sayani and Kelley, Robert},
	title = {A UAV and Deep Transfer Learning Based Environmental Monitoring: Application to Native and Invasive Species classification in Southern regions of the USA},
	year = {2023},
	journal = {2023 IEEE Conference on Technologies for Sustainability, SusTech 2023},
	pages = {6 – 11},
	doi = {10.1109/SusTech57309.2023.10129545},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161917154&doi=10.1109%2fSusTech57309.2023.10129545&partnerID=40&md5=c1ca315f6712ee6668f348e45c737354},
	affiliations = {Bellarmine University, Computer Science Department, Louisville, KY, United States},
	abstract = {Unmanned aerial vehicle (UAV) applications can be powerful tools in horticultural research. However, they have not yet been widely explored in the literature. One significant area of interest for the horticultural community is identifying invasive plant species that, if left unchecked, can hinder the growth and health of native plant species. To address this issue, we assembled a novel data set of invasive and native plant species for seven southern states in the United States and developed a plant classification technique using pre-trained convolution neural networks and transfer learning. We explored extracting features from our data set using several state-of-the-art deep convolution neural network models, including InceptionV 3, MobileNetV 2, ResNetV 2, VGG16, and Xception. We then used the extracted features to classify the plant species using a convolutional neural network with cross-validation. Our experiments demonstrated the potential of our proposed method for achieving performance with a 94% accuracy using the MobileNetV2-DCNN model with data augmentation, hyper-parameter optimization, and the softmax classification technique. The advantage of our approach is the learning process is automated and highly accurate. The data set to train the plant species classifier will be available on request.  © 2023 IEEE.},
	author_keywords = {Deep Learning; Drone; Feature Extraction; Image Classification; Invasive plant species; Native plant species; Transfer learning; UAV},
	keywords = {Antennas; Classification (of information); Convolution; Convolutional neural networks; Data mining; Deep learning; Image classification; Learning systems; Transfer learning; Aerial vehicle; Deep learning; Features extraction; Images classification; Invasive plant species; Invasive plants; Native plant species; Native plants; Plant species; Transfer learning; Unmanned aerial vehicle; Drones},
	correspondence_address = {S. Sarkar; Bellarmine University, Computer Science Department, Louisville, United States; email: ssarkar@bellarmine.edu},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835034548-3},
	language = {English},
	abbrev_source_title = {IEEE Conf. Technol. Sustain., SusTech},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2023 IEEE Conference on Technologies for Sustainability, SusTech 2023; Conference date: 19 April 2023 through 22 April 2023; Conference code: 188972}
}

@ARTICLE{Ghosh202315,
	author = {Ghosh, Sukanta and Singh, Amar and Kumar, Shakti},
	title = {PB3C-CNN: An integrated Parallel Big Bang-Big Crunch and CNN based approach for plant leaf classification},
	year = {2023},
	journal = {Inteligencia Artificial},
	volume = {26},
	number = {72},
	pages = {15 – 29},
	doi = {10.4114/intartif.vol26iss72pp15-29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160697368&doi=10.4114%2fintartif.vol26iss72pp15-29&partnerID=40&md5=364ab161ec2ed3b49b12d61466022c08},
	affiliations = {School of Computer Application, Lovely Professional University, Punjab, Phagwara, India; Panipat Institute of Engineering and Technology, Haryana, Panipat, India},
	abstract = {Plant identification and classification are critical to understand, protect, and conserve biodiversity. Traditional plant classification requires years of intensive training and experience, making it difficult for others to classify plants. Plant leaf classification is a challenging issue as similar features appear in different plant species. With the development of automated image-based classification, machine learning (ML) is becoming very popular. Deep learning (DL) methods have significantly improved plant image identification and classification. In the last decade, convolutional neural networks (CNN) have entirely dominated the field of computer vision, showing outstanding feature extraction capabilities and significant identification and classification performance. The capability of CNN lies in its network. The primary strategy to continue this trend in the literature relies on further scaling networks in size. However due to increase in network size, costs increase rapidly, while performance improvements may be marginal. Hence, there is a need to optimize the CNN network to get the desired result with optimal size of machine learning model. This paper proposes a parallel big bang-big crunch (PB3C) based approach to automatically evolve the architecture of CNN. The proposed approach is validated on plant leaf classification application and compared with other existing machine learning-based approaches. From the comparision results we observed that the obtained it was found that the proposed approach was able to outperforms all the 11 existing state-of-the-art techniques. © IBERAMIA and the authors.},
	author_keywords = {Deep Learning; Image Classification; Machine Learning; Nature-Inspired Computing; Plant Protection},
	keywords = {Biodiversity; Deep learning; Image enhancement; Neural networks; Big Bang; Big Crunch; Convolutional neural network; Deep learning; Images classification; Machine-learning; Nature inspired computing; Plant classification; Plant leaf classifications; Plant protection; Image classification},
	correspondence_address = {A. Singh; School of Computer Application, Lovely Professional University, Phagwara, Punjab, India; email: amar.23318@lpu.co.in},
	publisher = {Asociacion Espanola de Inteligencia Artificial},
	issn = {11373601},
	language = {English},
	abbrev_source_title = {Inteligencia Artif.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Athapaththu2023102,
	author = {Athapaththu, S.W. and Piumi Ishanka, U.A.},
	title = {Plant Leaf Recognition Using Texture, Colour, and Vein Density Features},
	year = {2023},
	journal = {ICARC 2023 - 3rd International Conference on Advanced Research in Computing: Digital Transformation for Sustainable Development},
	pages = {102 – 107},
	doi = {10.1109/ICARC57651.2023.10145733},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163482575&doi=10.1109%2fICARC57651.2023.10145733&partnerID=40&md5=2282e6516aef06539eba944ac6d2031b},
	affiliations = {University of Sri Lanka, Faculty of Computing Sabaragamuwa, Department of Computing and Information Systems, Belihuloya, Sri Lanka},
	abstract = {Plants are essential in our ecosystem. There is various kind of plants in our ecosystem. Hence recognizing and categorizing them is a crucial task for humankind. In a nutshell, the proposed study discusses how to identify plant species using image processing techniques, the proper feature extraction of plant species recognition, and how classification can improve the accuracy of plant leaf classification. Image input, image pre-processing, feature extraction, and classification using Support Vector Machine (SVM) are the four main stages used in the proposed recognition method. In this study, the vein density feature extraction uses parameters like the count of vein pixels and the area of the leaf and colour feature is extracted through red, green and blue channels respectively, while the texture feature extraction uses contrast, correlation, inverse different moments, and entropy parameters. Support Vector Machine classification was applied for all features, and the results for separated texture, colour and vein density accuracy were 65.44%, 74.17% and 38.91%, respectively by using the Flavia dataset. The accuracy for extracting all features at once was 84.46%. As per the result of the study, the best way of identifying plant species is the combination of all selected three features. © 2023 IEEE.},
	author_keywords = {colour; feature extraction; Support Vector Machine; texture; vein density},
	keywords = {Classification (of information); Color; Ecosystems; Extraction; Feature extraction; Image classification; Image enhancement; Textures; Color density; Density features; Features extraction; Image processing technique; Leaf recognition; Plant leaves; Plant species; Species recognition; Support vectors machine; Vein densities; Support vector machines},
	correspondence_address = {S.W. Athapaththu; University of Sri Lanka, Faculty of Computing Sabaragamuwa, Department of Computing and Information Systems, Belihuloya, Sri Lanka; email: swathapaththu@std.appsc.sab.ac.lk},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835034737-1},
	language = {English},
	abbrev_source_title = {ICARC - Int. Conf. Adv. Res. Comput.: Digit. Transform. Sustain. Dev.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Conference on Advanced Research in Computing, ICARC 2023; Conference date: 23 February 2023 through 24 February 2023; Conference code: 189421}
}

@ARTICLE{Hart2023929,
	author = {Hart, Adam G. and Bosley, Hayley and Hooper, Chloe and Perry, Jessica and Sellors-Moore, Joel and Moore, Oliver and Goodenough, Anne E.},
	title = {Assessing the accuracy of free automated plant identification applications},
	year = {2023},
	journal = {People and Nature},
	volume = {5},
	number = {3},
	pages = {929 – 937},
	doi = {10.1002/pan3.10460},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148938301&doi=10.1002%2fpan3.10460&partnerID=40&md5=907d2b54a523060990799c04d0783614},
	affiliations = {Department of Natural and Social Science, University of Gloucestershire, Cheltenham, United Kingdom; Taylor Wildlife, United Kingdom},
	abstract = {Widely available and inexpensive mobile phone applications offer users, whether professional ecologists or interested amateurs, the potential for simple and rapid automated identification of species, without the need to use field guides and identification keys. The increasing accuracy of machine learning is well established, but it is currently unclear if, and under what circumstances, free-to-use mobile phone applications are accurate for identifying plants to species level in real-world field conditions. We test five popular and free identification applications for plants using 857 professionally identified images of 277 species from 204 genera. Across all applications, 85% of images were identified correctly in the top five suggestions, and 69% were correct with the first suggestion. Plant type (woody, forbs, grasses, rushes/sedges, ferns/horsetails) was a significant determinant of identification performance for each application. For some applications, image saliency was also important; exposure and focus were not significant. Applications performed well, with at least one of the three best-performing applications identifying 96% of images correctly as their first suggestion. We conclude that, subject to some caveats, free phone-based plant identification applications are valid and useful tools for those wanting rapid identification and for anyone wanting to engage with the natural world. Read the free Plain Language Summary for this article on the Journal blog. © 2023 The Authors. People and Nature published by John Wiley & Sons Ltd on behalf of British Ecological Society.},
	author_keywords = {automated identification; Google Lens; iNaturalist Seek; LeafSnap; plant identification; PlantNet; PlantSnap; species ID},
	correspondence_address = {A.G. Hart; Department of Natural and Social Science, University of Gloucestershire, Cheltenham, United Kingdom; email: ahart@glos.ac.uk},
	publisher = {John Wiley and Sons Inc},
	issn = {25758314},
	language = {English},
	abbrev_source_title = {People.  Nat.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Green Open Access}
}

@ARTICLE{Pravin2023128,
	author = {Pravin, A. and Deepa, C.},
	title = {Deep Feature Extraction and Weight Updated Tuned Random Forest for Piper Plant Species Recognition},
	year = {2023},
	journal = {Advances in Systems Science and Applications},
	volume = {23},
	number = {2},
	pages = {128 – 151},
	doi = {10.25728/assa.2023.23.2.1393},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165977238&doi=10.25728%2fassa.2023.23.2.1393&partnerID=40&md5=15d51d9db59f58b41efad9bb428e2116},
	affiliations = {Department of Computer Science, Sri Ramakrishna College of Arts & Science, Coimbatore, India; Department of Computer Science (Artificial Intelligence & Data Science), Sri Ramakrishna College of Arts & Science, Coimbatore, India},
	abstract = {Recently, identifying plant species has become a significant research area as it is vital for securing biodiversity. Plants also possess various medicinal applications. Hence, predicting different species of plants is of utmost significance. However, determining plant species through conventional ways is a time-consuming process. That happens due to huge and distinct botanical terms. With the recent evolution of AI (Artificial Intelligence) based algorithms, researchers have undertaken various attempts to predict plant species. However, most studies averted the consideration of piper plant species which holds huge medicinal benefits. Existing research also failed to predict the plant species due to inefficient feature extraction accurately. Considering such a pitfall, this study proposes Deep CNN (Deep Convolutional Neural Network) and Inception V3 to extract features to perform all plant classification. In addition, the study proposes Deep CNN and VGG16 (Visual Geometry Group16) to extract suitable features for performing piper plant classification. Following this, the study considers PCA (Principle Component Analysis) for feature fusion as it can reduce noise in data and select relevant features for affording independent and uncorrelated data features. Finally, the study proposes WUT-RF (Weight Updated Tuned Random Forest) to classify piper and all plant species. In this process, hyperparameters of RF are tuned with convolutional likelihood weight to attain a high prediction rate. Optimal hyperparameter selection and tuning assist in improvising the performance of the proposed classifier. Performance analysis of this system about performance metrics exposes its effectiveness in plant species detection. © 2023 ASSA},
	author_keywords = {Artificial Intelligence; Deep Convolutional Neural Network; Inception V3; Piper Plants; Random Forest; VGG16},
	correspondence_address = {A. Pravin; Department of Computer Science, Sri Ramakrishna College of Arts & Science, Coimbatore, India; email: pravinresearchscholar1@gmail.com},
	publisher = {International Institute for General Systems Studies},
	issn = {10786236},
	language = {English},
	abbrev_source_title = {Adv. Syst. Sci. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Alao2023127,
	author = {Alao, Olaniyi Bayonle and Rother, Kristian and Henkler, Stefan},
	title = {Synthetic Data for Machine Learning on Embedded Systems in Precision Agriculture},
	year = {2023},
	journal = {IFIP Advances in Information and Communication Technology},
	volume = {669},
	pages = {127 – 138},
	doi = {10.1007/978-3-031-34214-1_11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164270028&doi=10.1007%2f978-3-031-34214-1_11&partnerID=40&md5=981e8e0001039ba8171ab52c23f167ee},
	affiliations = {Hamm-Lippstadt University of Applied Sciences, Lippstadt, 59557, Germany},
	abstract = {Embedded systems are used in precision agriculture for data collection via sensors and for the control of actuators such as sprayers based on machine learning models. For plant classification and monitoring, it is easier to collect data of healthy plants than it is to collect data of plants that are infected by various diseases, because they are simply more common. Sufficient data are therefore often lacking for the accurate detection of diseased plants. In this paper, we outline an approach for the generation of synthetic data of infected plants that can be used to train a machine learning model for the classification of sugar beets. We use image augmentation techniques to build a pipeline that can automatically overlay diseased areas on healthy areas of leaf images. © 2023, IFIP International Federation for Information Processing.},
	author_keywords = {Embedded Systems; Machine Learning; Precision Agriculture; Synthetic Data},
	keywords = {Data acquisition; Embedded systems; Precision agriculture; Sugar beets; Augmentation techniques; Data collection; Embedded-system; Machine learning models; Machine-learning; On-machines; Plant classification; Plant monitoring; Precision Agriculture; Synthetic data; Machine learning},
	correspondence_address = {K. Rother; Hamm-Lippstadt University of Applied Sciences, Lippstadt, 59557, Germany; email: kristian.rother@hshl.de},
	editor = {Henkler S. and Rettberg A. and Kreutz M. and Wehrmeister M.A. and Götz M.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18684238},
	isbn = {978-303134213-4},
	language = {English},
	abbrev_source_title = {IFIP Advances in Information and Communication Technology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 7th IFIP TC 10 International Embedded Systems Symposium, IESS 2022; Conference date: 3 November 2022 through 4 November 2022; Conference code: 296489}
}

@ARTICLE{Kang2023933,
	author = {Kang, Xiaoyan and Huang, Changping and Zhang, Lifu and Yang, Mi and Zhang, Ze and Lyu, Xin},
	title = {Assessing the severity of cotton Verticillium wilt disease from in situ canopy images and spectra using convolutional neural networks},
	year = {2023},
	journal = {Crop Journal},
	volume = {11},
	number = {3},
	pages = {933 – 940},
	doi = {10.1016/j.cj.2022.12.002},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146046562&doi=10.1016%2fj.cj.2022.12.002&partnerID=40&md5=600784fb62a5814e303b3e8e79ba61fb},
	affiliations = {National Engineering Research Center of Satellite Remote Sensing Applications, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; University of Chinese Academy of Sciences, Beijing, 100049, China; Xinjiang Production and Construction Corps Oasis Eco-Agriculture Key Laboratory, College of Agriculture, Shihezi University, Xinjiang, Shihezi, 832003, China},
	abstract = {Verticillium wilt (VW) is a common soilborne disease of cotton. It occurs mainly in the seedling and boll-opening stages and severely impairs the yield and quality of the fiber. Rapid and accurate identification and evaluation of VW severity (VWS) forms the basis of field cotton VW control, which has great significance to cotton production. Cotton VWS values are conventionally measured using in-field observations and laboratory test diagnoses, which require abundant time and professional expertise. Remote and proximal sensing using imagery and spectrometry have great potential for this purpose. In this study, we performed in situ investigations at three experimental sites in 2019 and 2021 and collected VWS values, in situ images, and spectra of 361 cotton canopies. To estimate cotton VWS values at the canopy scale, we developed two deep learning approaches that use in situ images and spectra, respectively. For the imagery-based method, given the high complexity of the in situ environment, we first transformed the task of healthy and diseased leaf recognition to the task of cotton field scene classification and then built a cotton field scenes (CFS) dataset with over 1000 images for each scene-unit type. We performed pretrained convolutional neural networks (CNNs) training and validation using the CFS dataset and then used the networks after training to classify scene units for each canopy. The results showed that the DarkNet-19 model achieved satisfactory performance in CFS classification and VWS values estimation (R2 = 0.91, root-mean-square error (RMSE) = 6.35%). For the spectroscopy-based method, we first designed a one-dimensional regression network (1D CNN) with four convolutional layers. After dimensionality reduction by sensitive-band selection and principal component analysis, we fitted the 1D CNN with varying numbers of principal components (PCs). The 1D CNN model with the top 20 PCs performed best (R2 = 0.93, RMSE = 5.77%). These deep learning-driven approaches offer the potential of assessing crop disease severity from spatial and spectral perspectives. © 2022 Crop Science Society of China and Institute of Crop Science, CAAS},
	author_keywords = {Canopy scale; Cotton verticillium wilt; Deep learning; Disease assessment; In situ imagery; In situ spectrometry},
	correspondence_address = {C. Huang; National Engineering Research Center of Satellite Remote Sensing Applications, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; email: huangcp@aircas.ac.cn},
	publisher = {KeAi Communications Co.},
	issn = {20955421},
	language = {English},
	abbrev_source_title = {Crop J.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Min2022,
	author = {Min, Cheol Woo and Jang, Jeong Woo and Lee, Gi Hyun and Gupta, Ravi and Yoon, Jinmi and Park, Hyun Ji and Cho, Hye Sun and Park, Sang Ryeol and Kwon, Soon-Wook and Cho, Lae-Hyeon and Jung, Ki-Hong and Kim, Yu-Jin and Wang, Yiming and Kim, Sun Tae},
	title = {TMT-based quantitative membrane proteomics identified PRRs potentially involved in the perception of MSP1 in rice leaves},
	year = {2022},
	journal = {Journal of Proteomics},
	volume = {267},
	doi = {10.1016/j.jprot.2022.104687},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135387919&doi=10.1016%2fj.jprot.2022.104687&partnerID=40&md5=600ade5172ae22b9b5299bf970be3b1f},
	affiliations = {Department of Plant Bioscience, Life and Industry Convergence Research Institute, Pusan National University, Miryang, 50463, South Korea; College of General Education, Kookmin University, Seoul, 02707, South Korea; Plant System Engineering Research Center, Korea Research Institute of Bioscience and Biotechnology, Daejeon, 34141, South Korea; National Institute of Agricultural Sciences, Rural Development Administration, Jeonju, 54874, South Korea; Graduate School of Biotechnology and Crop Biotech Institute, Kyung Hee University, Yongin, 17104, South Korea; Department of Life Science and Environmental Biochemistry, Life and Industry Convergence Research Institute, Pusan National University, Miryang, 50463, South Korea; Key Laboratory of Biological Interactions and Crop Health, Department of Plant Pathology, Nanjing Agricultural University, Nanjing, 210095, China},
	abstract = {Pathogen-associated molecular patterns (PAMPs) play a key role in triggering PAMPs triggered immunity (PTI) in plants. In the case of the rice-Magnaporthe oryzae pathosystem, fewer PAMPs and their pattern recognition receptors (PRRs) have been characterized. Recently, a M. oryzae snodprot1 homolog protein (MSP1) has been identified that functions as PAMP and triggering the PTI responses in rice. However, the molecular mechanism underlying MSP1-induced PTI is currently elusive. Therefore, we generated MSP1 overexpressed transgenic lines of rice, and a tandem mass tag (TMT)-based quantitative membrane proteomic analysis was employed to decipher the potential MSP1-induced signaling in rice using total cytosolic as well as membrane protein fractions. This approach led to the identification of 8033 proteins of which 1826 were differentially modulated in response to overexpression of MSP1 and/or exogenous jasmonic acid treatment. Of these, 20 plasma membrane-localized receptor-like kinases (RLKs) showed increased abundance in MSP1 overexpression lines. Moreover, activation of proteins related to the protein degradation and modification, calcium signaling, redox, and MAPK signaling was observed in transgenic lines expressing MSP1 in the apoplast. Taken together, our results identified potential PRR candidates involved in MSP1 recognition and suggested the overview mechanism of the MSP1-induced PTI signaling in rice leaves. Significance: In plants, recognition of pathogen pathogen-derived molecules, such as PAMPs, by plant plant-derived PRRs has an essential role for in the activation of PTI against pathogen invasion. Typically, PAMPs are recognized by plasma membrane (PM) localized PRRs, however, identifying the PM-localized PRR proteins is challenging due to their low abundance. In this study, we performed an integrated membrane protein enrichment by microsomal membrane extraction (MME) method and subsequent TMT-labeling-based quantitative proteomic analysis using MSP1 overexpressed rice. Based on these results, we successfully identified various intracellular and membrane membrane-localized proteins that participated in the MSP1-induced immune response and characterized the potential PM-localized PRR candidates in rice. © 2022 Elsevier B.V.},
	author_keywords = {Magnaporthe oryzae; MSP1; Proteomics; PRRs; Rice; TMT},
	keywords = {common acute lymphoblastic leukemia antigen; jasmonic acid; membrane protein; pathogen associated molecular pattern; pattern recognition receptor; phosphotransferase; transcription factor ERG; Agrobacterium tumefaciens; apoplast; Article; calcium signaling; cell membrane; cellular distribution; controlled study; deletion mutant; extraction; gene overexpression; gene silencing; immune response; Magnaporthe oryzae; MAPK signaling; microsome membrane; molecular recognition; Nicotiana benthamiana; nonhuman; oxidation reduction reaction; plant leaf; plant pathogen interaction; protein degradation; protein function; protein modification; protein phosphorylation; proteomics; quantitative analysis; real time polymerase chain reaction; rice; RNA isolation; signal transduction; TMT labeling; transgene; transient expression},
	correspondence_address = {S.T. Kim; Department of Plant Bioscience, Life and Industry Convergence Research Institute, Pusan National University, Miryang, 50463, South Korea; email: stkim71@pusan.ac.kr},
	publisher = {Elsevier B.V.},
	issn = {18743919},
	pmid = {35914717},
	language = {English},
	abbrev_source_title = {J. Proteomics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Yamabe2023511,
	author = {Yamabe, Towa and Saitoh, Takeshi},
	title = {Vision Transformer-Based Bark Image Recognition for Tree Identification},
	year = {2023},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {13836 LNCS},
	pages = {511 – 522},
	doi = {10.1007/978-3-031-25825-1_37},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148044090&doi=10.1007%2f978-3-031-25825-1_37&partnerID=40&md5=0046cc4a5f6847ac7b9e9891c6c93a5a},
	affiliations = {Kyushu Institute of Technology, 680–4 Kawazu, Fukuoka, Iizuka, Japan},
	abstract = {Our group is studying tree species recognition using image processing technology. In the previous research, we proposed an image-based bark recognition using CNN. In this paper, we propose a method of recognizing bark image using Vision Transformer (ViT), which has attracted attention in the image recognition task in recent years. Four public datasets of NewBarkTex, TRUNK12, BarkNet1.0, and Bark-101, and a new dataset of 150 tree species originally collected, KyutechBark150, were used in the evaluation experiment. Several CNN models were used as comparison methods. As a result of the recognition experiment, the highest recognition accuracy of ViT was obtained in all the datasets. In addition, the trained model was visualized by t-SNE and attention map, and this paper shows that ViT is effective for bark image recognition. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author_keywords = {Bark texture; Tree species recognition; Vision transformer},
	keywords = {Textures; Bark image; Bark texture; Image processing technology; Image-based; Public dataset; Species recognition; Tree identification; Tree species; Tree species recognition; Vision transformer; Image recognition},
	correspondence_address = {T. Saitoh; Kyushu Institute of Technology, Iizuka, 680–4 Kawazu, Fukuoka, Japan; email: saitoh@ai.kyutech.ac.jp},
	editor = {Yan W.Q. and Nguyen M. and Stommel M.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303125824-4},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 37th International Conference on Image and Vision Computing New Zealand, IVCNZ 2022; Conference date: 24 November 2022 through 25 November 2022; Conference code: 290019}
}

@ARTICLE{Zhou202316503,
	author = {Zhou, Jiawei and Chen, Xinglong and Li, Shuhan and Dong, Runyan and Wang, Xinrui and Zhang, Chong and Zhang, Li},
	title = {Multispecies individual tree crown extraction and classification based on BlendMask and high-resolution UAV images},
	year = {2023},
	journal = {Journal of Applied Remote Sensing},
	volume = {17},
	number = {1},
	pages = {16503},
	doi = {10.1117/1.JRS.17.016503},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151676763&doi=10.1117%2f1.JRS.17.016503&partnerID=40&md5=0b6ffb10f5386f3f88191b3e40f283b6},
	affiliations = {Beijing Forestry University, College of Science, Beijing, China; Beijing Institute of Technology, School of Optoelectronics, Beijing, China},
	abstract = {Accurate detection and segmentation of individual trees from unmanned aerial vehicle images is critical for forestry resource surveys and accurate forest management. Deep learning methods have been used for studies of individual tree crown segmentation, classification, and number of trees in mixed coniferous and broad-leaved forests, but the accuracy needs to be improved. Therefore, this study uses BlendMask, a simpler and more efficient algorithm that combines Mask R-CNN and Yolact algorithms to effectively combine instance-level information with semantic information at a finer granularity level, greatly improving crown segmentation accuracy and classification results. Three coniferous species and five broad-leaved species unmanned aerial vehicle images collected from the Jing Yue multispecies ecological forestry site in Changping District, Beijing, were used as the dataset, and the results were compared with Yolact and Mask R-CNN. The results show that the method described in this work has the highest Kappa coefficient (0.89) and overall accuracy (92.14%) in the test set. For segmentation accuracy, coniferous species' producer's accuracy was 0.91 to 0.95, whereas that of broad-leaved species was 0.89 to 0.92. For species classification, the F1-score and mean average precision for coniferous species were greater than 91%, whereas those for broad-leaved species were 77.64% to 85.63%. The accuracy of extracting stand density in low and medium canopy density stands was 0.9909 and 0.9422, respectively, whereas that in high canopy density stands was 0.8913. This study shows that the BlendMask model has a good effect in studying the classification of multiple tree species, the segmentation of individual tree crowns, and the statistics of the number of trees in complex forest areas. Compared with broad-leaved forests and high canopy density stands, this model is more suitable for coniferous forest and medium and low canopy density stand scenarios. This study provides an important tool for obtaining more accurate species classification, canopy segmentation, and resource inventory results in complex forest areas.  © 2023 Society of Photo-Optical Instrumentation Engineers (SPIE).},
	author_keywords = {BlendMask; crown segmentation; tree number detection; tree species identification; UAV image},
	keywords = {Aircraft detection; Antennas; Classification (of information); Deep learning; Image classification; Image segmentation; Learning systems; Semantics; Timber; Unmanned aerial vehicles (UAV); Aerial vehicle; Blendmask; Canopy density; Coniferous species; Crown segmentation; Individual tree crown; Multi-species; Tree number detection; Tree species identifications; UAV image; Forestry},
	correspondence_address = {L. Zhang; Beijing Forestry University, College of Science, Beijing, China; email: zhang_li@bjfu.edu.cn},
	publisher = {SPIE},
	issn = {19313195},
	coden = {JARSC},
	language = {English},
	abbrev_source_title = {J. Appl. Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yang202339,
	author = {Yang, Chengzhuan and Fang, Lincong and Yu, Qian and Wei, Hui},
	title = {A Learning Robust and Discriminative Shape Descriptor for Plant Species Identification},
	year = {2023},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	volume = {20},
	number = {1},
	pages = {39 – 51},
	doi = {10.1109/TCBB.2022.3148463},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124739603&doi=10.1109%2fTCBB.2022.3148463&partnerID=40&md5=ea34eec95b86c3547cf5f3ab95466346},
	affiliations = {Zhejiang Normal University, School of Mathematics and Computer Science, Jinhua, 321004, China; Zhejiang University of Finance and Economics, School of Information, Hangzhou, 310018, China; Fudan University, School of Computer Science, Shanghai, 200438, China; Jiangsu University of Technology, School of Computer Engineering, Changzhou, 213001, China; Fudan University, School of Computer Science, Laboratory of Cognitive Algorithm and Model, Shanghai Key Laboratory of Data Science, Shanghai, 201203, China},
	abstract = {Plant identification based on leaf images is a widely concerned application field in artificial intelligence and botany. The key problem is extracting robust discriminative features from leaf images and assigning a measure of similarity. This study proposes an effective, robust shape descriptor to identify plant species from images of their leaves, which we call the high-level triangle shape descriptor (HTSD). First, we extract a leaf image's external contour and internal salient point information. We then use triangle features to describe the leaf contour, which we call the contour point based on triangle features (CPTFs). The internal information of the leaf image is based on salient point triangle features (SPTFs). The third step is to apply the Fisher vector to encode the two kinds of point-based local triangle features into the HTSD. Finally, we employ the simple euclidean distance to calculate the dissimilarities between the HTSD characteristics of leaf images. We have extensively evaluated the proposed approach on several public leaf datasets successfully. Experimental results show that our method has superior recognition accuracy, outperforming current state-of-the-art shape-based and deep-learning plant identification approaches.  © 2012 IEEE.},
	author_keywords = {fisher vector; Plant species recognition; shape descriptor; triangle feature},
	keywords = {Algorithms; Artificial Intelligence; Plant Leaves; Plants; Deep learning; Feature extraction; Image processing; Neural networks; Plants (botany); Computational modelling; Convolutional neural network; Deep learning; Features extraction; Fisher vectors; Image color analysis; Plant species; Plant species recognition; Shape; Shape descriptors; Species recognition; Triangle feature; algorithm; artificial intelligence; plant; plant leaf; Character recognition},
	correspondence_address = {C. Yang; Zhejiang Normal University, School of Mathematics and Computer Science, Jinhua, 321004, China; email: chengzhuan_yang@163.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {15455963},
	pmid = {35130167},
	language = {English},
	abbrev_source_title = {IEEE/ACM Trans. Comput. BioL. Bioinf.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Quach2023777,
	author = {Quach, Boi M. and Dinh, V. Cuong and Pham, Nhung and Huynh, Dang and Nguyen, Binh T.},
	title = {Leaf recognition using convolutional neural networks based features},
	year = {2023},
	journal = {Multimedia Tools and Applications},
	volume = {82},
	number = {1},
	pages = {777 – 801},
	doi = {10.1007/s11042-022-13199-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131688019&doi=10.1007%2fs11042-022-13199-y&partnerID=40&md5=b4fe13fbc19681ce35a83d48ec802c8b},
	affiliations = {Dublin City University, Dublin City, Ireland; University of Science, Ho Chi Minh City, Viet Nam; Vietnam National University in Ho Chi Minh City, Ho Chi Minh City, Viet Nam; AISIA Research Lab, Ho Chi Minh City, Viet Nam; Hong Bang International University, Ho Chi Minh City, Viet Nam},
	abstract = {There is a warning light for the loss of plant habitats worldwide that entails concerted efforts to conserve plant biodiversity. Thus, plant species classification is crucial to address this environmental challenge. In recent years, there has been a considerable increase in studies related to plant taxonomy. While some researchers try to improve their recognition performance using novel approaches, others concentrate on computational optimization for their framework. In addition, a few studies are diving into feature extraction to gain significantly in terms of accuracy. This paper proposes an effective method for the leaf recognition problem. In our proposed approach, a leaf goes through some pre-processing to extract its refined color image, vein image, xy-projection histogram, handcrafted shape, texture features, and Fourier descriptors. These attributes are then transformed into a better representation by neural network-based encoders before a support vector machine (SVM) model is utilized to classify different leaves. Overall, our approach performs a state-of-the-art result on the Flavia leaf dataset, achieving the accuracy of 99.69% on test sets under random 10-fold cross-validation and bypassing the previous methods. Another important contribution is the trade-offs in classification performance while minimizing the feature categories used. In order to tackle this challenge, we designed several empirical experiments to analyze the performance of different combinations of feature sources and choose the best combination for features for the main problem. We also release our codes (Scripts are available at https://github.com/Tayerquach/flavia_recognition) for contributing to the research community in the leaf classification problem. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Deep learning; Leaf recognition; Support vector machines},
	keywords = {Classification (of information); Convolutional neural networks; Deep learning; Economic and social effects; Plants (botany); Statistical tests; Support vector machines; Textures; Convolutional neural network; Deep learning; Leaf recognition; Network-based; Performance; Plant habitats; Plant species; Species classification; Support vectors machine; Warning lights; Biodiversity},
	correspondence_address = {B.T. Nguyen; AISIA Research Lab, Ho Chi Minh City, Viet Nam; email: ngtbinh@hcmus.edu.vn},
	publisher = {Springer},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Green Open Access}
}

@ARTICLE{Kang2023,
	author = {Kang, Feilong and Li, Jia and Wang, Chunguang and Wang, Fuxiang},
	title = {A Lightweight Neural Network-Based Method for Identifying Early-Blight and Late-Blight Leaves of Potato},
	year = {2023},
	journal = {Applied Sciences (Switzerland)},
	volume = {13},
	number = {3},
	doi = {10.3390/app13031487},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148041655&doi=10.3390%2fapp13031487&partnerID=40&md5=e75a461d57405f596fbe4a9357455b08},
	affiliations = {Inner Mongolia Agricultural University, Hohhot, 010018, China; Inner Mongolia Autonomous Region Key Laboratory of Big Data Research and Application of Agriculture and Animal Husbandry, Hohhot, 010018, China},
	abstract = {Crop pests and diseases are one of the most critical disasters that limit agricultural production. In this paper, we trained a lightweight convolutional neural network model and built a Django framework-based potato disease leaf recognition system, which can recognize three types of potato leaf images including early blight, late blight, and healthy. A lightweight, neural network-based model for the identification of early potato leaf diseases significantly reduces the number of model parameters, whereas the accuracy of Top-1 identification is over 93%. We imported the trained model into the Django framework to build a website for a potato early leaf disease identification system, thus providing technical support for the implementation of a mobile-based potato leaf disease identification and early warning system. © 2023 by the authors.},
	author_keywords = {convolutional neural networks; Django framework; machine learning; potato disease leaf},
	correspondence_address = {J. Li; Inner Mongolia Agricultural University, Hohhot, 010018, China; email: lijia@imau.edu.cn},
	publisher = {MDPI},
	issn = {20763417},
	language = {English},
	abbrev_source_title = {Appl. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access}
}

@CONFERENCE{Sapaev2023,
	author = {Sapaev, J. and Arifjanov, A. and Ramazonov, Kh. and Sapaev, I.B.},
	title = {Smart technologies for determining water flow in irrigation systems},
	year = {2023},
	journal = {E3S Web of Conferences},
	volume = {383},
	doi = {10.1051/e3sconf/202338302012},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159565222&doi=10.1051%2fe3sconf%2f202338302012&partnerID=40&md5=ec5702afd4dd0a0ed290b71a11fc4c1a},
	affiliations = {National Research University TIIAME, st. Kori Niyazov, house 39, Tashkent, 100000, Uzbekistan; Tashkent State Pedagogical University, Bunyodkor avenue, 27, Tashkent, 100070, Uzbekistan},
	abstract = {Without the need for hand-coding, Machine Learning helps systems to enhance and develop dynamically from their experiences. As a result, numerous tech firms have been creating Artificial Intelligence applications in recent years. The majority of irrigation systems available today allow customers to program them to provide a specified amount of water at specific times. On the other hand, a garden frequently has a variety of plants, each of which needs a varying amount of water. This research planned an irrigation system that uses deep learning to regulate the quantity of water given to each type of plant based on plant identification to address this problem. The software and hardware are the two primary constituents of the technology. The former is linked to cameras for plant identification and uses a database to determine the appropriate amount of water; the other regulates the amount of water that can flow out. The technology is designed to predict how long to water the plants after discovering the perfect soil moisture with the applications and incorporating it with the outcome of the existing soil moisture level with the Arduino. This will allow the program to modify the software in the irrigation system controller to alter the period of time the regulator should be kept open. © The Authors, published by EDP Sciences, 2023.},
	correspondence_address = {J. Sapaev; National Research University TIIAME, Tashkent, st. Kori Niyazov, house 39, 100000, Uzbekistan; email: mohim@inbox.ru},
	editor = {Bieliatynskyi A. and Guda A.N.},
	publisher = {EDP Sciences},
	issn = {25550403},
	language = {English},
	abbrev_source_title = {E3S Web Conf.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2023 International Scientific Conference Transport Technologies in the 21st Century, TT21C 2023; Conference date: 5 April 2023 through 7 April 2023; Conference code: 188046; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Retallack2022,
	author = {Retallack, Angus and Finlayson, Graeme and Ostendorf, Bertram and Lewis, Megan},
	title = {Using deep learning to detect an indicator arid shrub in ultra-high-resolution UAV imagery},
	year = {2022},
	journal = {Ecological Indicators},
	volume = {145},
	doi = {10.1016/j.ecolind.2022.109698},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142148895&doi=10.1016%2fj.ecolind.2022.109698&partnerID=40&md5=78c6558767389b595c1dc9dea2756430},
	affiliations = {Department of Ecology and Evolutionary Biology, The University of Adelaide, SA, Australia; Bush Heritage Australia, PO Box 329, Flinders Lane, Melbourne, 8009, VIC, Australia},
	abstract = {Effective monitoring of arid and semi-arid rangelands around the world is essential to understand and combat degradation caused by anthropogenic use and facilitate effective management practices. Remote sensing technologies provide ideal approaches for enhancing traditional on-ground monitoring. However, while broad-scale monitoring of vegetation in rangelands using satellites has been widely adopted, there has been far less uptake of remote sensing for measuring fine-scale indicators of ecosystem condition. This study demonstrates the feasibility of using ultra-high-resolution UAV (Uncrewed Aerial Vehicle) imagery and deep-learning-based object detection models to provide plant recognition and survey information relevant for operational monitoring programmes in arid and semi-arid ecosystems. Seven different object detectors using varying convolutional neural network (CNN) architectures are tested at three image resolutions to detect a widespread, dominant arid shrub species (pearl bluebush, Maireana sedifolia) that serves as a key indicator of overall site condition in southern Australian rangelands. To maximise the strength of statistical analysis, each method is trained on six different training datasets (each using 2,000 to 3,000 training samples) at six widely dispersed sites. This results in 90 trained models, each validated at two sites. To test model generalisability, training and validation data was always sourced from separate vegetation monitoring sites. The influence of variability between sites on detection accuracy is also considered. The best performing models achieved F1 scores (overall accuracy) of around 75% for pearl bluebush detection, a level of accuracy that provides useful monitoring information to land managers. Information extracted from UAV imagery using this approach relates directly to indicators of ecological condition measured in ground-based monitoring; including dominant plant species count, location and density. Continued development and eventual implementation of this method would provide objective conservation-relevant information at broad scales in a far reduced time and at a lower cost than is currently achievable using on-ground approaches. © 2022 The Authors},
	author_keywords = {Arid ecology; Conservation; Deep learning; Rangelands; Remote sensing; UAV},
	keywords = {Australia; Aircraft detection; Antennas; Convolution; Convolutional neural networks; Deep learning; Ecosystems; Image resolution; Information use; Object detection; Personnel training; Unmanned aerial vehicles (UAV); Vegetation; Anthropogenics; Arid ecology; Deep learning; Effective management; Management practises; Rangeland; Remote-sensing; Semi-arid rangeland; Ultrahigh resolution; Uncrewed aerial vehicles; arid environment; ground-based measurement; remote sensing; satellite imagery; shrub; unmanned vehicle; Remote sensing},
	correspondence_address = {A. Retallack; Department of Ecology and Evolutionary Biology, The University of Adelaide, Adelaide, North Terrace Campus, 5005, Australia; email: angus.retallack@adelaide.edu.au},
	publisher = {Elsevier B.V.},
	issn = {1470160X},
	language = {English},
	abbrev_source_title = {Ecol. Indic.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Bhat2023703,
	author = {Bhat, Sachin S. and Ananth, Alaka and Shetty, Anup S. and Nayak, Deepak and Shettigar, Prasad J. and Shetty, Sagar},
	title = {Evaluation of Support Vector Machine and Binary Convolutional Neural Network for Automatic Medicinal Plant Species Identification},
	year = {2023},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {968},
	pages = {703 – 711},
	doi = {10.1007/978-981-19-7346-8_61},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152636002&doi=10.1007%2f978-981-19-7346-8_61&partnerID=40&md5=0dbf6928e614cc6b03988a9f8ff1c36e},
	affiliations = {Shri Madhwa Vadiraja Institute of Technology and Management, Bantakal, India; NMAM Institute of Technology, Nitte, India},
	abstract = {Enormous amount of diversified plant species are available in India. Recognition and classification of these species have become a major challenge and an important research field. Though different parts of plants can be used in identifying their genre, leaf is most useful and effective method in classification. Machine learning brings an ideal way to automate this system. A separate dataset is built by collecting 20 different leaf samples available mainly in Southern India. More than 20,000 such samples are collected to build this dataset. Here, we used two different machine learning models namely support vector machine and binary convolutional neural network. These algorithms gave a promising results of 79% and 89.5%, respectively. Various analytical methods are used to evaluate the performance of these models. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2023.},
	author_keywords = {Leaf dataset; Neural network; Plant classification; Support vector machine},
	keywords = {Classification (of information); Convolution; Convolutional neural networks; Learning systems; Convolutional neural network; Leaf dataset; Machine-learning; Medicinal plants; Neural-networks; Plant classification; Plant species; Plant species identification; Research fields; Support vectors machine; Support vector machines},
	correspondence_address = {S.S. Bhat; Shri Madhwa Vadiraja Institute of Technology and Management, Bantakal, India; email: sachinbhat88@gmail.com},
	editor = {Shukla A. and Hasteer N. and Murthy B.K. and VanBelle J.-P.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981197345-1},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2nd International Conference on Information Technology, InCITe 2022; Conference date: 3 March 2022 through 4 March 2022; Conference code: 292709}
}

@ARTICLE{Pradipkumar20222447,
	author = {Pradipkumar, Vaghela Himali and Alagu Raja, R.A.},
	title = {Automatic Identification of Tree Species from UAV Images Using Machine Learning Approaches},
	year = {2022},
	journal = {Journal of the Indian Society of Remote Sensing},
	volume = {50},
	number = {12},
	pages = {2447 – 2464},
	doi = {10.1007/s12524-022-01608-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139439360&doi=10.1007%2fs12524-022-01608-6&partnerID=40&md5=6abd58b04f16e665f074075f9a717341},
	affiliations = {Remote Sensing and GIS Lab, Thiagarajar College of Engineering, Tamilnadu, Madurai, 625 015, India},
	abstract = {Today, the identification of tree species has become essential in order to protect biodiversity and ecological equilibrium and fulfil medicinal purposes. It is beneficial for large parts of society, foresters, farmers, biologists, conservationists, and landscape architects. The process of identifying tree species by conventional methods is time-consuming and prone to human error. So, a method based on automatic tree species identification is required to reduce human error as well as time. This can be achieved by using artificial intelligence and image processing techniques. This paper deals with machine learning approaches, a subset of artificial intelligence, where different supervised classifiers such as support vector machine (SVM), k-nearest neighbour (kNN), decision tree (DT), random forest (RF), and eXtreme gradient boosting (XGBoost) are used to identify different tree species such as banana, coconut, mango, oil palm, and papaya. To implement each machine learning model, features are extracted from training images using the histogram of oriented gradient (HOG) descriptor. These feature vectors are fed as inputs to the classifiers so that the generated model can identify the unlabelled tree species. To demonstrate the effectiveness of the HOG descriptor in identifying tree species, two additional feature descriptors, gray-level co-occurrence matrix and linear binary pattern, are used. The performance of each model is assessed using the confusion matrix. The overall accuracy obtained for the classifiers SVM, kNN, DT, RF, and XGBoost using the HOG is 97.05%, 92.05%, 75.00%, 95.00%, and 97.94%, respectively. XGBoost gives higher accuracy because it combines the estimates of individual classifiers. © 2022, Indian Society of Remote Sensing.},
	author_keywords = {HOG feature descriptor; Machine learning; Tree species identification; XGBoost classifier},
	keywords = {algorithm; artificial intelligence; biodiversity; histogram; identification method; image processing; machine learning; unmanned vehicle},
	correspondence_address = {V.H. Pradipkumar; Remote Sensing and GIS Lab, Thiagarajar College of Engineering, Madurai, Tamilnadu, 625 015, India; email: hpvaghela10@gmail.com},
	publisher = {Springer},
	issn = {0255660X},
	language = {English},
	abbrev_source_title = {J. Ind. Soc. Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Custodio202329,
	author = {Custodio, Epie F.},
	title = {Classifying Philippine Medicinal Plants Based on Their Leaves Using Deep Learning},
	year = {2023},
	journal = {2023 IEEE World AI IoT Congress, AIIoT 2023},
	pages = {29 – 35},
	doi = {10.1109/AIIoT58121.2023.10174335},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166671492&doi=10.1109%2fAIIoT58121.2023.10174335&partnerID=40&md5=3a5059fd2fde54e574a8ba75169aef3c},
	affiliations = {Mindoro State University, College of Computer Studies, Calapan, Philippines},
	abstract = {Medicinal plants play an important role in human mental and physical well-being. As a result, recognizing and categorizing medicinal herbs is critical to providing quality medicines. The goal of this study is to create a model for classifying Philippine medicinal plants based on their leaves. The classifier in the study was built using VGG19. The dataset, which contains forty (40) different species of plants and a total of 4922 leaves, yielded an average accuracy of 92.67%. Furthermore, because data is limited, data augmentation is used to reduce computational burden and increase variance in training data. Based on the classifier developed, a prototype application was created to test its accuracy in a real-world scenario. The prototype application correctly identified all 21 leaf species, two out of three leaves from ten classifications, one out of three leaves from eight classifications, and none of three leaves from one category when three (3) leaf samples were tested for each category. According to the findings of the study, the classification of the Philippine medicinal plants could be done using a convolutional neural network using VGG19.  © 2023 IEEE.},
	author_keywords = {data augmentation; deep learning; medicinal plant leaf recognition; recognition application; transfer learning; VGG19},
	keywords = {Convolutional neural networks; Plants (botany); Software prototyping; Transfer learning; Data augmentation; Deep learning; Leaf recognition; Medicinal plant leaf recognition; Medicinal plants; Philippines; Plant leaves; Recognition application; Transfer learning; VGG19; Deep learning},
	correspondence_address = {E.F. Custodio; Mindoro State University, College of Computer Studies, Calapan, Philippines; email: epiefcustodio@gmail.com.ph},
	editor = {Chakrabarti S. and Paul R.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835033761-7},
	language = {English},
	abbrev_source_title = {IEEE World AI IoT Congr., AIIoT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2023 IEEE World AI IoT Congress, AIIoT 2023; Conference date: 7 June 2023 through 10 June 2023; Conference code: 190676}
}

@ARTICLE{Campos-Leal20222330,
	author = {Campos-Leal, Juan Augusto and Yee-Rendon, Arturo and Vega-Lopez, Ines Fernando},
	title = {Simplifying VGG-16 for Plant Species Identification},
	year = {2022},
	journal = {IEEE Latin America Transactions},
	volume = {20},
	number = {11},
	pages = {2330 – 2338},
	doi = {10.1109/TLA.2022.9904757},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139441665&doi=10.1109%2fTLA.2022.9904757&partnerID=40&md5=e5b2dec15e326e7f02f5c014f76c1740},
	affiliations = {Posgrado en Ciencias de la Informacion, Universidad Autonoma de Sinaloa, Culiacan, Mexico; Facultad de Informatica Culiacan, Universidad Autonoma de Sinaloa, Culiacan, Mexico; Parque de Innovacion Tecnologica, Universidad Autonoma de Sinaloa, Culiacan, Mexico},
	abstract = {Plant species identification represents an extraordinary challenge for machine learning due to visual interspecies similarities and large intraspecies variations. Furthermore, research literature reports that plant species identification usually lacks sufficiently large datasets for training classification models. In this paper, we address this problem with a model that simplifies the VGG-16 architecture, the N-VGG model. The idea behind N-VGG is to reduce experimentally observed overfitting on VGG-16 by using as few trainable parameters as possible. To do this, we substitute the flattening layer on the VGG architecture with a global average pooling layer. This reduces the size of the feature vector. In addition, we eliminate one of the two fully-connected layers and use a new hyper-parameter, N, to indicate the number of nodes on the remaining layer. To show the robustness of the N-VGG model, we conducted extensive experimentation. We trained N-VGG on five datasets for plant species identification. Four of these datasets are publicly available and have been widely used as benchmarks for plant identification models. For all datasets, we compare the accuracy of N-VGG to that of the VGG-16, Inception-v4, and EfficienNet-B3 models. The experimental results show that the N-VGG model achieved the best classification performance for all but one datasets, whereas all the models showed a remarkable performance for the remaining dataset. This evidence supports our initial idea that, for plant species classification, some accuracy might be lost due to overfitting and that having fewer trainable parameters helps in producing a more robust model.  © 2003-2012 IEEE.},
	author_keywords = {Convolutional Neural Network; Deep Learning; Fine-grained Classification; Plant Species Identification; VGG-16},
	keywords = {Deep neural networks; Large dataset; Network architecture; Convolutional neural network; Deep learning; Fine grained; Fine-grained classification; Intraspecies variation; Large datasets; Machine-learning; Overfitting; Plant species identification; VGG-16; Classification (of information)},
	publisher = {IEEE Computer Society},
	issn = {15480992},
	language = {English},
	abbrev_source_title = {IEEE. Lat. Am. Trans.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Li2022,
	author = {Li, Yang and Bao, Zhiyuan and Qi, Jiangtao},
	title = {Seedling maize counting method in complex backgrounds based on YOLOV5 and Kalman filter tracking algorithm},
	year = {2022},
	journal = {Frontiers in Plant Science},
	volume = {13},
	doi = {10.3389/fpls.2022.1030962},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142263313&doi=10.3389%2ffpls.2022.1030962&partnerID=40&md5=01b80b2cbd7a7b9f4ab67bf26f67e5d5},
	affiliations = {Key Laboratory of Bionic Engineering, Ministry of Education, Jilin University, Changchun, China; College of Biological and Agricultural Engineering, Jilin University, Changchun, China; Key Laboratory of Tea Quality and Safety Control, Ministry of Agriculture and Rural Affairs, Tea Research Institute, Chinese Academy of Agricultural Sciences, Hangzhou, China},
	abstract = {Maize population density is one of the most essential factors in agricultural production systems and has a significant impact on maize yield and quality. Therefore, it is essential to estimate maize population density timely and accurately. In order to address the problems of the low efficiency of the manual counting method and the stability problem of traditional image processing methods in the field complex background environment, a deep-learning-based method for counting maize plants was proposed. Image datasets of the maize field were collected by a low-altitude UAV with a camera onboard firstly. Then a real-time detection model of maize plants was trained based on the object detection model YOLOV5. Finally, the tracking and counting method of maize plants was realized through Hungarian matching and Kalman filtering algorithms. The detection model developed in this study had an average precision mAP@0.5 of 90.66% on the test dataset, demonstrating the effectiveness of the SE-YOLOV5m model for maize plant detection. Application of the model to maize plant count trials showed that maize plant count results from test videos collected at multiple locations were highly correlated with manual count results (R2 = 0.92), illustrating the accuracy and validity of the counting method. Therefore, the maize plant identification and counting method proposed in this study can better achieve the detection and counting of maize plants in complex backgrounds and provides a research basis and theoretical basis for the rapid acquisition of maize plant population density. Copyright © 2022 Li, Bao and Qi.},
	author_keywords = {counting prediction; maize plants; object detection; video tracking; YOLOv5},
	correspondence_address = {J. Qi; Key Laboratory of Bionic Engineering, Ministry of Education, Jilin University, Changchun, China; email: qijiangtao@jlu.edu.cn},
	publisher = {Frontiers Media S.A.},
	issn = {1664462X},
	language = {English},
	abbrev_source_title = {Front. Plant Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Mustafa202312065,
	author = {Mustafa, Hassan and Umer, Muhammad and Hafeez, Umair and Hameed, Ahmad and Sohaib, Ahmed and Ullah, Saleem and Madni, Hamza Ahmad},
	title = {Pepper bell leaf disease detection and classification using optimized convolutional neural network},
	year = {2023},
	journal = {Multimedia Tools and Applications},
	volume = {82},
	number = {8},
	pages = {12065 – 12080},
	doi = {10.1007/s11042-022-13737-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138230763&doi=10.1007%2fs11042-022-13737-8&partnerID=40&md5=a5f4a73aca502e1a96445aa23ea40957},
	affiliations = {Department of Computer Engineering, Khwaja Fareed University of Engineering and Information Technology, Rahim Yar Khan, Pakistan; Department of Computer Science, The Islamia University of Bahawalpur, Bahawalpur, Pakistan; Department of Computer Science, Khwaja Fareed University of Engineering and Information Technology, Rahim Yar Khan, Pakistan},
	abstract = {Agriculture production plays a significant role in the country’s economy. Diseases are quite natural and common among plants. Identification of diseases in plants is necessary for averting losses in the yield of agricultural products. Manual monitoring of plants requires expertise, immense effort, and excessive time. Automatic detection will not only help in reducing time and effort but will also help in detecting disease at an early stage, as soon as it will start appearing on plant leaves. Recently, image processing in agriculture has attained a surge of interest by researchers. This study presents a five-layered CNN model for automatic detection of plant disease utilizing leaf images. In order to better train a CNN model, 20,000 augmented images are generated. Experimental results demonstrate that proposed optimized-CNN model can predict pepper bell plant leaf as healthy or bacterial with 99.99% accuracy. Robust results make the proposed optimized-CNN model a preliminary warning tool that can be applied as a disease identification system in a real cultivation environment. [Figure not available: see fulltext.] © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Deep learning; Image classification; Leaf disease; Optimized convolutional neural network},
	keywords = {Agricultural products; Bells; Convolution; Convolutional neural networks; Cultivation; Deep learning; Plants (botany); Automatic Detection; CNN models; Convolutional neural network; Deep learning; Disease classification; Images classification; Leaf disease; Leaf disease detections; Optimized convolutional neural network; Plant leaves; Image classification},
	correspondence_address = {M. Umer; Department of Computer Science, The Islamia University of Bahawalpur, Bahawalpur, Pakistan; email: umersabir1996@gmail.com},
	publisher = {Springer},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Cui2023,
	author = {Cui, Liang and Chen, Shengbo and Mu, Yongling and Xu, Xitong and Zhang, Bin and Zhao, Xiuying},
	title = {Tree Species Classification over Cloudy Mountainous Regions by Spatiotemporal Fusion and Ensemble Classifier},
	year = {2023},
	journal = {Forests},
	volume = {14},
	number = {1},
	doi = {10.3390/f14010107},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146746261&doi=10.3390%2ff14010107&partnerID=40&md5=cc58e39441d8630721b5f50f16d3cbd5},
	affiliations = {College of Geo-Exploration Science and Technology, Jilin University, Changchun, 130026, China; Flight Research Institute, Air Force Aviation University, Changchun, 130022, China},
	abstract = {Accurate mapping of tree species is critical for the sustainable development of the forestry industry. However, the lack of cloud-free optical images makes it challenging to map tree species accurately in cloudy mountainous regions. In order to improve tree species identification in this context, a classification method using spatiotemporal fusion and ensemble classifier is proposed. The applicability of three spatiotemporal fusion methods, i.e., the spatial and temporal adaptive reflectance fusion model (STARFM), the flexible spatiotemporal data fusion (FSDAF), and the spatial and temporal nonlocal filter-based fusion model (STNLFFM), in fusing MODIS and Landsat 8 images was investigated. The fusion results in Helong City show that the STNLFFM algorithm generated the best fused images. The correlation coefficients between the fusion images and actual Landsat images on May 28 and October 19 were 0.9746 and 0.9226, respectively, with an average of 0.9486. Dense Landsat-like time series at 8-day time intervals were generated using this method. This time series imagery and topography-derived features were used as predictor variables. Four machine learning methods, i.e., K-nearest neighbors (KNN), random forest (RF), artificial neural networks (ANNs), and light gradient boosting machine (LightGBM), were selected for tree species classification in Helong City, Jilin Province. An ensemble classifier combining these classifiers was constructed to further improve the accuracy. The ensemble classifier consistently achieved the highest accuracy in almost all classification scenarios, with a maximum overall accuracy improvement of approximately 3.4% compared to the best base classifier. Compared to only using a single temporal image, utilizing dense time series and the ensemble classifier can improve the classification accuracy by about 20%, and the overall accuracy reaches 84.32%. In conclusion, using spatiotemporal fusion and the ensemble classifier can significantly enhance tree species identification in cloudy mountainous areas with poor data availability. © 2023 by the authors.},
	author_keywords = {image fusion; Landsat 8 OLI; machine learning; MODIS; time series; tree species mapping},
	keywords = {Accuracy; Classification; Forestry; Fusion; Mapping; Neural Networks; Topography; Trees; China; Helong; Jilin; Adaptive boosting; Forestry; Image classification; Image enhancement; Image fusion; Machine learning; Mapping; Nearest neighbor search; Neural networks; Radiometers; Time series; Topography; Ensemble-classifier; LANDSAT; Landsat 8 OLI; Machine-learning; MODIS; Spatio-temporal fusions; Species mapping; Times series; Tree species; Tree species mapping; artificial neural network; classification; cloud; Landsat; machine learning; mapping; MODIS; mountain region; spatiotemporal analysis; species inventory; sustainable development; time series; tree; Landsat},
	correspondence_address = {S. Chen; College of Geo-Exploration Science and Technology, Jilin University, Changchun, 130026, China; email: chensb@jlu.edu.cn},
	publisher = {MDPI},
	issn = {19994907},
	language = {English},
	abbrev_source_title = {Forests},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Li2023,
	author = {Li, Jia-Le and Su, Wen-Hao and Zhang, He-Yi and Peng, Yankun},
	title = {A real-time smart sensing system for automatic localization and recognition of vegetable plants for weed control},
	year = {2023},
	journal = {Frontiers in Plant Science},
	volume = {14},
	doi = {10.3389/fpls.2023.1133969},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152585198&doi=10.3389%2ffpls.2023.1133969&partnerID=40&md5=51dffae464217196267941148eff831b},
	affiliations = {College of Engineering, China Agricultural University, Haidian, Beijing, China},
	abstract = {Tomato is a globally grown vegetable crop with high economic and nutritional values. Tomato production is being threatened by weeds. This effect is more pronounced in the early stages of tomato plant growth. Thus weed management in the early stages of tomato plant growth is very critical. The increasing labor cost of manual weeding and the negative impact on human health and the environment caused by the overuse of herbicides are driving the development of smart weeders. The core task that needs to be addressed in developing a smart weeder is to accurately distinguish vegetable crops from weeds in real time. In this study, a new approach is proposed to locate tomato and pakchoi plants in real time based on an integrated sensing system consisting of camera and color mark sensors. The selection scheme of reference, color, area, and category of plant labels for sensor identification was examined. The impact of the number of sensors and the size of the signal tolerance region on the system recognition accuracy was also evaluated. The experimental results demonstrated that the color mark sensor using the main stem of tomato as the reference exhibited higher performance than that of pakchoi in identifying the plant labels. The scheme of applying white topical markers on the lower main stem of the tomato plant is optimal. The effectiveness of the six sensors used by the system to detect plant labels was demonstrated. The computer vision algorithm proposed in this study was specially developed for the sensing system, yielding the highest overall accuracy of 95.19% for tomato and pakchoi localization. The proposed sensor-based system is highly accurate and reliable for automatic localization of vegetable plants for weed control in real time. Copyright © 2023 Li, Su, Zhang and Peng.},
	author_keywords = {automated weeding; computer vision; crop signalling; plant identification; precision agriculture},
	correspondence_address = {W.-H. Su; College of Engineering, China Agricultural University, Beijing, Haidian, China; email: wenhao.su@cau.edu.cn},
	publisher = {Frontiers Media S.A.},
	issn = {1664462X},
	language = {English},
	abbrev_source_title = {Front. Plant Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Rajkomar202376,
	author = {Rajkomar, Gandhinee and Pudaruth, Sameerchand},
	title = {A Mobile App for the Identification of Flowers Using Deep Learning},
	year = {2023},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {14},
	number = {5},
	pages = {76 – 102},
	doi = {10.14569/IJACSA.2023.0140508},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161237782&doi=10.14569%2fIJACSA.2023.0140508&partnerID=40&md5=293f52fd89572a8362391ff5a96da4ec},
	affiliations = {ICT Department, FoICDT, University of Mauritius, Mauritius},
	abstract = {Flowers are admired and used by people all around the world for their fragrance, religious significance, and medicinal capabilities. The accurate taxonomy of these flower species is critical for biodiversity conservation and research. Non-experts typically need to spend a lot of time examining botanical guides in order to accurately identify a flower, which can be challenging and time-consuming. In this study, an innovative mobile application named FloralCam has been developed for the identification of flower species that are commonly found in Mauritius. Our dataset, named FlowerNet, was collected using a smartphone in a natural environment setting and consists of 11660 images, with 110 images for each of the 106 flower species. Seventy percent of the data was used for training, twenty percent for validation and the remaining ten percent for testing. Using the approach of transfer learning, pre-trained convolutional neural networks (CNNs) such as the InceptionV3, MobileNetV2 and ResNet50V2 were fine tuned on the custom dataset created. The best performance was achieved with the fine tuned MobileNetV2 model with accuracy 99.74% and prediction time 0.09 seconds. The best model was then converted to TensorFlow Lite format and integrated in a mobile application which was built using Flutter. Furthermore, the models were also tested on the benchmark Oxford 102 dataset and MobileNetV2 obtained the highest classification accuracy of 95.90%. The mobile application, the dataset and the deep learning models developed can be used to support future research in the field of flower recognition. © 2023, International Journal of Advanced Computer Science and Applications. All Rights Reserved.},
	author_keywords = {deep learning; Flowers; Mauritius; mobile application},
	keywords = {Biodiversity; Classification (of information); Conservation; Convolutional neural networks; E-learning; Learning systems; Mobile computing; Transfer learning; Biodiversity conservation; Convolutional neural network; Deep learning; Flower; Mauritius; Mobile app; Mobile applications; Natural environments; Smart phones; Transfer learning; Deep learning},
	publisher = {Science and Information Organization},
	issn = {2158107X},
	language = {English},
	abbrev_source_title = {Intl. J. Adv.  Comput. Sci. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Batchuluun202210474,
	author = {Batchuluun, Ganbayar and Nam, Se Hyun and Park, Kang Ryoung},
	title = {Deep learning-based plant classification and crop disease classification by thermal camera},
	year = {2022},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	volume = {34},
	number = {10},
	pages = {10474 – 10486},
	doi = {10.1016/j.jksuci.2022.11.003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142539871&doi=10.1016%2fj.jksuci.2022.11.003&partnerID=40&md5=f56f93bb2f471a14026f4be8b2b56b93},
	affiliations = {Division of Electronics and Electrical Engineering, Dongguk University, 30 Pildong-ro 1-gil, Jung-gu, Seoul, 04620, South Korea},
	abstract = {Studies regarding image classification based on plant and crop disease images that were acquired using a visible light camera have been conducted in the past, whereas those based on thermal images are limited. This is because the thermal images are blurry due to the nature of the thermal camera, which makes it extremely difficult to classify objects. Therefore, this study proposes a new plant and crop disease classification method based on thermal images. The proposed method used a convolutional neural network with explainable artificial intelligence (XAI) to improve plant and crop disease classification performance. A new thermal plant image dataset was built for conducting the experiments, which contained 4,720 various images of flowers and leaves. In addition, an open database of crop diseases was also used, such as the Paddy crop dataset. The proposed plant and crop disease classification method demonstrated a 98.55% accuracy for the thermal plant image dataset and a 90.04% accuracy for the Paddy crop dataset, both of which outperformed other existing methods. © 2022 The Author(s)},
	author_keywords = {Convolutional neural network; Crop disease image; Explainable artificial intelligence; Plant image classification; Thermal image},
	correspondence_address = {K.R. Park; Division of Electronics and Electrical Engineering, Dongguk University, Seoul, 30 Pildong-ro 1-gil, Jung-gu, 04620, South Korea; email: parkgr@dongguk.edu},
	publisher = {King Saud bin Abdulaziz University},
	issn = {13191578},
	language = {English},
	abbrev_source_title = { J. King Saud Univ. - Comput. Inform. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}@ARTICLE{Kumar2022,
	author = {Kumar, Amit Krishan and Mai, Nguyễn Ngọc and Kumar, Ashmit and Chand, Nividita V. and Assaf, Mansour H.},
	title = {Quantum classifier for recognition and identification of leaf profile features},
	year = {2022},
	journal = {European Physical Journal D},
	volume = {76},
	number = {6},
	doi = {10.1140/epjd/s10053-022-00429-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132995209&doi=10.1140%2fepjd%2fs10053-022-00429-z&partnerID=40&md5=3b45c82a68f9d71ce374f33b2b2918f6},
	affiliations = {State Key Laboratory of Intelligent Control and Decision of Complex Systems, School of Automation, Beijing Institute of Technology, Beijing, 100081, China; Nguyễn Tất Thành University, District 12, Ho Chi Minh, Viet Nam; College of Agriculture, Fisheries and Forestry, Fiji National University, Koronivia, Suva, Fiji; School of Information Technology, Engineering, Mathematics and Physics, The University of the South Pacific, Suva, Fiji},
	abstract = {Quantum-based classifiers and architecture are gaining lots of attention in image representation and cryptography. The proposed algorithm applies a quantum classifier to a computer vision system for leaf recognition which can be applied to a quantum computer. Images from ten species of leaves which are categorised into two groups, namely simple and palmately, are recognised using a quantum classifier. The pixels of images are transformed to qubit states using quantum Fourier transform (QFT) and Hadamard gates. The profile and structural features are extracted by applying 1D-convolution and controlled not (CNOT) gates. A quantum nearest neighbour search classifier is used to find the closest matching leaf based on probability. The results for different levels of image processing are evaluated and compared with the nearest neighbour classifier. The recognition rate of the quantum classifier for the best level of image processing is 97.33%. The recognition rate of the classifier is better than the nearest neighbour classifier and also has a low computation time. Graphical abstract: [Figure not available: see fulltext.]. © 2022, The Author(s), under exclusive licence to EDP Sciences, SIF and Springer-Verlag GmbH Germany, part of Springer Nature.},
	keywords = {Classification (of information); Image classification; Image representation; Nearest neighbor search; Quantum computers; Computer vision system; Image cryptographies; Image representations; Images processing; Leaf recognition; Nearest Neighbor classifier; Profile features; Quanta computers; Qubit state; Simple++; Quantum theory},
	correspondence_address = {A.K. Kumar; State Key Laboratory of Intelligent Control and Decision of Complex Systems, School of Automation, Beijing Institute of Technology, Beijing, 100081, China; email: fste_11@yahoo.com; N.N. Mai; Nguyễn Tất Thành University, Ho Chi Minh, District 12, Viet Nam; email: nnmai@ntt.edu.vn},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {14346060},
	coden = {EPJDF},
	language = {English},
	abbrev_source_title = {Eur. Phys. J. D},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Perrin2022,
	author = {Perrin, Jackson E. and Jernigan, Shaphan R. and Thayer, Jacob D. and Howell, Andrew W. and Leary, James K. and Buckner, Gregory D.},
	title = {Sensor Fusion with Deep Learning for Autonomous Classification and Management of Aquatic Invasive Plant Species},
	year = {2022},
	journal = {Robotics},
	volume = {11},
	number = {4},
	doi = {10.3390/robotics11040068},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133530156&doi=10.3390%2frobotics11040068&partnerID=40&md5=e51f032466bb8b7707d00e2cf2da9859},
	affiliations = {Mechanical and Aerospace Engineering, North Carolina State University, Raleigh, 27695, NC, United States; Center for Aquatic and Invasive Plants, University of Florida, Gainesville, 32653, FL, United States; Crop and Soil Sciences, North Carolina State University, Raleigh, 27695, NC, United States},
	abstract = {Recent advances in deep learning, including the development of AlexNet, Residual Network (ResNet), and transfer learning, offer unprecedented classification accuracy in the field of machine vision. A developing application of deep learning is the automated identification and management of aquatic invasive plants. Classification of submersed aquatic vegetation (SAV) presents a unique challenge, namely, the lack of a single source of sensor data that can produce robust, interpretable images across a variable range of depth, turbidity, and lighting conditions. This paper focuses on the development of a multi‐sensor (RGB and hydroacoustic) classification system for SAV that is robust to environmental conditions and combines the strengths of each sensing modality. The detection of invasive Hydrilla verticillata (hydrilla) is the primary goal. Over 5000 aerial RGB and hydroacoustic images were generated from two Florida lakes via an unmanned aerial vehicle and boat‐mounted sonar unit, and tagged for neural network training and evaluation. Classes included “HYDR,” containing hydrilla; “NONE”, lacking SAV, and “OTHER,” containing SAV other than hydrilla. Using a transfer learning approach, deep neural networks with the ResNet architecture were individually trained on the RGB and hydroacoustic datasets. Multiple data fusion methodologies were evaluated to ensemble the outputs of these neural networks for optimal classification accuracy. A method incorporating logic and a Monte Carlo dropout approach yielded the best overall classification accuracy (84%), with recall and precision of 84.5% and 77.5%, respectively, for the hydrilla class. The training and ensembling approaches were repeated for a DenseNet model with identical training and testing datasets. The overall classification accuracy was similar between the ResNet and DenseNet models when averaged across all approaches (1.9% higher accuracy for the ResNet vs. the DenseNet). © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {aquatic invasive plants; autonomous robotics; deep learning; sensor fusion},
	correspondence_address = {G.D. Buckner; Mechanical and Aerospace Engineering, North Carolina State University, Raleigh, 27695, United States; email: gbuckner@ncsu.edu},
	publisher = {MDPI},
	issn = {22186581},
	language = {English},
	abbrev_source_title = {Robotics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Araújo2022427,
	author = {Araújo, Voncarlos M. and Britto Jr., Alceu S. and Oliveira, Luiz S. and Koerich, Alessandro L.},
	title = {Two-view fine-grained classification of plant species},
	year = {2022},
	journal = {Neurocomputing},
	volume = {467},
	pages = {427 – 441},
	doi = {10.1016/j.neucom.2021.10.015},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118740826&doi=10.1016%2fj.neucom.2021.10.015&partnerID=40&md5=cb3c714d757904af15d9847fd46cacb7},
	affiliations = {Pontifical Catholic University of Paraná (PUCPR), Curitiba, PR, Brazil; State University of Ponta Grossa (UEPG), Ponta Grossa, PR, Brazil; Federal University of Paraná (UFPR), Curitiba, PR, Brazil; École de Technologie Supérieure (ÉTS), Université du Québec, Montréal, QC, Canada},
	abstract = {Automatic plant classification is challenging due to the vast biodiversity of the existing plant species in a fine-grained scenario. Robust deep learning architectures have been used to improve the classification performance in such a fine-grained problem but usually build models that are highly dependent on a large training dataset and are not scalable. This paper proposes a novel method based on a two-view leaf image representation and a hierarchical classification strategy for fine-grained plant species recognition. It uses the botanical taxonomy as a basis for a coarse-to-fine strategy applied to identify the plant genus and species. The two-view representation provides complementary global and local features of leaf images. A deep metric based on Siamese Convolutional Neural Networks is used to reduce the dependence on many training samples and make the method scalable to new plant species. The experimental results on two challenging fine-grained datasets of leaf images (i.e., PlantCLEF 2015 and LeafSnap) have shown the proposed method's effectiveness, which achieved recognition accuracy of 0.87 and 0.96, respectively. © 2021 Elsevier B.V.},
	author_keywords = {Deep metrics; Fine-grained classification; Plant species recognition; Siamese neural network},
	keywords = {Biodiversity; Convolutional neural networks; Deep learning; Image representation; Large dataset; Deep metric; Fine grained; Fine-grained classification; Leaf images; Neural-networks; Plant species; Plant species recognition; Siamese neural network; Species recognition; Two views; article; convolutional neural network; plant leaf; Classification (of information)},
	correspondence_address = {V.M. Araújo; Pontifical Catholic University of Paraná (PUCPR), Curitiba, Brazil; email: voncarlos.araujo@gmail.com},
	publisher = {Elsevier B.V.},
	issn = {09252312},
	coden = {NRCGE},
	language = {English},
	abbrev_source_title = {Neurocomputing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Green Open Access}
}

@ARTICLE{Joly2022257,
	author = {Joly, Alexis and Goëau, Hervé and Kahl, Stefan and Picek, Lukáš and Lorieul, Titouan and Cole, Elijah and Deneu, Benjamin and Servajean, Maximilien and Durso, Andrew and Glotin, Hervé and Planqué, Robert and Vellinga, Willem-Pier and Navine, Amanda and Klinck, Holger and Denton, Tom and Eggel, Ivan and Bonnet, Pierre and Šulc, Milan and Hrúz, Marek},
	title = {Overview of LifeCLEF 2022: An Evaluation of Machine-Learning Based Species Identification and Species Distribution Prediction},
	year = {2022},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {13390 LNCS},
	pages = {257 – 285},
	doi = {10.1007/978-3-031-13643-6_19},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136990366&doi=10.1007%2f978-3-031-13643-6_19&partnerID=40&md5=419666145d8f0f836096ce181e180a2e},
	affiliations = {Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France; CIRAD, UMR AMAP, Montpellier, Occitanie, France; Univ. Toulon, Aix Marseille Univ., CNRS, LIS, DYNI team, Marseille, France; Xeno-canto Foundation, Amsterdam, Netherlands; HES-SO, Sierre, Switzerland; KLYCCB, Cornell Lab of Ornithology, Cornell University, Ithaca, United States; LIRMM, AMI, Univ Paul Valéry Montpellier, Univ Montpellier, CNRS, Montpellier, France; Department of Computing and Mathematical Sciences, Caltech, Pasadena, United States; Department of Cybernetics, FAV, University of West Bohemia, Pilsen, Czech Republic; Department of Biological Sciences, Florida Gulf Coast University, Fort Myers, United States; Google LLC, San Francisco, United States; Rossum.ai, Prague, Czech Republic; Listening Observatory for Hawaiian Ecosystems, Univ. of Hawai’i at Hilo, Hilo, United States},
	abstract = {Building accurate knowledge of the identity, the geographic distribution and the evolution of species is essential for the sustainable development of humanity, as well as for biodiversity conservation. However, the difficulty of identifying plants, animals and fungi is hindering the aggregation of new data and knowledge. Identifying and naming living organisms is almost impossible for the general public and is often difficult even for professionals and naturalists. Bridging this gap is a key step towards enabling effective biodiversity monitoring systems. The LifeCLEF campaign, presented in this paper, has been promoting and evaluating advances in this domain since 2011. The 2022 edition proposes five data-oriented challenges related to the identification and prediction of biodiversity: (i) PlantCLEF: very large-scale plant identification, (ii) BirdCLEF: bird species recognition in audio soundscapes, (iii) GeoLifeCLEF: remote sensing based prediction of species, (iv) SnakeCLEF: snake species identification on a global scale, and (v) FungiCLEF: fungi recognition as an open set classification problem. This paper overviews the motivation, methodology and main outcomes of that five challenges. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	keywords = {Biodiversity; Conservation; Fungi; Geographical distribution; Machine learning; Population distribution; Remote sensing; Biodiversity conservation; Biodiversity monitoring; General publics; Geographics; Large-scales; Living organisms; Machine-learning; Monitoring system; Species distributions; Species identification; Forecasting},
	correspondence_address = {A. Joly; Department of Computing and Mathematical Sciences, Caltech, Pasadena, United States; email: alexis.joly@inria.fr},
	editor = {Barrón-Cedeño A. and Da San Martino G. and Faggioli G. and Ferro N. and Degli Esposti M. and Sebastiani F. and Macdonald C. and Pasi G. and Hanbury A. and Potthast M.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303113642-9},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26; Conference name: 13th International Conference of the Cross-Language Evaluation Forum for European Languages, CLEF 2022; Conference date: 5 September 2022 through 8 September 2022; Conference code: 282409; All Open Access, Green Open Access}
}

@CONFERENCE{Valdez2022,
	author = {Valdez, Daryl B. and Aliac, Chris Jordan G. and Feliscuzo, Larmie S.},
	title = {Medicinal Plant Classification using Convolutional Neural Network and Transfer Learning},
	year = {2022},
	journal = {4th IEEE International Conference on Artificial Intelligence in Engineering and Technology, IICAIET 2022},
	doi = {10.1109/IICAIET55139.2022.9936868},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142303623&doi=10.1109%2fIICAIET55139.2022.9936868&partnerID=40&md5=f8975bbb7811bbf14fe7aa013123ec94},
	affiliations = {College of Computer Studies, Cebu Institute of Technology-University, Cebu City, Philippines},
	abstract = {Medicinal plants are not only an essential source of therapeutic compounds but also an alternative source of medications used by most people around the world. Due to recent advances in computer vision, plant identification from images has become a rapidly developing research field. Various results showed good accuracy, precision, and real-world applications. This paper aimed to investigate an accurate and precise automated identification of medicinal plants. We present a new medicinal plant dataset containing images of ten (10) classes of medicinal plant species and one (1) class containing a mixed variety of weeds, vines, and non-medicinal plants. Then we proposed a model based on MobileNetV3 architecture for a low-cost, reliable, and efficient medicinal plant classification. Using the proposed model and Transfer Learning, results revealed a 97.43% accuracy on the challenging task. Overall, the findings revealed the feasibility of an efficient and reliable medicinal plant classifier for real-world applications.  © 2022 IEEE.},
	author_keywords = {Deep Learning; Image Classification; Plant Identification; Plants Dataset},
	keywords = {Classification (of information); Convolutional neural networks; Deep learning; Plants (botany); Transfer learning; Convolutional neural network; Deep learning; Images classification; Medicinal plants; Neural network learning; Plant classification; Plant dataset; Plant identification; Real-world; Transfer learning; Image classification},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166546837-4},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Artif. Intell. Eng. Technol., IICAIET},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 4th IEEE International Conference on Artificial Intelligence in Engineering and Technology, IICAIET 2022; Conference date: 13 September 2022 through 15 September 2022; Conference code: 184212}
}

@ARTICLE{Li2022,
	author = {Li, Cheng and Li, Ming and Zhu, Xinghui and Chen, Yineng and Wu, Yanbin and Deng, Nan and Fang, Kui},
	title = {Identification Method of Grape Leaf Diseases Based on Improved CCT Model},
	year = {2022},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	doi = {10.1142/S0218001422500379},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136543867&doi=10.1142%2fS0218001422500379&partnerID=40&md5=3ddf0afdedc2395cf4cb0d82c058de9d},
	affiliations = {College of Information and Intelligence, Hunan Agricultural University, Changsha, 410128, China; Hunan Agricultural Equipment Research Institute, Changsha, 410129, China; Network Security and Information Technology Center, Commerce and Tourism College, Changsha, 410116, China},
	abstract = {Grape is an important cash crop that is susceptible to diseases when growing, resulting in lower yield and quality. In recent years, transformers have achieved excellent performance in a variety of natural language processing and image recognition tasks through the self-attention mechanism. Therefore, this paper proposes a grape leaf disease recognition model named Dense Convolutional Transformer (DensCT). The compact convolutional transformer (CCT) is used as the backbone in this model, which improves the convolutional module of the original model by introducing densely connected modules, enhancing the transfer and reuse of features between networks. This also modifies the single-scale feature extraction method of the original model to multi-scale, which improves the feature extraction performance. Finally, the model was trained on two small-scale datasets from scratch, and the recognition accuracy of the final model on the test sets reached 89.19% and 93.92%. Compared with CCT, DenseNet121, ResNet50, MobileNetV3 and ViT, the recognition accuracy improved by 4.73%, 3.38%, 10.81%, 0.68% and 18.24% on the first dataset and 6.08%, 5.41%, 1.35%, 3.38% and 12.84% on the second dataset. The experimental results show that the proposed model can effectively identify grape leaf diseases, which can provide a reference for building disease leaf recognition models on small-scale datasets.  © 2022 World Scientific Publishing Company.},
	author_keywords = {densely connected; grape leaf diseases recognition; self-attention; Transformer},
	correspondence_address = {X. Zhu; College of Information and Intelligence, Hunan Agricultural University, Changsha, 410128, China; email: zhuxh@hunau.edu.cn},
	publisher = {World Scientific},
	issn = {02180014},
	coden = {IJPIE},
	language = {English},
	abbrev_source_title = {Int J Pattern Recognit Artif Intell},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Goëau20221916,
	author = {Goëau, Hervé and Bonnet, Pierre and Joly, Alexis},
	title = {Overview of PlantCLEF 2022: Image-based plant identification at global scale},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3180},
	pages = {1916 – 1928},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136913046&partnerID=40&md5=cf11c9831eecdc0ed4fa3e7e32701179},
	affiliations = {CIRAD, UMR AMAP, Occitanie, Montpellier, France; Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France},
	abstract = {It is estimated that there are more than 300,000 species of vascular plants in the world. Increasing our knowledge of these species is of paramount importance for the development of human civilization (agriculture, construction, pharmacopoeia, etc.), especially in the context of the biodiversity crisis. However, the burden of systematic plant identification by human experts strongly penalizes the aggregation of new data and knowledge. Since then, automatic identification has made considerable progress in recent years as highlighted during all previous editions of PlantCLEF. Deep learning techniques now seem mature enough to address the ultimate but realistic problem of global identification of plant biodiversity in spite of many problems that the data may present (a huge number of classes, very strongly unbalanced classes, partially erroneous identifications, duplications, variable visual quality, diversity of visual contents such as photos or herbarium sheets, etc). The PlantCLEF2022 challenge edition proposes to take a step in this direction by tackling a multi-image (and metadata) classification problem with a very large number of classes (80k plant species). This paper presents the resources and evaluations of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of key findings. © 2022 Copyright for this paper by its authors.},
	author_keywords = {benchmark; biodiversity informatics; evaluation; fine-grained classification; LifeCLEF; species identification},
	keywords = {Automation; Deep learning; Image classification; Benchmark; Biodiversity informatic; Evaluation; Fine grained; Fine-grained classification; Informatics; LifeCLEF; Number of class; Plant identification; Species identification; Biodiversity},
	correspondence_address = {H. Goëau; CIRAD, UMR AMAP, Montpellier, Occitanie, France; email: herve.goeau@cirad.fr},
	editor = {Faggioli G. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Ferro N. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Hanbury A. and Vienna University of Technology, Favoritenstrasse 9, Vienna and Potthast M. and University of Leipzig, Augustusplatz 10, Leipzig},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2022 Conference and Labs of the Evaluation Forum, CLEF 2022; Conference date: 5 September 2022 through 8 September 2022; Conference code: 181762}
}

@ARTICLE{Wu2022,
	author = {Wu, Tsan-Yu and Yeh, Kuan-Ting and Hsu, Hao-Chun and Yang, Chih-Kai and Tsai, Ming-Jer and Kuo, Yan-Fu},
	title = {Identifying Fagaceae and Lauraceae species using leaf images and convolutional neural networks},
	year = {2022},
	journal = {Ecological Informatics},
	volume = {68},
	doi = {10.1016/j.ecoinf.2021.101513},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121274165&doi=10.1016%2fj.ecoinf.2021.101513&partnerID=40&md5=0753852db11ec4efa64831ecb540149e},
	affiliations = {Department of Biomechatronics Engineering, National Taiwan University, Taipei, Taiwan; The Experimental Forest, College of Bio-Resources and Agriculture, National Taiwan University, Nantou, Taiwan; Department of Forestry, National Pingtung University of Science and Technology, Pingtung, Taiwan; School of Forestry and Resource Conservation, National Taiwan University, Taipei, Taiwan},
	abstract = {Lauraceae and Fagaceae are two large woody plant families that are predominant in the low- and middle-altitude regions in Taiwan. The highly interspecific similarity between some species of the family brings limitations on the management and utilization. This work proposed an approach for identifying 15 Lauraceae species and 20 Fagaceae species using leaf images and convolutional neural networks (CNNs). Leaf specimens of 35 species were collected from the northern, central, and southern parts of Taiwan. Images of the leaves were acquired using flat-bed scanners. Three CNN architectures—DenseNet-121, MobileNet V2, and Xception—were trained. Xception achieved the highest mean test accuracy of 99.39%, and MobileNet V2 required the shortest mean test time of 17.1 ms per image using a GPU. The saliency maps revealed that the characteristics learned by models matched the leaf features used by botanists. A pruning algorithm, gate decorator, was applied to the trained models for reducing the number of parameters and number of floating-point operations of the MobileNet V2 by 55.4% and 69.1%, respectively, while the model accuracy was maintained at 92.03%. Thus, MobileNet V2 has the potential to be used for identifying the Lauraceae and Fagaceae species on mobile devices. © 2021 Elsevier B.V.},
	author_keywords = {Deep learning; Leaf morphology; Plant identification; Pruning; Saliency map},
	keywords = {Taiwan; artificial island; artificial neural network; dicotyledon; ecological modeling; legume; pruning; woody plant},
	correspondence_address = {Y.-F. Kuo; Department of Biomechatronics Engineering, National Taiwan University, Taipei, No. 1, Sec. 4, Roosevelt Rd., 106, Taiwan; email: ykuo@ntu.edu.tw},
	publisher = {Elsevier B.V.},
	issn = {15749541},
	language = {English},
	abbrev_source_title = {Ecol. Informatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Laxmi2022,
	author = {Laxmi, Scindhiya and Gupta, S.K.},
	title = {Multi-category intuitionistic fuzzy twin support vector machines with an application to plant leaf recognition},
	year = {2022},
	journal = {Engineering Applications of Artificial Intelligence},
	volume = {110},
	doi = {10.1016/j.engappai.2022.104687},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126149413&doi=10.1016%2fj.engappai.2022.104687&partnerID=40&md5=eaa50fcf9ddf8b20d7d01964d6658d43},
	affiliations = {Department of Mathematics, Indian Institute of Technology Roorkee, Roorkee, 247 667, India},
	abstract = {The intuitionistic fuzzy twin support vector machine for multi-categorization is developed in this study, which incorporates both structural and empirical risk concepts. In this method, each training pattern is first aggregated with the appropriate membership and non-membership degrees, which describe the position of a pattern in relation to its class centre and surrounding circumstances in input or feature space, and then the separating hyperplane is constructed using the kernel function and convex quadratic programming. Empirical findings on an artificial and thirteen UCI standard datasets show that it outperforms well-known existing methods including improved support vector machines, K-nearest neighbour, logistic regression, decision trees, random forests, and multilayer perceptrons. Furthermore, the suggested classifier with linear, polynomial, and Gaussian kernels has been used to identify the leaves of various plants, where the shape, texture, and margin data are extracted from the leaf in order to categorize the plant species. The method's generalization capacity is demonstrated by the classification results on two leaf datasets of thirty and one hundred species, respectively. To compare the suggested method's prediction capacity with others, statistical analysis is performed using two non-parametric tests, Friedman and Wilcoxon, with a 5% threshold of significance. The results show that the proposed method yields better performance for both linear and non-linear kernels. © 2022 Elsevier Ltd},
	author_keywords = {Fuzzy set; Kernel function; Machine learning; Plant identification; Support vector machines},
	keywords = {Classification (of information); Fuzzy sets; Logistic regression; Nearest neighbor search; Plants (botany); Quadratic programming; Support vector regression; Textures; Vectors; Empirical risks; Intuitionistic fuzzy; Kernel function; Leaf recognition; Plant identification; Plant leaves; Risk concepts; Structural risks; Support vectors machine; Twin support vector machines; Decision trees},
	correspondence_address = {S.K. Gupta; Department of Mathematics, Indian Institute of Technology Roorkee, Roorkee, 247 667, India; email: s.gupta@ma.iitr.ac.in},
	publisher = {Elsevier Ltd},
	issn = {09521976},
	coden = {EAAIE},
	language = {English},
	abbrev_source_title = {Eng Appl Artif Intell},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Huang20222370,
	author = {Huang, Yanbo and Zhao, Xiaohu and Pan, Zeng and Reddy, Krishna N. and Zhang, Jingcheng},
	title = {Hyperspectral plant sensing for differentiating glyphosate-resistant and glyphosate-susceptible johnsongrass through machine learning algorithms},
	year = {2022},
	journal = {Pest Management Science},
	volume = {78},
	number = {6},
	pages = {2370 – 2377},
	doi = {10.1002/ps.6864},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126883896&doi=10.1002%2fps.6864&partnerID=40&md5=fb804f06b40aefe4d772467609a0fd8e},
	affiliations = {US Department of Agriculture, Agricultural Research Service, Genetics and Sustainable Agriculture Research Unit, MS, United States; Hangzhou Dianzi University, Hangzhou, China; US Department of Agriculture, Agricultural Research Service, Crop Production Systems Research Unit, Stoneville, MS, United States},
	abstract = {BACKGROUND: Johnsongrass (Sorghum halepense) is one of the weeds that evolves resistance to glyphosate [N-(phosphonomethyl)-glycine], the most widely used herbicide, and the weed may cause agronomic troublesome in the southern USA. This paper reports a study on developing a hyperspectral plant sensing approach to explore the spectral features of glyphosate-resistant (GR) and glyphosate-sensitive (GS) plants to evaluate this approach using machine learning algorithms to differentiate between GR and GS plants. RESULTS: On average, GR plants have higher spectral reflectance compared with GS plants. The sensitive spectral bands were optimally selected using the successive projections algorithm respectively wrapped with the machine learning algorithms of k-nearest neighbors (KNN), random forest (RF), and support vector machine (SVM) with Fisher linear discriminant analysis (FLDA) to classify between GS and GS plants. At 3 weeks after transplanting (WAT) KNN and SVM could not acceptably classify the GR and GS plants but they improved significantly with the stages to have their overall accuracies reaching 73% and 77%, respectively, at 5 WAT. RF and FLDA had a better ability to classify the plants at 3 WAT but RF was low in accuracy at 2 WAT while FLDA dropped accuracy to 50% at 4 WAT from 57% at 3 WAT and raised it to 73% at 5 WAT. CONCLUSIONS: Previous studies were conducted developing the hyperspectral imaging approach to differentiate GR Palmer amaranth from GS Palmer amaranth and GR Italian ryegrass from GS Italian ryegrass with classification accuracies of 90% and 80%, respectively. This study demonstrated that the hyperspectral plant sensing approach could be developed to differentiate GR johnsongrass from glyphosate-sensitive GS johnsongrass with the highest classification accuracy of 77%. The comparison with our previous studies indicated that the similar hyperspectral approach could be used and transferred from classification across different GR and GS weed biotypes, such as Palmer amaranth, Italian ryegrass and johnsongrass, so it is highly possible for classification of more other GR and GS weed biotypes as well. On the basis of classic pattern recognition approaches the process of plant classification can be enhanced by modeling using machine learning algorithms. © 2022 Society of Chemical Industry. This article has been contributed to by U.S. Government employees and their work is in the public domain in the USA. © 2022 Society of Chemical Industry. This article has been contributed to by U.S. Government employees and their work is in the public domain in the USA.},
	author_keywords = {glyphosate-resistant weed; hyperspectral plant sensing; johnsongrass; machine learning},
	keywords = {Glycine; Herbicide Resistance; Herbicides; Lolium; Machine Learning; Plant Weeds; Sorghum; United States; glycine; glyphosate; herbicide; discriminant analysis; glyphosate; grass; machine learning; pattern recognition; pesticide resistance; weed; herbicide resistance; Lolium; machine learning; sorghum; weed},
	correspondence_address = {Y. Huang; US Department of Agriculture, Agricultural Research Service, Genetics and Sustainable Agriculture Research Unit, United States; email: yanbo.huang@usda.gov},
	publisher = {John Wiley and Sons Ltd},
	issn = {1526498X},
	coden = {PMSCF},
	pmid = {35254728},
	language = {English},
	abbrev_source_title = {Pest Manage. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Pravin2022233,
	author = {Pravin, A. and Deepa, C.},
	title = {Piper Plant Classification using Deep CNN Feature Extraction and Hyperparameter Tuned Random Forest Classification},
	year = {2022},
	journal = {Transdisciplinary Journal of Engineering and Science},
	volume = {13},
	pages = {233 – 258},
	doi = {10.22545/2022/00202},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136980035&doi=10.22545%2f2022%2f00202&partnerID=40&md5=58d217227c072b030a0663be4a93d880},
	affiliations = {Department of Computer Science, Sri Ramakrishna College of Arts and Science, Coimbatore, India; Department of Computer Science (Artificial Intelligence & Data Science), Sri Ramakrishna College of Arts & Science, Tamilnadu, Coimbatore, India},
	abstract = {The plant has numerous uses in medicine, food, and industry and plays a major role in environmental protection. Hence it is crucial to identify and classify the specific plant species. In the agriculture production and botanical area, plant classification for images of leaves is considered the basic research. Due to the higher dimensionality and nature complexity of leaf image data, various effective algorithms are required to perform the classification of specific plant species. Hence, in this study, al l plant types and specific piper plant types are considered and classified based on the Hyperparameter tuned random forest algorithm due to its effective optimal hyperparameter tuning. Piper plants are selected in this research since they possess significant medicinal applications. Significantly the Deep CNN approach is considered an effective feature extraction of al l plants such as tomato, apple, cherry, and others and also piper plants like piper mulesa, piper nigrum, and others. However, initial ly the effective pre-processing of data augmentation to reduce overfitting and increase the amount of data and feature scaling for data features normalization are established. The experimental results show that the proposed hyperparameter tuned random forest classifier shows better results of showing an accuracy value of 0.94 for al l plants and 0.88 value for piper plant compared with other machine learning algorithms like SVM, naïve Bayes, and Logistic regression. © 2022, ATLAS. All rights reserved.},
	author_keywords = {Data mining; deep CNN; hyper parameter tuned random forest; medicinal applications; piper plant classification},
	correspondence_address = {A. Pravin; Department of Computer Science, Sri Ramakrishna College of Arts and Science, Coimbatore, India; email: pravinresearchscholar1@gmail.com},
	publisher = {ATLAS},
	issn = {19490569},
	language = {English},
	abbrev_source_title = {Transdiscipl. J. Eng. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Pukhrambam2022104,
	author = {Pukhrambam, Banita and Sahayadhas, Arun},
	title = {Advanced Medicinal Plant Classification and Bioactivity Identification based on Dense Net Architecture},
	year = {2022},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {13},
	number = {6},
	pages = {104 – 109},
	doi = {10.14569/IJACSA.2022.0130614},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133355431&doi=10.14569%2fIJACSA.2022.0130614&partnerID=40&md5=396c12581eeaefe834b5537a90ac556e},
	affiliations = {Department of Computer Science and Engineering, Vels Institute of Science, Technology and Advanced Studies, Pallavaram, Chennai, 600117, India},
	abstract = {Plant species identification helps a wide range of stakeholders, including forestry services, botanists, taxonomists, physicians and pharmaceutical laboratories, endangered species organizations, the government, and the general public. As a result, there has been a spike in interest in developing automated plant species recognition systems. Using computer vision and deep learning approaches, this work proposes a fully automated system for finding medical plants. As a result, work is being done to classify the correct therapeutic plants based on their images. A training data set contains image data; this work uses the Indian Medicinal Plants, Photochemistry, and Therapeutics (IMPPAT) benchmark dataset. Convolutional Neural Network (CNN) with DenseNet algorithm is a classification system for medicinal plants that explains how they work and what they're efficient. This study also suggests a standard dataset for medicinal plants that can be found in various parts of Manipur, India's northwest coast state. On the IMPPAT dataset, the suggested DenseNet model has a recognition rate of 99.56% and on the Manipuri dataset; it has a recognition rate of 98.51%, suggesting that the DenseNet method is a promising technique for smart forestry © 2022. International Journal of Advanced Computer Science and Applications.All Rights Reserved.},
	author_keywords = {Convolutional neural network; Densenet; Imppat dataset; Indian medicinal plants},
	keywords = {Automation; Classification (of information); Conservation; Convolution; Deep learning; Forestry; Image classification; Plants (botany); Timber; Convolutional neural network; Densenet; Endangered species; Forestry services; Imppat dataset; Indian medicinal plant; Medicinal plants; NET architecture; Plant classification; Plant species identification; Convolutional neural networks},
	publisher = {Science and Information Organization},
	issn = {2158107X},
	language = {English},
	abbrev_source_title = {Intl. J. Adv.  Comput. Sci. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Keceli2022,
	author = {Keceli, Ali Seydi and Kaya, Aydin and Catal, Cagatay and Tekinerdogan, Bedir},
	title = {Deep learning-based multi-task prediction system for plant disease and species detection},
	year = {2022},
	journal = {Ecological Informatics},
	volume = {69},
	doi = {10.1016/j.ecoinf.2022.101679},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130545264&doi=10.1016%2fj.ecoinf.2022.101679&partnerID=40&md5=67a3bf29b51d05c8fcd6154099d39efe},
	affiliations = {Department of Computer Engineering, Hacettepe University, Ankara, Turkey; Department of Computer Science and Engineering, Qatar University, Doha, Qatar; Information Technology Group, Wageningen University & Research, Wageningen, Netherlands},
	abstract = {The manual prediction of plant species and plant diseases is expensive, time-consuming, and requires expertise that is not always available. Automated approaches, including machine learning and deep learning, are increasingly being applied to surmount these challenges. For this, accurate models are needed to provide reliable predictions and guide the decision-making process. So far, these two problems have been addressed separately, and likewise, separate models have been developed for each of these two problems, but considering that plant species and plant disease prediction are often related tasks, they can be considered together. We therefore propose and validate a novel approach based on the multi-task learning strategy, using shared representations between these related tasks, because they perform better than individual models. We apply a multi-input network that uses raw images and transferred deep features extracted from a pre-trained deep model to predict each plant's type and disease. We develop an end-to-end multi-task model that carries out more than one learning task at a time and combines the Convolutional Neural Network (CNN) features and transferred features. We then evaluate this model using public datasets. The results of our experiments demonstrated that this Multi-Input Multi-Task Neural Network model increases efficiency and yields faster learning for similar detection tasks. © 2022 Elsevier B.V.},
	author_keywords = {Convolutional neural networks; Deep neural networks; Multi-task learning; Plant classification; Transfer learning},
	keywords = {algorithm; artificial neural network; decision making; detection method; machine learning},
	correspondence_address = {A.S. Keceli; Department of Computer Engineering, Hacettepe University, Ankara, Turkey; email: aliseydi@cs.hacettepe.edu.tr},
	publisher = {Elsevier B.V.},
	issn = {15749541},
	language = {English},
	abbrev_source_title = {Ecol. Informatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Rajani2022,
	author = {Rajani, S. and Veena, M.N.},
	title = {Ayurvedic Plants Identification based on Machine Learning and Deep Learning Technologies},
	year = {2022},
	journal = {4th International Conference on Emerging Research in Electronics, Computer Science and Technology, ICERECT 2022},
	doi = {10.1109/ICERECT56837.2022.10060533},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150676336&doi=10.1109%2fICERECT56837.2022.10060533&partnerID=40&md5=d4fd4099429f137804ce438cca483f08},
	affiliations = {PES College of Engineering, Department of MCA, Mandya, India},
	abstract = {Plant kingdom plays a vital role in human life and provides oxygen, food, medicine, fuel, gums, and much more. Plants have been enriched with their medicinal values and active ingredients. It is very important to recognize medicinal plants and create awareness in society to protect and promote their existence for the next generations. Various parts of medicinal plants are used in preparing drugs and disease treatments for different health conditions. Leaves and flowers play an important role in identifying features of medicinal plants and having the highest medicinal properties. The color, shape, and texture features are important features used in the proposed work to classify medicinal plants. The present work classifies locally available medicinal plants using leaf images. The experiment considers 100 species from every 15 different species, and a total of 1500 images are used in the experiment. In the current work, the accuracy of the proposed algorithm yields 98.7%. © 2022 IEEE.},
	author_keywords = {Deep learning; Flowers; Leaves; Machine learning; Medicinal plants},
	keywords = {Learning systems; Plants (botany); Textures; Deep learning; Flower; Human lives; Leaf; Learning technology; Machine-learning; Medicinal plants; On-machines; Plant identification; Plant kingdom; Deep learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166545635-7},
	language = {English},
	abbrev_source_title = {Int. Conf. Emerg. Res. Electron., Comput. Sci. Technol., ICERECT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference on Emerging Research in Electronics, Computer Science and Technology, ICERECT 2022; Conference date: 26 December 2022 through 27 December 2022; Conference code: 187203}
}

@ARTICLE{Roopashree2022,
	author = {Roopashree, S. and Anitha, J. and Mahesh, T.R. and Vinoth Kumar, V. and Viriyasitavat, Wattana and Kaur, Amandeep},
	title = {An IoT based authentication system for therapeutic herbs measured by local descriptors using machine learning approach},
	year = {2022},
	journal = {Measurement: Journal of the International Measurement Confederation},
	volume = {200},
	doi = {10.1016/j.measurement.2022.111484},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134654974&doi=10.1016%2fj.measurement.2022.111484&partnerID=40&md5=dfc590efdd7ebdcb3102064515a7132c},
	affiliations = {Department of Computer Science and Engineering, JAIN (Deemed-to-be-University), Bengaluru, India; Department of Computer Science and Engineering, RV Institute of Technology and Management, Bengaluru, India; Department of Computer Science and Engineering, Jain (Deemed to be University), Bangalore, India; Business Information Technology Division, Department of Statistics, Faculty of Commerce and Accountancy, Chulalongkorn University, Bangkok, Thailand; University Centre for Research and Development, Department of Computer Science and Engineering, Chandigarh University, Mohali, Gharuan, India},
	abstract = {The work aims to develop an automatic recognition model to classify medicinal plants using machine learning techniques to enrich the traditional medical system of India. Though many countries have accepted conventional medicine as the best alternative to synthetic drugs, there exists limitations such as lack of awareness among general public and unavailability of easy access to its source evidences that has led to its limited acceptance and usability. Herein, an intelligent system is proposed to use Raspberry Pi 3 Model B+ (RPi) and the RPi camera to capture the leaf images of Indian medicinal herbs and reveal their medical properties. Five types of models implemented to identify the medicinal plants. One of the models proposed as Herbmodel extracts a feature map from a captured medicinal leaf by combining three different feature extraction techniques, namely, Scale Invariant Feature Transform (SIFT), Oriented FAST and Rotated BRIEF (ORB) and histogram of oriented gradients on support vector machine as a classifier, predicts an average accuracy of 96.22% over a custom medicinal leaf dataset of 40 different species containing 2515 samples. Generate Bag of Visual Words (BoVW) by applying K-Means clustering on both SIFT and ORB descriptors to reduce the dimensionality. The combined feature vector is further analysed using random forest and k-nearest neighbor classifier. The efficacy of the proposed approach is benchmarked using Flavia dataset and artificial neural network (ANN) as a classifier. Our findings prove that the combination of local descriptors is an efficient measurement approach that benefits automatic recognition of plants based on leaf images. Also, a reliable source of medicinal leaf datasets with good quality leaf images is necessary to establish a machine learning model for medicinal plants. © 2022 Elsevier Ltd},
	author_keywords = {Artificial Neural Network; Histogram of oriented gradients; Machine learning; Medicinal leaf dataset; Medicinal plant classification; oriented FAST and rotated BRIEF; Raspberry Pi; Scale invariant feature transform; Support vector machine},
	keywords = {Classification (of information); Decision trees; Forestry; Graphic methods; Intelligent systems; K-means clustering; Nearest neighbor search; Neural networks; Histogram of oriented gradients; Invariant feature transforms; Machine-learning; Medicinal leaf dataset; Medicinal plant classification; Medicinal plants; Oriented FAST and rotated BRIEF; Plant classification; Raspberry pi; Scale invariant feature transform; Scale invariant features; Support vectors machine; Support vector machines},
	correspondence_address = {V. Vinoth Kumar; Department of Computer Science and Engineering, Jain (Deemed to be University), Bangalore, India; email: pvkumar243@gmail.com},
	publisher = {Elsevier B.V.},
	issn = {02632241},
	coden = {MSRMD},
	language = {English},
	abbrev_source_title = {Meas J Int Meas Confed},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10}
}

@CONFERENCE{Darshana2022,
	author = {Darshana, Subhashree and Soumyakanta, Kasturi},
	title = {A Revolutionary Machine-Learning based approach for identifying Ayurvedic Medicinal Plants},
	year = {2022},
	journal = {ASSIC 2022 - Proceedings: International Conference on Advancements in Smart, Secure and Intelligent Computing},
	doi = {10.1109/ASSIC55218.2022.10088298},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154588866&doi=10.1109%2fASSIC55218.2022.10088298&partnerID=40&md5=59d05b578501f064f7edd5665a2fa5cc},
	affiliations = {School of Computer Engineering, Kiit Deemed to Be University, Odisha, Bhubaneswar, India},
	abstract = {Developing an automated classification system for medicinal herbs is indeed a time-consuming and complicated task. Plants have been used for medicinal purposes for millennia. Ayurvedic herbs are gaining popularity in the medical industry due to fewer dangerous side effects and lower costs compared to modern pharmaceuticals. According to these facts, we have expressed a strong interest in the discovery research of Ayurvedic herbal medicines. This study examines theefficiency and reliability of several algorithms of machine learning for plant classification based on photos of leaves used in current history. Assessments of their benefits and drawbacks are also presented. The paper includes image processing algorithms that are used to recognize leaf and obtain significant leaf properties for particular machine learning approaches.  © 2022 IEEE.},
	author_keywords = {Algorithm; Classification; Machine Learning; Medicinal plants; Recognition},
	keywords = {Image processing; Learning algorithms; Plants (botany); Automated classification systems; Herbal medicines; Learning-based approach; Low-costs; Machine-learning; Medical industries; Medicinal herb; Medicinal plants; Recognition; Side effect; Machine learning},
	editor = {Mohanty J.R. and Tripathy H.K. and Mishra S.K. and Mishra S. and Sahoo K.S.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166546109-2},
	language = {English},
	abbrev_source_title = {ASSIC - Proc.: Int. Conf. Adv. Smart, Secur. Intell. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 International Conference on Advancements in Smart, Secure and Intelligent Computing, ASSIC 2022; Conference date: 19 November 2022 through 20 November 2022; Conference code: 187760}
}

@ARTICLE{Sapna2022541,
	author = {Sapna, R. and Sheshappa, S.N.},
	title = {An Extensive Study on Machine Learning Paradigms Towards Medicinal Plant Classification on Potential of Medicinal Properties},
	year = {2022},
	journal = {Lecture Notes in Networks and Systems},
	volume = {514 LNNS},
	pages = {541 – 555},
	doi = {10.1007/978-3-031-12413-6_43},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135865335&doi=10.1007%2f978-3-031-12413-6_43&partnerID=40&md5=3bb00455ba926238650cbf91adcb7988},
	affiliations = {Sir M Visvesvaraya Institute Of Technology, affiliated to Visveshwaraya Technological University, Bengaluru, India; Department of Computer Science and Engineering, Presidency University, Bengaluru, India},
	abstract = {The automatic classification of medicinal plants requires more exploration as it is considered as major issue for conservation, authentication, and manufacturing of medicines. Generally, medicinal plants have been classified by features of the leaf with respect to color, shape and texture. Leaf is a main parameter on analyzing its plant nutrition, plant contentions, plant soil-water association, plant preservation measures, crop ecosystems, plant respiration rate, plant transpiration rate and plant photosynthesis. Classification of the plant species is a primary and highly essential procedure for plant conservation. An object recognition system is required to classify the various species of the plant species and to protect them from various diseases. In this article, a detailed survey on machine learning models has been carried out to identify and classify medicinal plants by considering the texture and shape features of a plant leaf using linear and non linear feature descriptors. However the extracted features from the plant leaf image will be huge containing high redundancy information’s. On employment of feature selection techniques through weighted average strategies through metaheuristic techniques, those techniques reduces the redundancy on feature extracted and minimizes the equal error rate to obtain the optimum weighted features. Further numerous classification techniques on supervised and unsupervised types has been employed to classify the optimal feature on various dataset has been experimented and validated using cross fold validation using confusion matrix. It is vital and essential task for providing detailed insight on that classification model for medicinal plant with respect to its medicinal properties. The efficacy of each model has been demonstrated on single plant and multiple plants on basis of classifier and dataset employed. Finally outline of the proposed methodology as framework to classify the medicinal plant has been provided. Evaluation of models has been carried out on the processing of the dataset. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author_keywords = {Feature extraction; Feature normalization; Feature selection; Machine learning; Medicinal plant classification},
	correspondence_address = {R. Sapna; Sir M Visvesvaraya Institute Of Technology, affiliated to Visveshwaraya Technological University, Bengaluru, India; email: sapna.aradhya@gmail.com},
	editor = {Chen J.I.-Z. and Tavares J.M.R.S. and Shi F.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {23673370},
	isbn = {978-303112412-9},
	language = {English},
	abbrev_source_title = {Lect. Notes Networks Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 3rd International Conference on Image Processing and Capsule Networks, ICIPCN 2022; Conference date: 20 May 2022 through 21 May 2022; Conference code: 281489}
}

@CONFERENCE{Dandekar2022,
	author = {Dandekar, Yukta and Shinde, Kshitija and Gangan, Jai and Firdausi, Sabil and Bharne, Smita},
	title = {Weed Plant Detection from Agricultural Field Images using YOLOv3 Algorithm},
	year = {2022},
	journal = {2022 6th International Conference on Computing, Communication, Control and Automation, ICCUBEA 2022},
	doi = {10.1109/ICCUBEA54992.2022.10011010},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147420142&doi=10.1109%2fICCUBEA54992.2022.10011010&partnerID=40&md5=6561966aa222230b01edfeecbfb786f7},
	affiliations = {Ramrao Adik Institue of Technology, Dept. of Computer Engineering, Navi Mumbai, India},
	abstract = {Agriculture is the only way for humans to survive in this world. Weed plant detection and classification are critical technical and economic issues in agriculture. Weed creates problems on the field as they extract good nutrients required by other crops. To encounter this, manual weed detection has recently been carried out with specialized people. Later, as technology advanced, people began using herbicides to kill weeds. People are trying to detect weeds without human intervention, but they were unable to reach the public due to a lack of precision. This paper focuses on weed plant detection techniques that can be used to supplement physical detection methods. We have attempted to use the CNN technique called YOLOV3 which is an object detection technique that will help us to accurately identify weed crops. We aim to counteract and provide a better solution for the existing problems to detect weeds in crops.  © 2022 IEEE.},
	author_keywords = {agriculture; CNN; crop; dataset; deep learning; digital farming; weed detection; Yolo},
	keywords = {Agricultural robots; Deep learning; E-learning; Farms; Object detection; Agricultural fields; Dataset; Deep learning; Digital farming; Field images; Plant classification; Plant detections; Weed detection; Weed plants; Yolo; Crops},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166548451-0},
	language = {English},
	abbrev_source_title = {Int. Conf. Comput., Commun., Control Autom., ICCUBEA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 6th International Conference on Computing, Communication, Control and Automation, ICCUBEA 2022; Conference date: 26 August 2022 through 27 August 2022; Conference code: 186077}
}

@ARTICLE{Minowa2022,
	author = {Minowa, Yasushi and Kubota, Yuhsuke and Nakatsukasa, Shun},
	title = {Verification of a Deep Learning-Based Tree Species Identification Model Using Images of Broadleaf and Coniferous Tree Leaves},
	year = {2022},
	journal = {Forests},
	volume = {13},
	number = {6},
	doi = {10.3390/f13060943},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132590916&doi=10.3390%2ff13060943&partnerID=40&md5=1e2eb1357b31c52cda96ba741fc35c2a},
	affiliations = {Graduate School of Life and Environmental Sciences, Kyoto Prefectural University, Kyoto, 6068522, Japan; Faculty of Life and Environmental Sciences, Kyoto Prefectural University, Kyoto, 6068522, Japan},
	abstract = {The objective of this study was to verify the accuracy of tree species identification using deep learning with leaf images of broadleaf and coniferous trees in outdoor photographs. For each of 12 broadleaf and eight coniferous tree species, we acquired 300 photographs of leaves and used those to produce 72,000 256 × 256-pixel images. We used Caffe as the deep learning framework and AlexNet and GoogLeNet as the deep learning algorithms. We constructed four learning models that combined two learning patterns: one for individual classification of 20 species and the other for two-group classification (broadleaf vs. coniferous trees), with and without data augmentation, respectively. The performance of the proposed model was evaluated according to the MCC and F-score. Both classification models exhibited very high accuracy for all learning patterns; the highest MCC was 0.997 for GoogLeNet with data augmentation. The classification accuracy was higher for broadleaf trees when the model was trained using broadleaf only; for coniferous trees, the classification accuracy was higher when the model was trained using both tree types simultaneously than when it was trained using coniferous trees only. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {AlexNet; broadleaf trees; Caffe; coniferous trees; deep learning; F-score; GoogLeNet; MCC; tree species identification},
	keywords = {Accuracy; Classification; Forestry; Images; Leaves; Patterns; Photography; Trees; Deep learning; Forestry; Learning systems; Photography; Trees (mathematics); Alexnet; Broadleaf trees; Caffe; Coniferous trees; Deep learning; F-score; Googlenet; Learning patterns; MCC; Tree species identifications; accuracy assessment; broad-leaved forest; coniferous tree; image analysis; leaf; machine learning; numerical model; Learning algorithms},
	correspondence_address = {Y. Minowa; Graduate School of Life and Environmental Sciences, Kyoto Prefectural University, Kyoto, 6068522, Japan; email: sharmy@uf.kpu.ac.jp},
	publisher = {MDPI},
	issn = {19994907},
	language = {English},
	abbrev_source_title = {Forests},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access}
}

@ARTICLE{Jiang202211045,
	author = {Jiang, Zihan and Zhang, Renbo and Guo, Yubo and Hu, Mingrui and He, Liu and Li, Fumin and Zhu, Zimin},
	title = {Noise Interference Reduction in Vision Module of Intelligent Plant Cultivation Robot Using Better Cycle GAN},
	year = {2022},
	journal = {IEEE Sensors Journal},
	volume = {22},
	number = {11},
	pages = {11045 – 11055},
	doi = {10.1109/JSEN.2022.3164915},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127828070&doi=10.1109%2fJSEN.2022.3164915&partnerID=40&md5=c42400816907f4fce6d26dcb14690ae1},
	affiliations = {Northeast Forestry University, College of Information and Computer Engineering, Harbin, 150040, China; Northeastern University, College of Computer Science and Engineering, Shenyang, 110000, China; Northeast Forestry University, College of Mechanical and Electrical Engineering, Harbin, 150040, China; Changzhou University, Ali School of Big Data, Changzhou, 213164, China; Sichuan University of Science and Engineering, School of Biological Engineering, Yibin, 644000, China},
	abstract = {The vision recognition module is one of the very important modules in the normal operation of the intelligent plant cultivation robot. In the original vision module, by improving the original YOLOV3(The algorithm proposed in YOLOv3: An Incremental Improvement), a more excellent YOLOV3 was obtained to recognize plants under normal conditions. However, the improved YOLOV3 does not perform well in the visual recognition process when there is a lot of noise interference. To make the visual recognition module continue to work properly, in this paper, the better Cycle GAN (generative adversarial networks) model is proposed to deal with noise based on three common noises, and finally, through a lot of experiments, it is proved that the improvement has some significance for the vision recognition module, which extends the use of the vision recognition module and makes the intelligent plant cultivation robot avoid the interference of noise.  © 2001-2012 IEEE.},
	author_keywords = {cyclegan; denoising; Plant recognition; robot; self-attention},
	keywords = {Computer vision; Generative adversarial networks; Image recognition; Intelligent robots; Cyclegan; De-noising; Features extraction; Generator; Noise interference; Plant recognition; Robot; Self-attention; Vision modules; Vision recognition; Noise abatement},
	correspondence_address = {Z. Zhu; Northeast Forestry University, College of Information and Computer Engineering, Harbin, 150040, China; email: 279745779@qq.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {1530437X},
	language = {English},
	abbrev_source_title = {IEEE Sensors J.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Lu20221009,
	author = {Lu, Ruhua and Mo, Yueqing and Yao, Weiqiao and Li, Yalan},
	title = {A Leaf Recognition Algorithm Based on KNN Classifier},
	year = {2022},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {961 LNEE},
	pages = {1009 – 1015},
	doi = {10.1007/978-981-19-6901-0_104},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144490535&doi=10.1007%2f978-981-19-6901-0_104&partnerID=40&md5=a2e7e06ee6a97d82d999dfac67bbe808},
	affiliations = {School of Physics and Electronic Electrical Engineering, Xiangnan University, Hunan, Chenzhou, 423000, China; College of Computer and Artificial Intelligence, Xiangnan University, Hunan, Chenzhou, 423000, China},
	abstract = {In order to reduce the difficulty of identifying plants and stimulate people’s interest in plant cultivation, a leaf recognition model using KNN classifier is proposed according to the texture, color and morphological features of leaf images. The general framework of leaf recognition is divided into two steps: training and classification. At the training step, the training leaf sample with known classification results is processed through multiple steps such as image preprocessing, features extraction including shape, texture and color features to obtain the leaf feature model database. After completing the leaf model training, it enters the classification stage. At this stage, leaves with unknown classification can be recognized. First, the test leaf image data is processed with the same image preprocessing, features extraction as in the above training stage. Then, it is input to KNN classifier to complete leaf classification, and the function of plant leaf recognition is realized. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Classifier; KNN; Leaf recognition},
	keywords = {Extraction; Image texture; Learning algorithms; Plants (botany); Textures; Color features; Features extraction; Image preprocessing; KNN; Leaf images; Leaf recognition; Morphological features; Recognition algorithm; Recognition models; Texture features; Classification (of information)},
	correspondence_address = {Y. Li; School of Physics and Electronic Electrical Engineering, Xiangnan University, Chenzhou, Hunan, 423000, China; email: liyalan@xnu.edu.cn},
	editor = {Liu Q. and Liu X. and Cheng J. and Shen T. and Tian Y.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981196900-3},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 12th International Conference on Computer Engineering and Networks, CENet 2022; Conference date: 4 November 2022 through 7 November 2022; Conference code: 285459}
}

@ARTICLE{d'Andrimont2022,
	author = {d'Andrimont, Raphaël and Yordanov, Momchil and Martinez-Sanchez, Laura and van der Velde, Marijn},
	title = {Monitoring crop phenology with street-level imagery using computer vision},
	year = {2022},
	journal = {Computers and Electronics in Agriculture},
	volume = {196},
	doi = {10.1016/j.compag.2022.106866},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127177413&doi=10.1016%2fj.compag.2022.106866&partnerID=40&md5=b3c8122e3f7d146bf04248b57f44b4e0},
	affiliations = {European Commission, Joint Research Centre (JRC), Ispra, Italy},
	abstract = {Street-level imagery holds a significant potential to scale-up in-situ data collection. This is enabled by combining the use of cheap high-quality cameras with recent advances in deep learning compute solutions to derive relevant thematic information. We present a framework to collect and extract crop type and phenological information from street level imagery using computer vision. Monitoring crop phenology is critical to assess gross primary productivity and crop yield. During the 2018 growing season, high-definition pictures were captured with side-looking action cameras in the Flevoland province of the Netherlands. Each month from March to October, a fixed 200-km route was surveyed collecting one picture per second resulting in a total of 400,000 geo-tagged pictures. At 220 specific parcel locations, detailed on the spot crop phenology observations were recorded for 17 crop types (including bare soil, green manure, and tulips): bare soil, carrots, green manure, grassland, grass seeds, maize, onion, potato, summer barley, sugar beet, spring cereals, spring wheat, tulips, vegetables, winter barley, winter cereals and winter wheat. Furthermore, the time span included specific pre-emergence parcel stages, such as differently cultivated bare soil for spring and summer crops as well as post-harvest cultivation practices, e.g. green manuring and catch crops. Classification was done using TensorFlow with a well-known image recognition model, based on transfer learning with convolutional neural network (MobileNet). A hypertuning methodology was developed to obtain the best performing model among 160 models. This best model was applied on an independent inference set discriminating crop type with a Macro F1 score of 88.1% and main phenological stage at 86.9% at the parcel level. Potential and caveats of the approach along with practical considerations for implementation and improvement are discussed. The proposed framework speeds up high quality in-situ data collection and suggests avenues for massive data collection via automated classification using computer vision. © 2022 The Author(s)},
	author_keywords = {Agriculture; BBCH; CNN; Computer vision; Crop type; Deep learning; Earth observation; In situ; In-situ; Parcel; Phenology; Plant recognition; Remote sensing; Street view imagery; Survey},
	keywords = {Cameras; Computer vision; Convolutional neural networks; Cultivation; Data acquisition; Deep learning; Fertilizers; Image recognition; Remote sensing; Soils; Sugar beets; Surveys; BBCH; CNN; Crop type; Deep learning; Earth observations; In situ; In-situ; Parcel; Plant recognition; Remote-sensing; Street view imagery; computer vision; imagery; monitoring; phenology; Crops},
	correspondence_address = {R. d'Andrimont; European Commission, Joint Research Centre (JRC), Ispra, Italy; email: raphael.dandrimont@ec.europa.eu},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Abisha2022867,
	author = {Abisha, A. and Bharathi, N.},
	title = {Feature Extraction from Plant Leaves and Classification of Plant Health Using Machine Learning},
	year = {2022},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {858},
	pages = {867 – 876},
	doi = {10.1007/978-981-19-0840-8_67},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134328024&doi=10.1007%2f978-981-19-0840-8_67&partnerID=40&md5=94b699ee21942c8a64cb087d61cfb62c},
	affiliations = {Department of Computer Science and Engineering, SRM Institute of Science and Technology, Vadapalani Campus, No.1, Jawaharlal Nehru Rd, Tamil Nadu, Vadapalani, India},
	abstract = {Feature extraction (FE) is an important method that contributes dimensionality reduction especially when extracting the features from images. It is the method of converting the input images into a set of features understandable by the system or software. While processing images, it is desirable to extract features that are oriented toward discernment between two or more classes. In this proposed method, two classes are considered as infected and healthy, and data is distributed into six cases. Features are extracted from the plant images using feature descriptors Hu moments, Haralick texture and color histogram. After feature extraction, the images are classified as infected or healthy based on the feature descriptors. The extracted data are trained with the various classifier models such as LR, LDA, KNN, CART and RF, and validation is performed. 10 k cross-validation is used, and the three best algorithms are chosen to classify and predict the model. Among these models, RF, CART and KNN have given better performance, respectively. Finally, accuracy score above 95% and r2 score above 85% is found for various cases, and the results are visualized for all six cases. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Classification; Feature extraction; Machine learning; Plant health},
	keywords = {Classification (of information); Extraction; Image classification; Plants (botany); Textures; Dimensionality reduction; Feature descriptors; Features extraction; Hu moments; Input image; Machine-learning; Plant classification; Plant healths; Plant leaves; Sets of features; Feature extraction},
	correspondence_address = {A. Abisha; Department of Computer Science and Engineering, SRM Institute of Science and Technology, Vadapalani Campus, Vadapalani, No.1, Jawaharlal Nehru Rd, Tamil Nadu, India; email: abishaa@srmist.edu.in},
	editor = {Gupta D. and Sambyo K. and Prasad M. and Agarwal S.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981190839-2},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 3rd International Conference on Machine Intelligence and Signal Processing, MISP 2021; Conference date: 23 September 2021 through 25 September 2021; Conference code: 279949}
}

@CONFERENCE{Kursun2022,
	author = {Kursun, Ramazan and Cinar, Ilkay and Taspinar, Y. Selim and Koklu, Murat},
	title = {Flower Recognition System with Optimized Features for Deep Features},
	year = {2022},
	journal = {2022 11th Mediterranean Conference on Embedded Computing, MECO 2022},
	doi = {10.1109/MECO55406.2022.9797103},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133957490&doi=10.1109%2fMECO55406.2022.9797103&partnerID=40&md5=82bc3cf59d337797aa06522e71fccb9f},
	affiliations = {Selcuk University, Guneysinir Vocational School, Konya, Turkey; Selcuk University, Department of Computer Engineering, Konya, Turkey; Selcuk University, Doganhisar Vocational School, Konya, Turkey},
	abstract = {Looking at nature, flowers are everywhere. Classification is a difficult task, as the flowers have a large number of species that are very similar to each other in shape, appearance and color. Classification of flowers can be used in various fields of application such as product monitoring, flower identification, medicinal flowers, floriculture industry, plant taxonomy. In the study, a dataset with 4317 images from 5 types of flowers was used. In the classification study carried out in three stages, deep features were extracted from images with the SqueezeNet deep learning architecture of the transfer learning approach in the first stage. In the second stage the 1000 extracted features were classified using Neural Network and Logistic Regression methods from machine learning techniques. In the third stage, the deep features extracted were optimized with the help of particle swarm algorithm and the 488 features obtained were classified using machine learning Neural Network and Logistic Regression methods again. When the results obtained at both stages were compared, it was observed that the classification with the optimized features improved the success performance. The classification success of the features obtained by deep feature extraction was obtained as 85.1% by Neural Network and 79.7% by Logistic Regression method. In the classification results performed with the optimized features, the classification success was determined as 90.1% for Neural Network and 84.2% for Logistic Regression. The effect of optimized features on classification success is also understood in the study. © 2022 IEEE.},
	author_keywords = {feature selection; Flowers; logistic regression; neural network; particle swarm optimization},
	keywords = {Deep learning; Feature Selection; Neural networks; Particle swarm optimization (PSO); Classifieds; Features selection; Flower; Flower recognition; Logistic regression method; Logistics regressions; Neural-networks; Particle swarm; Particle swarm optimization; Swarm optimization; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166546828-2},
	language = {English},
	abbrev_source_title = {Mediterr. Conf. Embed. Comput., MECO},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 11th Mediterranean Conference on Embedded Computing, MECO 2022; Conference date: 7 June 2022 through 10 June 2022; Conference code: 180251}
}

@ARTICLE{Ibrahim202227783,
	author = {Ibrahim, Nehad M. and Gabr, Dalia Goda Ibrahim and Rahman, Atta-ur and Dash, Sujata and Nayyar, Anand},
	title = {A deep learning approach to intelligent fruit identification and family classification},
	year = {2022},
	journal = {Multimedia Tools and Applications},
	volume = {81},
	number = {19},
	pages = {27783 – 27798},
	doi = {10.1007/s11042-022-12942-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127337664&doi=10.1007%2fs11042-022-12942-9&partnerID=40&md5=21cb97f9fbc0bc56d9cf1b073a3866aa},
	affiliations = {Department of Computer Science (CS), College of Computer Science and Information Technology (CCSIT), Imam Abdulrahman Bin Faisal University (IAU), P.O. Box 1982, Dammam, 31441, Saudi Arabia; Department of Biology, College of Science, Imam Abdulrahman Bin Faisal University (IAU), P.O. Box 1982, Dammam, 31441, Saudi Arabia; Department of Computer Application, North Orissa University, Takatpur, Odisha, Baripada, 757003, India; Graduate School, Faculty of Information Technology, Duy Tan University, Da Nang, 550000, Viet Nam},
	abstract = {The deep learning techniques have been playing an important role in the identification and classification problems such as diseases in medical science, marketing in the industry, manufacturing in engineering, and identification in plant taxonomy science. Fruit identification and its family classification is among one of the areas that needs more emphasis for the sake of automation. With this inspiration, fruit images for 52 species belonging to four different families (Apiaceae, Brassicaceae, Asteraceae, and Apocynaceae) have been used in this study to build a deep learning analysis dataset. Further, the dataset has been augmented to 3800 images, divided to 2660 images for training and 1440 for testing, and different 14 fruit images belonging to the same families have been used for prediction of the testing module. A novel Convolution Neural Network (CNN) model architecture has been proposed to extract the fruit features, classify each image with its family, and use the trained model to predict that the new fruits belong to the same four families. The maximum accuracy obtained for the training and testing module was 99.82%. The prediction for this module succeeded by 93% since all fruits’ success predicted was attained except one from the family number 2 (Brassicaceae). The same dataset was applied to two different models to evaluate our proposed model, the Deep learning model, aka Residual Neural Network, 20 layers (ResNet-20), and Support Vector Machine (SVM). The proposed CNN model achieved higher accuracy and efficiency than the ResNet-20 and SVM. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Convolution neural network; Deep learning; Fruits identification; Image analysis, fruit and plant families; Plants classification},
	keywords = {Convolution; Convolutional neural networks; Deep learning; Forecasting; Image analysis; Image classification; Multilayer neural networks; Network layers; Statistical tests; Support vector machines; Brassicaceae; Convolution neural network; Deep learning; Fruit identification; Image analyse, fruit and plant family; Image-analysis; Neural network model; Plant classification; Plant families; Testing modules; Fruits},
	correspondence_address = {A. Nayyar; Graduate School, Faculty of Information Technology, Duy Tan University, Da Nang, 550000, Viet Nam; email: anandnayyar@duytan.edu.vn},
	publisher = {Springer},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 22}
}

@ARTICLE{Kesler2022,
	author = {Kesler, Selami and Karakan, Abdil and Oğuz, Yüksel},
	title = {Real-Time Strawberry Plant Classification and Efficiency Increase with Hybrid System Deep Learning: Microcontroller and Mobile Application},
	year = {2022},
	journal = {Applied Sciences (Switzerland)},
	volume = {12},
	number = {17},
	doi = {10.3390/app12178860},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137844282&doi=10.3390%2fapp12178860&partnerID=40&md5=f803a6823824e0d0fa65ab4f4f3a9ea5},
	affiliations = {Electrical and Electronics Engineering, Faculty of Engineering, Pamukkale University, Denizli, 20160, Turkey; Electrical Department, Dazkırı Vocational School, Afyon Kocatepe University, Afyonkarahisar, 03204, Turkey; Electrical and Electronics Engineering, Faculty of Technology, Afyon Kocatepe University, Afyonkarahisar, 03204, Turkey},
	abstract = {The strawberry plant has three life stages: seedling, blooming, and crop. It needs different acclimatization conditions in these life stages. A dataset consisting of 10,000 photographs of the strawberry plant was prepared. Using this dataset, classification in convolutional neural networks was performed in Matrix Laboratory (MATLAB). Nine different algorithms were used in this process. They were realized in ResNet101 architecture, and the highest accuracy rate was 99.8%. A low-resolution camera was used while growing strawberry plants in the application greenhouse. Every day at 10:00, a picture of the strawberry plant was taken. The captured image was processed in ResNet101 architecture. The result of the detection process appeared on the computer screen and was sent to the microcontroller via a USB connection. The microcontroller adjusted air-conditioning in the greenhouse according to the state of the strawberry plant. For this, it decided based on the data received from the temperature, humidity, wind direction, and wind speed sensors outside the greenhouse and the temperature, humidity, and soil moisture sensors inside the greenhouse. In addition, all data from the sensors and the life stage of the plant were displayed with a mobile application. The mobile application also provided the possibility for manual control. In the study, the greenhouse was divided into two. Strawberries were grown with the hybrid system on one side of the greenhouse and a normal system on the other side of the greenhouse. This study achieved 9.75% more crop, had a 4.75% earlier crop yield, and required 8.59% less irrigation in strawberry plants grown using the hybrid system. © 2022 by the authors.},
	author_keywords = {convolutional neural networks; deep learning; hybrid system; MATLAB; mobile application; productivity},
	correspondence_address = {A. Karakan; Electrical Department, Dazkırı Vocational School, Afyon Kocatepe University, Afyonkarahisar, 03204, Turkey; email: akarakan@aku.edu.tr},
	publisher = {MDPI},
	issn = {20763417},
	language = {English},
	abbrev_source_title = {Appl. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Sharma2022,
	author = {Sharma, Aviral and Nigam, Saumya},
	title = {Parametric Model for Flora Detection in Middle Himalayas},
	year = {2022},
	journal = {International Journal of Decision Support System Technology},
	volume = {14},
	number = {1},
	doi = {10.4018/IJDSST.286698},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149623562&doi=10.4018%2fIJDSST.286698&partnerID=40&md5=8951fbaa9238045aa5513c857daa6c0d},
	affiliations = {Department of Informatics, School of Computer Science, University of Petroleum and Energy Studies, Dehradun, India; University of Petroleum and Energy Studies, India},
	abstract = {Plant detection forms an integral part of the life of the forest guards, researchers, and students in the field of botany and for common people also who are curious about knowing a plant. But detecting plants suffer a major drawback that the true identifier is only the flower, and in certain species, flowering occurs at major time period gaps spanning from few months to over 100 years (in certain types of bamboos). Machine learning-based systems could be used in developing models where the experience of researchers in the field of plant sciences can be incorporated into the model. In this paper, the authors present a machine learning-based approach based upon other quantifiable parameters for the detection of the plant presented. The system takes plant parameters as the inputs and will detect the plant family as the output. Copyright © 2022, IGI Global.},
	author_keywords = {Machine Learning; Plant Recognition},
	keywords = {Forestry; Learning systems; Plants (botany); Himalayas; Integral part; Learning-based approach; Machine-learning; Parametric models; Plant detections; Plant parameters; Plant recognition; Plant science; Time-periods; Machine learning},
	publisher = {IGI Global},
	issn = {19416296},
	language = {English},
	abbrev_source_title = {Int. J. Decis. Support Syst. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Lozada2022268,
	author = {Lozada, Amiel Joseph M. and Monsanto, Nigel L. and Pepito, Glenn B.},
	title = {Comparative Study on Image Filtering for Herbal Plant Identification Using Xception Based Convolutional Neural Network},
	year = {2022},
	journal = {2022 International Seminar on Application for Technology of Information and Communication: Technology 4.0 for Smart Ecosystem: A New Way of Doing Digital Business, iSemantic 2022},
	pages = {268 – 272},
	doi = {10.1109/iSemantic55962.2022.9920424},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141458398&doi=10.1109%2fiSemantic55962.2022.9920424&partnerID=40&md5=dca20018aa74261f08edc094e25b7c3b},
	affiliations = {University of San Carlos, Department of Computer, Information Sciences, and Mathematics, Cebu City, Philippines},
	abstract = {Plant identification, through the use of Convolutional Neural Networks (CNNs), has been utilized in several studies over recent years. With CNNs being almost the default approach when dealing with image processing, the researchers shifted their focus on image filtering techniques. This study determined to investigate the most effective image filter for herbal plant identification. An image dataset of eleven medicinal plants was used by the researchers, made into four copies for image processing. Three image filters were then applied to three different copies of the dataset, namely: Canny Edge Detection filter, Color Saturation filter, and Contrast Enhancement and Thresholding filter; none were applied to the fourth copy since it served as the control group of the study. The Xception model was trained using each of the processed datasets. Afterwards, the researchers discerned which CNN and image filter yielded the most accurate results during testing through the confusion matrix. It was calculated and concluded that the Color Saturation filter was the best image filtering technique to use for identifying herbal plants, achieving 100% in the metrics used during the study. The results of this study can be applied in works and systems that focus on plant identification and image processing in general.  © 2022 IEEE.},
	author_keywords = {computer vision; image filtering; image processing; plant classification; plant identification},
	keywords = {Convolution; Convolutional neural networks; Image classification; Image enhancement; Plants (botany); Color saturation; Comparatives studies; Convolutional neural network; Filtering technique; Herbal plants; Image filtering; Image filters; Images processing; Plant classification; Plant identification; Computer vision},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166548839-6},
	language = {English},
	abbrev_source_title = {Int. Semin. Appl. Technol. Inf. Commun.: Technol. 4.0 Smart Ecosyst.: A New Way Doing Digit. Bus., iSemantic},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 International Seminar on Application for Technology of Information and Communication, iSemantic 2022; Conference date: 17 September 2022 through 18 September 2022; Conference code: 183710}
}

@CONFERENCE{Mirandilla2022,
	author = {Mirandilla, Jayde Paolo C. and Bating, Carlos B. and Cabatuan, Melvin K. and Jose, John Anthony C.},
	title = {Classification of Philippine Herbal Medicine Plant Using EfficientNet on Mobile Platform},
	year = {2022},
	journal = {IEEE Region 10 Annual International Conference, Proceedings/TENCON},
	volume = {2022-November},
	doi = {10.1109/TENCON55691.2022.9977715},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145651421&doi=10.1109%2fTENCON55691.2022.9977715&partnerID=40&md5=0e0ec805b1768a863e9dace150712ad3},
	affiliations = {Gokongwei College of Engineering, De la Salle University, Department of Electronics and Computer Engineering, Manila, Philippines; Engineering Research and Development for Technology of the Republic of the Philippines, Department of Science and Technology, Philippines},
	abstract = {So far, there is a lack of study on deep learning that is compatible with a mobile platform for Philippine herbal plant identification. This paper applied EfficientNet-B0 to classify Philippine herbal plants with 97.4% accuracy. Using a mobile phone camera, the model achieved top-1 inference accuracy of >92.2% versus top-2 with <2.3%. © 2022 IEEE.},
	author_keywords = {Deep Learning; EfficientNet; Image Classification; Mobile Platform; Philippine Herbal Medicine Plant},
	keywords = {Deep learning; Medicine; Plant extracts; Deep learning; Efficientnet; Herbal medicines; Herbal plants; Images classification; Mobile phone cameras; Mobile platform; Philippine herbal medicine plant; Philippines; Plant identification; Image classification},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21593442},
	isbn = {978-166545095-9},
	coden = {85QXA},
	language = {English},
	abbrev_source_title = {IEEE Reg 10 Annu Int Conf Proc TENCON},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 IEEE Region 10 International Conference, TENCON 2022; Conference date: 1 November 2022 through 4 November 2022; Conference code: 185310}
}

@ARTICLE{Goyal2022203,
	author = {Goyal, Neha and Kumar, Nitin and Gupta, Kapil},
	title = {Lower-dimensional intrinsic structural representation of leaf images and plant recognition},
	year = {2022},
	journal = {Signal, Image and Video Processing},
	volume = {16},
	number = {1},
	pages = {203 – 210},
	doi = {10.1007/s11760-021-01983-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110757511&doi=10.1007%2fs11760-021-01983-6&partnerID=40&md5=d5d1f31cbacef70c5049cfe626f9f288},
	affiliations = {NIT Kurukshetra, Kurukshetra, India; NIT Kurukshetra, Uttarakhand, India},
	abstract = {This paper proposes a statistical representation called “Eigenleaves” for leaf images dependent on their natural structure. The proposed presentation possesses several improvements over traditional image representation methods as a feature vector, such as being simple and fully automatic. We aim toward representing an application of the eigenvector that finds the geometrical structure and correlated information. The method automates feature extraction without explicitly specifying the most dominating attribute and deals with issues raised when working with high-dimensional images. Our finding confirms that the proposed representation is efficient and significant. It is comparable to deep learning approaches that require lots of computational costs and a high number of training samples to find the hidden structure of leaves that need to be defined at an earlier stage. Encoded leaf images are stored for futuristic applications with lesser dimensionality and essential information. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
	author_keywords = {Basis vectors; Biodiversity conservation; Eigenleaves; Image reconstruction; Plant taxonomy; Principal component},
	keywords = {Deep learning; Plants (botany); Structure (composition); Computational costs; Geometrical structure; High-dimensional images; Image representations; Learning approach; Natural structures; Statistical representations; Structural representation; Image enhancement},
	correspondence_address = {N. Goyal; NIT Kurukshetra, Kurukshetra, India; email: neha.goyal2309@gmail.com},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18631703},
	language = {English},
	abbrev_source_title = {Signal Image Video Process.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Rzanny2022,
	author = {Rzanny, Michael and Wittich, Hans Christian and Mäder, Patrick and Deggelmann, Alice and Boho, David and Wäldchen, Jana},
	title = {Image-Based Automated Recognition of 31 Poaceae Species: The Most Relevant Perspectives},
	year = {2022},
	journal = {Frontiers in Plant Science},
	volume = {12},
	doi = {10.3389/fpls.2021.804140},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124502635&doi=10.3389%2ffpls.2021.804140&partnerID=40&md5=cc98b0c31d0038c9e09e7363544a4478},
	affiliations = {Department of Biogeochemical Integration, Max Planck Institute for Biogeochemistry, Jena, Germany; Data-intensive Systems and Visualisation, Technische Universität Ilmenau, Ilmenau, Germany; Faculty of Biological Sciences, Friedrich Schiller University, Jena, Germany},
	abstract = {Poaceae represent one of the largest plant families in the world. Many species are of great economic importance as food and forage plants while others represent important weeds in agriculture. Although a large number of studies currently address the question of how plants can be best recognized on images, there is a lack of studies evaluating specific approaches for uniform species groups considered difficult to identify because they lack obvious visual characteristics. Poaceae represent an example of such a species group, especially when they are non-flowering. Here we present the results from an experiment to automatically identify Poaceae species based on images depicting six well-defined perspectives. One perspective shows the inflorescence while the others show vegetative parts of the plant such as the collar region with the ligule, adaxial and abaxial side of the leaf and culm nodes. For each species we collected 80 observations, each representing a series of six images taken with a smartphone camera. We extract feature representations from the images using five different convolutional neural networks (CNN) trained on objects from different domains and classify them using four state-of-the art classification algorithms. We combine these perspectives via score level fusion. In order to evaluate the potential of identifying non-flowering Poaceae we separately compared perspective combinations either comprising inflorescences or not. We find that for a fusion of all six perspectives, using the best combination of feature extraction CNN and classifier, an accuracy of 96.1% can be achieved. Without the inflorescence, the overall accuracy is still as high as 90.3%. In all but one case the perspective conveying the most information about the species (excluding inflorescence) is the ligule in frontal view. Our results show that even species considered very difficult to identify can achieve high accuracies in automatic identification as long as images depicting suitable perspectives are available. We suggest that our approach could be transferred to other difficult-to-distinguish species groups in order to identify the most relevant perspectives. Copyright © 2022 Rzanny, Wittich, Mäder, Deggelmann, Boho and Wäldchen.},
	author_keywords = {accuracy; automated plant identification; deep learning; fine-grained image classification; image recognition; machine learning; plant perspective; Poaceae},
	correspondence_address = {M. Rzanny; Department of Biogeochemical Integration, Max Planck Institute for Biogeochemistry, Jena, Germany; email: mrzanny@bgc-jena.mpg.de},
	publisher = {Frontiers Media S.A.},
	issn = {1664462X},
	language = {English},
	abbrev_source_title = {Front. Plant Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Twum202283204,
	author = {Twum, Frimpong and Missah, Yaw Marfo and Oppong, Stephen Opoku and Ussiph, Najim},
	title = {Textural Analysis for Medicinal Plants Identification Using Log Gabor Filters},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {83204 – 83220},
	doi = {10.1109/ACCESS.2022.3196788},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135740571&doi=10.1109%2fACCESS.2022.3196788&partnerID=40&md5=9926e3f06d6eb8693140ae5454237b26},
	affiliations = {Kwame Nkrumah University of Science and Technology, Department of Computer Science, Kumasi, Ghana; University of Education, Department of Ict Education, Winneba, Ghana},
	abstract = {Texture plays a crucial role in computer vision, providing valuable information about image regions. Log Gabor filters that mimic the human eye's visual cortex are used as feature extractors to identify medicinal plants based on the leaf textural features. This method was tested on a dataset developed from the Centre of Plant Medicine Research, Ghana, consisting of forty-nine (49) plant species as well as the Flavia and Swedish Leaf datasets, which are benchmark datasets. The Log Gabor filter outperformed the Gabor filters, which have been extensively used in this area when tested on nine supervised classifiers (K-Nearest Neighbour, Support Vector Machine, Naïve Bayes, Logistic Regression, Decision tree, Random Forest, Multilayer Perceptron, Gradient Boosting and Stochastic Gradient Descent) with 10-fold cross-validation. The Support Vector Machine and Multilayer Perceptron were the best performing classifiers for both Log Gabor filter and Gabor filter in terms of accuracy, precision, true positive rate, F1 score and false positive rate. The Log Gabor filter's highest accuracy was 79% for Mydatastet, 97% for Flavia, and 98% for the Swedish Leaf dataset whiles the Gabor filter's highest accuracy was 66% for Mydatastet, 92% for Flavia and 96% for the Swedish Leaf dataset. © 2013 IEEE.},
	author_keywords = {Gabor filter; log Gabor filter; medicinal plants; supervised classifiers; Texture},
	keywords = {Filter banks; Gabor filters; Multilayers; Nearest neighbor search; Plants (botany); Stochastic systems; Support vector machines; Biomedical imaging; Computational modelling; Features extraction; Filters bank; Log-gabor filter; Medicinal plants; Shape; Supervised classifiers; Support vectors machine; Swedishs; Decision trees},
	correspondence_address = {S.O. Oppong; University of Education, Department of Ict Education, Winneba, Ghana; email: sooppong@uew.edu.gh},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access}
}

@ARTICLE{Liu2022,
	author = {Liu, Keng-Hao and Yang, Meng-Hsien and Huang, Sheng-Ting and Lin, Chinsu},
	title = {Plant Species Classification Based on Hyperspectral Imaging via a Lightweight Convolutional Neural Network Model},
	year = {2022},
	journal = {Frontiers in Plant Science},
	volume = {13},
	doi = {10.3389/fpls.2022.855660},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128738296&doi=10.3389%2ffpls.2022.855660&partnerID=40&md5=7f06c6e9a58a9094ca8face49d1950ee},
	affiliations = {Department of Mechanical and Electro-Mechanical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Department of Forestry and Natural Resources, National Chiayi University, Chiayi, Taiwan},
	abstract = {In recent years, many image-based approaches have been proposed to classify plant species. Most methods utilized red green blue (RGB) imaging materials and designed custom features to classify the plant images using machine learning algorithms. Those works primarily focused on analyzing single-leaf images instead of live-crown images. Without considering the additional features of the leaves’ color and spatial pattern, they failed to handle cases that contained leaves similar in appearance due to the limited spectral information of RGB imaging. To tackle this dilemma, this study proposes a novel framework that combines hyperspectral imaging (HSI) and deep learning techniques for plant image classification. We built a plant image dataset containing 1,500 images of 30 different plant species taken by a 470–900 nm hyperspectral camera and designed a lightweight conventional neural network (CNN) model (LtCNN) to perform image classification. Several state-of-art CNN classifiers are chosen for comparison. The impact of using different band combinations as the network input is also investigated. Results show that using simulated RGB images achieves a kappa coefficient of nearly 0.90 while using the combination of 3-band RGB and 3-band near-infrared images can improve to 0.95. It is also found that the proposed LtCNN can obtain a satisfactory performance of plant classification (kappa = 0.95) using critical spectral features of the green edge (591 nm), red-edge (682 nm), and near-infrared (762 nm) bands. This study also demonstrates the excellent adaptability of the LtCNN model in recognizing leaf features of plant live-crown images while using a relatively smaller number of training samples than complex CNN models such as AlexNet, GoogLeNet, and VGGNet. Copyright © 2022 Liu, Yang, Huang and Lin.},
	author_keywords = {convolutional neural network; deep learning; dimensionality reduction; hyperspectral imaging; leaf feature recognition; live-crown features; plant species classification; plant stress detection},
	correspondence_address = {C. Lin; Department of Forestry and Natural Resources, National Chiayi University, Chiayi, Taiwan; email: chinsu@mail.ncyu.edu.tw},
	publisher = {Frontiers Media S.A.},
	issn = {1664462X},
	language = {English},
	abbrev_source_title = {Front. Plant Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Shahmiri2022,
	author = {Shahmiri, Lida and Wong, Patrick and Dooley, Laurence S.},
	title = {Accurate Medicinal Plant Identification in Natural Environments by Embedding Mutual Information in a Convolution Neural Network Model},
	year = {2022},
	journal = {5th IEEE International Image Processing, Applications and Systems Conference, IPAS 2022},
	doi = {10.1109/IPAS55744.2022.10053008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149729667&doi=10.1109%2fIPAS55744.2022.10053008&partnerID=40&md5=cd7c28a693585c2f0f0cc461f70c18e2},
	affiliations = {Sch of Computing & Communications, The Open University, Milton Keynes, United Kingdom},
	abstract = {Medicinal plants are a primary source of disease treatment in many countries. As most are edible however, consumption of the wrong herbal plants can have serious consequences and even lead to death. Automatic accurate recognition of plant species to help users who do not have specialist knowledge of herbal plants is thus a desirable aim. Several automatic medicinal plant identification systems have been proposed, though most are significantly constrained either in the small number of species or in requiring manual image segmentation of plant leaves. This means they are captured on a plain background rather than being readily identified in their natural surroundings, which often involve complex and noisy backgrounds. While deep learning (DL) based methods have made considerable strides in recent times, their potential has not always been maximised because they are trained with samples which are not always fully representative of the intra-class and inter-class differences between the plant species concerned. This paper addresses this challenge by incorporating mutual information into a Convolutional Neural Network (CNN) model to select samples for the training, validation, and testing sets based on a similarity measure. A critical comparative evaluation of this new CNN medicinal plant classification model incorporating a mutual information guided training (MIGT) algorithm for sample selection, corroborates the superior classification performance achieved for the VNPlant-200 dataset, with an average accuracy of more than 97%, while the precision and recall values are also consistently above 97%. This is significantly better than existing CNN classification methods for this dataset as it crucially means false positive rates are substantially lower thus affording improved identification reliability. © 2022 IEEE.},
	author_keywords = {Convolutional Neural Networks; Medicinal plants; Mutual Information; Natural Environment; training dataset},
	keywords = {Classification (of information); Convolution; Deep learning; Image segmentation; Neural network models; Plants (botany); Convolutional neural network; Embeddings; Herbal plants; Medicinal plants; Mutual informations; Natural environments; Neural network model; Plant identification; Plant species; Training dataset; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166546219-8},
	language = {English},
	abbrev_source_title = {IEEE Int. Image Process., Appl. Syst. Conf., IPAS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 5th IEEE International Image Processing, Applications and Systems Conference, IPAS 2022; Conference date: 5 December 2022 through 7 December 2022; Conference code: 187000; All Open Access, Green Open Access}
}

@ARTICLE{Geollegue2022727,
	author = {Geollegue, Kim Wallie Vergara and Arboleda, Edwin Romeroso and Dizon, Andy Agustin},
	title = {Seed of rice plant classification using coarse tree classifier},
	year = {2022},
	journal = {IAES International Journal of Artificial Intelligence},
	volume = {11},
	number = {2},
	pages = {727 – 735},
	doi = {10.11591/ijai.v11.i2.pp727-735},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129193999&doi=10.11591%2fijai.v11.i2.pp727-735&partnerID=40&md5=b5e28f4564fcb632a3108d1b2f71072b},
	affiliations = {Department of Computer and Electronics Engineering, Cavite State University, Indang, Philippines},
	abstract = {The goal of this paper is to help the agriculture to have consistent observation in the status of seeds in rice plants and have a good quality post-production by classifying the seeds automatically leading to reduction of low-quality rice plants while achieving higher demands in exportation as the quality increases. Additionally, manually observing the seeds of rice plants does not give an accurate evaluation as factors such as fatigue and emotion can affect the result. Using image processing and color feature extraction, it extracted the red, green, and blue (RGB) color feature lying in the pixel point of the seed in the healthy and unhealthy images of rice plants and classified by coarse tree classifier (CTC). The classifier achieved a 100% accuracy and training time of 0.32189 seconds, hence the fitted machine learning approach in the study. © 2022, Institute of Advanced Engineering and Science. All rights reserved.},
	author_keywords = {Coarse tree classifier; Image processing; Machine learning; RGB values; Rice seed plant},
	correspondence_address = {K.W.V. Geollegue; Department of Computer and Electronics Engineering, Cavite State University, Indang, Cavite, Philippines; email: kimwallie.geollegue@cvsu.edu.ph},
	publisher = {Institute of Advanced Engineering and Science},
	issn = {20894872},
	language = {English},
	abbrev_source_title = {IAES Int. J. Artif. Intell.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Carranza-Rojas20222014,
	author = {Carranza-Rojas, Jose and Gonzalez-Villanueva, Ruben and Jimenez-Morales, Kelvin and Quesada-Montero, Kevin and Esquivel-Barboza, Esteban A. and Carvajal-Barboza, Nicole},
	title = {Extreme Automatic Plant Identification Under Constrained Resources},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3180},
	pages = {2014 – 2024},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136926182&partnerID=40&md5=3ca6280127db499848e9d12d96c13fc8},
	affiliations = {Costa Rica Institute of Technology, Cartago, Costa Rica; Microsoft, San Jose, Costa Rica},
	abstract = {The estimated amount of plant species in our planet Earth is calculated to be around 400, 000. In order to create an automatic plant identification system that aims to identify any plant species on Earth, machine learning techniques must scale to a high volume of images and species. This leads to Extreme Classification, an area of machine learning that aims to develop models that can classify among hundreds of thousands, or even millions of classes. This work depicts BioMachina’s team participation in the PlantCLEF 2022 challenge. Our approach was based on deep learning techniques for constrained environments, where resources are scarce for the creation of large models to deal with the considerable amount of species of the challenge. Additionally to using several training techniques to alleviate resource consumption, we developed a 2-level hierarchical softmax. By simulating a small and inferred plant taxonomy, we allowed the model to learn a 2 level hierarchy of classes on its own, reducing model sizes significantly. Our implementation of hierarchical softmax resulted in position 4 of the overall PlantCLEF 2022 ranking, while keeping model sizes reasonably small and computationally efficient, with a 5.67x reduction of parameters compared to vanilla softmax. © 2022 Copyright for this paper by its authors.},
	author_keywords = {Automatic Plant Identification; Extreme Classification; Hierarchical Softmax; Taxonomy},
	keywords = {Deep learning; Earth (planet); Learning algorithms; Automatic plant identification; Constrained resources; Extreme classification; Hierarchical softmax; Model size; Planet earth; Plant identification; Plant identification systems; Plant species; Under-constrained; Taxonomies},
	correspondence_address = {J. Carranza-Rojas; Costa Rica Institute of Technology, Cartago, Costa Rica; email: jcarranza@itcr.ac.cr},
	editor = {Faggioli G. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Ferro N. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Hanbury A. and Vienna University of Technology, Favoritenstrasse 9, Vienna and Potthast M. and University of Leipzig, Augustusplatz 10, Leipzig},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 Conference and Labs of the Evaluation Forum, CLEF 2022; Conference date: 5 September 2022 through 8 September 2022; Conference code: 181762}
}

@CONFERENCE{Chandani2022543,
	author = {Chandani, Priyanka and Gupta, Sh. Sachin and Patnaik, M. S. Pradeep Kumar and Munagala, N V L M Krishna and Sivasangari, A. and Tannady, Hendy},
	title = {Efficient Plant Disease Prediction based on Convolutional Neural Network using Optimized Proposed Logistic Decision Regression},
	year = {2022},
	journal = {Proceedings of International Conference on Technological Advancements in Computational Sciences, ICTACS 2022},
	pages = {543 – 548},
	doi = {10.1109/ICTACS56270.2022.9988195},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146368642&doi=10.1109%2fICTACS56270.2022.9988195&partnerID=40&md5=f1fd4d909eb3c613f0943e417a271fb2},
	affiliations = {CSE-DS, Noida Institute of Engineering and Technology, Uttar Pradesh, Greater Noida, India; Sanskriti University, Department of Management, Uttar Pradesh, Mathura, India; Gitam Deemed to Be University, Department of Electrical Electronics and Communication Engineering, Andhra Pradesh, Visakhapatnam, India; Sathyabama Institute of Science and Technology, Tamilnadu, Chennai, India; Universitas Multimedia, Department of Management, Nusantara, Indonesia},
	abstract = {Agriculture nature is important for growing plants with supports of artificial intelligence. This work aims to detect the disease in the leaves, realizing the image analysis and classification technology. Manual identification of medicinal plants is a time-consuming process that requires the help of plant identification experts and manual identification of medicinal plants is a time-consuming process that requires the help of plant identification experts. Specifically, there are several innovations in image segmentation and recognition system for plant disease detection. In this way, to proposed Logistic Decision Regression (LDR) algorithm and Convolutional Neural Network (CNN) is implemented detecting the feature selection and classification. Initially the preprocessing and filter process correction task is usually performed by the wrapping filters. Then LDR feature selection is used to select the best features of medicinal plants for reducing classification problems. Leaves are most used to identify medicinal plants, also stems, flowers, petals, seeds, and even the entire plant used in an automated process. An automated disease detection system is based on the development of changes in the disease status of the plant's leave. For Convolutional Neural Network (CNN), it uses a complex feed-forward neural network, and a CNN has high accuracy in image classification and recognition. After evaluating the results of different image training library systems, effective image recognition function has been demonstrated to have high precision and strong reliability.  © 2022 IEEE.},
	author_keywords = {classification; Convolutional Neural Network (CNN); Image Recognition; Internet of Things (IoT); LDR; Medicinal plant; Plant Disease},
	keywords = {Automation; Classification (of information); Convolution; Convolutional neural networks; Feature extraction; Image classification; Image recognition; Image segmentation; Plants (botany); Convolutional neural network; Images classification; Internet of thing; Logistic decision regression; Logistics decisions; Manual identification; Medicinal plants; Plant disease; Plant identification; Internet of things},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166547657-7},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Technol. Adv. Comput. Sci., ICTACS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2nd International Conference on Technological Advancements in Computational Sciences, ICTACS 2022; Conference date: 10 October 2022 through 12 October 2022; Conference code: 185594}
}

@CONFERENCE{Ali2022,
	author = {Ali, Asfand Yar and Fahad, Labiba Gillani},
	title = {Multi-Organ Plant Classification Using Deep Learning},
	year = {2022},
	journal = {2022 24th International Multitopic Conference, INMIC 2022},
	doi = {10.1109/INMIC56986.2022.9972979},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145439251&doi=10.1109%2fINMIC56986.2022.9972979&partnerID=40&md5=de773f94defda7c0181623530473d1a1},
	affiliations = {Fast (NUCES), Department of Ai and Data Science, Islamabad, Pakistan},
	abstract = {The variability in the shape and appearance of the same plant organs and similarity between organs of different plants results in fewer inter-class and high intra-class variations making organ-based plant classification a challenging problem. Classification of plants using a single organ may not be able to deal with these challenges. Thus the use of multiple organs can be more effective in improving the classification performance by learning different aspects of the same class. Existing approaches mainly focus on generic features of plants while ignoring features related to multiple organs. In the proposed approach, Convolutional Neural Network (CNN) is used to exploit the information of multiple organs instead of a single organ for the classification of plants. Moreover, the representation of minority classes is increased through DC GAN. The comparison of the proposed approach with the existing approaches on the publicly available PlantCLEF dataset shows its better performance in the accurate classification of plants.  © 2022 IEEE.},
	author_keywords = {Convolutional Neural network (CNN); Generative Adversarial Networks(GANs); generic; Multi-Organ},
	keywords = {Classification (of information); Convolution; Convolutional neural networks; Deep learning; Classification performance; Convolutional neural network; Generic; Generic features; Inter class; Intra-class variation; Multi-organ; Plant classification; Plant organs; Generative adversarial networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835039710-9},
	language = {English},
	abbrev_source_title = {Int. Multitopic Conf., INMIC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 24th International Multitopic Conference, INMIC 2022; Conference date: 21 October 2022 through 22 October 2022; Conference code: 185085}
}

@CONFERENCE{Chang2022152,
	author = {Chang, Limei and Ding, Xuewen},
	title = {Research on classification and recognition method of plant leaves based on deep learning},
	year = {2022},
	journal = {Proceedings - 2022 International Symposium on Advances in Informatics, Electronics and Education, ISAIEE 2022},
	pages = {152 – 155},
	doi = {10.1109/ISAIEE57420.2022.00039},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152189644&doi=10.1109%2fISAIEE57420.2022.00039&partnerID=40&md5=0281921d882bba7a73a6cbaa93d47c67},
	affiliations = {Tianjin University of Technology and Education, Tianjin, China},
	abstract = {With the development of global science technology and economy, the coordination of social relations and ecological relations has become an important problem that needs to be solved urgently in contemporary development. Tree resources are the foundation of the ecosystem. In order to make better use of tree resources and implement strategies tailored to local conditions, it is necessary to identify the types of trees. Deep learning is an important method to distinguish tree species by analyzing leaves. Deep learning provides incomparable advantages of traditional object detection methods for accurately extracting the deep features and classification of leaf images. In order to achieve accurate and rapid classification and identification of plant leaves, this paper uses two network models, YOLOv5 network and image recognition ResNet-50 based on faster regions with convolutional (Faster-RCNN), to make a comparative study on the constructed 10 types of common plant leaf datasets. The experimental results show that yolov5 network can quickly and accurately identify plant leaves, and its lightweight model can be easily deployed on mobile terminals. Faster-RCNN network model can accurately extract the multi-layer feature images of plant leaves to obtain a average accuracy that is 1.1% higher than that of the YOLOv5, but the recognition speed is slower. It can be used for fine classification and leaf identification in subsequent plant experiments.  © 2022 IEEE.},
	author_keywords = {deep learning; Faster-RCNN; mAP; plant recognition; Yolov5},
	keywords = {Classification (of information); Deep learning; Ecology; Learning systems; Object detection; Plants (botany); Classification and recognition; Classification methods; Deep learning; Fast region with convolutional; MAP; Network models; Plant leaves; Plant recognition; Recognition methods; Yolov5; Image recognition},
	correspondence_address = {X. Ding; Tianjin University of Technology and Education, Tianjin, China; email: dingxw1@126.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166546357-7},
	language = {English},
	abbrev_source_title = {Proc. - Int. Symp. Adv. Informatics, Electron. Educ., ISAIEE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Symposium on Advances in Informatics, Electronics and Education, ISAIEE 2022; Conference date: 17 December 2022 through 19 December 2022; Conference code: 187546}
}

@ARTICLE{Labrighli2022814,
	author = {Labrighli, Khaoula and Moujahdi, Chouaib and El Oualidi, Jalal and Rhazi, Laila},
	title = {Artificial Intelligence for Automated Plant Species Identification: A Review},
	year = {2022},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {13},
	number = {10},
	pages = {814 – 825},
	doi = {10.14569/IJACSA.2022.0131097},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141756901&doi=10.14569%2fIJACSA.2022.0131097&partnerID=40&md5=86927c19297b8238df772bb989f38c11},
	affiliations = {Department of Botany & Plant Ecology, Scientific Institute of Rabat, Mohammed V University in Rabat, Morocco; Laboratory of Botany, Mycology & Environment, Research Center of Plant and Microbial Biotechnology, Biodiversity & Environment, Faculty of Sciences, Mohammed V University in Rabat, Morocco},
	abstract = {Plants are very important for life on Earth. There is a wide variety of plant species and their number increases each year. The plants identification using conventional keys is complex, takes time and it is frustrating for non-experts because of the use of specific botanical terms/techniques. This creates a difficult obstacle to overcome for novices interested in acquiring knowledge about species, which is very important to develop any environmental study, like climate change anticipation models for example. Today, there is an increasing interest in automating the species identification process. The availability and omnipresence of relevant technologies, such as digital cameras, mobile devices, pattern recognition and artificial intelligence techniques in general, have allowed the idea of automated species identification to become a reality. In this paper, we present a review of automated plant identification over all significant available studies in literature. The main result of this synthesis is that the performance of advanced deep learning models, despite the presence of several challenges, is becoming close to the most advanced human expertise. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.},
	author_keywords = {Artificial intelligence; Deep learning; Machine learning; Plants identification; Species},
	keywords = {Automation; Climate change; Deep learning; Digital devices; Learning systems; Pattern recognition; Conventional keys; Deep learning; Environmental studies; Identification process; Machine-learning; Plant identification; Plant species; Plant species identification; Species; Species identification; Climate models},
	publisher = {Science and Information Organization},
	issn = {2158107X},
	language = {English},
	abbrev_source_title = {Intl. J. Adv.  Comput. Sci. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Goyal202232243,
	author = {Goyal, Neha and Kumar, Nitin and Kapil},
	title = {Leaf Bagging: A novel meta heuristic optimization based framework for leaf identification},
	year = {2022},
	journal = {Multimedia Tools and Applications},
	volume = {81},
	number = {22},
	pages = {32243 – 32264},
	doi = {10.1007/s11042-022-12825-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128092630&doi=10.1007%2fs11042-022-12825-z&partnerID=40&md5=c6045736cbefba2dd4ae6bd5f90bedc6},
	affiliations = {National Institute of Technology, Kurukshetra, India},
	abstract = {Automated plant recognition based on leaf images is a challenging task among the researchers from several fields. This task requires distinguishing features derived from leaf images for assigning class label to a leaf image. There are several methods in literature for extracting such distinguishing features. In this paper, we propose a novel automated framework for leaf identification. The proposed framework works in multiple phases i.e. pre-processing, feature extraction, classification using bagging approach. Initially, leaf images are pre-processed using image processing operations such as boundary extraction and cropping. In the feature extraction phase, popular nature inspired optimization algorithms viz. Spider Monkey Optimization (SMO), Particle Swarm Optimization (PSO) and Gray Wolf Optimization (GWO) have been exploited for reducing the dimensionality of features. In the last phase, a leaf image is classified by multiple classifiers and then output of these classifiers is combined using majority voting. The effectiveness of the proposed framework is established based on the experimental results obtained on three datasets i.e. Flavia, Swedish and self-collected leaf images. On all the datasets, it has been observed that the classification accuracy of the proposed method is better than the individual classifiers. Furthermore, the classification accuracy for the proposed approach is comparable to deep learning based method on the Flavia dataset. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Bootstrap aggregating; Ensemble approach; Feature selection; Leaf classification; Nature-based optimization},
	keywords = {Biomimetics; Classification (of information); Deep learning; Extraction; Image processing; Particle swarm optimization (PSO); Statistical methods; Bootstrap aggregating; Classification accuracy; Ensemble approaches; Features extraction; Features selection; Leaf classification; Leaf identification; Leaf images; Nature-based optimization; Optimisations; Feature extraction},
	correspondence_address = {N. Goyal; National Institute of Technology, Kurukshetra, India; email: neha.goyal2309@gmail.com},
	publisher = {Springer},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Sukumar2022612,
	author = {Sukumar, G. and Priya, W. Deva},
	title = {Classification of Flower Species based on Flower Texture to Improve Accuracy of Classifier using Linear Regression and Comparing with SVM algorithm},
	year = {2022},
	journal = {Journal of Pharmaceutical Negative Results},
	volume = {13},
	pages = {612 – 618},
	doi = {10.47750/pnr.2022.13.S04.068},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143201086&doi=10.47750%2fpnr.2022.13.S04.068&partnerID=40&md5=06bf610ee85bb96b7ab38a8b885b7f5e},
	affiliations = {Department of Computer Science and Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Saveetha University, Tamil Nadu, Chennai, 602105, India},
	abstract = {Aim: Classification of flower species based on innovative flower texture to improve accuracy of classifier using linear regression and comparing with SVM algorithm. Methods and Materials: Flower species recognition is performed using Linear Regression (N=10) over SVM (N=10) with the split size of training and testing dataset 70% and 30% respectively. Calculation of samples is done by using G power of 80% which contains two different groups, alpha (0.05), power (80%) and environment ratio 1. Results: Linear Regression has significantly better accuracy (95.9%) compared to SVM (93.3%) and attained significance value of p = 0.01. Conclusion: Linear regression achieved significantly better flower recognition than SVM for identifying the different types of flower species. © 2022 Wolters Kluwer Medknow Publications. All rights reserved.},
	author_keywords = {Flower Recognition; Flower Species; Innovative Flower Texture; Linear Regression; Machine Learning; SVM},
	keywords = {algorithm; article; calculation; classifier; clinical article; flower; linear regression analysis; machine learning},
	publisher = {ResearchTrentz Academy Publishing Education Services},
	issn = {09769234},
	language = {English},
	abbrev_source_title = {J. Pharm. Negat. Results},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Bronze Open Access}
}

@ARTICLE{Figueroa-Mata2022,
	author = {Figueroa-Mata, Geovanni and Mata-Montero, Erick and Valverde-Otárola, Juan Carlos and Arias-Aguilar, Dagoberto and Zamora-Villalobos, Nelson},
	title = {Using Deep Learning to Identify Costa Rican Native Tree Species From Wood Cut Images},
	year = {2022},
	journal = {Frontiers in Plant Science},
	volume = {13},
	doi = {10.3389/fpls.2022.789227},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128472696&doi=10.3389%2ffpls.2022.789227&partnerID=40&md5=64959cc6060a02ebfe1d2412594a83c9},
	affiliations = {School of Mathematics, Costa Rica Institute of Technology, Cartago, Costa Rica; School of Computing, Costa Rica Institute of Technology, Cartago, Costa Rica; School of Forestry Engineering, Costa Rica Institute of Technology, Cartago, Costa Rica; Cooperativa de Productividad Forestal, Facultad de Ciencias Forestales, Universidad de Concepción, Concepción, Chile},
	abstract = {Tree species identification is critical to support their conservation, sustainable management and, particularly, the fight against illegal logging. Therefore, it is very important to develop fast and accurate identification systems even for non-experts. In this research we have achieved three main results. First, we developed—from scratch and using new sample collecting and processing protocols—an dataset called CRTreeCuts that comprises macroscopic cross-section images of 147 native tree species from Costa Rica. Secondly, we implemented a CNN for automated tree species identification based on macroscopic images of cross-sections of wood. For this CNN we apply the fine-tuning technique with VGG16 as a base model, pre-trained with the ImageNet data set. This model is trained and tested with a subset of 75 species from CRTreeCuts. The top-1 and top-3 accuracies achieved in the testing phase are 70.5% and 80.3%, respectively. The Same-Specimen-Picture Bias (SSPB), which is known to erroneously increase accuracy, is absent in all experiments. Finally, the third result is Cocobolo, an Android mobile application that uses the developed CNN as back-end to identify Costa Rican tree species from images of cross-sections of wood. Copyright © 2022 Figueroa-Mata, Mata-Montero, Valverde-Otárola, Arias-Aguilar and Zamora-Villalobos.},
	author_keywords = {automated image-based tree species identification; convolutional neural network; costa rican tree species; deep learning; plant classification; xylotheques},
	correspondence_address = {G. Figueroa-Mata; School of Mathematics, Costa Rica Institute of Technology, Cartago, Costa Rica; email: gfigueroa@itcr.ac.cr},
	publisher = {Frontiers Media S.A.},
	issn = {1664462X},
	language = {English},
	abbrev_source_title = {Front. Plant Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Jaju2022307,
	author = {Jaju, Sanay and Chandak, Manoj},
	title = {A Transfer Learning Model Based on ResNet-50 for Flower Detection},
	year = {2022},
	journal = {Proceedings - International Conference on Applied Artificial Intelligence and Computing, ICAAIC 2022},
	pages = {307 – 311},
	doi = {10.1109/ICAAIC53929.2022.9792697},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133452587&doi=10.1109%2fICAAIC53929.2022.9792697&partnerID=40&md5=514562d3a124c05b902b077ec9185f5a},
	affiliations = {Shri Ramdeobaba College of Engineering and Management, Department of Computer Science and Engineering, Nagpur, India},
	abstract = {The world is full of data. Each frame of time has a huge amount of data generated like this report that you are watching and the text you are reading. So, to handle this data and classify it, major deep learning algorithms are developed. This paper proposes a method to perform object detection to classify images according to its content. The method is a transfer learning-based object detection method. The diverse convolutional layers perform different task in order to understand the mage and classify it. The dataset that was used is a flower recognition dataset from Kaggle. The results that are obtained after evaluating this method shows how transfer learning can be used for creating new models. This in turn gives people hope that everything is not needed to be made from scratch. The aim is to explore the field of deep learning and its various concepts to solve the problem of object detection for classifying data. © 2022 IEEE.},
	author_keywords = {CNN; Deep Learning; Object Detection; ResNet-50; Transfer Learning},
	keywords = {Classification (of information); Convolutional neural networks; Deep learning; Image classification; Learning algorithms; Learning systems; Object recognition; Transfer learning; Deep learning; Flower detections; Flower recognition; Learning models; Model-based OPC; Object detection method; Objects detection; Resnet-50; Transfer learning; Object detection},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549710-7},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Appl. Artif. Intell. Comput., ICAAIC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 1st International Conference on Applied Artificial Intelligence and Computing, ICAAIC 2022; Conference date: 9 May 2022 through 11 May 2022; Conference code: 180160}
}

@CONFERENCE{Ihsan2022373,
	author = {Ihsan, Muhammad Fikri and Sunyoto, Andi and Arief, Muhammad Rudyanto},
	title = {Gray Level Co-Occurrence Matrix Algorithm and Backpropagation Neural Networks for Herbal Plants Identification},
	year = {2022},
	journal = {ICOIACT 2022 - 5th International Conference on Information and Communications Technology: A New Way to Make AI Useful for Everyone in the New Normal Era, Proceeding},
	pages = {373 – 378},
	doi = {10.1109/ICOIACT55506.2022.9972087},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145359556&doi=10.1109%2fICOIACT55506.2022.9972087&partnerID=40&md5=ded8181b6b53530c31dcb9920f8693f7},
	affiliations = {Master of Informatics Engineering, Universitas Amikom Yogyakarta, Yogyakarta, Indonesia},
	abstract = {Leaves are one of the most crucial plant structures. Leaves function as absorption, processing food through photosynthesis, and as a means of transpiration. Various medicinal plants have been known to the public for a long time. However, it is still difficult for ordinary people to remember that all leaves are relatively the same color, green, and do not know the characteristics of the leaves. In this paper, we will detect the types of herbal leaves of medicinal plants by identifying leaves using the GLCM (Gray Level Co-occurrence Matrix) algorithm and artificial neural networks backpropagation. The dataset obtained from the Kaggle-Leafsnap dataset has as many as 50 types of herbal plant leaves. The GLCM algorithm is a method with probability and statistics. This method extracts leaf images, converts the image into grayscale, and then changes the shape of the image data into numerical. The GLCM algorithm is combined with Backpropagation in classifying data. There are several stages in the classification process in this study, namely data retrieval, data preprocessing, data classification, and evaluation method. This research aims to achieve high accuracy through the proposed method.  © 2022 IEEE.},
	author_keywords = {Backpropagation; GLCM; Herbal Leaf; Leafsnap},
	keywords = {Classification (of information); Image processing; Matrix algebra; Neural networks; Plant extracts; Back-propagation neural networks; Gray-level co-occurrence matrix; Grey-level co-occurrence matrixes; Herbal leaf; Herbal plants; Leafsnap; Matrix algorithms; Medicinal plants; Plant identification; Plant structures; Numerical methods},
	correspondence_address = {A. Sunyoto; Master of Informatics Engineering, Universitas Amikom Yogyakarta, Yogyakarta, Indonesia; email: andi@amikom.ac.id},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166545140-6},
	language = {English},
	abbrev_source_title = {ICOIACT - Int. Conf. Inf. Commun. Technol.: A New Way Make AI Useful Everyone New Norm. Era, Proceeding},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 5th International Conference on Information and Communications Technology, ICOIACT 2022; Conference date: 24 August 2022 through 25 August 2022; Conference code: 185076}
}

@ARTICLE{Ganguly2022,
	author = {Ganguly, Shreyan and Bhowal, Pratik and Oliva, Diego and Sarkar, Ram},
	title = {BLeafNet: A Bonferroni mean operator based fusion of CNN models for plant identification using leaf image classification},
	year = {2022},
	journal = {Ecological Informatics},
	volume = {69},
	doi = {10.1016/j.ecoinf.2022.101585},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125131501&doi=10.1016%2fj.ecoinf.2022.101585&partnerID=40&md5=e989f50a289ce0e7774f0d701d29012b},
	affiliations = {Department of Construction Engineering, Jadavpur University, India; Department of Instrumentation and Electronics Engineering, Jadavpur University, India; Depto. Innovación Basada en la Información y el Conocimiento, Universidad de Guadalajara, CUCEI, Mexico; Department of Computer Science and Engineering, Jadavpur University, India; School of Computer Science and Robotics, Tomsk Polytechnic University, Tomsk, Russian Federation},
	abstract = {Plants, the only natural source of oxygen, are the most important resources for every species in the world. A proper identification of plants is important for different fields. The observation of leaf characteristics is a popular method as leaves are easily available for examination. Researchers are increasingly applying image processing techniques for the identification of plants based on leaf images. In this paper, we have proposed a leaf image classification model, called BLeafNet, for plant identification, where the concept of deep learning is combined with Bonferroni fusion learning. Initially, we have designed five classification models, using ResNet-50 architecture, where five different inputs are separately used in the models. The inputs are the five variants of the leaf grayscale images, RGB, and three individual channels of RGB - red, green, and blue. For fusion of the five ResNet-50 outputs, we have used the Bonferroni mean operator as it expresses better connectivity among the confidence scores, and it also obtains better results than the individual models. We have also proposed a two-tier training method for properly training the end-to-end model. To evaluate the proposed model, we have used the Malayakew dataset, collected at the Royal Botanic Gardens in New England, which is a very challenging dataset as many leaves from different species have a very similar appearance. Besides, the proposed method is evaluated using the Leafsnap and the Flavia datasets. The obtained results on both the datasets confirm the superiority of the model as it outperforms the results achieved by many state-of-the-art models. © 2022 Elsevier B.V.},
	author_keywords = {Bonferroni operator; Deep learning; Ensemble learning; Leaf image; Plant identification},
	keywords = {England; New England; United Kingdom; United States; connectivity; identification method; image classification; image processing},
	correspondence_address = {D. Oliva; Depto. Innovación Basada en la Información y el Conocimiento, Universidad de Guadalajara, CUCEI, Mexico; email: diego.oliva@cucei.udg.mx},
	publisher = {Elsevier B.V.},
	issn = {15749541},
	language = {English},
	abbrev_source_title = {Ecol. Informatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19}
}

@ARTICLE{Zheng2022799,
	author = {Zheng, Jiawen and Li, Junqiu and Zhang, Qinghui and He, Xin},
	title = {Intelligent Cultivation System of Green Plant Based on Convolution Neural Network and STM32},
	year = {2022},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {920 LNEE},
	pages = {799 – 806},
	doi = {10.1007/978-981-19-3927-3_78},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135025319&doi=10.1007%2f978-981-19-3927-3_78&partnerID=40&md5=187aa60128e04adfde8ce7dbadbea0f6},
	affiliations = {School of Big Data and Intelligent Engineering, University of Southwest Forestry, Kunming, 650051, China},
	abstract = {An intelligent green plant cultivation system based on convolution neural network and STM32 is designed, which is designed to be applied to indoor green plant maintenance, intelligent greenhouse and other fields. The design idea of intelligently identifying the species of green plants and relating their growth habits to the cultivation process was put forward. Device control, data display, taking photos of green plants and sending pictures to the server are integrated in the mobile terminal. The server uses the Netty framework to realize the communication between the mobile phone and the lower computer, the deep learning algorithm is used to identify plant species and the growth habit parameters of green plants and the configuration parameters of hardware devices are stored in the database. STM32F103C8T6 is selected as the core controller of the hardware system, which forms a closed loop between the suitable plant growth environment and the control process. Then watering, lighting, collecting plant growth environment parameters are controlled. MXNet deep learning framework is used for green plant recognition. Identity Block and Convolution Block are used to build ResNet. Combined with random gradient descent algorithm, the algorithm is implemented in Windows Server R2 enterprise in the 64 bit operating system, the model parameters are obtained by training 3000 green plant image samples of 7 varieties. The system puts forward the design idea that through the intelligent identification of green plant species, we can know its growth habits, and ultimately relate to its automatic cultivation process, which can be applied to the fields of intelligent greenhouse, flower gardening and so on. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Android; CNN; Green plant identification; Netty; STM32},
	keywords = {Android (operating system); Convolution; Deep learning; Display devices; Image processing; Learning algorithms; Parameter estimation; Process control; Smartphones; Android; Convolution neural network; Design ideas; Green plant identification; Green plants; Growth habit; Intelligent greenhouse; Netty; Plant identification; STM32; Gradient methods},
	correspondence_address = {J. Li; School of Big Data and Intelligent Engineering, University of Southwest Forestry, Kunming, 650051, China; email: 2453575333@qq.com},
	editor = {S. Shmaliy Y. and Abdelnaby Zekry A.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981193926-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 6th International Conference on Computing, Control, and Industrial Engineering, CCIE 2021; Conference date: 15 January 2022 through 16 January 2022; Conference code: 280349}
}

@CONFERENCE{Viodor20221,
	author = {Viodor, Ariel Christian C. and Aliac, Chris Jordan G. and Santos-Feliscuzo, Larmie T.},
	title = {Mangrove Species Identification Using Deep Neural Network},
	year = {2022},
	journal = {Proceeding - 6th International Conference on Information Technology, Information Systems and Electrical Engineering: Applying Data Sciences and Artificial Intelligence Technologies for Environmental Sustainability, ICITISEE 2022},
	pages = {1 – 6},
	doi = {10.1109/ICITISEE57756.2022.10057793},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150443000&doi=10.1109%2fICITISEE57756.2022.10057793&partnerID=40&md5=587bdd26d2466b86c7966ebfcd831c4b},
	affiliations = {College of Computer Studies, Cebu Institute of Technology-University, Cebu City, Philippines},
	abstract = {Mangroves play an essential component in the coastal ecosystem and the community. Identifying mangrove plant species is critical for biodiversity assessment, restoration, or conservation programs apart from ecosystem protection from anthropogenic activities. Yet, comprehensive data for managing coastal ecosystems sustainably are being gathered manually. Furthermore, identifying mangrove species by traditional keys is complicated, time-consuming, and frustrating due to technical terminology for non-experts. The growing need to identify mangroves in a timely and cost-effective manner motivates our study to create an automated approach for classifying the species from images of mangrove leaves. Recent advances in deep learning algorithms have resulted in studies demonstrating excellent performance identification tasks for plant species. This paper examines existing deep neural network-based machine learning applications for identifying plants designed for hand-held device usage. Driven by low-cost computation, size, and time requirements, we propose the use of MobileNet architecture and transfer learning for leaf-based plant recognition. We provide a dataset of images comprising five (5) several types of mangrove species. Then, we perform experimental validation on the proposed state-of-the-art architecture and transfer learning. Our results show that accuracy up to 97.07% was achieved using MobileNetV3Small and transfer learning for mangrove species identification proving the viability of its use in mobile applications. The saved model has a low-cost computation that runs within a smartphone application.  © 2022 IEEE.},
	author_keywords = {Deep Neural Network; Mangroves Species; Mobile Application; MobileNet; Transfer Learning},
	keywords = {Biodiversity; Conservation; Cost effectiveness; Ecosystems; Learning algorithms; Learning systems; Mobile computing; Network architecture; Terminology; Architecture learning; Coastal ecosystems; Low-costs; Mangrove species; Mobile applications; Mobilenet; Plant species; Restoration project; Species identification; Transfer learning; Deep neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835039961-5},
	language = {English},
	abbrev_source_title = {Proceeding - Int. Conf. Inf. Technol., Inf. Syst. Electr. Eng.: Appl. Data Sci. Artif. Intell. Technol. Environ. Sustain., ICITISEE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 6th International Conference on Information Technology, Information Systems and Electrical Engineering, ICITISEE 2022; Conference date: 13 December 2022 through 14 December 2022; Conference code: 187142}
}

@ARTICLE{Lu20222549,
	author = {Lu, Zhiguo and Zheng, Yuhong and Zhang, Pengchong and Fan, Boyuan and Yu, Aimin and Fu, Li},
	title = {Electrochemical Identification of Yulania spp. by Fingerprinting of Leaves Using Glassy Carbon Electrode},
	year = {2022},
	journal = {Phyton-International Journal of Experimental Botany},
	volume = {91},
	number = {11},
	pages = {2549 – 2558},
	doi = {10.32604/phyton.2022.021288},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144207961&doi=10.32604%2fphyton.2022.021288&partnerID=40&md5=ddf858d811cd0bc0da71d370cbeae3ba},
	affiliations = {Institute of Botany, Jiangsu Province and Chinese Academy of Sciences (Nanjing Botanical Garden, Mem. Sun Yat-Sen), Nanjing, 210014, China; Hangzhou Botanical Garden (Hangzhou West Lake Academy of Landscape Science), Hangzhou, 310013, China; College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; Department of Chemistry and Biotechnology, Faculty of Science, Engineering and Technology, Swinburne University of Technology, Hawthorn, 3122, VIC, Australia},
	abstract = {In this communication, we used electrochemical sensor for recording the electrochemical profiles of eleven species of Yulania spp. from leaf extract. Two solvents and two buffer conditions were used for electrochemical fingerprints collection. Their electrochemical fingerprints can be converted to different patterns and consequently for species recognition. The results indicate the pattern recognition is much convenient than that of the recognition of species directly using voltammetric signal. The current information in electrochemical fingerprinting repre-sents the type and amount of electrochemically active molecules, which linked to the genetic differences among the plants. Therefore, the electrochemical fingerprints were applied for further phylogenetic study. The phylogenetic tree deduced from voltametric curves is divided into three main groups. The first clade contains Y. denudate, Liriodendron chinense, Y. cylindrica, Y. biondii, Y. sprengeri. The second clade contains Y. zenii, Y. liliiflora, Y. kobus, and Y. amoena. The third clade contains Y. × soulangeana, Manglietia fordiana and Y. sinostellata. In addi-tion, Y. salicifolia is not in these main clades. The results demonstrate that electrochemical fingerprinting can be used as a com-plementary tool in the study of phylogenetics. © 2022, Tech Science Press. All rights reserved.},
	author_keywords = {chemotaxonomy; fingerprints; Phylogenetic; plant identification; Yulania spp},
	correspondence_address = {Y. Zheng; Institute of Botany, Jiangsu Province and Chinese Academy of Sciences (Nanjing Botanical Garden, Mem. Sun Yat-Sen), Nanjing, 210014, China; email: zhengyuhong@cnbg.net; L. Fu; College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; email: fuli@hdu.edu.cn},
	publisher = {Tech Science Press},
	issn = {00319457},
	coden = {PHYBA},
	language = {English},
	abbrev_source_title = {Phyton-International Journal of Experimental Botany},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@CONFERENCE{Susa2022,
	author = {Susa, Julie Ann B. and Nombrefia, Wendy C. and Abustan, Alfredo S. and MacAlisang, Jonel and Maaliw, Renato R.},
	title = {Deep Learning Technique Detection for Cotton and Leaf Classification Using the YOLO Algorithm},
	year = {2022},
	journal = {SIST 2022 - 2022 International Conference on Smart Information Systems and Technologies, Proceedings},
	doi = {10.1109/SIST54437.2022.9945757},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143428842&doi=10.1109%2fSIST54437.2022.9945757&partnerID=40&md5=53f610115bffc2471cef2dfcc781977c},
	affiliations = {College Of Engineering, Southern Luzon State University, Lucban, Quezon, Philippines; Southern Luzon State University, Agriculture Department, Infanta, Quezon, Philippines; Southern Luzon State, University, Industrial Technology Department, Infanta, Quezon, Philippines; Technological University Of The Philippines, Technology Licensing Office- Itso, Manila, Philippines},
	abstract = {Cotton is one of the world's most significant crops, and it is widely grown. It is vulnerable to a number of plant diseases, resulting in a significant reduction in yield and productivity. It is critical to discover a disease at an early stage to receive prompt diagnosis and treatment. As a result, a deep learning approach was used to suggest a cotton plant classification system. The YOLOv3 algorithm was used in the study, which is the most important real-time object identification system for detecting and classifying damaged and healthy plants and leaves. The applied model has an mAP (mean Average Precision) score of 96.09 %, with training accuracy of 96.79 % and validation accuracy of 92.26 %. The detection accuracy of video frames ranges between 98 and 99 % in the testing results, whereas the detection accuracy of live stream image frames ranges between 74 and 99 %. As a result, the model outperformed other current algorithms and is the best choice for cotton plant detection and categorization.  © 2022 IEEE.},
	author_keywords = {algorithm; Cotton Plant classification; deep learning; detection; mean Average Precision; YOLOv3},
	keywords = {Deep learning; Diagnosis; Learning algorithms; Learning systems; Object detection; Plants (botany); Cotton plant classification; Cotton plants; Deep learning; Detection; Detection accuracy; Leaf classification; Learning techniques; Mean average precision; Plant classification; YOLOv3; Cotton},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166546790-2},
	language = {English},
	abbrev_source_title = {SIST - Int. Conf. Smart Inf. Syst. Technol., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 International Conference on Smart Information Systems and Technologies, SIST 2022; Conference date: 28 April 2022 through 30 April 2022; Conference code: 184566}
}

@CONFERENCE{Pathak2022164,
	author = {Pathak, Disha Mohini and Srivastava, Somya and Gupta, Shelly},
	title = {Plant Recognition using Convolutional Neural Network},
	year = {2022},
	journal = {Proceedings - 2022 5th International Conference on Computational Intelligence and Communication Technologies, CCICT 2022},
	pages = {164 – 168},
	doi = {10.1109/CCiCT56684.2022.00040},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141686963&doi=10.1109%2fCCiCT56684.2022.00040&partnerID=40&md5=bcacc77cd6a512268ca21cc36662356e},
	affiliations = {ABES Engineering College, Computer Science Department, 19th KM Stone, NH 24 Uttar Pradesh, Ghaziabad, 201009, India},
	abstract = {There are currently around 375000 known species of plants in the world. Expert botanists are able to easily classify and classify them based on either division/phylum, order, class, species, genus or family. But common people like students and new people in the field may find it difficult to classify them appropriately due to lack of experience or exposure to those plants. In the proposed solution, we plan on suggesting a system which would use deep learning models for image processing. This system can be trained on an ample amount of plant leaves images and tree images dataset containing pictures of various plant leaves and trees. Many prominent datasets like Flavia, Swedish Leaf datasets, etc can be used to train the model. The model will be built using a combination of 2-D Convolutional layers, Max Pooling and Dense layers. The system will take the images of the plant or tree as input and the built model will work on and predict the output. The output will be the classification of the plant.  © 2022 IEEE.},
	author_keywords = {Convolutional Neural Network; Data Augmentation; Deep Learning; Image Processing},
	keywords = {Convolution; Deep learning; Forestry; Image processing; Learning systems; Plants (botany); Convolutional neural network; Data augmentation; Deep learning; Image datasets; Images processing; Learning models; Plant leaf images; Plant leaves; Plant recognition; Tree images; Convolutional neural networks},
	editor = {Mittal H.K. and Jain V. and Polkowski Z.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166547224-1},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Comput. Intell. Commun. Technol., CCICT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 5th International Conference on Computational Intelligence and Communication Technologies, CCICT 2022; Conference date: 8 July 2022 through 9 July 2022; Conference code: 183485}
}

@ARTICLE{Boudra2022,
	author = {Boudra, Safia and Yahiaoui, Itheri and Behloul, Ali},
	title = {Tree trunk texture classification using multi-scale statistical macro binary patterns and CNN},
	year = {2022},
	journal = {Applied Soft Computing},
	volume = {118},
	doi = {10.1016/j.asoc.2022.108473},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124416377&doi=10.1016%2fj.asoc.2022.108473&partnerID=40&md5=337741eb2536674dad1be28a5abc43e1},
	affiliations = {University of Batna 2, LaSTIC, Batna, 05000, Algeria; Université de Reims Champagne Ardenne, CReSTIC EA 3804, Reims, 51097, France},
	abstract = {Automated plant classification using tree trunk has attracted increasing interest in the computer vision community as a contributed solution for the management of biodiversity. It is based on the description of the texture information of the bark surface. The multi-scale variants of the local binary patterns have achieved prominent performance in bark texture description. However, these approaches encode the scale levels of the macrostructure separately from each other. In this paper, a novel handcrafted texture descriptor termed multi-scale Statistical Macro Binary Patterns (ms-SMBP) is proposed to encode the characterizing macro pattern of different bark species. The proposed approach consists of defining a sampling scheme at high scale levels and summarizing the intensity distribution using statistical measures. The characterizing macro pattern is encoded by an in-depth gradient that describes the relationship between the scale levels and their adaptive statistical prototype. Besides this handcrafted feature descriptor, a learning-based description is performed with the ResNet34 model for bark classification. Extensive and comprehensive experiments on challenging and large-scale bark datasets demonstrate the effectiveness of ms-SMBP to identify bark species and outperforming different multi-scale LBP approaches. The tree trunk classification with ResNet34 shows interesting results on a very large-scale dataset. © 2022 Elsevier B.V.},
	author_keywords = {Macro binary pattern; ResNet; Statistical description; Texture; Tree Bark},
	keywords = {Binary trees; Biodiversity; Classification (of information); Encoding (symbols); Forestry; Large dataset; Sampling; Statistics; Binary patterns; Macro binary pattern; Multi-scales; Plant classification; Resnet; Scale levels; Statistical descriptions; Texture classification; Tree barks; Vision communities; Textures},
	correspondence_address = {S. Boudra; Université de Reims Champagne Ardenne, CReSTIC EA 3804, Reims, 51097, France; email: safia.boudra@univ-reims.fr; I. Yahiaoui; Université de Reims Champagne Ardenne, CReSTIC EA 3804, Reims, 51097, France; email: itheri.yahiaoui@univ-reims.fr},
	publisher = {Elsevier Ltd},
	issn = {15684946},
	language = {English},
	abbrev_source_title = {Appl. Soft Comput.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Bronze Open Access}
}

@ARTICLE{Cen2022,
	author = {Cen, Yi and Shen, Changming and Zheng, Xiaorong and Li, Junfei and Jiang, Jianwei},
	title = {Development of A Fast Method for Fructus Aurantii Identification by Electrochemical Fingerprint},
	year = {2022},
	journal = {International Journal of Electrochemical Science},
	volume = {17},
	doi = {10.20964/2022.11.66},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141752778&doi=10.20964%2f2022.11.66&partnerID=40&md5=d4cbf3bd959ee948406aa6425d123a06},
	affiliations = {Department of Pharmacy, Ningbo Beilun District People's Hospital, Zhejiang, Ningbo, 315800, China; Department of Pharmacy, Cancer Hospital of the University of Chinese Academy of Sciences (Zhejiang Cancer Hospital), Zhejiang, Hangzhou, 310022, China; Department of Pharmacy, Institute of Cancer and Basic Medicine (IBMC), Chinese Academy of Sciences, Zhejiang, Hangzhou, 310022, China; Integrated Traditional Chinese and Western Medicine Oncology Laboratory, Key Laboratory of Traditional Chinese Medicine of Zhejiang Province, Zhejiang, Hangzhou, 310022, China},
	abstract = {Electrochemical analysis techniques can be used for the identification of plant samples. This work describes the identification of fructus aurantii and its closely related species by electrochemical fingerprinting. For a better extraction of electrochemically active substances, DMSO-CHCl3-CH3OH, 2:2:1, v/v was used as a solvent. Electrochemical fingerprints were collected in two different buffer solutions. The collected fingerprint profiles can be used for density plots construction. The plants can be automatically identified by feature extraction of density plot. The oxidation peaks exhibited in the electrochemical fingerprint are most likely the oxidation of narirutin, naringin, hesperidin, and neohesperidin. Therefore, HPLC was used for the validation of the standards and samples. Finally, electrochemical techniques were used to document the electrochemical behavior of these four substances. The results suggest that narirutin, naringin, hesperidin, and neohesperidin may be the most significant substances contributing to the electrochemical fingerprinting of fructus aurantii. © 2022 The Authors. Published by ESG (www.electrochemsci.org). This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution license (http://creativecommons.org/licenses/by/4.0/).},
	author_keywords = {Feature extraction; Fructus aurantii. electrochemical fingerprints; Pattern recognition; Plant identification},
	correspondence_address = {J. Jiang; Department of Pharmacy, Cancer Hospital of the University of Chinese Academy of Sciences (Zhejiang Cancer Hospital), Hangzhou, Zhejiang, 310022, China; email: jiangjw@zjcc.org.cn},
	publisher = {Electrochemical Science Group},
	issn = {14523981},
	language = {English},
	abbrev_source_title = {Int.J.Electrochem.Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Sapna202263,
	author = {Sapna, R. and Sheshappa, S.N. and Vijayakarthik, P. and Raja, S Pravinth},
	title = {Global Pattern Feedforward Neural Network Structure with Bacterial Foraging Optimization towards Medicinal Plant Leaf Identification and Classification},
	year = {2022},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {13},
	number = {12},
	pages = {63 – 70},
	doi = {10.14569/IJACSA.2022.0131209},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146692560&doi=10.14569%2fIJACSA.2022.0131209&partnerID=40&md5=7f5e601719a85a99a4d0a450eb222c0f},
	affiliations = {Sir MVIT, Affiliated to VTU, Belgaum, India; Department of Computer Science and Engineering, Presidency University, Bengaluru, India; Sir MVIT, Department of Information Science and Engineering, Bangalore, India; Department of Computer Science and Engineering, Presidency University, Bangalore, India},
	abstract = {Medicinal Plant species help to cure various diseases across the world. The automated identification of medicinal plant species to treat disease based on their structure is much required in pharmaceutical laboratories. Plant Species with a complex background in the field will make the detection and classification more difficult. In this paper, optimization of bacterial foraging technique has been employed towards medicinal plant prediction and classification architecture based on feed-forward neural network. It is capable of identifying both complex structures of medicinal plants. Feed-forward Neural Networks are considered to have good recognition accuracy compared to other machine learning approaches. Further bacterial foraging has been implemented to minimize the feature search space to the classifier and provides optimal features for the plant classification. The experimental outcomes of the proposed approach has been analysed by employing the medley dataset and evaluating the performance of the proposed approach with respect to dice similarity coefficient, Specificity and sensitivity towards medicinal plant classification. The findings are very positive, and further research will focus on using a large dataset and increased computing resources to examine how well deep-learning neural networks function in identifying medicinal plants for use in health care. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.},
	author_keywords = {Bacterial forging; Feed-forward neural networks; Linear discriminant analysis; Medicinal plant},
	keywords = {Complex networks; Deep neural networks; Discriminant analysis; Large dataset; Learning systems; Plants (botany); Bacterial foraging optimization; Bacterial forging; Feed forward neural net works; Global patterns; Linear discriminant analyze; Medicinal plants; Neural networks structure; Plant classification; Plant leaves; Plant species; Feedforward neural networks},
	publisher = {Science and Information Organization},
	issn = {2158107X},
	language = {English},
	abbrev_source_title = {Intl. J. Adv.  Comput. Sci. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Kalyani2022,
	author = {Kalyani, Achanta Lakshmi and Singamaneni, Rajesh and Penugonda, Greeshmanth},
	title = {Remembrance of Monocotyledons Using Residual Networks},
	year = {2022},
	journal = {6th IEEE International Conference on Computational System and Information Technology for Sustainable Solutions, CSITSS 2022},
	doi = {10.1109/CSITSS57437.2022.10026401},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147849483&doi=10.1109%2fCSITSS57437.2022.10026401&partnerID=40&md5=3c777fdc078fb5ed76beddad9845818a},
	affiliations = {Vr Siddhartha Engineering College, Dept. of Cse, Vijayawada, India},
	abstract = {Plant identification has several applications in the area of agriculture, ethnopharmacology, horticulture. There are existing systems that identify the plant using leaf images. Because leaves are one of a plant's easily recognizable characteristics, they are frequently used for identifying by prediction based Machine learning algorithms or by performing trait segmentation by extracting information from plant's leaf. As the structure and features of the leaf may be affected by various stages of leaves, different colors during stages, torn leaves. So instead of using a leaf image, the entire plant image is a far more convincing approach. Dataset is prepared by images of some monocots which are collected manually. As the count of collected images is less in number, Image Augmentation is used to generate new images with different perspectives. A neural network is used for feature extraction and to identify the plant. As the image contains a lot of features needed to be extracted, a neural network is needed which can be suitable for this scenario. Residual Networks (ResNet) is a worthy approach as it is a way to handle the vanishing gradient problem in very deep CNNs. The proposed system is developed using ResNet-50 where the pre-trained weights are imported and the model is trained over the prepared dataset, where the model recognizes the respective species of monocot by capturing the whole image and results in achieving an accuracy of 99.3%.  © 2022 IEEE.},
	author_keywords = {Deep CNNs; Feature Extraction; Image Augmentation; Leaf; Monocot; Neural Network; Residual Networks; ResNet-50; Trait segmentation; Vanishing gradient problem},
	keywords = {Agricultural robots; Deep neural networks; Extraction; Horticulture; Image segmentation; Learning algorithms; Plants (botany); Deep CNN; Features extraction; Image augmentation; Leaf; Monocots; Neural-networks; Residual network; Residual network-50; Trait segmentation; Vanishing gradient; Vanishing gradient problem; Feature extraction},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166545698-2},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Comput. Syst. Inf. Technol. Sustain. Solut., CSITSS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 6th IEEE International Conference on Computational System and Information Technology for Sustainable Solutions, CSITSS 2022; Conference date: 21 December 2022 through 23 December 2022; Conference code: 186484}
}

@ARTICLE{Patil202280,
	author = {Patil, Sheetal S. and Patil, Suhas H. and Pawar, Avinash M. and Patil, Netra S. and Rao, Gauri R.},
	title = {Automatic Classification of Medicinal Plants Using State-Of-The-Art Pre-Trained Neural Networks},
	year = {2022},
	journal = {Journal of Advanced Zoology},
	volume = {43},
	number = {1},
	pages = {80 – 88},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141488308&partnerID=40&md5=a1d4cdb478206f0839ad9cf3df85a3fd},
	affiliations = {Computer Engineering Department, Bharati Vidyapeeth (Deemed to be University), College of Engineering, Pune, India; Mechanical Engineering Department, Bharati Vidyapeeth's College of Engineering for Women, Pune, India},
	abstract = {Now a days every mankind is suffering due to infections. Ayurveda, the science of life helped to take preventive measures which boost our immunity. It is plant-based science. Many medicinal plants found useful in daily life of common people for boosting immunity. Identifying the plant species having medicinal plant is challenging, it requires botanical expert. In the process of manual identification, botanical experts use various plant features as the identification keys, which are examined adaptively and progressively to identify plant species. The shortage of experts and trained taxonomist created global taxonomic impediment problem which is one of the major challenges. Various researchers have worked in the field of automatic classification of plants since the last decade. The leaf is considered as primary input as it is available throughout the whole year. The research paper mainly focuses on the study of transfer learning approach for medicinal plant classification, which reuse already developed model at the starting point for model on a second task. Transfer learning approach is a black box approach used for image classification and many more applications by extracting features from an image. Some of the transfer learning models are MobileNet-V1, VGG-19, ResNet-50, VGG-16. Here it uses Mendeley dataset of Indian medicinal plant species which is freely available. Output layer classifies the species of leaves. The result provides evaluation and variations of above listed features extracted models. MobileNetV1 achieves maximum accuracy of 98%. © 2022 Taru Publications. All rights reserved.},
	author_keywords = {Automated Classification; machine learning; medicinal plants; MobileNet V1 and MobileNet V2; Pretrained CNN models; Transfer learning; VGG-16; VGG-19},
	correspondence_address = {S.S. Patil; Computer Engineering Department, Bharati Vidyapeeth (Deemed to be University), College of Engineering, Pune, India; email: sspatil@bvucoep.edu.in},
	publisher = {ASSOC ADVANCEMENT ZOOLOGY},
	issn = {02537214},
	language = {English},
	abbrev_source_title = {J. Adv. Zool.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Mendis2022363,
	author = {Mendis, Omesha and Perera, Amanda and Ranasinghe, Savindu and Chandrasiri, Sanjeevi},
	title = {GreenEye: Smart Consulting System for Domestic Farmers},
	year = {2022},
	journal = {4th International Conference on Advancements in Computing, ICAC 2022 - Proceeding},
	pages = {363 – 368},
	doi = {10.1109/ICAC57685.2022.10025193},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148591127&doi=10.1109%2fICAC57685.2022.10025193&partnerID=40&md5=e2ad3c97a030855b116c7165ecbaa084},
	affiliations = {Department of Information Technology, Sri Lanka Institute of Information Technology, Malabe, 10115, Sri Lanka},
	abstract = {Always it is challenging for typical domestic farmers to maintain a good homestead in today's world and with the ever-growing economic concerns. To save time, money, and energy, they must keep up with the advancements of incorporating technology in their farming practices to ensure that their crops are up to standard and optimized for the maximum yield. Domestic farmers may grow crops for economic gain, pleasure, stress relief, decorative purposes, Etc. However, regardless of the purpose, everyone must be aware of good farming practices. No matter the intention, challenges, and outcomes, everyone engaged with plant growth is the same. In today's highly advanced technological world, a lot of domestic farmers are using modern technology in their growing practices. Experimenting with intelligent growth mechanisms and intend to use modern technologies to provide advice that is useful for all gardeners who prefer home gardening. Additionally, the most crucial aspects of plant care are recognizing the ideal plants for each season, identifying stress factors, identifying diseases, identifying soil moisture levels, and predicting the harvest based on the current environmental conditions. Green Eye mobile application aims to provide a comprehensive solution to technologized domestic farmers using image processing technologies for their most common concerns. © 2022 IEEE.},
	author_keywords = {Domestic Farmers; Maturity Level; Plant Identification; Smart Consulting; Soil Moisture; Stress-factors},
	keywords = {Crops; Farms; Image processing; Stress relief; Consulting systems; Domestic farmer; Economic concerns; Farming practices; Maturity levels; Modern technologies; Plant identification; Save energy; Smart consulting; Stress factors; Soil moisture},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835039809-0},
	language = {English},
	abbrev_source_title = {Int. Conf. Adv. Comput., ICAC - Proceeding},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference on Advancements in Computing, ICAC 2022; Conference date: 9 December 2022 through 10 December 2022; Conference code: 186461}
}

@ARTICLE{Liu2022686,
	author = {Liu, Zihao and Zhang, Sulan and Jia, Xiaojun and Yang, Jun},
	title = {A NOVEL WOOD FEATURE EXTRACTION METHOD BASED ON IMPROVED BLOCKED HIGHER-ORDER LOCAL AUTO-CORRELATION},
	year = {2022},
	journal = {Wood Research},
	volume = {67},
	number = {4},
	pages = {686 – 699},
	doi = {10.37763/wr.1336-4561/67.4.686699},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138456077&doi=10.37763%2fwr.1336-4561%2f67.4.686699&partnerID=40&md5=dd948059ffbfbafb298b9806f1c7e601},
	affiliations = {JIAXING UNIVERSITY, COLLEGE OF INFORMATION SCIENCE AND ENGINEERING, No.118 JIAHANG ROAD, ZHEJIANG PROVINCE, JIAXING CITY, China},
	abstract = {Traditionally, HLAC (Higher-order Local Auto-Correlation) algorithm was used to extract texture features of wood images. However, heavy memory consumption and complexity of high-order mask pattern were common in HLAC. A novel feature extraction strategy based on improved blocked higher-order local auto-correlation (IBHLAC) is proposed to circumvent these problems. Initially, sequences of the whole wood image frames, which are the grayscale treatment, were being divided into series of subdivisions vertically and horizontally. Additionally, to enhance auto-correlation ability of the proposed method, different high-order patterns of masks were rebuilt based on zero-order mask by introducing the morphology and affine transformation. Finally, time-consumption and memory occupation of related four methods were compared. Experiment results indicated IBHLAC costs less time and fewer memory consumption on the wood texture database compared with other methods, which reveal that IBHLAC is efficient. © 2022 Statny Drevarsky Vyskumny Ustav. All rights reserved.},
	author_keywords = {Feature extraction; HLAC; mask pattern; wood recognition},
	keywords = {Consumption; Correlation; Extraction; Methods; Patterns; Sequences; Texture; Wood; Extraction; Masks; Textures; Wood; Feature extraction methods; Features extraction; High-order; High-order local auto-correlation; Higher-order; Local auto-correlation; Mask patterns; Memory consumption; Wood features; Wood recognition; Feature extraction},
	correspondence_address = {Z. Liu; JIAXING UNIVERSITY, COLLEGE OF INFORMATION SCIENCE AND ENGINEERING, JIAXING CITY, No.118 JIAHANG ROAD, ZHEJIANG PROVINCE, China; email: lzh@zjxu.edu.cn},
	publisher = {Pulp and Paper Research Institute},
	issn = {13364561},
	language = {English},
	abbrev_source_title = {Wood Res.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Bronze Open Access}
}

@CONFERENCE{Zhang2022148,
	author = {Zhang, Yang and Wang, Yizhen and Tang, Zhicheng and Zhai, Zhenduo and Shang, Yi and Viegut, Reid},
	title = {Deep Learning Methods for Tree Detection and Classification},
	year = {2022},
	journal = {Proceedings - 2022 IEEE 4th International Conference on Cognitive Machine Intelligence, CogMI 2022},
	pages = {148 – 155},
	doi = {10.1109/CogMI56440.2022.00030},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150683058&doi=10.1109%2fCogMI56440.2022.00030&partnerID=40&md5=f33772faef6c1386c6ba2779065abe08},
	affiliations = {University of Missouri, Department of Electrical Engineering and Computer Science (EECS), Columbia, MO, United States; University of Missouri, School of Natural Resources, Columbia, MO, United States},
	abstract = {This paper presents the results of our deep learning methods for tree detection and classification on aerial images in the Plant Recognition University Challenge sponsored by Ameren in 2021-2022. The task was to locate the trees in an aerial image and predict their family, genus, and species. For tree detection, we applied various supervised learning methods with labeled training data as well as semi-supervised learning methods with the addition of unlabeled data. Our experimental results show that the semi-supervised learning method outperformed the supervised learning methods, improving the f1-score by an average of three percent on the set of images used in the final Plant Challenge competition. For tree classification, We applied various machine learning methods and deep learning models for image classification to predict family, genus and species on the portions of images detected of trees by the detection models. By considering the relationships between family, genus and species, we developed a multi-head ResNet18-based neural network and increased mean accuracy by two percent over the baseline ResNet18. Finally, our team ranked first among all teams in the Plant Challenge competition.  © 2022 IEEE.},
	author_keywords = {aerial image; deep learning; multi-spectral image; RGB image; semi-supervised learning; supervised learning; tree classification; tree detection},
	keywords = {Antennas; Computer vision; Deep learning; Image classification; Learning algorithms; Learning systems; Supervised learning; Trees (mathematics); Aerial images; Deep learning; Learning methods; Multispectral images; RGB images; Semi-supervised learning; Semi-supervised learning methods; Supervised learning methods; Tree classification; Tree detections; Image enhancement},
	correspondence_address = {Y. Zhang; University of Missouri, Department of Electrical Engineering and Computer Science (EECS), Columbia, United States; email: zhangy1@missouri.edu},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166547406-1},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. Cogn. Mach. Intell., CogMI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th IEEE International Conference on Cognitive Machine Intelligence, CogMI 2022; Conference date: 14 December 2022 through 17 December 2022; Conference code: 187265}
}

@ARTICLE{Onishi2022,
	author = {Onishi, Masanori and Watanabe, Shuntaro and Nakashima, Tadashi and Ise, Takeshi},
	title = {Practicality and Robustness of Tree Species Identification Using UAV RGB Image and Deep Learning in Temperate Forest in Japan},
	year = {2022},
	journal = {Remote Sensing},
	volume = {14},
	number = {7},
	doi = {10.3390/rs14071710},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128183792&doi=10.3390%2frs14071710&partnerID=40&md5=cbf24f766f44375a4f477c8e3bcc144f},
	affiliations = {Graduate School of Agriculture, Kyoto University, Kyoto, 606-8502, Japan; Graduate School of Science and Engineering, Kagoshima University, Kagoshima, 890-0065, Japan; Field Science Education and Research Center, Kyoto University, Kyoto, 606-8502, Japan},
	abstract = {Identifying tree species from the air has long been desired for forest management. Recently, combination of UAV RGB image and deep learning has shown high performance for tree identification in limited conditions. In this study, we evaluated the practicality and robustness of the tree identification system using UAVs and deep learning. We sampled training and test data from three sites in temperate forests in Japan. The objective tree species ranged across 56 species, including dead trees and gaps. When we evaluated the model performance on the dataset obtained from the same time and same tree crowns as the training dataset, it yielded a Kappa score of 0.97, and 0.72, respectively, for the performance on the dataset obtained from the same time but with different tree crowns. When we evaluated the dataset obtained from different times and sites from the training dataset, which is the same condition as the practical one, the Kappa scores decreased to 0.47. Though coniferous trees and representative species of stands showed a certain stable performance regarding identification, some misclassifications occurred between: (1) trees that belong to phylogenetically close species, (2) tree species with similar leaf shapes, and (3) tree species that prefer the same envi-ronment. Furthermore, tree types such as coniferous and broadleaved or evergreen and deciduous do not always guarantee common features between the different trees belonging to the tree type. Our findings promote the practicalization of identification systems using UAV RGB images and deep learning. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {deep learning; tree species identification; UAV RGB image},
	keywords = {Deep learning; Forestry; Condition; Deep learning; Performance; RGB images; Temperate forests; Tree crowns; Tree identification; Tree species; Tree species identifications; UAV RGB image; Unmanned aerial vehicles (UAV)},
	correspondence_address = {M. Onishi; Graduate School of Agriculture, Kyoto University, Kyoto, 606-8502, Japan; email: onishi.masanori.25e@kyoto-u.jp},
	publisher = {MDPI},
	issn = {20724292},
	language = {English},
	abbrev_source_title = {Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@CONFERENCE{Gu2022,
	author = {Gu, Fenfei},
	title = {Intelligent Flower Recognition System Based on Deep Learning},
	year = {2022},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {12329},
	doi = {10.1117/12.2646860},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139553119&doi=10.1117%2f12.2646860&partnerID=40&md5=c5d8d04a6880c8218741f4f98a212bba},
	affiliations = {Dept. of Big Data and Artificial Intelligence, Anhui Xinhua University, Anhui, Hefei, China},
	abstract = {With the continuous expansion of the application fields of deep learning, human life is becoming more and more wonderful. This paper mainly uses the application of deep learning in the field of image recognition and uses deep learning algorithm to recognition the problem of flowers. Due to the huge number of flowering species, How to automatically recognition flowers through the system gives people more understanding of flowers while appreciating them, which is a topic with practical application value. This paper constructs a deep learning multi-classification model based on ResNet, and conducts training based on PyTorch framework on the data set of more than 3000 pictures of five kinds of flowers. After 50 rounds of iterative training, it finally achieves 91% average accuracy on the test dataset. Finally, a recognition system based on this model is realized by visualization technology, so as to facilitate users to effectively recognize the kind of flowers through this system. © 2022 SPIE.},
	author_keywords = {deep learning; Flower recognition; pytorch; ResNet},
	keywords = {Classification (of information); Deep learning; Iterative methods; Learning algorithms; Learning systems; Statistical tests; Application fields; Classification models; Deep learning; Flower recognition; Human lives; Model-based OPC; Multi-classification; Pytorch; Recognition systems; Resnet; Image recognition},
	correspondence_address = {F. Gu; Dept. of Big Data and Artificial Intelligence, Anhui Xinhua University, Hefei, Anhui, China; email: gufenfei@axhu.edu.cn},
	editor = {Yang S.},
	publisher = {SPIE},
	issn = {0277786X},
	isbn = {978-151065728-1},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 3rd International Conference on Artificial Intelligence and Electromechanical Automation, AIEA 2022; Conference date: 8 April 2022 through 10 April 2022; Conference code: 182866}
}

@ARTICLE{Ayumi2022138,
	author = {Ayumi, Vina and Noprisson, Handrie and Jumaryadi, Yuwan and Ermatita, Ermatita and Abdiansah, Abdiansah and Purba, Mariana and Utami, Marissa and Putra, Erwin Dwika},
	title = {Transfer Learning for Medicinal Plant Leaves Recognition: A Comparison with and without a Fine-Tuning Strategy},
	year = {2022},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {13},
	number = {9},
	pages = {138 – 144},
	doi = {10.14569/IJACSA.2022.0130916},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139353854&doi=10.14569%2fIJACSA.2022.0130916&partnerID=40&md5=44b5edd904a7b3d7bbc2dd8482f58484},
	affiliations = {Doctor of Engineering Universitas Sriwijaya, Indonesia; Faculty of Computer Science Universitas Mercu Buana, Indonesia; Faculty of Computer Science Universitas Sriwijaya, Indonesia; Program of Informatics Universitas Sjakhyakirti, Indonesia; Faculty of Engineering Universitas Muhammadiyah Bengkulu, Indonesia},
	abstract = {Plant leaves are another common source of information for determining plant species. According to the dataset that has been collected, we propose transfer learning models VGG16, VGG19, and MobileNetV2 to examine the distinguishing features to identify medicinal plant leaves. We also improved algorithm using fine-tuning strategy and analyzed a comparison with and without a fine-tuning strategy to transfer learning models performance. Several protocols or steps were used to conduct this study, including data collection, data preparation, feature extraction, classification, and evaluation. The distribution of training and validation data is 80% for training data and 20% for validation data, with 1500 images of thirty species. The testing data consisted of a total of 43 images of 30 species. Each species class consists of 1-3 images. With a validation accuracy of 96.02 percent, MobileNetV2 with fine-tuning had the best validation accuracy. MobileNetV2 with fine-tuning also had the best testing accuracy of 81.82%. © 2022,International Journal of Advanced Computer Science and Applications. All Rights Reserved.},
	author_keywords = {Deep learning; Medicinal leaf plant; Phytomedicine; Transfer learning},
	keywords = {Data acquisition; Plants (botany); Transfer learning; Deep learning; Fine tuning; Learning models; Medicinal leaf plant; Medicinal plants; Phytomedicines; Plant leaves; Training data; Transfer learning; Tuning strategy; Deep learning},
	publisher = {Science and Information Organization},
	issn = {2158107X},
	language = {English},
	abbrev_source_title = {Intl. J. Adv.  Comput. Sci. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Chulif20222025,
	author = {Chulif, Sophia and Lee, Sue Han and Chang, Yang Loong},
	title = {A Global-Scale Plant Identification using Deep Learning: NEUON Submission to PlantCLEF 2022},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3180},
	pages = {2025 – 2035},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136955272&partnerID=40&md5=f55156ff91152d9fa2ec50e169c56bd0},
	affiliations = {Swinburne University of Technology Sarawak Campus, Sarawak, 93350, Malaysia; Department of Artificial Intelligence, NEUON AI, Sarwak, 94300, Malaysia},
	abstract = {With the increasing knowledge of plants globally, it is becoming difficult for human experts to identify plants manually and systematically. Vascular plants alone are estimated to be more than 300,000 species. However, deep learning methods have recently made progress in automating plant identification. The PlantCLEF 2022 challenge this year aims to tackle the problems faced in global plant identification. With the aggregation of various data from different sources, it is a real problem to deal with big data consisting of many classes, unbalanced classes, inaccuracies, duplications, and a diversity of visual contents and quality. Given a training dataset of 4 million images and 80,000 species, the task of the challenge was to identify the correct plant species from 26,868 multi-image plant observations. This paper describes the submissions made by our team to PlantCLEF 2022. We trained several deep learning models based on the Inception-v4 and Inception-ResNet-v2 architectures. The types of networks constructed were a single convolutional neural network (CNN) and a triplet network. They were either initialised on weights pre-trained from the ImageNet dataset or the weights pre-trained from PlantCLEF 2022 dataset. Although we intended to compare the performance between our single CNN and triplet models, unfortunately, we did not manage to obtain the complete results due to resource and time constraints. Nevertheless, we submitted nine runs and our best submission achieved a Macro Averaged Mean Reciprocal Rank score of 0.6078, placing 4th among the 45 submitted runs. In addition, we have shown that web or noisy data does improve generalisation in the identification. Moreover, the ensemble of models from different network architectures, i.e., Inception-v4 and Inception-ResNet-v2, give higher accuracy than a single model. © 2022 Copyright for this paper by its authors.},
	author_keywords = {computer vision; convolutional neural network; deep learning; machine learning; plant classification},
	keywords = {Convolution; Convolutional neural networks; Deep learning; Learning systems; Network architecture; Convolutional neural network; Deep learning; Global scale; Human expert; Learning methods; Machine-learning; Plant classification; Plant identification; Real problems; Vascular plant; Computer vision},
	correspondence_address = {S. Chulif; Swinburne University of Technology Sarawak Campus, Sarawak, 93350, Malaysia; email: schulif@swinburne.edu.my},
	editor = {Faggioli G. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Ferro N. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Hanbury A. and Vienna University of Technology, Favoritenstrasse 9, Vienna and Potthast M. and University of Leipzig, Augustusplatz 10, Leipzig},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 Conference and Labs of the Evaluation Forum, CLEF 2022; Conference date: 5 September 2022 through 8 September 2022; Conference code: 181762}
}

@ARTICLE{Tasci202230195,
	author = {Tasci, Erdal and Ugur, Aybars},
	title = {A novel pattern recognition framework based on ensemble of handcrafted features on images},
	year = {2022},
	journal = {Multimedia Tools and Applications},
	volume = {81},
	number = {21},
	pages = {30195 – 30218},
	doi = {10.1007/s11042-022-12909-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127613433&doi=10.1007%2fs11042-022-12909-w&partnerID=40&md5=b88025ee4651ab0f9da142f8119420f0},
	affiliations = {Computer Engineering Department, Ege University, Bornova, Turkey},
	abstract = {Nowadays, with the advances and use of technological possibilities and devices, the number of digital images is increasing gradually. Computer-aided classification of image types is widely applied in many applications such as medicine, security, and automation. The feature extraction and selection stages have great importance in terms of improving the classification performance as sub-stages of the pattern recognition process. Researchers apply different feature extraction methods for their works due to the requirements. In this study, a novel pattern recognition framework combining diverse and large-scale handcrafted feature extraction methods (shape-based and texture-based) and the selection stage on images is developed. Genetic algorithms are also used for feature selection. In the experimental studies, Flavia leaf recognition, Caltech101 object classification image datasets, and five supervised classification models (random forest, ECOC-SVM, k-nearest neighbor, AdaBoost, classification tree) with different parameters’ values are used. The experimental results show that the proposed method achieves 98.39% and 82.77% accuracy rates on Flavia and Caltech101 datasets with the ECOC-SVM model, respectively. The proposed framework is also competitive with the existing state-of-the-art methods in the related literature. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Feature extraction; Feature selection; Image processing; Machine learning; Pattern recognition},
	keywords = {Adaptive boosting; Classification (of information); Decision trees; Digital devices; Extraction; Genetic algorithms; Image classification; Nearest neighbor search; Support vector machines; Textures; Classification performance; Computer Aided Classification; Digital image; Feature extraction and selection; Feature extraction methods; Features extraction; Features selection; Images processing; Novel patterns; Selection stages; Feature extraction},
	correspondence_address = {E. Tasci; Computer Engineering Department, Ege University, Bornova, Turkey; email: arif.erdal.tasci@ege.edu.tr},
	publisher = {Springer},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Sookpong2022,
	author = {Sookpong, Satida and Kasetkasem, Teerasit and Phatrapornnant, Teera and Yu, Jaehoon},
	title = {A Unhealthy Plant Identification System Using a Generative Adversarial Network},
	year = {2022},
	journal = {19th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2022},
	doi = {10.1109/ECTI-CON54298.2022.9795542},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133393460&doi=10.1109%2fECTI-CON54298.2022.9795542&partnerID=40&md5=5e2c21a64755e13f1556af3fe096ffdf},
	affiliations = {Kasetsart University, AIoT TAIST Tokyo-Tech, Bangkok, Thailand; Kasetsart University, Department of Electrical Engineering, Bangkok, Thailand; National Electronics and Computer Technology Center, Pathum Thani, Thailand; Tokyo Institute of Technology, Department of Information and Communications Engineering, Tokyo, Japan},
	abstract = {In this work, we investigated the use of Generative Adversarial Networks (GAN) for the unhealthy leaves identification in crop images for the automatic crop monitoring system. Here, we trained the GAN to generate healthy crop images from a healthy leaves image dataset. An unhealthy crop image can be detected by comparing the best-fit healthy crop image generated by GAN, and actually observed image. If the Mahalanobis distance between the observed image and model-generated one is larger than a pre-defined threshold, an image can be considered an unhealthy leaves image. The results from the ROC and Precision-Recall curves show that our model can detect unhealthy crop images with high accuracy. © 2022 IEEE.},
	author_keywords = {Anomaly Detection; Convolutional Neural Networks; Generative Adversarial Networks},
	keywords = {Anomaly detection; Computer vision; Convolutional neural networks; Crops; Plants (botany); Anomaly detection; Best fit; Convolutional neural network; Crop monitoring; Image datasets; Leaf identification; Leaf images; Mahalanobis distances; Monitoring system; Plant identification systems; Generative adversarial networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166548584-5},
	language = {English},
	abbrev_source_title = {Int. Conf. Electr. Eng./Electron., Comput., Telecommun. Inf. Technol., ECTI-CON},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 19th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2022; Conference date: 24 May 2022 through 27 May 2022; Conference code: 180163}
}

@CONFERENCE{Lemikhova2022715,
	author = {Lemikhova, Liliya and Nesteruk, Sergey and Somov, Andrey},
	title = {Transfer Learning for Few-Shot Plants Recognition: Antarctic Station Greenhouse Use-Case},
	year = {2022},
	journal = {IEEE International Symposium on Industrial Electronics},
	volume = {2022-June},
	pages = {715 – 720},
	doi = {10.1109/ISIE51582.2022.9831723},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135772219&doi=10.1109%2fISIE51582.2022.9831723&partnerID=40&md5=eff93f130f08bc531fe25e4183e9a332},
	affiliations = {Skolkovo Institute of Science and Technology, Moscow, Russian Federation},
	abstract = {In this paper, we apply computer vision for plant recognition at the Antarctic station greenhouse, a training facility for future space colonization missions. Our experiments rely on transfer learning and explore the importance of the pre-training data domain. We show that a common approach of using models pre-trained on the Imagenet dataset can be further improved using publicly available domain-specific datasets. The classification results of 17 plant varieties with the ResNet50 model increase the F-score from 75% to 82 % using only 3 training images. We also achieve 78% top-3 accuracy without any training data.  © 2022 IEEE.},
	author_keywords = {computer vision; few-shot learning; plant phenotyping; transfer learning; zero-shot learning},
	keywords = {Greenhouses; Image enhancement; Transfer learning; Zero-shot learning; Antarctic stations; Data domains; Few-shot learning; Plant phenotyping; Plant recognition; Pre-training; Space colonization; Training data; Training facility; Transfer learning; Computer vision},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166548240-0},
	coden = {85PTA},
	language = {English},
	abbrev_source_title = {IEEE Int Symp Ind Electron},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 31st IEEE International Symposium on Industrial Electronics, ISIE 2022; Conference date: 1 June 2022 through 3 June 2022; Conference code: 181352}
}

@CONFERENCE{Othman20221043,
	author = {Othman, Nor Azlan and Damanhuri, Nor Salwa and Ali, Nabilah Md and Chiew Meng, Belinda Chong and Abd Samat, Ahmad Asri},
	title = {Plant Leaf Classification Using Convolutional Neural Network},
	year = {2022},
	journal = {2022 8th International Conference on Control, Decision and Information Technologies, CoDIT 2022},
	pages = {1043 – 1048},
	doi = {10.1109/CoDIT55151.2022.9804121},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134298291&doi=10.1109%2fCoDIT55151.2022.9804121&partnerID=40&md5=f3c204389e85ee123204dab351c45e5e},
	affiliations = {Universiti Teknologi Mara, Centre for Electrical Engineering Studies, Cawangan Pulau Pinang, Permatang Pauh Campus, Pulau Pinang, 13500, Malaysia},
	abstract = {Plant classification systems, in general, could be a beneficial tool in the agricultural industry, especially when it comes to recognising plant types in a systematic and manageable manner. Previously, plant growers used to rely on observation and experienced personnel to distinguish between plant varieties. However, some plants, such as leaves and branches, have nearly identical traits, making identification difficult. Hence, there is a need for a system capable of resolving this issue. Thus, the focus of this research is on classifying plant leaves using a convolutional neural network (CNN) technique. Coriander and parsley were chosen as test subjects for this study because their leaves have comparable structures. The input image was subjected to a number of filter layers using CNN. A total of 100 coriander and parsley leaf photos are collected for this research. These photos were filtered using kernels. These kernels have a set size and extract features from the input photos to create a feature map. These extracted features will then be used to classify plant leaves according to its classes type. With the use of the Graphical User Interface (GUI), the end user will be able to determine the type of leaf. Results show that, using the ReLu activation layer with 15 layers of network design and a 70-30 training-testing proportion, this plant leaf classification system was able to attain a coriander and parsley classification accuracy of 90% with an error rate of 0.1. In addition, due to its great accuracy, this system can be extended for additional uses such as recognising plant diseases and species.  © 2022 IEEE.},
	keywords = {Agriculture; Convolution; Convolutional neural networks; Image processing; Network layers; Plants (botany); Agricultural industries; Classification system; Convolutional neural network; Filter layers; Input image; Neural network techniques; Plant classification; Plant leaf classifications; Plant leaves; Plant types; Graphical user interfaces},
	correspondence_address = {N.A. Othman; Universiti Teknologi Mara, Centre for Electrical Engineering Studies, Cawangan Pulau Pinang, Pulau Pinang, Permatang Pauh Campus, 13500, Malaysia; email: azlan253@uitm.edu.my},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549607-0},
	language = {English},
	abbrev_source_title = {Int. Conf. Control, Decis. Inf. Technol., CoDIT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 8th International Conference on Control, Decision and Information Technologies, CoDIT 2022; Conference date: 17 May 2022 through 20 May 2022; Conference code: 180591}
}

@CONFERENCE{Hamdan2022,
	author = {Hamdan, R.N.A. and Zakaria, R. and Daliman, S.},
	title = {Identification of Zingiberaceae Species Based on Leaf Recognition Using Multiclass Support Vector Machine},
	year = {2022},
	journal = {IOP Conference Series: Earth and Environmental Science},
	volume = {1102},
	number = {1},
	doi = {10.1088/1755-1315/1102/1/012008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144024234&doi=10.1088%2f1755-1315%2f1102%2f1%2f012008&partnerID=40&md5=b8e6aadd05c8ae31c8123826c3edb5ee},
	affiliations = {Faculty of Earth Science, Universiti Malaysia Kelantan, Jeli, 17600, Malaysia; Universitas Muhammadiyah Aceh, Jl. Muhammadiyah No.91, Batoh, Kec. Lueng Bata, Kota Banda Aceh, Aceh, 23123, Indonesia},
	abstract = {The family Zingiberaceae or known as the ginger family of flowering plants famous for its medicinal values and is widely distributed especially in Southeast Asia. Malaysia is one of the countries with abundance of Zingiberaceous species. Usually, identification of the plant species begins by identifying the leaves. Identification based on leaf recognition is the most effective method because a healthy plant has leaves and it exists all the time, unlike fruits and flowers which may only exist at certain times. However, the leaf recognition considered as intricate task and challenging especially when using conventional approaches as most plants have similar shape and colour. Thus, the aim of this research is to develop an interactive application for classification and identification of selected Zingiberaceae species, namely: Zingiber officinale, Curcuma longa, Etlingera elatior and Alpinia galanga based on leaf recognition using multiclass Support Vector Machine (SVM). Six steps were constructed to develop an interactive application for classification of Zingiberaceae species: 1) Collection of leaf samples, 2) Creation of database from captured leaf images, 3) Image preprocessing, 4) Leaf image processing, 5) Classification of image textures using SVM and 6) Design of graphical user interface (GUI). The image features extraction in leaf image processing were based on gray-level co-ccurrence matrix (GLCM), Canny and Prewitt algorithms. The combination of GLCM and Prewitt achieved the highest accuracy which was recorded of 95% from the overall accuracy in classification of Zingiberaceae species. By using the automatic plant identification system, results will come up more accurate and faster. © Published under licence by IOP Publishing Ltd.},
	correspondence_address = {S. Daliman; Faculty of Earth Science, Universiti Malaysia Kelantan, Jeli, 17600, Malaysia; email: shaparas@umk.edu.my},
	publisher = {Institute of Physics},
	issn = {17551307},
	language = {English},
	abbrev_source_title = {IOP Conf. Ser. Earth Environ. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference on Tropical Resources and Sustainable Sciences, CTReSS 2022; Conference date: 3 July 2022 through 5 July 2022; Conference code: 184800; All Open Access, Bronze Open Access}
}

@ARTICLE{Yang2022579,
	author = {Yang, Xin and Ni, Haiming and Li, Jingkui and Lv, Jialuo and Mu, Hongbo and Qi, Dawei},
	title = {Leaf recognition using BP-RBF hybrid neural network},
	year = {2022},
	journal = {Journal of Forestry Research},
	volume = {33},
	number = {2},
	pages = {579 – 589},
	doi = {10.1007/s11676-021-01362-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108789469&doi=10.1007%2fs11676-021-01362-4&partnerID=40&md5=5f74b669e7b9767f89d51ac106089bfc},
	affiliations = {College of Science, Northeast Forestry University, Harbin, 150040, China},
	abstract = {Plant recognition has great potential in forestry research and management. A new method combined back propagation neural network and radial basis function neural network to identify tree species using a few features and samples. The process was carried out in three steps: image pretreatment, feature extraction, and leaf recognition. In the image pretreatment processing, an image segmentation method based on hue, saturation and value color space and connected component labeling was presented, which can obtain the complete leaf image without veins and background. The BP-RBF hybrid neural network was used to test the influence of shape and texture on species recognition. The recognition accuracy of different classifiers was used to compare classification performance. The accuracy of the BP-RBF hybrid neural network using nine dimensional features was 96.2%, highest among all the classifiers. © 2021, The Author(s).},
	author_keywords = {BP-RBF neural network; Feature extraction; Image processing; Leaf recognition; Machine learning},
	keywords = {artificial neural network; back propagation; data processing; forestry; leaf; segmentation},
	correspondence_address = {H. Mu; College of Science, Northeast Forestry University, Harbin, 150040, China; email: mhb506@nefu.edu.cn; D. Qi; College of Science, Northeast Forestry University, Harbin, 150040, China; email: qidw9806@126.com},
	publisher = {Northeast Forestry University},
	issn = {1007662X},
	language = {English},
	abbrev_source_title = {J. For. Res.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Chen2022196,
	author = {Chen, Wenbo},
	title = {Leaf Feature Extraction and Classification Based on Combination Algorithm and Probabilistic Neural Network},
	year = {2022},
	journal = {Proceedings of the 2022 5th IEEE International Conference on Knowledge Innovation and Invention, ICKII 2022},
	pages = {196 – 200},
	doi = {10.1109/ICKII55100.2022.9983589},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146268404&doi=10.1109%2fICKII55100.2022.9983589&partnerID=40&md5=bfbc315c44b95773e73a8fcffedf1636},
	affiliations = {Wuhan University of Technology, Wuhan, China},
	abstract = {In order to solve the problem of low precision in plant leaf identification, a method of plant leaf recognition is proposed based on a combination algorithm and probabilistic neural network. Firstly, the features of the leaf shape are quantitatively extracted by the improved corner point detection algorithm SUSAN, Hough transform, and other methods. Then, the improved probabilistic neural network (PNN) model is established to judge the type of leaves, and the leaves are classified again by using the texture data of leaves in parallel series. The experimental results show that the average recognition accuracy is 92.3%. Compared with other recognition techniques, this method improves the accuracy of leaf recognition.  © 2022 IEEE.},
	author_keywords = {corner point detection algorithm SUSAN; image processing; leaf feature; probabilistic neural network PNN},
	keywords = {Edge detection; Feature extraction; Hough transforms; Image processing; Plants (botany); Signal detection; Textures; Corner point detection algorithm SUSAN; Corner point detections; Images processing; Leaf feature; Neural-networks; Point detection algorithm; Probabilistic neural network probabilistic neural network; Probabilistics; Neural networks},
	correspondence_address = {W. Chen; Wuhan University of Technology, Wuhan, China; email: 1006618095@qq.com},
	editor = {Meen T.-H.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166547929-5},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Knowl. Innov. Invent., ICKII},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 5th IEEE International Conference on Knowledge Innovation and Invention, ICKII 2022; Conference date: 22 July 2022 through 24 July 2022; Conference code: 185421}
}

@CONFERENCE{Kajihara2022,
	author = {Kajihara, Alexandre Yuji and Bertolini, Diego and Schwerz, Andre Luis},
	title = {Identification of herbarium specimens: a case study with Piperaceae Giseke family},
	year = {2022},
	journal = {International Conference on Systems, Signals, and Image Processing},
	volume = {2022-June},
	doi = {10.1109/IWSSIP55020.2022.9854444},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137164609&doi=10.1109%2fIWSSIP55020.2022.9854444&partnerID=40&md5=56a7f55f882cc5b5b2bc022f65fa5ef6},
	affiliations = {Federal University of Technology, Paraná Campo Mourão, Paraná, Brazil},
	abstract = {Although millions of herbarium specimens have been recently digitized, many of them have not yet been properly identified or reviewed. The main reason is that the classification process is manual, slow, and error-prone. Machine Learning techniques are promising alternatives for supporting herbarium plants identification. This paper evaluates feature extraction techniques and classification algorithms to identify herbarium specimens of the Piperaceae Giseke family at the genus level. For the evaluation, we extracted a balanced subset of pre-processed images from the five genera of the Piperaceae family (Manekia, Ottonia, Peperomia, Piper, and Pothomorphe) from the speciesLink repository. Our experiments point to potential support in identifying of herbarium images of the Piperaceae family, mainly for the genera Manekia, Peperomia and Ottonia. The best accuracy was 80.53% achieved by combining MobileNet-V2 and the SVM classifier.  © 2022 IEEE.},
	author_keywords = {herbarium specimens; identification support; Machine Learning; Piperaceae},
	keywords = {Image processing; Learning systems; Plants (botany); Case-studies; Classification process; Error prones; Feature extraction techniques; Herbarium specimens; Identification support; Machine learning techniques; Machine-learning; Piperaceae; Plant identification; Support vector machines},
	editor = {Marinova G.},
	publisher = {IEEE Computer Society},
	issn = {21578672},
	isbn = {978-166549578-3},
	language = {English},
	abbrev_source_title = {Int. Conf. Syst. Signals Image Process.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 29th International Conference on Systems, Signals and Image Processing, IWSSIP 2022; Conference date: 1 June 2022 through 3 June 2022; Conference code: 182030}
}

@CONFERENCE{Hanafiah2022,
	author = {Hanafiah, Mastura and Adnan, Mohd Azraei and Abdul-Rahman, Shuzlina and Mutalib, Sofianita and Malik, Ariff Md Ab and Shamsuddin, Mohd Razif},
	title = {Flower Recognition using Deep Convolutional Neural Networks},
	year = {2022},
	journal = {IOP Conference Series: Earth and Environmental Science},
	volume = {1019},
	number = {1},
	doi = {10.1088/1755-1315/1019/1/012021},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130287011&doi=10.1088%2f1755-1315%2f1019%2f1%2f012021&partnerID=40&md5=cee17a0e596b480b5070b71ff2930d5c},
	affiliations = {Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; Faculty of Business and Management, Puncak Alam Campus, Universiti Teknologi MARA Selangor, Selangor, Puncak Alam, Malaysia; Research Initiative Group of Intelligent Systems, Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia},
	abstract = {This study investigates the suitable model for flower recognition based on deep Convolutional Neural Networks (CNN) with transfer learning approach. The dataset used in the study is a benchmark dataset from Kaggle. The performance of CNN for plant identification using images of flower are investigated using two popular image classification models: AlexNet and VGG16. Results show that CNN is proven to produce outstanding results for object recognition, but its achievement can still be influenced by the type of images and the number of layers of the CNN architecture. The models produced adequate performance rates, with the VGG16 model achieving the best results. AlexNet and VGG16 models achieved the accuracy of 85.69% and 95.02% respectively. This model can be replicated for flower recognition in other areas, especially in our national heritage, Taman Negara which is among the richest flora ecosystem in the world. The significant feature extraction processes were discussed as well, and this is useful for other types of flowers than the trained dataset. © Published under licence by IOP Publishing Ltd.},
	author_keywords = {Convolutional Neural Networks; Flower Recognition; SDG; Taman Negara Pahang},
	correspondence_address = {M. Hanafiah; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; email: mastura.hanafiah@uitm.edu.my},
	editor = {Salleh S.A. and Pardi F.},
	publisher = {Institute of Physics},
	issn = {17551307},
	language = {English},
	abbrev_source_title = {IOP Conf. Ser. Earth Environ. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 1st International Conference on Biodiversity and Sustainable Development 2021, iBioSDG 2021; Conference date: 23 November 2021 through 24 November 2021; Conference code: 179193; All Open Access, Gold Open Access}
}

@CONFERENCE{Kurniawan2022110,
	author = {Kurniawan, Ibnu F. and Aneiba, Adel and Hussain, Ambreen and Idrissi, Moad and Dunggio, Iswan and Asyhari, A. Taufiq},
	title = {Large-scale Tree Detection through UAV-based Remote Sensing in Indonesia: Wallacea Case Study},
	year = {2022},
	journal = {Proceedings - 2022 8th International Conference on Information Management, ICIM 2022},
	pages = {110 – 115},
	doi = {10.1109/ICIM56520.2022.00027},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137071099&doi=10.1109%2fICIM56520.2022.00027&partnerID=40&md5=ff104de7700e39882b305622774aae03},
	affiliations = {School of Computing and Digital Technology, Birmingham City University, Birmingham, B4 7XG, United Kingdom; State University of Gorontalo, Gorontalo, Indonesia; Coventry University, Centre for Computational Science and Mathematical Modelling, Coventry, CV1 2JH, United Kingdom},
	abstract = {The Wallacea region of Sulawesi, Indonesia is renowned for its biodiversity and exceptional endemism. Over the last decade, the region is vulnerable to deforestation, degradation and illegal activities. Frequent monitoring in terms of tree counting provides useful information for various stakeholders such as forest management, government institutions, and environmental agencies. Existing monitoring methods include labour intensive manual observations and satellite imaging remote sensing technology. Satellite-based imagery is low resolution, infrequent, and sometimes include cloud cover. To overcome these drawbacks, this research utilises UAV-based high-resolution RGB images processed by machine learning algorithm to detect tree species, i.e., Sugarpalm, Clove, and Coconut. We compared many deep learning algorithms and found that YOLOv5 model is lightweight, easy to use, fast and accurate for tree species identification.  © 2022 IEEE.},
	author_keywords = {Deep learning application; Forest monitoring; Remote sensing; Wallacea region},
	keywords = {Aircraft detection; Biodiversity; Deep learning; Deforestation; Learning algorithms; Satellite imagery; Unmanned aerial vehicles (UAV); Case-studies; Deep learning application; Degradation activity; Forest monitoring; Illegal activities; Indonesia; Large-scales; Remote-sensing; Tree detections; Wallacea region; Remote sensing},
	correspondence_address = {I.F. Kurniawan; School of Computing and Digital Technology, Birmingham City University, Birmingham, B4 7XG, United Kingdom; email: ibnu.kurniawan@bcu.ac.uk},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166545174-1},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Inf. Manag., ICIM},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 8th International Conference on Information Management, ICIM 2022; Conference date: 25 March 2022 through 27 March 2022; Conference code: 181824}
}

@CONFERENCE{Zhang2022,
	author = {Zhang, Zhe and Du, Zhongkang and Gao, Yici and Fang, Sheng and You, Pengyu and Zhao, YanHeng and Zhang, Kai and Guo, KaiLi},
	title = {Leaf Disease Recognition Based on Multi-Kernel-Size Efficient Channel Attention Convolutional Neural Network},
	year = {2022},
	journal = {CTISC 2022 - 2022 4th International Conference on Advances in Computer Technology, Information Science and Communications},
	doi = {10.1109/CTISC54888.2022.9849729},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136966793&doi=10.1109%2fCTISC54888.2022.9849729&partnerID=40&md5=867c17551621db1c553550c91fbd02d5},
	affiliations = {State Grid of China Technology College, Shandong, JiNan, China; University of Southampton, Optoelectronics Research Centre, Southampton, United Kingdom; Shandong University of Science and Technology, Qingdao, China; Jinan Thomas School, Shandong, JiNan, China},
	abstract = {In recent years, the emergence of channel attention mechanism provided a new idea for extracting effective lesion regions in leaf recognition. However, there are few channel attention methods for leaf disease detection. Facing the problem of leaf disease image recognition under the actual background, these methods have not made adaptive improvement and adjustment, which leads to the difficulty of feature extraction of leaves key areas.In order to solve this problem, a Multi-Kernel-Size Efficient Channel Attention Convolutional Neural Network(MKS-ECANet) is proposed. Firstly, The IRNet with the inverted residual as the basic structure is constructed as the basic network for adding the attention module,which is used to obtain the rich feature representation of leaf lesions; Secondly, a Multi-Kernel-Size Efficient Channel Attention module (MKS-ECA) is proposed, which use a combination of one-dimensional convolutions of different sizes to increase the information interaction between different channels. Finally, we construct MKS-ECANet by adding the MKS-ECA module to IRNet in a suitable way, and conduct experiments on the apple leaf dataset in the actual background. The experimental results show that the accuracy of MKS-ECANet is 95.6%, the parameter is 1.2m, and the flops is 403M. Compared with MoblieNetv2 and ECANet, the accuracy of MKS-ECANet is improved by about 5% and 3% respectively. In terms of parameters and flops, MKS-ECANet also has better performance than MoblieNetv2 and ECANet.  © 2022 IEEE.},
	author_keywords = {CNN; Inverted Residual; leaf diseases; Multi-Kernel-Size Efficient Channel Attention(MKS-ECA)},
	keywords = {Convolution; Image enhancement; Image recognition; Attention mechanisms; Convolutional neural network; Efficient channels; Inverted residual; Kernel size; Leaf disease; Leaf disease detections; Leaf recognition; Multi-kernel; Multi-kernel-size efficient channel attention; Convolutional neural networks},
	correspondence_address = {Z. Du; State Grid of China Technology College, JiNan, Shandong, China; email: a761709800@163.com},
	editor = {Gerogianni V.C. and Yue Y. and Kamareddine F.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166545872-6},
	language = {English},
	abbrev_source_title = {CTISC - Int. Conf. Adv. Comput. Technol., Inf. Sci. Commun.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference on Advances in Computer Technology, Information Science and Communications, CTISC 2022; Conference date: 22 April 2022 through 24 April 2022; Conference code: 181983}
}

@CONFERENCE{Sibiya2022,
	author = {Sibiya, Malusi and Nkosi, Sithembile and Xulu, Sifiso},
	title = {Optimised Detection of Anredera Cordifolia (Madeira Vine) using a Mask-RCNN and Anredera Cordifolia's prominent features as object classes},
	year = {2022},
	journal = {Proceedings - 3rd International Conference on Next Generation Computing Applications, NextComp  2022},
	doi = {10.1109/NextComp55567.2022.9932192},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142361610&doi=10.1109%2fNextComp55567.2022.9932192&partnerID=40&md5=baa844004c9fd7898b4f6583a605e32f},
	affiliations = {Central University of Technology,Free State, Department of Electrical, Electronic and Computer Engineering, Bloemfontein, South Africa; University of KwaZulu Natal Mangosuthu University of Technology, College of Agriculture Engineering and Science, Department of Nature and Conservation, Durban, South Africa; University of the Free State, Department of Geography, Phuthaditjhaba, South Africa},
	abstract = {The research interest in plants using current technologies is growing, and with it is plant recognition using deep Convolutional Neural Networks (CNNs). The CNNs and its variants such as RCNN have recently become a popular method of plant feature recognition due to their superior ability to classify, detect, and label features with high fidelity. Anredera cordifolia, also known as Madeira Vine, is a plant species that unnecessarily invade environments, hence destroying the plants occupying those environments. Here, we develop a computer vision model for the detection of Anredera cordifolia with Mask-RCNN for use in environments that may need drones to detect the presence of this foreign plant species. To optimize the model's confidence in detecting the presence of the Anredera cordifolia, a Mask-RCNN was trained with images of the Anredera cordifolia using three distinct features of this plant as object classes. These features that were used to build classes of the Mask-RCNN were leaves, flowers, and the tubers. This novel approach ensures the detection of the Anredera cordifolia as the prominent features were used as class objects of the Mask-RCNN. The results of the experiments showed that the Mask- RCNN was able to overlay masks and bounding boxes around the Anredera cordifolia features that were detected. © 2022 IEEE.},
	author_keywords = {Anredera cordifolia; Deep Learning; Flowers; Invasive alien plants; Leaves; Machine Learning; Mask-RCNN; Tubers},
	keywords = {Aircraft detection; Convolutional neural networks; Deep neural networks; Feature extraction; Alien plants; Anrederum cordifolium; Deep learning; Flower; Invasive alien plant; Leaf; Machine-learning; Mask-RCNN; Prominent features; Tuber; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166546954-8},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Next Gener. Comput. Appl., NextComp},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Conference on Next Generation Computing Applications, NextComp  2022; Conference date: 6 October 2022 through 8 October 2022; Conference code: 183965}
}

@CONFERENCE{Xu20222238,
	author = {Xu, Mingle and Yoon, Sook and Jeong, Yongchae and Lee, Jaesu and Park, Dong Sun},
	title = {Transfer Learning with Self-Supervised Vision Transformer for Large-Scale Plant Identification},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3180},
	pages = {2238 – 2252},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136912052&partnerID=40&md5=ecb536d7464396f12e4e7777d6fe012d},
	affiliations = {Department of Electronics Engineering, Jeonbuk National University, Jeonbuk, 54896, South Korea; Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonbuk, 54896, South Korea; Department of Computer Engineering, Mokpo National University, Jeonnam, 58554, South Korea; Rural Development Administration, Jeonbuk, 54875, South Korea},
	abstract = {This paper is a working note for the PlantCLEF2022 challenge aiming to identify plants with a large-scale dataset, several millions of images and 80,000 classes. Although there are many images, each class only includes 36 images around on average and thus it can be regarded as a few-shot image classification. To address this issue, transfer learning is validated to be useful in many scenarios and a popular strategy is employing a convolution neural network (CNN) pretrained in a supervised manner. But inspired by the literature on computer vision, we instead leverage a self-supervised vision transformer (ViT) and secure the first place with MA-MRR 0.62692, 0.019 higher than the second place, and 0.116 than the third. Furthermore, we achieve 0.64079 if training the model twenty epochs longer. Compared to the popular strategy with CNN, self-supervised ViT has two advantages. First, ViT does not embrace any inductive bias, such as translating invariance embraced in CNN, and thus owns a more powerful model capacity. Second, self-supervised pretraining obtains a task-agnostic feature extractor that may be better for the downstream task. To be more specific, a recently proposed self-supervised ViT model pretrained in ImageNet, masked autoencoder (MAE), is finetuned in PlantCLEF2022 dataset and then tested to report the evaluation. Except for the challenge, we discuss its possible impacts, such as taking the dataset to pretrain a model for plant-related tasks. Especially, our preliminary results suggest that the pretrained model in PlantCLEF2022 essentially contributes to image-based plant disease recognition on several public datasets. Via our analysis and experimental results, we believe that our work encourages the community to utilize the self-supervised ViT model, the PlantCLEF2022 dataset, and our pretrained model in the dataset. Our codes and trained model are public at https://github.com/xml94/PlantCLEF2022. © 2022 Copyright for this paper by its authors.},
	author_keywords = {computer vision; image classification; plant identification; self-supervised; transfer learning; vision transformer},
	keywords = {Convolutional neural networks; Image classification; Large dataset; Transfer learning; Convolution neural network; Images classification; Inductive bias; Large-scale datasets; Large-scales; Plant identification; Self-supervised; Transfer learning; Transformer modeling; Vision transformer; Computer vision},
	correspondence_address = {M. Xu; Department of Electronics Engineering, Jeonbuk National University, Jeonbuk, 54896, South Korea; email: xml@jbnu.ac.kr},
	editor = {Faggioli G. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Ferro N. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Hanbury A. and Vienna University of Technology, Favoritenstrasse 9, Vienna and Potthast M. and University of Leipzig, Augustusplatz 10, Leipzig},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 Conference and Labs of the Evaluation Forum, CLEF 2022; Conference date: 5 September 2022 through 8 September 2022; Conference code: 181762}
}

@CONFERENCE{Villaruz2022297,
	author = {Villaruz, Jolitte A. and Salido, Julie Ann A. and Barrios Ii, Dennis M. and Felizardo, Rogelio L.},
	title = {Image-Based Recognition of Fruit Bearing Rambutan (Nephelium Lappaceum) Using Deep Learning},
	year = {2022},
	journal = {Proceedings - 2022 2nd International Conference in Information and Computing Research, iCORE 2022},
	pages = {297 – 301},
	doi = {10.1109/iCORE58172.2022.00070},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152198448&doi=10.1109%2fiCORE58172.2022.00070&partnerID=40&md5=6e788513ee4a6656f9e83010d7e5f7d1},
	affiliations = {Aklan State University College of Industrial Technology, Aklan, Kalibo, Philippines; Aklan State University College of Agriculture, Forestry and Environmental Sciences, Aklan, Kalibo, Philippines},
	abstract = {Rambutan is one of the world's most essential fruits because of its nutritional and medicinal benefits. However, 40 to 60 percent of the common cultivars grown from seeds are male and do not bear fruit. The worst part is that it takes four to five years from planting for this to be discovered when the plant is already in the flowering stage. This study leverages transfer learning to fine-Tune three pre-Trained deep learning models, notably AlexNet, VGG16, and GoogLeNet, to enable the recognition of fruit-bearing rambutan at early stages based on their leaf images. Overall, fine-Tuning deep learning models to classify the sexuality of a plant using a novel dataset is feasible, evidenced by more than 95 % accuracy across all models, even to a limited number of training images, with VGG 16 exhibited the highest with 98.40 %. © 2022 IEEE.},
	author_keywords = {deep learning; deep neural networks; fruit-bearing rambutan; plant classification; rambutan classification; transfer learning},
	keywords = {Classification (of information); Deep neural networks; Image classification; Image recognition; Learning systems; Plants (botany); Deep learning; Flowering stage; Fruit-bearing rambutan; Image-based; Learning models; Nephelium lappaceum; Plant classification; Plantings; Rambutan classification; Transfer learning; Fruits},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835033390-9},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Inf. Comput. Res., iCORE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2nd International Conference in Information and Computing Research, iCORE 2022; Conference date: 10 December 2022 through 11 December 2022; Conference code: 187547}
}

@CONFERENCE{Zhang20221217,
	author = {Zhang, Jie},
	title = {Research and Implementation of the Algorithm of Flower Recognition Based on Deep Learning},
	year = {2022},
	journal = {2022 4th International Academic Exchange Conference on Science and Technology Innovation, IAECST 2022},
	pages = {1217 – 1221},
	doi = {10.1109/IAECST57965.2022.10061910},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150834049&doi=10.1109%2fIAECST57965.2022.10061910&partnerID=40&md5=ef608ce075fc157385f102c58c4f2f3f},
	affiliations = {Xiamen Institute of Technology, Fujian, Xiamen, China},
	abstract = {In the field of flower agriculture, it is often necessary to identify and process some flower varieties and monitor their growth status, but the classification and monitoring work can only be carried out smoothly under the guidance of professional technicians, which is very inefficient and greatly restricts the development of flower agriculture. Because of the similarity between different flowers and the difference of the same kind of flowers, it is difficult to solve the problem using traditional image classification methods. This paper focuses on flower recognition, which is a kind of non rigid object. By studying the new algorithm of machine learning, a deep convolution neural network model (AlexNet) algorithm for flower recognition is designed.  © 2022 IEEE.},
	author_keywords = {AlexNet; Classification of Flowers; Deep Learning},
	keywords = {Deep learning; Learning algorithms; Alexnet; Classification methods; Classification of flower; Convolution neural network; Deep learning; Flower recognition; Images classification; Machine-learning; Neural network model; Non-rigid objects; Agriculture},
	correspondence_address = {J. Zhang; Xiamen Institute of Technology, Xiamen, Fujian, China; email: 123871298@qq.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {979-835032000-8},
	language = {English},
	abbrev_source_title = {Int. Acad. Exch. Conf. Sci. Technol. Innov., IAECST},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Academic Exchange Conference on Science and Technology Innovation, IAECST 2022; Conference date: 9 December 2022 through 11 December 2022; Conference code: 187297}
}

@ARTICLE{Reedha2022,
	author = {Reedha, Reenul and Dericquebourg, Eric and Canals, Raphael and Hafiane, Adel},
	title = {Transformer Neural Network for Weed and Crop Classification of High Resolution UAV Images},
	year = {2022},
	journal = {Remote Sensing},
	volume = {14},
	number = {3},
	doi = {10.3390/rs14030592},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123838044&doi=10.3390%2frs14030592&partnerID=40&md5=9b5a47dd956ab28598561c70cc6a83d6},
	affiliations = {INSA CVL, University of Orleans, PRISME Laboratory EA 4229, Bourges, 18022, France; INSA CVL, University of Orleans, PRISME Laboratory EA 4229, Orleans, 45067, France},
	abstract = {Monitoring crops and weeds is a major challenge in agriculture and food production today. Weeds compete directly with crops for moisture, nutrients, and sunlight. They therefore have a significant negative impact on crop yield if not sufficiently controlled. Weed detection and mapping is an essential step in weed control. Many existing research studies recognize the importance of remote sensing systems and machine learning algorithms in weed management. Deep learning approaches have shown good performance in many agriculture-related remote sensing tasks, such as plant classification, disease detection, etc. However, despite the success of these approaches, they still face many challenges such as high computation cost, the need of large labelled datasets, intra-class discrimination (in growing phase weeds and crops share many attributes similarity as color, texture, and shape), etc. This paper aims to show that the attention-based deep network is a promising approach to address the forementioned problems, in the context of weeds and crops recognition with drone system. The specific objective of this study was to investigate visual transformers (ViT) and apply them to plant classification in Unmanned Aerial Vehicles (UAV) images. Data were collected using a high-resolution camera mounted on a UAV, which was deployed in beet, parsley and spinach fields. The acquired data were augmented to build larger dataset, since ViT requires large sample sets for better performance, we also adopted the transfer learning strategy. Experiments were set out to assess the effect of training and validation dataset size, as well as the effect of increasing the test set while reducing the training set. The results show that with a small labeled training dataset, the ViT models outperform state-of-the-art models such as EfficientNet and ResNet. The results of this study are promising and show the potential of ViT to be applied to a wide range of remote sensing image analysis tasks. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Agriculture; Computer vision; Deep learning; Drone; Image classification; Remote sensing; Self-attention; Vision transformers},
	keywords = {Aircraft detection; Antennas; Classification (of information); Computer vision; Crops; Deep learning; Drones; Image classification; Large dataset; Learning algorithms; Statistical tests; Textures; Weed control; Deep learning; Drone; Images classification; Neural-networks; Performance; Plant classification; Remote-sensing; Self-attention; Vehicle images; Vision transformer; Remote sensing},
	correspondence_address = {E. Dericquebourg; INSA CVL, University of Orleans, PRISME Laboratory EA 4229, Bourges, 18022, France; email: eric.dericquebourg@insa-cvl.fr},
	publisher = {MDPI},
	issn = {20724292},
	language = {English},
	abbrev_source_title = {Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 40; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Xiaoyu2022593,
	author = {Xiaoyu, Fang and Linlin, Wang and Chang, Liu and Tao, Hong},
	title = {An Improved Method of Image Recognition with Deep Learning Combined with Attention Mechanism},
	year = {2022},
	journal = {2022 7th International Conference on Image, Vision and Computing, ICIVC 2022},
	pages = {593 – 598},
	doi = {10.1109/ICIVC55077.2022.9887045},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139484254&doi=10.1109%2fICIVC55077.2022.9887045&partnerID=40&md5=06455f442c9b1ddfb4887102811d6dd0},
	affiliations = {Shenyang Aerospace University, College of Artificial Intelligence, Shenyang, China; Shenyang Aerospace University, College of Computer Science, Shenyang, China},
	abstract = {An improved convolutional neural network (CNN) recognition model is proposed for the problems involving low recognition rate and weak generalization ability for flower images. Highly abstracted features after multiple convolutions are integrated, and the performance of network is improved by adding the network model for multi-attention mechanism after residual module for Inception-resnet-V2 Network and fully connected layer before activating the function. The improved model is simulated by integrating OxFlowers 17 and Oxford 102 flower data sets. The results show that the recognition rate of the model based on Inception-resnet-V2 Network combined with attention mechanism is up to 97.6%, being 5.1% higher than that of the original model, and the accuracy for flowers recognition is improved significantly.  © 2022 IEEE.},
	author_keywords = {attention mechanism; convolutional neural network; image recognition; inception-resnet-V2},
	keywords = {Convolution; Convolutional neural networks; Deep learning; Image enhancement; Attention mechanisms; Convolutional neural network; Generalization ability; Inception-resnet-v2; Method of images; Multiple convolution; Network models; Neural network recognition; Performance; Recognition models; Image recognition},
	correspondence_address = {W. Linlin; Shenyang Aerospace University, College of Artificial Intelligence, Shenyang, China; email: wlin_23@163.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166546734-6},
	language = {English},
	abbrev_source_title = {Int. Conf. Image, Vis. Comput., ICIVC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 7th International Conference on Image, Vision and Computing, ICIVC 2022; Conference date: 26 July 2022 through 28 July 2022; Conference code: 182902}
}

@CONFERENCE{Sahu2022353,
	author = {Sahu, Sabita and Amudha, J.},
	title = {Maize Plant Disease classification using optimized DenseNet121},
	year = {2022},
	journal = {Proceedings - 2022 OITS International Conference on Information Technology, OCIT 2022},
	pages = {353 – 358},
	doi = {10.1109/OCIT56763.2022.00073},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150234832&doi=10.1109%2fOCIT56763.2022.00073&partnerID=40&md5=c5dc184b8c6b1b2df5bead271b50e988},
	affiliations = {Amrita School of Engineering, Department of Computer Science and Engineering, Bengaluru, India},
	abstract = {In many countries, agriculture is the predominant root of income.Agriculture provides food, as well as income to farmers. Maize is one of world's leading crops and universally cultivated as cereal grain. Usually, agricultural specialists or farmers use their skills to identify pests and diseases that affect fruit and leaves on the spot. Even the most experienced farmer is prone to making errors in disease identification while growing crops in a greater scale. To treat leaf disease, pesticides are used, however, this is damaging to people's health [1]. Several Machine learning, Deep learning algorithms are suggested to classify diseases in the maize plant. Identification of maize leaf disease is a great challenge due to environmental changes and illumination variation in weather conditions. This research focuses on using different Deep Learning architectures like optimized DenseNet121,CNN, ResNet50, MobileNet, VGG16, and Inception-V3for classification of maize leaves disease so that preventive measures can be taken by the farmers at early stage to protect the crops. Our proposed optimized Densenet121 model outperformed compared to optimized CNN, and ResNet50 with lesser parameters and higher accuracy.  © 2022 IEEE.},
	author_keywords = {Convolutional Neural Network; Deep Learning; Densenet121; Hyper-parameter; Maize Leaf},
	keywords = {Convolutional neural networks; Crop protection; Deep learning; Learning algorithms; Learning systems; Plants (botany); Cereal grains; Convolutional neural network; Deep learning; Densenet121; Disease classification; Hyper-parameter; Leaf disease; Maize leaf; Maize plants; Plant disease; Cultivation},
	correspondence_address = {S. Sahu; Amrita School of Engineering, Department of Computer Science and Engineering, Bengaluru, India; email: sabita.sahu.official@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549348-2},
	language = {English},
	abbrev_source_title = {Proc. - OITS Int. Conf. Inf. Technol., OCIT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 20th OITS International Conference on Information Technology, OCIT 2022; Conference date: 14 December 2022 through 16 December 2022; Conference code: 187105}
}

@ARTICLE{Oppong2022,
	author = {Oppong, Stephen Opoku and Twum, Frimpong and Hayfron-Acquah, James Ben and Missah, Yaw Marfo},
	title = {A Novel Computer Vision Model for Medicinal Plant Identification Using Log-Gabor Filters and Deep Learning Algorithms},
	year = {2022},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2022},
	doi = {10.1155/2022/1189509},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139322322&doi=10.1155%2f2022%2f1189509&partnerID=40&md5=3814da27463a1af6ce35cbf33b22177c},
	affiliations = {Department of ICT Education, University of Education, Winneba, Ghana; Department of Computer Science, Kwame Nkrumah University of Science and Technology, Kumasi, Ghana},
	abstract = {Computer vision is the science that enables computers and machines to see and perceive image content on a semantic level. It combines concepts, techniques, and ideas from various fields such as digital image processing, pattern matching, artificial intelligence, and computer graphics. A computer vision system is designed to model the human visual system on a functional basis as closely as possible. Deep learning and Convolutional Neural Networks (CNNs) in particular which are biologically inspired have significantly contributed to computer vision studies. This research develops a computer vision system that uses CNNs and handcrafted filters from Log-Gabor filters to identify medicinal plants based on their leaf textural features in an ensemble manner. The system was tested on a dataset developed from the Centre of Plant Medicine Research, Ghana (MyDataset) consisting of forty-nine (49) plant species. Using the concept of transfer learning, ten pretrained networks including Alexnet, GoogLeNet, DenseNet201, Inceptionv3, Mobilenetv2, Restnet18, Resnet50, Resnet101, vgg16, and vgg19 were used as feature extractors. The DenseNet201 architecture resulted with the best outcome of 87% accuracy and GoogLeNet with 79% preforming the worse averaged across six supervised learning algorithms. The proposed model (OTAMNet), created by fusing a Log-Gabor layer into the transition layers of the DenseNet201 architecture achieved 98% accuracy when tested on MyDataset. OTAMNet was tested on other benchmark datasets; Flavia, Swedish Leaf, MD2020, and the Folio dataset. The Flavia dataset achieved 99%, Swedish Leaf 100%, MD2020 99%, and the Folio dataset 97%. A false-positive rate of less than 0.1% was achieved in all cases.  © 2022 Stephen Opoku Oppong et al.},
	keywords = {Algorithms; Artificial Intelligence; Deep Learning; Humans; Neural Networks, Computer; Plants, Medicinal; Biomimetics; Computer graphics; Computer vision; Convolutional neural networks; Gabor filters; Learning algorithms; Learning systems; Network architecture; Pattern matching; Plants (botany); Semantics; Computer vision system; Convolutional neural network; Image content; Log-gabor filter; Medicinal plants; Pattern-matching; Plant identification; Semantic levels; Swedishs; Vision model; algorithm; artificial intelligence; human; medicinal plant; Deep learning},
	correspondence_address = {S.O. Oppong; Department of ICT Education, University of Education, Winneba, Ghana; email: sooppong@uew.edu.gh},
	publisher = {Hindawi Limited},
	issn = {16875265},
	pmid = {36203732},
	language = {English},
	abbrev_source_title = {Comput. Intell. Neurosci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Pravinkrishnan20222087,
	author = {Pravinkrishnan, K. and Sivakumar, Naren and Jebaraj, Ainsely and Pooja, C. Padma and Sridhar, Supraja and Balasundaram, Prabavathy and Kalinathan, Lekshmi},
	title = {Classification of Plant Species Using AlexNet Architecture},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3180},
	pages = {2087 – 2093},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136946674&partnerID=40&md5=8688ca8566ffec9211acc4a651871d55},
	affiliations = {Sri Sivasubramaniya Nadar College of Engineering, SH 49A, Tamil Nadu, Kalavakkam, 603110, India},
	abstract = {Plant classification is the process of identifying plant species using image processing techniques. This system is helpful in identifying the different plant species which has useful applications in botanical studies and many plant based industries. The previous works have built systems based on data sets containing limited number of classes . However, the plant classification systems can further be improved in order to cover more number of species. The proposed system is capable of identifying 80000 classes of plant species and is built with a large dataset using AlexNet deep learning architecture. A combination of AdaGrad and KL Divergence optimization and loss functions respectively is used to train the model that produced a MRR score of 0.00029. © 2022 Copyright for this paper by its authors.},
	author_keywords = {AlexNet; CNN; Deep learning; Transfer learning},
	keywords = {Deep learning; Image processing; Transfer learning; Alexnet; Build systems; Classification system; Data set; Deep learning; Image processing technique; Number of class; Plant classification; Plant species; Transfer learning; Large dataset},
	correspondence_address = {K. Pravinkrishnan; Sri Sivasubramaniya Nadar College of Engineering, Kalavakkam, SH 49A, Tamil Nadu, 603110, India; email: pravinkrishnan19082@cse.ssn.edu.in},
	editor = {Faggioli G. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Ferro N. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Hanbury A. and Vienna University of Technology, Favoritenstrasse 9, Vienna and Potthast M. and University of Leipzig, Augustusplatz 10, Leipzig},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 Conference and Labs of the Evaluation Forum, CLEF 2022; Conference date: 5 September 2022 through 8 September 2022; Conference code: 181762}
}

@ARTICLE{Pandey2022393,
	author = {Pandey, Surya and Sindhuja, Bangari and Nagamanjularani, C.S. and Nagarajan, Sasikala},
	title = {Exploring Transfer Learning Techniques for Flower Recognition Using CNN},
	year = {2022},
	journal = {Lecture Notes in Networks and Systems},
	volume = {462},
	pages = {393 – 401},
	doi = {10.1007/978-981-19-2211-4_35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135008625&doi=10.1007%2f978-981-19-2211-4_35&partnerID=40&md5=1d8640db01bafa59f6eca451bc9d0aa9},
	affiliations = {Department of Computer Science and Engineering, New Horizon College of Engineering, Bengaluru, India},
	abstract = {Flowers are a plant's most appealing and defining characteristic. As a result, flower recognition can assist in learning more about the plant. Color and shape are the two most distinguishing characteristics of flowers. These characteristics can be used to train the model so that it can recognize an unknown bloom in the future. It can be used to create image-based searching applications in the disciplines of botanical taxonomy, environmental monitoring systems, and multimedia. The paper's goal is to create a machine learning classifier for floral photos from the Oxford-17 dataset. For this, we tested two approaches: developing a bespoke model from scratch and comparing the accuracies of different pre-trained models. Due to the tiny amount of the dataset, this was a difficult task to solve. The RegNetY 16GF model with pre-trained weights provided the best accuracy. The highest level of accuracy achieved was 93.4%. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Convolution neural network; Flower classification; Machine learning; Oxford-17 dataset; RegNet; Transfer learning},
	correspondence_address = {S. Pandey; Department of Computer Science and Engineering, New Horizon College of Engineering, Bengaluru, India; email: suryaspandey@gmail.com},
	editor = {Shukla S. and Gao X.-Z. and Kureethara J.V. and Mishra D.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {23673370},
	isbn = {978-981192210-7},
	language = {English},
	abbrev_source_title = {Lect. Notes Networks Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: International Conference on Data Science, Computation, and Security, IDSCS 2022; Conference date: 11 February 2022 through 12 February 2022; Conference code: 280239}
}

@CONFERENCE{Nikam2022,
	author = {Nikam, Mihir and Ranade, Ameya and Patel, Rushil and Dalvi, Prachi and Karande, Aarti},
	title = {Explainable Approach for Species Identification using LIME},
	year = {2022},
	journal = {IBSSC 2022 - IEEE Bombay Section Signature Conference},
	doi = {10.1109/IBSSC56953.2022.10037417},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149152579&doi=10.1109%2fIBSSC56953.2022.10037417&partnerID=40&md5=254b695b45ccaf6a56b923a5798e1003},
	affiliations = {Sardar Patel Institute of Technology, Department of Information Technology, Mumbai, India; Sardar Patel Institute of Technology, Department of Computer Engineering, Mumbai, India; Sardar Patel Institute of Technology, Department of Master in Computer Application, Mumbai, India},
	abstract = {Plant identification has a wide array of applications in the fields of agronomy and the discovery of natural and medicinal products. This research aims to explore various deep learning techniques like InceptionV3, Xpection, and ResNet to identify plants. Highly accurate machine learning models generally lack explainability and interpretability. Neural networks are usually opaque systems and thus a direct understanding of the interpretations becomes necessary. We aim to remove this ambiguity of how the model reaches its conclusion by introducing Explainable AI (XAI) techniques. Explainability aims to break such barriers by diminishing the lack of transparency in Artificial Intelligence and Machine Learning models, thus taking a step toward making AI reliable. In this paper, Convolutional Neural Network has been used to identify Vietnamese medicinal plant images based on the characteristics of the leaves, stems and other parts of the plant. Upon identification, our paper also elaborates on how each model predicts which part of the image helps the CNN model to make a prediction by integrating Explainable AI (XAI) using the Lime package. Through this research, we generated images using LIME package which highlight pixels that determine the result of our plant identification process. © 2022 IEEE.},
	author_keywords = {Convolutional Neural Network; Deep Neural Network; Explainable Artificial Intelligence (XAI); InceptionV3; Medicinal Plants Classification; ResNet; Xpection},
	keywords = {Convolution; Convolutional neural networks; Learning systems; Lime; Plants (botany); Convolutional neural network; Explainable artificial intelligence (XAI); Inceptionv3; Machine learning models; Medicinal plant classification; Medicinal plants; Plant classification; Plant identification; Resnet; Xpection; Deep neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549291-1},
	language = {English},
	abbrev_source_title = {IBSSC - IEEE Bombay Sect. Signat. Conf.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th IEEE Bombay Section Signature Conference, IBSSC 2022; Conference date: 8 December 2022 through 10 December 2022; Conference code: 186735}
}

@ARTICLE{Al-Qurran202230143,
	author = {Al-Qurran, Raffi and Al-Ayyoub, Mahmoud and Shatnawi, Ali},
	title = {Plant classification in the wild: Energy evaluation for deep learning models},
	year = {2022},
	journal = {Multimedia Tools and Applications},
	volume = {81},
	number = {21},
	pages = {30143 – 30167},
	doi = {10.1007/s11042-022-12695-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127541086&doi=10.1007%2fs11042-022-12695-5&partnerID=40&md5=ee9d9db6d5336ac228c2d4d450b1a94c},
	affiliations = {Jordan University of Science and Technology, Irbid, Jordan},
	abstract = {Having a system that can take an image of a natural scene and accurately classify the plants in it is of undeniable importance. However, the complexities of dealing with natural scene images and the vast diversity of plants in the wild make designing such a classifier a challenging task. Deep Learning (DL) lends itself as viable solution to tackle such complex problem. However, advanced in DL architectures and software (including DL frameworks) come with a high cost in terms of energy consumption especially when employing Graphics Processing Units (GPU). As data expands rapidly, the need to create energy-aware models increases in order to reduce energy consumption and move towards “Greener AI”. Since the problem of designing energy-aware architectures for plant classification has not been studied significantly in the literature, our work comes to start bridging this gap by focusing not only on the models’ performance, but also on their energy usage on both CPU and GPU platforms. We consider different state-of-the-art Convolutional Neural Networks (CNN) architectures and train them on two famous challenging plants datasets: iNaturalist and Herbarium. Our experiments are meant to highlight the trade-off between accuracy and energy consumption. For examples, the results show that while GPU-bound models can be about 40% faster in terms of training time than simple models running on CPU, the latter’s energy consumption is only two thirds of the former. We hope that such findings will encourage the community to reduce its reliance on accuracy measures to compare different architectures and start taking other factors into account such as power consumption, simplicity, etc. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Convolutional neural networks; Energy consumption; Green AI; Herbarium; iNaturlist},
	keywords = {Computer graphics; Convolution; Convolutional neural networks; Deep learning; Economic and social effects; Graphics processing unit; Network architecture; Plants (botany); Power management; Program processors; Convolutional neural network; Energy aware; Energy evaluation; Energy-consumption; Graphics processing; Green AI; Herbarium; Inaturlist; Plant classification; Processing units; Energy utilization},
	correspondence_address = {M. Al-Ayyoub; Jordan University of Science and Technology, Irbid, Jordan; email: maalshbool@just.edu.jo},
	publisher = {Springer},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Batchuluun2022,
	author = {Batchuluun, Ganbayar and Nam, Se Hyun and Park, Kang Ryoung},
	title = {Deep Learning-Based Plant-Image Classification Using a Small Training Dataset},
	year = {2022},
	journal = {Mathematics},
	volume = {10},
	number = {17},
	doi = {10.3390/math10173091},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137822093&doi=10.3390%2fmath10173091&partnerID=40&md5=3165a3d61afd6eefc6096d81aa746c33},
	affiliations = {Division of Electronics and Electrical Engineering, Dongguk University, 30 Pildong-ro, 1-gil, Jung-gu, Seoul, 04620, South Korea},
	abstract = {Extensive research has been conducted on image augmentation, segmentation, detection, and classification based on plant images. Specifically, previous studies on plant image classification have used various plant datasets (fruits, vegetables, flowers, trees, etc., and their leaves). However, existing plant-based image datasets are generally small. Furthermore, there are limitations in the construction of large-scale datasets. Consequently, previous research on plant classification using small training datasets encountered difficulties in achieving high accuracy. However, research on plant image classification based on small training datasets is insufficient. Accordingly, this study performed classification by reducing the number of training images of plant-image datasets by 70%, 50%, 30%, and 10%, respectively. Then, the number of images was increased back through augmentation methods for training. This ultimately improved the plant-image classification performance. Based on the respective preliminary experimental results, this study proposed a plant-image classification convolutional neural network (PI-CNN) based on plant image augmentation using a plant-image generative adversarial network (PI-GAN). Our proposed method showed the higher classification accuracies compared to the state-of-the-art methods when the experiments were conducted using four open datasets of PlantVillage, PlantDoc, Fruits-360, and Plants. © 2022 by the authors.},
	author_keywords = {deep learning; image augmentation; PI-CNN; PI-GAN; plant image classification},
	correspondence_address = {K.R. Park; Division of Electronics and Electrical Engineering, Dongguk University, Seoul, 30 Pildong-ro, 1-gil, Jung-gu, 04620, South Korea; email: parkgr@dongguk.edu},
	publisher = {MDPI},
	issn = {22277390},
	language = {English},
	abbrev_source_title = {Mathematics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@CONFERENCE{Reddy20221050,
	author = {Reddy, Kovvuri Uday Surya Deveswar and Shaik, Ayesha and Balasundaram, A. and Nithin, Mattapally Sai and Kakarla, Lakshmi Sai Ram and Noor Mahammad, S.K.},
	title = {Classification of Indian Medicinal Leaves using Transfer Learning based Convolutional Neural Networks},
	year = {2022},
	journal = {3rd International Conference on Smart Electronics and Communication, ICOSEC 2022 - Proceedings},
	pages = {1050 – 1058},
	doi = {10.1109/ICOSEC54921.2022.9952074},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143666623&doi=10.1109%2fICOSEC54921.2022.9952074&partnerID=40&md5=f03a2c33258a4ec419c9f58bb63b4113},
	affiliations = {School of Computer Science and Engineering, Vellore Institute of Technology Chennai, Chennai, India; Iiitdm Kancheepuram, Dept. of Computer Science and Engineering, Chennai, India},
	abstract = {There's a rapid increment in the utilization of herbal-based items over the world. Medicinal herbs play a major part in creating ayurvedic drugs and homegrown items. Herbal medications are regarded as a substitute for synthetic drugs in both developing and developed countries, owing to their lack of adverse effects. Many medicinal species are extant in India. Identifying those medicinal plants/leaves over a bunch of different kinds of plant species is necessary as they are many health benefits with medicinal plants. Classification of medicinal leaves can be automated based on their respective textures, shape, and edge or margin of a leaf. In this paper, Indian medicinal leaves classifying utilizing transfer learning based convolutional neural networks has been proposed. In order to make the classification more accurate, the data augmentation technique had been utilized to enlarge the data which avoids insufficiency of data. The regularization technique was utilized to avoid the menace of overfitting. The dataset consists of 1835 images and thirty different types of beneficial Indian medicinal herbs such as Tulsi, Jasmine, etc. Data has been trained using different architectures such as VGG16, Xception, and efficieNetv2 whereas these architectures achieve 99.55%, 99.99%, and 99.46% test accuracy respectively. The preliminary results of the Indian medicinal herbs dataset showed that the Xception architecture performed better than EfficientNetv2 and VGG16.  © 2022 IEEE.},
	author_keywords = {Classification; Convolutional Neural Networks (CNN); Data Augmentation; Deep Learning; Medicinal Herbs; Regularization},
	keywords = {Classification (of information); Convolution; Deep learning; Network architecture; Plants (botany); Statistical tests; Textures; Transfer learning; Convolutional neural network; Data augmentation; Deep learning; Developed countries; Medicinal herb; Medicinal plants; Regularisation; Synthetic drug; Transfer learning; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549764-0},
	language = {English},
	abbrev_source_title = {Int. Conf. Smart Electron. Commun., ICOSEC - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Conference on Smart Electronics and Communication, ICOSEC 2022; Conference date: 20 October 2022 through 22 October 2022; Conference code: 184610}
}

@ARTICLE{Ballo202256,
	author = {Ballo, Abou Bakary and Mamadou, Diarra and Ayikpa, Kacoutchy Jean and Yao, Konan and Ablan, Emma Ake Assi and Kouame, Koffi Fernand},
	title = {Automatic Identification of Ivorian Plants from Herbarium Specimens using Deep Learning},
	year = {2022},
	journal = {International Journal of Emerging Technology and Advanced Engineering},
	volume = {12},
	number = {5},
	pages = {56 – 66},
	doi = {10.46338/ijetae0522_07},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130145141&doi=10.46338%2fijetae0522_07&partnerID=40&md5=4edc4327667a1d70c68a54269ca541b4},
	affiliations = {Unite de Recherche et d'Expertise Numerique, de l'Universite Virtuelle, de Cote d'Ivoire 28 BP 536 28, Abidjan, Cote d'Ivoire; Laboratoire de Mecanique et informatique, Universite Felix Houphouet-Boigny Cocody, BP V 34, Cote d'Ivoire., Abidjan, Cote d'Ivoire; Centre National de Floristique, Universite Felix Houphouet-Boigny Cocody, BP V 34, Cote d'Ivoire, Abidjan, Cote d'Ivoire},
	abstract = {Plant identification is most often based on visual observations by botanists and systematists. Deep learning has become a tool that provides an alternative to automatic plant identification. Our study consists in implementing a method for plant recognition from herbarium specimens using deep learning classification methods. These methods were evaluated on the dataset of ten plant families from the national herbarium of Côte d'Ivoire. The proposed work uses CNN architectures such as DensNet-121, InceptionV3, VGG19, MobileNet, and ResNet101. The dataset contains 7543 images of herbarium specimens. The database is structured in three parts: training, testing, and validation. The accuracies obtained for the first scenario without preprocessing of herbarium specimen images are 76.94% for MobileNet, 77.77% for VGG19, and 77.96% for InceptionV3, 80.41% for ResNet101, and 83.47% for DensNet-121, respectively. The best performance was obtained with DensNet-121 with 83.47%. In the second scenario with preprocessing of herbarium specimens, the accuracies obtained were 82.80% for InceptionV3, 84.40% for VGG19, 85.53% for MobileNet, and 85.80% for ResNet101. The best accuracy was obtained with ResNet121 with 85.80%. From the analysis obtained, the results show that ResNet101 gives the best accuracy compared to the other architectures. In particular, the data preprocessing improves the prediction results, of the Convolutional Neural Network algorithms. © 2022 IJETAE Publication House. All Rights Reserved.},
	author_keywords = {Classification; Convolutional Neural Network; Deep learning; Herbarium specimens; image preprocessing},
	publisher = {IJETAE Publication House},
	issn = {22502459},
	language = {English},
	abbrev_source_title = {Int. J. Emerg. Technol. Adv. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Bronze Open Access}
}

@ARTICLE{De Bortoli2022,
	author = {De Bortoli, Luca and Marsi, Stefano and Marinello, Francesco and Carrato, Sergio and Ramponi, Giovanni and Gallina, Paolo},
	title = {Structure from Linear Motion (SfLM): An On-the-Go Canopy Profiling System Based on Off-the-Shelf RGB Cameras for Effective Sprayers Control},
	year = {2022},
	journal = {Agronomy},
	volume = {12},
	number = {6},
	doi = {10.3390/agronomy12061276},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131508756&doi=10.3390%2fagronomy12061276&partnerID=40&md5=4de7fcd84051689762d47c481199db07},
	affiliations = {Image Processing Laboratory (IPL), Department of Engineering and Architecture, University of Trieste, Via A. Valerio 10, Trieste, 34127, Italy; Department of Land, Environment, Agriculture and Forestry (TESAF), University of Padova, Viale Dell’Università 16, Padova, 35122, Italy; Applied Mechanics for Machinery, Department of Engineering and Architecture, University of Trieste, Trieste, 34127, Italy},
	abstract = {Phytosanitary treatment is one of the most critical operations in vineyard management. Ideally, the spraying system should treat only the canopy, avoiding drift, leakage and wasting of product where leaves are not present: variable rate distribution can be a successful approach, allowing the minimization of losses and improving economic as well as environmental performances. The target of this paper is to realize a smart control system to spray phytosanitary treatment just on the leaves, optimizing the overall costs/benefits ratio. Four different optical-based systems for leaf recognition are analyzed, and their performances are compared using a synthetic vineyard model. In the paper, we consider the usage of three well-established methods (infrared barriers, LIDAR 2-D and stereoscopic cameras), and we compare them with an innovative low-cost real-time solution based on a suitable computer vision algorithm that uses a simple monocular camera as input. The proposed algorithm, analyzing the sequence of input frames and exploiting the parallax property, estimates the depth map and eventually reconstructs the profile of the vineyard’s row to be treated. Finally, the performances obtained by the new method are evaluated and compared with those of the other methods on a well-controlled artificial environment resembling an actual vineyard setup while traveling at standard tractor forward speed. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {automated orchard or vineyard treatments; depth analysis; FDA foliage detector algorithm; innovative sprayer; machine vision; monocular vision; precision agriculture},
	correspondence_address = {P. Gallina; Applied Mechanics for Machinery, Department of Engineering and Architecture, University of Trieste, Trieste, 34127, Italy; email: pgallina@units.it},
	publisher = {MDPI},
	issn = {20734395},
	language = {English},
	abbrev_source_title = {Agronomy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Mostafa2022,
	author = {Mostafa, Sakib and Mondal, Debajyoti and Beck, Michael A. and Bidinosti, Christopher P. and Henry, Christopher J. and Stavness, Ian},
	title = {Leveraging Guided Backpropagation to Select Convolutional Neural Networks for Plant Classification},
	year = {2022},
	journal = {Frontiers in Artificial Intelligence},
	volume = {5},
	doi = {10.3389/frai.2022.871162},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130684341&doi=10.3389%2ffrai.2022.871162&partnerID=40&md5=3fbb2f53c59215a88681c700e464f4a9},
	affiliations = {Department of Computer Science, University of Saskatchewan, Saskatoon, SK, Canada; Department of Physics, University of Winnipeg, Winnipeg, MB, Canada; Department of Applied Science, University of Winnipeg, Winnipeg, MB, Canada},
	abstract = {The development of state-of-the-art convolutional neural networks (CNN) has allowed researchers to perform plant classification tasks previously thought impossible and rely on human judgment. Researchers often develop complex CNN models to achieve better performances, introducing over-parameterization and forcing the model to overfit on a training dataset. The most popular process for evaluating overfitting in a deep learning model is using accuracy and loss curves. Train and loss curves may help understand the performance of a model but do not provide guidance on how the model could be modified to attain better performance. In this article, we analyzed the relation between the features learned by a model and its capacity and showed that a model with higher representational capacity might learn many subtle features that may negatively affect its performance. Next, we showed that the shallow layers of a deep learning model learn more diverse features than the ones learned by the deeper layers. Finally, we propose SSIM cut curve, a new way to select the depth of a CNN model by using the pairwise similarity matrix between the visualization of the features learned at different depths by using Guided Backpropagation. We showed that our proposed method could potentially pave a new way to select a better CNN model. Copyright © 2022 Mostafa, Mondal, Beck, Bidinosti, Henry and Stavness.},
	author_keywords = {convolutional neural network; deep learning—artificial neural network; explainable AI; Guided Backpropagation; neural network visualization},
	correspondence_address = {S. Mostafa; Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; email: sakib.mostafa@usask.ca},
	publisher = {Frontiers Media S.A.},
	issn = {26248212},
	language = {English},
	abbrev_source_title = {Frontier. Artif. Intell.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Pushpa2022,
	author = {Pushpa, B.R. and Rameez Rauf, K. and Prajwal Yadav, Br},
	title = {Leaf Species Classification Using Teeth Features},
	year = {2022},
	journal = {IEEE International Conference on Data Science and Information System, ICDSIS 2022},
	doi = {10.1109/ICDSIS55133.2022.9915999},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141527825&doi=10.1109%2fICDSIS55133.2022.9915999&partnerID=40&md5=8bdc8830211344f18b613e02aa56bd7a},
	affiliations = {Amrita Vishwa Vidyapeetham, Department of Computer Science, Mysuru, India},
	abstract = {Plant recognition is a vital and challenging task. Leaf recognition plays a vital role in plant recognition and its key issue lies in whether selected features are solid and have a good potentiality to prejudice between different kinds of leaves. Leaf teeth are one of the most essential and intricate aspects of the leaf that are employed in automatic plant identification systems to classify and identify plant species. Automatic species identification has a huge amount of advantages over traditional species identification. Presently, most plant automatic identification strategies focus on shape, venation and texture, a feature unremarkably utilized in ancient species identification that is unnoticed. Different species of leaves have distinct characteristics that aid in the classification of certain plant species. These features aid botanists in more precisely recognizing major species of plants from leaf images. One of the most essential and complex aspects of leaf in plant species is the teeth. The leaf images are pre-processed in this study, and the segmentation is done using Canny edge detection. The features are extracted using a feature extraction model and CNN model namely fine-tuned VGG16. These features are then classified using the CNN model and acquired an average accuracy of 95.45 percent.  © 2022 IEEE.},
	author_keywords = {classification; Conventional Neural Network; feature extraction; vgg16},
	keywords = {Automation; Classification (of information); Extraction; Image segmentation; Plants (botany); Textures; CNN models; Conventional neural network; Features extraction; Leaf images; Neural-networks; Plant recognition; Plant species; Species classification; Species identification; Vgg16; Feature extraction},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549801-2},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Data Sci. Inf. Syst., ICDSIS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 IEEE International Conference on Data Science and Information System, ICDSIS 2022; Conference date: 29 July 2022 through 30 July 2022; Conference code: 183506}
}

@ARTICLE{Bacanin20221007,
	author = {Bacanin, Nebojsa and Zivkovic, Miodrag and Sarac, Marko and Petrovic, Aleksandar and Strumberger, Ivana and Antonijevic, Milos and Petrovic, Andrija and Venkatachalam, K.},
	title = {A Novel Multiswarm Firefly Algorithm: An Application for Plant Classification},
	year = {2022},
	journal = {Lecture Notes in Networks and Systems},
	volume = {504 LNNS},
	pages = {1007 – 1016},
	doi = {10.1007/978-3-031-09173-5_115},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135088153&doi=10.1007%2f978-3-031-09173-5_115&partnerID=40&md5=16d04b97b39aa99b7bfa32df493cad5e},
	affiliations = {Singidunum University, Danijelova 32, Belgrade, 11000, Serbia; University of Hradec Kralove, Hradec Kralove, Czech Republic},
	abstract = {Areas of swarm intelligence and machine learning are constantly evolving, recently attracting even more researchers world-wide. This stems from the no free lunch which states that universal approach that could render satisfying results for all practical challenges does not exist. Therefore, in this research a novel multi-swarm firefly algorithm, that tries to address flaws of original firefly metaheuristics, is proposed. Devised algorithm is applied to interesting and important practical challenge of plants classification, as part of the hybrid framework between machine learning and optimization metaheuristics. For this purpose, a set of 1,000 random images of healthy leaves, from one public repository, is retrieved for the following plants: apple, cherry, pepper and tomato. Hybrid framework includes pre-processing, constructing bag of features and classification steps. After pre-processing, a bag of features is constructed by utilizing well-known scale-invariant feature transform algorithm, K-means-based vocabulary generation and histogram. Such images are then categorized with support vector machine classifier. However, to obtain satisfying results for a particular dataset, the support vector machines hyper-parameters’ need to be tuned and in the proposed research multi-swarm firefly algorithm is employed to determine optimal (sub-optimal) hyper-parameters’ values for this practical challenge. Comparative analysis with the basic firefly metaheuristics and other well-known swarm intelligence algorithms was conducted to assess the performance of the proposed method in terms of precision, recall, F-score for this multi-class classification challenge. Obtained results show significant performance improvements of devised method over the original firefly algorithm and also better metrics than other state-of-the-art techniques in the majority of cases. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author_keywords = {Multi-swarm firefly algorithm; Optimization; Plan classification; Support vector machine; Swarm intelligence},
	correspondence_address = {N. Bacanin; Singidunum University, Belgrade, Danijelova 32, 11000, Serbia; email: nbacanin@singidunum.ac.rs},
	editor = {Kahraman C. and Cevik Onar S. and Oztaysi B. and Sari I.U. and Tolga A.C. and Cebi S.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {23673370},
	isbn = {978-303109172-8},
	language = {English},
	abbrev_source_title = {Lect. Notes Networks Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: International Conference on Intelligent and Fuzzy Systems, INFUS 2022; Conference date: 19 July 2022 through 21 July 2022; Conference code: 280209}
}

@CONFERENCE{Aviles-Mejia2022575,
	author = {Aviles-Mejia, Jorge E. and Soto, Daniel and Stephant, Joanny and Labbani-Igbida, Ouiddad},
	title = {Autonomous Vision-Based Navigation and Control for Intra-Row Weeding},
	year = {2022},
	journal = {IEEE International Conference on Automation Science and Engineering},
	volume = {2022-August},
	pages = {575 – 582},
	doi = {10.1109/CASE49997.2022.9926683},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141720823&doi=10.1109%2fCASE49997.2022.9926683&partnerID=40&md5=f72ba93669b86641e45b349e2fce6058},
	affiliations = {University of Limoges, Xlim Research Institute, Limoges, 87032, France},
	abstract = {For agriculture to become more sustainable, new practices and new cropping systems are needed to limit inputs such as fertilisers and phytosanitary products. This paper exposes an autonomous vision-based approach for navigation and intra-row weeding. To preserve crops (here concerned with maize and bean), a plant classification algorithm using easy-to-extract features has been developed and integrated on a real agricultural system. The approach uses frontal and proximal detection; the first allows for autonomous vision-based navigation without relying on GPS; and the latter regulates the weeding tool action. Realistic simulation scenarios are presented in order to validate the proposed approach as well as open-field experiments in real agricultural conditions obtained within the framework and evaluation campaigns of the ROSE challenge. © 2022 IEEE.},
	keywords = {Air navigation; Computer vision; Agricultural system; Classification algorithm; Cropping systems; Navigation and control; Phytosanitary products; Plant classification; Realistic simulation; Vision based control; Vision based navigation; Vision-based approaches; Agriculture},
	publisher = {IEEE Computer Society},
	issn = {21618070},
	isbn = {978-166549042-9},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Autom. Sci. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 18th IEEE International Conference on Automation Science and Engineering, CASE 2022; Conference date: 20 August 2022 through 24 August 2022; Conference code: 183896}
}

@ARTICLE{Li2022,
	author = {Li, Yingbo and Chai, Guoqi and Wang, Yueting and Lei, Lingting and Zhang, Xiaoli},
	title = {ACE R‐CNN: An Attention Complementary and Edge Detection‐Based Instance Segmentation Algorithm for Individual Tree Species Identification Using UAV RGB Images and LiDAR Data},
	year = {2022},
	journal = {Remote Sensing},
	volume = {14},
	number = {13},
	doi = {10.3390/rs14133035},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133215496&doi=10.3390%2frs14133035&partnerID=40&md5=4efc9cd38a582f3bb14cbe922ce66dce},
	affiliations = {Beijing Key Laboratory of Precision Forestry, College of Forestry, Beijing Forestry University, Beijing, 100083, China; Key Laboratory of Forest Cultivation and Protection, Ministry of Education, Beijing Forestry University, Beijing, 100083, China},
	abstract = {Accurate and automatic identification of tree species information at the individual tree scale is of great significance for fine‐scale investigation and management of forest resources and scientific assessment of forest ecosystems. Despite the fact that numerous studies have been con-ducted on the delineation of individual tree crown and species classification using drone high‐res-olution red, green and blue (RGB) images, and Light Detection and Ranging (LiDAR) data, performing the above tasks simultaneously has rarely been explored, especially in complex forest environ-ments. In this study, we improve upon the state of the Mask region‐based convolution neural network (Mask R‐CNN) with our proposed attention complementary network (ACNet) and edge detection R‐CNN (ACE R‐CNN) for individual tree species identification in high‐density and complex forest environments. First, we propose ACNet as the feature extraction backbone network to fuse the weighted features extracted from RGB images and canopy height model (CHM) data through an attention complementary module, which is able to selectively fuse weighted features extracted from RGB and CHM data at different scales, and enables the network to focus on more effective information. Second, edge loss is added to the loss function to improve the edge accuracy of the segmentation, which is calculated through the edge detection filter introduced in the Mask branch of Mask R‐CNN. We demonstrate the performance of ACE R‐CNN for individual tree species identification in three experimental areas of different tree species in southern China with precision (P), recall (R), F1‐score, and average precision (AP) above 0.9. Our proposed ACNet–the backbone network for feature extraction–has better performance in individual tree species identification compared with the ResNet50‐FPN (feature pyramid network). The addition of the edge loss obtained by the Sobel filter further improves the identification accuracy of individual tree species and accel-erates the convergence speed of the model training. This work demonstrates the improved performance of ACE R‐CNN for individual tree species identification and provides a new solution for tree‐level species identification in complex forest environments, which can support carbon stock estimation and biodiversity assessment. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {ACE R‐CNN; attention complementary module; edge detection; individual tree species identification; Mask R‐CNN; UAV RGB and CHM data},
	keywords = {Aircraft detection; Automation; Biodiversity; Complex networks; Data mining; Ecosystems; Feature extraction; Forestry; Image segmentation; Information management; Optical radar; Unmanned aerial vehicles (UAV); (ACNet) and edge detection R‐CNN; Attention complementary module; Canopy Height Models; Convolution neural network; Individual tree; Individual tree species identification; Mask region‐based convolution neural network; Modeling data; Region-based; Tree species identifications; UAV red, green and blue and canopy height model data; Extraction},
	correspondence_address = {X. Zhang; Beijing Key Laboratory of Precision Forestry, College of Forestry, Beijing Forestry University, Beijing, 100083, China; email: zhangxl@bjfu.edu.cn},
	publisher = {MDPI},
	issn = {20724292},
	language = {English},
	abbrev_source_title = {Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Gold Open Access}
}

@CONFERENCE{Taib2022,
	author = {Taib, Mohd Sukry Mohd and Nazar, Nur Khalisa and Sulaiman, Hanifah and Halim, Suhaila Abd},
	title = {Piper Betle and Piper Sarmentosum Identification using Support Vector Machine},
	year = {2022},
	journal = {AIP Conference Proceedings},
	volume = {2465},
	doi = {10.1063/5.0078984},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160354257&doi=10.1063%2f5.0078984&partnerID=40&md5=3223b7dbaf106df10c51f226c1f57de9},
	affiliations = {Faculty of Computer and Mathematical Sciences, Universiti Teknologi Mara, Selangor, Shah Alam, Malaysia},
	abstract = {Plant identification is an important task for biologists, chemists, scientists and environmentalists to identify every plant species in the world for environmental protection. However, the process of plants identification is usually very time consuming and low efficiency to identify between plants that are almost identical to each other especially in terms of shape. The objectives of this study are to extract the features properties and to identify the Piper betle and Piper sarmentosum using Support Vector Machine (SVM). These two types of plants are identical to each other because they come from the same family that is Piperaceae. The image of Piper betle and Piper sarmentosum leaves are used as the data to extract their features using image processing techniques. The features extraction properties from the leaf such as area, perimeter, convex hull, major axis, minor axis and the ratio of axes are calculated and extracted using MATLAB. The classification method of SVMused the extracted features to find the best possible hyperplane that can identify and classify between these two types of plants. Besides, the optimization value of the box constraint and kernel scale also been measured automatically by using the function code in image processing toolbox. The result demonstrated that the successful rate of plant identification of Piper betle and Piper sarmentosum is up to 80% using SVM method. As conclusion, the used of SVM is beneficial in classifying between Piper betle and Piper sarmentosum although they are almost identical in shape. © 2022 American Institute of Physics Inc.. All rights reserved.},
	author_keywords = {Image Processing; MATLAB; Piper betle; Piper sarmentosum; Support Vector Machine},
	correspondence_address = {H. Sulaiman; Faculty of Computer and Mathematical Sciences, Universiti Teknologi Mara, Shah Alam, Selangor, Malaysia; email: hanifahsulaiman@uitm.edu.my},
	publisher = {American Institute of Physics Inc.},
	issn = {0094243X},
	language = {English},
	abbrev_source_title = {AIP Conf. Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 1st Joint International Conference on Mathematics, Statistics and Engineering, JCoMSE 2021; Conference date: 12 July 2021 through 13 July 2021; Conference code: 188730}
}

@CONFERENCE{Hamzah202255,
	author = {Hamzah, Robiah and Md.noor, Mohammad Faizuddin},
	title = {Visualization of Tree Species Identification Using Mask RCNNs for Tropical Forests in Malaysia},
	year = {2022},
	journal = {Proceedings - 2022 International Conference on Computer and Drone Applications, IConDA 2022},
	pages = {55 – 60},
	doi = {10.1109/ICONDA56696.2022.10000378},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146659072&doi=10.1109%2fICONDA56696.2022.10000378&partnerID=40&md5=fb56165e05ab224d836bb011f17b56da},
	affiliations = {Malaysian Institute of Information Technology, Universiti Kuala Lumpur, Kuala Lumpur, Malaysia},
	abstract = {A variety of species support environmental efforts, and drones provide easier and more precise imaging than satellites in comparison to their satellite data due to their vastly improved image quality compared to satellite data. This project seeks to establish a novel method for recognizing tree species based on drone imagery that can be evaluated. As part of this project, we will design a novel model for identifying tree species using drone imagery, which we will then test and evaluate it. This project will gather aerial images of forests using a drone. After aerial images have been obtained, a bespoke dataset based on the captured aerial images will be developed. To estimate the tree species based on aerial images from this dataset, we will use machine learning to make predictions using this information. This research initiative intends to collect photographs of Malaysian forests and their fauna, as Malaysian species are distinct from those of other nations due to the country's climate. The purpose of this study is to identify and segment tree species from high-resolution RGB images of tropical forests using the Mask R-CNN method, a robust system for detecting tree species. The developed algorithm was validated on two datasets in which the experimental results indicate a mean average precision (mAP) of 19.17%, 67.9% for precision, and 16.9% for recall for the improved model. Specifically, mAP improved by 6.03%, 46.80% precision, and 2.27 recall over the original Mask R-CNN. Using different datasets, the enhanced model demonstrated high detection and segmentation accuracy as well as making it suitable for providing technical support in intelligent forest management. A successful outcome of this endeavor could result in the first phase of a forest inventory, which would have repercussions for logging and forest management in the region.  © 2022 IEEE.},
	author_keywords = {Drone aerial images; instance segmentation; Mask R-CNN; tree species identification; tropical forest},
	keywords = {Antennas; Forestry; Image enhancement; Image segmentation; Satellites; Tropics; Aerial images; Drone aerial image; Instance segmentation; Malaysia; Malaysians; Mask R-CNN; Satellite data; Tree species; Tree species identifications; Tropical forest; Drones},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549235-5},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Comput. Drone Appl., IConDA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 International Conference on Computer and Drone Applications, IConDA 2022; Conference date: 28 November 2022 through 29 November 2022; Conference code: 185864}
}

@ARTICLE{Heidary-Sharifabad2022,
	author = {Heidary-Sharifabad, Ahmad and Zarchi, Mohsen Sardari and Zarei, Gholamreza},
	title = {Padeep: A Patched Deep Learning Based Model for Plants Recognition on Small Size Dataset: Chenopodiaceae Case Study},
	year = {2022},
	journal = {International Journal of Computational Intelligence and Applications},
	volume = {21},
	number = {1},
	doi = {10.1142/S1469026822500055},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128752949&doi=10.1142%2fS1469026822500055&partnerID=40&md5=7067d7643edb36f166c64e501986d78e},
	affiliations = {Department of Computer Engineering, Maybod Branch, Islamic Azad University, Meybod, Iran; Department of Computer Engineering, Meybod University, Meybod, Iran; Department of Agronomy, Maybod Branch, Islamic Azad University, Meybod, Iran},
	abstract = {A large training sample is prerequisite for the successful training of each deep learning model for image classification. Collecting a large dataset is time-consuming and costly, especially for plants. When a large dataset is not available, the challenge is how to use a small or medium size dataset to train a deep model optimally. To overcome this challenge, a novel model is proposed to use the available small size plant dataset efficiently. This model focuses on data augmentation and aims to improve the learning accuracy by oversampling the dataset through representative image patches. To extract the relevant patches, ORB key points are detected in the training images and then image patches are extracted using an innovative algorithm. The extracted ORB image patches are used for dataset augmentation to avoid overfitting during the training phase. The proposed model is implemented using convolutional neural layers, where its structure is based on ResNet architecture. The proposed model is evaluated on a challenging ACHENY dataset. ACHENY is a Chenopodiaceae plant dataset, comprising 27030 images from 30 classes. The experimental results show that the patch-based strategy outperforms the classification accuracy achieved by traditional deep models by 9%. © World Scientific Publishing Europe Ltd.},
	author_keywords = {ACHENY; Chenopodiaceae; deep learning; ORB; Patch-based; plant classification},
	keywords = {Classification (of information); Deep learning; ACHENY; Chenopodiaceae; Deep learning; Image patches; Large datasets; Learning Based Models; ORB; Patch based; Plant classification; Plant recognition; Image enhancement},
	correspondence_address = {M.S. Zarchi; Department of Computer Engineering, Meybod University, Meybod, Iran; email: sardari@meybod.ac.ir},
	publisher = {World Scientific},
	issn = {14690268},
	language = {English},
	abbrev_source_title = {Int. J. Comput. Intell. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Almazaydeh20223664,
	author = {Almazaydeh, Laiali and Alsalameen, Reyad and Elleithy, Khaled},
	title = {HERBAL LEAF RECOGNITION USING MASK-REGION CONVOLUTIONAL NEURAL NETWORK (MASK R-CNN)},
	year = {2022},
	journal = {Journal of Theoretical and Applied Information Technology},
	volume = {100},
	number = {11},
	pages = {3664 – 3671},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133521367&partnerID=40&md5=5700a3eb9ef62d9faf4c2f3b09f19c1e},
	affiliations = {Al-Hussein Bin Talal University, Department of Software Engineering, Jordan; University of Bridgeport, Department of Computer Science & Engineering, United States},
	abstract = {Recent rapid technological advancements in pattern recognition and computer vision have led to great results on a wide range of applications. One of these applications is herbal plant species identification, as the proper automated system for the recognition of herbal plants is required for botanists to study therapeutic and nutritional uses of herbs. In literature, many studies have adopted classical machine learning approaches while some studies have adopted deep learning approaches, via the leaf images. For this work, we use the latest state-of-the-art framework, namely Mask R-CNN, to build such a classification system to identify a medicinal plant. In this paper, we demonstrate the development of the classification system using Mask R-CNN and its backbone: region proposal network, RoI Pooling, RoI Align, and the network head: classification & detection, segmentation. The trained model achieved average accuracy of 95.7% for the identification of 30 medicinal plant species loaded from the Mendely Dataset. The model output is obtained as bounding box for object detection, mask, and class indicating a plant species. © 2022 Little Lion Scientific},
	author_keywords = {CNN; Deep learning; Mask R-CNN; PPIR; RPN},
	publisher = {Little Lion Scientific},
	issn = {19928645},
	language = {English},
	abbrev_source_title = {J. Theor. Appl. Inf. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Simon2022575,
	author = {Simon, Santhosh and Jacob, Pramod Mathew},
	title = {Weed Plant Identification and Removal Using Machine Vision and Robotics},
	year = {2022},
	journal = {2022 International Conference on Data Analytics for Business and Industry, ICDABI 2022},
	pages = {575 – 579},
	doi = {10.1109/ICDABI56818.2022.10041569},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149309499&doi=10.1109%2fICDABI56818.2022.10041569&partnerID=40&md5=61b5fe223e911eed20ef6346f54740ef},
	affiliations = {Providence College of Engineering, Computer Science Department, Kerala, Chengannur, India},
	abstract = {Weeds are undesired plants that grow around crops or plants of desire. Farmers and gardeners have always been fighting against weeds because weed removal is a tiring and laborious process. That is where precision farming becomes the need of the hour. To implement precision farming for weed removal, we need a system that detects weeds from crops and also a robotic system that will execute the weed removal process using a robotic arm, mimicking the human action of removing weeds. The proposed system uses the YOLOv4 algorithm to detect weeds and crops in real time and removes the detected weeds using a weeding robot. The system proves to be a good solution of removing weeds from land. © 2022 IEEE.},
	author_keywords = {Artificial Intelligence; Automation; Deep Learning; Robotics in Agriculture and Forestry},
	keywords = {Agricultural robots; Computer vision; Deep learning; Farms; Intelligent robots; Deep learning; Human actions; Identification and removal; Machine-vision; Plant identification; Precision-farming; Removal process; Robotic in agriculture and forestry; Robotic systems; Weed plants; Crops},
	correspondence_address = {S. Simon; Providence College of Engineering, Computer Science Department, Chengannur, Kerala, India; email: santhosh.s@providence.edu.in},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549058-0},
	language = {English},
	abbrev_source_title = {Int. Conf. Data Anal. Bus. Ind., ICDABI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2022 International Conference on Data Analytics for Business and Industry, ICDABI 2022; Conference date: 25 October 2022 through 26 October 2022; Conference code: 186761}
}@ARTICLE{Güldenring2021,
	author = {Güldenring, Ronja and Nalpantidis, Lazaros},
	title = {Self-supervised contrastive learning on agricultural images},
	year = {2021},
	journal = {Computers and Electronics in Agriculture},
	volume = {191},
	doi = {10.1016/j.compag.2021.106510},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118586411&doi=10.1016%2fj.compag.2021.106510&partnerID=40&md5=e11a5ebc9974007f32088735bd5d45b4},
	affiliations = {Department of Electrical Engineering, Elektrovej, Lyngby, 2800, Denmark},
	abstract = {Agriculture emerges as a prominent application domain for advanced computer vision algorithms. As much as deep learning approaches can help solve problems such as plant detection, they rely on the availability of large amounts of annotated images for training. However, relevant agricultural datasets are scarce and at the same time, generic well-established image datasets such as ImageNet do not necessarily capture the characteristics of agricultural environments. This observation has motivated us to explore the applicability of self-supervised contrastive learning on agricultural images. Our approach considers numerous non-annotated agricultural images, which are easy to obtain, and uses them to pre-train deep neural networks. We then require only a limited number of annotated images to fine-tune those networks in a supervised training manner for relevant downstream tasks, such as plant classification or segmentation. To the best of our knowledge, contrastive self-supervised learning has not been explored before in the area of agricultural images. Our results reveal that it outperforms conventional deep learning approaches in classification downstream tasks, especially for small amounts of available annotated training images where up to 14% increase of average top-1 classification accuracy has been observed. Furthermore, the computational cost for generating data-specific pre-trained weights is fairly low, allowing one to generate easily new pre-trained weights for any custom model architecture or task. © 2021 The Authors},
	author_keywords = {Contrastive learning; Deep learning; Self-supervision; SwAV; Transfer-learning},
	keywords = {Agricultural robots; Deep neural networks; Image segmentation; Applications domains; Computer vision algorithms; Contrastive learning; Deep learning; Down-stream; Learning approach; Plant detections; Self-supervision; SwAV; Transfer learning; agricultural application; algorithm; artificial neural network; detection method; training; Agriculture},
	correspondence_address = {R. Güldenring; Department of Electrical Engineering, Lyngby, Elektrovej, 2800, Denmark; email: ronjag@elektro.dtu.dk},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Satake2022108,
	author = {Satake, Sara S. and Calvo, Rodrigo and Britto, Alceu S. and Costa, Yandre M. G.},
	title = {Classification of Toxic Ornamental Plants for Domestic Animals Using CNN},
	year = {2022},
	journal = {Communications in Computer and Information Science},
	volume = {1527 CCIS},
	pages = {108 – 120},
	doi = {10.1007/978-3-030-96878-6_10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126184025&doi=10.1007%2f978-3-030-96878-6_10&partnerID=40&md5=574dc85cfc7150af50a50fe3322fdbc3},
	affiliations = {State University of Maringa (UEM), Maringá, Brazil; Pontifical Catholic University of Parana (PUCPR), Curitiba, Brazil},
	abstract = {Veterinary medicine emphasizes accidents caused by toxic plants with domestic animals as an extremely important topic, as the right diagnosis can be crucial for the affected animal. In this work, we propose the classification of toxic ornamental plants, according to nine different categories, using five widely-known CNN architectures, namely: DenseNet, ResNet, VGG16, VGG19 and Xception. The rationale behind it is that the automatic identification of these types of plant can be a useful tool to help in the prevention of those accidents. The authors have carefully curated a database to support the development of this work, collecting images available on the Pinterest website, and also performing some important data pre-processing. This database was also made available as a contribution of this work. Transfer learning was employed by taking advantage of feature learned from the ImageNet dataset. We also analyzed the heat maps generated by the Layer-wise Relevant Propagation method, which allowed to observe the individual behavior of the best and worst architectures. The best performance was achieved using DenseNet, with an accuracy of 97.67%. That model managed to generalize very well, even to deal with noisy images, which are frequent in photos of decorative environments. © 2022, Springer Nature Switzerland AG.},
	author_keywords = {Computer vision; Convolutional Neural Networks; Layer-wise Relevant Propagation; Machine learning; Pattern recognition; Plant classification; Toxic ornamental plants for animals},
	keywords = {Automation; Backpropagation; Computer vision; Convolutional neural networks; Data handling; Multilayer neural networks; Network architecture; Network layers; Automatic identification; Convolutional neural network; Data preprocessing; Domestic animals; Layer-wise; Layer-wise relevant propagation; Ornamental plants; Plant classification; Toxic ornamental plant for animal; Transfer learning; Animals},
	correspondence_address = {Y.M.G. Costa; State University of Maringa (UEM), Maringá, Brazil; email: yandre@din.uem.br},
	editor = {Rozinaj G. and Vargic R.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18650929},
	isbn = {978-303096877-9},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 28th International Conference for Systems, Signal and Image Processing, IWSSIP 2021; Conference date: 2 June 2021 through 4 June 2021; Conference code: 274269}
}

@ARTICLE{Homan2021,
	author = {Homan, Dewald and du Preez, Johan A.},
	title = {Automated feature-specific tree species identification from natural images using deep semi-supervised learning},
	year = {2021},
	journal = {Ecological Informatics},
	volume = {66},
	doi = {10.1016/j.ecoinf.2021.101475},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119019677&doi=10.1016%2fj.ecoinf.2021.101475&partnerID=40&md5=0263a1379cc0b33ef72c8c1f46ea5554},
	affiliations = {Faculty of Engineering, Stellenbosch University, Stellenbosch, 7602, South Africa},
	abstract = {Prior work on plant species classification predominantly focuses on building models from isolated plant attributes. Hence, there is a need for tools that can assist in species identification in the natural world. We present a novel and robust two-fold approach capable of identifying trees in a real-world natural setting. Additionally, we leverage unlabelled data through deep semi-supervised learning and demonstrate superior performance to supervised learning. Our single-GPU implementation for feature recognition uses minimal annotated data and achieves accuracies of 93.96% and 93.11% for leaves and bark, respectively. Further, we extract feature-specific datasets of 50 species by employing this technique. Finally, our semi-supervised species classification method attains 94.04% top-5 accuracy for leaves and 83.04% top-5 accuracy for bark. © 2021 Elsevier B.V.},
	author_keywords = {Computer vision; Deep learning; Natural images; Neural network; Semi-supervised learning; Species classification; Transfer learning},
	keywords = {accuracy assessment; algorithm; data set; identification method; image analysis; learning},
	correspondence_address = {D. Homan; Faculty of Engineering, Stellenbosch University, Stellenbosch, 7602, South Africa; email: dewald.homan@gmail.com},
	publisher = {Elsevier B.V.},
	issn = {15749541},
	language = {English},
	abbrev_source_title = {Ecol. Informatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Green Open Access}
}

@CONFERENCE{Moreno2022287,
	author = {Moreno, Bruno M. and Cruvinel, Paulo E.},
	title = {Computer Vision System for Identifying on Farming Weed Species},
	year = {2022},
	journal = {Proceedings - 16th IEEE International Conference on Semantic Computing, ICSC 2022},
	pages = {287 – 292},
	doi = {10.1109/ICSC52841.2022.00054},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127566314&doi=10.1109%2fICSC52841.2022.00054&partnerID=40&md5=42286df6f627976a5e2f236f8d4ff3a3},
	affiliations = {Embrapa Instrumentation (CNPDIA), Sao Carlos, 13560-970, Brazil; Computer Science Program, Federal University of Sao Carlos, Sao Carlos, Brazil},
	abstract = {In the world, agriculture has been developed by combining new technologies for aid production and profitability while keeping environmental and social responsibility. The sector is primarily responsible to supply food for people, as well as fibers and energy. To keep such results, farmers have faced the need to seek, increasingly rational use of inputs, as is the use of pesticides, plant regulators, and liquid fertilizers. This paper presents a discussion related to the design and development of a computer vision system for precision spraying in the control of weed species into agricultural crops, based on the identification of invasive plants and their quantities. Concepts such as plant segmentation using field-acquired images, leaf features and descriptors, and classification of species are analyzed and implemented in a prototype.  © 2022 IEEE.},
	author_keywords = {agricultural industry; decision making; embedded platform; plant recognition; weed control},
	keywords = {Behavioral research; Computer control systems; Computer vision; Crops; Energy utilization; Environmental technology; Image segmentation; Weed control; Agricultural industries; Computer vision system; Decisions makings; Embedded platforms; Environmental responsibility; Plant recognition; Production and profitabilities; Social responsibilities; Weed control; Weed species; Decision making},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166543418-8},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. Semant. Comput., ICSC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 16th IEEE International Conference on Semantic Computing, ICSC 2022; Conference date: 26 January 2022 through 28 January 2022; Conference code: 177927}
}

@ARTICLE{Racsmány2021,
	author = {Racsmány, Mihály and Bencze, Dorottya and Pajkossy, Péter and Szőllősi, Ágnes and Marián, Miklós},
	title = {Irrelevant background context decreases mnemonic discrimination and increases false memory},
	year = {2021},
	journal = {Scientific Reports},
	volume = {11},
	number = {1},
	doi = {10.1038/s41598-021-85627-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102758126&doi=10.1038%2fs41598-021-85627-2&partnerID=40&md5=652840caf24ba116e0700a105db5d461},
	affiliations = {Department of Cognitive Science, Budapest University of Technology and Economics, Egry Jozsef utca 1, Budapest, 1111, Hungary; Institute of Cognitive Neuroscience and Psychology, Research Centre for Natural Sciences, Budapest, Hungary},
	abstract = {One of the greatest commonplaces in memory research is that context improves recall and enhances or leaves recognition intact. Here we present results which draw attention to the fact that the reappearance of irrelevant and unattended background contexts of encoding significantly impairs memory discrimination functions. This manuscript presents the results of two experiments in which participants made indoor/outdoor judgements for a large number of object images presented together with individual, irrelevant and presumably unattended background scenes. On a subsequent unexpected recognition test participants saw the incidentally encoded target objects, visually similar lures or new foil objects on the same or new background scenes. Our results showed that although the reappearance of the background scene raised the hit rate for target objects, it decreased mnemonic discrimination, a behavioral score for pattern separation, a hippocampal function that is affected in early dementia. Furthermore, the presence of the encoded background scene at the recognition test increased the false recognition of lure objects, even when participants were explicitly instructed to neglect the context scene. Altogether these results gave evidence that if context increases recognition hits for target memories, it does so at the cost of increasing false recognition and diminished discriminability for similar information. © 2021, The Author(s).},
	keywords = {Adolescent; Adult; Discrimination, Psychological; Hippocampus; Humans; Male; Mental Recall; Pattern Recognition, Visual; Photography; Recognition, Psychology; adolescent; adult; hippocampus; human; male; pattern recognition; photography; physiology; recall},
	correspondence_address = {M. Racsmány; Department of Cognitive Science, Budapest University of Technology and Economics, Budapest, Egry Jozsef utca 1, 1111, Hungary; email: racsmany@cogsci.bme.hu},
	publisher = {Nature Research},
	issn = {20452322},
	pmid = {33737589},
	language = {English},
	abbrev_source_title = {Sci. Rep.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Sinha2021,
	author = {Sinha, Saurabh Kumar and Kumar, Sunny and Kumar, Surjit and Katiyar, Gauri and Chandola, Rahul},
	title = {Plant Identification Using Machine Learning},
	year = {2021},
	journal = {2021 Asian Conference on Innovation in Technology, ASIANCON 2021},
	doi = {10.1109/ASIANCON51346.2021.9544670},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117564535&doi=10.1109%2fASIANCON51346.2021.9544670&partnerID=40&md5=a80a79136687baad767b43528b649b35},
	affiliations = {Galgotias College of Engineering and Technology, Department of Electrical and Electronics Engineering, Gr. Noida, India},
	abstract = {Identifying plants through their leaves is a thoroughly pursued endeavor that has widely varying applications ranging from ecology, horticulture, disease identification, rare plant preservation in plants to medicinal applications in Ayurveda and various plant bases medical systems. Our purpose in this project is to identify plant species digitally using the image of a single leaf through neural networks. We will approach our project using Keras, Tensorflow, and Convolutional Neural Networks. Fore mentioned approach gives satisfactory results with high accuracy.  © 2021 IEEE.},
	author_keywords = {Convolutional Neural Network (CNN); Deep Learning; Keras; Leaf image detector; Tensorflow},
	keywords = {Convolution; Deep learning; Plants (botany); Convolutional neural network; Deep learning; Image detector; Keras; Leaf image detector; Leaf images; Machine-learning; Plant identification; Tensorflow; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818583-5},
	language = {English},
	abbrev_source_title = {Asian Conf. Innov. Technol., ASIANCON},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2021 Asian Conference on Innovation in Technology, ASIANCON 2021; Conference date: 28 August 2021 through 29 August 2021; Conference code: 172291}
}

@ARTICLE{Nesteruk202117564,
	author = {Nesteruk, Sergey and Shadrin, Dmitrii and Pukalchik, Mariia and Somov, Andrey and Zeidler, Conrad and Zabel, Paul and Schubert, Daniel},
	title = {Image Compression and Plants Classification Using Machine Learning in Controlled-Environment Agriculture: Antarctic Station Use Case},
	year = {2021},
	journal = {IEEE Sensors Journal},
	volume = {21},
	number = {16},
	pages = {17564 – 17572},
	doi = {10.1109/JSEN.2021.3050084},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099580587&doi=10.1109%2fJSEN.2021.3050084&partnerID=40&md5=dd3509d890a66f73dc1f71094826d48b},
	affiliations = {Center for Computational and Data-Intensive Science and Engineering (CDISE), Skolkovo Institute of Science and Technology, Moscow, Russian Federation; Institute of Space Systems, German Aerospace Center (DLR), Bremen, Germany},
	abstract = {In this article, we share our experience in the scope of controlled-environment agriculture automation in the Antarctic station greenhouse facility called EDEN ISS. For remote plant monitoring, control, and maintenance, we solve the problem of plant classification. Due to the inherent communication limitations between Antarctica and Europe, we first propose the image compression mechanism for the data collection. We show that we can compress the images, on average, 7.2 times for efficient transmission over the weak channel. Moreover, we prove that decompressed images can be further used for computer vision applications. Upon decompressing images, we apply machine learning for the classification task. We achieve 92.6% accuracy on an 18-classes unbalanced dataset. The proposed approach is promising for a number of agriculture related applications, including the plant classification, identification of plant diseases, and deviation of plant phenology.  © 2001-2012 IEEE.},
	author_keywords = {Classification; computer vision; controlled-environment agriculture; image compression; machine learning},
	keywords = {Agricultural robots; Agriculture; Image classification; Machine learning; Agriculture-related; Antarctic stations; Classification tasks; Compression mechanism; Computer vision applications; Controlled environment agricultures; Plant classification; Plant monitoring; Image compression},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {1530437X},
	language = {English},
	abbrev_source_title = {IEEE Sensors J.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 23; All Open Access, Green Open Access}
}

@ARTICLE{Abbas2021204,
	author = {Abbas, Sawaid and Peng, Qian and Wong, Man Sing and Li, Zhilin and Wang, Jicheng and Ng, Kathy Tze Kwun and Kwok, Coco Yin Tung and Hui, Karena Ka Wai},
	title = {Characterizing and classifying urban tree species using bi-monthly terrestrial hyperspectral images in Hong Kong},
	year = {2021},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	volume = {177},
	pages = {204 – 216},
	doi = {10.1016/j.isprsjprs.2021.05.003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107051422&doi=10.1016%2fj.isprsjprs.2021.05.003&partnerID=40&md5=3078f904919045f00828af0d8aa5f703},
	affiliations = {Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong; Research Institute for Sustainable Urban Development, The Hong Kong Polytechnic University, Hong Kong; Faculty of Geosciences and Environmental Engineering, & State-Province Joint Engineering Laboratory in Spatial Information Technology for High-Speed Railway Safety, Southwest Jiaotong University, Chengdu, China; Key Laboratory of Ministry of Education on Land Resources Evaluation and Monitoring in Southwest China, Sichuan Normal University, Chengdu, China; Landscape Division, The Highways Department, HKSAR Government, China},
	abstract = {Urban trees exhibit a wide range of ecosystem services that have long been unveiled and increasingly reported. The ability to map tree species and analyze tree health conditions would become vividly essential. Remote sensing techniques, especially hyperspectral imaging, are being evolved for species identification and vegetation monitoring from spectral reponse patterns. In this study, a hyperspectral library for urban tree species in Hong Kong was established comprising 75 urban trees belonging to 19 species. 450 bi-monthly images were acquired by a terrestrial hyperspectral camera (SPECIM-IQ) from November 2018 to October 2019. A Deep Neural Network classification model was developed to identify tree species from the hyperspectral imagery with an overall accuracy ranging from 85% to 96% among different seasons. Representative spectral reflectance curves of healthy and unhealthy conditions for each species were extracted and analyzed. The hyperspectral phenology models were developed to achieve high accuracy and optimization of data acquisition. The bi-monthly canopy signatures and vegetation indices revealed different seasonality patterns of evergreen and deciduous species in Hong Kong. We explored the utility of terrestrial hyperspectral remote sensing and Deep Neural Network for urban tree species identification and characterizing. This provides a unique baseline to understand hyperspectral characteristics and seasonality of urban tree species in Hong Kong that can also contribute to hyperspectral imaging and database development elsewhere in the world. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
	author_keywords = {Deep learning; Hyperspectral library; Seasonality; SPECIM-IQ; Tree species; Urban tree},
	keywords = {China; Hong Kong; artificial neural network; data acquisition; deciduous forest; ecosystem service; evergreen forest; image analysis; imaging method; numerical model; remote sensing; spectral analysis; spectral reflectance; terrestrial ecosystem; urban area; urban ecosystem},
	correspondence_address = {M.S. Wong; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong; email: Ls.charles@polyu.edu.hk},
	publisher = {Elsevier B.V.},
	issn = {09242716},
	coden = {IRSEE},
	language = {English},
	abbrev_source_title = {ISPRS J. Photogramm. Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19}
}

@CONFERENCE{Zhang2021,
	author = {Zhang, Ying and Tang, Jianxiong and Wang, Yi},
	title = {Research on plant leaf classification and retrieval method based on machine learning},
	year = {2021},
	journal = {IOP Conference Series: Earth and Environmental Science},
	volume = {714},
	number = {2},
	doi = {10.1088/1755-1315/714/2/022071},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104244582&doi=10.1088%2f1755-1315%2f714%2f2%2f022071&partnerID=40&md5=41ff2687cda63df02654818cd97d71cb},
	affiliations = {College of Informatics, Huazhong Agricultural University, Wuhan, China},
	abstract = {In this article, a variety of leaf images are taken as the research objects. The images were preprocessed and the color, shape and texture features of the leaf images were extracted. Five machine learning algorithms were used to classify and retrieve the feature values of these leaf images, and the recognition effects of each algorithm was obtained. © Published under licence by IOP Publishing Ltd.},
	author_keywords = {Feature extraction; Machine learning; Plant classification; Plant retrieval},
	keywords = {Machine learning; Textures; Feature values; Leaf images; On-machines; Plant leaf classifications; Research object; Retrieval methods; Shape and textures; Learning algorithms},
	correspondence_address = {Y. Zhang; College of Informatics, Huazhong Agricultural University, Wuhan, China; email: zy@mail.hzau.edu.cn},
	publisher = {IOP Publishing Ltd},
	issn = {17551307},
	language = {English},
	abbrev_source_title = {IOP Conf. Ser. Earth Environ. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2020 6th International Conference on Environmental Science and Material Application, ESMA 2020; Conference date: 19 December 2020 through 20 December 2020; Conference code: 168350; All Open Access, Gold Open Access}
}

@CONFERENCE{Kanmani2021592,
	author = {Kanmani, R. and Muthulakshmi, S. and Subitcha, K Sri and Sriranjani, M. and Radhapoorani, R. and Suagnya, N.},
	title = {Modern Irrigation System using Convolutional Neural Network},
	year = {2021},
	journal = {2021 7th International Conference on Advanced Computing and Communication Systems, ICACCS 2021},
	pages = {592 – 597},
	doi = {10.1109/ICACCS51430.2021.9441917},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108022880&doi=10.1109%2fICACCS51430.2021.9441917&partnerID=40&md5=eb889a6cae1b33d42238937caf120ee9},
	affiliations = {Sri Krishna College of Technology, Department of Information Technology, TamilNadu, Coimbatore, India},
	abstract = {Over the past two decades, there is no balance between population and food production, here agriculture plays a vital role in everyone's life. And now farming has grown technologically. In the present era, agriculture is the key to living. So to evade the gap between people and agriculture, with the rise of technology in Machine Learning (ml) along with Internet of Things (IOT) it is possible. We have proposed our idea of using Convolutional Neural Networks (CNN) algorithm that paves the way for the best result. Our architecture enables analytical approach to irrigation, farming to improve quality farming. The data driven method here will increase the yield in agriculture and will reduce the overall cost, as it is a one-time setup. The conducted experiments give us a conclusion that when CNN algorithm is applied in the field of IOT it produces efficient results. This technology covers the following ideas (a) a database containing some known plants (b) a mobile application that shows the farmer, the state of his field. (c) an IoT device that contains a moisture sensor, water pump and NodeMCU (d) a server that interfaces with all the previous components (e) two machine learning models; one for plant recognition and one for wilt detection. © 2021 IEEE.},
	author_keywords = {Balance; CNN; database; Node MCU},
	keywords = {Agricultural robots; Convolution; Internet of things; Irrigation; Machine learning; Moisture control; Analytical approach; Data-driven methods; Food production; Internet of Things (IOT); Irrigation systems; Mobile applications; Moisture sensors; Plant recognition; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166540520-1},
	language = {English},
	abbrev_source_title = {Int. Conf. Adv. Comput. Commun. Syst., ICACCS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 7th International Conference on Advanced Computing and Communication Systems, ICACCS 2021; Conference date: 19 March 2021 through 20 March 2021; Conference code: 169336}
}

@ARTICLE{Pushpanathan2022413,
	author = {Pushpanathan, Kalananthni and Hanafi, Marsyita and Masohor, Syamsiah and Ilahi, Wan Fazilah Fazlil},
	title = {MYLPHerb-1: A Dataset of Malaysian Local Perennial Herbs for the Study of Plant Images Classification under Uncontrolled Environment},
	year = {2022},
	journal = {Pertanika Journal of Science and Technology},
	volume = {30},
	number = {1},
	pages = {413 – 431},
	doi = {10.47836/pjst.30.1.23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129463352&doi=10.47836%2fpjst.30.1.23&partnerID=40&md5=862e612d5530b03ac21d810301105b1d},
	affiliations = {Department of Computer and Communication Systems Engineering, Faculty of Engineering, Universiti Putra Malaysia, UPM, Selangor Darul Ehsan, Serdang, 43400, Malaysia; Department of Agriculture Technology, Faculty of Agriculture, Universiti Putra Malaysia, UPM, Selangor Darul Ehsan, Serdang, 43400, Malaysia},
	abstract = {Research in the medicinal plants’ recognition field has received great attention due to the need of producing a reliable and accurate system that can recognise medicinal plants under various imaging conditions. Nevertheless, the standard medicinal plant datasets publicly available for research are very limited. This paper proposes a dataset consisting of 34200 images of twelve different high medicinal value local perennial herbs in Malaysia. The images were captured under various imaging conditions, such as different scales, illuminations, and angles. It will enable larger interclass and intraclass variability, creating abundant opportunities for new findings in leaf classification. The complexity of the dataset is investigated through automatic classification using several high-performance deep learning algorithms. The experiment results showed that the dataset creates more opportunities for advanced classification research due to the complexity of the images. The dataset can be accessed through https://www.mylpherbs.com/. © Universiti Putra Malaysia Press.},
	author_keywords = {Deep learning; Leaf identification; Medicinal plants; Perennial herbs; Plant dataset},
	correspondence_address = {M. Hanafi; Department of Computer and Communication Systems Engineering, Faculty of Engineering, Universiti Putra Malaysia, UPM, Serdang, Selangor Darul Ehsan, 43400, Malaysia; email: marsyita@upm.edu.my},
	publisher = {Universiti Putra Malaysia Press},
	issn = {01287680},
	language = {English},
	abbrev_source_title = {Pertanika J. Sci. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Zheng2021,
	author = {Zheng, Yuhong and Wang, Da and Li, Xiaolong and Wang, Ziyang and Zhou, Qingwei and Fu, Li and Yin, Yunlong and Creech, David},
	title = {Biometric identification of taxodium spp. And their hybrid progenies by electrochemical fingerprints},
	year = {2021},
	journal = {Biosensors},
	volume = {11},
	number = {10},
	doi = {10.3390/bios11100403},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117925628&doi=10.3390%2fbios11100403&partnerID=40&md5=1512009e6dc155ca316059a11331a73c},
	affiliations = {Jiangsu Engineering Research Center for Taxodium Rich, Germplasm Innovation and Propagation, Institute of Botany, Jiangsu Province and Chinese Academy of Sciences, Nanjing Botanical Garden, Memorial Sun Yat-Sen, Nanjing, 210014, China; College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; Arthur Temple College of Forestry and Agriculture, Stephen F. Austin State University, Nacogdoches, 75962, TX, United States},
	abstract = {The use of electrochemical fingerprints for plant identification is an emerging application in biosensors. In this work, Taxodium ascendens, T. distichum, T. mucronatum, and 18 of their hybrid progenies were collected for this purpose. This is the first attempt to use electrochemical fingerprinting for the identification of plant hybrid progeny. Electrochemical fingerprinting in the leaves of Taxodium spp. was recorded under two conditions. The results showed that the electrochemical fingerprints of each species and progeny possessed very suitable reproducibility. These electrochemical fingerprints represent the electrochemical behavior of electrochemically active substances in leaf tissues under specific conditions. Since these species and progenies are very closely related to each other, it is challenging to identify them directly using a particular electrochemical fingerprinting. Therefore, electrochemical fingerprints measured under different conditions were used to perform pattern recognition. We can identify different species and progenies by locating the features in different pattern maps. We also performed a phylogenetic study with data from electrochemical fingerprinting. The results proved that the electrochemical classification results and the relationship between them are closely related. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Biometrics; Electroanalysis; Fingerprints; Plant identification; Taxodium spp},
	keywords = {Biometric Identification; Biosensing Techniques; Phylogeny; Reproducibility of Results; Taxodium; Palmprint recognition; Plants (botany); Biometric identifications; Condition; Electroanalyse; Electrochemicals; Emerging applications; Fingerprint; Plant identification; Reproducibilities; Taxodium; Taxodium spp; article; biometry; leaf tissue; nonhuman; pattern recognition; plant identification; progeny; reproducibility; Taxodium; genetic procedures; phylogeny; Biometrics},
	correspondence_address = {Y. Zheng; Jiangsu Engineering Research Center for Taxodium Rich, Germplasm Innovation and Propagation, Institute of Botany, Jiangsu Province and Chinese Academy of Sciences, Nanjing Botanical Garden, Memorial Sun Yat-Sen, Nanjing, 210014, China; email: zhengyuhong@cnbg.net; L. Fu; College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; email: fuli@hdu.edu.cn},
	publisher = {MDPI},
	issn = {20796374},
	coden = {BISSE},
	pmid = {34677359},
	language = {English},
	abbrev_source_title = {Biosensors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 31; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Ghosh20224257,
	author = {Ghosh, Sukanta and Singh, Amar and Kavita and Jhanjhi, N.Z. and Masud, Mehedi and Aljahdali, Sultan},
	title = {SVM and KNN Based CNN Architectures for Plant Classification},
	year = {2022},
	journal = {Computers, Materials and Continua},
	volume = {71},
	number = {2},
	pages = {4257 – 4274},
	doi = {10.32604/cmc.2022.023414},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122758141&doi=10.32604%2fcmc.2022.023414&partnerID=40&md5=b0d0e729064e3294e60e9e679cd7125a},
	affiliations = {Lovely Professional University, Jalandhar, 144005, India; Department of Computer Science and Engineering, Chandigarh University, Mohali, India; School of Computer Science and Engineering, Tailor’s University, Subang Jaya, 47500, Malaysia; Department of Computer Science, College of Computers and Information Technology, Taif University, Taif, 21944, Saudi Arabia},
	abstract = {Automatic plant classification through plant leaf is a classical problem in Computer Vision. Plants classification is challenging due to the introduction of new species with a similar pattern and look-a-like. Many efforts are made to automate plant classification using plant leaf, plant flower, bark, or stem. After much effort, it has been proven that leaf is the most reliable source for plant classification. But it is challenging to identify a plant with the help of leaf structure because plant leaf shows similarity in morphological variations, like sizes, textures, shapes, and venation. Therefore, it is required to normalize all plant leaves into the same size to get better performance. Convolutional Neural Networks (CNN) provides a fair amount of accuracy when leaves are classified using this approach. But the performance can be improved by classifying using the traditional approach after applying CNN. In this paper, two approaches, namely CNN + Support Vector Machine (SVM) and CNN + K-Nearest Neighbors (kNN) used on 3 datasets, namely LeafSnap dataset, Flavia Dataset, and MalayaKew Dataset. The datasets are augmented to take care all the possibilities. The assessments and correlations of the predetermined feature extractor models are given. CNN + kNN managed to reach maximum accuracy of 99.5%, 97.4%, and 80.04%, respectively, in the three datasets. © 2022 Tech Science Press. All rights reserved.},
	author_keywords = {Artificial intelligence; Deep CNN; Deep learning; KNN; Plant leaf classification; SVM; Training epoch},
	keywords = {Convolutional neural networks; Deep learning; Nearest neighbor search; Plants (botany); Textures; Convolutional neural network; Deep learning; KNN; Nearest-neighbour; Performance; Plant classification; Plant leaf classifications; Plant leaves; Support vectors machine; Training epochs; Support vector machines},
	correspondence_address = {Kavita; Department of Computer Science and Engineering, Chandigarh University, Mohali, India; email: kavita@ieee.org},
	publisher = {Tech Science Press},
	issn = {15462218},
	language = {English},
	abbrev_source_title = {Comput. Mater. Continua},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access}
}

@ARTICLE{Saxena202277,
	author = {Saxena, Deepak Kumar and Jhanwar, Deepak and Gautam, Diwakar},
	title = {Classification of Leaf Disease on Using Triangular Thresholding Method and Machine Learning},
	year = {2022},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {771},
	pages = {77 – 88},
	doi = {10.1007/978-981-16-2818-4_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115209691&doi=10.1007%2f978-981-16-2818-4_8&partnerID=40&md5=353e2d9987292ac1332edc4b6dcc2df6},
	affiliations = {Electronics and Communication Engineering, Engineering College, Ajmer, India},
	abstract = {Plant Leaves recognition and estimation is one of the most problematic companies in image processing techniques. This article subtleties techniques and methods are investigated to distinguish and estimate the severity of the disease caused by growth patterns on leaves of plants using threshold triangular strategy. Four set of images obtained from several species of plants and proposed analysis led to identify and measure the extent of the damage caused by disease-causing organisms to the leaves. Experimental research performed using machine learning techniques and final classification result being 97% accuracy. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {AI; Division; Leaf sickness; Picture securing; Thresholding},
	keywords = {Image processing; Machine learning; Optical data processing; Classification results; Experimental research; Growth patterns; Image processing technique; Leaf disease; Leaves of plants; Machine learning techniques; Thresholding methods; Plants (botany)},
	correspondence_address = {D.K. Saxena; Electronics and Communication Engineering, Engineering College, Ajmer, India; email: Deepaksaxena218@gmail.com},
	editor = {Tiwari M. and Maddila R.K. and Garg A.K. and Kumar A. and Yupapin P.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981162817-7},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 4th International Conference on Optical and Wireless Technologies, OWT 2020; Conference date: 3 October 2020 through 4 October 2020; Conference code: 264829}
}

@ARTICLE{Kaur202283,
	author = {Kaur, Priya Pinder and Singh, Sukhdev},
	title = {Random Forest Classifier Used for Modelling and Classification of Herbal Plants Considering Different Features Using Machine Learning},
	year = {2022},
	journal = {Lecture Notes in Networks and Systems},
	volume = {339},
	pages = {83 – 94},
	doi = {10.1007/978-981-16-7018-3_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126342504&doi=10.1007%2f978-981-16-7018-3_6&partnerID=40&md5=6afd460ec086fd47b6a78b8fe31fdd2a},
	affiliations = {Punjabi University, Punjab, Patiala, India; M.M. Modi College, Punjab, Patiala, India},
	abstract = {The advancement of technology in all the fields not limited to one area but today demand in the field of herbal plants has been also increased. As the change in lifestyle of human being leads to various difficulties for their living and health-related issues, there is a need for the system that will help for the recognition and identification of herbal plants that are not known to everyone and carries useful properties in life of humankind. The researcher takes advantage to carry out their research in the field of computer science using image recognition, machine learning and extends further into deep learning. In this paper images of herbal plants are taken by mobile camera and then resized to 20% from the original size for easy and fast calculation of features. Then those RGB images are converted into grayscale then to binary for reading features and calculation purposes. The random forest classifier is used to classify different herbal plant species considering shape as the main feature, based on various features such as length, width, area of the leaf, perimeter of leaf, leaf area enclosed in a rectangle, leaf percentage in the rectangle, leaf pixels in four different quadrants and rectangle pixels in four different quadrants are extracted in feature extraction. Training data consists of 85% and for testing 15% data has been used and classification results are discussed. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Classification; Feature extraction; Plant recognition; Random forest},
	correspondence_address = {P.P. Kaur; Punjabi University, Patiala, Punjab, India; email: sandhukaurpinder@gmail.com},
	editor = {Marriwala N. and Tripathi C.C. and Jain S. and Kumar D.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {23673370},
	isbn = {978-981167017-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Networks Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2nd International Conference on Mobile Radio Communications and 5G Networks, MRCN 2021; Conference date: 10 June 2021 through 12 June 2021; Conference code: 274409}
}

@ARTICLE{Vizcarra2021,
	author = {Vizcarra, Gerson and Bermejo, Danitza and Mauricio, Antoni and Zarate Gomez, Ricardo and Dianderas, Erwin},
	title = {The Peruvian Amazon forestry dataset: A leaf image classification corpus},
	year = {2021},
	journal = {Ecological Informatics},
	volume = {62},
	doi = {10.1016/j.ecoinf.2021.101268},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103308612&doi=10.1016%2fj.ecoinf.2021.101268&partnerID=40&md5=88de1c84fc2c3ebc8069b767f6a4d3dc},
	affiliations = {GESCON, Instituto de Investigaciones de la Amazonía Peruana (IIAP), Av. A. Quiñones km 2,5, Iquitos, Loreto, 16007, Peru; Universidad Nacional del Altiplano, Puno, Peru},
	abstract = {Forest census allows getting precise data for logging planning and elaboration of the forest management plan. Species identification blunders carry inadequate forest management plans and high risks inside forest concessions. Hence, an identification protocol prevents the exploitation of non-commercial or endangered timber species. The current Peruvian legislation allows the incorporation of non-technical experts, called “materos”, during the identification. Materos use common names given by the folklore and traditions of their communities instead of formal ones, which generally lead to misclassifications. In the real world, logging companies hire materos instead of botanists due to cost/time limitations. Given such a motivation, we explore an end-to-end software solution to automatize the species identification. This paper introduces the Peruvian Amazon Forestry Dataset, which includes 59,441 leaves samples from ten of the most profitable and endangered timber-tree species. The proposal contemplates a background removal algorithm to feed a pre-trained CNN by the ImageNet dataset. We evaluate the quantitative (accuracy metric) and qualitative (visual interpretation) impacts of each stage by ablation experiments. The results show a 96.64% training accuracy and 96.52% testing accuracy on the VGG-19 model. Furthermore, the visual interpretation of the model evidences that leaf venations have the highest correlation in the plant recognition task. © 2021},
	author_keywords = {Deep learning; Interpretation; Leaves dataset; Peruvian Amazon; Visual interpretation},
	keywords = {Amazon River; Peru; accuracy assessment; algorithm; community forestry; forest management; forestry; image classification; timber},
	correspondence_address = {E. Dianderas; GESCON, Instituto de Investigaciones de la Amazonía Peruana (IIAP), Loreto, Av. A. Quiñones km 2,5, Iquitos, 16007, Peru; email: edianderas@iiap.gob.pe},
	publisher = {Elsevier B.V.},
	issn = {15749541},
	language = {English},
	abbrev_source_title = {Ecol. Informatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Barhate2022,
	author = {Barhate, Deepti and Pathak, Sunil and Dubey, Ashutosh Kumar and Nemade, Varsha},
	title = {Cohort study on recognition of plant species using Deep Learning methods},
	year = {2022},
	journal = {Journal of Physics: Conference Series},
	volume = {2273},
	number = {1},
	doi = {10.1088/1742-6596/2273/1/012006},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132731860&doi=10.1088%2f1742-6596%2f2273%2f1%2f012006&partnerID=40&md5=a22bc353097e26e7383546fafea5d37f},
	affiliations = {Amity School of Engineering & Technology, Department of Computer Science & Engineering, Amity University Rajasthan, Jaipur, India; Chitkara University School of Engineering and Technology, Chitkara University, Himachal Pradesh, India; SVKM's NMIMS MPSTME, Maharashtra, Shirpur, India},
	abstract = {Plants play a vital role in each living organism's life since it maintains the environment and provides us valuable medicine, food, fragrance etc. Knowledge of species is important for the protection of biodiversity. The identification of species of plants by a manual method by botanist is tedious work besides the complex botanical terms used by an expert are annoying for a non-expert. This may lead to the obstruction for learners interested in procuring knowledge of plant species. By applying the classification of species one can also capture crops from weed for automated weedicide. Species of plant recognition are a matter of huge significance in various areas of farming, maintenance of environmental, natural, manufactured goods and medicine invention, and other related areas. Leaf color leaves contour, shape, leaf size, flowers, texture, margins, etc. are the features of plants which can be used for classification, and however, extraction of traits from selected features is the most important status in the classification. In this paper, a review-based study is done which is based on approaches such as Machine learning algorithm, Deep Learning, Convolutional Neural Networks (CNN), etc. are compared. Various classification methods like K-nearest neighbor (KNN), Naïve baise(NB), Random forest are also studied. Mostly used datasets such as Flavia, swedish, Leafsnap, ICL with their species wise features were studied. © Published under licence by IOP Publishing Ltd.},
	author_keywords = {CNN; Deep learning; KNN; Machine learning; NB},
	keywords = {Biodiversity; Classification (of information); Deep neural networks; Learning algorithms; Nearest neighbor search; Plants (botany); Textures; Cohort studies; Convolutional neural network; Deep learning; K-near neighbor; Learning methods; Living organisms; Machine-learning; Naive baise; Nearest-neighbour; Plant species; Decision trees},
	editor = {Sahu M. and Jhariya D.C. and Mohdiwale S.},
	publisher = {Institute of Physics},
	issn = {17426588},
	language = {English},
	abbrev_source_title = {J. Phys. Conf. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2022 International Conference on Applications of Intelligent Computing in Engineering and Science, AICES 2022; Conference date: 12 February 2022 through 13 February 2022; Conference code: 179931; All Open Access, Bronze Open Access}
}

@ARTICLE{Utsuki-Alexander20224315,
	author = {Utsuki-Alexander, Taku and Rios-Martinez, Jorge and Madera, Francisco A. and Pérez-Espinosa, Humberto},
	title = {Towards an intelligent personal assistant for hearing impaired people},
	year = {2022},
	journal = {Journal of Intelligent and Fuzzy Systems},
	volume = {42},
	number = {5},
	pages = {4315 – 4326},
	doi = {10.3233/JIFS-219222},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128244443&doi=10.3233%2fJIFS-219222&partnerID=40&md5=8167076531dda013420c2178e6d3016c},
	affiliations = {Facultad de Matemáticas, Universidad Autónoma de Yucatán, Mexico; CICESE-UT3, Nayarit, Mexico},
	abstract = {This work has been focused on the part of the population with hearing impairment who owns a dog and that worries about not listening the dog barks, specially when a risky situation is taking place at home. A survey was carried out on people with deafness problems to find out hazard situations which they are exposed at home. A system prototype was developed to be integrated as a component of ambient intelligence (AmI) for ambient assisted living (AAL) that serves to Hearing Impaired People (HIP). The prototype detects dog barks and notifies users through both a smart mobile app and a visual feedback. It consists of a connection between a Raspberry Pi 3 card and a ReSpeaker Mic Array v2.0 microphone array; a communication module with a smartphone was implemented, which displays written messages or vibrations when receiving notifications. The cylinder-shaped device was designed by the authors and sent it to 3D print with a resin material. The prototype recognized the barking efficiently by using a machine learning model based on Support Vector Machine technique. The prototype was tested with deaf people which were satisfied with precision, signal intensity, and activation of lights.  © 2022 - IOS Press. All rights reserved.},
	author_keywords = {ambient assisted living; Ambient intelligence; dog bark recognition; smart assistant device},
	keywords = {3D printers; Assisted living; Audition; Support vector machines; Visual communication; Ambient assisted living; Dog bark recognition; Hearing impaired; Hearing impairments; Impaired people; Microphone arrays; Mobile app; Smart assistant device; System prototype; Visual feedback; Ambient intelligence},
	correspondence_address = {J. Rios-Martinez; Fac. de Matemáticas, Uady, Anillo Periférico Norte, Colonia Chuburná Hidalgo Inn, Tablaje Cat. 13615 Mérida Yucatán, Mexico; email: jorge.rios@correo.uady.mx},
	publisher = {IOS Press BV},
	issn = {10641246},
	language = {English},
	abbrev_source_title = {J. Intelligent Fuzzy Syst.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ghosh2022,
	author = {Ghosh, Aditi and Roy, Parthajit},
	title = {An automated model for leaf image-based plant recognition: an optimal feature-based machine learning approach},
	year = {2022},
	journal = {Innovations in Systems and Software Engineering},
	doi = {10.1007/s11334-022-00440-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126375787&doi=10.1007%2fs11334-022-00440-y&partnerID=40&md5=9bf48137642d2e5eeb19d4cef79dafd8},
	affiliations = {The Department of Computer Science, The University of Burdwan, West Bengal, Purba Bardhaman, 713104, India},
	abstract = {Present study focuses on identifying plant species automatically from leaf images using optimal feature set. The need of classifying a plant species is increasing because of its numerous application domains. In this study an automatic leaf recognition model has been introduced based on image processing, pattern analysis and Machine Learning (ML) technology. The main focus of this study is to find out optimal feature set with higher accuracy. Two most popular datasets Flavia and Swedish have been taken for classification. Different types of features like Gray Level Co-occurrence Matrix (GLCM), Local Binary Pattern (LBP), Hu Invariant Moment have been used in different combinations to achieve higher accuracy. Accuracy obtained from different feature combinations have been given in the result section elaborately. Using this optimal feature set result obtained from different classifiers have also been given. In our previous research work the accuracy was 93.98 and 94.66%, respectively, on this two dataset. A significant improvement has been achieved in accuracy in this present study. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
	author_keywords = {Artificial Neural Network; Deep Learning; Image Processing; Local Binary Pattern; Machine Learning},
	keywords = {Deep neural networks; Image processing; Automated modelling; Deep learning; High-accuracy; Image-based; Images processing; Leaf images; Local binary patterns; Machine-learning; Optimal feature sets; Plant species; Classification (of information)},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {16145046},
	language = {English},
	abbrev_source_title = {Innov. Syst. Softw. Eng.},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Shapiee2022541,
	author = {Shapiee, Muhammad Nur Aiman and Abdul Manan, Amirul Asyraf and Mohd Razman, Mohd Azraai and Mohd Khairuddin, Ismail and P. P. Abdul Majeed, Anwar},
	title = {Chili Plant Classification Using Transfer Learning Models Through Object Detection},
	year = {2022},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {900},
	pages = {541 – 551},
	doi = {10.1007/978-981-19-2095-0_46},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131127114&doi=10.1007%2f978-981-19-2095-0_46&partnerID=40&md5=583c6b5e38ca4f41818c07302a5fa965},
	affiliations = {Innovative Manufacturing, Mechatronics and Sports Laboratory, Faculty of Manufacturing and Mechatronic Engineering Technology, Universiti Malaysia Pahang, Pahang Darul Makmur, Pekan, 26600, Malaysia; School of Robotics, XJTLU Entrepreneur College (Taicang), Xi’an Jiaotong - Liverpool University, Suzhou, 215123, China},
	abstract = {This study presents the use of a Convolutional Neural Network (CNN) based detector to detect chili and its leaves in the chili plant image. Detecting chili on its plant is essential for the development of robotic vision and monitoring. Thus, helps us supervise the plant growth, furthermore, analyses their productivity and quality. This paper aims to develop a system that can monitor and identify bird’s eye chili plants by implementing machine learning. First, the development of methodology for efficient detection of bird’s eye chili and its leaf was made. The image labeling will provide the source of images between chili and leaf. The dataset would be split into training, verification and test set for 70:20:10%, correspondingly. The images YOLO V4 Darknet was implemented to train the dataset. After a series of experiments were conducted, the model is compared with other transfer learning models like YOLO V4 Tiny, Faster R-CNN, and EfficientDet. The classification performance of these transfer learning models has been calculated and compared with each other. The experimental result would discuss on part of hyper parameter optimization and transfer learning application. Firstly, the optimization of hyper parameter shows that the YOLO V4 Darknet model achieves mAP of 76.54%, followed by EfficientDet at 73.66% for 512 × 512 input layers. Next, the application of transfer learning. The result shows that YOLO V4 Darknet achieves highest mAP value, 75.69% follow by EfficientDet, with mAP of 71.85%. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Chili plant; Machine learning; Object detection; Precision agriculture; Transfer learning},
	keywords = {Birds; Computer aided instruction; Machine learning; Object recognition; Plants (botany); Precision agriculture; Statistical tests; Chili plant; Convolutional neural network; Darknets; Learning models; Network-based; Objects detection; Plant classification; Precision Agriculture; Robotic vision; Transfer learning; Object detection},
	correspondence_address = {A. P. P. Abdul Majeed; Innovative Manufacturing, Mechatronics and Sports Laboratory, Faculty of Manufacturing and Mechatronic Engineering Technology, Universiti Malaysia Pahang, Pekan, Pahang Darul Makmur, 26600, Malaysia; email: amajeed@ump.edu.my},
	editor = {Khairuddin I.M. and Abdullah M.A. and Ab. Nasir A.F. and Mat Jizat J.A. and Mohd. Razman M.A. and Abdul Ghani A.S. and Zakaria M.A. and Mohd. Isa W.H. and Abdul Majeed A.P.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981192094-3},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: Innovative Manufacturing, Mechatronics and Materials Forum, iM3F 2021; Conference date: 20 September 2021 through 20 September 2021; Conference code: 277979}
}

@ARTICLE{Ariyapadath20211587,
	author = {Ariyapadath, Sujith},
	title = {Plant Leaf Classification and Comparative Analysis of Combined Feature Set Using Machine Learning Techniques},
	year = {2021},
	journal = {Traitement du Signal},
	volume = {38},
	number = {6},
	pages = {1587 – 1598},
	doi = {10.18280/ts.380603},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123320831&doi=10.18280%2fts.380603&partnerID=40&md5=ec3f7f5a8252ad85642fffe35157d3b2},
	affiliations = {Department of Computer Science, University of Kerala, Research Center, Thiruvananthapuram, 695581, India},
	abstract = {The main purpose of this research work is to apply machine learning and image processing techniques for plant classification efficiently. In the plant classification system, the conventional method is time-consuming and needs to apply expensive analytical instruments. The automated plant classification system helps to predict plant classes easily. The most challenging part of the automated plant classification research is to extract unique features of leaves. This paper proposes a plant classification model using an optimal feature set with combined features. The proposed model is used to extract features from leaf images and applied to image classification algorithms. After the evaluation process, it is found that GIST, Local Binary Pattern and Pyramid Histogram Oriented Gradient have better results than others in this particular application. Combined these three features extraction techniques and selected the optimal feature set through Neighbourhood Component Analysis. The optimal feature set helps classify plants with maximum accuracy in minimal time. Here performed an extensive experimental comparison of the proposed optimal feature set and other feature extraction methods using different classifiers and tested on different data sets (Swedish Leaves, Flavia, D-Leaf). The results confirm that this optimal feature set with NCA using ANN classifier leads to better classification achieved 98.99% accuracy in 353.39 seconds. © 2021 Lavoisier. All rights reserved.},
	author_keywords = {Artificial neural network; GIST; Local binary pattern; Machine learning; Neighbourhood component analysis; Optimal feature set; Plant classification; Pyramid histogram oriented gradient},
	keywords = {Classification (of information); Extraction; Graphic methods; Image classification; Machine learning; Plants (botany); Classification system; Combined features; GIST; Local binary patterns; Neighborhood component analysis; Optimal feature sets; Oriented gradients; Plant classification; Plant leaf classifications; Pyramid histogram oriented gradient; Neural networks},
	correspondence_address = {S. Ariyapadath; Department of Computer Science, University of Kerala, Research Center, Thiruvananthapuram, 695581, India; email: sujithmail.dcb@keralauniversity.ac.in},
	publisher = {International Information and Engineering Technology Association},
	issn = {07650019},
	language = {English},
	abbrev_source_title = {Trait. Signal},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Bronze Open Access}
}

@ARTICLE{Atique20222195,
	author = {Atique, Aneeq and Karim, Saira and Shahid, Saman and Alamgir, Zareen},
	title = {IDENTIFICATION OF PLANT SPECIES THROUGH LEAF VEIN MORPHOMETRIC AND DEEP LEARNING},
	year = {2022},
	journal = {Pakistan Journal of Botany},
	volume = {54},
	number = {6},
	pages = {2195 – 2202},
	doi = {10.30848/PJB2022-6(38)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132966948&doi=10.30848%2fPJB2022-6%2838%29&partnerID=40&md5=fe2ca7dbad0fed5c3ba8f19d28bd28c3},
	affiliations = {FAST School of Computing (FSC), National University of Computer and Emerging Sciences (NUCES), FAST Lahore Campus, Pakistan; Department of Sciences & Humanities (S&H), National University of Computer and Emerging Sciences (NUCES), FAST Lahore Campus, Pakistan},
	abstract = {Taxonomy language is challenging to comprehend and automated knowledge is required to identify the plant species. The study focused on developing an improved deep neural network: Residual Neural Network-ResNet & and Densely Connected Convolution Network (DenseNet) for the plant identification with plant leaf vein architecture. There was a total of 44 species. Each species had 64 images, each of which was further divided into 52 images for the training data and 12 images for the test data. The Canny edge detection method was deployed to detect the vein architecture of the leaves. For ResNet and DenseNet, the 224 x 224 binary image was used. The size of the feature maps in 4 dense blocks was: 56 x 56, 28 x 28, 14 x 14, and 7 x 7, respectively. MalayaKew (MK) data set was used for the experiment. There was a total of 44 classes and images were divided into the training set and the test set. The training set contained 2288 images, with each class having 52 images. Test class contained 528 images, with each class having 12 images. After preprocessing these images, they were fed to various networks of ResNet and DenseNet. Two algorithms, Stochastic gradient descent (SGD) and Adam optimization, were used in each network. Through SGD, the model ResNet, had 26, 34, 50, 101, and 152 layers. The best accuracy achieved was 89.24% using 50 layers. DenseNet had 121, 169, and 201 layers. The best accuracy achieved was 94.20% using 169 layers. In Adam optimizer, the ResNet model had 26, 34, 50, 101, and 152 layers. The best accuracy achieved was 89.50% using 101 layers. DenseNet had 121, 169, and 201 layers. The best accuracy achieved was 95.72% using 169 layers. Overall, the best performance was achieved using Adam optimizer using the DenseNet model with 169 layers and came out to be 95.72%. This also surpassed the accuracy that was achieved using D-leaf architecture. The proposed deep learning (DL) methods were very accurate in identifying plants. © 2022, Pakistan Botanical Society. All rights reserved.},
	author_keywords = {Densely connected convolution network-densenet; Plant identification; Residual neural network-resnet},
	correspondence_address = {S. Shahid; Department of Sciences & Humanities (S&H), National University of Computer and Emerging Sciences (NUCES), FAST Lahore Campus, Pakistan; email: drshahidsaman@gmail.com},
	publisher = {Pakistan Botanical Society},
	issn = {05563321},
	language = {English},
	abbrev_source_title = {Pak. J. Bot.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Kaur2022,
	author = {Kaur, Priya Pinder and Singh, Sukhdev and Sandhu, Inderjeet Singh and Sandhanwalia, Damandeep Singh},
	title = {Leaf Recognition System Based on Supervised Machine Learning Techniques},
	year = {2022},
	journal = {2022 IEEE Delhi Section Conference, DELCON 2022},
	doi = {10.1109/DELCON54057.2022.9753062},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129420997&doi=10.1109%2fDELCON54057.2022.9753062&partnerID=40&md5=eec91987e47b161b61c57d56c16e83fe},
	affiliations = {Punjabi University, Department of Computer Science, Patiala, India; Multani Mal Modi College, Department of Computer Science, Patiala, India; Punjab Engineering College, Department of Mechanical Engineering, Chandigarh, India; Gndu, Department of Computer Science, Amritsar, India},
	abstract = {In today's era, the development of science and technology is achieving a large number of advancements not in various multidimensional fields. The advancement of machine learning provides a path for researchers to explore more deeply the field of deep learning. Image processing can be used for the recognition, identification, and classification of plants. In this paper different features of plants such as height, perimeter, width, area, aspect ratio, rectangularity, etc. are used to classify the plants based on leaf images and using multiple machine learning techniques. Different supervised learning classifiers such as random forest, k-nearest neighbor, naïve bayes, support vector machine, and logistic regression are used to recognize the plant, and their results are discussed to build an automatic recognition system. Flavia, Mendeley, and self-created datasets are used for analysis. SVM model produced maximum accuracy of 95% for Flavia dataset, 94% for self-created dataset whereas Mendeley dataset resulted in 89% accuracy. © 2022 IEEE.},
	author_keywords = {Herbal plants; K-Nearest Neighbor; Logistic Regression; Naïve Bayes; Random Forest; Supervised Learning; Support Vector Machine},
	keywords = {Aspect ratio; Classifiers; Deep learning; Image processing; Logistic regression; Nearest neighbor search; Random forests; Support vector regression; Herbal plants; K-near neighbor; Leaf recognition; Logistics regressions; Machine learning techniques; Naive bayes; Nearest-neighbour; Random forests; Supervised learning; Support vectors machine; Decision trees},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166545883-2},
	language = {English},
	abbrev_source_title = {IEEE Delhi Sect. Conf., DELCON},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 IEEE Delhi Section Conference, DELCON 2022; Conference date: 11 February 2022 through 13 February 2022; Conference code: 178847}
}

@ARTICLE{Ghosh2022243,
	author = {Ghosh, Aditi and Roy, Parthajit},
	title = {A Convolutional Neural Network Model for Automatic Leaf Recognition},
	year = {2022},
	journal = {Lecture Notes in Networks and Systems},
	volume = {292},
	pages = {243 – 255},
	doi = {10.1007/978-981-16-4435-1_24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113417475&doi=10.1007%2f978-981-16-4435-1_24&partnerID=40&md5=29a1556613b5938925274fa27b0c8b1e},
	affiliations = {The Department of Master of Computer Applications, Techno India Hooghly, Chinsurah, West Bengal, 712101, India; The Department of Computer Science, The University of Burdwan, Purba Bardhaman, West Bengal, 713104, India},
	abstract = {In this study, an automatic recognition system has been designed for detection of leaves from their images. The model has been developed based on the Object Oriented software engineering model. For the bottom level machine learning, convolution neural network based deep learning has been used. The present study uses two benchmark datasets one consisting of 1906 instances of 32 species and the other consisting of 1125 instances of 15 different leaf images. The results of the proposed model have been tested in standard indexes and the same are compared with that of other existing models. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Convolutional neural network; Deep learning; Image processing; Leaf recognition; Object oriented analysis and design},
	correspondence_address = {P. Roy; The Department of Computer Science, The University of Burdwan, Purba Bardhaman, West Bengal, 713104, India; email: roy.parthajit@gmail.com},
	editor = {Mandal J.K. and De D.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {23673370},
	isbn = {978-981164434-4},
	language = {English},
	abbrev_source_title = {Lect. Notes Networks Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 6th International Conference on Emerging Applications of Information Technology, EAIT 2020; Conference date: ; Conference code: 263369}
}

@ARTICLE{Günder2022,
	author = {Günder, Maurice and Ispizua Yamati, Facundo R and Kierdorf, Jana and Roscher, Ribana and Mahlein, Anne-Katrin and Bauckhage, Christian},
	title = {Agricultural plant cataloging and establishment of a data framework from UAV-based crop images by computer vision},
	year = {2022},
	journal = {GigaScience},
	volume = {11},
	doi = {10.1093/gigascience/giac054},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132454886&doi=10.1093%2fgigascience%2fgiac054&partnerID=40&md5=17cc31a7b5eb6f509c7d13245e451ff8},
	affiliations = {Fraunhofer Institute for Intelligent Analysis and Information Systems Iais, Schloss Birlinghoven, Sankt Augustin, 53757, Germany; Institute for Computer Science Iii, University of Bonn, Friedrich-Hirzebruch-Allee 5, Bonn, 53115, Germany; Institute for Sugar Beet Research (IfZ), Holtenser Landstraße 77, Göttingen, 37079, Germany; Institute for Geodesy and Geoinformation, University of Bonn, Niebuhrstraße 1a, Bonn, 53113, Germany; Department of Aerospace and Geodesy, Data Science in Earth Observation, Technical University of Munich, Lise-Meitner-Straße 9, Ottobrunn, 85521, Germany},
	abstract = {Background: Unmanned aerial vehicle (UAV)-based image retrieval in modern agriculture enables gathering large amounts of spatially referenced crop image data. In large-scale experiments, however, UAV images suffer from containing a multitudinous amount of crops in a complex canopy architecture. Especially for the observation of temporal effects, this complicates the recognition of individual plants over several images and the extraction of relevant information tremendously. Results: In this work, we present a hands-on workflow for the automatized temporal and spatial identification and individualization of crop images from UAVs abbreviated as "cataloging"based on comprehensible computer vision methods. We evaluate the workflow on 2 real-world datasets. One dataset is recorded for observation of Cercospora leaf spot - a fungal disease - in sugar beet over an entire growing cycle. The other one deals with harvest prediction of cauliflower plants. The plant catalog is utilized for the extraction of single plant images seen over multiple time points. This gathers a large-scale spatiotemporal image dataset that in turn can be applied to train further machine learning models including various data layers. Conclusion: The presented approach improves analysis and interpretation of UAV data in agriculture significantly. By validation with some reference data, our method shows an accuracy that is similar to more complex deep learning-based recognition techniques. Our workflow is able to automatize plant cataloging and training image extraction, especially for large datasets. © 2022 The Author(s) 2022. Published by Oxford University Press GigaScience.},
	author_keywords = {plant identification; plant individualization; precision agriculture; remote sensing; UAV imaging},
	keywords = {Agriculture; Computers; Crops, Agricultural; Remote Sensing Technology; Article; Cercospora; computer vision; controlled study; deep learning; image processing; information processing; leaf spot; machine learning; mycosis; nonhuman; plant identification; precision agriculture; remote sensing; unmanned aerial vehicle; workflow; agriculture; computer; crop; procedures; remote sensing},
	correspondence_address = {M. Günder; Fraunhofer Iais, Schloss Birlinghoven, Sankt Augustin, 53757, Germany; email: mguender@uni-bonn.de},
	publisher = {Oxford University Press},
	issn = {2047217X},
	pmid = {35715875},
	language = {English},
	abbrev_source_title = {GigaScience},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Arshed202240,
	author = {Arshed, Muhammad Asad and Ghassan, Hadia and Hussain, Mubashar and Hassan, Muhammad and Kanwal, Ayesha and Fayyaz, Rimsha},
	title = {A Light Weight Deep Learning Model for Real World Plant Identification},
	year = {2022},
	journal = {2022 2nd International Conference on Distributed Computing and High Performance Computing, DCHPC 2022},
	pages = {40 – 45},
	doi = {10.1109/DCHPC55044.2022.9731841},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127441349&doi=10.1109%2fDCHPC55044.2022.9731841&partnerID=40&md5=e6e48a442894e0617ced87ed51a3f5a1},
	affiliations = {School of Systems and Technology (SST), University of Management and Technology, Lahore, Pakistan; Minhaj University Lahore, Department of Computer Science, Lahore, Pakistan; University of Engineering and Technology, Department of Computer Science, Lahore, Pakistan},
	abstract = {Automatic identification and classification of different plant leaf species have become a common trend among researchers and scientists. To obtain a result with better precision, they use various methods and techniques of deep learning to build a model. Convolutional neural networks are becoming the most common method used by scientists to classify plant leaves. However, the classification of plant leaves can be challenging with more rare species and complicated backgrounds, for which researchers build several models to achieve high-level accuracy. In the present study for the classification of leaves, we have created a model for plant leaf classification based on a dataset we collected. We've used the Resnet-50 model, a well-known CNN architecture, which provided an efficient method to organize and analyze a deep classification to reduce the complexity so that there will be fewer parameters for training and low time consumption as well. Using Resnet-50, we intended to develop a significant result in our classification model. The convolutional neural network is famous for its influential abilities in feature extraction and classification. And Resnet-50 being a residual network enabled us to train deep networks in our model. The average training accuracy reached 98.3%, while the average testing accuracy reached 92.5%. The key contribution of this study is effective accuracy as well as we have trained the model on our own prepared dataset that we have prepared from real world environment. Data Availability: https://drive.google.com/file/d/1bD7B257l-6wqUCQHBWhle95xyrotUbwO/view?usp=sharing  © 2022 IEEE.},
	author_keywords = {Convolutional Neural Network (CNN); Deep Learning; Plant Classification; Resnet-50},
	keywords = {Classification (of information); Convolution; Deep learning; Plants (botany); Convolutional neural network; Deep learning; Learning models; Light weight; Plant classification; Plant identification; Plant leaves; Real-world; Resnet-50; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549672-8},
	language = {English},
	abbrev_source_title = {Int. Conf. Distrib. Comput. High Perform. Comput., DCHPC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2nd International Conference on Distributed Computing and High Performance Computing, DCHPC 2022; Conference date: 2 March 2022 through 3 March 2022; Conference code: 177844}
}

@ARTICLE{Wang20221018,
	author = {Wang, Bin and Gao, Yongsheng and Yuan, Xiaohui and Xiong, Shengwu},
	title = {Local R-Symmetry Co-Occurrence: Characterising Leaf Image Patterns for Identifying Cultivars},
	year = {2022},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	volume = {19},
	number = {2},
	pages = {1018 – 1031},
	doi = {10.1109/TCBB.2020.3031280},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120548479&doi=10.1109%2fTCBB.2020.3031280&partnerID=40&md5=5874bbad6434c8095ba23e1c7a195aee},
	affiliations = {School of Engineering and Built Environment, Griffith University, Brisbane, Australia; School of Computer Science and Technology, Wuhan University of Technology, Wuhan, China},
	abstract = {Leaf image recognition techniques have been actively researched for plant species identification. However it remains unclear whether analysing leaf patterns can provide sufficient information for further differentiating cultivars. This paper reports our attempt on cultivar recognition from leaves as a general very fine-grained pattern recognition problem, which is not only a challenging research problem but also important for cultivar evaluation, selection and production in agriculture. We propose a novel local R-symmetry co-occurrence method for characterising discriminative local symmetry patterns to distinguish subtle differences among cultivars. Through scalable and moving R-relation radius pairs, we generate a set of radius symmetry co-occurrence matrices (RsCoM)and their measures for describing the local symmetry properties of interior regions. By varying the size of the radius pair, the RsCoM measures local R-symmetry co-occurrence from global/coarse to fine scales. A new two-phase strategy of analysing the distribution of local RsCoM measures is designed to match the multiple scale appearance symmetry pattern distributions of similar cultivar leaf images. We constructed three leaf image databases, SoyCultivar, CottCultivar, and PeanCultivar, for an extensive experimental evaluation on recognition across soybean, cotton and peanut cultivars. Encouraging experimental results of the proposed method in comparison with the state-of-the-art leaf species recognition methods demonstrate the effectiveness of the proposed method for cultivar identification, which may advance the research in leaf recognition from species to cultivar.  © 2004-2012 IEEE.},
	author_keywords = {cotton cultivar; cultivar classification; fine-grained recognition; leaf image; peanut cultivar; Plant species recognition; R-symmetry co-occurrence; soybean cultivar},
	keywords = {Agriculture; Plant Leaves; Cotton; Image classification; Image recognition; Plants (botany); Co-occurrence; Cotton cultivars; Cultivar classification; Fine grained; Fine-grained recognition; Leaf images; Peanut cultivar; Plant species; Plant species recognition; R-symmetry co-occurrence; Soybean cultivar; Species recognition; agriculture; plant leaf; Oilseeds},
	correspondence_address = {Y. Gao; School of Engineering and Built Environment, Griffith University, Brisbane, Australia; email: yongsheng.gao@griffith.edu.au},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {15455963},
	pmid = {33055018},
	language = {English},
	abbrev_source_title = {IEEE/ACM Trans. Comput. BioL. Bioinf.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{Sun2022,
	author = {Sun, Zhu and Guo, Xiangyu and Xu, Yang and Zhang, Songchao and Cheng, Xiaohui and Hu, Qiong and Wang, Wenxiang and Xue, Xinyu},
	title = {Image Recognition of Male Oilseed Rape (Brassica napus) Plants Based on Convolutional Neural Network for UAAS Navigation Applications on Supplementary Pollination and Aerial Spraying},
	year = {2022},
	journal = {Agriculture (Switzerland)},
	volume = {12},
	number = {1},
	doi = {10.3390/agriculture12010062},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122271165&doi=10.3390%2fagriculture12010062&partnerID=40&md5=be24321c3b28d8885317ae9f17e6f9ff},
	affiliations = {Nanjing Institute of Agricultural Mechanization, Ministry of Agriculture and Rural Affairs, Nanjing, 210014, China; Sino-USA Pesticide Application Technology Cooperative Laboratory, Nanjing, 210014, China; College of Biosystems Engineering and Food Science, Zhejiang University, Hangzhou, 310058, China; Oil Crops Research Institute, Chinese Academy of Agricultural Sciences, Wuhan, 430062, China; Key Laboratory of Aviation Plant Protection, Ministry of Agriculture and Rural Affairs, Anyang, 455000, China},
	abstract = {To ensure the hybrid oilseed rape (OSR, Brassica napus) seed production, two important things are necessary, the stamen sterility on the female OSR plants and the effective pollen spread onto the pistil from the OSR male plants to the OSR female plants. The unmanned agricultural aerial system (UAAS) has developed rapidly in China. It has been used on supplementary pollination and aerial spraying during the hybrid OSR seed production. This study developed a new method to rapidly recognize the male OSR plants and extract the row center line for supporting the UAAS navigation. A male OSR plant recognition model was constructed based on the convolutional neural network (CNN). The sequence images of male OSR plants were extracted, the feature regions and points were obtained from the images through morphological and boundary process methods and horizontal segmentation, respectively. The male OSR plant image recognition accuracies of different CNN structures and segmentation sizes were discussed. The male OSR plant row center lines were fitted using the least-squares method (LSM) and Hough transform. The results showed that the segmentation algorithm could segment the male OSR plants from the complex background. The highest average recognition accuracy was 93.54%, and the minimum loss function value was 0.2059 with three convolutional layers, one fully connected layer, and a segmentation size of 40 pix × 40 pix. The LSM is better for center line fitting. The average recognition model accuracies of original input images were 98% and 94%, and the average root mean square errors (RMSE) of angle were 3.22◦ and 1.36◦ under cloudy day and sunny day lighting conditions, respectively. The results demonstrate the potential of using digital imaging technology to recognize the male OSR plant row for UAAS visual navigation on the applications of hybrid OSR supplementary pollination and aerial spraying, which would be a meaningful supplement in precision agriculture. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Aerial spraying; Convolutional neural network; Hybrid oilseed rape; Image processing; Male parent recognition; Seed production; UAAS visual navigation},
	correspondence_address = {S. Zhang; Nanjing Institute of Agricultural Mechanization, Ministry of Agriculture and Rural Affairs, Nanjing, 210014, China; email: zhangsongchao@caas.cn; X. Xue; Nanjing Institute of Agricultural Mechanization, Ministry of Agriculture and Rural Affairs, Nanjing, 210014, China; email: xuexinyu@caas.cn},
	publisher = {MDPI},
	issn = {20770472},
	language = {English},
	abbrev_source_title = {Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access}
}

@ARTICLE{Salve20221361,
	author = {Salve, Pradip and Yannawar, Pravin and Sardesai, Milind},
	title = {Multimodal plant recognition through hybrid feature fusion technique using imaging and non-imaging hyper-spectral data},
	year = {2022},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	volume = {34},
	number = {1},
	pages = {1361 – 1369},
	doi = {10.1016/j.jksuci.2018.09.018},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055435272&doi=10.1016%2fj.jksuci.2018.09.018&partnerID=40&md5=9bf8eddb29c486aa1091306ac65a04ba},
	affiliations = {Vision & Intelligence Lab, Department of Computer Science and IT, Dr. Babasaheb Ambedkar, Marathwada University, Aurangabad, (MS), India; Floristic Research Lab, Department of Botany, Savitribai Phule Pune University, Pune, India},
	abstract = {Automatic classification of the plants is growing area of association with computer science and Botany, it has attracted many researchers to subsidize plant classification using image processing and machine learning techniques. Plants can be classified using number of traits such as leaf color, flowers, leaves, roots, leaf shape, leaf size etc. highly depends upon feature selection methods. However extraction of features from selected trait is most significant state in classification. State-of-the-art classification can be achieved by using leaf characteristics such as leaf venation patterns, leaf spectral signatures, leaf color, leaf shape, etc. This paper describes multimodal plant classification system using leaf venation patterns and its spectral signatures as a significant features. This paper shows that the feature fusion can be used to achieve efficient plant identification. The accuracy of identification for leaf spectral data, leaf venation features and HOG features is validated, it signifies that feature fusion technique performs better than that of non-imaging spectral signatures features only, with recognition result of 98.03% GAR and 93.51% GAR, respectively. © 2018 The Authors},
	author_keywords = {Leaf spectral signature; Leaf venation; Multimodal plant; Plant identification},
	correspondence_address = {P. Salve; Department of Computer science and IT, Dr. Babasaheb Ambedkar, Marathwada University, Aurangabad, 431004, India; email: pradipslv@gmail.com},
	publisher = {King Saud bin Abdulaziz University},
	issn = {13191578},
	language = {English},
	abbrev_source_title = { J. King Saud Univ. - Comput. Inform. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access, Gold Open Access}
}

@ARTICLE{Wang2021,
	author = {Wang, Da and Li, Dongling and Fu, Li and Zheng, Yuhong and Gu, Yonghua and Chen, Fei and Zhao, Shichao},
	title = {Can electrochemical sensors be used for identification and phylogenetic studies in lamiaceae?},
	year = {2021},
	journal = {Sensors},
	volume = {21},
	number = {24},
	doi = {10.3390/s21248216},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120783877&doi=10.3390%2fs21248216&partnerID=40&md5=f6b3c1add3825a6a096c5b8393dd5770},
	affiliations = {Key Laboratory of Novel Materials for Sensor of Zhejiang Province, College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; The Jiangsu Provincial Platform for Conservation and Utilization of Agricultural Germplasm, Institute of Botany, Jiangsu Province and Chinese Academy of Sciences (Nanjing Botanical Garden Memorial Sun Yat‐Sen), Nanjing, 210014, China},
	abstract = {Electrochemical sensors have shown potential in recent years for plant species identification and phylogenetic studies. These works have been used to investigate the affinities of different species in many genera. However, the ability of electrochemical sensors to study relationships between different genera within a family has not been investigated. In this work, we selected 31 species in the Labiatae and 5 exotaxa as subjects to investigate the feasibility of electrochemical sensors at the genus level. The results show that electrochemical sensors are still very effective for the identification of these plants. Different pattern recognition techniques can make the identification more efficient. Also, the fingerprint profiles collected by the sensors can be used for phylogenetic studies of Labiatae. The phylogram divides all the species into five clusters, where the exotaxa are in one cluster. Species in the Labiatae are mainly distributed in four other clusters. Importantly, the different genera of species all showed close affinities, representing that electrochemical fingerprinting can well distinguish the affinities between the different genera. The results of this work demonstrate the great potential of electrochemical sensors in the study of plant phylogeny. Its application is not limited to the study at the species level, but can be extended to the genus level. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Electrochemical sensor; Fingerprints; Labiatae; Plant identification; Plant phylogeny},
	keywords = {Humans; Lamiaceae; Phylogeny; Plants; Biology; Palmprint recognition; Electrochemicals; Fingerprint; ITS applications; Labiatae; Lamiaceae; Pattern recognitions techniques; Phylogenetic studies; Plant identification; Plant phylogeny; Plant species identification; human; Lamiaceae; phylogeny; plant; Electrochemical sensors},
	correspondence_address = {L. Fu; Key Laboratory of Novel Materials for Sensor of Zhejiang Province, College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; email: fuli@hdu.edu.cn},
	publisher = {MDPI},
	issn = {14248220},
	pmid = {34960306},
	language = {English},
	abbrev_source_title = {Sensors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 29; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Wu2021,
	author = {Wu, Peng and Qian, Zhou},
	title = {Leaf Classification Based on Convolutional Neural Network},
	year = {2021},
	journal = {Journal of Physics: Conference Series},
	volume = {1820},
	number = {1},
	doi = {10.1088/1742-6596/1820/1/012161},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103287930&doi=10.1088%2f1742-6596%2f1820%2f1%2f012161&partnerID=40&md5=972cf254609eb95d96b41996af9156da},
	affiliations = {Shandong Institute of Commerce and Technology, No. 4516, Tourist Road, Jinan City, Shandong Prov. Jinan, 250103, China},
	abstract = {Convolutional Neural Network (CNN), a very important neural network structure in deep learning, is a network model often used in image classification, target recognition and other fields. In botany, leaf classification and recognition is very important for identifying new or scarce tree species. In nature, plants are widely distributed, and the survival and development of all living things on the earth depend on plants. Identification of species by leaves and related research are of great help to the study the evolution law of plants, the protection of plant species and the development of agriculture. This paper uses convolutional neural network in artificial intelligence to identify the leaves of several kinds of trees collected by Kunming Institute of Botany, Yunnan Province, which can realize the automatic extraction of leaf image features, reduce tedious labor costs, and realize the use of artificial intelligence to classify leaves, thus providing an auxiliary means of artificial intelligence for botany research.  © Published under licence by IOP Publishing Ltd.},
	keywords = {Agricultural robots; Convolution; Deep learning; Forestry; Manufacture; Plants (botany); Wages; Automatic extraction; Evolution law; Leaf classification; Living things; Network modeling; Neural network structures; Target recognition; Yunnan province; Convolutional neural networks},
	correspondence_address = {Z. Qian; Shandong Institute of Commerce and Technology, Jinan City, Shandong Prov. Jinan, No. 4516, Tourist Road, 250103, China; email: zhouqian@sict.edu.cn},
	publisher = {IOP Publishing Ltd},
	issn = {17426588},
	language = {English},
	abbrev_source_title = {J. Phys. Conf. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2021 International Conference on Mechanical Engineering, Intelligent Manufacturing and Automation Technology, MEMAT 2021; Conference date: 15 January 2021 through 17 January 2021; Conference code: 167813; All Open Access, Gold Open Access}
}

@CONFERENCE{Sun2021393,
	author = {Sun, Zhe and Feng, Wei and Jin, Jintao and Lei, Qujiang and Gui, Guangchao and Wang, Weijun},
	title = {Intelligent Fertilization System Based on Image Recognition},
	year = {2021},
	journal = {2021 IEEE 6th International Conference on Computer and Communication Systems, ICCCS 2021},
	pages = {393 – 399},
	doi = {10.1109/ICCCS52626.2021.9449144},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113344275&doi=10.1109%2fICCCS52626.2021.9449144&partnerID=40&md5=00dc2cf1483ea2a293816e48ad27092d},
	affiliations = {Guangzhou Institute of Advanced Technology, Chinese Academy of Sciences, Guangzhou, 511458, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China},
	abstract = {In order to solve the problem that the normal fertilization robot cannot feed different species of plants in one time, an intelligent fertilization robot based on image recognitions was designed. Based on the customers' requirements, the system we designed can supply the machine ability of normal plants classification and auto fertilization after verifying the plants. In this paper, the Intelligent Fertilization System Based on Image Recognition will be introduced but base on the data leakage of dataset, it requires the plants to be pictured in the same background. © 2021 IEEE.},
	author_keywords = {convolutional neural network; deep learning; image classification; plants species recognition},
	keywords = {Robots; Security of data; Data leakage; Machine ability; One-time; Image recognition},
	correspondence_address = {Q. Lei; Guangzhou Institute of Advanced Technology, Chinese Academy of Sciences, Guangzhou, 511458, China; email: qj.lei@giat.ac.cn},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-073812604-3},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Comput. Commun. Syst., ICCCS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 6th IEEE International Conference on Computer and Communication Systems, ICCCS 2021; Conference date: 23 April 2021 through 26 April 2021; Conference code: 170923}
}

@CONFERENCE{Begum2022307,
	author = {Begum, M. Sharmila and Haris, R. and Vetrimaran, V. and Raj, P. Ashwin},
	title = {Prediction of Herbs with its Benefits using Deep Learning Techniques},
	year = {2022},
	journal = {International Conference on Sustainable Computing and Data Communication Systems, ICSCDS 2022 - Proceedings},
	pages = {307 – 310},
	doi = {10.1109/ICSCDS53736.2022.9760850},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130089787&doi=10.1109%2fICSCDS53736.2022.9760850&partnerID=40&md5=09fb064e705c362b6c7906740cb4e24e},
	affiliations = {B.Tech Computer Science and Engineering, Pmist, Tamil Nadu Vallam, Thanjavur, India},
	abstract = {Automated plant identification is a very promising solution for bridging the taxonomic gap, which is receiving much attention from botany and computer science. As machine learning technology advances, more complex models have been proposed to automate crop identification. Herbal remedies are considered in the pharmaceutical industry due to fewer harmful side effects and less expensive than modern medicine. Based on these data, many researchers have shown great interest in studying the recognition of natural herbal medicines. There are various possibilities for moving towards solid phase production capable of accurately discriminating medicinal plants in real time. In this project, efficient and reliable machine learning algorithms for plant catalogues using leaf images used in recent years are being studied. The review covers image processing techniques used to locate leaves and extract important leaf features from other machine learning steps. These deep learning stages are classified according to their function when it comes to discriminating leaf images based on common plant characteristics, i.e. shapes, ridges, textures and combinations of many elements. Then you get results using herbs with improved accuracy. The test results indicate that the proposed system provides an improved level of accuracy.  © 2022 IEEE.},
	author_keywords = {deep learning classifiers; deep neural networks; image identification; leaf; machine learning; medicinal plants; plant identification},
	keywords = {Deep neural networks; Image classification; Learning algorithms; Textures; Deep learning classifier; Image identification; Leaf; Leaf images; Learning classifiers; Learning techniques; Machine learning technology; Medicinal plants; Plant identification; Technology advances; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166547884-7},
	language = {English},
	abbrev_source_title = {Int. Conf. Sustain. Comput. Data Commun. Syst., ICSCDS - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 International Conference on Sustainable Computing and Data Communication Systems, ICSCDS 2022; Conference date: 7 April 2022 through 9 April 2022; Conference code: 179007}
}

@ARTICLE{Yang2021,
	author = {Yang, Chengzhuan},
	title = {Plant leaf recognition by integrating shape and texture features},
	year = {2021},
	journal = {Pattern Recognition},
	volume = {112},
	doi = {10.1016/j.patcog.2020.107809},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099478543&doi=10.1016%2fj.patcog.2020.107809&partnerID=40&md5=b0e8efbb8fe5fada80171a23a489b260},
	affiliations = {School of Mathematics and Computer Science, Zhejiang Normal University, No.688 Yingbin Road, Jinhua, 321004, Zhejiang Province, China},
	abstract = {Plant leaf identification is a significant challenge in the fields of computer vision and pattern recognition. This article presents a new approach to plant leaf identification, one that integrates shape and texture characteristics. First, we introduce the shape and texture features used by the proposed plant leaf recognition method. The proposed multiscale triangle descriptor (MTD) is employed to characterize the shape information of a plant leaf, and the local binary pattern histogram Fourier (LBP-HF) is used as the texture feature. Then, the shape and texture features of a leaf image are combined by weighted distance measurement, where L1 distance and chi-square distance are used for shape and texture features, respectively. The proposed approach provides a robust descriptor for the task of plant leaf recognition by combining the complementary MTD and LBP-HF features. The proposed approach has been thoroughly evaluated on three benchmark leaf datasets, including the Flavia, Swedish and MEW2012 leaf datasets. Our method achieves 77.6%, 85.7%, and 67.5% retrieval accuracy on the Flavia, Swedish and MEW2012 leaf datasets, respectively, while the corresponding classification accuracy is 99.1%, 98.4%, 95.6%. The recognition performance of our method is better or comparable to prior state-of-the-art plant leaf recognition method. © 2020},
	author_keywords = {Local binary pattern; Multiscale triangle descriptor; Plant leaf recognition; Shape descriptor; Texture feature},
	keywords = {Classification (of information); Textures; Chi Square distance; Classification accuracy; Local binary patterns; Retrieval accuracy; Shape and textures; Shape information; State of the art; Texture features; Pattern recognition},
	publisher = {Elsevier Ltd},
	issn = {00313203},
	coden = {PTNRA},
	language = {English},
	abbrev_source_title = {Pattern Recogn.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 41}
}

@CONFERENCE{Lin2022646,
	author = {Lin, DeShu and Cheng, CaiFeng},
	title = {Research on flower image recognition algorithm},
	year = {2022},
	journal = {Proceedings - 2022 International Conference on Big Data, Information and Computer Network, BDICN 2022},
	pages = {646 – 649},
	doi = {10.1109/BDICN55575.2022.00124},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129623546&doi=10.1109%2fBDICN55575.2022.00124&partnerID=40&md5=7c82e1ce13e4d23fd1dfcc0333df5472},
	affiliations = {Guangdong Songshan Polytechnic, School of Computer and Information Engineering, Shaoguan, China},
	abstract = {Flower recognition is one of the task to distinguish the species of flowers. Since the species of flowers with a great of variety, it is a hard task to distinguish the species by just using the manual features. Recently, with the development of deep learning technique, it can help us to discovery much more features hidden in the image and then help us to improve the precious of recognition. Thus, motived by the advantage machine learning technique on feature extraction and the depiction of handcraft features on image detail, we build our flower recognition model based on the features extracted from the following three types: feature extracted by using deep learning, dual-view features, and multi-modal features. © 2022 IEEE.},
	author_keywords = {attention mechanism; ensemble learning; feature fusion; flower recognition; image recognition},
	keywords = {Deep learning; Image enhancement; Learning algorithms; Attention mechanisms; Ensemble learning; Features extraction; Features fusions; Flower recognition; Hard task; Image details; Learning techniques; Machine learning techniques; Recognition algorithm; Image recognition},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166548476-3},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Big Data, Inf. Comput. Netw., BDICN},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2022 International Conference on Big Data, Information and Computer Network, BDICN 2022; Conference date: 20 January 2022 through 22 January 2022; Conference code: 178912}
}

@ARTICLE{Bahri2022239,
	author = {Bahri, Abdelkhalak and Bourass, Youssef and Badi, Imad and Zouaki, Hamid and El moutaouakil, Karim and Satori, Khalid},
	title = {Dynamic CNN Combination for Morocco Aromatic and Medicinal Plant Classification},
	year = {2022},
	journal = {International Journal of Computing and Digital Systems},
	volume = {11},
	number = {1},
	pages = {239 – 249},
	doi = {10.12785/ijcds/110120},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123542868&doi=10.12785%2fijcds%2f110120&partnerID=40&md5=5eae69d9158313e36f0420ff6c710081},
	affiliations = {Laboratoire des Sciences Appliquées (LSA), National School of Applied Sciences, UAE University, Al-Hoceima, Morocco; Laboratoire d'Informatique, Signaux, Automatique et Cognitivisme (LISAC), Faculty of science, USMBA University Fes, Morocco; Laboratory of Computer Science and Mathematics and their Applications (LIMA), Faculty of science, University Chouaib Doukkali, El Jadida, 24000, Morocco; Laboratoire des sciences de l’ingénieur, Polydisciplinary Faculty of Taza, USMBA University, Morocco},
	abstract = {In few last years, deep learning has itself set up as the new strategy for plant classification. Deep learning has a best performance for object recognition. In this paper, we have focused on the case of Morocco aromatic and medicinal plant (AMP) classification. Leaf is an important organ of plant, it has shown satisfying performances for plant classification and recognition In addition to leaves, we have used an others organs of AMP.ie. leaf veins and branches. We have proposed a new model combining dynamically the CNN classification result using the entropy impurity method. In the experiments, we have used VGG16, ResNet50 and Inception V3 CNN models. We have used Keras with Tensorflow backend to build and compile all neural network models. The experiments present that our model shown the higher classification accuracy. © 2022 University of Bahrain. All rights reserved.},
	author_keywords = {Classification; CNN; Combination; Deep learning; Plants},
	publisher = {University of Bahrain},
	issn = {2210142X},
	language = {English},
	abbrev_source_title = {Int. J. Comput. Digit. Syst.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Chavan2021350,
	author = {Chavan, Shruti and Ford, John and Yu, Xinrui and Saniie, Jafar},
	title = {Plant Species Image Recognition using Artificial Intelligence on Jetson Nano Computational Platform},
	year = {2021},
	journal = {IEEE International Conference on Electro Information Technology},
	volume = {2021-May},
	pages = {350 – 354},
	doi = {10.1109/EIT51626.2021.9491893},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111846434&doi=10.1109%2fEIT51626.2021.9491893&partnerID=40&md5=fe3d6374f278a0a337737e0a419d8e4c},
	affiliations = {Illinois Institute of Technology, Department of Electrical and Computer Engineering, Chicago, IL, United States},
	abstract = {The ongoing research for plant/animal species identification by computer vision engineers is exciting and vast. This paper describes a deep learning approach to identify plant species using image analysis. An efficient Artificial Intelligence System is designed and implemented with minimal components, including a camera and Jetson Nano (single-board embedded computing device). Convolutional Neural Networks are trained to capture the features from images and recognize the plant species. Thus, the experiment used, in particular, CNN architectures- AlexNet, ResNet50, and MobileNetv2, within Python's Tensorflow framework, to accomplish species identification. Of these, AlexNet provided the best results, with 72% validation accuracy after 15 epochs. A portion of the LeafSnap dataset, containing 15 plant species and 30 images per species, was used to compare the performance of architectures. © 2021 IEEE.},
	author_keywords = {convolutional neural network; Jetson Nano; LeafSnap dataset; plant identification},
	keywords = {Convolutional neural networks; Deep learning; Image recognition; Artificial intelligence systems; Computational platforms; Embedded computing devices; Learning approach; Plant species; Species identification; Network architecture},
	publisher = {IEEE Computer Society},
	issn = {21540357},
	isbn = {978-166541846-1},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Electro Inform. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2021 IEEE International Conference on Electro Information Technology, EIT 2021; Conference date: 14 May 2021 through 15 May 2021; Conference code: 170664}
}

@ARTICLE{Agushinta Rahayu2022148,
	author = {Agushinta Rahayu, Dewi and Hustinawaty and Jatnika, Ihsan and Lolita, Baby},
	title = {A Method of CNN Deep Learning for Indonesia Ornamental Plant Classification},
	year = {2022},
	journal = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
	volume = {436 LNICST},
	pages = {148 – 156},
	doi = {10.1007/978-3-031-01984-5_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130289855&doi=10.1007%2f978-3-031-01984-5_12&partnerID=40&md5=b2b23aa9f20423fe87798b197c15732b},
	affiliations = {Gunadarma University, Jl. Margonda Raya No. 100, Pondok Cina, West Java, Depok, 16424, Indonesia},
	abstract = {Indonesia is considered one of the most biodiverse regions in the world, with 670 mammal species, 1,604 birds, 787 reptiles, and 392 amphibian species as per the IUCN. Flower ornamental plants count among the potential commodities that can be developed both on a small and large scale, as evidenced by the increasing public interest in agribusiness. Many people are still not familiar with the existing types of flower ornamental plants. Several flower ornamental plants only grow or live in some parts of the area. Technological developments can help provide knowledge to the public. The technology currently being widely used is the Deep Learning technique. Deep Learning is a type of artificial neural network algorithm that uses metadata as input and processes it with many hidden layers, using the Convolutional Neural Network (CNN) method. The Convolutional Neural Network (CNN) method can classify objects, essentially images, and recognise them. This study will explore a CNN Deep Learning method that can classify various types of Indonesian ornamental plants object images. The results should pave the way for a prototype that can easily recognise Indonesian ornamental flowers in the future. © 2022, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.},
	author_keywords = {Convolutional Neural Network; Deep Learning; Ornamental plants},
	keywords = {Convolution; Deep learning; Developing countries; Image classification; Mammals; Multilayer neural networks; Amphibian species; Convolutional neural network; Deep learning; Indonesia; Large-scales; Mammal species; Neural network method; Ornamental plants; Plant classification; Small scale; Convolutional neural networks},
	correspondence_address = {D. Agushinta Rahayu; Gunadarma University, Depok, Jl. Margonda Raya No. 100, Pondok Cina, West Java, 16424, Indonesia; email: dewiar@staff.gunadarma.ac.id},
	editor = {Seyman M.N.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18678211},
	isbn = {978-303101983-8},
	language = {English},
	abbrev_source_title = {Lect. Notes Inst. Comput. Sci. Soc. Informatics Telecommun. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 1st International Congress of Electrical and Computer Engineering, ICECENG 2022; Conference date: 9 February 2022 through 12 February 2022; Conference code: 277759}
}

@CONFERENCE{Gracelin20221412,
	author = {Gracelin, S. and Raimond, Kumudha},
	title = {Deep Learning based Indigenous Herbal Medicinal Plants Recognition: A Comprehensive Review},
	year = {2022},
	journal = {Proceedings - 6th International Conference on Computing Methodologies and Communication, ICCMC 2022},
	pages = {1412 – 1418},
	doi = {10.1109/ICCMC53470.2022.9753825},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129199748&doi=10.1109%2fICCMC53470.2022.9753825&partnerID=40&md5=c1182b1cdcae806fc0ab8bc95be62349},
	affiliations = {Karunya Institute of Technology and Sciences, Dept. of Computer Science and Engineering, Tamilnadu, India},
	abstract = {In this modern medicinal world, numerous medicines with various chemical compositions have been discovered. Many individuals consume them these days because they are quick in healing. However, they have a variety of drawbacks, including the failure of internal organs, which may even lead to demise. As a result, some alternative treatment that is both effective and free of adverse effects is required. Medicinal herbs have been utilized to cure practically every disease since ancient times. It is quite effective, and it is also free of side effects. Nowadays, Machine Learning (ML) and Deep Learning (DL) algorithms are used frequently to solve many real-time problems. They predict results with much accuracy. In this paper, a systematic review is devised for identifying therapeutic plants using ML and DL models. The performance of various models as well as the features used by each model is compared to choose the best performing model. © 2022 IEEE.},
	author_keywords = {DL; Medicinal Herbs; ML; Plant recognition},
	keywords = {Plants (botany); Adverse effect; Chemical compositions; Deep learning; Internal organs; Machine-learning; Medicinal herb; Medicinal plants; Plant recognition; Real-time problems; Side effect; Deep learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166541028-1},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Comput. Methodol. Commun., ICCMC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 6th International Conference on Computing Methodologies and Communication, ICCMC 2022; Conference date: 29 March 2022 through 31 March 2022; Conference code: 178761}
}

@CONFERENCE{Anchitaalagammai2021,
	author = {Anchitaalagammai, J.V. and Revathy, Shantha Lakshmi J.S. and Kavitha, S. and Murali, S.},
	title = {Factors influencing the use of Deep Learning for Medicinal Plants Recognition},
	year = {2021},
	journal = {Journal of Physics: Conference Series},
	volume = {2089},
	number = {1},
	doi = {10.1088/1742-6596/2089/1/012055},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121553939&doi=10.1088%2f1742-6596%2f2089%2f1%2f012055&partnerID=40&md5=b8d40e89351accb171821cf8f22d0110},
	affiliations = {Department of Computer Science and Engineering, Velammal college of Engineering and Technology, Madurai, India},
	abstract = {Medicinal plants are very essential in maintaining the physical and mental health of human beings. For providing better treatment, Identification and classification of medicinal plants is essential. In this research paper, main objective is to create a medicinal plant identification system using Deep Learning concept. This system identifies and classifies the medicinal plant species with high accuracy. In this system, five different Indian medicinal plant species namely Pungai, Jamun (Naval), Jatropha curcas, kuppaimeni and Basil are used for identification and classification. The dataset contains 58,280 images, includes approximately 10,000 images for each species. The leaf texture, shape, color, physiological or morphological as the features set for leaf identification. The CNN architecture is used to train the collected dataset and develop the system with high accuracy. As result of this model, 96.67% success rate in finding the corresponding medicinal plant. This model is advisable to use as early detection tool for finding the medicinal plant because of its best success rate © 2021 Institute of Physics Publishing. All rights reserved.},
	author_keywords = {Medicinal Plant Identification; Neural Networks; —Deep Learning},
	keywords = {Deep neural networks; Textures; High-accuracy; Medicinal plant identification; Medicinal plants; Mental health; Neural-networks; Physical health; Plant identification; Plant recognition; Plant species; —deep learning; Plants (botany)},
	publisher = {IOP Publishing Ltd},
	issn = {17426588},
	language = {English},
	abbrev_source_title = {J. Phys. Conf. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 1st International Conference on Applied Mathematics, Modeling and Simulation in Engineering, AMSE 2021; Conference date: 15 September 2021 through 16 September 2021; Conference code: 175457; All Open Access, Bronze Open Access}
}

@ARTICLE{Gajjar202237882,
	author = {Gajjar, Viraj K. and Nambisan, Anand K. and Kosbar, Kurt L.},
	title = {Plant Identification in a Combined-Imbalanced Leaf Dataset},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {37882 – 37891},
	doi = {10.1109/ACCESS.2022.3165583},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128281376&doi=10.1109%2fACCESS.2022.3165583&partnerID=40&md5=1bcd394b0391776cab1da4b0e3757d65},
	affiliations = {Electrical and Computer Engineering Department, Missouri University of Science and Technology, Rolla, 65409, MO, United States},
	abstract = {Plant identification has applications in ethnopharmacology and agriculture. Since leaves are one of a distinguishable feature of a plant, they are routinely used for identification. Recent developments in deep learning have made it possible to accurately identify the majority of samples in five publicly available leaf datasets. However, each dataset captures the images in a highly controlled environment. This paper evaluates the performance of EfficientNet and several other convolutional neural network (CNN) architectures when applied to a combination of the LeafSnap, Middle European Woody Plants 2014, Flavia, Swedish, and Folio datasets. To normalize the impact of imbalance resulting from combining the original datasets, we used oversampling, undersampling, and transfer learning techniques to construct an end-to-end CNN classifier. We placed greater emphasis on metrics appropriate for a diverse-imbalanced dataset rather than stressing high performance on any one of the original datasets. A model from EfficientNet's family of CNN models achieved a highly accurate F-score of 0.9861 on the combined dataset.  © 2013 IEEE.},
	author_keywords = {convolutional neural networks; imbalanced dataset; Leaf dataset; plant identification; transfer learning},
	keywords = {Classification (of information); Convolution; Data transfer; Deep learning; Neural networks; Plants (botany); Convolutional neural network; Features extraction; Imbalanced dataset; Leaf dataset; Plant (biology); Plant biology; Plant identification; Support vectors machine; Training data; Transfer learning; Support vector machines},
	correspondence_address = {V.K. Gajjar; Electrical and Computer Engineering Department, Missouri University of Science and Technology, Rolla, 65409, United States; email: vgf4c@mst.edu},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Gupta2022138,
	author = {Gupta, Jaya and Pathak, Sunil and Kumar, Gireesh},
	title = {Bare Skin Image Classification using Convolution Neural Network},
	year = {2022},
	journal = {International Journal of Emerging Technology and Advanced Engineering},
	volume = {12},
	number = {1},
	pages = {138 – 145},
	doi = {10.46338/IJETAE0122_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124090035&doi=10.46338%2fIJETAE0122_13&partnerID=40&md5=6bb67886a46f7850f5b203b26d1c3408},
	affiliations = {Amity School of Engineering and Technology, Department of Computer Science & Engineering, Amity University Rajasthan, Jaipur, India; Institute of Engineering & Technology, Department of Computer Science & Engineering, JK Lakshmipat University Jaipur, Rajasthan, India},
	abstract = {Image classification is critical and significant research problems in computer vision applications such as facial expression classification, satellite image classification, and plant classification based on images. Here in the paper, the image classification model is applied for identifying the display of daunting pictures on the internet. The proposed model uses Convolution neural network to identify these images and filter them through different blocks of the network, so that it can be classified accurately. The model will work as an extension to the web browser and will work on all websites when activated. The extension will be blurring the images and deactivating the links on web pages. This means that it will scan the entire web page and find all the daunting images present on that page. Then we will blur those images before they are loaded and the children could see them. © 2022 IJETAE Publication House. All Rights Reserved.},
	author_keywords = {Activation function; CNN; Images classification; Optimizers; VGG-19},
	publisher = {IJETAE Publication House},
	issn = {22502459},
	language = {English},
	abbrev_source_title = {Int. J. Emerg. Technol. Adv. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; All Open Access, Bronze Open Access}
}

@ARTICLE{Berihu2022108,
	author = {Berihu, Mengisti and Fang, Juan and Lu, Shuaibing},
	title = {Automatic Classification of Medicinal Plants of Leaf Images Based on Convolutional Neural Network},
	year = {2022},
	journal = {Communications in Computer and Information Science},
	volume = {1496 CCIS},
	pages = {108 – 116},
	doi = {10.1007/978-981-16-9709-8_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124132836&doi=10.1007%2f978-981-16-9709-8_8&partnerID=40&md5=8b458539c5acfee2613a4abe00bb72c5},
	affiliations = {Faculty of Information Technology, Beijing University of Technology, No. 100 Pingleyuan Street, Beijing, 100124, China},
	abstract = {Plants are the basis of all living things on earth, supplying us with oxygen, food, shelter, medicine, and preserving the planet from dam-ages that could face climate changes. Concerning their medicinal abilities, limited access to proper medical centers in many rural areas and developing countries made traditional medicine preferable by the community. In addition, their lower side effect and affordability also plays a big role. More than half of the population uses medicinal plants directly and indirectly for animals and personal use in Ethiopia. However, accurate medicinal plant identification has always been a challenge for manual identification and automatic recognition systems mainly because the knowledge transfer between the knowledge holders (traditional physicians, elderly) and modern science have a huge gap. Several studies addressed an automatic plant recognition system using different feature extraction methods and classification algorithms. In this paper, a novel dataset, which was based on Ethiopian medicinal plants, that use the leaf part of the plant, as a medicine was used to automatically classify the plants accordingly using their leaf image. An attempt has been made to collect leaf images of medicinal plants in Ethiopia, to train, test collected dataset images, and classify those images using convolutional neural network models like GoogleNet and AlexNet. The proposed convolutional neural networks were fine-tuned with the adjustment of hyper-parameters like learning rate, the number of epochs, optimizers to the models. Image augmentation is also implemented to enlarge the dataset. The experimental result for the augmented dataset and more training epoch gave better performance and accuracy in the classification of the images. From the two selected convolutional neural network models, the best model is then determined based on the result in accuracy and loss; from an experiment conducted, the best model, which is GoogLeNet with an accuracy of 96.7 % chosen to develop a web-based automatic medicinal plant classification system. © 2022, Springer Nature Singapore Pte Ltd.},
	author_keywords = {AlexNet; Convolutional neural network; GoogleNet; Leaf images; Medicinal plants},
	keywords = {Classification (of information); Climate change; Convolution; Convolutional neural networks; Developing countries; Image classification; Knowledge management; Learning systems; Statistical tests; Alexnet; Automatic classification; Best model; Convolutional neural network; Ethiopia; Googlenet; Image-based; Leaf images; Medicinal plants; Neural network model; Plants (botany)},
	correspondence_address = {J. Fang; Faculty of Information Technology, Beijing University of Technology, Beijing, No. 100 Pingleyuan Street, 100124, China; email: fangjuan@bjut.edu.cn},
	editor = {Liao X. and Zhao W. and Chen E. and Xiao N. and Wang L. and Gao Y. and Shi Y. and Wang C. and Huang D.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18650929},
	isbn = {978-981169708-1},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 9th CCF Conference on Big Data, BigData 2021; Conference date: 8 January 2022 through 10 January 2022; Conference code: 271149}
}

@ARTICLE{Yu20223003,
	author = {Yu, Wanwan and Zhao, Ping and Xu, Kaijian and Zhao, Yuejiao and Shen, Pengju and Ma, Jingjing},
	title = {Evaluation of red-edge features for identifying subtropical tree species based on Sentinel-2 and Gaofen-6 time series},
	year = {2022},
	journal = {International Journal of Remote Sensing},
	volume = {43},
	number = {8},
	pages = {3003 – 3027},
	doi = {10.1080/01431161.2022.2079018},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131225878&doi=10.1080%2f01431161.2022.2079018&partnerID=40&md5=eab265a86f5568b10593275fb1bcb10c},
	affiliations = {School of Resources and Environmental Engineering, Hefei University of Technology, Hefei, China; Institute for Spatial Information Intelligence Analysis and Application, Hefei University of Technology, Hefei, China; Intelligent Interconnected Systems Laboratory of Anhui Province (Hefei University of Technology), Hefei, China},
	abstract = {Fast and accurate tree species mapping using remote sensing technology is invaluable to forest sustainable management. However, due to the limitation of ecosystem complexity and the frequent cloudy weather, it remains challenging to obtain detailed knowledge about the varieties in subtropical areas. In this study, 18 scenes of Gaofen-6 (GF6) and 13 scenes of Sentinel-2 (S2) images were harmonized using linear regression models, and the identification abilities of 23 red-edge vegetation index (REVI) time-series and the normalized difference vegetation index (NDVI) time-series were evaluated using the random forest classifier (RF). The feature reduction methods were employed to boost the effectiveness of the input features, and the optimal red-edge combination consisting of spectral and textural features was built to map the dominant tree species precisely in Southeast China. Our results showed as follows: (1) GF6 and S2 could be harmonized with a mean coefficient of determination (R2) greater than 0.94 and the root means squared error (RMSE) less than 0.02, which was helpful to improve the consistency between multi-source remote sensing data and subsequently derived products. (2) Most of the REVI time-series performed better than NDVI time-series, and the transformed chlorophyll absorption ratio indexes (TCARI2) got the highest overall accuracy of 86.28%, which could be further improved to 88.37% by adding 26 textural features. (3) The phenological characteristics from different seasons were conducive to tree species classification compared to the effect of different sensors, and the contribution of spring images played the most important role in RF, followed by autumn and winter, while the mid-summer images seemed to be negligible in the classifier. Overall, this study proposed an accurate and cost-efficient approach for tree species identification, which demonstrated good potential in providing a profound reference for subtropical forest landscape studying. © 2022 Informa UK Limited, trading as Taylor & Francis Group.},
	author_keywords = {GF-6; red-edge vegetation index; Sentinel-2; time series; tree species identification},
	keywords = {China; Decision trees; Forestry; Image processing; Regression analysis; Remote sensing; Tropics; Vegetation; GF-6; Normalized difference vegetation index time series; Random forest classifier; Red edge; Red-edge vegetation index; Sentinel-2; Times series; Tree species; Tree species identifications; Vegetation index; ecosystem function; forest management; mapping method; NDVI; remote sensing; Sentinel; sustainable development; time series analysis; Time series},
	correspondence_address = {K. Xu; School of Resources and Environmental Engineering, Hefei University of Technology, Institute for Spatial Information IntelligenceAnalysis and Application, Hefei University of Technology, Intelligent Interconnected Systems Laboratory ofAnhui Province, Hefei, China; email: kaijianxu@hfut.edu.cn},
	publisher = {Taylor and Francis Ltd.},
	issn = {01431161},
	coden = {IJSED},
	language = {English},
	abbrev_source_title = {Int. J. Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Alsaedi2022182,
	author = {Alsaedi, Najla and Alahmadi, Hanan and Syed, Liyakathunisa},
	title = {Deep Learning Technique for Dessert Plant Classification and Recognition},
	year = {2022},
	journal = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
	volume = {438 LNICST},
	pages = {182 – 194},
	doi = {10.1007/978-3-031-04409-0_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131139479&doi=10.1007%2f978-3-031-04409-0_17&partnerID=40&md5=673fd2d1cfe7d8b6606bf45eb1c315c2},
	affiliations = {Madinah, Saudi Arabia},
	abstract = {Recognition of desert plants has been a difficult activity for both human and computers due to similarities between these plants. In this paper, we propose an approach for recognizing desert plants by images of the bark. This approach depends on deep learning techniques for image recognition. The recognition process depends on texture of the bark. Therefore, we use Prewitt edge detection and Hough transform to detect the bark from original image. Further, we build a bark dataset for desert plants; this dataset consists of 1660 bark images for five species of desert plants. Each species in the dataset has 332 images. These species are Palm Dates, Mimosa Scabrella, Sidr, Lemon and Pomegranate. Convolutional Neural Network (CNN) is a deep learning technique that used in image classification tasks. Therefore, we test CNN on our dataset, and it gives an accuracy of 99.8%. Performance of CNN is very high, hence CNN can be adapted for recognition of desert plants. © 2022, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.},
	author_keywords = {Bark texture; Deep learning techniques; Prewitt edge detection},
	keywords = {Deep learning; Edge detection; Feature extraction; Hough transforms; Image recognition; Landforms; Plants (botany); Statistical tests; Textures; Bark texture; Classification and recognition; Convolutional neural network; Deep learning technique; Learning techniques; Original images; Plant classification; Plant recognition; Prewitt edge detection; Recognition process; Learning algorithms},
	correspondence_address = {N. Alsaedi; Madinah, Saudi Arabia; email: nalsaedi@taibahvalley.com.sa},
	editor = {Jiang X.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18678211},
	isbn = {978-303104408-3},
	language = {English},
	abbrev_source_title = {Lect. Notes Inst. Comput. Sci. Soc. Informatics Telecommun. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 6th EAI International Conference on Machine Learning and Intelligent Communications, MLICOM 2021; Conference date: 17 November 2021 through 18 November 2021; Conference code: 278069}
}

@CONFERENCE{Patil20211138,
	author = {Patil, Sumedh and Patra, Baba and Goyal, Neha and Gupta, Kapil},
	title = {Recognizing Plant species using Digitized leaves- A comparative study},
	year = {2021},
	journal = {Proceedings of the 5th International Conference on Trends in Electronics and Informatics, ICOEI 2021},
	pages = {1138 – 1143},
	doi = {10.1109/ICOEI51242.2021.9453003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113583166&doi=10.1109%2fICOEI51242.2021.9453003&partnerID=40&md5=9f663bf8e800fdadea77300b44fa6b9b},
	affiliations = {National Institute of Technology, Kurukshetra, India},
	abstract = {Plants play a crucial role in nature and the well-being of the population. They have a significant contribution towards ecological stability and are also sources of our needs like food, medicine, and essential commercial products. As a result of massive scale deforestation, topsoil erosion, and habitat destruction, both the number and type of plants' existing species are steadily declining. So, plantation and identification and classification of plant species are essential for preserving plant species and accelerated farm as it will help in the better understanding of plants. Nevertheless, they are difficult to exercise as plant identification needs domain knowledge and experience. However, due to advances in machine learning and deep learning, this problem is tackled correctly. Various machine Learning and Deep Learning algorithms like Support Vector Machine, Artificial Neural Network, Convolutional Neural Network, Probabilistic Neural Network have successfully experimented on plant leaf images to identify the species with near correct accuracy. This article attempts a comparative analysis of various approaches used for plant identification. Several experiments with Swedish leaves confirm the effectiveness of machine learning and CNN based classification model.  © 2021 IEEE.},
	author_keywords = {Computer vision; Convolutional neural network; Leaf classification},
	keywords = {Convolutional neural networks; Deep learning; Deforestation; Learning systems; Plants (botany); Support vector machines; Classification models; Commercial products; Comparative analysis; Comparative studies; Ecological stability; Habitat destruction; Plant identification; Probabilistic neural networks; Learning algorithms},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166541571-2},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Trends Electron. Informatics, ICOEI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 5th International Conference on Trends in Electronics and Informatics, ICOEI 2021; Conference date: 3 June 2021 through 5 June 2021; Conference code: 170931}
}

@CONFERENCE{Patil2022,
	author = {Patil, Akshata and Dan, Rama Bansidhar and Priya, N.},
	title = {Flower identification system using vision based technique},
	year = {2022},
	journal = {2022 2nd International Conference on Artificial Intelligence and Signal Processing, AISP 2022},
	doi = {10.1109/AISP53593.2022.9760663},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129673078&doi=10.1109%2fAISP53593.2022.9760663&partnerID=40&md5=6cf8d06a3aad2070c0bc1952e053f66b},
	affiliations = {Department of Information Science & Engineering, New Horizon College of Engineering, Bangalore, India},
	abstract = {Flower Species recognition has been a major field in image processing. Recognition fails many times the reason behind this is lack of knowledge about medicinal flower among the normal ones. Vision based technique has been used to create automated system which helps even common man to identify flowers around them. The main goal is to extract certain features from the input image by applying different techniques like machine learning and computer vision in order to classify image. In this paper, it is analyzed that flowers recognition has given success rate using image processing. © 2022 IEEE.},
	author_keywords = {Convolution Neural Network; Convolution Neural Tensor Flow; Feature extraction; Tensor Flow},
	keywords = {Automation; Convolution; Image processing; Automated systems; Convolution neural network; Convolution neural tensor flow; Features extraction; Flower recognition; Images processing; Input image; Species recognition; Tensor flow; Vision based; Tensors},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166544290-9},
	language = {English},
	abbrev_source_title = {Int. Conf. Artif. Intell. Signal Process., AISP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2nd International Conference on Artificial Intelligence and Signal Processing, AISP 2022; Conference date: 12 February 2022 through 14 February 2022; Conference code: 179006}
}

@ARTICLE{Kamyshova20228577,
	author = {Kamyshova, Galina and Osipov, Aleksey and Gataullin, Sergey and Korchagin, Sergey and Ignar, Stefan and Gataullin, Timur and Terekhova, Nadezhda and Suvorov, Stanislav},
	title = {Artificial Neural Networks and Computer Vision's-Based Phytoindication Systems for Variable Rate Irrigation Improving},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {8577 – 8589},
	doi = {10.1109/ACCESS.2022.3143524},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123295128&doi=10.1109%2fACCESS.2022.3143524&partnerID=40&md5=8e591c8faa792c35ea61bf318a731380},
	affiliations = {Department of Data Analysis and Machine Learning, Financial University, Government of the Russian Federation, Moscow, 125993, Russian Federation; Department of Hydraulic Engineering, Warsaw University of Life Sciences, Warsaw, 02-776, Poland; Department of Mathematical Methods in Economics and Management, State University of Management, Moscow, 109542, Russian Federation; Department of Mathematics, Mechanics, and Engineering, Saratov State Agrarian University, Saratov, 410012, Russian Federation; Department of Applied Informatics, Moscow Polytechnic University, Moscow, 107023, Russian Federation},
	abstract = {The article proposes a methodology for optimizing the process of irrigation of crops using a phytoindication system based on computer vision methods. We have proposed an algorithm and developed a system for obtaining a map of irrigation for maize in low latency mode. The system can be installed on a center pivot irrigation and consists of 8 IP cameras connected to a DVR connected to a laptop. The algorithm consists of three stages. Image preprocessing stage - applying an integrated excess green and excess red difference (ExGR) index. The classification stage is the application of the method that we choose depending on the system's operating conditions. At the final stage, a neural network trained using the Resilient Propagation method is used, which determines the rate of watering of plants in the current sector of the location of the sprinkler. The selected methods of pretreatment and classification made it possible to achieve an accuracy of plant identification up to 93%, growth stages - up to 92% (with unconsolidated maize sowing and good lighting). System performance up to 100 plants in one second, which exceeds the performance of similar systems. The neural network showed an accuracy of 92% on the training set and 87% on the test set. Dynamic analysis of spatial and temporal variability leads to an increase in productivity and efficiency of water use. In addition, given the ubiquitous distribution of agribusiness management systems, this approach is quite simple to implement in the farm's conditions. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/},
	author_keywords = {Agriculture; Crops; Indexes; Irrigation; Machine learning; Soil; Water resources},
	keywords = {Computer vision; Image classification; Machine learning; Sprinkler systems (irrigation); A-center; Center-pivot irrigation; Image preprocessing; Images classification; IP camera; Low latency; Neural computers; Neural-networks; Phytoindication; Variable rate irrigations; Neural networks},
	correspondence_address = {G. Kamyshova; Department of Data Analysis and Machine Learning, Financial University, Government of the Russian Federation, Moscow, 125993, Russian Federation; email: gnkamyshova@fa.ru},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access, Gold Open Access}
}

@ARTICLE{Xia2021,
	author = {Xia, Yixi and Yabuki, Nobuyoshi and Fukuda, Tomohiro},
	title = {Development of a system for assessing the quality of urban street-level greenery using street view images and deep learning},
	year = {2021},
	journal = {Urban Forestry and Urban Greening},
	volume = {59},
	doi = {10.1016/j.ufug.2021.126995},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100286930&doi=10.1016%2fj.ufug.2021.126995&partnerID=40&md5=98d3c89952682e60da36bcd73c462d5e},
	affiliations = {Division of Sustainable Energy and Environmental Engineering, Graduate School of Engineering, Osaka University, Japan},
	abstract = {Street greenery has long played a vital role in the quality of urban landscapes and is closely related to people's physical and mental health. Also, the level of street greenery is an important indicator of urban environmental quality. However, despite extensive research into environmental assessment methods for urban greenery, plant identification and greenery index calculations are still mostly done manually. In this research, we developed a method based on semantic segmentation processing of street view images to calculate the Green View Index of urban streets, and the Panoramic View Green View Index (PVGVI) is proposed for measuring the visible street-level greenery. We validated the results by comparison with those of manual inspection and the Pyramid Scene Parsing Network method, and found that the proposed method satisfied the accuracy requirements for vegetation detection. In addition, we conducted a case study of street-level greenery using the PVGVI and confirmed that this method can better visualize urban street-level greenery. The proposed method is scalable and automatable, and it contributes to the growing trend of integrating large freely available street view image datasets with semantic segmentation to inform urban planners. © 2021 Elsevier GmbH},
	author_keywords = {Deep learning; Green View Index (GVI); Image segmentation; Panoramic View Green View Index (PVGVI); Street view images; Urban green space},
	keywords = {Indicator indicator; detection method; environmental assessment; environmental quality; greenspace; image analysis; machine learning; urban area; urban development; vegetation cover},
	correspondence_address = {Y. Xia; Division of Sustainable Energy and Environmental Engineering, Graduate School of Engineering, Osaka University, Suita, 2-1 Yamadaoka, 565-0871, Japan; email: yixi@it.see.eng.osaka-u.ac.jp},
	publisher = {Elsevier GmbH},
	issn = {16188667},
	language = {English},
	abbrev_source_title = {Urban For. Urban Greening},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 45}
}

@ARTICLE{Suwais202292,
	author = {Suwais, Khaled and Alheeti, Khattab and Al_Dosary, Duaa},
	title = {A Review on Classification Methods for Plants Leaves Recognition},
	year = {2022},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {13},
	number = {2},
	pages = {92 – 100},
	doi = {10.14569/IJACSA.2022.0130211},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126095875&doi=10.14569%2fIJACSA.2022.0130211&partnerID=40&md5=9cf1d1fb68cecf3ea29b6fc0627d3453},
	affiliations = {Faculty of Computer Studies, Arab Open University (AOU), Riyadh, Saudi Arabia; College of Computer Sciences and Information Technology, University of Anbar, Anbar, Iraq},
	abstract = {Plants leaves recognition is an important scientific field that is concerned of recognizing leaves using image processing techniques. Several methods are presented using different algorithms to achieve the highest possible accuracy. This paper provides an analytical survey of various methods used in image processing for the recognition of plants through their leaves. These methods help in extracting useful information for botanists to utilize the medicinal properties of these leaves, or for any other agricultural and environmental purposes. We also provide insights and a complete review of different techniques used by researchers that consider different features and classifiers. These features and classifiers are studied in term of their capabilities in enhancing the accuracy ratios of the classification methods. Our analysis shows that both of the Support Victor Machines (SVM) and the Convolutional Neural Network (CNN) are positively dominant among other methods in term of accuracy © 2022, International Journal of Advanced Computer Science and Applications.All Rights Reserved.},
	author_keywords = {Classifiers; Feature extraction; Image processing; Leaf features; Leaf recognition},
	keywords = {Classification (of information); Convolutional neural networks; Image classification; Support vector machines; Analytical surveys; Classification methods; Features extraction; Image processing technique; Images processing; Leaf feature; Leaf recognition; Plant leaves; Property; Scientific fields; Plants (botany)},
	publisher = {Science and Information Organization},
	issn = {2158107X},
	language = {English},
	abbrev_source_title = {Intl. J. Adv.  Comput. Sci. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@CONFERENCE{Karyemsetty2022843,
	author = {Karyemsetty, Nagarjuna and Rudra, Poojitha and Yaswanth, Gollapudi and Nikhitha, Gannamaneni and Kodali, Navya Sri and Prasad, Chitturi},
	title = {A Machine Learning Approach to Classification of Okra},
	year = {2022},
	journal = {Proceedings - 4th International Conference on Smart Systems and Inventive Technology, ICSSIT 2022},
	pages = {843 – 847},
	doi = {10.1109/ICSSIT53264.2022.9716357},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127313408&doi=10.1109%2fICSSIT53264.2022.9716357&partnerID=40&md5=a4b703a2701439744bc1ad128edd9618},
	affiliations = {Department of CSE, Koneru Lakshmaiah Education Foundation, Vaddeswaram, AP, 522502, India},
	abstract = {Different machine learning techniques have been used for image classification purposes in agriculture. They can be applied to either roots, leaves or plants' detection and classification in order to assist farmer's tasks. This paper aims to propose alternatives or solutions to post-harvest manual classification of okras by Indian farmers in Okinawa. Thus, we implement Deep Learning to classify okras into categoriespreestablished by the Japan Agricultural Cooperatives. The classification features of okras in this study are their length and shape, and they classified into two: Class A and B. A set of pre-processing layers such as background noise cancellation, gray scaling and enhancement, image resizing and reconstruction are utilized to provide a higher detection rate. Moreover, a Convolutional Neural Network (CNN) is implemented to detect the patterns and predict the outputs. © 2022 IEEE},
	author_keywords = {Agricultural Cooperatives; classify products; noise cancellation; patterns; post-harvest},
	keywords = {Agriculture; Convolutional neural networks; Deep learning; Plants (botany); Spurious signal noise; Agricultural cooperatives; Classify product; Images classification; Machine learning approaches; Machine learning techniques; Noise cancellation; Pattern; Plant classification; Plant detections; Postharvest; Image enhancement},
	correspondence_address = {C. Prasad; Department of CSE, Koneru Lakshmaiah Education Foundation, Vaddeswaram, AP, 522502, India; email: chprasad4u@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166540118-0},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Smart Syst. Inventive Technol., ICSSIT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 4th International Conference on Smart Systems and Inventive Technology, ICSSIT 2022; Conference date: 20 January 2022 through 22 January 2022; Conference code: 177665}
}

@ARTICLE{Halstead2021,
	author = {Halstead, Michael and Ahmadi, Alireza and Smitt, Claus and Schmittmann, Oliver and McCool, Chris},
	title = {Crop Agnostic Monitoring Driven by Deep Learning},
	year = {2021},
	journal = {Frontiers in Plant Science},
	volume = {12},
	doi = {10.3389/fpls.2021.786702},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122268061&doi=10.3389%2ffpls.2021.786702&partnerID=40&md5=9c08aa294a3830deec041586c9fe4ecb},
	affiliations = {Agricultural Robotics, Institute of Agricultural Engineering, University of Bonn, Bonn, Germany},
	abstract = {Farmers require diverse and complex information to make agronomical decisions about crop management including intervention tasks. Generally, this information is gathered by farmers traversing their fields or glasshouses which is often a time consuming and potentially expensive process. In recent years, robotic platforms have gained significant traction due to advances in artificial intelligence. However, these platforms are usually tied to one setting (such as arable farmland), or algorithms are designed for a single platform. This creates a significant gap between available technology and farmer requirements. We propose a novel field agnostic monitoring technique that is able to operate on two different robots, in arable farmland or a glasshouse (horticultural setting). Instance segmentation forms the backbone of this approach from which object location and class, object area, and yield information can be obtained. In arable farmland, our segmentation network is able to estimate crop and weed at a species level and in a glasshouse we are able to estimate the sweet pepper and their ripeness. For yield information, we introduce a novel matching criterion that removes the pixel-wise constraints of previous versions. This approach is able to accurately estimate the number of fruit (sweet pepper) in a glasshouse with a normalized absolute error of 4.7% and an R2 of 0.901 with the visual ground truth. When applied to cluttered arable farmland scenes it improves on the prior approach by 50%. Finally, a qualitative analysis shows the validity of this agnostic monitoring algorithm by supplying decision enabling information to the farmer such as the impact of a low level weeding intervention scheme. Copyright © 2021 Halstead, Ahmadi, Smitt, Schmittmann and McCool.},
	author_keywords = {artificial intelligence; convolutional neural network; deep learning; field plant observation; image segmentation; plant classification},
	correspondence_address = {M. Halstead; Agricultural Robotics, Institute of Agricultural Engineering, University of Bonn, Bonn, Germany; email: michael.halstead@uni-bonn.de},
	publisher = {Frontiers Media S.A.},
	issn = {1664462X},
	language = {English},
	abbrev_source_title = {Front. Plant Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Oishi2021,
	author = {Oishi, Yu and Habaragamuwa, Harshana and Zhang, Yu and Sugiura, Ryo and Asano, Kenji and Akai, Kotaro and Shibata, Hiroyuki and Fujimoto, Taketo},
	title = {Automated abnormal potato plant detection system using deep learning models and portable video cameras},
	year = {2021},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	volume = {104},
	doi = {10.1016/j.jag.2021.102509},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119153032&doi=10.1016%2fj.jag.2021.102509&partnerID=40&md5=bcff0d3e3f9cbe2531c65126c6c44553},
	affiliations = {Core Technology Research Headquarters, National Agriculture and Food Research Organization, 1-31-1 Kannondai, Tsukuba, 305-0856, Ibaraki, Japan; Hokkaido Agricultural Research Center, National Agriculture and Food Research Organization, 9-4 Shinseiminami, Memuro, Kasai, 082-0081, Hokkaido, Japan; Tokachi Federation of Agricultural Cooperatives, Nokyoren Bldg. Nishi 3, Minami 7, 14, Obihiro, 080-0013, Hokaido, Japan; Institute for Plant Protection, National Agriculture and Food Research Organization, 2-1-18 Kannondai, Tsukuba, 305-8666, Ibaraki, Japan},
	abstract = {Potatoes are the world's most important root and tuber crop. A diseased seed potato can produce approximately 10 potato tubers, and the disease can propagate through the seed potato production cycle. To promote stable potato production, quality seed potatoes that are healthy and disease-free should be supplied. The Japanese government established a propagation system for the production and distribution of seed potatoes. Experienced laborers are required in the fields for visual inspection and removal of abnormal plants during seed potato production. To aid visual detection, reduce labor effort, and improve assessment time, we developed an automated abnormal potato plant detection system that utilizes a portable video camera and deep learning models. The proposed system detects abnormal plants or leaves considering the stage of growth. It detects three cases: (i) abnormal potato plants in the early growth stage, (ii) abnormal potato plants in comparison to the surrounding plants, and (iii) abnormal potato leaves. For the abnormal and healthy potato plant classification, the accuracy was ~90%, and the average precision (AP) for detection was 78.2%. Furthermore, the classification accuracy of the abnormal and healthy potato leaf classification was 96.7%, and the AP for detection was 90.5%. Therefore, the proposed system can be used to detect abnormal potato plants. © 2021 The Authors},
	author_keywords = {Deep learning; Disease diagnosis; Image classification; Object detection; Portable camera},
	keywords = {detection method; image classification; leaf; modeling; potato},
	correspondence_address = {Y. Oishi; Core Technology Research Headquarters, National Agriculture and Food Research Organization, Tsukuba, 1-31-1 Kannondai, 305-0856, Japan; email: oishi.yu@affrc.go.jp},
	publisher = {Elsevier B.V.},
	issn = {15698432},
	language = {English},
	abbrev_source_title = {Int. J. Appl. Earth Obs. Geoinformation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@ARTICLE{Huang2021,
	author = {Huang, Sheng-Feng and Shih, Wei-Li and Chen, Yi-Yi and Wu, Yi-Min and Chen, Lin-Chi},
	title = {Ion composition profiling and pattern recognition of vegetable sap using a solid-contact ion-selective electrode array},
	year = {2021},
	journal = {Biosensors and Bioelectronics: X},
	volume = {9},
	doi = {10.1016/j.biosx.2021.100088},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119473867&doi=10.1016%2fj.biosx.2021.100088&partnerID=40&md5=c4f78783461f2b8967aeed1b218cda28},
	affiliations = {Department of Biomechatronics Engineering, National Taiwan University, Taipei, 10617, Taiwan},
	abstract = {This is the first study to develop an effective tool for plant sap analysis based on a solid-contact ion-selective electrode (SCISE) array, which has the advantages of on-site, direct and fast analysis. SCISEs are all-solid-state ion-selective electrodes with a conducting polymer for ion-to-electron transduction. With the conducting polymer solid-contact, the electrodes perform high stability and short response time (<30 s) during potentiometric sensing. The developed SCISE array consists of potassium (SEN = 51.9 mV/decade), sodium (64.2 mV/decade), ammonium (59.3 mV/decade), calcium (32.1 mV/decade), and magnesium (33.0 mV/decade) selective electrodes. To verify the application, seven types of fresh crude vegetable leaf juices (ion concentration range: 10−3–10−4 M) were measured with the array, and the result was compared with ion chromatography. It was found that the array was able to obtain the unique, distinguishable ion composition profile as a radar plot of each vegetable sap, implying the fingerprint application of the present technology. Furthermore, applying principle component analysis (PCA) and K-means clustering, lettuces grown in different environments (deficiency either in potassium or in nitrate) are able to be discriminated. In summary, we demonstrate a tool for on-site, high-throughput and direct plant sap analysis based on SCISE array. Moreover, it could combine with pattern recognition and become a promising tools which could provide a diagnosis of the nutritive status of vegetables. © 2021 The Authors},
	author_keywords = {Pattern recognition; Sap analysis; Solid-contact ion-selective electrode},
	keywords = {Ion chromatography; Ion selective electrodes; Ions; K-means clustering; Lettuce; Nearest neighbor search; Pattern recognition; Principal component analysis; Vegetables; ammonia; calcium; ion; magnesium; nitrate; polymer; potassium; sodium; Direct analysis; Effective tool; Ion composition; Ion-selective electrode; Ion-selective electrodes array; On-site analysis; Plant sap; Sap analyse; Solid contacts; Solid-contact ion-selective electrode; analytic method; Article; chemical composition; concentration (parameter); controlled study; electron; high throughput analysis; ion chromatography; k means clustering; lettuce; nonhuman; nutritional status; pattern recognition; plant growth; plant identification; principal component analysis; reaction time; signal transduction; solid state; vegetable; vegetable juice; voltammetry; Conducting polymers},
	correspondence_address = {L.-C. Chen; Department of Biomechatronics Engineering, National Taiwan University, Taipei, 10617, Taiwan; email: chenlinchi@ntu.edu.tw},
	publisher = {Elsevier Ltd},
	issn = {25901370},
	language = {English},
	abbrev_source_title = {Biosens. Bioelectron.  X},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Lu2021,
	author = {Lu, Jianqiang and Lin, Weize and Chen, Pingfu and Lan, Yubin and Deng, Xiaoling and Niu, Hongyu and Mo, Jiawei and Li, Jiaxing and Luo, Shengfu},
	title = {Research on lightweight citrus flowering rate statistical model combined with anchor frame clustering optimization},
	year = {2021},
	journal = {Sensors},
	volume = {21},
	number = {23},
	doi = {10.3390/s21237929},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120008087&doi=10.3390%2fs21237929&partnerID=40&md5=622629e7b9217557aab475111768e998},
	affiliations = {School College of Electronic Engineering and School College of Artificial Intelligence, South China Agricultural University, Guangzhou, 510642, China; National International Joint Research Center of Precision Agriculture Aviation Application Technology, Guangzhou, 510642, China; Lingnan Modern Agriculture Guangdong Laboratory, Guangzhou, 510642, China},
	abstract = {At present, learning-based citrus blossom recognition models based on deep learning are highly complicated and have a large number of parameters. In order to estimate citrus flower quantities in natural orchards, this study proposes a lightweight citrus flower recognition model based on improved YOLOv4. In order to compress the backbone network, we utilize MobileNetv3 as a feature extractor, combined with deep separable convolution for further acceleration. The Cutout data enhancement method is also introduced to simulate citrus in nature for data enhancement. The test results show that the improved model has an mAP of 84.84%, 22% smaller than that of YOLOv4, and approximately two times faster. Compared with the Faster R-CNN, the improved citrus flower rate statistical model proposed in this study has the advantages of less memory usage and fast detection speed under the premise of ensuring a certain accuracy. Therefore, our solution can be used as a reference for the edge detection of citrus flowering. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Citrus flowering rate; Deep learning; Edge computing; Light weight; YOLOv4},
	keywords = {Algorithms; Citrus; Cluster Analysis; Models, Statistical; Neural Networks, Computer; Deep learning; Edge detection; Citrus flowering rate; Clustering optimizations; Data enhancement; Deep learning; Edge computing; Light weight; Model-based OPC; Recognition models; Statistic modeling; YOLOv4; algorithm; Citrus; cluster analysis; statistical model; Edge computing},
	correspondence_address = {X. Deng; School College of Electronic Engineering and School College of Artificial Intelligence, South China Agricultural University, Guangzhou, 510642, China; email: dengxl@scau.edu.cn},
	publisher = {MDPI},
	issn = {14248220},
	pmid = {34883932},
	language = {English},
	abbrev_source_title = {Sensors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Leonida2022488,
	author = {Leonida, Althani Miguel G. and Caballero, Arlene R.},
	title = {Aleaf: An Android-Based Phytotherapy Leaf Recognition Using Custom Vision Machine Learning},
	year = {2022},
	journal = {ICBIR 2022 - 2022 7th International Conference on Business and Industrial Research, Proceedings},
	pages = {488 – 493},
	doi = {10.1109/ICBIR54589.2022.9786509},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133136428&doi=10.1109%2fICBIR54589.2022.9786509&partnerID=40&md5=db8f85cb50fc2a35bc62437e0c66641a},
	affiliations = {Lyceum of the Philippines University, College of Technology, Manila, Philippines},
	abstract = {Phytotherapy is the area of medicine which deals with the practice of medicinal plants as remedies for illnesses or as therapeutic agents. Due to its budget-friendliness, high accessibility, and long account of effectivity, plant-based therapy as complementary and alternative medicine (CAM) is widespread in the first world while remains the primary health care for the third world. The more traditional use of plant-based therapy for medicinal purposes is to preserve the original properties of the plant; vitiated components are minimum. Even with its popularity, information regarding phytotherapy and the benefits of herbs and plants is nil. Even with its utilization among people, especially with the guidance of the elderlies, the knowledge is still lacking. This may be due to the fact that the practice of phytotherapy is usually based on experience and information is passed down through verbal transmission. The main objective of the study is to implement an Android application that acts as a leaf identification system, capable of detecting whether a plant has medicinal properties or not through the captured leaf image and its patterns. It displays their therapeutic benefits to the human body, its preparation, administration, dosage and frequency and duration of usage.  © 2022 IEEE.},
	author_keywords = {Custom Vision; Leaf Recognition; Machine Learning; Phytotherapy; Transfer Learning Algorithm},
	keywords = {Android (operating system); Budget control; Learning algorithms; Medicine; Plants (botany); Custom vision; Leaf recognition; Machine-learning; Medicinal plants; Phytotherapy; Property; Therapeutic agents; Transfer learning; Transfer learning algorithm; Vision machine; Machine learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549474-8},
	language = {English},
	abbrev_source_title = {ICBIR - Int. Conf. Bus. Ind. Res., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 7th International Conference on Business and Industrial Research, ICBIR 2022; Conference date: 19 May 2022 through 20 May 2022; Conference code: 179887}
}

@ARTICLE{Rajesh202211,
	author = {Rajesh, K.V.N. and Bhaskari, D. Lalitha},
	title = {Multi-class classification using convolution neural networks for plant leaf recognition of Ayurvedic plants},
	year = {2022},
	journal = {International Journal of Computational Science and Engineering},
	volume = {25},
	number = {1},
	pages = {11 – 21},
	doi = {10.1504/IJCSE.2022.120790},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125110191&doi=10.1504%2fIJCSE.2022.120790&partnerID=40&md5=0702618af6cbc08c65bfbe38e9342da8},
	affiliations = {Department of Computer Science and Systems Engineering, Andhra University, College of Engineering (Autonomous), Andhra University, Visakhapatnam, 530003, India},
	abstract = {Ayurveda is the traditional medicine system of India. The ingredients from which Ayurvedic medicines are made are mostly herbal and mineral in nature. Also, there are many herbal home remedies in India for general ailments. This knowledge has been passed down from generation to generation in large joint families. This knowledge is slowly fading away in the current generation of nuclear families. The current generation is unable to identify even locally available plants. The authors have come up with the idea of using convolution neural networks for solving this problem. In this solution, the images of leaves are used to identify the plant. This problem is a case of multi-class classification. A leaf image database is created and a neural network model is built using convolutional neural network (CNN). Keras deep learning framework with tensorflow as backend, is used for this purpose. The work presented in this paper is a part of larger research work in this area. This paper explains the developed CNN model and presents the results corresponding to six Ayurvedic leaves commonly available in and around the City of Visakhapatnam in the State of Andhra Pradesh. © 2022 Inderscience Enterprises Ltd.},
	author_keywords = {CNNs; Convolution neural networks; Leaf feature extraction; Multi-class classification; Plant leaf recognition; PLR},
	keywords = {Classification (of information); Classifiers; Convolutional neural networks; Deep learning; Plants (botany); CNN; Convolution neural network; Current generation; Features extraction; Leaf feature extraction; Leaf recognition; Neural network model; Plant leaf recognition; Plant leaves; PLR; Convolution},
	correspondence_address = {K.V.N. Rajesh; Department of Computer Science and Systems Engineering, Andhra University, College of Engineering (Autonomous), Andhra University, Visakhapatnam, 530003, India; email: kvn.rajesh@gmail.com},
	publisher = {Inderscience Publishers},
	issn = {17427185},
	language = {English},
	abbrev_source_title = {Int. J. Comput. Sci. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Bronze Open Access}
}

@ARTICLE{Bojamma2021989,
	author = {Bojamma, A.M. and Shastry, Chandrasekar},
	title = {A study on the machine learning techniques for automated plant species identification: current trends and challenges},
	year = {2021},
	journal = {International Journal of Information Technology (Singapore)},
	volume = {13},
	number = {3},
	pages = {989 – 995},
	doi = {10.1007/s41870-019-00379-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091739824&doi=10.1007%2fs41870-019-00379-7&partnerID=40&md5=350a64ccf0f030cda96feb30dabb857a},
	affiliations = {Department of Computer Science, Jain University, Atria Campus, Ambedkar Veedhi, Bangalore, India},
	abstract = {Plant identification plays a crucial role in sustaining the balance of the environment and protecting the biodiversity of a region. Recognizing different species of plants using conventional methods for conservation purposes is a tedious task. Today there is a cumulative effort made by computer scientists and botanists to automate the entire process of plant identification with leaf being a key feature for distinguishing different species of plants. With the advancement and utilization of relevant technologies like digital cameras, mobile cameras, newer techniques in image processing, pattern recognition, machine learning, automation of this system has been a reality. In this paper we have reviewed the current status of research on computer vision methodologies for taxonomical identification of plants and have also focused on the research challenges such as the diversity of the taxa to be identified, morphological variation in plants belonging to the same species, smaller interspecies variations, the challenges in acquisition of high quality images and standard datasets. The future trends in use of new technologies, creation of standard databases and interdisciplinary aspect of research is also discussed. © 2019, Bharati Vidyapeeth's Institute of Computer Applications and Management.},
	author_keywords = {Automated plant species identification; Image processing},
	correspondence_address = {A.M. Bojamma; Department of Computer Science, Jain University, Bangalore, Atria Campus, Ambedkar Veedhi, India; email: bojamma@sjc.ac.in},
	publisher = {Springer Science and Business Media B.V.},
	issn = {25112104},
	language = {English},
	abbrev_source_title = {Int. J. Inf. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Kalyoncu2022733,
	author = {Kalyoncu, Cem},
	title = {Sorted Uniform Local Binary Patterns},
	year = {2022},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {829 LNEE},
	pages = {733 – 739},
	doi = {10.1007/978-981-16-8129-5_112},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125225215&doi=10.1007%2f978-981-16-8129-5_112&partnerID=40&md5=e722724cbb0d507fee12ae2db8cb2882},
	affiliations = {European University of Lefke, Lefke, 10, Turkey},
	abstract = {Sorted Uniform LBP (SULBP) is a rotation invariant Local Binary Patterns (LBP) variant that is proposed for leaf image identification. This method is simple and effective, and is shown to outperform other LBP variants in leaf recognition. However, comparative analysis of this method in terms of texture classification is limited. In this paper, we present our extensive comparative analysis of SULBP. Additionally, we have explored different parameters of this system and their effect on the performance. Finally, we have used multiple datasets and classifiers to show the validity of these experiments. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Computer vision; LBP; Local binary patterns; Texture classification; Uniform local binary patterns},
	keywords = {Classification (of information); Textures; Comparative analyzes; Image identification; Leaf images; Leaf recognition; Local binary patterns; Rotation invariant; Simple++; Texture classification; Uniform local binary patterns; Computer vision},
	correspondence_address = {C. Kalyoncu; European University of Lefke, Lefke, 10, Turkey; email: ckalyoncu@eul.edu.tr},
	editor = {Mahyuddin N.M. and Mat Noor N.R. and Mat Sakim H.A.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981168128-8},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 11th International Conference on Robotics, Vision, Signal Processing and Power Applications, RoViSP 2021; Conference date: 5 April 2021 through 6 April 2021; Conference code: 272139}
}

@CONFERENCE{Pushpa2021315,
	author = {Pushpa, B.R. and Athira, P.R.},
	title = {Plant species recognition based on texture and geometric features of leaf},
	year = {2021},
	journal = {2021 3rd International Conference on Signal Processing and Communication, ICPSC 2021},
	pages = {315 – 320},
	doi = {10.1109/ICSPC51351.2021.9451683},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112858220&doi=10.1109%2fICSPC51351.2021.9451683&partnerID=40&md5=2acdcc3ddcbf7e0ef49871bb0bf6193d},
	affiliations = {Amrita Vishwa Vidyapeetham, Amrita School of Arts and Sciences, Department of Computer Science, India},
	abstract = {This paper proposed a methodology for classification and feature extraction of plant leaves.based on texture and geometric features. In India, a wide variety of medicinal plants are available, manual identification of the leaf is very problematic as it consume more time. Identifying a plant based on leaf features is a challenging task. For plant recognition most significant features such as geometry, shape, texture, color and vein patterns of leaf are utilized. In the first stage of proposed work leaf images are preprocessed to remove noise and to enhance the image that suits for further procedure. Texture features and geometrical features of leaves are computed. Finally, the extracted features are fed to the Multiple Support Vector Machine (MSVM) classifier. The experiments are carried out on a self-built database that contains 1800 images belonging to twenty different plant species. The selected texture and geometric features achieved an accuracy of 96%. © 2021 IEEE.},
	author_keywords = {GLCM; Leaf classification; Morphological Features; MSVM; Zernike Moments},
	keywords = {Geometry; Image enhancement; Support vector machines; Textures; Geometric feature; Geometrical features; Manual identification; Medicinal plants; Plant recognition; Plant species; REmove noise; Texture features; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166542864-4},
	language = {English},
	abbrev_source_title = {Int. Conf. Signal Process. Commun., ICPSC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 3rd International Conference on Signal Processing and Communication, ICPSC 2021; Conference date: 13 May 2021 through 14 May 2021; Conference code: 170853}
}

@CONFERENCE{Ghapar2021,
	author = {Ghapar, Hafizza Abdul and Khairuddin, Uswah and Yusof, Rubiyah and Khairuddin, Anis Salwa Mohd and Ahmad, Azlin},
	title = {New Feature Extraction for Wood Species Recognition System via Statistical Properties of Line Distribution},
	year = {2021},
	journal = {3rd International Conference on Electrical, Communication and Computer Engineering, ICECCE 2021},
	doi = {10.1109/ICECCE52056.2021.9514115},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115077681&doi=10.1109%2fICECCE52056.2021.9514115&partnerID=40&md5=5c979a73366ebc1ff08b67a41aa20ec8},
	affiliations = {Centre of Artificial Intelligence and Robotics (CAIRO), Malaysia-Japan International Institute of Technology (MJIIT), Universiti Teknologi Malaysia, Kuala Lumpur, 53100, Malaysia; Universiti Malaya, Department of Electrical Engineering, Kuala Lumpur, 50603, Malaysia; Universiti Teknologi Mara, Centre of Information Systems Studies, Faculty of Computer and Mathematical Sciences, Shah Alam Selangor, 40450, Malaysia},
	abstract = {A key to wood identification is the distinguishable features found on the cross-sectional surface of each tree species. The surface pattern on the wood cross-section may look very similar to non-experts. However, trained experts may identify wood species based on distinct and discriminant features of the pattern. An automatic wood recognition system based on machine vision to emulate the experts, the KenalKayu has been developed with high classification accuracy. Unfortunately, when more wood species were added into the system's database, the accuracy of the system reduced. It is important for the system to have a customized feature extractor solely for wood pattern such as the statistical properties of pores distribution (SPPD) which has been proven to increase the system's accuracy. As the wood surface pattern is not only defined by pores, but lines as well, this paper presented additional new feature extraction method based on statistical properties of line distribution (SPLD) to capture the discriminant line features of each species. When used alone as feature extractor, the SPLD managed to get 88% accuracy, and the number increases to 99.5% when combined with SPPD features and 100% when combined with both SPPD and Basic Grey Level Aura Matrix features. It shows that the SPLD is an essential customized feature extractor for wood identification purposes. © 2021 IEEE.},
	author_keywords = {Line Ditribution; Statistical Properties; Texture Features; Wood Recognition System; Wood Species},
	keywords = {Extraction; Feature extraction; Textile printing; Wood; Classification accuracy; Feature extraction methods; Feature extractor; Pores distribution; Species recognition systems; Statistical properties; Wood identification; Wood recognition; Population distribution},
	correspondence_address = {U. Khairuddin; Centre of Artificial Intelligence and Robotics (CAIRO), Malaysia-Japan International Institute of Technology (MJIIT), Universiti Teknologi Malaysia, Kuala Lumpur, 53100, Malaysia; email: uswah.kl@utm.my},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166543897-1},
	language = {English},
	abbrev_source_title = {Int. Conf. Electr., Commun. Comput. Eng., ICECCE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Conference on Electrical, Communication and Computer Engineering, ICECCE 2021; Conference date: 12 June 2021 through 13 June 2021; Conference code: 171467}
}

@ARTICLE{Vaghela2021204,
	author = {Vaghela, Himali Pradipkumar and Raja, Ramasamy Alagumalai Alagu},
	title = {Comparison of Support Vector Machine and k-Nearest Neighbor Classifiers for Tree Species Identification},
	year = {2021},
	journal = {Indian Journal of Radio and Space Physics},
	volume = {50},
	number = {4},
	pages = {204 – 210},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125227914&partnerID=40&md5=10f11e59a27c639ea18362cc63e5f014},
	affiliations = {Department of Electronics & Communication, Thiagarajar College of Engineering, Madurai, 625015, India},
	abstract = {Tree species are a group of organisms that have different characteristics, and the identification of tree species has become essential to protect biodiversity, ecosystem balancing, as well as for medicinal purposes. It takes too much time for experts to identify individual tree species, so research based on automatic tree species identification is required to save time. Remote sensing-based satellite Image Processing (IP) techniques are useful for detecting features of different tree species like shape, texture, color, etc., but IP techniques alone are not sufficient for automatic detection of tree species. So, some Machine Learning (ML) algorithms are useful for the identification of tree species. Here, Histogram of Oriented Gradient (HOG) features have been introduced to identify different tree species like, banana, coconut, lemon, mango, oil palm, papaya trees, and those which do not belong to any category, are recognized as a not specified class. So, the combination of IP and ML techniques are useful. Here, comparison of k-Nearest-Neighbor (kNN) and Support Vector Machine (SVM) classifiers with HOG has been introduced, and overall accuracy (OA) for kNN and SVM can be obtained 85.71% and 93.33% respectively. For further evaluation, K-fold cross-validation is used, and it gives 88.39% and 94.34% OA for kNN and SVM respectively © 2021,Indian Journal of Radio and Space Physics.All Rights Reserved},
	author_keywords = {Hog feature descriptor; K fold cross-validation; K-nearest-neighbor (knn); Support vector machine (svm); Tree species identification},
	correspondence_address = {H.P. Vaghela; Department of Electronics & Communication, Thiagarajar College of Engineering, Madurai, 625015, India; email: vaghela@student.tce.edu},
	publisher = {National Institute of Science Communication and Information Resources},
	issn = {03678393},
	language = {English},
	abbrev_source_title = {Indian J. Radio Space Phys.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Prakash2022205,
	author = {Prakash, Kolla Bhanu and Sreedevi, Ch. and Lanke, Pallavi and Vadla, Pradeep Kumar and Ranganayakulu, S.V. and Tripathi, Suman Lata},
	title = {Flower Detection Using Advanced Deep Learning Techniques},
	year = {2022},
	journal = {Lecture Notes in Networks and Systems},
	volume = {355},
	pages = {205 – 212},
	doi = {10.1007/978-981-16-8512-5_23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127080698&doi=10.1007%2f978-981-16-8512-5_23&partnerID=40&md5=a8d44b1c0cb6b5082d5fd8e774de5ce4},
	affiliations = {Department of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Andhra Pradesh, Vaddeswaram, Guntur, India; Department of Computer Science and Engineering, BV Raju Institute of Technology, Telangana, Narsapur, Medak, India; Guru Nanak Institutions Technical Campus (Autonomous), Telangana, Khanapur, India; School of Electronics and Electrical Engineering, Lovely Professional University, Punjab, Phagwara, India},
	abstract = {In nature, we have found different types of flower plants. It is difficult to identify and recognize which flower species it is. Since the recent growth of deep learning in computer vision, identification of objects is extended through various fields. In this paper we aim to detect the flowers on Oxford17 flower dataset. Due to the wide variety of flower species with varying colors, shapes, and sizes, as well as their surroundings with leaves, shrubs, and other objects, flower recognition is the most difficult task in the subject of object detection. We present a formal contract Yolo object detection model in this research for rapid and accurate detections. The proposed model is a novel single-step object detection method for differentiating flowers from a wide variety of species. This system performs both localization and object recognition in the image automatically. The flower region is automatically split to enable for the creation of the smallest bounding box feasible around it, and the items in the image are then marked. We use advanced measures throughout the training stage to improve classification stability, precision, and speed. We evaluated our method on Oxford17 dataset and Google images dataset. The experimental study results have shown better results and exceed 98% on the dataset which is effective than the others. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Computer vision; Flower detection; Object classification; Object detection; YOLO},
	correspondence_address = {K.B. Prakash; Department of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, Guntur, Andhra Pradesh, India; email: drkbp@kluniversity.in},
	editor = {Saini H.S. and Singh R.K. and Tariq Beg M. and Mulaveesala R. and Mahmood M.R.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {23673370},
	isbn = {978-981168511-8},
	language = {English},
	abbrev_source_title = {Lect. Notes Networks Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 9th International Conference on Innovations in Electronics and Communication Engineering, ICIECE 2021; Conference date: 13 August 2021 through 14 August 2021; Conference code: 274929}
}

@ARTICLE{Martins20215503,
	author = {Martins, Jefferson G. and Oliveira, Luiz E. S. and Weingaertner, Daniel and Barison, Andersson and Oliveira, Gerlon A. R. and Lião, Luciano M.},
	title = {A database for automatic classification of gender in Araucaria angustifolia plants},
	year = {2021},
	journal = {Soft Computing},
	volume = {25},
	number = {7},
	pages = {5503 – 5517},
	doi = {10.1007/s00500-020-05551-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099924587&doi=10.1007%2fs00500-020-05551-x&partnerID=40&md5=a1eeb8849b9d4c6abf325cb00ee6d6c6},
	affiliations = {Federal University of Technology - Paraná (UTFPR), Rua Cristo Rei, 19, Toledo, 85902-490, PR, Brazil; Federal University of Paraná (UFPR), Rua Cel. Francisco H. dos Santos, 100, Curitiba, 81531-990, PR, Brazil; Federal University of Goiás (UFG), Av. Esperança, s/n, Goiânia, 74690-900, GO, Brazil},
	abstract = {Forests have been disorderly exploited, and many species are considered endangered. Some initiatives have been taken in order to prevent forests from being destroyed. A good alternative would be to plan a spatial distribution of plants, with higher number of females than males. Determining the gender of seedlings would provide important information for a possible strategy. Another common problem that researchers in this field very often face, in order to perform their experiments, is the lack of a representative database. To overcome this difficulty, we introduce a new database in this work, which is composed of nuclear magnetic resonance of adult Araucaria angustifolia plants. In order to gain better insight into this database, we have tested different strategies and classifiers. A first set of experiments took three classifiers trained to discriminate males from females considering the original database. A second round of experiments applied the genetic algorithm technique to select subsets of attributes based on single-objective and two-objective functions. After analyzing the achieved results, we have also proposed a new strategy based on statistical measures for selecting subsets from the attributes. A comprehensive set of experiments has shown that the proposed selecting strategy has achieved better performances, with an accuracy of 80.3% (AUC = 79.4). We believe that researchers will find this database a useful tool in their work on determining the Araucaria angustifolia gender. On the other hand, the proposed selecting strategy would be useful for reducing the complexity of databases and accelerating the process of building classification models. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.},
	author_keywords = {Feature selection; Gender plant classification; Pattern recognition},
	keywords = {Database systems; Forestry; Genetic algorithms; Automatic classification; Classification models; Objective functions; Single objective; Statistical measures; Classification (of information)},
	correspondence_address = {J.G. Martins; Federal University of Technology - Paraná (UTFPR), Toledo, Rua Cristo Rei, 19, 85902-490, Brazil; email: martins@utfpr.edu.br},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {14327643},
	language = {English},
	abbrev_source_title = {Soft Comput.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Aminuddin2022266,
	author = {Aminuddin, Raihah and Maskan, Farizul Azlan and Abdul Jalil, Ummu Mardhiah and Ahmad Fesol, Siti Feirusz and Ibrahim, Shafaf},
	title = {Support Vector Machine-based approach for Recognizing Bonsai Species using Leaf Image},
	year = {2022},
	journal = {2022 IEEE 18th International Colloquium on Signal Processing and Applications, CSPA 2022 - Proceeding},
	pages = {266 – 271},
	doi = {10.1109/CSPA55076.2022.9781913},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132769605&doi=10.1109%2fCSPA55076.2022.9781913&partnerID=40&md5=a122b639a5947b6c95f45da803363b17},
	affiliations = {Faculty of Computer and Mathematical Sciences Universiti Teknologi MARA, Melaka, Malaysia; Protech Synergy Sdn. Bhd., Selangor, Malaysia},
	abstract = {Recognition of Bonsai plant is one of the most challenging task. This is because most of the people have less knowledge about Bonsai especially for a beginner. For those who new to this field, it might be hard for them to recognize and identify the species of Bonsai because of its similarity in terms of shape, colour and etc. The incorrect identification of species, may resulting in damaging the Bonsai plant. Furthermore, different species of Bonsai may have different ways to take care of it. Therefore, the information about the Bonsai need to be accessible with the recognition of the species. As a solution, the aims of this project is to develop a system for recognising three species of Bonsai: 1) Adenium, 2) Red Japanese Maple and 3) Natal Plum by using its leaf. The project implemented a Rapid Application Development (RAD) Model as the methodology. There are four phases in RAD: 1) Planning, 2) Design, 3) Implementation and 4) Finalization. In pre-processed phase, feature extraction of the leaf is using colour moment and Gray-Level Co-occurrence Matrix (GLCM) were used for extracting the colour of the leaf. The species of Bonsai has been classified using Support Vector Machine-based approach and the system has been successfully recognize the species of Bonsai with accuracy of 98.2%.  © 2022 IEEE.},
	author_keywords = {bonsai; leaf recognition; machine learning; support vector machine},
	keywords = {Color; Bonsai; Bonsai plants; Development model; Four-phase; Leaf images; Leaf recognition; Machine-learning; Phase features; Rapid application development; Support vectors machine; Support vector machines},
	correspondence_address = {U.M. Abdul Jalil; Faculty of Computer and Mathematical Sciences Universiti Teknologi MARA, Melaka, Malaysia; email: ummu.mardhiah.ajalil@uitm.edu.my},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166548529-6},
	language = {English},
	abbrev_source_title = {IEEE Int. Colloq. Signal Process. Appl., CSPA - Proceeding},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 18th IEEE International Colloquium on Signal Processing and Applications, CSPA 2022; Conference date: 12 May 2022; Conference code: 179572}
}

@ARTICLE{Ghosh2022133,
	author = {Ghosh, Sukanta and Singh, Amar},
	title = {The Analysis of Plants Image Classification Based on Machine Learning Approaches},
	year = {2022},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {841},
	pages = {133 – 148},
	doi = {10.1007/978-981-16-8774-7_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127663257&doi=10.1007%2f978-981-16-8774-7_12&partnerID=40&md5=44824cf9fa4e3e379582b53c1a2225c5},
	affiliations = {Lovely Professional University, Phagwara, India},
	abstract = {With the fast development in urbanization and populace, it has become a sincere errand to support and develop plants that are both significant in supporting the nature and the living creature’s needs. Moreover, there is a requirement for saving the plants having worldwide significance both financially and naturally. Finding such species from the backwoods or bushes having human contribution is a tedious and expensive undertaking to perform. Classification and identification of plants-leaf are useful for individuals to viably comprehend and ensure plants. The leaves of plants are the main acknowledgment organs. With the advancement of artificial intelligence and computer vision innovation, plant-leaf recognition dependent on plant-leaf image investigation is utilized to improve the information on plant classification and insurance. Deep learning is the condensing of deep neural network learning technique and has a place with neural organization structure. It can naturally take in highlights from huge information and utilize artificial neural network dependent on back propagation methods to prepare and order plant-leaf tests. There are numerous machine learning approaches for identification and classification of plant-leaf image. Some of the famous and effective approaches are Random Forest, Support Vector Machines, ResNet50, CNN, VGG16, VGG19, PNN, KNN, etc. In this paper, we are going to apply 9 machine learning techniques on Flavia plant-leaf image dataset. Flavia plant-leaf image dataset consists of 32 different species of plant. The images are first preprocessed and then their shape, color and texture-based features are extracted from the processed image. Initial these images are in the size of 256*256 pixels. These images were preprocessed and taken up to the size of 64*64 for fast processing. ResNet50 has given the best results with an accuracy of 98%. Though SVM and S-Inception have also provided a good accuracy. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Clustering; CNN; Image classification; Multivariate data; Random forest; SVM; Thresholding},
	keywords = {Backpropagation; Classification (of information); Decision trees; Deep neural networks; Image enhancement; Plants (botany); Support vector machines; Textures; Clusterings; CNN; Images classification; Machine learning approaches; Multivariate data; Plant leaf images; Plant leaves; Random forests; SVM; Thresholding; Image classification},
	correspondence_address = {S. Ghosh; Lovely Professional University, Phagwara, India; email: sukantaghoshmca@hotmail.com},
	editor = {Marriwala N. and Tripathi C.C. and Jain S. and Mathapathi S.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981168773-0},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: International Conference on Emergent Converging Technologies and Biomedical Systems, ETBS 2021; Conference date: 29 April 2021 through 30 April 2021; Conference code: 275949}
}

@ARTICLE{Azimi2021,
	author = {Azimi, Shiva and Gandhi, Tapan K.},
	title = {3-D maximum likelihood estimation sample consensus for correspondence grouping in 3-D plant point cloud},
	year = {2021},
	journal = {IEEE Sensors Letters},
	volume = {5},
	number = {6},
	doi = {10.1109/LSENS.2021.3075459},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105054568&doi=10.1109%2fLSENS.2021.3075459&partnerID=40&md5=8944a385e5aad522718348a0ed7fcc19},
	affiliations = {Department of Electrical Engineering, Indian Institute of Technology-Delhi, New Delhi, 110016, India},
	abstract = {Computer vision based plant phenomics can be used to monitor the health and the growth of plants. This letter presents the extension of 2-D maximum likelihood matching to 3-D maximum likelihood estimation sample consensus (MLEASAC) and provides a comparative evaluation of some popular 3-D correspondence grouping algorithms. We test these algorithms on 3-D point clouds of plants along with two standard benchmarks addressing shape retrieval and point cloud registration scenarios. The performance of the correspondence grouping algorithms is evaluated in terms of precision and recall. The results show that of all the evaluated algorithms, 3-D random sample consensus (RANSAC) and MLEASAC perform the best, with MLEASAC being slightly more efficient while being computationally less intense than RANSAC. © 2017 IEEE.},
	author_keywords = {3-D point cloud; computer vision; correspondence grouping (CG); plant phenotyping; Sensor signal processing},
	keywords = {Computer vision; Object recognition; Object tracking; Stereo image processing; Stereo vision; Three dimensional computer graphics; Comparative evaluations; Computer vision applications; Correspondence groupings; Plant recognition; Point cloud registration; Precision and recall; Random sample consensus; Stereo reconstruction; Maximum likelihood estimation},
	correspondence_address = {S. Azimi; Department of Electrical Engineering, Indian Institute of Technology-Delhi, New Delhi, 110016, India; email: shiva.azimi@yahoo.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {24751472},
	language = {English},
	abbrev_source_title = {IEEE Sensors Let.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Liu2021,
	author = {Liu, Maohua and Han, Ziwei and Chen, Yiming and Liu, Zhengjun and Han, Yanshun},
	title = {Tree species classification of LiDAR data based on 3D deep learning},
	year = {2021},
	journal = {Measurement: Journal of the International Measurement Confederation},
	volume = {177},
	doi = {10.1016/j.measurement.2021.109301},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103951003&doi=10.1016%2fj.measurement.2021.109301&partnerID=40&md5=a43aab70dbf895106eb12b189f847fbc},
	affiliations = {School of Transportation Engineering, Shenyang Jianzhu University, Shenyang, 110168, China; Chinese Academy of Surveying and Mapping, Beijing, 100089, China},
	abstract = {Accurate tree species identification is essential for ecological evaluation and other forest applications. In this paper, we proposed a point-based deep neural network called LayerNet. For light detection and ranging (LiDAR) data in forest regions, the network can divide multiple overlapping layers in Euclidean space to obtain the local three-dimensional (3D) structural features of the tree. The features of all layers are aggregated, and the global feature is obtained by convolution to classify the tree species. To validate the proposed framework, multiple experiments, including airborne and ground-based LiDAR datasets, are conducted and compared with several existing tree species classification algorithms. The test results show that LayerNet can directly use 3D data to accurately classify tree species, with the highest classification accuracy of 92.5%. Also, the results of comparative experiments demonstrate that the proposed framework has obvious advantages in classification accuracy and provides an effective solution for tree species classification tasks. © 2021 Elsevier Ltd},
	author_keywords = {3D deep learning; LiDAR; Point cloud; Tree species classification},
	keywords = {Deep neural networks; Forestry; Optical radar; 3d deep learning; Classification accuracy; Ecological evaluation; Light detection and ranging; Point-based; Point-clouds; Species classification; Tree species; Tree species classification; Tree species identifications; Classification (of information)},
	correspondence_address = {Y. Chen; Chinese Academy of Surveying and Mapping, Beijing, 100089, China; email: sjzulmh@163.com},
	publisher = {Elsevier B.V.},
	issn = {02632241},
	coden = {MSRMD},
	language = {English},
	abbrev_source_title = {Meas J Int Meas Confed},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18}
}

@CONFERENCE{Daliman2021,
	author = {Daliman, S. and Abdul Ghapar, N.},
	title = {Classification of Artocarpus species based on leaf recognition using multiclass support vector machine},
	year = {2021},
	journal = {IOP Conference Series: Earth and Environmental Science},
	volume = {842},
	number = {1},
	doi = {10.1088/1755-1315/842/1/012073},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114994900&doi=10.1088%2f1755-1315%2f842%2f1%2f012073&partnerID=40&md5=97880943a453339900c6e6187d55e971},
	affiliations = {Faculty of Earth Science, Universiti Malaysia Kelantan, Jeli, 17600, Malaysia},
	abstract = {The demand of automated tools has been increasing regarding to the lack of people that expert in taxonomist. The aim of this research is to identify the classification of Artocarpus species based on leaf recognition using multiclass Support Vector Machine. This study focusses on identification and classification of selected Artocarpus species which are A. heterophyllus, A. altilis, A. integer and A. odoratissimus that belong to genus Artocarpus and family Moraceae through their morphological and features extraction by using image processing method. Multiclass Support Vector Machine (SVM) will be used to get the highest accuracy for the classification of Artocarpus species. The combination of Prewitt algorithm, Canny algorithm and gray level co-occurrence matrix will be used in SVM. This study capable to provide the results for current accuracy data representation of the selected Artocarpus species. The development of Graphical User Interface (GUI) for classification of Artocarpus species help user to identify and differentiate the species in faster and easier way especially botanist, taxonomist, and researcher. This system can increase the accuracy and speed of the processing and extraction of features from digital images of leaves samples. A Graphical User Interface utilizes a combination of devices and technologies to give a platform where users can interact with and producing information. © Published under licence by IOP Publishing Ltd.},
	correspondence_address = {S. Daliman; Faculty of Earth Science, Universiti Malaysia Kelantan, Jeli, 17600, Malaysia; email: shaparas@umk.edu.my},
	publisher = {IOP Publishing Ltd},
	issn = {17551307},
	language = {English},
	abbrev_source_title = {IOP Conf. Ser. Earth Environ. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 3rd International Conference on Tropical Resources and Sustainable Sciences, CTReSS 2021; Conference date: 14 July 2021 through 15 July 2021; Conference code: 171582; All Open Access, Bronze Open Access}
}

@ARTICLE{Chompookham2021553,
	author = {Chompookham, Thipwimon and Surinta, Olarik},
	title = {Ensemble methods with deep convolutional neural networks for plant leaf recognition},
	year = {2021},
	journal = {ICIC Express Letters},
	volume = {15},
	number = {6},
	pages = {553 – 565},
	doi = {10.24507/icicel.15.06.553},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106385395&doi=10.24507%2ficicel.15.06.553&partnerID=40&md5=da9ab45526b76644792abebd82b7f5d6},
	affiliations = {Department of Information Technology, Faculty of Informatics, Mahasarakham University, Maha Sarakham, 44150, Thailand; Multi-Agent Intelligent Simulation Laboratory (MISL), Department of Information Technology, Faculty of Informatics, Mahasarakham University, Maha Sarakham, 44150, Thailand},
	abstract = {Recognition of plant leaves and diseases from images is a challenging task in computer vision and machine learning. This is because various problems directly affect the performance of the system, such as the leaf structure, differences of the intra-class, similarity of shape between inter-class, perspective of the image, and even recording time. In this paper, we propose the ensemble convolutional neural network (CNN) method to tackle these issues and improve plant leaf recognition performance. We trained five CNN models: MobileNetV1, MobileNetV2, NASNetMobile, DenseNet121, and Xception, accordingly to discover the best CNN based model. Ensemble methods, unweighted average, weighted average, and unweighted majority vote methods were then applied to the CNN output probabilities of each model. We have evaluated these ensemble CNN methods on a mulberry leaf dataset and two leaf disease datasets: tomato and corn leaf disease. As a result, the individual CNN model shows that MobileNetV2 outperforms every CNN model with an accuracy of 91.19% on the mulberry leaf dataset. The Xception combined with data augmentation techniques (Height Shift+Vertical Flip+Fill Mode) obtains an accuracy of 91.77%. We achieved very high accuracy above 99% from the DenseNet121 and Xception models on the leaf disease datasets. For the ensemble CNNs method, we selected the based models according to the best CNN models and predicted the output of each CNN with the weighted average ensemble method. The results showed that 3-Ensemble CNNs (3-EnsCNNs) performed better on plant leaf disease datasets, while 5-EnsCNNs outperforms on the mulberry leaf dataset. Surprisingly, the data augmentation technique did not affect the ensemble CNNs on the mulberry leaf and corn leaf disease datasets. On the other hand, application of data augmentation was slightly better than without only on the tomato leaf disease dataset. © 2021 ICIC International. All rights reserved.},
	author_keywords = {Convolutional neural network; Ensemble convolutional neural network; Ensemble learning method; Ensemble method; Plant leaf recognition},
	correspondence_address = {O. Surinta; Multi-Agent Intelligent Simulation Laboratory (MISL), Department of Information Technology, Faculty of Informatics, Mahasarakham University, Maha Sarakham, 44150, Thailand; email: olarik.s@msu.ac.th},
	publisher = {ICIC International},
	issn = {1881803X},
	language = {English},
	abbrev_source_title = {ICIC Express Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10}
}

@ARTICLE{Pärtel2021,
	author = {Pärtel, Jaak and Pärtel, Meelis and Wäldchen, Jana},
	title = {Plant image identification application demonstrates high accuracy in Northern Europe},
	year = {2021},
	journal = {AoB PLANTS},
	volume = {13},
	number = {4},
	doi = {10.1093/aobpla/plab050},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115002152&doi=10.1093%2faobpla%2fplab050&partnerID=40&md5=075d38c0aafd887d5184f8dad33244b8},
	affiliations = {Hugo Treffner Gymnasium, Munga 12, Tartu, 51007, Estonia; Institute of Ecology and Earth Sciences, University of Tartu, Lai 40, Tartu, 51005, Estonia; Max Planck Institute for Biogeochemistry, Jena, 07745, Germany},
	abstract = {Automated image-based plant identification has experienced rapid development and has been already used in research and nature management. However, there is a need for extensive studies on how accurately automatic plant identification works and which characteristics of observations and study species influence the results. We investigated the accuracy of the Flora Incognita application, a research-based tool for automated plant image identification. Our study was conducted in Estonia, Northern Europe. Photos originated from the Estonian national curated biodiversity observations database, originally without the intention to use them for automated identification (1496 photos, 542 species) were examined. Flora Incognita was also directly tested in field conditions in various habitats, taking images of plant organs as guided by the application (998 observations, 1703 photos, 280 species). Identification accuracy was compared among species characteristics: plant family, growth forms and life forms, habitat type and regional frequency. We also analysed image characteristics (plant organs, background, number of species in focus), and the number of training images that were available for particular species to develop the automated identification algorithm. From database images 79.6 % of species were correctly identified by Flora Incognita; in the field conditions species identification accuracy reached 85.3 %. Overall, the correct genus was found for 89 % and the correct plant family for 95 % of the species. Accuracy varied among different plant families, life forms and growth forms. Rare and common species and species from different habitats were identified with equal accuracy. Images with reproductive organs or with only the target species in focus were identified with greater success. The number of training images per species was positively correlated with the identification success. Even though a high accuracy has been already achieved for Flora Incognita, allowing its usage for research and practices, our results can guide further improvements of this application and automated plant identification in general.  © 2021 The Author(s) 2021.},
	author_keywords = {Artificial intelligence; Automated plant species identification; Citizen science; Convolutional neural networks; Deep learning; Estonian flora; Flora Incognita; Identification application; Plant identification},
	correspondence_address = {M. Pärtel; Hugo Treffner Gymnasium, Tartu, Munga 12, 51007, Estonia; email: meelis.partel@ut.ee},
	publisher = {Oxford University Press},
	issn = {20412851},
	language = {English},
	abbrev_source_title = {AoB Plants},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Gao2021,
	author = {Gao, Ronghua and Wang, Rong and Feng, Lu and Li, Qifeng and Wu, Huarui},
	title = {Dual-branch, efficient, channel attention-based crop disease identification},
	year = {2021},
	journal = {Computers and Electronics in Agriculture},
	volume = {190},
	doi = {10.1016/j.compag.2021.106410},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114809623&doi=10.1016%2fj.compag.2021.106410&partnerID=40&md5=f6e9c289189982904a0a16326e075a6f},
	affiliations = {Beijing Research Center for Information Technology in Agriculture, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; College of Information Engineering, Northwest Agriculture and Forestry University, Yangling, 712100, China; National Engineering Research Center for Information Technology in Agriculture, Beijing, 100097, China; Key Laboratory for Information Technologies in Agriculture, Ministry of Agriculture, Beijing, 100097, China},
	abstract = {Efficient and accurate recognition of crop diseases plays an important role in disease prevention. Aiming at the low accuracy of existing crop disease recognition methods, we improve the attention mechanism and residual neural network (ResNet) to propose a dual-branch, efficient, channel attention (DECA)-based crop disease recognition model. The DECA module uses a dual-branch 1D convolution operation to filter effective feature information. It also introduces an adaptive convolution kernel parameter kand an adaptive parameter αto participate in the reverse training of the model so that the model can independently select effective features and establish dependencies between channels. Then, the DECA module is added to the residual module to recalibrate the channel characteristics and improve the characteristic representability of the residual module. Finally, a DECA_ResNet model is proposed to realize crop disease recognition. The crop disease model based on improved attention is verified on the AI Challenger 2018 dataset, PlantVillage dataset, and self-collected cucumber disease dataset; the disease recognition accuracies are 86.35%, 99.74% and 98.54%, respectively, which are higher than those of the ResNet and squeeze-and-excitation network (SENet) before improvement. The recognition accuracy of the proposed model in the PlantVillage dataset is higher than that of existing models. The experimental results show that the proposed plant recognition model with 18 layers (DECA_ResNet18) can achieve a high recognition accuracy. The DECA module can independently select each branch feature without causing a significant increase in the number of parameters, which improves the plant disease recognition accuracy and reduces the extraction of redundant features. © 2021 Elsevier B.V.},
	author_keywords = {Attention mechanism; Crop disease identification; Image classification; Residual neural network},
	keywords = {Cucumis sativus; Convolution; Crops; Feature extraction; Attention mechanisms; Crop disease; Crop disease identification; Disease prevention; Efficient channels; Images classification; Neural-networks; Recognition accuracy; Recognition models; Residual neural network; accuracy assessment; crop plant; detection method; experimental study; identification method; Image classification},
	correspondence_address = {R. Wang; Beijing Research Center for Information Technology in Agriculture, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; email: rongw@tju.edu.cn},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 25}
}

@CONFERENCE{Kumar Thella20211330,
	author = {Kumar Thella, Prabhat and Ulagamuthalvi, V.},
	title = {A Brief Review on Methods for Classification of Medical Plant Images},
	year = {2021},
	journal = {Proceedings - 5th International Conference on Computing Methodologies and Communication, ICCMC 2021},
	pages = {1330 – 1333},
	doi = {10.1109/ICCMC51019.2021.9418380},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105975298&doi=10.1109%2fICCMC51019.2021.9418380&partnerID=40&md5=6d350f8615aaa091cf6ff788e7e6c6e4},
	affiliations = {Sathyabama Institute of Science and Technology, Department of Computer Science and Engineering, Chennai, India},
	abstract = {Plants on earth play a significant role both in human and other lives. Plants have the greatest influence in the nature cycle. Plant identification in biology and farming is very demanding, as new plant discoveries and plant computerization are more well established. For a broad range of uses, including environmental conservation, plant resource assessment and education, automatic plant classification systems have to be implemented. The leaves are considered the way medicinal plants are characterized. The automated identification of plant species using photographic leaves is an interesting objective, as biodiversity decreases rapidly in the current mix and taxonomists lack properly trained. The identification of the right plant is the most critical step in the preparation of the medicine that is carried out manually. The identification of these plants is immediately relevant because of demand for mass production. The physical and mental health of human beings is crucial for medicines. For better care it is important to identify and classify medicinal plants. The lack of experts in this area makes it tedious that medicinal plants are properly identified and classified. A fully automated system is therefore highly desirable for medicinal plant classification. This article provides a brief survey of the various models for identifying medicinal plants by taking into account the form and texture of a plant leaf. © 2021 IEEE.},
	author_keywords = {Classification; Clustering; Feature Extraction; Leaf Recognition; Medical Plant Classification; Shape Extraction},
	keywords = {Automation; Biodiversity; Image classification; Medical imaging; Textures; Automated identification; Environmental conservation; Fully automated; Mass production; Medicinal plants; Plant classification; Plant identification; Resource assessments; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166540360-3},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Comput. Methodol. Commun., ICCMC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 5th International Conference on Computing Methodologies and Communication, ICCMC 2021; Conference date: 8 April 2021 through 10 April 2021; Conference code: 168766}
}

@CONFERENCE{Arun2022,
	author = {Arun, Yagan and Viknesh, G S},
	title = {Leaf Classification for Plant Recognition Using EfficientNet Architecture},
	year = {2022},
	journal = {Proceedings of IEEE 2022 4th International Conference on Advances in Electronics, Computers and Communications, ICAECC 2022},
	doi = {10.1109/ICAECC54045.2022.9716637},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126778762&doi=10.1109%2fICAECC54045.2022.9716637&partnerID=40&md5=c1bc8efd1af308117730a675519dff87},
	affiliations = {St. Joseph's College of Engineering, Department of Computer Science and Engineering, Chennai, India},
	abstract = {Automatic plant species classification has always been a great challenge. Classical machine learning methods have been used to classify leaves using handcrafted features from the morphology of plant leaves which has given promising results. However, we focus on using non-handcrafted features of plant leaves for classification. So, to achieve it, we utilize a deep learning approach for feature extraction and classification of features. Recently Deep Convolution Neural Networks have shown remarkable results in image classification and object detection-based problems. With the help of the transfer learning approach, we explore and compare a set of pre-trained networks and define the best classifier. That set consists of eleven different pre-trained networks loaded with ImageNet weights: AlexNet, EfficientNet BO to B7, ResNet50, and Xception. These models are trained on the plant leaf image data set, consisting of leaf images from eleven different unique plant species. It was found that EfficientNet-B5 performed better in classifying leaf images compared to other pre-trained models. Automatic plant species classification could be helpful for food engineers, people related to agriculture, researchers, and ordinary people.  © 2022 IEEE.},
	author_keywords = {Deep Learning; EfficientNet; Plant Leaf Recognition; Transfer Learning},
	keywords = {Classification (of information); Deep learning; Image classification; Object detection; Transfer learning; Deep learning; Efficientnet; Leaf images; Leaf recognition; Learning approach; Plant leaf recognition; Plant leaves; Plant species; Species classification; Transfer learning; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166540239-2},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Adv. Electron., Comput. Commun., ICAECC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 4th IEEE International Conference on Advances in Electronics, Computers and Communications, ICAECC 2022; Conference date: 10 January 2022 through 11 January 2022; Conference code: 177412}
}

@CONFERENCE{Khalid2022204,
	author = {Khalid, Fatimah and Abdullah, Azfar Husna and Abdullah, Lili Nurliyana},
	title = {SMARTFLORA Mobile Flower Recognition Application Using Machine Learning Tools},
	year = {2022},
	journal = {2022 IEEE 18th International Colloquium on Signal Processing and Applications, CSPA 2022 - Proceeding},
	pages = {204 – 209},
	doi = {10.1109/CSPA55076.2022.9781961},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132701850&doi=10.1109%2fCSPA55076.2022.9781961&partnerID=40&md5=766427a29370901f8b645a406e9aae53},
	affiliations = {University Putra Malaysia, Faculty of Computer Science and Information Technology, Selangor, Serdang, Malaysia},
	abstract = {There are around 369,000 flowering plant species documented globally. However, the majority of people have difficulties telling these blooms apart. Usually, people often consult specialists, study floral reference books, or do keyword searches on relevant web resources. Therefore, this flower recognition mobile application was proposed to ease those people to recognize types of flowers without using any computer or machine. In this paper, a system architecture is designed based on Teachable Machine Learning platform, Tensorflow Lite Model and Android Studio to develop a SMARTFLORA Mobile Flower Recognition application that allows users to identify three types of flower species: daisies, roses, and sunflowers. Kaggle dataset has been used and the accuracy was 88%.  © 2022 IEEE.},
	author_keywords = {Flower classification; Machine Learning; Teachable Machine Learning; Tensorflow Lite},
	keywords = {Flower classification; Flower recognition; Flowering plants; Keyword search; Learning tool; Machine-learning; Plant species; Teachable machine learning; Tensorflow lite; Machine learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166548529-6},
	language = {English},
	abbrev_source_title = {IEEE Int. Colloq. Signal Process. Appl., CSPA - Proceeding},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 18th IEEE International Colloquium on Signal Processing and Applications, CSPA 2022; Conference date: 12 May 2022; Conference code: 179572}
}

@CONFERENCE{Zhou2021,
	author = {Zhou, Cheng-Li and Ge, Lin-Mei and Guo, Yan-Bu and Zhou, Dong-Ming and Cun, Yu-Peng},
	title = {A comprehensive comparison on current deep learning approaches for plant image classification},
	year = {2021},
	journal = {Journal of Physics: Conference Series},
	volume = {1873},
	number = {1},
	doi = {10.1088/1742-6596/1873/1/012002},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104766393&doi=10.1088%2f1742-6596%2f1873%2f1%2f012002&partnerID=40&md5=4fda362dd7d7c0bbf72f1822fec07321},
	affiliations = {School of Informatics Science and Engineering, Yunnan University, Kunming, 650002, China; IFlora Bioinformatics Center, Germplasm Bank of Wild Species, Kunming Institute of Botany, Chinese Academy of Sciences, Kunming, 650201, China},
	abstract = {Plant identification and classification play a key role in understanding, protecting and conserving biodiversity. Traditional plant taxonomy needs long time intensive training and experience, which limited others to identify plant categories. With the development of automated image-based classification, machine learning (ML) is becoming a popular tool. Image classification, especially plant images taxonomy, has achieved great improvement in these years by deep learning (DL) methods. In this study, we first reviewed current deep learning applications in the field of plant image classification, and then we tested six deep learning methods in four public plant image datasets. In order to test the classification power of DL methods at cultivar level, we prepared a Camellia sasanqua Thunb. dataset, which is called Camellia@clab, for assessing classification performance of the six DL methods. These DL models' classification performance all exceeded 70% in the four public plant image datasets, and LeNet and DenseNet had stable good performance, with median prediction accuracy of the LeNet was over 87.29% and that of DenseNet was over 93.8% in the four public datasets at species level. At cultivar level, the lowest median prediction accuracy of those DL methods decreased to 62%, but LeNet and DenseNet still performed very well. The prediction accuracy of LeNet and DenseNet was 82.3% and 100% in the Camellia@clab dataset, respectively. DenseNet model showed a stable best classification performance among the five datasets. To our knowledge, this is the first study that provides a comprehensive review and comparison on applying current DL methods to plant image classification. This study will provide guidance for DL applications in plant image classification, and point out the protentional DL research direction for modeling improvement. © Published under licence by IOP Publishing Ltd.},
	keywords = {Biodiversity; Deep learning; Forecasting; Image enhancement; Learning systems; Taxonomies; Classification performance; Classification power; Comprehensive comparisons; Image-based classification; Learning approach; Plant identification; Prediction accuracy; Provide guidances; Image classification},
	correspondence_address = {D.-M. Zhou; School of Informatics Science and Engineering, Yunnan University, Kunming, 650002, China; email: zhoudm@ynu.edu.cn; Y.-P. Cun; IFlora Bioinformatics Center, Germplasm Bank of Wild Species, Kunming Institute of Botany, Chinese Academy of Sciences, Kunming, 650201, China; email: cunyupeng@mail.kib.ac.cn},
	publisher = {IOP Publishing Ltd},
	issn = {17426588},
	language = {English},
	abbrev_source_title = {J. Phys. Conf. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 2021 2nd International Workshop on Electronic communication and Artificial Intelligence, IWECAI 2021; Conference date: 12 March 2021 through 14 March 2021; Conference code: 168463; All Open Access, Bronze Open Access}
}

@ARTICLE{Abdennour2022420,
	author = {Abdennour, Iliasse and Mah, Dembele and Bernoussi, Abdes Samed and Amharref, Mina},
	title = {Automated Recognition of Tree Species by Laser Scanning from 3D Geometric Texture of Tree Barks: Case of the Wadi Cherrat Arboretum},
	year = {2022},
	journal = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
	volume = {443 LNICST},
	pages = {420 – 428},
	doi = {10.1007/978-3-031-06374-9_27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131921420&doi=10.1007%2f978-3-031-06374-9_27&partnerID=40&md5=a09e806ec9ce10e4eadac4f1c27a1a67},
	affiliations = {GAT Team, Abdelmalek-Essaadi University, B.P. 416, Tangier, Morocco; MMC Team, Abdelmalek-Essaadi University, B.P. 416, Tangier, Morocco; Abdelmalek-Essaadi University, B.P. 416, Tangier, Morocco},
	abstract = {This work aims to develop an efficient and intelligent method for forest resource management. The objective is to implement an automatic tree species identification system based on 3D data obtained from terrestrial laser scans. The approach adopted concerns first the acquisition of 2D and 3D data, then the processing of LIDAR data and finally a process of identification of tree species by machine learning. A platform is designed and developed to meet this objective. The platform is a means that can be used by local researchers for the identification of tree species, providing a forestry database of the Wadi Cherrat arboretum. © 2022, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.},
	author_keywords = {Forest resource management; Forestry database; LIDAR; Species identification},
	keywords = {Data handling; Natural resources management; Optical radar; Resource allocation; Textures; Timber; 3D data; Automated recognition; Forest resource managements; Forestry database; Geometric texture; Intelligent method; Laser scanning; Species identification; Tree barks; Tree species; Forestry},
	correspondence_address = {I. Abdennour; GAT Team, Abdelmalek-Essaadi University, Tangier, B.P. 416, Morocco; email: iliasseabdennour@gmail.com},
	editor = {Sheikh Y.H. and Rai I.A. and Bakar A.D.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18678211},
	isbn = {978-303106373-2},
	language = {English},
	abbrev_source_title = {Lect. Notes Inst. Comput. Sci. Soc. Informatics Telecommun. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 13th EAI International Conference on e-Infrastructure and e-Services for Developing Countries, AFRICOMM 2021; Conference date: 1 December 2021 through 3 December 2021; Conference code: 278399}
}

@CONFERENCE{Veni2021243,
	author = {Veni, S. and Anand, R. and Mohan, Divya and Sreevidya, P.},
	title = {Leaf Recognition and Disease Detection using Content based Image Retrieval},
	year = {2021},
	journal = {2021 7th International Conference on Advanced Computing and Communication Systems, ICACCS 2021},
	pages = {243 – 247},
	doi = {10.1109/ICACCS51430.2021.9441805},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108029401&doi=10.1109%2fICACCS51430.2021.9441805&partnerID=40&md5=02f325a93462b4d7d2334910d242ff55},
	affiliations = {Amrita School of Engineering, Department of Electronics and Communication Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Sona College of Technology, Salem Research Scholar Amrita Vishwa Vidyapeetham University Amrita School of Engineering, Sona Sipro, Department of Ece, Coimbatore, India; Ahalia Engineering and Technology, Dept. of Ece, Palakkad, India; Federal Institute of Science and Technology, Electronics An Instrumentation Engineering, Angamaly, India},
	abstract = {The agricultural domain in past few decades has seen a decrease in its productivity. The main cause for this was found to be an increase in plant diseases. Having diseases in plants is quite common, but due to improper care there have been serious effects on plants. But we cannot keep inspecting each and every plant present in thousands. Hence, in this work an approach is developed which provides faster and more accurate results of the detected plant leaves and its corresponding diseases. The proposed work approach uses various image processing techniques for recognising the plant leaf type and detecting disease. The system uses two different classification methods namely, Support Vector Machine (SVM) and K-Nearest Neighbours (KNN) and their performances are compared. © 2021 IEEE.},
	author_keywords = {Image Processing; K-means clustering; K-Nearest Neighbours; Support Vector Machine},
	keywords = {Agricultural robots; Content based retrieval; Image processing; Nearest neighbor search; Support vector machines; Classification methods; Content based image retrieval; Disease detection; Image processing technique; K nearest neighbours (k-NN); Leaf recognition; Plant disease; Plant leaves; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166540520-1},
	language = {English},
	abbrev_source_title = {Int. Conf. Adv. Comput. Commun. Syst., ICACCS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 7th International Conference on Advanced Computing and Communication Systems, ICACCS 2021; Conference date: 19 March 2021 through 20 March 2021; Conference code: 169336}
}

@ARTICLE{Cai2022,
	author = {Cai, Xingquan and Huo, Yuqing and Chen, Yunbo and Xi, Mengyao and Tu, Yuxin and Sun, Chen and Sun, Haiyan},
	title = {Real-Time Leaf Recognition Method Based on Image Segmentation and Feature Extraction},
	year = {2022},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	volume = {36},
	number = {1},
	doi = {10.1142/S0218001421540331},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124235338&doi=10.1142%2fS0218001421540331&partnerID=40&md5=c231b2413a27923581d87bb8a43dcd60},
	affiliations = {School of Information Science and Technology, North China University of Technology, Beijing, 100144, China},
	abstract = {Leaf recognition has been an important research field of image recognition in the recent past. However, traditional leaf recognition methods can be easily affected by environments and cannot realize multi-leaf recognition under a complex background in real time. In this work, we present a real-time leaf recognition method based on image segmentation and feature recognition. First, we denoise the input of a leaf image, performing a leaf segmentation with an improved FCN network model, and then optimize the contour edge with a CRF algorithm to get a leaf segmentation image. Second, we extract the content features of the segmented leaf image with an Inception-V2 network model to get a feature map of the leaf image. Third, we input the feature map into an RPN network to obtain a set of regional candidate frames and then integrate the feature map and the information of candidate frames in a RoI Pooling layer, which can extract the feature map of a candidate frame area and scale it to a fixed-size feature map. Finally, we send the feature map to a fully connected layer to classify each preselection box content through the calculation of preselection feature maps, and then obtain the final accurate position of the prediction box by utilizing a bounding box regression. The experimental results show that the proposed method can achieve multi-leaf recognitions with high accuracy and fast speed under complex environments in real time.  © 2022 World Scientific Publishing Company.},
	author_keywords = {FCN network; feature extraction; image segmentation; Leaf recognition},
	keywords = {Complex networks; Edge detection; Extraction; Image enhancement; Image recognition; Image segmentation; FCN network; Feature map; Features extraction; Images segmentations; Leaf images; Leaf recognition; Network models; Pre-selection; Real- time; Recognition methods; Feature extraction},
	correspondence_address = {X. Cai; School of Information Science and Technology, North China University of Technology, Beijing, 100144, China; email: xingquancai@126.com; H. Sun; School of Information Science and Technology, North China University of Technology, Beijing, 100144, China; email: sunhaiyan80@hotmail.com},
	publisher = {World Scientific},
	issn = {02180014},
	coden = {IJPIE},
	language = {English},
	abbrev_source_title = {Int J Pattern Recognit Artif Intell},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Wu2021,
	author = {Wu, Lin and Yang, Jingjing and Gu, Zhihao and Guo, Jiaqian and Zhang, Xiao},
	title = {Rare and Endangered Plant Leaf Identification Method Based on Transfer Learning and Knowledge Distillation},
	year = {2021},
	journal = {International Journal of Agricultural and Environmental Information Systems},
	volume = {12},
	number = {4},
	doi = {10.4018/IJAEIS.288037},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123385671&doi=10.4018%2fIJAEIS.288037&partnerID=40&md5=a18e29ae0e6457bdfae0a504d1b62e97},
	affiliations = {School of Information Science and Engineering, Hebei North University, China},
	abstract = {Considering the limited sample size of rare and endangered plant leaves and the issue that leaf identification is mainly conducted using mobile smart devices and other technology with low computing power, this paper proposes a rare and endangered plant leaf identification method based on transfer learning and knowledge distillation. Following the expansion of data sets containing rare and endangered plant leaves, the last fully connected layer was replaced with trained Alexnet, VGG16, GoogLeNet, and ResNet models to conduct transfer learning, and realize a relatively high success rate in identifying images of these species. Then, knowledge distillation was utilized to transfer Alexnet, VGG16, GoogLeNet, and ResNet models into a lightweight model. The experiment results indicate that, compared with other methods, the lightweight rare and endangered plant identification model trained with the methods described in this paper was not only more accurate but also less complex than its alternatives. © 2017 Societe des Oceanistes. All rights reserved.},
	author_keywords = {Convolutional Neural Network (CNN); Image Identification; Knowledge Distillation; Leaf Identification; Lightweight Network Model; Low Computing Power; Rare and Endangered Species; Transfer Learning},
	keywords = {Conservation; Distillation; Image processing; Neural networks; Computing power; Convolutional neural network; Image identification; Knowledge distillation; Leaf identification; Lightweight network model; Low computing power; Network models; Rare and endangered species; Transfer learning; artificial neural network; computer simulation; conservation management; distillation; endangered species; learning; species conservation; Plants (botany)},
	publisher = {IGI Global},
	issn = {19473192},
	language = {English},
	abbrev_source_title = {Int. J. Agric. Environ. Inf. Syst.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Bronze Open Access}
}

@ARTICLE{Kritsis2021,
	author = {Kritsis, Kosmas and Kiourt, Chairi and Stamouli, Spyridoula and Sevetlidis, Vasileios and Solomou, Alexandra and Karetsos, George and Katsouros, Vassilis and Pavlidis, George},
	title = {Grasp-125: A dataset for greek vascular plant recognition in natural environment},
	year = {2021},
	journal = {Sustainability (Switzerland)},
	volume = {13},
	number = {21},
	doi = {10.3390/su132111865},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118164731&doi=10.3390%2fsu132111865&partnerID=40&md5=20d015ad74d1406f0746e40bb504c8e9},
	affiliations = {Athena Research Center, Institute for Language and Speech Processing, Athens, 15125, Greece; Institute of Mediterranean Forest Ecosystems, Hellenic Agricultural Organization DEMETER, Athens, 11528, Greece},
	abstract = {Plant identification from images has become a rapidly developing research field in computer vision and is particularly challenging due to the morphological complexity of plants. The availability of large databases of plant images, and the research advancements in image processing, pattern recognition and machine learning, have resulted in a number of remarkably accurate and reliable image-based plant identification techniques, overcoming the time and expertise required for conventional plant identification, which is feasible only for expert botanists. In this paper, we introduce the GReek vAScular Plants (GRASP) dataset, a set of images composed of 125 classes of different species, for the automatic identification of vascular plants of Greece. In this context, we describe the methodology of data acquisition and dataset organization, along with the statistical features of the dataset. Furthermore, we present results of the application of popular deep learning architectures to the classification of the images in the dataset. Using transfer learning, we report 91% top-1 and 98% top-5 accuracy. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Deep learning; Image classification; Plant identification; Transfer learning},
	keywords = {Greece; Tracheophyta; computer vision; data acquisition; data set; detection method; methodology; pattern recognition; vascular plant},
	correspondence_address = {K. Kritsis; Athena Research Center, Institute for Language and Speech Processing, Athens, 15125, Greece; email: kosmas.kritsis@athenarc.gr; C. Kiourt; Athena Research Center, Institute for Language and Speech Processing, Athens, 15125, Greece; email: chairiq@athenarc.gr},
	publisher = {MDPI},
	issn = {20711050},
	language = {English},
	abbrev_source_title = {Sustainability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Hussain2021,
	author = {Hussain, Nazar and Farooque, Aitazaz A. and Schumann, Arnold W. and Abbas, Farhat and Acharya, Bishnu and McKenzie-Gopsill, Andrew and Barrett, Ryan and Afzaal, Hassan and Zaman, Qamar U. and Cheema, Muhammad J.M.},
	title = {Application of deep learning to detect Lamb's quarters (Chenopodium album L.) in potato fields of Atlantic Canada},
	year = {2021},
	journal = {Computers and Electronics in Agriculture},
	volume = {182},
	doi = {10.1016/j.compag.2021.106040},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101302626&doi=10.1016%2fj.compag.2021.106040&partnerID=40&md5=9377ee03fabea88ed37ef8911fff2d1b},
	affiliations = {Faculty of Sustainable Design Engineering, University of Prince Edward Island, Charlottetown, C1A4P3, PE, Canada; School of Climate Change and Adaptation, University of Prince Edward Island, Charlottetown, C1A 4P3, PE, Canada; Citrus Research and Education Center, University of Florida, Gainesville, 32611, FL, United States; Department of Chemical and Biological Engineering, University of Saskatchewan, Saskatoon, S7N 5A9, SK, Canada; Agriculture and Agri-Food Canada, Charlottetown Research and Development Centre, Charlottetown, C1A4N6, PE, Canada; Prince Edward Island Potato Board, West Royalty Business Park, 90 Hillstrom Avenue, Charlottetown, C1E 2C6, PE, Canada; Engineering Department, Dalhousie University, Agriculture Campus, Truro, B2N 5E3, NS, Canada; Faculty of Agricultural Engineering and Technology, PMAS-Arid Agriculture University Rawalpindi, Rawalpindi, 46000, PB, Pakistan},
	abstract = {Excessive use of herbicides for weed control increases the cost of crop production and can lead to environmental degradation. An intelligent spraying system can apply agrochemicals on an as-needed basis by detecting and selectively targeting the weeds. The objective of this research was to investigate the feasibility of using deep convolutional neural networks (DCNNs) for detecting lamb's quarters (Chenopodium album) in potato fields. Five potato fields were selected in Prince Edward Island (PEI) and New Brunswick (NB), Canada to collect images of spatially and temporally varied potato plants and lamb's quarters. The image database included pictures, taken under varying growth stages of potato, outdoor light (clear, cloudy, and partly cloudy), and shadowy conditions. The images were trained for DCNN models, namely GoogLeNet, VGG-16, and EfficientNet to classify lamb's quarters and potato plants. Performance of two frameworks, namely TensorFlow and PyTorch, were compared in training, testing, and during inferring the DCNNs. Results showed excellent performance of DCNNs in lamb's quarters and potato plant classification (accuracy > 90%). However, the EfficientNet with PyTorch framework showed a maximum accuracy of (0.92–0.97) for every growth stage of the plants. Inference times of DCNNs were recorded using three graphics processing units (GPUs), namely Nvidia GeForce 930MX, Nvidia GeForce GTX1080 Ti, and Nvidia GeForce GTX1050. All the DCNNs performed better with PyTorch than TensorFlow frameworks. It was concluded that the trained models can be used in automation of the spraying systems for the site-specific application of agrochemicals for weed control in potato fields. Such precision agriculture technologies will ensure economically viable and environmentally safe potato cultivation. © 2021 Elsevier B.V.},
	author_keywords = {Agrochemicals; Deep convolutional neural network; Deep learning; Image processing; Precision agriculture technologies; Smart sprayer},
	keywords = {Canada; New Brunswick; Prince Edward Islands; Chenopodium album; Solanum tuberosum; Agricultural chemicals; Agricultural robots; Biodegradation; Computer graphics; Convolutional neural networks; Cultivation; Deep neural networks; Graphics processing unit; Program processors; Weed control; Crop production; Economically viable; Environmentally safe; Image database; Maximum accuracies; Precision agriculture technology; Prince edward islands; Spraying system; agrochemical; automation; crop production; database; environmental degradation; learning; potato; precision agriculture; weed control; Deep learning},
	correspondence_address = {A.A. Farooque; Faculty of Sustainable Design Engineering, University of Prince Edward Island, Charlottetown, C1A4P3, Canada; email: afarooque@upei.ca},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@CONFERENCE{Pawar20221442,
	author = {Pawar, Lokesh and Singh, Jaspreet and Bajaj, Rohit and Singh, Gurpreet and Rana, Sanjima},
	title = {Optimized Ensembled Machine Learning Model for IRIS Plant Classification},
	year = {2022},
	journal = {2022 6th International Conference on Trends in Electronics and Informatics, ICOEI 2022 - Proceedings},
	pages = {1442 – 1446},
	doi = {10.1109/ICOEI53556.2022.9776724},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131932242&doi=10.1109%2fICOEI53556.2022.9776724&partnerID=40&md5=90cf08f15beaee87f7de1ac2a20db0f1},
	affiliations = {Chandigarh University, Computer Science and Engineering, Mohali, India},
	abstract = {Pattern recognition is one of the major concerns in the field of machine learning. Appropriate Recognition of patterns leads to addressing the performance concerns. Machine Learning [ML], a subset of Artificial Intelligence [AI] plays an important role in the process of classification, clustering and predictive modeling. This paper focuses on developing a novel classification technique to classify the iris of the plant in order to categorize the flower pattern. An optimized ensemble model is proposed to recognize and categorize the pattern. Initially, Decision Tree, OneR, Adaboost, Random Forest, and Bayesnet models are applied and to improve the performance an ensemble model is proposed. © 2022 IEEE.},
	author_keywords = {Eager Learning; Ensemble model in Machine learning; Lazy Learning; Optimization},
	keywords = {Adaptive boosting; Decision trees; Pattern recognition; Classification/clustering; Eager learning; Ensemble model in machine learning; Ensemble models; Lazy learning; Machine learning models; Machine-learning; Optimisations; Performance; Plant classification; Machine learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166548328-5},
	language = {English},
	abbrev_source_title = {Int. Conf. Trends Electron. Informatics, ICOEI - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 6th International Conference on Trends in Electronics and Informatics, ICOEI 2022; Conference date: 28 April 2022 through 30 April 2022; Conference code: 179497}
}

@CONFERENCE{Liang2021158,
	author = {Liang, Yuan and Yihao, Li and Yang, Wang and Ailong, Mao and Kun, Chen},
	title = {Detection and Identification of Sandwich Materials in Tobacco Packing Boxes Based on Three-channel Two-Dimensional Convolutional Neural Network},
	year = {2021},
	journal = {Proceedings - 2021 4th International Conference on Intelligent Autonomous Systems, ICoIAS 2021},
	pages = {158 – 161},
	doi = {10.1109/ICoIAS53694.2021.00036},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115878363&doi=10.1109%2fICoIAS53694.2021.00036&partnerID=40&md5=5f5ebcd8d21b154fb4a5ee3516ca9317},
	affiliations = {Zhengzhou University of Light Industry, Zheng Zhou, China; Zhengzhou University of Light Industry, Zheng Zhou, China; Tobacco Henan Industrial Co. Ltd., Golden Leaf Production and Manufacturing Center of China, Zheng Zhou, China; Tobacco Henan Industrial Co. Ltd., Golden Leaf Production and Manufacturing Center of China, Zheng Zhou, China; Shanghai Institute of Spaceflight Control Technology, Shanghai, China},
	abstract = {At present, the common problem in the tobacco factory is whether there is yellow cardboard sandwich material in the tobacco packaging box. The KUKA unpacking robot can not independently determine the shape of the cardboard, the central point position, and whether there is any, resulting in the problem of misoperation and repeated operation, resulting in the decline in the efficiency of tobacco unpacking. So this article with the method of image processing combined with convolution neural network puts forward a kind of complex tobacco packaging sandwich materials based on machine vision detection method, shape to calibrate the strawboards interlining, determine the center position, whether there is any problem and confirm the strawboards interlining, to realize accurate positioning KUKA robot clip strawboards sandwich material. The experiment shows that the efficiency of judgment is better than that of manual inspection, and the production efficiency of the factory is increased by 82%, saving human resources to some extent. © 2021 IEEE.},
	author_keywords = {convolutional neural network; machine vision; tobacco leaf recognition},
	keywords = {Detection; Efficiency; Packaging Materials; Packing; Positioning; Robots; Shape; Tobacco; Cardboard; Cigarette manufacture; Convolution; Efficiency; Image processing; Packaging materials; Robots; Tobacco; Convolution neural network; Detection and identifications; Detection methods; Manual inspection; Method of images; Production efficiency; Sandwich materials; Tobacco packaging; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166544195-7},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Intell. Auton. Syst., ICoIAS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference on Intelligent Autonomous Systems, ICoIAS 2021; Conference date: 14 May 2021 through 16 May 2021; Conference code: 171635}
}

@ARTICLE{Taslim20213341,
	author = {Taslim, Amiruzzaki and Saon, Sharifah and Mahamad, Abd Kadir and Muladi, Muladi and Hidayat, Wahyu Nur},
	title = {Plant leaf identification system using convolutional neural network},
	year = {2021},
	journal = {Bulletin of Electrical Engineering and Informatics},
	volume = {10},
	number = {6},
	pages = {3341 – 3352},
	doi = {10.11591/eei.v10i6.2332},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120616566&doi=10.11591%2feei.v10i6.2332&partnerID=40&md5=c396918a21bc8c30faf5032abc87bfa3},
	affiliations = {Idependent Reseacher, Kanchong Darat, Banting, Selangor, Malaysia; Faculty of Electrical and Electronic Engineering (FKEE), Universiti Tun Hussein Onn Malaysia (UTHM), Malaysia; Department of Electrical Engineering, Universitas Negeri Malang, Malang, Indonesia},
	abstract = {This paper proposes a leaf identification system using convolutional neural network (CNN). This proposed system can identify five types of local Malaysia leaf which were acacia, papaya, cherry, mango and rambutan. By using CNN from deep learning, the network is trained from the database that acquired from leaf images captured by mobile phone for image classification. ResNet-50 was the architecture has been used for neural networks image classification and training the network for leaf identification. The recognition of photographs leaves requested several numbers of steps, starting with image pre-processing, feature extraction, plant identification, matching and testing, and finally extracting the results achieved in MATLAB. Testing sets of the system consists of 3 types of images which were white background, and noise added and random background images. Finally, interfaces for the leaf identification system have developed as the end software product using MATLAB app designer. As a result, the accuracy achieved for each training sets on five leaf classes are recorded above 98%, thus recognition process was successfully implemented. © 2021, Institute of Advanced Engineering and Science. All rights reserved.},
	author_keywords = {Convolutional neural network; Image classification; Leaf identification system; ResNet-50},
	correspondence_address = {S. Saon; Faculty of Electrical and Electronic Engineering (FKEE)Universiti Tun Hussein Onn Malaysia (UTHM), Parit Raja, Batu Pahat, 86400, Malaysia; email: sharifa@uthm.edu.my},
	publisher = {Institute of Advanced Engineering and Science},
	issn = {20893191},
	language = {English},
	abbrev_source_title = {Bull. Electr. Eng. Inform.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Heidary-Sharifabad2021,
	author = {Heidary-Sharifabad, Ahmad and Zarchi, Mohsen Sardari and Emadi, Sima and Zarei, Gholamreza},
	title = {Efficient deep learning models for categorizing Chenopodiaceae in the wild},
	year = {2021},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	volume = {35},
	number = {10},
	doi = {10.1142/S0218001421520157},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106187956&doi=10.1142%2fS0218001421520157&partnerID=40&md5=0b085048f98aa7ea20bfd6d4101eea7c},
	affiliations = {Department of Computer Engineering, Yazd Branch, Islamic Azad University, Yazd, Iran; Department of Computer Engineering, Meybod University, Meybod, Iran; Department of Agronomy, Maybod Branch, Islamic Azad University, Maybod, Iran},
	abstract = {The Chenopodiaceae species are ecologically and financially important, and play a significant role in biodiversity around the world. Biodiversity protection is critical for the survival and sustainability of each ecosystem and since plant species recognition in their natural habitats is the first process in plant diversity protection, an automatic species classification in the wild would greatly help the species analysis and consequently biodiversity protection on earth. Computer vision approaches can be used for automatic species analysis. Modern computer vision approaches are based on deep learning techniques. A standard dataset is essential in order to perform a deep learning algorithm. Hence, the main goal of this research is to provide a standard dataset of Chenopodiaceae images. This dataset is called ACHENY and contains 27030 images of 30 Chenopodiaceae species in their natural habitats. The other goal of this study is to investigate the applicability of ACHENY dataset by using deep learning models. Therefore, two novel deep learning models based on ACHENY dataset are introduced: First, a lightweight deep model which is trained from scratch and is designed innovatively to be agile and fast. Second, a model based on the EfficientNet-B1 architecture, which is pre-trained on ImageNet and is fine-tuned on ACHENY. The experimental results show that the two proposed models can do Chenopodiaceae fine-grained species recognition with promising accuracy. To evaluate our models, their performance was compared with the well-known VGG-16 model after fine-tuning it on ACHENY. Both VGG-16 and our first model achieved about 80% accuracy while the size of VGG-16 is about 16× larger than the first model. Our second model has an accuracy of about 90% and outperforms the other models where its number of parameters is 5× than the first model but it is still about one-third of the VGG-16 parameters.  © 2021 World Scientific Publishing Company.},
	author_keywords = {Biodiversity protection; Chenopodiaceae; Convolutional neural networks; Deep learning; Image classification; Plant classification; Standard dataset},
	keywords = {Biodiversity; Computer vision; Ecosystems; Learning algorithms; Learning systems; Biodiversity protection; Learning techniques; Model-based OPC; Natural habitat; Plant diversity; Species analysis; Species classification; Species recognition; Deep learning},
	correspondence_address = {M.S. Zarchi; Department of Computer Engineering, Meybod University, Meybod, Iran; email: sardari@meybod.ac.ir},
	publisher = {World Scientific},
	issn = {02180014},
	coden = {IJPIE},
	language = {English},
	abbrev_source_title = {Int J Pattern Recognit Artif Intell},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Wöber2021,
	author = {Wöber, Wilfried and Mehnen, Lars and Sykacek, Peter and Meimberg, Harald},
	title = {Investigating explanatory factors of machine learning models for plant classification},
	year = {2021},
	journal = {Plants},
	volume = {10},
	number = {12},
	doi = {10.3390/plants10122674},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120608545&doi=10.3390%2fplants10122674&partnerID=40&md5=0c6142148c38d14ca80e9637ee503f1c},
	affiliations = {Department of Integrative Biology and Biodiversity Research, Institute of Integrative Conservation Research, University of Natural Resources and Life Sciences, Gregor Mendel Str. 33, Vienna, 1080, Austria; Department Industrial Engineering, University of Applied Sciences Technikum Wien, Höchstädtplatz 6, Vienna, 1200, Austria; Department Computer Science, University of Applied Sciences Technikum Wien, Höchstädtplatz 6, Vienna, 1200, Austria; Department of Biotechnology, Institute of Computational Biology, University of Natural Resources and Life Sciences, Muthgasse 18, Vienna, 1190, Austria},
	abstract = {Recent progress in machine learning and deep learning has enabled the implementation of plant and crop detection using systematic inspection of the leaf shapes and other morphological characters for identification systems for precision farming. However, the models used for this approach tend to become black-box models, in the sense that it is difficult to trace characters that are the base for the classification. The interpretability is therefore limited and the explanatory factors may not be based on reasonable visible characters. We investigate the explanatory factors of recent machine learning and deep learning models for plant classification tasks. Based on a Daucus carota and a Beta vulgaris image data set, we implement plant classification models and compare those models by their predictive performance as well as explainability. For comparison we implemented a feed forward convolutional neuronal network as a default model. To evaluate the performance, we trained an unsupervised Bayesian Gaussian process latent variable model as well as a convolutional autoencoder for feature extraction and rely on a support vector machine for classification. The explanatory factors of all models were extracted and analyzed. The experiments show, that feed forward convolutional neuronal networks (98.24% and 96.10% mean accuracy) outperforms the Bayesian Gaussian process latent variable pipeline (92.08% and 94.31% mean accuracy) as well as the convolutional autoenceoder pipeline (92.38% and 93.28% mean accuracy) based approaches in terms of classification accuracy, even though not significant for Beta vulgaris images. Additionally, we found that the neuronal network used biological uninterpretable image regions for the plant classification task. In contrast to that, the unsupervised learning models rely on explainable visual characters. We conclude that supervised convolutional neuronal networks must be used carefully to ensure biological interpretability. We recommend unsupervised machine learning, careful feature investigation, and statistical feature analysis for biological applications. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Deep learning; Explainable AI; Machine learning; Plant leaf morphometrics},
	correspondence_address = {W. Wöber; Department of Integrative Biology and Biodiversity Research, Institute of Integrative Conservation Research, University of Natural Resources and Life Sciences, Vienna, Gregor Mendel Str. 33, 1080, Austria; email: woeber@technikum-wien.at},
	publisher = {MDPI},
	issn = {22237747},
	language = {English},
	abbrev_source_title = {Plants},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Roslan20225,
	author = {Roslan, Noor Aini Mohd and Mat Diah, Norizan and Ibrahim, Zaidah and Hanum, Haslizatul Mohamed and Ismail, Marina},
	title = {Automatic Plant Recognition: A Survey of Relevant Algorithms},
	year = {2022},
	journal = {2022 IEEE 18th International Colloquium on Signal Processing and Applications, CSPA 2022 - Proceeding},
	pages = {5 – 9},
	doi = {10.1109/CSPA55076.2022.9782022},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132754076&doi=10.1109%2fCSPA55076.2022.9782022&partnerID=40&md5=4a43cee55303dbf2a3a450599bbb9b14},
	affiliations = {Universiti Teknologi MARA, Faculty of Computer and Mathematical Sciences, Selangor, Malaysia},
	abstract = {Plants are one of the most important elements since they provide oxygen, which is necessary for human survival. Plant recognition applications have been widely developed, and these applications can help botanists tackle various real-world problems. This paper reviews machine learning and deep learning algorithms discussed for plant recognition. Different algorithms used for plant identification and recognition research between the year 2007 until the year 2020 are reviewed. The main algorithms discussed are Convolutional Neural Network (CNN), Support Vector Machine (SVM), Artificial Neural Network (ANN), and K-Nearest Neighbours (KNN). This paper also compares the performance between selected algorithms and proposes the best technique from the research outcomes.  © 2022 IEEE.},
	author_keywords = {ANN; Automatic plant detection; CNN; deep learning; KNN; machine learning; plant recognition; SVM},
	keywords = {Convolutional neural networks; Deep learning; Learning algorithms; Nearest neighbor search; Automatic plant detection; Convolutional neural network; Deep learning; Human survival; K-near neighbor; Machine-learning; Nearest-neighbour; Plant detections; Plant recognition; Support vectors machine; Support vector machines},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166548529-6},
	language = {English},
	abbrev_source_title = {IEEE Int. Colloq. Signal Process. Appl., CSPA - Proceeding},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 18th IEEE International Colloquium on Signal Processing and Applications, CSPA 2022; Conference date: 12 May 2022; Conference code: 179572}
}

@ARTICLE{Hussein2021,
	author = {Hussein, Burhan Rashid and Malik, Owais Ahmed and Ong, Wee-Hong and Slik, Johan Willem Frederik},
	title = {Reconstruction of damaged herbarium leaves using deep learning techniques for improving classification accuracy},
	year = {2021},
	journal = {Ecological Informatics},
	volume = {61},
	doi = {10.1016/j.ecoinf.2021.101243},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100787056&doi=10.1016%2fj.ecoinf.2021.101243&partnerID=40&md5=f762716b19dfd2fb76e4499dfc49757d},
	affiliations = {Digital Science, Faculty of Science, Universiti Brunei Darussalam, Jalan Tungku Link, Gadong, BE1410, Brunei Darussalam; Institute of Applied Data Analytics, Universiti Brunei Darussalam, Jalan Tungku Link, Gadong, BE1410, Brunei Darussalam; Environmental and Life Sciences, Faculty of Science, Universiti Brunei Darussalam, Jalan Tungku Link, Gadong, BE1410, Brunei Darussalam},
	abstract = {Leaf is one of the most commonly used organs for species identification. The traditional identification process involves a manual analysis of individual dried or fresh leaf's features by the botanists. Recent advancements in computer vision techniques have assisted in automating the plants families/species identification process based on the digital images of leaves. However, most of the existing studies have focused on using datasets for fresh and intact leaves. A huge amount of data for preserved plants in the form of digitized herbaria specimens have not been effectively utilized for the task of automated identification because of the presence of damaged leaves in specimens. In this study, deep learning techniques have been proposed as a tool for reconstructing the damaged herbarium leaves in order to maximize the usefulness of the digitized specimens for automated plant identification task by increasing the number of individual samples of leaves. The reconstruction results of two different families of convolution neural networks (CNNs) have been compared for data from ten different plant families namely Anacardiaceae, Annonaceae, Dipterocarpaceae, Ebenaceae, Euphorbiaceae, Malvaceae, Phyllanthaceae, Polygalaceae, Rubiaceae and Sapotaceae. The performance of automated identification task was improved by more than 20% using the reconstructed leaves images as compared to using the original data (i.e. images of specimens with damaged leaves). This work evidently suggests that deep learning techniques can be utilized for reconstruction of damaged leaves even on a challenging herbarium leaves dataset. © 2021 Elsevier B.V.},
	author_keywords = {Damaged leaves; Deep learning; Generative adversarial networks; Herbaria; Partial convolution; Plant species identification},
	keywords = {Anacardiaceae; Annonaceae; Dipterocarpaceae; Ebenaceae; Euphorbiaceae; Malvaceae; Phyllanthaceae; Polygalaceae; Rubiaceae; Sapotaceae; accuracy assessment; artificial neural network; classification; herbarium; identification method; image analysis; leaf; reconstruction},
	correspondence_address = {O.A. Malik; Digital Science, Faculty of Science, Universiti Brunei Darussalam, Gadong, Jalan Tungku Link, BE1410, Brunei Darussalam; email: owais.malik@ubd.edu.bn},
	publisher = {Elsevier B.V.},
	issn = {15749541},
	language = {English},
	abbrev_source_title = {Ecol. Informatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Villaruz2021226,
	author = {Villaruz, Jolitte A.},
	title = {Deep convolutional neural network feature extraction for berry trees classification},
	year = {2021},
	journal = {Journal of Advances in Information Technology},
	volume = {12},
	number = {3},
	pages = {226 – 233},
	doi = {10.12720/jait.12.3.226-233},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111602379&doi=10.12720%2fjait.12.3.226-233&partnerID=40&md5=28595c1a8b6fcf0fbb7a794efa7df220},
	affiliations = {Technology Department, Aklan State University-Kalibo Campus, Aklan, Kalibo, Philippines},
	abstract = {To support biodiversity conservations, plant classification studies, particularly from images, are necessary. This study explores the use of the deep convolutional neural network as a feature extractor to a plant classification problem. An original dataset consisting of images of seedlings of the three most important berry trees belonging to the Philippine indigenous plants was used. The result shows that as the network layers are getting deeper, they are becoming better at extracting discriminative features, such that, irrespective of classifier used their prediction performance keeps on improving. When the different layers were individually visualized, the features extracted were far from random, uninterpretable patterns. Rather, they show relevant properties that are capable of sorting patterns progressively from low to higher level. Hence, for classification problems bounded with the limitation of data, time, and computational hardware, leveraging the representational power of the deep convolutional neural network is very useful. © 2021 J. Adv. Inf. Technol.},
	author_keywords = {AlexNet; Deep convolutional neural network; Deep learning; Feature extraction; Plant classification; SVM},
	correspondence_address = {J.A. Villaruz; Technology Department, Aklan State University-Kalibo Campus, Kalibo, Aklan, Philippines; email: jvillaruz@asu.edu.ph},
	publisher = {Engineering and Technology Publishing},
	issn = {17982340},
	language = {English},
	abbrev_source_title = {J. Adv. Inf.  Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; All Open Access, Gold Open Access}
}

@ARTICLE{Joly2022390,
	author = {Joly, Alexis and Goëau, Hervé and Kahl, Stefan and Picek, Lukáš and Lorieul, Titouan and Cole, Elijah and Deneu, Benjamin and Servajean, Maximilien and Durso, Andrew and Bolon, Isabelle and Glotin, Hervé and Planqué, Robert and Vellinga, Willem-Pier and Klinck, Holger and Denton, Tom and Eggel, Ivan and Bonnet, Pierre and Müller, Henning and Šulc, Milan},
	title = {LifeCLEF 2022 Teaser: An Evaluation of Machine-Learning Based Species Identification and Species Distribution Prediction},
	year = {2022},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {13186 LNCS},
	pages = {390 – 399},
	doi = {10.1007/978-3-030-99739-7_49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128722595&doi=10.1007%2f978-3-030-99739-7_49&partnerID=40&md5=a6f9e6f3ec72557d1eb17a817c89c17b},
	affiliations = {Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France; CIRAD, UMR AMAP, Occitanie, Montpellier, France; Univ. Toulon, Aix Marseille Univ., CNRS, LIS, DYNI Team, Marseille, France; Xeno-canto Foundation, Amsterdam, Netherlands; HES-SO, Sierre, Switzerland; KLYCCB, Cornell Lab of Ornithology, Cornell University, Ithaca, United States; LIRMM, AMIS, Univ Paul Valéry Montpellier, University Montpellier, CNRS, Montpellier, France; ISG, Department of Community Health and Medicine, UNIGE, Geneva, Switzerland; Department of Computing and Mathematical Sciences, Caltech, Pasadena, United States; Department of Cybernetics, FAV, University of West Bohemia, Plzen, Czech Republic; Department of Biological Sciences, Florida Gulf Coast University, Fort Myers, United States; Google LLC, San Francisco, United States; Department of Cybernetics, FEE, CTU in Prague, Prague, Czech Republic},
	abstract = {Building accurate knowledge of the identity, the geographic distribution and the evolution of species is essential for the sustainable development of humanity, as well as for biodiversity conservation. However, the difficulty of identifying plants, animals and fungi is hindering the aggregation of new data and knowledge. Identifying and naming living organisms is almost impossible for the general public and is often difficult even for professionals and naturalists. Bridging this gap is a key step towards enabling effective biodiversity monitoring systems. The LifeCLEF campaign, presented in this paper, has been promoting and evaluating advances in this domain since 2011. The 2022 edition proposes five data-oriented challenges related to the identification and prediction of biodiversity: (i) PlantCLEF: very large-scale plant identification, (ii) BirdCLEF: bird species recognition in audio soundscapes, (iii) GeoLifeCLEF: remote sensing based prediction of species, (iv) SnakeCLEF: Snake Species Identification in Medically Important scenarios, and (v) FungiCLEF: Fungi recognition from images and metadata. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	keywords = {Biodiversity; Conservation; Forecasting; Geographical distribution; Machine learning; Population distribution; Remote sensing; Biodiversity conservation; Biodiversity monitoring; General publics; Large-scales; Living organisms; Machine-learning; Monitoring system; Plant identification; Species distributions; Species identification; Fungi},
	correspondence_address = {A. Joly; Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France; email: alexis.joly@inria.fr},
	editor = {Hagen M. and Verberne S. and Macdonald C. and Seifert C. and Balog K. and Nørvåg K. and Setty V.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303099738-0},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; Conference name: 44th European Conference on Information Retrieval, ECIR 2022; Conference date: 10 April 2022 through 14 April 2022; Conference code: 276459; All Open Access, Green Open Access}
}

@ARTICLE{Cao20211981,
	author = {Cao, Shuai and Song, Biao},
	title = {Visual attentional-driven deep learning method for flower recognition},
	year = {2021},
	journal = {Mathematical Biosciences and Engineering},
	volume = {18},
	number = {3},
	pages = {1981 – 1991},
	doi = {10.3934/MBE.2021103},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103344690&doi=10.3934%2fMBE.2021103&partnerID=40&md5=4464202d24314bcfae97e18073de93e7},
	affiliations = {School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Nanjing University of Information Science and Technology, Nanjing, 210044, China},
	abstract = {As a typical fine-grained image recognition task, flower category recognition is one of the most popular research topics in the field of computer vision and forestry informatization. Although the image recognition method based on Deep Convolutional Neural Network (DCNNs) has achieved acceptable performance on natural scene image, there are still shortcomings such as lack of training samples, intra-class similarity and low accuracy in flowers category recognition. In this paper, we study deep learning-based flowers' category recognition problem, and propose a novel attention-driven deep learning model to solve it. Specifically, since training the deep learning model usually requires massive training samples, we perform image augmentation for the training sample by using image rotation and cropping. The augmented images and the original image are merged as a training set. Then, inspired by the mechanism of human visual attention, we propose a visual attention-driven deep residual neural network, which is composed of multiple weighted visual attention learning blocks. Each visual attention learning block is composed by a residual connection and an attention connection to enhance the learning ability and discriminating ability of the whole network. Finally, the model is training in the fusion training set and recognize flowers in the testing set. We verify the performance of our new method on public Flowers 17 dataset and it achieves the recognition accuracy of 85.7%. © 2021 the Author(s), licensee AIMS Press. This is an open access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0)},
	author_keywords = {Attention learning; Deep learning; Feature extraction; Flower recognition},
	keywords = {Behavioral research; Convolutional neural networks; Deep neural networks; Image recognition; Learning systems; Sampling; Acceptable performance; Category recognition; Discriminating abilities; Human visual attention; Learning abilities; Natural scene images; Recognition accuracy; Recognition methods; article; deep learning; diagnostic test accuracy study; feature extraction; flower; human; human experiment; residual neural network; rotation; visual attention; Deep learning},
	correspondence_address = {B. Song; Nanjing University of Information Science and Technology, Nanjing, 210044, China; email: caosh18@lzu.edu.cn},
	publisher = {American Institute of Mathematical Sciences},
	issn = {15471063},
	pmid = {33892533},
	language = {English},
	abbrev_source_title = {Math. Biosci. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Gold Open Access}
}

@ARTICLE{Barreto2021,
	author = {Barreto, Abel and Lottes, Philipp and Ispizua Yamati, Facundo Ramón and Baumgarten, Stephen and Wolf, Nina Anastasia and Stachniss, Cyrill and Mahlein, Anne-Katrin and Paulus, Stefan},
	title = {Automatic UAV-based counting of seedlings in sugar-beet field and extension to maize and strawberry},
	year = {2021},
	journal = {Computers and Electronics in Agriculture},
	volume = {191},
	doi = {10.1016/j.compag.2021.106493},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117191675&doi=10.1016%2fj.compag.2021.106493&partnerID=40&md5=c32b843dc9e0f613dc8e2a5ff151be1e},
	affiliations = {Institute of Sugar Beet Research (IfZ), Holtenser Landstraße 77, Göttingen, 37079, Germany; Pheno-Inspect GmbH, Straßburger Straße 109, Oberhausen, 46047, Germany; ARGE NORD e.V., Helene-Künne-Allee 5, Braunschweig, 38122, Germany; University of Bonn, Photogrammetry & Robotics Lab, Nussallee 15, Bonn, 53115, Germany},
	abstract = {Counting crop seedlings is a time-demanding activity involved in diverse agricultural practices like plant cultivating, experimental trials, plant breeding procedures, and weed control. Unmanned Aerial Vehicles (UAVs) carrying RGB cameras are novel tools for automatic field mapping, and the analysis of UAV images by deep learning methods can provide relevant agronomic information. UAV-based camera systems and a deep learning image analysis pipeline are implemented for a fully automated plant counting in sugar beet, maize, and strawberry fields in the present study. Five locations were monitored at different growth stages, and the crop number per plot was automatically predicted by using a fully convolutional network (FCN) pipeline. Our FCN-based approach is a single model for jointly determining both the exact stem location of crop and weed plants and a pixel-wise plant classification considering crop, weed, and soil. To determinate the approach performance, predicted crop counting was compared to visually assessed ground truth data. Results show that UAV-based counting of sugar-beet plants delivers forecast errors lower than 4.6%, and the main factors for performance are related to the intra-row distance and the growth stage. The pipeline's extension to other crops is possible; the errors of the predictions are lower than 4% under practical field conditions for maize and strawberry fields. This work highlight the feasibility of automatic crop counting, which can reduce manual effort to the farmers. © 2021 Elsevier B.V.},
	author_keywords = {Deep learning; FCN; Growth stage; Intra-row distance; Plant segmentation; Sugar beet; Time-series; UAV},
	keywords = {Beta vulgaris subsp. vulgaris; Fragaria x ananassa; Antennas; Cameras; Crops; Data visualization; Deep learning; Fruits; Image analysis; Pipelines; Seed; Sugar beets; Weed control; Convolutional networks; Deep learning; Fully convolutional network; Growth stages; Intra-row distance; Performance; Plant segmentation; Row distance; Sugar beet fields; Times series; image analysis; maize; plant breeding; seedling; sugar beet; unmanned vehicle; vegetation classification; vegetation mapping; weed control; Unmanned aerial vehicles (UAV)},
	correspondence_address = {A. Barreto; Institute of Sugar Beet Research (IfZ), Göttingen, Holtenser Landstraße 77, 37079, Germany; email: barreto@ifz-goettingen.de},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12}
}

@ARTICLE{Meshram2022255,
	author = {Meshram, Rohit Sunil and Patil, Nagamma},
	title = {Classification of Medicinal Plants Using Machine Learning},
	year = {2022},
	journal = {Smart Innovation, Systems and Technologies},
	volume = {289},
	pages = {255 – 267},
	doi = {10.1007/978-981-19-0011-2_24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132022159&doi=10.1007%2f978-981-19-0011-2_24&partnerID=40&md5=1a4c1405a54c51d970c6e276ff089dd1},
	affiliations = {Department of Information Technology, National Institute of Technology, Karnataka, Surathkal, India},
	abstract = {Nowadays, peoples are not having information about the surrounding plants and their medicinal values. If some person wants to know about the medicinal plants, they have to contact the person who is having deep knowledge about the medicinal plants and its uses. In order to solve this problem we can use the current technology to give a tool which will help the common people to know more about the medicinal plants. For doing this we can use many machine learning techniques for classifying the medicinal plants with more accuracy. Different kind of medicinal plant species are available on the planet earth but classification of the Particular medicinal plant is very difficult without knowing about the plants first. The information about the medicinal plants is collected by the scientists and urban people. Generally this kind of knowledge is passed through generation to generation and sometimes there might be some changes in the information and its contents. So according to the current situation we can use the machine learning technology to make the tool which will be helpful to solve the medicinal plant classification problem. Machine learning model can easily classify the medicinal plants after the feature extraction and applying the model. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Feature extraction; Machine learning technology; Medicinal plants; ResNet50; SVM},
	keywords = {Classification (of information); Earth (planet); Extraction; Plants (botany); Support vector machines; Current technology; Deep knowledge; Features extraction; Machine learning techniques; Machine learning technology; Machine-learning; Medicinal plants; Medicinal values; Resnet50; SVM; Feature extraction},
	correspondence_address = {R.S. Meshram; Department of Information Technology, National Institute of Technology, Karnataka, Surathkal, India; email: meshramrohit959@gmail.com},
	editor = {Reddy V.S. and Prasad V.K. and Mallikarjuna Rao D.N. and Satapathy S.C.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {21903018},
	isbn = {978-981190010-5},
	language = {English},
	abbrev_source_title = {Smart Innov. Syst. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: International Conference on Intelligent Systems and Sustainable Computing, ICISSC 2021; Conference date: 24 September 2021 through 25 September 2021; Conference code: 278599}
}

@ARTICLE{Goyal20224076,
	author = {Goyal, Neha and Gupta, Kapil and Kumar, Nitin},
	title = {Clustering-Based Hierarchical Framework for Multiclass Classification of Leaf Images},
	year = {2022},
	journal = {IEEE Transactions on Industry Applications},
	volume = {58},
	number = {3},
	pages = {4076 – 4085},
	doi = {10.1109/TIA.2022.3153757},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125331324&doi=10.1109%2fTIA.2022.3153757&partnerID=40&md5=b4b7781c41c9cdd03f98bc0c80e62301},
	affiliations = {National Institute of Technology Kurukshetra, Kurukshetra, 136119, India; National Institute of Technology Uttarakhand, Srinagar, 246174, India},
	abstract = {This article introduces a multiclass classification approach accustoming the benefits of partitioning-based strategies and hierarchical techniques. The proposed hierarchical framework creates a hierarchy with the notion of grouping classes with similar traits as one group. It overcomes the deficiency of the existing multiclass extension approaches, viz., nonlinearity, imbalanced class classification, and increasing classification cost with increasing number of classes. The hierarchical framework presents the idea of decomposing several classes hierarchically, where every cluster contains a set of classes having similar traits. The approach aims to maximize the intercluster distance and minimize the intracluster distribution. The effectiveness of the proposed method is evaluated on real-world and complex problems of plant recognition. Three leaf image datasets are considered for performance evaluation using a support vector machine. The results signify that the proposed approach for multiclass classification is an efficient approach with significantly improved recognition accuracy. It is a robust and effective approach with the least computational cost. The speedup factor of the proposed approach in the binary structure is 16, 6.5, and 5.5 as compared to a one-versus-one traditional support vector machine for Flavia, Swedish, and self-collected leaf datasets, respectively. © 1972-2012 IEEE.},
	author_keywords = {Dunn index (DI); hierarchical approach; multiclass classification; separability matrix; silhouette value},
	keywords = {Classification (of information); Clustering algorithms; Image classification; Job analysis; Support vector machines; Dunn index; Features extraction; Hierarchical approach; Index; matrix; Partitioning algorithms; Separability matrix; Silhouette value; Support vectors machine; Task analysis; Classifiers},
	correspondence_address = {N. Goyal; National Institute of Technology Kurukshetra, Kurukshetra, 136119, India; email: neha.goyal2309@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {00939994},
	coden = {ITIAC},
	language = {English},
	abbrev_source_title = {IEEE Trans Ind Appl},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Apriyanti2021,
	author = {Apriyanti, Diah Harnoni and Spreeuwers, Luuk J. and Lucas, Peter J.F. and Veldhuis, Raymond N.J.},
	title = {Automated color detection in orchids using color labels and deep learning},
	year = {2021},
	journal = {PLoS ONE},
	volume = {16},
	number = {10 October},
	doi = {10.1371/journal.pone.0259036},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118269684&doi=10.1371%2fjournal.pone.0259036&partnerID=40&md5=392d787e41307bce8d7e3fb940bf8248},
	affiliations = {Faculty of EEMCS, University of Twente, Enschede, Netherlands; Indonesian Institute of Sciences (LIPI), Jakarta, Indonesia; LIACS, Leiden University, Leiden, Netherlands},
	abstract = {The color of particular parts of a flower is often employed as one of the features to differentiate between flower types. Thus, color is also used in flower-image classification. Color labels, such as 'green', 'red', and 'yellow', are used by taxonomists and lay people alike to describe the color of plants. Flower image datasets usually only consist of images and do not contain flower descriptions. In this research, we have built a flower-image dataset, especially regarding orchid species, which consists of human-friendly textual descriptions of features of specific flowers, on the one hand, and digital photographs indicating how a flower looks like, on the other hand. Using this dataset, a new automated color detection model was developed. It is the first research of its kind using color labels and deep learning for color detection in flower recognition. As deep learning often excels in pattern recognition in digital images, we applied transfer learning with various amounts of unfreezing of layers with five different neural network architectures (VGG16, Inception, Resnet50, Xception, Nasnet) to determine which architecture and which scheme of transfer learning performs best. In addition, various color scheme scenarios were tested, including the use of primary and secondary color together, and, in addition, the effectiveness of dealing with multi-class classification using multi-class, combined binary, and, finally, ensemble classifiers were studied. The best overall performance was achieved by the ensemble classifier. The results show that the proposed method can detect the color of flower and labellum very well without having to perform image segmentation. The result of this study can act as a foundation for the development of an image-based plant recognition system that is able to offer an explanation of a provided classification.  © 2021 Apriyanti et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	keywords = {Algorithms; Color; Deep Learning; Flowers; Plants; article; classifier; deep learning; flower; human; human experiment; image segmentation; multiclass classification; nonhuman; Orchidaceae; pattern recognition; photography; residual neural network; transfer of learning; algorithm; classification; color; flower; plant},
	correspondence_address = {D.H. Apriyanti; Faculty of EEMCS, University of Twente, Enschede, Netherlands; email: d.h.apriyanti@utwente.nl},
	publisher = {Public Library of Science},
	issn = {19326203},
	coden = {POLNC},
	pmid = {34705870},
	language = {English},
	abbrev_source_title = {PLoS ONE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Hwang2021,
	author = {Hwang, Sung-Wook and Sugiyama, Junji},
	title = {Evaluation of image partitioning strategies for preserving spatial information of cross-sectional micrographs in automated wood recognition of Fagaceae},
	year = {2021},
	journal = {Journal of Wood Science},
	volume = {67},
	number = {1},
	doi = {10.1186/s10086-021-01953-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101880690&doi=10.1186%2fs10086-021-01953-z&partnerID=40&md5=c5d23fb8e4938caf62e338e3646fb3aa},
	affiliations = {Graduate School of Agriculture, Kyoto University, Kyoto, 606-8502, Japan; College of Materials Science and Engineering, Nanjing Forestry University, Nanjing, 210037, China},
	abstract = {Although wood cross sections contain spatiotemporal information regarding tree growth, computer vision-based wood identification studies have traditionally favored disordered image representations that do not take such information into account. This paper describes image partitioning strategies that preserve the spatial information of wood cross-sectional images. Three partitioning strategies are designed, namely grid partitioning based on spatial pyramid matching and its variants, radial and tangential partitioning, and their recognition performance is evaluated for the Fagaceae micrograph dataset. The grid and radial partitioning strategies achieve better recognition performance than the bag-of-features model that constitutes their underlying framework. Radial partitioning, which is a strategy for preserving spatial information from pith to bark, further improves the performance, especially for radial-porous species. The Pearson correlation and autocorrelation coefficients produced from radially partitioned sub-images have the potential to be used as auxiliaries in the construction of multi-feature datasets. The contribution of image partitioning strategies is found to be limited to species recognition and is unremarkable at the genus level. © 2021, The Author(s).},
	author_keywords = {Computer vision; Image recognition; Spatial pyramid matching; Wood identification},
	keywords = {Bags; Computers; Images; Performance; Preservation; Sections; Trees; Wood; Correlation methods; Autocorrelation coefficient; Cross sectional image; Image representations; Partitioning strategies; Pearson correlation; Spatial informations; Spatial Pyramid Matching; Spatiotemporal information; Wood},
	correspondence_address = {J. Sugiyama; Graduate School of Agriculture, Kyoto University, Kyoto, 606-8502, Japan; email: sugiyama.junji.6m@kyoto-u.ac.jp},
	publisher = {Springer},
	issn = {14350211},
	coden = {JWSCF},
	language = {English},
	abbrev_source_title = {J. Wood Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Ran2021,
	author = {Ran, Juan and Shi, Yu and Yu, Jinhao and Li, Delong},
	title = {A Multi-Feature Convolution Neural Network for Automatic Flower Recognition},
	year = {2021},
	journal = {Journal of Circuits, Systems and Computers},
	volume = {30},
	number = {15},
	doi = {10.1142/S0218126621502819},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108229201&doi=10.1142%2fS0218126621502819&partnerID=40&md5=de90a6bda292f5f4e279438b7d87f38d},
	affiliations = {Department of Computer Science and Technology, Tianjin University Renai College, Tianjin, China},
	abstract = {This paper discusses how to efficiently recognize flowers based on a convolutional neural network (CNN) using multiple features. Our proposed work consists of three phases including segmentation by Otsu thresholding with particle swarm optimization algorithms, feature extraction of color, shape, texture and recognition with the LeNet-5 neural network. In the feature extraction, an improved H component with the definition of WGB value is applied to extract the color feature, and a new algorithm based on local binary pattern (LBP) is proposed to enhance the accuracy of texture extraction. Besides this, we replace ReLU with Mish as activation function in the network design, and therefore increase the accuracy by 8% accuracy according to our comparison. The Oxford-102 and Oxford-17 datasets are adopted for benchmarking. The experimental results show that the combination of color features and texture features generates the highest recognition accuracy as 92.56% on Oxford-102 and 93% on Oxford-17.  © 2021 World Scientific Publishing Company.},
	author_keywords = {Convolutional neural network; Feature extraction; Image segmentation; Mish},
	keywords = {Color; Convolution; Extraction; Feature extraction; Particle swarm optimization (PSO); Textures; Activation functions; Convolution neural network; Flower recognition; Local binary patterns; Otsu thresholding; Particle swarm optimization algorithm; Recognition accuracy; Texture extraction; Convolutional neural networks},
	correspondence_address = {Y. Shi; Department of Computer Science and Technology, Tianjin University Renai College, Tianjin, China; email: sonia_yushi93@outlook.com},
	publisher = {World Scientific},
	issn = {02181266},
	coden = {JCSME},
	language = {English},
	abbrev_source_title = {J. Circuits Syst. Comput.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Scholl2021,
	author = {Scholl, Victoria M. and McGlinchy, Joseph and Price-Broncucia, Teo and Balch, Jennifer K. and Joseph, Maxwell B.},
	title = {Fusion neural networks for plant classification: Learning to combine RGB, hyperspectral, and lidar data},
	year = {2021},
	journal = {PeerJ},
	volume = {9},
	doi = {10.7717/peerj.11790},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111664935&doi=10.7717%2fpeerj.11790&partnerID=40&md5=e10867b6258a1e5f45c0dd9e6e9f60d5},
	affiliations = {Earth Lab, Cooperative Institute for Research in Environmental Science, University of Colorado at Boulder, Boulder, CO, United States; Department of Geography, University of Colorado at Boulder, Boulder, CO, United States; Department of Computer Science, University of Colorado at Boulder, Boulder, CO, United States},
	abstract = {Airborne remote sensing offers unprecedented opportunities to efficiently monitor vegetation, but methods to delineate and classify individual plant species using the collected data are still actively being developed and improved. The Integrating Data science with Trees and Remote Sensing (IDTReeS) plant identification competition openly invited scientists to create and compare individual tree mapping methods. Participants were tasked with training taxon identification algorithms based on two sites, to then transfer their methods to a third unseen site, using field-based plant observations in combination with airborne remote sensing image data products from the National Ecological Observatory Network (NEON). These data were captured by a high resolution digital camera sensitive to red, green, blue (RGB) light, hyperspectral imaging spectrometer spanning the visible to shortwave infrared wavelengths, and lidar systems to capture the spectral and structural properties of vegetation. As participants in the IDTReeS competition, we developed a two-stage deep learning approach to integrate NEON remote sensing data from all three sensors and classify individual plant species and genera. The first stage was a convolutional neural network that generates taxon probabilities from RGB images, and the second stage was a fusion neural network that “learns” how to combine these probabilities with hyperspectral and lidar data. Our two-stage approach leverages the ability of neural networks to flexibly and automatically extract descriptive features from complex image data with high dimensionality. Our method achieved an overall classification accuracy of 0.51 based on the training set, and 0.32 based on the test set which contained data from an unseen site with unknown taxa classes. Although transferability of classification algorithms to unseen sites with unknown species and genus classes proved to be a challenging task, developing methods with openly available NEON data that will be collected in a standardized format for 30 years allows for continual improvements and major gains for members of the computational ecology community. We outline promising directions related to data preparation and processing techniques for further investigation, and provide our code to contribute to open reproducible science efforts. © Copyright 2021 Scholl et al.},
	author_keywords = {Airborne remote sensing; Data science competition; Deep learning; Machine learning; National ecological observatory network; Neural networks; Open science; Remote sensing; Species classification},
	correspondence_address = {V.M. Scholl; Earth Lab, Cooperative Institute for Research in Environmental Science, University of Colorado at Boulder, Boulder, United States; email: Victoria.Scholl@colorado.edu},
	publisher = {PeerJ Inc.},
	issn = {21678359},
	language = {English},
	abbrev_source_title = {PeerJ},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Pushpa2022773,
	author = {Pushpa, B.R. and Shobha Rani, N.},
	title = {A simple and efficient technique for leaf extraction in complex backgrounds of low resolution mobile photographed images},
	year = {2022},
	journal = {Journal of Intelligent and Fuzzy Systems},
	volume = {43},
	number = {1},
	pages = {773 – 789},
	doi = {10.3233/JIFS-212451},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131730561&doi=10.3233%2fJIFS-212451&partnerID=40&md5=1bdab6cc86e39284b5bb59096526f989},
	affiliations = {Department of Computer Science, Amrita School of Arts and Sciences, Mysuru Campus, Amrita Vishwa Vidyapeetham, India},
	abstract = {Low resolution mobile photographed images pose a complex set of research challenges as compared to non-mobile captured images, which really is a significant issue these days. For non-mobile captured and high-resolution photos, current plant recognition systems are the best solution providers. This study proposes the identification and extraction of leaf regions from complex backgrounds to meet the automatic recognition needs of a variety of mobile phone users. Additionally multiple factors complicate the leaf region extraction from complex backgrounds such as varying background patterns, clutters, varying leaf shape/size and varying illumination due to volatile weather conditions. In this paper, a simple and efficient method for leaf extraction from complex background of mobile photographed low resolution images is proposed based on color channel thresholding and morphological operations. A self-built database of 5000 mobile photographed images in realistic environments is adapted for experimentations. Experiments were conducted on various resolution categories, and it was discovered that the proposed model has an average dice similarity measure of 99.5 percent for successful extraction of the leaf region in 13MP mobile photographed images. Furthermore, our comparative investigation reveals that the suggested model outperforms both traditional and state-of-the-art techniques.  © 2022 - IOS Press. All rights reserved.},
	author_keywords = {color thresholding; gradient image analysis; Leaf extraction; mobile camera images; morphological operations; realistic backgrounds},
	keywords = {Image processing; Mathematical morphology; Camera images; Color thresholding; Gradient image analyse; Gradient images; Image-analysis; Leaf extraction; Mobile camera; Mobile camera image; Morphological operations; Realistic background; Extraction},
	correspondence_address = {N. Shobha Rani; Department of Computer Science, Amrita School of Arts and Sciences, Mysuru Campus, Amrita Vishwa Vidyapeetham, India; email: n_shobharani@my.amrita.edu},
	publisher = {IOS Press BV},
	issn = {10641246},
	language = {English},
	abbrev_source_title = {J. Intelligent Fuzzy Syst.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}@CONFERENCE{Li2021,
	author = {Li, Xiaoxue and Lv, Rongxin and Yin, Yanzhen and Xin, Kangkang and Liu, Zeyuan and Li, Zhongzhi},
	title = {Flower image classification based on generative adversarial network and transfer learning},
	year = {2021},
	journal = {IOP Conference Series: Earth and Environmental Science},
	volume = {647},
	number = {1},
	doi = {10.1088/1755-1315/647/1/012180},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101708336&doi=10.1088%2f1755-1315%2f647%2f1%2f012180&partnerID=40&md5=4e96561fafefc1c8c75519dfc3940c29},
	affiliations = {College of Electronic and Information Engineering, Shenyang Aerospace University, Shenyang, Liaoning, 110136, China; College of Innovation and Entrepreneurship, Shenyang Aerospace University, Shenyang, Liaoning, 110136, China; College of Science, Shenyang Aerospace University, Shenyang, Liaoning, 110136, China; Academy of Aeronautics and Astronautics, Shenyang Aerospace University, Shenyang, Liaoning, 110136, China; College of Computer Science, Shenyang Aerospace University, Shenyang, Liaoning, 110136, China},
	abstract = {Aiming at the problem that the classification accuracy of the traditional flower classification method is low and the deep neural network requires a large amount of original data. This paper designs a flower classification model that combines generative adversarial network and ResNet-101 transfer learning algorithm, and uses stochastic gradient descent algorithm to optimize the training process of the model. The experimental results on the the international public flower recognition dataset, Oxford flower-102 dataset, show that by enhancing the original data, the accuracy of the network's recognition and classification of flowers is improved. At the same time, the model proposed in this paper is superior to other traditional network models, with higher recognition accuracy and robustness. © Content from this work may be used under the terms of the Creative Commons Attribution 3.0 licence.},
	keywords = {Classification (of information); Deep learning; Deep neural networks; Energy resources; Gradient methods; Image classification; Stochastic models; Stochastic systems; Transfer learning; Adversarial networks; Classification accuracy; Classification methods; Classification models; Flower recognition; Recognition accuracy; Stochastic gradient descent algorithm; Training process; Learning algorithms},
	correspondence_address = {Z. Li; College of Computer Science, Shenyang Aerospace University, Shenyang, Liaoning, 110136, China; email: sau_lzz@email.sau.edu.cn},
	publisher = {IOP Publishing Ltd},
	issn = {17551307},
	language = {English},
	abbrev_source_title = {IOP Conf. Ser. Earth Environ. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2020 6th International Conference on Advances in Energy Resources and Environment Engineering, ICAESEE 2020; Conference date: 20 November 2020 through 22 November 2020; Conference code: 167242; All Open Access, Bronze Open Access}
}

@ARTICLE{Ananth Pai2021303,
	author = {Ananth Pai, K. and Apoorva, B.R. and Mendonca, Daisy Sheetal and Hegde, Durgaprasad S. and Hegde, Roopa B.},
	title = {Development of an Automated Plant Classification System Using Deep Learning Approach},
	year = {2021},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {752 LNEE},
	pages = {303 – 315},
	doi = {10.1007/978-981-16-0443-0_25},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105869380&doi=10.1007%2f978-981-16-0443-0_25&partnerID=40&md5=8105b6c86d741e86f6c7099dd7fee907},
	affiliations = {NMAM Institute of Technology, Nitte (Visvesvaraya Technological University, Belagavi), Udupi, Karnataka, 574110, India},
	abstract = {Plants being the most widely distributed species play a vital role in maintaining balance in the environment. They are a fundamental part of people’s lives and an indispensable part of the ecosystem. A large number of plants are reaching the endangered state with some being extinct making their conservation and rapid identification is a great concern. Manual searching and identification of these plants in the wild might consume a lot of time and manpower. Hence, there is a need for the development of an automated plant identification system. In this work, we propose an automated plant identification system using a deep learning approach. Leaf images are used for the classification of plants. In this study, several experiments are carried out using color and grayscale images of different sizes of input images. We obtained an average accuracy of 98.3% for the identification of six types of plants using color images. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Convolutional neural network; Data augmentation; Deep learning; Multiclass classification; Raspberry Pi},
	keywords = {Automation; Embedded systems; Internet of things; Learning systems; Power electronics; Signal processing; VLSI circuits; Color images; Different sizes; Gray-scale images; Leaf images; Learning approach; Plant classification; Plant identification systems; Rapid identification; Deep learning},
	correspondence_address = {K. Ananth Pai; NMAM Institute of Technology, Nitte (Visvesvaraya Technological University, Belagavi), Udupi, Karnataka, 574110, India; email: ananthpai9@gmail.com},
	editor = {Kalya S. and Kulkarni M. and Shivaprakasha K.S.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981160442-3},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: International Conference on VLSI, Signal Processing, Power Electronics, IoT, Communication and Embedded Systems, VSPICE 2020; Conference date: 22 December 2020 through 23 December 2020; Conference code: 257909}
}

@ARTICLE{Kim202123,
	author = {Kim, Gulnar and Demyanenko, Alexandr and Savostin, Alexey and Iklassova, Kainizhamal},
	title = {DEVISING A METHOD FOR RECOGNIZING THE CAUSES OF DEVIATIONS IN THE DEVELOPMENT OF THE PLANT ALOE ARBORESCENS L. USING MACHINE LEARNING CAPABILITIES},
	year = {2021},
	journal = {Eastern-European Journal of Enterprise Technologies},
	volume = {2},
	number = {2-110},
	pages = {23 – 31},
	doi = {10.15587/1729-4061.2021.228219},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107722191&doi=10.15587%2f1729-4061.2021.228219&partnerID=40&md5=0d20f48e845a1361614269221f6d4df7},
	affiliations = {Department of Information and Communication Technologies, Manash Kozybayev North Kazakhstan University, Pushkin str., 86, Petropavlovsk, 150000, Kazakhstan; Department of Energetic and Radioelectronics, Manash Kozybayev North Kazakhstan University, Pushkin str., 86, Petropavlovsk, 150000, Kazakhstan},
	abstract = {This paper considers the process of developing a method to recognize the causes of plant growth deviations from normal using the advancements in artificial intelligence. The medicinal plant Aloe arborescens L. was chosen as the object of this research given that this plant had been for decades one of the best-selling new products in the world. Aloe arborescens L. is famous for its medicinal properties used in medicine, cosmetology, and even the food industry. Diagnosing the abnormalities in the plant development in a timely and accurate manner plays an important role in preventing the loss of crop production yields. The current study has built a method for recognizing the causes of abnormalities in the development of Aloe arborescens L. caused by a lack of watering or lighting, based on the use of transfer training of the VGG-16 convolutional neural network (United Kingdom). A given architecture is aimed at recognizing objects in images, which is the main reason for using it to achieve the goal set. The analysis of the quality metrics of the proposed image classification process by specified classes has revealed high recognition reliability (for a normally developing plant, 91%; for a plant without proper watering, 89%; and for a plant without proper lighting, 83%). The analysis of the validity of test sample recognition has demonstrated a similar validity of the plant’s classification to one of three classes: 92.6%; 87.5%; and 85.5%, respectively. The results reported here make it possible to supplement the automated systems that control the mode parameters of hydroponic installations by the world’s major producers with the main feedback on the deviation of the plant’s development from the specified values © 2021. All Rights Reserved.},
	author_keywords = {Aloe arborescens L; hydroponic systems; image recognition; machine learning; neural network},
	correspondence_address = {G. Kim; Department of Information and Communication Technologies, Manash Kozybayev North Kazakhstan University, Petropavlovsk, Pushkin str., 86, 150000, Kazakhstan; email: halle-alison@mail.ru},
	publisher = {Technology Center},
	issn = {17293774},
	language = {English},
	abbrev_source_title = {East. Eur. J. Enterp. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Gu20212189,
	author = {Gu, Jian and Yu, Pengfei and Lu, Xinwei and Ding, Wenqian},
	title = {Leaf species recognition based on VGG16 networks and transfer learning},
	year = {2021},
	journal = {IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)},
	pages = {2189 – 2193},
	doi = {10.1109/IAEAC50856.2021.9390789},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104664145&doi=10.1109%2fIAEAC50856.2021.9390789&partnerID=40&md5=d7b645f8dfee0ac39b610ebf61fbddf5},
	affiliations = {Yunnan University, School of Information Science Engineering, Yunnan, China},
	abstract = {Compared with other parts of the plant, leaves are easy to obtain and have obvious features. Therefore, leaf identification is also considered as one of the best methods for plant identification. For this reason, a transfer learning leaf recognition method based on VGG16 deep learning network is proposed. This method firstly carries out preprocess operations such as background whitening and standard normalization processing, then carries out data augmentation, then uses transfer learning to train VGG16 network model, and finally completes leaf recognition. The experimental results of Middle European Woody (MEW) plants data set and UCI Folio Leaf data set show that the recognition accuracy under a single background reaches 93.4% and 97.9%, respectively, which are significantly higher than traditional convolutional neural network and other recognition methods, effectively improving the recognition accuracy rate of plant leaves. © 2021 IEEE.},
	author_keywords = {image classification; leaf recognition; preprocess; transfer learning; VGG16},
	keywords = {Convolutional neural networks; Deep learning; Learning systems; Plants (botany); Background whitening; Data augmentation; Leaf identification; Leaf recognition; Plant identification; Recognition accuracy; Recognition methods; Species recognition; Transfer learning},
	issn = {26896621},
	language = {English},
	abbrev_source_title = {Adv. Inf. Tech. Electron. and Autom. Control Conf.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 5th IEEE Advanced Information Technology, Electronic and Automation Control Conference, IAEAC 2021; Conference date: 12 March 2021 through 14 March 2021; Conference code: 168296}
}

@ARTICLE{Huang20212557,
	author = {Huang, Penggui and Zhao, Fan and Zhu, Zheng and Zhang, Yanfeng and Li, Xiaoping and Wu, Zhangkang},
	title = {Application of Variant Transfer Learning in Wood Recognition},
	year = {2021},
	journal = {BioResources},
	volume = {16},
	number = {2},
	pages = {2557 – 2569},
	doi = {10.15376/biores.16.2.2557-2569},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124215740&doi=10.15376%2fbiores.16.2.2557-2569&partnerID=40&md5=d9adeaf7e1628dc1844dc7db10d21bfa},
	affiliations = {School of Big Data and Intelligent Engineering, Southwest Forestry University, Kunming, 650224, China; School of Materials Engineering, Southwest Forestry University, Kunming, 650224, China},
	abstract = {Wood is a material commonly found in nature and is widely used in all professions and industries. Because wood has varied growth cycles and physical properties, there are large differences in its usage and commercial price. In addition, some woods are nationally protected species. Therefore, it is of great importance to accurately identify the type of wood. Traditional wood recognition methods rely on experts and specialized equipment. To facilitate wood recognition, this paper proposes an approach for wood recognition using images. Next, a transfer learning technology was used to extract the textural features of wood, and a global average pooling (GAP) layer was used to reduce the number of features. Finally, the extreme learning machine (ELM) was used for classification. The recognition accuracy of this approach for the Wood Species Dataset was 93.07%, which was higher than the method used by the data provider. This approach had a higher recognition accuracy and a more stable recognition performance than previous approaches. © 2021, North Carolina State University. All rights reserved.},
	author_keywords = {Extreme learning machine; Transfer learning; Wood recognition},
	keywords = {Accuracy; Classification; Data; Performance; Technology; Transfer; Wood Species; Knowledge acquisition; Machine learning; Extreme learning machine; Growth cycle; Learning machines; Learning technology; Recognition accuracy; Recognition methods; Specialized equipment; Textural feature; Transfer learning; Wood recognition; Wood},
	correspondence_address = {F. Zhao; School of Big Data and Intelligent Engineering, Southwest Forestry University, Kunming, 650224, China; email: fzhao@swfu.edu.cn},
	publisher = {North Carolina State University},
	issn = {19302126},
	language = {English},
	abbrev_source_title = {BioResour.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Bronze Open Access}
}

@ARTICLE{Mahajan20211,
	author = {Mahajan, Shubham and Raina, Akshay and Gao, Xiao-Zhi and Pandit, Amit Kant},
	title = {Plant recognition using morphological feature extraction and transfer learning over SVM and adaboost},
	year = {2021},
	journal = {Symmetry},
	volume = {13},
	number = {2},
	pages = {1 – 16},
	doi = {10.3390/sym13020356},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102391605&doi=10.3390%2fsym13020356&partnerID=40&md5=723c25e18d822f238421e168de53d423},
	affiliations = {School of Electronics & Communication, Shri Mata Vaishno Devi University, Katra, 182320, India; School of Electrical Engineering, Shri Mata Vaishno Devi University, Katra, 182320, India; School of Computing, University of Eastern Finland, Kuopio, 70210, Finland},
	abstract = {Plant species recognition from visual data has always been a challenging task for Artificial Intelligence (AI) researchers, due to a number of complications in the task, such as the enormous data to be processed due to vast number of floral species. There are many sources from a plant that can be used as feature aspects for an AI-based model, but features related to parts like leaves are considered as more significant for the task, primarily due to easy accessibility, than other parts like flowers, stems, etc. With this notion, we propose a plant species recognition model based on morphological features extracted from corresponding leaves’ images using the support vector machine (SVM) with adaptive boosting technique. This proposed framework includes the pre-processing, extraction of features and classification into one of the species. Various morphological features like centroid, major axis length, minor axis length, solidity, perimeter, and orientation are extracted from the digital images of various categories of leaves. In addition to this, transfer learning, as suggested by some previous studies, has also been used in the feature extraction process. Various classifiers like the kNN, decision trees, and multilayer perceptron (with and without AdaBoost) are employed on the opensource dataset, FLAVIA, to certify our study in its robustness, in contrast to other classifier frameworks. With this, our study also signifies the additional advantage of 10-fold cross validation over other dataset partitioning strategies, thereby achieving a precision rate of 95.85%. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {AdaBoost; Feature extraction; Feature selection; Non-separable data; Plant species recognition; SVM; Transfer learning},
	correspondence_address = {S. Mahajan; School of Electronics & Communication, Shri Mata Vaishno Devi University, Katra, 182320, India; email: 19dec001@smvdu.ac.in},
	publisher = {MDPI AG},
	issn = {20738994},
	language = {English},
	abbrev_source_title = {Symmetry},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; All Open Access, Gold Open Access}
}

@CONFERENCE{Hridoy2021,
	author = {Hridoy, Rashidul Hasan and Akter, Fatema and Afroz, Maisha},
	title = {An Efficient Computer Vision Approach for Rapid Recognition of Poisonous Plants by Classifying Leaf Images using Transfer Learning},
	year = {2021},
	journal = {2021 12th International Conference on Computing Communication and Networking Technologies, ICCCNT 2021},
	doi = {10.1109/ICCCNT51525.2021.9580011},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126208559&doi=10.1109%2fICCCNT51525.2021.9580011&partnerID=40&md5=79395d99257ca7ad798dc2156911e7c5},
	affiliations = {Department of Computer Science and Engineering, Daffodil International University, Dhaka, Bangladesh},
	abstract = {Livestock poisoning by several kinds of poisonous plants causes grievous economic losses to the livestock industry. Poisonous plants are also a fatal threat to humans, ingesting these plants can cause several side effects in the body because of their toxicity. Hence, it is essential to develop a rapid approach to recognize poisonous plants efficiently. This paper addresses a recognition approach for eighteen poisonous plants using poisonous plants leaf (PPL) dataset which has been generated using image augmentation techniques that contains 54000 training, 27000 validation, and 9000 testing images. Six different state-of-the-art deep learning models have been used in this study such as Xception, ResNet152V2, InceptionResNetV2, MobileNetV2, DenseNet201, and NASNetLarge for classifying leaf images of poisonous plants. Xception has shown more significant performance than other models, achieved 99.71% training and 99.37% testing accuracy. NASNetLarge and InceptionResNetV2 have achieved 96.89% and 95.18% test accuracy, respectively, and MobileNetV2 achieved the lowest test accuracy. © 2021 IEEE.},
	author_keywords = {Deep Learning; Depthwise Separable Convolutions; Poisonous Plants Recognition; Transfer Learning; Xception},
	keywords = {Computer vision; Deep learning; Image classification; Losses; Statistical tests; Deep learning; Depthwise separable convolution; Economic loss; Leaf images; Plant recognition; Poisonoi plant recognition; Side effect; Test accuracy; Transfer learning; Xception; Agriculture},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818595-8},
	language = {English},
	abbrev_source_title = {Int. Conf. Comput. Commun. Netw. Technol., ICCCNT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 12th International Conference on Computing Communication and Networking Technologies, ICCCNT 2021; Conference date: 6 July 2021 through 8 July 2021; Conference code: 177114}
}

@ARTICLE{Musa2021244,
	author = {Musa, Md. and Arman, Md. Shohel and Hossain, Md. Ekram and Thusar, Ashraful Hossen and Nisat, Nahid Kawsar and Islam, Arni},
	title = {Classification of Immunity Booster Medicinal Plants Using CNN: A Deep Learning Approach},
	year = {2021},
	journal = {Communications in Computer and Information Science},
	volume = {1440 CCIS},
	pages = {244 – 254},
	doi = {10.1007/978-3-030-81462-5_23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119009507&doi=10.1007%2f978-3-030-81462-5_23&partnerID=40&md5=2997d948df2aa8b82da321d695bdfa2a},
	affiliations = {Daffodil International University, Dhaka, 1207, Bangladesh},
	abstract = {Environment has blessed us with various kinds of plants. Some of them uses as resources of medicines as it is called medicinal plant. In Bangladesh medicinal plants are also known as Ayurveda, Homeopathy and Unani. Experts says medicinal plants can be very useful in the fight with recent pandemic which is Covid-19. As we know health of a body depends on its immune system, so it is important to keep immunity stronger. Strong immune system can be influential to any infectious virus, bacteria and pathogens. On the other hand, inactive one can get easily infected with virus and other illness. There are certain medicinal plants which reinforce our immunity. Therefore, classification of these medical plants is very important. For this classification we have collected leaf images for six different classes which’s local names are Darchini, Tulshi, Tejpata, Sojne, Neem, Pathorkuchi. In this article we introduced a famous algorithm for classification named CNN (Convolutional neural network). We used CNN (Convolutional neural network) to recognize the plant from leaf images and got 95.58% accuracy. In future infectious virus can appear which can be more threatening than others, our research will help people to know about immune system and medicinal plants which reinforce our immunity, so that they can fight with diseases and viruses. © 2021, Springer Nature Switzerland AG.},
	author_keywords = {Convolutional neural network; Immunity system; Medicinal plant; Plant classification},
	keywords = {Convolution; Convolutional neural networks; Disease control; Diseases; Immune system; Plants (botany); Viruses; Bangladesh; Convolutional neural network; Immunity boosters; Immunity system; Infectious pathogens; Infectious virus; Leaf images; Learning approach; Medicinal plants; Plant classification; Deep learning},
	correspondence_address = {M. Musa; Daffodil International University, Dhaka, 1207, Bangladesh; email: musa35-1870@diu.edu.bd},
	editor = {Singh M. and Tyagi V. and Gupta P.K. and Flusser J. and Ören T. and Sonawane V.R.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18650929},
	isbn = {978-303081461-8},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 5th International Conference on Advances in Computing and Data Sciences, ICACDS 2021; Conference date: 23 April 2021 through 24 April 2021; Conference code: 267269}
}

@CONFERENCE{Neethu2021,
	author = {Neethu, S. and Baby Syla, L.},
	title = {Wood Species Recognition Using Machine Learning},
	year = {2021},
	journal = {Proceedings of the 4th International Conference on Microelectronics, Signals and Systems, ICMSS 2021},
	doi = {10.1109/ICMSS53060.2021.9673601},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125340081&doi=10.1109%2fICMSS53060.2021.9673601&partnerID=40&md5=0a16ff72aa71e1016f5efb7ec85aed0e},
	affiliations = {A P J Abdul Kalam Technical University, Department of Computer Applications, CET, Trivandrum, India},
	abstract = {Although a computer vision-based automated wood species detection system is not extensively employed today, it is in high demand in a range of industries. Because each species of wood has distinct features and traits that determine how it is used and implemented. Human experts have  traditionally been used nearly entirely in the identifying process, which is based on a variety of features of wood, such as texture, structure, and colour. Even competent scientists have difficulties distinguishing between different species of wood found in nature. The most difficult aspect of wood recognition is that some wood species have a texture that is identical to that of others, hence feature extraction is the most important step in the process. So in this work, two statistical feature extraction techniques are used 1) Local Binary Pattern (LBP) 2) Gray Level Co-Occurrence Matrix (GLCM). Features extracted from both methods are combined to see whether they increase classification accuracy or not. Multi-SVM is used for classification. For this study, a new dataset is created which includes the three most common tree species in Indian territory. This study found that integrating LBP and GLCM features enhanced classification accuracy. Proposed system produces a result of 97.2%, which is better than individual approaches.  © 2021 IEEE.},
	author_keywords = {GLCM; IMAGE PROCESSING; LBP; ML; SVM; TEXTURE; WOOD RECOGNITION},
	keywords = {Classification (of information); Extraction; Feature extraction; Image processing; Support vector machines; Textures; Classification accuracy; Gray-level co-occurrence matrix; Grey-level co-occurrence matrixes; Images processing; Local binary patterns; Machine-learning; ML; Species recognition; SVM; Wood recognition; Wood},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166544885-7},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Microelectron., Signals Syst., ICMSS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference on Microelectronics, Signals and Systems, ICMSS 2021; Conference date: 18 November 2021 through 19 November 2021; Conference code: 176456}
}

@ARTICLE{Patnaik2021239,
	author = {Patnaik, Vijaya and Mohanty, Monalisa and Subudhi, Asit Kumar},
	title = {Identification of healthy biological leafs using hybrid-feature classifier},
	year = {2021},
	journal = {Imaging Science Journal},
	volume = {69},
	number = {5-8},
	pages = {239 – 253},
	doi = {10.1080/13682199.2022.2157533},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145102197&doi=10.1080%2f13682199.2022.2157533&partnerID=40&md5=782675edeb0f310cc2b4e3791aa18575},
	affiliations = {Department of ECE, ITER, SOA Deemed to be University, Odisha, India},
	abstract = {Plants are an important element of the ecosystem that helps in controlling carbon emissions and environmental changes. Characterization and identification are a need for protecting plants and for people to understand plant protection. Plant leaves are the main parts for detection. Characterizing leaves now has been a significant and complicated task, particularly with the features of leaves. Leaf images of two different types are considered here, one is healthy while the other is unhealthy, and divided into two distinct classes. The proposed method incorporates features of the leaf images that are extracted utilizing the Gray Level Co-occurrence Matrix (GLCM) and Gray Level Run Length Matrix (GLRLM) feature extraction techniques. The outcomes are classified using three different classifiers: Random Forest, Multilayer Perceptron, and Naïve Bayes with an accuracy of 95.84%, 98.33%, and 82.89% respectively. The classifiers successfully classify the healthy and diseased leaves of various plants that were considered here. Hence as per the investigation, the study can be valuable for analysts for plant recognition, characterization, and diagnosis. © 2022 The Royal Photographic Society.},
	author_keywords = {GLCM; GLRLM; Leaf classification; MLP; Naive Bayes; random forest},
	keywords = {Classification (of information); Image processing; Plants (botany); Gray level run length; Gray level run length matrix; Gray-level co-occurrence matrix; Grey-level co-occurrence matrixes; Leaf classification; Leaf images; matrix; MLP; Naive bayes; Random forests; Classifiers},
	correspondence_address = {A.K. Subudhi; SOA University, Bhubaneswar, Odisha, 751030, India; email: asitsubudhi@soa.ac.in},
	publisher = {Taylor and Francis Ltd.},
	issn = {13682199},
	language = {English},
	abbrev_source_title = {Imag. Sci. J.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Goëau20211422,
	author = {Goëau, Hervé and Bonnet, Pierre and Joly, Alexis},
	title = {Overview of PlantCLEF 2021: Cross-domain plant identification},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {2936},
	pages = {1422 – 1436},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113537870&partnerID=40&md5=d98fbf8ec098d819d420ee8086c4c132},
	affiliations = {CIRAD, UMR, AMAP, Montpellier, Occitanie, France; Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France},
	abstract = {Automated plant identification has improved considerably thanks to recent advances in deep learning and the availability of training data with more and more field photos. However, this profusion of data concerns only a few tens of thousands of species, mainly located in North America and Western Europe, much less in the richest regions in terms of biodiversity such as tropical countries. On the other hand, for several centuries, botanists have systematically collected, catalogued and stored plant specimens in herbaria, especially in tropical regions, and recent efforts by the biodiversity informatics community have made it possible to put millions of digitised records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF 2021") was designed to assess the extent to which automated identification of flora in data-poor regions can be improved by using herbarium collections. It is based on a dataset of about 1,000 species mainly focused on the Guiana Shield of South America, a region known to have one of the highest plant diversities in the world. The challenge was evaluated as a cross-domain classification task where the training set consisted of several hundred thousand herbarium sheets and a few thousand photos to allow learning a correspondence between the two domains. In addition to the usual metadata (location, date, author, taxonomy), the training data also includes the values of 5 morphological and functional traits for each species. The test set consisted exclusively of photos taken in the field. This article presents the resources and evaluations of the assessment carried out, summarises the approaches and systems used by the participating research groups and provides an analysis of the main results. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
	author_keywords = {Amazon rainforest; Benchmark; Cross-domain classification; Domain adaptation; Evaluation; Fine-grained classification; Guiana shield; LifeCLEF; Plant; PlantCLEF; Species identification; Tropical flora},
	keywords = {Biodiversity; Deep learning; Image enhancement; Tropics; Automated identification; Classification tasks; Functional traits; Plant diversity; Plant identification; Research groups; Tropical countries; Tropical regions; Classification (of information)},
	correspondence_address = {H. Goëau; CIRAD, UMR, AMAP, Occitanie, Montpellier, France; email: herve.goeau@cirad.fr},
	editor = {Faggioli G. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Ferro N. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Joly A. and University of Montpellier, LIRMM, Inria ZENITH, 161 Rue Ada, Montpellier Cedex 5 and Maistro M. and University of Copenhagen, Department of Computer Science, Universitetsparken 1, Copenhagen and Piroi F. and Vienna University of Technology (TU), Institute of Information Systems Engineering, Favoritenstrasse 9-11/188, Vienna},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 2021 Working Notes of CLEF - Conference and Labs of the Evaluation Forum, CLEF-WN 2021; Conference date: 21 September 2021 through 24 September 2021; Conference code: 171327}
}

@ARTICLE{Dat2021,
	author = {Dat, Trinh Tan and Le Thien Vu, Pham Cung and Truong, Nguyen Nhat and Anh Dang, Le Tran and Thanh Sang, Vu Ngoc and Bao, Pham The},
	title = {Leaf Recognition Based on Joint Learning Multiloss of Multimodel Convolutional Neural Networks: A Testing for Vietnamese Herb},
	year = {2021},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2021},
	doi = {10.1155/2021/5032359},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116607147&doi=10.1155%2f2021%2f5032359&partnerID=40&md5=b7e082b8bf7b7c943d98fb1732147346},
	affiliations = {Information Science Faculty, Sai Gon University, Ho Chi Minh City, Viet Nam},
	abstract = {A new modification of multi-CNN ensemble training is investigated by combining multiloss functions from state-of-the-art deep CNN architectures for leaf image recognition. We first apply the U-Net model to segment leaf images from the background to improve the performance of the recognition system. Then, we introduce a multimodel approach based on a combination of loss functions from the EfficientNet and MobileNet (called as multimodel CNN (MMCNN)) to generalize a multiloss function. The joint learning multiloss model designed for leaf recognition allows each network to perform its task and cooperate with the others simultaneously, where knowledge from various trained deep networks is shared. This cooperation-proposed multimodel is forced to deal with more complicated problems rather than a simple classification. Therefore, the network can learn much rich information and improve its generalization capability. Furthermore, a multiloss trade-off strategy between two deep learning models can reduce the effect of redundancy problems in ensemble classifiers. The performance of our approach is evaluated by our custom Vietnamese herbal leaf species dataset, and public datasets such as Flavia, Leafsnap, and Folio are used to build test cases. The results confirm that our approach enhances the leaf recognition performance and outperforms the current standard single networks while having less low computation cost. © 2021 Trinh Tan Dat et al.},
	keywords = {Asian Continental Ancestry Group; Humans; Neural Networks, Computer; Plant Leaves; Convolutional neural networks; Deep learning; Economic and social effects; Image enhancement; Image segmentation; Statistical tests; Transfer learning; Convolutional neural network; Joint learning; Leaf images; Leaf recognition; Multi-modelling; Net model; Performance; Recognition systems; State of the art; Vietnamese; Asian continental ancestry group; human; plant leaf; Image recognition},
	correspondence_address = {P.T. Bao; Information Science Faculty, Sai Gon University, Ho Chi Minh City, Viet Nam; email: ptbao@sgu.edu.vn},
	publisher = {Hindawi Limited},
	issn = {16875265},
	pmid = {34603432},
	language = {English},
	abbrev_source_title = {Comput. Intell. Neurosci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Abdulazeez20211,
	author = {Abdulazeez, Adnan Mohsin and Zeebaree, Diyar Qader and Zebari, Dilovan Asaad and Hameed, Thamer Hassan},
	title = {Leaf identification based on shape, color, texture and vines using probabilistic neural network},
	year = {2021},
	journal = {Computacion y Sistemas},
	volume = {25},
	number = {3},
	pages = {1 – 15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118782983&partnerID=40&md5=f12f252c6b353dd5398f80cbc6384b83},
	affiliations = {Presidency of Duhok Polytechnic University, Kurdistan Region, Duhok, Iraq; Research Center of Duhok Polytechnic University, Iraq; University of Duhok, College of Agricultural Engineering Sciences, Iraq},
	abstract = {The importance of the plant for the human being and the environment led to deeply been studied and classified in detail. The advancement of the technology is the main factor in finding many ways for plant identification process. Some kind of initial intelligence systems in order to identify plant, followed by many theories and concepts using methods like; Moment Invariant (MI), Zernike Moments (ZM) and Polar Fourier Transform (PFT), and technologies for classification like; Neural Network (NN), K-Nearest Neighbor Classifier (KNN) and Support Vector Machine (SVM), were used by many researchers through past years. In this paper is Centroid-Radii (C-R) combined with geometric features of the leaves, in order to cover most of shape feature of the leaves, color moments and Grey-Level Co-occurrence Matrix (GLCM) to improve the accuracy of the system identification. in addition to the above features, Veins also involved in the method been used plus Principal Component Analysis (PCA), which is used to convert features into orthogonal features and the results were inputted to the classifiers that used Probabilistic Neural Network (PNN). Two datasets have been used for test, first dataset is created especially for this work and collected from 24 kinds of plants and second dataset is called Flavia, which contains 32 kinds. The results were clearly improved to identify the plants. The maximum accuracy reached up to 98.50%, when using the first data set and 98.16% for the second dataset. © 2021 Instituto Politecnico Nacional. All rights reserved.},
	author_keywords = {Centroid-Radii; Geometric feature extraction; Principal component analysis; Probabilistic neural network},
	correspondence_address = {D.Q. Zeebaree; Research Center of Duhok Polytechnic University, Iraq; email: diloven.zebari@dpu.edu.krd},
	publisher = {Instituto Politecnico Nacional},
	issn = {14055546},
	language = {English},
	abbrev_source_title = {Comput. Sist.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{Balestra2021155,
	author = {Balestra, Mattia and Chiappini, Stefano and Malinverni, Eva Savina and Galli, Andrea and Marcheggiani, Ernesto},
	title = {A Machine Learning Approach for Mapping Forest Categories: An Application of Google Earth Engine for the Case Study of Monte Sant’Angelo, Central Italy},
	year = {2021},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12955 LNCS},
	pages = {155 – 168},
	doi = {10.1007/978-3-030-87007-2_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116004594&doi=10.1007%2f978-3-030-87007-2_12&partnerID=40&md5=ce6a2723e6e627635b9e6fcef2348e05},
	affiliations = {Dipartimento di Scienze Agrarie, Alimentari e Ambientali (D3A), Università Politecnica delle Marche, Via Brecce Bianche 10, Ancona, 60131, Italy; Dipartimento d’Ingegneria Civile, Edile e Architettura (DICEA), Università Politecnica delle Marche, Via Brecce Bianche 12, Ancona, 60131, Italy; Department of Earth and Environmental Sciences, KU Leuven, Leuven, 3001, Belgium},
	abstract = {Remote Sensing plays a critical role in forest tree species identification. Regarding the current debate on earth observation and monitoring, many see this valuable technology useful for a wide range of purposes, including forest conservation and management. This paper focuses on a workflow for mapping forest tree species from satellite images, by statistic algorithms and machine learning. Among the world satellite platforms, the Sentinel-2 program was selected to investigate the mixed forest area of Monte Sant’Angelo in Central Italy. A list of monthly images from 2018 to 2020 have been processed using the Google Earth Engine geospatial processing service. The process includes the computation of vegetation indexes like the Normalized Difference Vegetation Index (NDVI), Transformed Difference Vegetation Index (TDVI), the Enhanced Vegetation Index (EVI) and the Green Normalized Difference Vegetation Index (GNDVI). A forest class time series was generated for each index. The Artificial Intelligence algorithm models (Machine Learning) were trained identifying accurate ground truths. Principal Component Analysis (PCA) was performed to reduce variables redundancy in Random Forest classifications. Four forest categories have been identified: holm oak woodlands, conifer reforestation, Ostryo Carpinion alliance and mixed hardwood forest. Due to the phenological differences among species, the classification global accuracy ranges from 70% to 80%. © 2021, Springer Nature Switzerland AG.},
	author_keywords = {Google Earth Engine; PCA; Random forest; Sentinel-2; Time-series; Tree species classification},
	keywords = {Decision trees; Engines; Learning algorithms; Machine learning; Mapping; Reforestation; Remote sensing; Time series; Vegetation; Central Italy; Google earth engine; Google earths; Principal-component analysis; Random forests; Sentinel-2; Species classification; Times series; Tree species; Tree species classification; Principal component analysis},
	correspondence_address = {S. Chiappini; Dipartimento di Scienze Agrarie, Alimentari e Ambientali (D3A), Università Politecnica delle Marche, Ancona, Via Brecce Bianche 10, 60131, Italy; email: m.balestra@pm.univpm.it},
	editor = {Gervasi O. and Murgante B. and Misra S. and Garau C. and Blečić I. and Taniar D. and Apduhan B.O. and Rocha A.M. and Tarantino E. and Torre C.M.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303087006-5},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 21st International Conference on Computational Science and Its Applications, ICCSA 2021; Conference date: 13 September 2021 through 16 September 2021; Conference code: 265069}
}

@ARTICLE{Bakhshipour2021,
	author = {Bakhshipour, Adel},
	title = {Cascading feature filtering and boosting algorithm for plant type classification based on image features},
	year = {2021},
	journal = {IEEE Access},
	doi = {10.1109/ACCESS.2021.3086269},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107360826&doi=10.1109%2fACCESS.2021.3086269&partnerID=40&md5=761e5e5b0b25b32f1dc070e4df99f757},
	affiliations = {Department of Biosystems Engineering, Faculty of Agricultural Sciences, University of Guilan, Rasht, Iran. (e-mail: abakhshipour@guilan.ac.ir)},
	abstract = {Crop and weeds identification is of important steps towards the development of efficient automotive weed control systems. The higher the accuracy of plant detection and classification, the higher the performance of the weeding machine. In this study, the capability of two popular boosting methods including Adaboost.M1 and LogitBoost algorithms was evaluated to enhance the plant classification performance of four classifiers, namely Multi-Layer Perceptron (MLP), k-Nearest Neighbors (kNN), Random Forest (RF), and Support Vector Machine (SVM). Four feature filtering techniques including Correlation-based Feature Selection (CFS), Information Gain (IG), Gain Ratio (GR), and OneR were applied to the image-extracted features and 10 of the most significant features were selected and fed into single and boosted classifiers. The RF model trained by IG selected features (IG-RF) was the most appropriate classifier among the evaluated models whether in single or boosted modes. It was also found that boosting of IG-RF by using Adaboost.M1 and LogitBoost algorithms improved the classification accuracy. Regarding the performance values, the LogitBoost-IG-RF structure, which provided a classification accuracy of 99.58%, a kappa (k) of 0.9948, and a Root Mean Squared Error (RMSE) of 0.0688 on training dataset, was selected as the most appropriate classifier for plant discrimination in peanut fields. The accuracy, k, and RMSE criteria of this combination on test dataset were 95.00%, 0.9375, and 0.1591, respectively. It was concluded that combination of boosting algorithms and feature selection methods can promote plant type discrimination accuracy, which is a crucial factor in the development of precision weed control systems. CCBY},
	author_keywords = {Agriculture; Boosting; Classification algorithms; Ensemble learning; Feature extraction; Feature selection; Image color analysis; Image processing; Plant identification; Precision agriculture; Shape; Training},
	keywords = {Adaptive boosting; Classification (of information); Decision trees; Feature extraction; Identification (control systems); Image classification; Information filtering; Mean square error; Multilayer neural networks; Nearest neighbor search; Statistical tests; Weed control; Classification accuracy; Correlation based feature selections (CFS); Discrimination accuracy; Feature selection methods; K nearest neighbor (KNN); Multi layer perceptron; Plant classification; Root mean squared errors; Support vector machines},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 13; All Open Access, Gold Open Access}
}

@CONFERENCE{Muhammed Shafi2021328,
	author = {Muhammed Shafi, K. and Mohammed Ismail, B.},
	title = {Systematic Study on Different Methods Used for Identification and Classification of Plant Leaves},
	year = {2021},
	journal = {2021 5th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques, ICEECCOT 2021 - Proceedings},
	pages = {328 – 332},
	doi = {10.1109/ICEECCOT52851.2021.9708023},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126826216&doi=10.1109%2fICEECCOT52851.2021.9708023&partnerID=40&md5=a125448c1b3fa9cbc73153a9481e32ea},
	affiliations = {Kannur University, Department of Information Technology, Mangattuparamba, India},
	abstract = {Plants can be recognized based on its different parts such as leaves, flowers, seeds, fruits, roots etc. Plants are identified for different purposes. Wide varieties of plant species are available in nature. Botanists are research plants to classify plant species in the wild and take their benefits out. There is a lot of demand for identifying plant species manually or visually inspecting their characteristics, study their availability on various geographical locations to reap their benefits effectively. Based on the wide variety of flora available it is practically impossible to manually or visually classify, differentiate or record their features. Hence automated or computer aided methods are developed assisting botanists to accurately feature wise differentiate the plant species. The difficulties faced by a botanist are that some plants having the same characteristics require extra features to be identified. Automated plant identification helps common people to identify and use them having no professional prior knowledge about the plant. Plant identification is done in a variety of ways by researchers. The aim of this paper is to investigate and summarize a systematic study of various plant classification methodologies.  © 2021 IEEE.},
	author_keywords = {Machine learning; Neural Network; Plant Morphology},
	keywords = {Morphology; Plants (botany); Seed; Characteristics studies; Computer aided methods; Geographical locations; Is researches; Neural-networks; Plant identification; Plant leaves; Plant morphology; Plant species; Systematic study; Machine learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166543272-6},
	language = {English},
	abbrev_source_title = {Int. Conf. Electr., Electron., Commun., Comput. Technol. Optim. Techniques, ICEECCOT - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 5th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques, ICEECCOT 2021; Conference date: 10 December 2021 through 11 December 2021; Conference code: 177306}
}

@ARTICLE{Krishna Vinesha2021675,
	author = {Krishna Vinesha, P. and Lakshmi Priyanka, P. and Lakshmanan, L.},
	title = {Automatic Recognition of Medicinal Plants},
	year = {2021},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {691},
	pages = {675 – 683},
	doi = {10.1007/978-981-15-7511-2_69},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101574599&doi=10.1007%2f978-981-15-7511-2_69&partnerID=40&md5=5aca2a14f1fa8044f552719a99b617bd},
	affiliations = {Department of Computer Science and Engineering, Sathyabama Institute of Science and Technology, Chennai, India},
	abstract = {A fully robotic strategy for recognizing therapeutic plants using PC vision and artificial intelligence methods has been demonstrated. The leaves of 25 distinctive therapeutic plant types were gathered and captured using an advanced cell in a research environment. Countless highlights have been separated from each sheet, for example, length, width, border and area, number of vertices, shading, border and frame territory. Then, some highlights determined by these properties have been elaborated. The best results were obtained by a CNN algorithm that uses a 10 times cross-approval strategy. By an accuracy of 98.3%, the CNN algorithm accomplishes superior than any other AI approach, such as the nearest K-neighbor, Naïve Bayes, KNN and neural systems. Those results are enriching and later work will be equipped to use a huge data set and elite processing offices to explore the exposure of deep learning neural systems to recognize restoration plants used in essential medicinal services. It is expected that an electronic or portable PC framework for the programmed recognition of restoration plants will guide the population of the neighborhood to enhance their knowledge of therapeutic plants, to help taxonomists develop recognizable test methods for increasingly competent species. © 2021, Springer Nature Singapore Pte Ltd.},
	author_keywords = {CNN; Image recognition; Medicinal plant recognition},
	keywords = {Barium compounds; Deep learning; Nearest neighbor search; Restoration; Smart power grids; Sodium compounds; Artificial intelligence methods; Automatic recognition; K-neighbors; Medicinal plants; Neural systems; Plant types; Research environment; Test method; Plants (botany)},
	correspondence_address = {P. Krishna Vinesha; Department of Computer Science and Engineering, Sathyabama Institute of Science and Technology, Chennai, India; email: pabbithivineesha@gmail.com},
	editor = {Sherpa K.S. and Bhoi A.K. and Kalam A. and Mishra M.K.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981157510-5},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2nd International Conference on Emerging Trends and Advances in Electrical Engineering and Renewable Energy, ETAEERE 2020; Conference date: 5 March 2020 through 6 March 2020; Conference code: 253999}
}

@ARTICLE{Bhuiyan2021371,
	author = {Bhuiyan, Md. Rafiuzzaman and Abdullahil-Oaphy, Md. and Khanam, Rifa Shanzida and Islam, Md. Sanzidul},
	title = {MediNET: A Deep Learning Approach to Recognize Bangladeshi Ordinary Medicinal Plants Using CNN},
	year = {2021},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {1248},
	pages = {371 – 380},
	doi = {10.1007/978-981-15-7394-1_35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097652132&doi=10.1007%2f978-981-15-7394-1_35&partnerID=40&md5=c8544cd8fc4cdb398a8663ce22c44d3e},
	affiliations = {Department of Computer Science and Engineering, Daffodil International University, Dhanmondi, Dhaka, 1207, Bangladesh; Department of Software Engineering, Daffodil International University, Dhanmondi, Dhaka, 1207, Bangladesh},
	abstract = {Medicine is the only thing by which we use where we feel bad condition of our body’s physical and mental illness. Most of medicines are made of specific plants from our nature. These plants are also known as a medicinal plant. All the traditional Bangladeshi medical systems, namely Ayurveda, Unani, Homeopathy, prominently use medicinal plants. So, it is important to classify the right plant for medical preparation. The ability to identify these plants automatically is needed in recent days. For this, we proposed a renowned algorithm called convolutional neural network for recognizing the plants from leaf image. Our algorithm got 84.58% accuracy. We developed this. We believe that in the future the individuals who do not distinguish medicinal plants will recognize using this methodology. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Convolutional neural network; Medicinal plant; Plant recognition},
	keywords = {Convolutional neural networks; Diseases; Plants (botany); Soft computing; Leaf images; Learning approach; Medical systems; Medicinal plants; Mental illness; Deep learning},
	correspondence_address = {M.R. Bhuiyan; Department of Computer Science and Engineering, Daffodil International University, Dhanmondi, Dhaka, 1207, Bangladesh; email: rafiuzzaman15-9655@diu.edu.bd},
	editor = {Borah S. and Pradhan R. and Dey N. and Gupta P.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {21945357},
	isbn = {978-981157393-4},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 3rd International Conference on Computing and Communication, IC3 2020; Conference date: 20 March 2020 through 21 March 2020; Conference code: 252469}
}

@ARTICLE{Pushpanathan2021305,
	author = {Pushpanathan, Kalananthni and Hanafi, Marsyita and Mashohor, Syamsiah and Fazlil Ilahi, Wan Fazilah},
	title = {Machine learning in medicinal plants recognition: a review},
	year = {2021},
	journal = {Artificial Intelligence Review},
	volume = {54},
	number = {1},
	pages = {305 – 327},
	doi = {10.1007/s10462-020-09847-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084680347&doi=10.1007%2fs10462-020-09847-0&partnerID=40&md5=5e18e8b2add1ba41e59f9bd5e65a4656},
	affiliations = {Faculty of Engineering, University Putra Malaysia, Serdang, 43400 UPM, Selangor Darul Ehsan, Malaysia; Faculty of Agriculture, University Putra Malaysia, Serdang, 43400 UPM, Selangor Darul Ehsan, Malaysia},
	abstract = {Medicinal plants are gaining attention in the pharmaceutical industry due to having less harmful effects reactions and cheaper than modern medicine. Based on these facts, many researchers have shown considerable interest in the research of automatic medicinal plants recognition. There are various opportunities for advancement in producing a robust classifier that has the ability to classify medicinal plants accurately in real-time. In this paper, various effective and reliable machine learning algorithms for plant classifications using leaf images that have been used in recent years are reviewed. The review includes the image processing methods used to detect leaf and extract important leaf features for some machine learning classifiers. These machine learning classifiers are categorised according to their performance when classifying leaf images based on typical plant features, namely shape, vein, texture and a combination of multiple features. The leaf databases that are publicly available for automatic plants recognition are reviewed as well and we conclude with a discussion of prominent ongoing research and opportunities for enhancement in this area. © 2020, Springer Nature B.V.},
	author_keywords = {Classification; Leaf identification; Machine learning; Medicinal plants},
	keywords = {Image classification; Machine learning; Plants (botany); Textures; Harmful effects; Image processing - methods; Leaf database; Medicinal plants; Modern medicine; Multiple features; Pharmaceutical industry; Plant classification; Learning algorithms},
	correspondence_address = {M. Hanafi; Faculty of Engineering, University Putra Malaysia, Serdang, 43400 UPM, Malaysia; email: marsyita@upm.edu.my},
	publisher = {Springer Science and Business Media B.V.},
	issn = {02692821},
	coden = {AIRVE},
	language = {English},
	abbrev_source_title = {Artif Intell Rev},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 33; All Open Access, Green Open Access}
}

@ARTICLE{Oppong20211210,
	author = {Oppong, Stephen Opoku and Twum, Frimpong and Hayfron-Acquah, James Ben and Missah, Yaw Marfo},
	title = {Medicinal Plant Identification using Gabor Filters and Deep Learning Techniques: A Paper Review},
	year = {2021},
	journal = {Journal of Computer Science},
	volume = {17},
	number = {12},
	pages = {1210 – 1221},
	doi = {10.3844/jcssp.2021.1210.1221},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123085102&doi=10.3844%2fjcssp.2021.1210.1221&partnerID=40&md5=dec95ad7899e57e4fce2ade68e1c556c},
	affiliations = {Department of ICT Education, University of Education, Winneba, Ghana; Department of Computer Science, Kwame Nkrumah University of Science and Technology, Kumasi, Ghana},
	abstract = {Computer-aided identification of plants is a branch of machine learning that has become more recognized recently and proves itself as a vital tool in numerous sectors including pharmacological science, forestry and agriculture. This has essentially generated a zeal in creating automated systems for the identification of diverse species of plants. This study reviewed plant species classification relying on leaf textural features using Gabor filters and revealed that Gabor filters perform better when combined with other feature extraction methods. Therefore, this study proposes using Log-Gabor filter in the field of plant identification to improve accuracy since they overcome the drawbacks of Gabor filters which are; the maximum bandwidth of a Gabor filter is limited to approximately one octave and Gabor filters are not optimal if one is seeking broad spectral information with maximal spatial localization © 2021. Stephen Opoku Oppong, Frimpong Twum, James Ben Hayfron-Acquah and Yaw Marfo Missah. This open access article is distributed under a Creative Commons Attribution (CC-BY) 4.0 license},
	author_keywords = {Deep learning; Gabor filter; Log-gabor filter; Plant identification; Texture},
	correspondence_address = {S.O. Oppong; Department of ICT Education, University of Education, Winneba, Ghana; email: sooppong@uew.edu.gh},
	publisher = {Science Publications},
	issn = {15493636},
	language = {English},
	abbrev_source_title = {J. Comput. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Pukhrambam2021956,
	author = {Pukhrambam, Banita and Rathna, R.},
	title = {A smart study on medicinal plants identification and classification using image processing techniques},
	year = {2021},
	journal = {Proceedings of the 3rd International Conference on Intelligent Communication Technologies and Virtual Mobile Networks, ICICV 2021},
	pages = {956 – 962},
	doi = {10.1109/ICICV50876.2021.9388566},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104483579&doi=10.1109%2fICICV50876.2021.9388566&partnerID=40&md5=3b4b940210e23e6de42f7ac81cd9b720},
	affiliations = {Vels Institute of Science, Technology Advanced Studies, Dept. of Computer Science and Engineering, Chennai, India},
	abstract = {Plants play an important role in human life for providing oxygen, food, housing, medicine, energy, housing, and environmental protection. Plants are rich in medicinal esteem and contain dynamic elements for medicinal use because of the global warming populace, lack of expert help for research, lack of government upholding research exercises, and familiarity with medicinal plants. Numerous utility plants are getting pulverized. Manual identification of plants requires significant investment and requires the identification of plants with the help of experts. To address this issue, individuals need to acquire a more prominent advantage in robotized identification and medicinal plant classification. The research region's image process field is dynamic in the automatic identification and classification of medicinal plants. Feature extraction and classification are the important developments in identifying medicinal plants that affect the overall accuracy of the system for scientific categorization. This research examines recent methods of image processing for the classification of medicinal plants. © 2021 IEEE.},
	author_keywords = {Classification; Deep Learning; Feature Extraction; Machine Learning; Preprocessing; Segmentation},
	keywords = {Automation; Cellular radio systems; Classification (of information); Global warming; Housing; Image classification; Mobile telecommunication systems; Plants (botany); Wireless networks; Dynamic elements; Feature extraction and classification; Human lives; Image processing technique; Manual identification; Medicinal plants; Overall accuracies; Investments},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-073811183-4},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Intell. Commun. Technol. Virtual Mob. Networks, ICICV},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 3rd International Conference on Intelligent Communication Technologies and Virtual Mobile Networks, ICICV 2021; Conference date: 4 February 2021 through 6 February 2021; Conference code: 168213}
}

@CONFERENCE{Fatimazahra2021,
	author = {Fatimazahra, Jakjoud and Anas, Hatim and Bouaaddi, Abella},
	title = {FPGA-based hardware acceleration for SVM machine learning algorithm},
	year = {2021},
	journal = {E3S Web of Conferences},
	volume = {229},
	doi = {10.1051/e3sconf/202122901024},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100166184&doi=10.1051%2fe3sconf%2f202122901024&partnerID=40&md5=459eed6971f8471b7c1a4ec50ca4031c},
	affiliations = {Laboratory of Energy Engineering, Materials and Systems, National School of Applied Sciences, Ibn Zohr University, Agadir, Morocco; National School of Applied Sciences, Cady Ayad University, Marrakech, Morocco},
	abstract = {object recognition algorithms are both large consumers of computing power and memory which affects the quality and performance especially when it comes to large image datasets, in this paper we propose an algorithm for fruit/plant recognition that we will accelerate it using the PYNQ Board to evaluate the execution time and the accuracy of the classifier. © The Authors, published by EDP Sciences, 2021.},
	keywords = {Classification (of information); Field programmable gate arrays (FPGA); Large dataset; Object recognition; Support vector machines; Computing power; Hardware acceleration; Large consumers; Large images; Object recognition algorithm; Learning algorithms},
	correspondence_address = {J. Fatimazahra; Laboratory of Energy Engineering, Materials and Systems, National School of Applied Sciences, Ibn Zohr University, Agadir, Morocco; email: jfatimzehra@gmail.com},
	editor = {Krit S.},
	publisher = {EDP Sciences},
	issn = {25550403},
	language = {English},
	abbrev_source_title = {E3S Web Conf.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 3rd International Conference of Computer Science and Renewable Energies, ICCSRE 2020; Conference date: 22 December 2020 through 24 December 2020; Conference code: 166696; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Lv2021649,
	author = {Lv, Rongxin and Li, Zhongzhi and Zuo, Jiankai and Liu, Jing},
	title = {Flower Classification and Recognition Based on Significance Test and Transfer Learning},
	year = {2021},
	journal = {2021 IEEE International Conference on Consumer Electronics and Computer Engineering, ICCECE 2021},
	pages = {649 – 652},
	doi = {10.1109/ICCECE51280.2021.9342468},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101221509&doi=10.1109%2fICCECE51280.2021.9342468&partnerID=40&md5=ae67c381925782d68e04d57559170ae9},
	affiliations = {Shenyang Aerospace University, School of Innovation and Entrepreneurship, Shenyang, 110136, China; Shenyang Aerospace University, School of Computer Science, Shenyang, 110136, China; Tongji University, Department of Computer Science and Technology, Shanghai, 201804, China},
	abstract = {Aiming at the problem that traditional flower classification methods and ordinary convolutional neural networks are difficult to reduce the effect of flower background, the classification effect is not ideal. This paper designs a flower classification model that combines saliency detection and VGG-16 convolutional neural network, and adopts stochastic gradient descent algorithm and prevents over-fitting technology to improve the model. Experiments on the international public flower recognition data set Oxford flower-102 show that the model proposed in this paper is better than other traditional network models and has high recognition accuracy, robustness and generalization ability, which can classify flowers and have higher practical value accurately and quickly. © 2021 IEEE.},
	author_keywords = {Deep Learning; Flower Recognition.; Significance Detection; Transfer Learning},
	keywords = {Convolution; Convolutional neural networks; Gradient methods; Stochastic models; Stochastic systems; Transfer learning; Classification and recognition; Classification methods; Classification models; Flower recognition; Generalization ability; Recognition accuracy; Saliency detection; Stochastic gradient descent algorithm; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818319-0},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Consum. Electron. Comput. Eng., ICCECE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 2021 IEEE International Conference on Consumer Electronics and Computer Engineering, ICCECE 2021; Conference date: 15 January 2021 through 17 January 2021; Conference code: 167001}
}

@CONFERENCE{Bhat202118,
	author = {Bhat, Sachin and Dsouza, Preema and Sharanyalaxmi, K. and Shreeraksha and Tejasvini and Ananth, Alaka},
	title = {Classification of Plant Leaves of Western Ghats using Deep Learning},
	year = {2021},
	journal = {2021 IEEE International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics, DISCOVER 2021 - Proceedings},
	pages = {18 – 23},
	doi = {10.1109/DISCOVER52564.2021.9663698},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124803861&doi=10.1109%2fDISCOVER52564.2021.9663698&partnerID=40&md5=761c66a74019889559e9504d558960b6},
	affiliations = {Shri Madhwa Vadiraja Institute of Technology and Management, Karnataka, India; N.M.A.M Institute of Technology, Nitte, Karnataka, India},
	abstract = {Countless numbers of plants are available in this world. Identifying each and every plant and then classifying them has become one of the important and difficult tasks.Various parts of plants such as flowers, seeds, leaves can be used for identification, but recognizing leaves is the simplest and most effective method. Deep learning technique brings out effective way of leaf recognition system. Here we have used customised Convolutional Neural Network model to recognize the leaves specially growing in western ghats. A separate dataset has been created by collecting more than 50000 leaf samples of 48 different types of plants. The relevant information about the set of plants are collected from the botanists. Various architectures of CNN such as InceptionV3, MobileNet, VGG16, DensNet are used to evaluate the results. Model gives a satisfactory accuracy of 93.79% on 48 classes.  © 2021 IEEE.},
	author_keywords = {convolutional neural network; plant recognition; Western ghats},
	keywords = {Convolution; Deep learning; Plants (botany); Seed; Convolutional neural network; Leaf recognition; Learning techniques; Neural network model; Plant leaves; Plant recognition; Recognition systems; Simple++; Western ghats; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166541244-5},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Distrib. Comput., VLSI, Electr. Circuits Robot., DISCOVER - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2021 IEEE International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics, DISCOVER 2021; Conference date: 19 November 2021 through 20 November 2021; Conference code: 176241}
}

@ARTICLE{Bal2021515,
	author = {Bal, Jyotisagar and Rath, Manas Kumar and Swain, Prasanta Kumar},
	title = {Plant Leaf Identification Using HOG and Random Forest Regressor},
	year = {2021},
	journal = {Smart Innovation, Systems and Technologies},
	volume = {224},
	pages = {515 – 525},
	doi = {10.1007/978-981-16-1502-3_51},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112725782&doi=10.1007%2f978-981-16-1502-3_51&partnerID=40&md5=141da3b7ace46c7622fb3b376cc46b53},
	affiliations = {North Orissa University, Baripada, India; KIIT Deemed University, Bhubaneswar, India},
	abstract = {According to Indian state of Forest Report 2019, the total forest in India is 712,249 km2 which is 22% of total geographical area of India. India is rich in biodiversity which includes more than 40,000 species of plants. It is very difficult to distinguish between different plant species. Botanist can identify these plants using the characteristic of leaf but the process is very difficult and time taking. So, in this paper, we proposed a method to recognize plants on the basis of its leaf pattern. Here, Histogram of Oriented Gradients (HOG) and Random Forest Regressor techniques are used for plant leaf image classification and recognition. Experimental results show that the proposed model achieves accuracy up to 99% in leaf recognition. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Classification; HOG; Machine learning; Plant leaf; Random Forest},
	keywords = {Biodiversity; Decision trees; Geographical area; Histogram of oriented gradients (HOG); Leaf recognition; Plant leaf; Plant species; Random forests},
	correspondence_address = {P.K. Swain; North Orissa University, Baripada, India; email: prsantanou@gmail.com},
	editor = {Satapathy S.C. and Bhateja V. and Favorskaya M.N. and Adilakshmi T.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {21903018},
	isbn = {978-981161501-6},
	language = {English},
	abbrev_source_title = {Smart Innov. Syst. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference on Smart Computing and Informatics, SCI 2020; Conference date: 9 October 2020 through 10 October 2020; Conference code: 262569}
}

@ARTICLE{Roopashree2021135927,
	author = {Roopashree, S. and Anitha, J.},
	title = {DeepHerb: A Vision Based System for Medicinal Plants Using Xception Features},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {135927 – 135941},
	doi = {10.1109/ACCESS.2021.3116207},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117005868&doi=10.1109%2fACCESS.2021.3116207&partnerID=40&md5=ffcec3f58b898e972f1954e1f66c32fb},
	affiliations = {Department of Computer Science and Engineering (AIML) and (Cybersecurity), Faculty of Engineering and Technology, JAIN (Deemed-to-be University), Bengaluru, 560041, India; Research Scholar, VTU-RRC, Visvesvaraya Technological University, Belagavi, 590018, India; Department of Computer Science and Engineering, RV Institute of Technology and Management, Bengaluru, 560076, India},
	abstract = {The conservation of biodiversity is crucial as many plant species are critically under extinction. The traditional medicinal system, an alternative to synthetic drugs, promote healthy living and mainly depends on the wide repository of plants. A vision-based automatic medicinal plant identification system is proposed using different neural network techniques in computer vision and deep learning. The challenge lies in the unavailability of the medicinal herb dataset. The paper showcases a novel medicinal leaf dataset entitled DeepHerb dataset comprising of 2515 leaf images from 40 varied species of Indian herbs. The efficacy of the dataset is revealed by comparing pre-trained deep convolution neural network architectures such as VGG16, VGG19, InceptionV3 and Xception. The work concentrates on adopting the transfer learning technique on the pre-trained models to extract features and classify using Artificial Neural Network (ANN) and Support Vector Machine (SVM). The SVM hyperparameters are tuned further by Bayesian optimization to achieve a better performance model. The proposed DeepHerb model learned from Xception and ANN outperformed by 97.5% accuracy. A cross-platform mobile application entitled HerbSnap developed integrating the DeepHerb model identifies the herb image with a prediction time of 1 second per image and reveals the pertinent details of herbs from the database. This research will further focus on expanding the dataset to benefit stakeholders and thus, enriches society with the knowledge of herbs and their medicinal properties. © 2013 IEEE.},
	author_keywords = {Bayesian optimization; computer vision; deep learning; medicinal plants; support vector machine; transfer learning; Xception},
	keywords = {Biodiversity; Computer vision; Conservation; Deep neural networks; Network architecture; Plants (botany); Bayesian optimization; Deep learning; Medicinal plants; Plant species; Support vectors machine; Synthetic drug; Transfer learning; Vision based; Vision-based system; Xception; Support vector machines},
	correspondence_address = {S. Roopashree; Department of Computer Science and Engineering (AIML) and (Cybersecurity), Faculty of Engineering and Technology, JAIN (Deemed-to-be University), Bengaluru, 560041, India; email: roopashaily@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access, Gold Open Access}
}

@ARTICLE{Liu2021526,
	author = {Liu, Wenhui and Yuan, Changan and Qin, Xiao and Wu, Hongjie},
	title = {Plant Leaf Recognition Network Based on Fine-Grained Visual Classification},
	year = {2021},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12836 LNCS},
	pages = {526 – 534},
	doi = {10.1007/978-3-030-84522-3_43},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113619272&doi=10.1007%2f978-3-030-84522-3_43&partnerID=40&md5=f51422b6deb1e5a3465006c3b0868b5a},
	affiliations = {Institute of Machine Learning and Systems Biology, School of Electronics and Information Engineering, Tongji University, Shanghai, China; Guangxi Academy of Science, Nanning, 530007, China; School of Computer and Information Engineering, Nanning Normal University, Nanning, 530299, China; School of Electronic and Information Engineering, Suzhou University of Science and Technology, Suzhou, 215009, China},
	abstract = {Plant classification and recognition research is the basic research work of botany research and agricultural production. It is of great significance to identify and distinguish plant species and explore the relationship between plants. In recent years, most of the research methods focus on feature extraction and feature engineering related aspects. In this paper, a plant leaf recognition method based on fine-grained image classification is proposed, which can better find the regional block information of different species of plant leaves. In this study, the hierarchical and progressive training strategy is adopted, the method of cutting and generating jigsaw is used to force the model to find information of different granularity levels. The experiment proves that the model trained by the fine-grained classification method can better solve the problems of large intra-class spacing and small inter-class spacing of plant slices. © 2021, Springer Nature Switzerland AG.},
	author_keywords = {Plant leaf recognition; Progressive training fine-grained visual classification},
	keywords = {Agricultural robots; Agriculture; Classification (of information); Computation theory; Engineering research; Intelligent computing; Agricultural productions; Classification methods; Different granularities; Feature engineerings; Network-based; Plant classification; Training strategy; Visual classification; Plants (botany)},
	correspondence_address = {W. Liu; Institute of Machine Learning and Systems Biology, School of Electronics and Information Engineering, Tongji University, Shanghai, China; email: liuwenhui_a@126.com},
	editor = {Huang D. and Jo K. and Li J. and Gribova V. and Premaratne P.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303084521-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 17th International Conference on Intelligent Computing, ICIC 2021; Conference date: 12 August 2021 through 15 August 2021; Conference code: 263609}
}

@CONFERENCE{Rakibul2021425,
	author = {Rakibul, S.K. and Wadhawan, Ankita},
	title = {Identification of plants using deep learning: A review},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {2786},
	pages = {425 – 435},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102504902&partnerID=40&md5=b858355f32702cba575a8c26d625dab5},
	affiliations = {Lovely Professional University (LPU University), Jalandhar-Delhi, G.T. Road, Phagwara, Punjab, 144411, India},
	abstract = {Identification of plants is a very important field in the earth's ecology to maintain a healthy atmosphere. Certain of these plants have significant medicinal properties. Nowadays of finding a plant is not easy by looking at its physical properties. This paper provides an academic database of literature between the duration of 2015-2020. It has been observed that the new generation of convolutionary neural networks (CNNs) in the space area of image recognition has produced remarkable performance. In this paper, techniques are discussed the concepts of Deep learning and different leaf recognition methods. © 2021 CEUR-WS. All rights reserved.},
	author_keywords = {Artificial Intelligence; Convolutional Neural Network; Deep Learning; Fully Connected Neurons; Image Processing; Machine Learning},
	keywords = {Earth atmosphere; Image recognition; Semantics; Leaf recognition; Deep learning},
	editor = {Jain S. and Groppe S.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2021 International Semantic Intelligence Conference, ISIC 2021; Conference date: 25 February 2021 through 27 February 2021; Conference code: 167510}
}

@ARTICLE{Mary Sobha202130,
	author = {Mary Sobha, P.G. and Thomas, Princy Ann},
	title = {Classification of Plant Species with Compound and Simple Leaves Using CNN Fusion Networks},
	year = {2021},
	journal = {Communications in Computer and Information Science},
	volume = {1368},
	pages = {30 – 42},
	doi = {10.1007/978-981-16-0404-1_3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102593254&doi=10.1007%2f978-981-16-0404-1_3&partnerID=40&md5=36a5e5401d1fccc773a84cb701f3d18d},
	affiliations = {Department of Computer Engineering, Government Polytechnic College, Cherthala, India; Department of Computer Science and Engineering, Government Engineering College Idukki, Idukki, India},
	abstract = {An automatic plant species identification system could help to identify plant species very easily. Deep learning is an AI function which works like the human brain, with artificial neural networks. In neural networks, neuron nodes are connected like a web and used to extract higher-level features from input. Convolutional neural network (CNN), deep belief network (DBF) and recurrent neural network (RNN) etc. are deep learning networks and can extract more detailed information compared to conventional machine learning techniques [1, 10]. CNN is a very good choice for image processing, and it can work with large datasets efficiently. In our project VGG16 CNN is used to extract the features from leaf images of a simple and compound leaf. For the identification of plant species with simple and compound leaves with real complex background images, this paper proposes a fusion CNN model using original whole leaf images and patch images. A transfer learned VGG16 CNN was used for feature extraction and classification of real complex background images [1]. Feature extraction and classification are done with original (model1) and patch (model2) images separately with VGG16 CNN models. The feature maps from intermediate levels of model1 and model2 are taken, then concatenated and classified using SVM and KNN. The CNN-SVM model has the best performance over model1, model2 and CNN-KNN model. The proposed fusion model shows the efficiency of 98.6% accuracy in model evaluation and 90% accuracy in plant identification using complex background leaf images. © 2021, Springer Nature Singapore Pte Ltd.},
	author_keywords = {CNN-SVM fusion model; Convolutional neural networks (CNN); Fusion model; Intermediate feature maps; Plant identification; Real complex background leaf images},
	keywords = {Classification (of information); Complex networks; Convolutional neural networks; Extraction; Feature extraction; Image fusion; Image processing; Large dataset; Learning systems; Plants (botany); Support vector machines; Complex background; Conventional machines; Deep belief networks; Feature extraction and classification; Intermediate level; Plant identification; Plant species identification; Recurrent neural network (RNN); Recurrent neural networks},
	correspondence_address = {P.G. Mary Sobha; Department of Computer Engineering, Government Polytechnic College, Cherthala, India; email: msobhapg@gmail.com},
	editor = {Garg D. and Wong K. and Sarangapani J. and Gupta S.K.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18650929},
	isbn = {978-981160403-4},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 10th International Advanced Computing Conference, IACC 2020; Conference date: 5 December 2020 through 6 December 2020; Conference code: 255449}
}

@CONFERENCE{Augustine2021,
	author = {Augustine, Annie and Sherly, K.K.},
	title = {Various Approaches in Plant Species Identification and Plant Disease Detection Using Digital Images of Leaves},
	year = {2021},
	journal = {10th International Conference on Advances in Computing and Communications, ICACC 2021},
	doi = {10.1109/ICACC-202152719.2021.9708308},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125804896&doi=10.1109%2fICACC-202152719.2021.9708308&partnerID=40&md5=3d06c96e4990bbc8b609e4fc67407f34},
	affiliations = {Rajagiri School of Engineering Technology, Department of Information Technology, Kochi, India},
	abstract = {Plants and trees are an inevitable part of our life. All these species contribute to biodiversity, provide clean and fresh air, prevent soil erosion, home remedy, and many more. Acquiring knowledge about species is essential for protecting biodiversity. The identification of plants by conventional methods is complex and time-consuming for novices. Also, many species are endangered because of human encroachment and due to diseases affecting plants. These plant diseases cause economic, social, and ecological losses. In this context, diagnosing diseases accurately and timely and doing the necessary control measures is of the utmost importance. There have been many technological advancements in the area of computer vision to identify plant species and diseases automatically. This paper aims to present various approaches in leaf recognition and disease detection using digital images of leaves. Various phases in image classification using conventional machine learning models and deep learning techniques have been discussed. A comparative study on various paper works and their performance have also been analyzed.  © 2021 IEEE.},
	author_keywords = {Convolutional Neural Network; Deep Learning; leaf disease detection; plant species classification; Siamese Neural Network; Transfer Learning},
	keywords = {Biodiversity; Deep learning; Diagnosis; Disease control; E-learning; Image classification; Plants (botany); Transfer learning; Convolutional neural network; Deep learning; Leaf disease detections; Neural-networks; Plant disease; Plant species; Plant species classification; Siamese neural network; Species classification; Transfer learning; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166543919-0},
	language = {English},
	abbrev_source_title = {Int. Conf. Adv. Comput. Commun., ICACC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 10th International Conference on Advances in Computing and Communications, ICACC 2021; Conference date: 21 October 2021 through 23 October 2021; Conference code: 177331}
}

@CONFERENCE{Tsuya2021703,
	author = {Tsuya, Kohei and Fujii, Nobutada and Kokuryo, Daisuke and Kaihara, Toshiya and Sunami, Yasuhiro and Izuno, Reini and Mano, Masahito},
	title = {A Study on tree species discrimination using machine learning in forestry},
	year = {2021},
	journal = {Procedia CIRP},
	volume = {99},
	pages = {703 – 706},
	doi = {10.1016/j.procir.2021.03.094},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106444961&doi=10.1016%2fj.procir.2021.03.094&partnerID=40&md5=5e0269be3fd02eb7bf8095f01919bce0},
	affiliations = {Kobe University, 1-1, Rokkodai, Nada, Kobe, Hyogo, Japan; TACHIBANA CONSULTANT, 10-3, Kouno, Tachibana, Anan, Tokushima, Japan; FIELDCOM.INC, 365-2, Hirota hirota, Minami awaji, Hyogo, Japan},
	abstract = {As the present state of forestry in Japan, the period of full-scale utilization of planted forests is approaching; consolidation of forest practices aimed at the stable and efficient supply of domestic timber is being promoted. Visualization of forest boundaries is important, however it takes a lot of cost and time because forest surveys are conducted by human's exploration. This study proposes an efficient tree species identification method using deep learning method from aerial images of forests using drones for the purpose of visualizing forest boundaries. The effectiveness of the proposed method is confirmed by experiments using actual images taken by drones. © 2021 The Authors. Published by Elsevier B.V.},
	author_keywords = {Convolutional neural network; Drone; Forestry; Machine learning; Tree species classfication},
	keywords = {Antennas; Deep learning; Drones; Neural networks; Timber; Convolutional neural network; Drone; Elsevier; Forest practices; Human exploration; Machine-learning; Scale utilization; Tree species; Tree species classfication; Tree species identifications; Forestry},
	correspondence_address = {K. Tsuya; Kobe University, Nada, Kobe, Hyogo, 1-1, Rokkodai, Japan; email: tsuya@kaede.cs.kobe-u.ac.jp},
	editor = {Teti R. and D'Addona D.M.},
	publisher = {Elsevier B.V.},
	issn = {22128271},
	language = {English},
	abbrev_source_title = {Procedia CIRP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, CIRP ICME 2020; Conference date: 15 July 2020 through 17 July 2020; Conference code: 169041; All Open Access, Gold Open Access}
}

@ARTICLE{Bisen20216443,
	author = {Bisen, Dhananjay},
	title = {Deep convolutional neural network based plant species recognition through features of leaf},
	year = {2021},
	journal = {Multimedia Tools and Applications},
	volume = {80},
	number = {4},
	pages = {6443 – 6456},
	doi = {10.1007/s11042-020-10038-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092598405&doi=10.1007%2fs11042-020-10038-w&partnerID=40&md5=45aaefbcf5a0232ab747ea0e8e62a86c},
	affiliations = {Rajkiya Engineering College Banda, Atarra, UP, India},
	abstract = {In present scenario, the research under image processing has been rapidly transformed from machine learning to deep learning. The deep learning algorithms are usually applied in the various areas like images to be classified or identified more accurately. One of the application areas of deep learning is the plant identification through its leaf which helps to recognize plant species. Botanists consume most of time in identifying plant species by manually scrutinizing and finding its features. This paper proposes an automated plant identification system, for identifying the plants species through their leaf. This task is accomplished using deep convolutional neural network to achieve higher accuracy. Image pre-processing, feature extraction and recognition are three main identification steps which are taken under consideration. Proposed CNN classifier learns the features of plants such as classification of leafs by using hidden layers like convolutional layer, max pooling layer, dropout layers and fully connected layers. The model acquires a knowledge related to features of Swedish leaf dataset in which 15 tree classes are available, that helps to predict the correct category of unknown plant with accuracy of 97% and minimum losses. Result is slightly better than the previous work that analyzes 93.75% of accuracy. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {CNN classifier; Deep learning; Feature extraction; Image pre-processing; Leaf recognition; Swedish leaf dataset},
	keywords = {Classification (of information); Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Image processing; Application area; Feature extraction and recognition; Hidden layers; Image preprocessing; Minimum loss; Plant identification; Plant identification systems; Plant species; Learning algorithms},
	correspondence_address = {D. Bisen; Rajkiya Engineering College Banda, Atarra, India; email: bisen.it2007@gmail.com},
	publisher = {Springer},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 30}
}

@CONFERENCE{Ayumi202140,
	author = {Ayumi, Vina and Ermatita, Ermatita and Abdiansah, Abdiansah and Noprisson, Handrie and Purba, Mariana and Utami, Marissa},
	title = {A Study on Medicinal Plant Leaf Recognition Using Artificial Intelligence},
	year = {2021},
	journal = {Proceedings - 3rd International Conference on Informatics, Multimedia, Cyber, and Information System, ICIMCIS 2021},
	pages = {40 – 45},
	doi = {10.1109/ICIMCIS53775.2021.9699363},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127033136&doi=10.1109%2fICIMCIS53775.2021.9699363&partnerID=40&md5=c26287a2eead8435e1699e99eb292285},
	affiliations = {Universitas Sriwijaya, Doctoral Program in Engineering, Palembang, Indonesia; Universitas Sriwijaya, Faculty of Computer Science, Palembang, Indonesia; Universitas Mercu Buana, Faculty of Computer Science, Jakarta, Indonesia; Program of Informatics Management, Politeknik Anika, Palembang, Indonesia; Muhammadiyah University of Bengkulu, Faculty of Engineering, Bengkulu, Indonesia},
	abstract = {Medicinal plant recognition manually takes a lot of time and money. Moreover, to reduce these resources, some researchers propose to implement artificial intelligence technology. This paper aims are to conduct a systematic literature review of medicinal plant leaf recognition published in the last two years (2019-2020) from IEEE, Springer and Science Direct. We obtained 15 studies in the field of medicinal plant leaf recognition using artificial intelligence. The dataset used for medicinal plant leaf recognition is mostly used private dataset, however, there are public dataset named Leaf, Flavia, Swedish dataset. We also found robust method that can be used for medicinal plant leaf recognition is Multichannel Modified Local Gradient Pattern (MCMLGP) and Gray Level Co-Occurrence Matrix (GLCM) as feature extraction; and Convolutional Neural Network (CNN), Multi-Layer Perceptron trained with Backpropagation algorithm (MLP-BP), Support Vector Machine (SVM), and Transfer Learning (VGG19) as classifier.  © 2021 IEEE.},
	author_keywords = {artificial intelligent; image processing; machine learning; medicinal plant},
	keywords = {Convolutional neural networks; Multilayer neural networks; Plants (botany); Support vector machines; Artificial intelligence technologies; Artificial intelligent; Images processing; Leaf recognition; Medicinal plants; Plant leaves; Plant recognition; Public dataset; Swedishs; Systematic literature review; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166542733-3},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Informatics, Multimed., Cyber, Inf. Syst., ICIMCIS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 3rd International Conference on Informatics, Multimedia, Cyber, and Information System, ICIMCIS 2021; Conference date: 28 October 2021 through 29 October 2021; Conference code: 177116}
}

@ARTICLE{Turkoglu20211,
	author = {Turkoglu, Muammer and Aslan, Muzaffer and Ari, Ali and Alçin, Zeynep Mine and Hanbay, Davut},
	title = {A Multi-Division Convolutional Neural Network-Based Plant Identification System},
	year = {2021},
	journal = {PeerJ Computer Science},
	volume = {7},
	pages = {1 – 18},
	doi = {10.7717/PEERJ-CS.572},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108877532&doi=10.7717%2fPEERJ-CS.572&partnerID=40&md5=73a8aa428fe340c08c3739d8e1a4a783},
	affiliations = {Faculty of Engineering, Department of Software Engineering, Samsun University, Samsun, Turkey; Engineering Faculty, Electrical and Electronics Engineering Department, Bingol University, Bingöl, Turkey; Engineering Faculty, Computer Engineering Department, Inonu University, Malatya, Turkey; Vedat Topçuoğlu Anatolian Vocational High School, Electrical and Electronics Department, Gaziantep, Turkey},
	abstract = {Background. Plants have an important place in the life of all living things. Today, there is a risk of extinction for many plant species due to climate change and its environmental impact. Therefore, researchers have conducted various studies with the aim of protecting the diversity of the planet's plant life. Generally, research in this area is aimed at determining plant species and diseases, with works predominantly based on plant images. Advances in deep learning techniques have provided very successful results in this field, and have become widely used in research studies to identify plant species. Methods. In this paper, a Multi-Division Convolutional Neural Network (MD-CNN)- based plant recognition system was developed in order to address an agricultural problem related to the classification of plant species. In the proposed system, we divide plant images into equal nxn-sized pieces, and then deep features are extracted for each piece using a Convolutional Neural Network (CNN). For each part of the obtained deep features, effective features are selected using the Principal Component Analysis (PCA) algorithm. Finally, the obtained effective features are combined and classification conducted using the Support Vector Machine (SVM) method. Results. In order to test the performance of the proposed deep-based system, eight different plant datasets were used: Flavia, Swedish, ICL, Foliage, Folio, Flower17, Flower102, and LeafSnap. According to the results of these experimental studies, 100% accuracy scores were achieved for the Flavia, Swedish, and Folio datasets, whilst the ICL, Foliage, Flower17, Flower102, and LeafSnap datasets achieved results of 99.77%, 99.93%, 97.87%, 98.03%, and 94.38%, respectively. Copyright 2021 Turkoglu et al.},
	author_keywords = {Deep features; Division process; Plant Identification System; Principal component analysis; Support Vector Machine},
	keywords = {Agricultural robots; Climate change; Convolution; Deep learning; Environmental impact; Support vector machines; Learning techniques; Living things; Plant identification systems; Plant life; Plant recognition; Plant species; Research studies; Swedishs; Convolutional neural networks},
	correspondence_address = {M. Aslan; Engineering Faculty, Electrical and Electronics Engineering Department, Bingol University, Bingöl, Turkey; email: muzafferaslan@bingol.edu.tr},
	publisher = {PeerJ Inc.},
	issn = {23765992},
	language = {English},
	abbrev_source_title = {PeerJ Comput. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Shi202113643,
	author = {Shi, Yun and Ma, Donghui and Lv, Jie and Li, Jie},
	title = {ACTL: Asymmetric Convolutional Transfer Learning for Tree Species Identification Based on Deep Neural Network},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {13643 – 13654},
	doi = {10.1109/ACCESS.2021.3051015},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099543059&doi=10.1109%2fACCESS.2021.3051015&partnerID=40&md5=cca330a56f1823bdaeb3e1d51fc5a995},
	affiliations = {Collage of Geomatics, Xi'An University of Science and Technology, Xi'an, 710054, China; Key Laboratory of Coal Resources Exploration and Comprehensive Utilization, Ministry of Natural Resources, Xi'an, 710021, China},
	abstract = {The identification of tree species is of great significance to the sustainable management and utilization of forest ecosystems. Hyperspectral data provide sufficient spectral and spatial information to classify tree species. Convolutional neural networks (CNN) have achieved great success in hyperspectral image (HSI) classification. The outstanding performance of CNN in HSI classification relies on sufficient training samples. However, it's expensive and time consuming to acquire labeled training samples. In this article, a novel asymmetric convolutional transfer learning model for HSI classification is proposed. First, the tree species identification dataset is built from Goddard's LiDAR, Hyperspectral Thermal (G-LiHT) data. Then, the asymmetric convolutional transfer learning model and weights trained on ImageNet dataset are used to initialize the weights of the HSI classification model. Finally, a well fine-tuned neural network on tree species dataset is used to perform the HSI classification task. The experimental results reveal that the proposed model with asymmetric convolutional blocks effectively improves the accuracy of Howland forest tree species identification and provides a new idea for the classification of hyperspectral remote sensing images. © 2013 IEEE.},
	author_keywords = {asymmetric convolution block; Deep neural networks; G-LiHT; hyperspectral image; transfer learning},
	keywords = {Classification (of information); Convolution; Convolutional neural networks; Deep neural networks; Ecosystems; Forestry; Image enhancement; Learning systems; Remote sensing; Sampling; Spectroscopy; Transfer learning; Classification models; Classification tasks; Hyperspectral Data; Hyperspectral Remote Sensing Image; Lidar , hyperspectral; Spatial informations; Sustainable management; Tree species identifications; Deep learning},
	correspondence_address = {D. Ma; Collage of Geomatics, Xi'An University of Science and Technology, Xi'an, 710054, China; email: donghm1106@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@CONFERENCE{Kumar2021549,
	author = {Kumar, Lalit and Singh, Dushyant Kumar},
	title = {Analyzing Computational Response and Performance of Deep Convolution Neural Network for Plant Disease Classification using Plant Leave Dataset},
	year = {2021},
	journal = {Proceedings - 2021 IEEE 10th International Conference on Communication Systems and Network Technologies, CSNT 2021},
	pages = {549 – 553},
	doi = {10.1109/CSNT51715.2021.9509632},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124686966&doi=10.1109%2fCSNT51715.2021.9509632&partnerID=40&md5=265039df5d3227706c6241de26981bd8},
	affiliations = {Computer Science & Engineering, Motilal Nehru National Institute of Technology, Allahabad, India},
	abstract = {Computer vision plays a vital role in the area of agriculture for various applications like irrigation monitoring, animal monitoring in farm areas, plant classification, disease detection in plant leaves, and more. In agriculture, identifying diseases in the plant leaf is a leading challenge for the farmers because disease in plants reduces crop production and also affects the income of farmers. To ensure the minimum losses, an advanced monitoring system is needed to detect and classify the disease for the plant leaves. To resolve this challenge, this article proposed a method for plant leaf disease classification by adopting a slight variant of the deep convolutional network. In this experiment, we use the plant village dataset that contains five classes with 4197 training images and 430 test images. The experimental outcomes demonstrate that the proposed methodology reveals encouraging results with accuracy up to 97.90%, which outperforms other existing machine learning-based methods. © 2021 IEEE.},
	author_keywords = {Convolutional neural network; Deep learning; Logistics regression; Machine learning; Support vector machine},
	keywords = {Classification (of information); Convolution; Convolutional neural networks; Cultivation; Deep neural networks; Plants (botany); Statistical tests; Convolution neural network; Convolutional neural network; Deep learning; Disease classification; Logistics regressions; Performance; Plant classification; Plant disease; Plant leaves; Support vectors machine; Support vector machines},
	editor = {Tomar G.S.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-073810523-9},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. Commun. Syst. Netw. Technol., CSNT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 10th IEEE International Conference on Communication Systems and Network Technologies, CSNT 2021; Conference date: 18 June 2021 through 19 June 2021; Conference code: 176802}
}

@ARTICLE{Joly2021371,
	author = {Joly, Alexis and Goëau, Hervé and Kahl, Stefan and Picek, Lukáš and Lorieul, Titouan and Cole, Elijah and Deneu, Benjamin and Servajean, Maximillien and Durso, Andrew and Bolon, Isabelle and Glotin, Hervé and Planqué, Robert and de Castañeda, Rafael Ruiz and Vellinga, Willem-Pier and Klinck, Holger and Denton, Tom and Eggel, Ivan and Bonnet, Pierre and Müller, Henning},
	title = {Overview of LifeCLEF 2021: An Evaluation of Machine-Learning Based Species Identification and Species Distribution Prediction},
	year = {2021},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12880 LNCS},
	pages = {371 – 393},
	doi = {10.1007/978-3-030-85251-1_24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115882091&doi=10.1007%2f978-3-030-85251-1_24&partnerID=40&md5=fb02580b47562e9922c93d27eefc36f5},
	affiliations = {Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France; CIRAD, UMR AMAP, Montpellier, Occitanie, France; Univ. Toulon, Aix Marseille Univ., CNRS, LIS, DYNI Team, Marseille, France; Xeno-canto Foundation, Amsterdam, Netherlands; HES-SO, Sierre, Switzerland; KLYCCB, Cornell Lab of Ornithology, Cornell University, Ithaca, United States; LIRMM, AMI, Univ. Paul Valéry Montpellier, Univ. Montpellier, CNRS, Montpellier, France; ISG, Department of Community Health and Medicine, UNIGE, Geneva, Switzerland; Department of Computing and Mathematical Sciences, Caltech, Pasadena, United States; Department of Cybernetics, FAV, University of West Bohemia, Pilsen, Czech Republic; Department of Biological Sciences, Florida Gulf Coast University, Fort Myers, United States; Google LLC, San Francisco, United States},
	abstract = {Building accurate knowledge of the identity, the geographic distribution and the evolution of species is essential for the sustainable development of humanity, as well as for biodiversity conservation. However, the difficulty of identifying plants and animals is hindering the aggregation of new data and knowledge. Identifying and naming living plants or animals is almost impossible for the general public and is often difficult even for professionals and naturalists. Bridging this gap is a key step towards enabling effective biodiversity monitoring systems. The LifeCLEF campaign, presented in this paper, has been promoting and evaluating advances in this domain since 2011. The 2021 edition proposes four data-oriented challenges related to the identification and prediction of biodiversity: (i) PlantCLEF: cross-domain plant identification based on herbarium sheets, (ii) BirdCLEF: bird species recognition in audio soundscapes, (iii) GeoLifeCLEF: remote sensing based prediction of species, and (iv) SnakeCLEF: Automatic Snake Species Identification with Country-Level Focus. © 2021, Springer Nature Switzerland AG.},
	keywords = {Biodiversity; Birds; Conservation; Geographical distribution; Historic preservation; Machine learning; Remote sensing; Biodiversity conservation; Biodiversity monitoring; Bird species; Cross-domain; General publics; Machine-learning; Monitoring system; Plant identification; Species distributions; Species identification; Forecasting},
	correspondence_address = {A. Joly; Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France; email: alexis.joly@inria.fr},
	editor = {Candan K.S. and Ionescu B. and Goeuriot L. and Larsen B. and Müller H. and Joly A. and Maistro M. and Piroi F. and Faggioli G. and Ferro N.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303085250-4},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; Conference name: 12th International Conference of the Cross-Language Evaluation Forum for European Languages, CLEF 2021; Conference date: 21 September 2021 through 24 September 2021; Conference code: 265349; All Open Access, Green Open Access}
}

@ARTICLE{Sujith2021269,
	author = {Sujith, A. and Neethu, R.},
	title = {Classification of plant leaf using shape and texture features},
	year = {2021},
	journal = {Lecture Notes in Networks and Systems},
	volume = {145},
	pages = {269 – 282},
	doi = {10.1007/978-981-15-7345-3_22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092080888&doi=10.1007%2f978-981-15-7345-3_22&partnerID=40&md5=f92582e3689f8f5ec3bc7353a4113168},
	affiliations = {Department of Computer Science, Research Centre, University of Kerala, Thiruvananthapuram, Kerala, India; Department of Computer Science, University Institute of Technology, Kollam District, Mukhathala, Kerala, India},
	abstract = {Plants are mainly classified based on their characteristics of plant components such as leaves, flower, stem, root, seed, etc. Feature or characteristics is an essential fact for plant classification. A good feature extraction technique can help to extract quality features that give clear information to discriminate against each class. Computer engineers can help botanists to identify plants and their species through advanced computational techniques with the stipulated time. The proposed method gives efficient hybrid feature extraction using the PHOG, LBP, and GLCM feature extraction techniques. The fused feature vector is normalized and reduced size by Neighborhood Components Analysis (NCA). The efficient feature extraction and feature selection techniques have helped to improve the classification performance and reduced the model complexity. Two benchmark plant dataset Flavia and Swedish Leaves used to evaluate the proposed work. The primary contributions of this paper are introducing a multi-feature fusion shape and texture method for plant leaf image classification. The experimental result shows the average accuracy of the proposed method is 98.23%, and the average computational complexity is 147.98 s. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2021.},
	author_keywords = {Gray level co-occurrence matrix; Local binary pattern; Neighborhood components analysis; Plant classification; Pyramid histogram of oriented gradients},
	correspondence_address = {A. Sujith; Department of Computer Science, Research Centre, University of Kerala, Thiruvananthapuram, India; email: sujdcbin@gmail.com},
	editor = {Ranganathan G. and Chen J. and Rocha A.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {23673370},
	isbn = {978-981157344-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Networks Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 4th International Conference on Inventive Communication and Computational Technologies, ICICCT 2020; Conference date: 28 May 2020 through 29 May 2020; Conference code: 249349}
}

@CONFERENCE{Hussain2021,
	author = {Hussain, Ambreen and Barua, Bidushi and Osman, Ahmed and Abozariba, Raouf and Taufiq Asyhari, A.},
	title = {Performance of MobileNetV3 Transfer Learning on Handheld Device-based Real-Time Tree Species Identification},
	year = {2021},
	journal = {2021 26th International Conference on Automation and Computing: System Intelligence through Automation and Computing, ICAC 2021},
	doi = {10.23919/ICAC50006.2021.9594222},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123194288&doi=10.23919%2fICAC50006.2021.9594222&partnerID=40&md5=dde8ae262282f5e446fbef66547b8a25},
	affiliations = {Birmingham City University, School of Computing and Digital Technology, United Kingdom},
	abstract = {Detailed information on tree species constitutes an essential factor to support forest health monitoring and biodiversity conservation. Current deep learning-based mobile applications for tree and plant identification require excessive computation. They largely depend on a network connection to perform computing tasks on powerful remote servers in the Cloud. Many forestry areas are remote with limited or no cellular coverage, which is an obstacle for these applications to recognize trees and plants in these areas in real-time. This paper investigates existing CNN-based machine learning applications for plant identification tailored for handheld device usages. Driven by network independence, reduced computation, size and time requirements, we propose the use of MobileNet (a mobile computer vision architecture) transfer learning to improve the accuracy of offline leaf-based plant recognition. We then carry out experimental validation of state-of-the-art MobileNet. Our findings reveal that using MobileNetV3 transfer learning, accuracy up to 90% can be achieved within fewer iterations than end-to-end CNN-based models for plant identification. The lightweight model comes with reduced computation that runs independently within a smartphone application without internet access, ideal for tree species identification in rural forests.  © 2021 Chinese Automation and Computing Society in the UK-CACSUK.},
	author_keywords = {CNN; mobile devices; MobileNet; plant identification; transfer learning},
	keywords = {Automation; Biodiversity; Conservation; Forestry; Hand held computers; CNN; Forest health monitoring; Hand held device; Mobilenet; Performance; Plant identification; Real- time; Transfer learning; Tree species; Tree species identifications; Deep learning},
	correspondence_address = {A. Hussain; Birmingham City University, School of Computing and Digital Technology, United Kingdom; email: ambreen.hussain@bcu.ac.uk; A. Taufiq Asyhari; Birmingham City University, School of Computing and Digital Technology, United Kingdom; email: taufiq-a@ieee.org},
	editor = {Yang C.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-186043557-7},
	language = {English},
	abbrev_source_title = {Int. Conf. Autom. Comput.: Syst. Intell. through Autom. Comput., ICAC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 26th International Conference on Automation and Computing, ICAC 2021; Conference date: 2 September 2021 through 4 September 2021; Conference code: 174197; All Open Access, Green Open Access}
}

@CONFERENCE{Sai Kumar2021,
	author = {Sai Kumar, S.T. and Prabalakshmi, A. and Arunaggiri Pandian, K. and Alagammal, S.},
	title = {A Comparative Study on Plant Classification Performance using Deep Learning Optimizers},
	year = {2021},
	journal = {2021 IEEE International Conference on Emerging Trends in Industry 4.0, ETI 4.0 2021},
	doi = {10.1109/ETI4.051663.2021.9619238},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123432460&doi=10.1109%2fETI4.051663.2021.9619238&partnerID=40&md5=27b0c30fd1423f56a5fcc040d4930acd},
	affiliations = {Thiagarajar College of Engineering, Department of Electronics and Communication Engineering, Madurai, India; Mepco Schlenk College of Engineering, Department of Electrical and Electronics Engineering, Sivakasi, India},
	abstract = {Recently, many Deep Learning architectures have been employed in the identification and classification of a wide variety of plants. This research mainly focuses on classifying the medicinal plants that are available in rural areas. To do so, six well-known pre-trained Convolutional Neural Networks (CNN) namely Dense121, InceptionV3, VGG16, Xception, VGG19, and MobileNet, that were trained for the ImageNet dataset, were chosen by implementing Transfer Learning concept. These models were examined with their pre-trained weights for the Rural Medicinal Plant (RMP) dataset that was created using 8 different classes of medicinal plants that sum up to a total of 16000 images. The performance of these models was improved by training through two state-of-the-art Deep Learning optimizers namely, Stochastic Gradient Descent (SGD) and Adam. These models were trained using Keras with a TensorFlow backend. A comparative evaluation was made for these models to identify the model that attains the best classification. The research concluded that for RMP dataset, the MobileNet architecture, in which the training performance was improved with the SGD optimizer is the best suited model to classify medicinal plants and thus proves the novelty of this research. Therefore, the proposed model can be used by traditional medicine practitioners for the identification and classification of medicinal plants.  © 2021 IEEE.},
	author_keywords = {Classification; Convolutional Neural Network; Deep Learning; Medicinal Plants; Transfer Learning},
	keywords = {Classification (of information); Convolution; Convolutional neural networks; Deep learning; Gradient methods; Network architecture; Optimization; Stochastic models; Stochastic systems; Transfer learning; Classification performance; Comparatives studies; Convolutional neural network; Deep learning; Medicinal plants; Optimizers; Performance; Plant classification; Stochastic gradient descent; Transfer learning; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166542237-6},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Emerg. Trends Ind. 4.0, ETI 4.0},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 1st IEEE International Conference on Emerging Trends in Industry 4.0, ETI 4.0 2021; Conference date: 19 May 2021 through 21 May 2021; Conference code: 175124}
}

@CONFERENCE{Divekar2021,
	author = {Divekar, Sarit and Rabaev, Irina and Litvak, Marina},
	title = {Urban Planter: A Web App for Automatic Classification of Urban Plants},
	year = {2021},
	journal = {2021 International Conference on Visual Communications and Image Processing, VCIP 2021 - Proceedings},
	doi = {10.1109/VCIP53242.2021.9675318},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125258054&doi=10.1109%2fVCIP53242.2021.9675318&partnerID=40&md5=f2de052aedcbd3661a6a3a2707689d5a},
	affiliations = {Shamoon College of Engineering, Software Engineering Department, Israel},
	abstract = {Plant classification requires an expert because subtle differences in leaves or petal forms might differentiate between different species. On the contrary, some species are characterized by high variability in appearance. This paper introduces a web app for assisting people in identifying plants for discovering the best growing methods. The uploaded picture is submitted to the back-end server, and a pre-trained neural network classifies it to one of the predefined classes. The classification label and confidence are displayed to the end user on the front-end page. The application focuses on the house and garden plant species that can be grown mainly in a desert climate and are not covered by existing datasets. For training a model, we collected the Urban Planter dataset. The installation code of the alpha version and the demo video of the app can be found on https://github.com/UrbanPlanter/urbanplanterapp. © 2021 IEEE.},
	author_keywords = {Deep Learning; Plant Classification; Urban Plants Dataset; Web App},
	keywords = {Classification (of information); E-learning; HTTP; Plants (botany); Automatic classification; Back-end servers; Classification confidence; Classification labels; Deep learning; Plant classification; Pre-defined class; Trained neural networks; Urban plant dataset; Web App; Deep learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818551-4},
	language = {English},
	abbrev_source_title = {Int. Conf. Vis. Commun. Image Process., VCIP - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2021 International Conference on Visual Communications and Image Processing, VCIP 2021; Conference date: 5 December 2021 through 8 December 2021; Conference code: 176486}
}

@CONFERENCE{Htun2021,
	author = {Htun, Phyu Phyu and Boschetti, Marco and Buriro, Attaullah and Confalonieri, Roberto and Sun, Boyuan and Htwe, Ah Nge and Tillo, Tammam},
	title = {A Lightweight Approach for Wood Hyperspectral Images Classification},
	year = {2021},
	journal = {2021 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2021},
	doi = {10.1109/ICMEW53276.2021.9455943},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130680061&doi=10.1109%2fICMEW53276.2021.9455943&partnerID=40&md5=edf338b2c2cca485ffefa2a345c57ac6},
	affiliations = {University of Computer Studies, Yangon, Myanmar; Microtec Srl/GmbH, Italy; Free University of Bozen-Bolzano, Italy; Indraprastha Institute of Information Technology, Delhi, India},
	abstract = {This paper presents a Convolutional Neural Network (CNN)based spatial classifier to classify hyperspectral images for wood recognition. The spatial classifier is built by adapting the input and output units of Cifar10Net, a conventional image classifier that accepts three-band images as input. Obtained results in terms of accuracy and training time show that the proposed classifier can be trained using few training data, and few computational resources.  © 2021 IEEE.},
	author_keywords = {computer vision; Hyperspectral images classification; wood recognition},
	keywords = {Classification (of information); Convolutional neural networks; Hyperspectral imaging; Image classification; Spectroscopy; Computational resources; Convolutional neural network; Hyperspectral image classification; Image Classifiers; Input and outputs; Network-based; Training data; Training time; Wood recognition; Computer vision},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166544989-2},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Multimed. Expo Workshops, ICMEW},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2021 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2021; Conference date: 5 July 2021 through 9 July 2021; Conference code: 179254}
}

@CONFERENCE{Wan20203341,
	author = {Wan, Weilin and Tang, Bingyu and Sun, Ziheng and Ye, Haochen},
	title = {Design of Fine-grained Plant Dataset and A Plant Image Acquisition Tool},
	year = {2020},
	journal = {Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020},
	pages = {3341 – 3346},
	doi = {10.1109/BigData50022.2020.9377857},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103844604&doi=10.1109%2fBigData50022.2020.9377857&partnerID=40&md5=364c53d531c2daa8e4a1814936a4cdd0},
	affiliations = {Fudan University, School of Computer Science, Shanghai, China},
	abstract = {Fine-grained plant classification has attracted great interest for its wide application. However, there is currently no suitable fine-grained plant dataset for deep learning. In this paper, we propose a method of constructing fine-grained plant dataset with adequate annotations. the dataset can help the methods to achieve better plant classification performance. Moreover, we develop an acquisition tool for collecting fine-grained plant dataset. The tool can mitigate the problem of collecting plant images in the fields. © 2020 IEEE.},
	author_keywords = {acquisition tool; fine-grained classification; fine-grained dataset; plants},
	keywords = {Big data; Deep learning; Image acquisition; Acquisition tools; Fine grained; Plant classification; Classification (of information)},
	editor = {Wu X. and Jermaine C. and Xiong L. and Hu X.T. and Kotevska O. and Lu S. and Xu W. and Aluru S. and Zhai C. and Al-Masri E. and Chen Z. and Saltz J.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172816251-5},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. Big Data, Big Data},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 8th IEEE International Conference on Big Data, Big Data 2020; Conference date: 10 December 2020 through 13 December 2020; Conference code: 168025}
}

@ARTICLE{Fabijańska2021,
	author = {Fabijańska, Anna and Danek, Małgorzata and Barniak, Joanna},
	title = {Wood species automatic identification from wood core images with a residual convolutional neural network},
	year = {2021},
	journal = {Computers and Electronics in Agriculture},
	volume = {181},
	doi = {10.1016/j.compag.2020.105941},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100059260&doi=10.1016%2fj.compag.2020.105941&partnerID=40&md5=7f049dbd180e9a45356143e56733ad33},
	affiliations = {Lodz University of Technology, Institute of Applied Computer Science, 18/22 Stefanowskiego Str., Lodz, 90-924, Poland; AGH University of Science and Technology, Department of Environmental Analysis, Mapping and Economic Geology, Mickiewicza 30 Av., Krakow, 30-059, Poland; AGH University of Science and Technology, Department of General Geology and Geotourism, A. Mickiewicza 30 Av., Krakow, 30-059, Poland},
	abstract = {This paper tackles the problem of automatic tree species identification from scanned images of wood cores. A convolutional neural network with residual connections is proposed to perform this task. The model is applied to consecutive image patches following the sliding window strategy to recognize a patch central pixel's membership. It then decides about the resulting tree species via a majority voting. The model's performance was assessed concerning a dataset of 312 wood core images representing 14 European tree species, including both conifer and angiosperm (ring-porous and diffuse-porous) wood. Two tasks were considered, including wood patch classification and wood core classification. In these tasks, the proposed model correctly recognized species of almost 93% the wood image patches and 98.7% of wood core images. It also outperformed the state-of-the-art convolutional neural network-based competitor by 9% and 3%, respectively. The influence of the model's parameters and training set-up on its performance is analyzed in the manuscript to ensure the highest recognition rates of wood species. The source code of the proposed method is released together with the corresponding image dataset to facilitate the reproduction of results. © 2020 Elsevier B.V.},
	author_keywords = {Convolutional neural network; Deep learning; Residual connections; Texture classification; Wood species identification},
	keywords = {Coniferophyta; Magnoliophyta; Automation; Cell proliferation; Convolution; Facsimile; Forestry; Image processing; Wood; Consecutive images; Diffuse-porous; Image datasets; Scanned images; Sliding-window strategies; State of the art; Training sets; Tree species identifications; artificial neural network; automation; data set; identification method; performance assessment; recognition; wood; Convolutional neural networks},
	correspondence_address = {A. Fabijańska; Lodz University of Technology, Institute of Applied Computer Science, 18/22 Stefanowskiego Str., Lodz, 90-924, Poland; email: anna.fabijanska@p.lodz.pl},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20}
}

@ARTICLE{Pires2021367,
	author = {Pires, Willian Oliveira and Fernandes, Ricardo Corso and de Paula Filho, Pedro Luiz and Candido Junior, Arnaldo and Teixeira, João Paulo},
	title = {Leaf-Based Species Recognition Using Convolutional Neural Networks},
	year = {2021},
	journal = {Communications in Computer and Information Science},
	volume = {1488 CCIS},
	pages = {367 – 380},
	doi = {10.1007/978-3-030-91885-9_27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121833827&doi=10.1007%2f978-3-030-91885-9_27&partnerID=40&md5=cc63d7364b2bcc97f1bf359412879093},
	affiliations = {Federal University of Technology - Paraná, Medianeira Campus, Curitiba, Brazil; Research Centre in Digitalization and Intelligent Robotics (CEDRI) – Instituto Politecnico de Braganca, Braganca, Portugal},
	abstract = {Identifying plant species is an important activity in specie control and preservation. The identification process is carried out mainly by botanists, consisting of a comparison of already known specimens or using the aid of books, manuals or identification keys. Artificial Neural Networks have been shown to perform well in classification problems and are a suitable approach for species identification. This work uses Convolutional Neural Networks to classify tree species by leaf images. In total, 29 species were collected. This work analyzed two network models, Darknet-19 and GoogLeNet (Inception-v3), presenting a comparison between them. The Darknet and GoogLeNet models achieved recognition rates of 86.2% and 90.3%, respectively. © 2021, Springer Nature Switzerland AG.},
	author_keywords = {Deep learning; Leaf recognition; Tree classification},
	keywords = {Convolutional neural networks; Deep learning; Forestry; Plants (botany); Convolutional neural network; Darknets; Deep learning; Identification keys; Identification process; Leaf recognition; Plant species; Species control; Species recognition; Tree classification; Convolution},
	correspondence_address = {A. Candido Junior; Federal University of Technology - Paraná, Medianeira Campus, Curitiba, Brazil; email: arnaldoc@utfpr.edu.br},
	editor = {Pereira A.I. and Fernandes F.P. and Coelho J.P. and Teixeira J.P. and Pacheco M.F. and Alves P. and Lopes R.P.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18650929},
	isbn = {978-303091884-2},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 1st International Conference on Optimization, Learning Algorithms and Applications, OL2A 2021; Conference date: 19 July 2021 through 21 July 2021; Conference code: 269979; All Open Access, Green Open Access}
}

@ARTICLE{Miftahushudur202154,
	author = {Miftahushudur, Tajul and Grieve, Bruce and Yin, Hujun},
	title = {Ensemble Synthetic Oversampling with Manhattan Distance for Unbalanced Hyperspectral Data},
	year = {2021},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {13113 LNCS},
	pages = {54 – 64},
	doi = {10.1007/978-3-030-91608-4_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126435632&doi=10.1007%2f978-3-030-91608-4_6&partnerID=40&md5=74d3730b5b8feb1d5eac274bfb207319},
	affiliations = {Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, M13 9PL, United Kingdom; Research Center for Electronics and Telecommunication, Indonesian Institute of Sciences (LIPI), Bandung, Indonesia},
	abstract = {Hyperspectral imaging is a spectroscopic imaging technique that can cover a broad range of electromagnetic wavelengths and subdivide those into spectral bands. As a consequence, it may distinguish specific features more effectively than conventional colour cameras. This technology has been increasingly used in agriculture for various applications such as crop leaf area index, plant classification and disease monitoring. However, the abundance of information in hyperspectral imagery may cause high dimensionality problem, leading to computational complexity and storage issues. Furthermore, data availability is another major issue. In agriculture application, typically, it is difficult to collect equal number of samples as some classes or diseases are rare while others are abundant and easy to collect. This may give rise to an imbalanced data problem that can severely reduce machine learning performance and introduce bias in performance measurement. In this paper, an oversampling method is proposed based on Safe-Level synthetic minority oversampling technique (Safe-Level SMOTE), which is modified in terms of its k-nearest neighbours (KNN) function to make it fit better with high dimensional data. Using convolutional neural networks (CNN) as the classifier combined with ensemble bagging with differentiated sampling rate (DSR), the approach demonstrates better performances than the other state-of-the-art methods in handling imbalance situations. © 2021, Springer Nature Switzerland AG.},
	author_keywords = {CNN; Ensemble; Hyperspectral imaging; Imbalanced data; Plant analysis; Safe-level SMOTE},
	keywords = {Agriculture; Clustering algorithms; Digital storage; Electromagnetic waves; Nearest neighbor search; Spectroscopy; Convolutional neural network; Ensemble; Hyperspectral Data; Imbalanced data; Manhattan distance; Over sampling; Performance; Plant analysis; Safe-level SMOTE; Spectroscopic imaging techniques; Hyperspectral imaging},
	correspondence_address = {T. Miftahushudur; Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, M13 9PL, United Kingdom; email: mtaj001@lipi.go.id},
	editor = {Camacho D. and Tino P. and Allmendinger R. and Yin H. and Tallón-Ballesteros A.J. and Tang K. and Cho S. and Novais P. and Nascimento S.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303091607-7},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 22nd International Conference on Intelligent Data Engineering and Automated Learning, IDEAL 2021; Conference date: 25 November 2021 through 27 November 2021; Conference code: 269299}
}

@ARTICLE{Jakovlev2021165,
	author = {Jakovlev, Dmitri and Kamaletdinova, Iuliia and Shevlyakov, Georgy},
	title = {Feature-Based Plant Seedlings Classification},
	year = {2021},
	journal = {Smart Innovation, Systems and Technologies},
	volume = {220},
	pages = {165 – 175},
	doi = {10.1007/978-981-33-6632-9_14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105852625&doi=10.1007%2f978-981-33-6632-9_14&partnerID=40&md5=372426b6642c50aca55953b574540aaa},
	affiliations = {Peter the Great St. Petersburg Polytechnic University, St. Petersburg, Russian Federation},
	abstract = {The application of image features in the plant classification task is studied. The used dataset was created at Aarhus University Flakkebjerg Research. This work aims at different approaches in plant species categorization. In this study, the feature-based approach is used. It allows to perform classifying using less computational resources. The features usage is motivated by the following purpose: to distinguish weeds from other plants by selecting the defining features of all classes of plants. The proposed method combines image thresholding, feature selection, and feature extraction for the further multiclass classification by such well-known machine learning algorithms as the support vector machines, K-nearest neighbors, decision tree, and Naive Bayes. To a greater extent, we use computer vision algorithms for the image processing step. The main classification method we choose for the task is the support vector machines: It shows the best performance among other tested algorithms; the K-nearest neighbors algorithm is slightly worse. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Computer vision; Feature extraction; Image processing},
	keywords = {Decision trees; Feature extraction; Learning algorithms; Motion compensation; Nearest neighbor search; Seed; Support vector machines; Classification methods; Computational resources; Computer vision algorithms; Feature based approaches; Image thresholding; K-nearest neighbors; Multi-class classification; Plant classification; Classification (of information)},
	correspondence_address = {G. Shevlyakov; Peter the Great St. Petersburg Polytechnic University, St. Petersburg, Russian Federation; email: georgy.shevlyakov@phmf.spbstu.ru},
	editor = {Voinov N. and Schreck T. and Khan S.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {21903018},
	isbn = {978-981336631-2},
	language = {English},
	abbrev_source_title = {Smart Innov. Syst. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 1st International Scientific Conference on Telecommunications, Computing and Control, TELECCON 2019; Conference date: 18 November 2019 through 19 November 2019; Conference code: 258289}
}

@CONFERENCE{Liu20211128,
	author = {Liu, Yang and Li, Yating and Zhao, Yanjie and Na, Xitai},
	title = {Image Classification and Recognition of Medicinal Plants Based on Convolutional Neural Network},
	year = {2021},
	journal = {International Conference on Communication Technology Proceedings, ICCT},
	volume = {2021-October},
	pages = {1128 – 1133},
	doi = {10.1109/ICCT52962.2021.9658028},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124421560&doi=10.1109%2fICCT52962.2021.9658028&partnerID=40&md5=debcd564b12ed1adb156d731489568e3},
	affiliations = {Inner Mongolia University, College of Electronic Information Engineering, Hohhot, China},
	abstract = {With the development of computer vision, plant identification systems have helped botanists recognize and identify plant species more quickly. We usually differentiate plants of different species based on morphology, texture, color, and other characteristics. These features can exhibit high degrees of similarity, and the differences between species can be minor. This paper proposes a classification and recognition method for medicinal plants based on the ResNet34 convolutional neural network, focusing on the image recognition of medicinal grassland plants. First, we use targeted and effective classifications for species tasks based on the Plant-Net. Then, weakly supervised attention functions are used to learn to locate and recognize more discriminative features and improve the efficiency and accuracy of recognition. Moreover, data augmentation methods combined with label-smooth loss functions are used to improve the generalization of model by introducing more data variances. A comprehensive experiment on a grassland plant dataset demonstrates the effectiveness and superiority of our method over other state-of-the-art methods. The total number of images from the dataset is 5000, and the accuracy rate reaches 96.8%. © 2021 IEEE.},
	author_keywords = {attention learning; convolutional neural network; feature extraction; plant recognition},
	keywords = {Convolution; Convolutional neural networks; Image classification; Plants (botany); Textures; Attention learning; Classification and recognition; Convolutional neural network; Degree of similarity; Features extraction; Images classification; Medicinal plants; Plant identification systems; Plant recognition; Plant species; Image recognition},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166543206-1},
	language = {English},
	abbrev_source_title = {Int. Conf. Commun. Technol. Proc. ICCT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 21st IEEE International Conference on Communication Technology, ICCT 2021; Conference date: 13 October 2021 through 16 October 2021; Conference code: 176156}
}

@CONFERENCE{Luo2021121,
	author = {Luo, Yana and Wang, Zhongsheng},
	title = {An Improved ResNet Algorithm Based On CBAM},
	year = {2021},
	journal = {Proceedings - 2021 International Conference on Computer Network, Electronic and Automation, ICCNEA 2021},
	pages = {121 – 125},
	doi = {10.1109/ICCNEA53019.2021.00036},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123193621&doi=10.1109%2fICCNEA53019.2021.00036&partnerID=40&md5=4319070b8be0f3c62fa3bd51ce29957e},
	affiliations = {School Of Computer Science And Engineering, Xi'an Technological University, Xi'an, 710021, China},
	abstract = {Flower recognition has important application value in the field of flower cultivation and planting. As a kind of fine-grained image recognition, the traditional flower recognition has the problem of low recognition accuracy. In order to solve these problems, this paper proposes a neural network algorithm of ResNet structure that integrates CBAM mechanism, and adds residual blocks of attention modules to the second layer to the fifth layer of the ResNet structure. Finally, the results are output through adaptive average pooling and full connection layer. Experimental results show that the recognition accuracy of the model is close to 98% even when the recognition is difficult and there are few training data. Compared with the traditional deep learning model, the model proposed in this paper significantly improves the recognition accuracy. © 2021 IEEE.},
	author_keywords = {CBAM; Feature Extraction; Image Classification; ResNet},
	keywords = {Deep learning; Image enhancement; Image recognition; Multilayer neural networks; CBAM; Features extraction; Fine grained; Flower recognition; Images classification; Neural networks algorithms; Plantings; Recognition accuracy; Resnet; Second layer; Image classification},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166544486-6},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Comput. Netw., Electron. Autom., ICCNEA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 4th International Conference on Computer Network, Electronic and Automation, ICCNEA 2021; Conference date: 24 September 2021 through 26 September 2021; Conference code: 174432}
}

@ARTICLE{Abouzahir2021179,
	author = {Abouzahir, Saad and Sadik, Mohamed and Sabir, Essaid},
	title = {Bag-of-visual-words-augmented Histogram of Oriented Gradients for efficient weed detection},
	year = {2021},
	journal = {Biosystems Engineering},
	volume = {202},
	pages = {179 – 194},
	doi = {10.1016/j.biosystemseng.2020.11.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099185136&doi=10.1016%2fj.biosystemseng.2020.11.005&partnerID=40&md5=ea5a92351214e43e8e2517ee32913145},
	affiliations = {NEST Research Group, LRI Lab., ENSEM, Hassan II University of Casablanca, Morocco; Department of Computer Science, University of Quebec at Montreal (UQAM), Montreal, H2L 2C4, QC, Canada},
	abstract = {As season-long weeds competition produces important yield losses, early detection of these plants is essential to sustain productivity. Machine vision as a non-destructive surveying technique requires features that can describe weeds in a real field case. Colours and shapes provide good results in controlled conditions. However, when different crops or weeds appear in clusters, such solutions fail to meet satisfactory performance. Therefore, considering features that are less specific to field conditions is crucial for integrated weed management. In this study, we provide effective use of the Histogram of Oriented Gradients (HOG) to improve its performance for weed detection. The concept is based on the Bag-of-Visual-Words (BOVW) approach. We use the HOG blocks as keypoints to generate the visual-words, and the features vectors are the histograms of these visual-words. Next, we use the Backpropagation Neural Network to detect weeds and classify plants for three different crop fields. Namely, we consider sugar-beet, soybean, and carrot as target crops. Results demonstrate that the proposed weed detection system can locate weeds for site-specific treatment and selective spraying of herbicides. The proposed BOVW-based HOG can discriminate between weeds and crops with an accuracy of 97.7%, 93%, and 96.6% in sugar-beet, carrot and soybean fields respectively. For plant classification, our method can classify plants with an accuracy of 90.4%, 92.4%, and 94.1% in sugar-beet, carrot and soybean fields respectively. Our results turn out 37.6% better than the classical HOG that produces an accuracy ranging from 71.2% to 83.3% in weed detection and 49.1%–82.1% in plant classification. © 2020 IAgrE},
	author_keywords = {Bag of visual words; Computer vision; Histogram of oriented gradients; Neural Network; Weed detection},
	keywords = {Backpropagation; Crops; Graphic methods; Sugar beets; Back propagation neural networks; Bag-of-visual-words; Controlled conditions; Field conditions; Histogram of oriented gradients; Histogram of oriented gradients (HOG); Non destructive; Plant classification; Weed control},
	correspondence_address = {S. Abouzahir; NEST Research Group, LRI Lab., ENSEM, Hassan II University of Casablanca, Morocco; email: saad.abouzahir@ensem.ac.ma},
	publisher = {Academic Press},
	issn = {15375110},
	coden = {BEINB},
	language = {English},
	abbrev_source_title = {Biosyst. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16}
}

@ARTICLE{Kainat2021,
	author = {Kainat, Jaweria and Sajid Ullah, Syed and Alharithi, Fahd S. and Alroobaea, Roobaea and Hussain, Saddam and Nazir, Shah},
	title = {Blended Features Classification of Leaf-Based Cucumber Disease Using Image Processing Techniques},
	year = {2021},
	journal = {Complexity},
	volume = {2021},
	doi = {10.1155/2021/9736179},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122883813&doi=10.1155%2f2021%2f9736179&partnerID=40&md5=03615b1491c7f3e12e65f6aaa8dce5b8},
	affiliations = {Department of Computer Science, COMSATS University Islamabad, Wah Cantt, Pakistan; Department of Electrical and Computer Engineering, Villanova University, Villanova, PA, United States; Department of Computer Science, College of Computers and Information Technology, Taif University, P.O. Box 11099, Taif, 21944, Saudi Arabia; School of Digital Science, Universiti Brunei Darussalam, Jalan Tungku Link Gadong, BE1410, Brunei Darussalam; Department of Computer Science, University of Swabi, Khyber Pakhtunkhwa, Swabi, Pakistan},
	abstract = {Existing plant leaf disease detection approaches are based on features of extracting algorithms. These algorithms have some limits in feature selection for the diseased portion, but they can be used in conjunction with other image processing methods. Diseases of a plant can be classified from their symptoms. We proposed a cucumber leaf recognition approach, consisting of five steps: preprocessing, normalization, features extraction, features fusion, and classification. Otsu's thresholding is implemented in preprocessing and Tan-Triggs normalization is applied for normalizing the dataset. During the features extraction step, texture and shape features are extracted. In addition, increasing the instances improves some characteristics. Through a principal component analysis approach, serial feature fusion is employed to provide a feature score. Fused features can be classified through a support vector machine. The accuracy of the Fine KNN is 94.30%, which is higher than the previous work in past papers.  © 2021 Jaweria Kainat et al.},
	keywords = {Classification (of information); Extraction; Image classification; Principal component analysis; Processing; Support vector machines; Textures; Classifieds; Cucumber disease; Detection approach; Feature classification; Features extraction; Features fusions; Image processing technique; Leaf disease detections; Normalisation; Plant leaves; Feature extraction},
	correspondence_address = {S. Hussain; School of Digital Science, Universiti Brunei Darussalam, Gadong, Jalan Tungku Link, BE1410, Brunei Darussalam; email: saddamicup1993@gmail.com; S. Nazir; Department of Computer Science, University of Swabi, Swabi, Khyber Pakhtunkhwa, Pakistan; email: shahnazir@uoswabi.edu.pk},
	publisher = {Hindawi Limited},
	issn = {10762787},
	language = {English},
	abbrev_source_title = {Complexity},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@ARTICLE{Masuzawa2021359,
	author = {Masuzawa, H. and Miura, J.},
	title = {Image-based recognition of green perilla leaves using a deep neural network for robotic harvest support},
	year = {2021},
	journal = {Advanced Robotics},
	volume = {35},
	number = {6},
	pages = {359 – 367},
	doi = {10.1080/01691864.2021.1873846},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099968697&doi=10.1080%2f01691864.2021.1873846&partnerID=40&md5=f067e5e0d8a5dd145b7081c0be7aaa7e},
	affiliations = {Toyohashi University of Technology, Toyohashi, Aichi, Japan},
	abstract = {This paper describes a method of recognizing green perilla leaves using a deep neural network for harvest support in greenhouse horticulture. We are developing a robot for harvest support, which automates the selection and bundling process. In order to manipulate green perilla leaves correctly, the robot needs to precisely estimate their geometrical parameters such as width, height, and orientation. It also needs to detect leaves with anomalies. Therefore, we develop an image-based leaf recognition method, adopting deep neural network (DNN) techniques. To reduce computation time, we design a network for executing multiple tasks simultaneously, namely, segmentation and classification. We also developed an annotated dataset using conventional image processing techniques. Experimental results show the efficiency and effectiveness of the proposed method. © 2021 Informa UK Limited, trading as Taylor & Francis Group and The Robotics Society of Japan.},
	author_keywords = {deep neural network; harvest support robot; Image recognition},
	keywords = {Agricultural robots; Deep neural networks; Geometry; Harvesting; Image processing; Image recognition; Optical character recognition; Robotics; Robots; Computation time; Image processing technique; Image-based; Leaf recognition; Multiple tasks; Perilla; Neural networks},
	correspondence_address = {H. Masuzawa; Toyohashi University of Technology, Toyohashi, 1-1, Hibarigaoka, Tenpaku-cho, 441-8580, Japan; email: masuzawa@aisl.cs.tut.ac.jp},
	publisher = {Robotics Society of Japan},
	issn = {01691864},
	coden = {ADROE},
	language = {English},
	abbrev_source_title = {Adv .Rob.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Li2021,
	author = {Li, Jie and Yang, Jie},
	title = {Supervised Classification of Plant Image Based on Attention Mechanism},
	year = {2021},
	journal = {ICSAI 2021 - 7th International Conference on Systems and Informatics},
	doi = {10.1109/ICSAI53574.2021.9664220},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124938087&doi=10.1109%2fICSAI53574.2021.9664220&partnerID=40&md5=719efcc02698ccb1d68c31706f25364f},
	affiliations = {Shanghai Jiao Tong University, Institute of Image Processing and Pattern Recognition, Shanghai, China},
	abstract = {In view of the wide variety of plants on the earth, the plant species identification is particularly necessary to protect and preserve biodiversity. In this work, we propose a plant image classification method based on the encoder-decoder model with additive attention mechanism to extract plant image features and convert them into text descriptions related to plant features. In a well-trained network, it can successfully classify on the species of the generated plant texts. We show that, the proposed method not only equalizes the results of deep convolutional neural network on classification task, but also uses of the prior information of botanists in classification, and thus provide a significant prediction result.  © 2021 IEEE.},
	author_keywords = {Attention mechanism; component; Deep learning; Machine learning; Plant Recognition; Text classification},
	keywords = {Biodiversity; Character recognition; Classification (of information); Convolutional neural networks; Deep neural networks; Supervised learning; Text processing; Attention mechanisms; Classification methods; Component; Deep learning; Image-based; Images classification; Plant recognition; Plant species identification; Supervised classification; Text classification; Image classification},
	editor = {Yang J. and Li K. and Tu W. and Xiao Z. and Wang L.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166542624-4},
	language = {English},
	abbrev_source_title = {ICSAI - Int. Conf. Syst. Informatics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 7th International Conference on Systems and Informatics, ICSAI 2021; Conference date: 13 November 2021 through 15 November 2021; Conference code: 176292}
}

@CONFERENCE{Atitallah2021573,
	author = {Atitallah, Safa Ben and Driss, Maha and Boulila, Wadii and Koubaa, Anis and Atitallah, Nesrine and Ghézala, Henda Ben},
	title = {An Enhanced Randomly Initialized Convolutional Neural Network for columnar cactus recognition in unmanned aerial vehicle imagery},
	year = {2021},
	journal = {Procedia Computer Science},
	volume = {192},
	pages = {573 – 581},
	doi = {10.1016/j.procs.2021.08.059},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116946797&doi=10.1016%2fj.procs.2021.08.059&partnerID=40&md5=403c592dccaf3cf05399fddb0f69b980},
	affiliations = {RIADI Laboratory, National School of Computer Sciences, University of Manouba, Tunisia; IS Department, College of Computer Science and Engineering, Taibah University, Saudi Arabia; Robotics and Internet-of-Things Laboratory, Prince Sultan University, Saudi Arabia; Faculty of Computer Studies, Arab Open University, Saudi Arabia; CES Laboratory, National Engineering School of Sfax, University of Sfax, Tunisia},
	abstract = {Recently, Convolutional Neural Networks (CNNs) have made a great performance for remote sensing image classification. Plant recognition using CNNs is one of the active deep learning research topics due to its added-value in different related fields, especially environmental conservation and natural areas preservation. Automatic recognition of plants in protected areas helps in the surveillance process of these zones and ensures the sustainability of their ecosystems. In this work, we propose an Enhanced Randomly Initialized Convolutional Neural Network (ERI-CNN) for the recognition of columnar cactus, which is an endemic plant that exists in the Tehuacán-Cuicatlán Valley in southeastern Mexico. We used a public dataset created by a group of researchers that consists of more than 20000 remote sensing images. The experimental results confirm the effectiveness of the proposed model compared to other models reported in the literature like InceptionV3 and the modified LeNet-5 CNN. Our ERI-CNN provides 98% of accuracy, 97% of precision, 97% of recall, 97.5% as f1-score, and 0.056 loss. © 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International.},
	author_keywords = {Columnar cactus; Convolutional neural networks; Randomization; Recognition; Remote sensing images; Weight initialization},
	keywords = {Antennas; Conservation; Convolution; Convolutional neural networks; Deep learning; Environmental protection; Image enhancement; Remote sensing; Sustainable development; Columnar cactus; Convolutional neural network; Performance; Plant recognition; Randomisation; Recognition; Remote sensing image classification; Remote sensing images; Research topics; Weight initialization; Unmanned aerial vehicles (UAV)},
	correspondence_address = {S.B. Atitallah; RIADI Laboratory, National School of Computer Sciences, University of Manouba, Tunisia; email: safa.benatitallah@ensi-uma.tn},
	editor = {Watrobski J. and Salabun W. and Toro C. and Zanni-Merk C. and Howlett R.J. and Jain L.C. and Jain L.C.},
	publisher = {Elsevier B.V.},
	issn = {18770509},
	language = {English},
	abbrev_source_title = {Procedia Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 25th KES International Conference on Knowledge-Based and Intelligent Information and Engineering Systems, KES 2021; Conference date: 8 September 2021 through 10 September 2021; Conference code: 172181; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Wu2021428,
	author = {Wu, Huisi and Shi, Zhouan and Huang, Haiming and Wen, Zhenkun and Sun, Fuchun},
	title = {Automatic Leaf Recognition Based on Attention DenseNet},
	year = {2021},
	journal = {Communications in Computer and Information Science},
	volume = {1397 CCIS},
	pages = {428 – 436},
	doi = {10.1007/978-981-16-2336-3_40},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106422039&doi=10.1007%2f978-981-16-2336-3_40&partnerID=40&md5=18e133ce55bd45114ed59a4a573b1523},
	affiliations = {College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, 518060, China; College of Electronics and Information Engineering, Shenzhen University, Shenzhen, 518060, China; Department of Computer Science and Technology, Tsinghua University, Beijing, 100083, China},
	abstract = {Automatic leaf recognition algorithm is widely used in plant taxonomy, horticulture teaching, traditional Chinese medicine research and plant protection, which is one of the research hotspots in information science. Due to the diversity of plant leaves, the variety of leaf forms, and the susceptibility to seasonal and other external factors, there is often a small inter-class variance and a large intra-class variance, which brings great challenges to the task of automatic leaf recognition. To solve this problem, we propose a leaf recognition algorithm base on the attention mechanism and dense connection. Firstly, base on dense connection, DenseNet is applied to realize the cross-layer learning of our model, which effectively improves the generalization ability of the network to the intra-class variance. At the same time, the learning ability of our model to the discriminative features such as the veins and textures of plant leaves is also improved. Secondly, we also employ the attention mechanism to further enhance the ability of our network in learning discriminative features of plant leaves. The experimental results show that our Attention DenseNet achieves a high accuracy of leaf recognition in our plant leaf database, including the challenging cases. Visual and statistical comparisons with state-of-the-art methods also demonstrate its effectiveness. © 2021, Springer Nature Singapore Pte Ltd.},
	author_keywords = {Attention mechanism; Deep learning; DenseNet; Plant leaf recognition},
	keywords = {Cognitive systems; Learning systems; Medical education; Medicine; Signal processing; Textures; Attention mechanisms; Discriminative features; Generalization ability; Learning abilities; Plant protection; State-of-the-art methods; Statistical comparisons; Traditional Chinese Medicine; Plants (botany)},
	correspondence_address = {Z. Wen; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, 518060, China; email: wenzk@szu.edu.cn},
	editor = {Sun F. and Liu H. and Fang B.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18650929},
	isbn = {978-981162335-6},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 5th International Conference on Cognitive Systems and Signal Processing, ICCSIP 2020; Conference date: 25 December 2020 through 27 December 2020; Conference code: 259729}
}

@ARTICLE{Pushpa2021343,
	author = {Pushpa, B.R. and Amaljith, K.B. and Megha, N.},
	title = {Medicinal Leaves Recognition Using Contour-Based Segmentation},
	year = {2021},
	journal = {Smart Innovation, Systems and Technologies},
	volume = {213 SIST},
	pages = {343 – 355},
	doi = {10.1007/978-981-33-4443-3_33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104781480&doi=10.1007%2f978-981-33-4443-3_33&partnerID=40&md5=d42c855a906204dac918fe2b62c03f5a},
	affiliations = {Department of Computer Science, Amrita School of Arts and Science, Amrita Vishwa Vidyapeetham, Mysore, India},
	abstract = {Classification of medicinal plants is a challenging process through the automated system and achieving proper result is a rigorous work. India is well known for its prosperity of medicinal plants and its medicinal practice. In this modern world a person may know few plants which are common in place. There are wide varieties of plants which are unaware. To come out of this problem and to make use of all the medicinal plants an automated system is useful. The system can be used by researchers, students and in the medicinal production sector. The plant has its own properties and uses, preserving such plants makes very helpful for the future. The existing system also helps in classifying the plants and our proposed study helps in classifying the plants with lower quality images whereas the existing asks for the high-resolution images. One of the major goals of the study is to create native dataset using low cost capturing efforts. Proposed work contains contour-based segmentation which deeply considers leaf morphology, feature extraction using Local Binary Pattern and Wavelet methods and classification using supervised K-NN classifier. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Contour-based segmentation; KNN; LBP; WAVELET},
	keywords = {Automation; Manufacture; Sustainable development; Automated systems; Existing systems; High resolution image; k-NN classifier; Local binary patterns; Medicinal plants; Production sector; Wavelet methods; Plants (botany)},
	correspondence_address = {K.B. Amaljith; Department of Computer Science, Amrita School of Arts and Science, Amrita Vishwa Vidyapeetham, Mysore, India; email: amaljithkb01@gmail.com},
	editor = {Reddy A.N. and Marla D. and Favorskaya M.N. and Satapathy S.C.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {21903018},
	isbn = {978-981334442-6},
	language = {English},
	abbrev_source_title = {Smart Innov. Syst. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: International Conference on Intelligent Manufacturing and Energy Sustainability, ICIMES 2020; Conference date: 21 August 2020 through 22 August 2020; Conference code: 257489}
}

@CONFERENCE{Patil2020,
	author = {Patil, Savitha and Sasikala, M.},
	title = {An automated system for identification of the medicinal leaf using MKSVM},
	year = {2020},
	journal = {Proceedings of 2020 IEEE International Conference on Technology, Engineering, Management for Societal Impact Using Marketing, Entrepreneurship and Talent, TEMSMET 2020},
	doi = {10.1109/TEMSMET51618.2020.9557418},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117446330&doi=10.1109%2fTEMSMET51618.2020.9557418&partnerID=40&md5=203a3395c0099757611163515c9f9440},
	affiliations = {Appa Institute of Engineering and Technology, Department of Computer Science and Engineering, Kalaburagi, India; Godutai Engineering College for Women's Kalaburagi, Department of Electrical and Electronics Engineering, India},
	abstract = {The primary source of traditional medicine is found in medicinal plants. And these protect human health. The resource preservation towards traditional medicine has important implications found by the R&D of medicine leaf. Identifying the medicinal plants manually is a time-consuming process that requires the help of experts for plant identification. This paper comes up with a robotic system for the classification in the medical field, which is towards restricting manual classification, which is based on medicinal plant identification. The proposed system has three modules, namely pre-processing of the image, image feature extraction, and later the image classification. In the initial pre-processing step, the conversion of RGB is conducted to extract the green band in the input images. The median filter method is used to remove noise present in the input images obtained from the green band. In the second step, after pre-processing, some of the features like shape, color, and texture, are extracted from the pre-processed image. The multi kernel-based support vector machine (MKSVM) classifier is used to classify the image as medicinal or regular leaf by the extracted features. The performance of the recommended methodology is examined in terms of different metrics, and performance is compared against different classification methods. Achived accuracy is 95.8%.  © 2020 IEEE.},
	author_keywords = {and RGB image; color feature; median filter; medicinal leaf; multi-kernel support vector machine; shape feature; texture feature},
	keywords = {Automation; Classification (of information); Image classification; Image texture; Plant extracts; Support vector machines; Textures; And RGB image; Color features; Median-Filter; Medicinal leaf; Multi-kernel; Multi-kernel support vector machine; RGB images; Shape features; Support vectors machine; Texture features; Median filters},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-073814271-5},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Technol., Eng., Manag. Soc. Impact Using Mark., Entrep. Talent, TEMSMET},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2020 IEEE International Conference on Technology, Engineering, Management for Societal Impact Using Marketing, Entrepreneurship and Talent, TEMSMET 2020; Conference date: 10 December 2020; Conference code: 172426}
}

@CONFERENCE{Divya202149,
	author = {Divya, P. and Palanivel Rajan, D. and Nithya, K.},
	title = {Review on various deep learning methods adopted to improve the plant leaf disease classification in precision agriculture},
	year = {2021},
	journal = {12th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2021},
	volume = {2021-August},
	pages = {49 – 54},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117789529&partnerID=40&md5=5de344bbcfaed493a6b4a82c3e8ede2e},
	affiliations = {CSE Dept., Bannari Amman Institute of Technology, Tamil nadu, Erode, India; CSE Dept., CMR Engineering College Kandlakoya (V), Telangana, Hyderabad, India},
	abstract = {Precision agriculture achieves substantial development to increase the agricultural yield. Recent studies in perception helps to enhance the precision agriculture by the earlier finding of diseases and infection in plants. In this method, we conduct a review in various Imaging techniques for a crop monitoring method. Here various image processing applications are considered for plants classification, based on the harshness of disease or early detection of stress and classification accuracy. The main focus of this method is the utilization of hyper-spectral imaging and classification of plant health and its ability to classify the disease present in the plant leaves. The study in general considers various deep learning strategies adopted to classify the hyper-spectral images. It further studies the classification accuracy of the deep learning methods like deep Convolutional Neural Network (CNNs), Deep Neural Networks (DNNs) and deep transfer learning. The study further provides the inferences related to the classification of hyper-spectral imaging using deep learning algorithms and its pitfalls like falling at local minima and other related challenges during classification. © Grenze Scientific Society, 2021.},
	author_keywords = {Classification; Crop monitoring; Deep learning algorithm; Precision agriculture},
	keywords = {Convolutional neural networks; Crops; Deep neural networks; Plants (botany); Precision agriculture; Spectroscopy; Agricultural yields; Classification accuracy; Crop monitoring; Deep learning algorithm; Disease classification; Leaf disease; Learning methods; Monitoring methods; Plant leaves; Precision Agriculture; Learning algorithms},
	publisher = {Grenze Scientific Society},
	isbn = {978-000000000-2},
	language = {English},
	abbrev_source_title = {Int. Conf. Adv. Comput., Control, Telecommun. Technol., ACT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 12th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2021; Conference date: 27 August 2021 through 28 August 2021; Conference code: 172116}
}

@CONFERENCE{Arafat2021,
	author = {Arafat, Syed Yasser and Arshad, Nimra and Khan, Rimsha},
	title = {Holistic Based Plant Identification Using Deep Learning},
	year = {2021},
	journal = {ICET 2021 - 16th International Conference on Emerging Technologies 2021, Proceedings},
	doi = {10.1109/ICET54505.2021.9689804},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126810755&doi=10.1109%2fICET54505.2021.9689804&partnerID=40&md5=5db6da5ac97172bed1cb5e0b43b180ab},
	affiliations = {Mirpur Mirpur University of Science and Technology (MUST), Department of Computer Science and Information Technology, Mirpur, 10250, Pakistan},
	abstract = {Plants are the most important forms of life on earth. They are the fundamental asset for human prosperity, furnishing us with oxygen and food. Humans have widely utilized plants in medication, food formation, and the cosmetic business worldwide in their daily life activities. Identification of plants is extremely a challenging task due to the recursive nature of the shapes of plants. Moreover, identification has been done on a few categories and is limited to veins or leaf patterns. This paper introduced an automated plant classification framework for recognizing plants dependent on their holistic shape. The system uses image preprocessing, deep learning techniques, and ResNet18 to identify plants through the camera; it will give their names in Urdu, English, and the plant's scientific name as an output. Moreover, the system can describe the plant along with its audio. The methodology was evaluated using two datasets. The first dataset, GRANDYMU, consists of 70 different plant species containing 3500 images, and the second is the famous SWEDISH leaf dataset. We trained different models based on Resnet18, and the maximum accuracy that we achieved was 99%. Also, we got an accuracy of 99.9% for the latter dataset.  © 2021 IEEE.},
	author_keywords = {CNN; GRANDYMU Dataset; ResNet18; Swedish},
	keywords = {Computer vision; Classification framework; CNN; Daily life activities; GRANDYMU dataset; Image preprocessing; Plant classification; Plant identification; Resnet18; Swedishs; System use; Deep learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166549437-3},
	language = {English},
	abbrev_source_title = {ICET - Int. Conf. Emerg. Technol., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 16th International Conference on Emerging Technologies, ICET 2021; Conference date: 22 December 2021 through 23 December 2021; Conference code: 176852}
}

@ARTICLE{Zheng20211,
	author = {Zheng, Yuhong and Li, Xiaolong and Han, Fugui and Fu, Li and Sun, Jinbo},
	title = {Identification of foliage plants Heuchera based on electrochemical profile of active molecules},
	year = {2021},
	journal = {International Journal of Electrochemical Science},
	volume = {16},
	pages = {1 – 10},
	doi = {10.20964/2021.11.44},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117967362&doi=10.20964%2f2021.11.44&partnerID=40&md5=e109897906a71b11c8d0ec1bf47d655a},
	affiliations = {Institute of Botany, Jiangsu Province & Chinese Academy of Sciences (Nanjing Botanical Garden Mem. Sun Yat-Sen), Nanjing, 210014, China; Key Laboratory of Novel Materials for Sensor of Zhejiang Province, College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; Shuyang Jindi Landscaping Engineering Co. Ltd, Shuyang, 223600, China},
	abstract = {An electrochemical profile of plant tissue can reflect the genetic differences among different plants. In this work, the electrochemical profiles were used to identify 12 Heuchera cultivars. Leaf extracts of Heuchera ‘Rio’, Heuchera ‘Tapestry’, Heuchera ‘Caramel’, Heuchera ‘Paris’, Heuchera ‘Huashiliu’, Heuchera ‘Cherry Cola’, Heuchera ‘Tiramisu’, Heuchera ‘Obsidian’, Heuchera ‘Midnight Ruffles’, Heuchera ‘lectra’, Heuchera ‘Georgia Peach’ and Heuchera ‘Red Wine’ were prepared for recording the electrochemical profiles. These electrochemical profiles were used to construct different pattern recognition strategies and to identify cultivars. The identification of patterns was more accurate than that of individual electrochemical profiles. © 2021. The Authors. Published by ESG (www.electrochemsci.org). This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution license (http://creativecommons.org/licenses/by/4.0/).},
	author_keywords = {Biometrics; Electroanalysis; Heuchera; Leaf extract; Plant identification},
	correspondence_address = {Y. Zheng; Institute of Botany, Jiangsu Province & Chinese Academy of Sciences (Nanjing Botanical Garden Mem. Sun Yat-Sen), Nanjing, 210014, China; email: zhengyuhong@cnbg.net},
	publisher = {Electrochemical Science Group},
	issn = {14523981},
	language = {English},
	abbrev_source_title = {Int.J.Electrochem.Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Sani2021394,
	author = {Sani, Radhwan and Cheaitou, Ali and Rabie, Tamer},
	title = {Autonomous localization of the best depth blob using TOPSIS: application on forage plants for the Sharjah pastures project},
	year = {2021},
	journal = {Proceedings - International Conference on Developments in eSystems Engineering, DeSE},
	volume = {2021-December},
	pages = {394 – 400},
	doi = {10.1109/DESE54285.2021.9719515},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126764646&doi=10.1109%2fDESE54285.2021.9719515&partnerID=40&md5=09c4ba0792eb427b534f5578061345d9},
	affiliations = {Dept. of Industrial Engineering and Engineering Management, College of Engineering, University of Sharjah, Sharjah, United Arab Emirates; Dept. of Computer Engineering, College of Computing and Informatics, University of Sharjah, Sharjah, United Arab Emirates},
	abstract = {This research constitutes a building block in a larger decision support system, which combines computer vision and multi-criteria decision making, to identify indigenous forage plants in close-range aerial images. Such a system enables managing the open pastures project of the Emirate of Sharjah, United Arab Emirates (UAE). The system aims at the identification of forage plants through the detection of plant inflorescences in depth images, and applying TOPSIS, to select and localize the best plant inflorescence.  © 2021 IEEE.},
	author_keywords = {Blobs; Cenchrus ciliaris; Computer Vision; Decision Making Process; Decision Support System; Depth; DSS; forage; Pennisetum divisum; Plant Identification; TOPSIS},
	keywords = {Agriculture; Antennas; Artificial intelligence; Computer vision; Decision making; Vegetation; Blob; Cenchrus ciliaris; Decision-making process; Depth; DSS; Forage; Forage plants; Pennisetum divisum; Plant identification; TOPSIS; Decision support systems},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21611343},
	isbn = {978-166540888-2},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Dev. eSystems Eng., DeSE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 14th International Conference on Developments in eSystems Engineering, DeSE 2021; Conference date: 7 December 2021 through 10 December 2021; Conference code: 177754}
}

@ARTICLE{Joly2021601,
	author = {Joly, Alexis and Goëau, Hervé and Cole, Elijah and Kahl, Stefan and Picek, Lukáš and Glotin, Hervé and Deneu, Benjamin and Servajean, Maximilien and Lorieul, Titouan and Vellinga, Willem-Pier and Bonnet, Pierre and Durso, Andrew M. and de Castañeda, Rafael Ruiz and Eggel, Ivan and Müller, Henning},
	title = {LifeCLEF 2021 Teaser: Biodiversity Identification and Prediction Challenges},
	year = {2021},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12657 LNCS},
	pages = {601 – 607},
	doi = {10.1007/978-3-030-72240-1_70},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107353373&doi=10.1007%2f978-3-030-72240-1_70&partnerID=40&md5=cef207f21d5b3d606884e0fd66290f2d},
	affiliations = {Inria, LIRMM, University of Montpellier, Montpellier, France; CIRAD, UMR AMAP, Montpellier, France; Caltech, Pasadena, United States; Center for Conservation Bioacoustics, Cornell Lab of Ornithology, Cornell University, Ithaca, United States; Dept. of Cybernetics, FAV, University of West Bohemia, Pilsen, Czech Republic; Aix Marseille Univ, Université de Toulon, CNRS, LIS, DYNI, Marseille, France; LIRMM, Université Paul Valéry, University of Montpellier, CNRS, Montpellier, France; INRA, UMR AMAP, Montpellier, France; Xeno-canto Foundation, The Hague, Netherlands; Department of Biological Sciences, Florida Gulf Coast University,  Fort Myers, United States; Institute of Global Health, Faculty of Medicine, University of Geneva, Geneva, Switzerland; HES-SO, Sierre, Switzerland},
	abstract = {Building accurate knowledge of the identity, the geographic distribution and the evolution of species is essential for the sustainable development of humanity, as well as for biodiversity conservation. However, the difficulty of identifying plants and animals in the field is hindering the aggregation of new data and knowledge. Identifying and naming living plants or animals is almost impossible for the general public and is often difficult even for professionals and naturalists. Bridging this gap is a key step towards enabling effective biodiversity monitoring systems. The LifeCLEF campaign, presented in this paper, has been promoting and evaluating advances in this domain since 2011. The 2021 edition proposes four data-oriented challenges related to the identification and prediction of biodiversity: (i) PlantCLEF: cross-domain plant identification based on herbarium sheets, (ii) BirdCLEF: bird species recognition in audio soundscapes, (iii) GeoLifeCLEF: location-based prediction of species based on environmental and occurrence data and (iv) SnakeCLEF: image-based snake identification. © 2021, Springer Nature Switzerland AG.},
	author_keywords = {AI; Biodiversity; Bird identification; Machine learning; Plant identification; Snake identification; Species distribution model; Species identification; Species prediction},
	keywords = {Animals; Biodiversity; Forecasting; Geographical distribution; Historic preservation; Information retrieval; Optical character recognition; Sustainable development; Biodiversity conservation; Biodiversity monitoring; Bird species; Cross-domain; General publics; Location based; Plant identification; Snake identifications; Conservation},
	correspondence_address = {A. Joly; Inria, LIRMM, University of Montpellier, Montpellier, France; email: alexis.joly@inria.fr},
	editor = {Hiemstra D. and Moens M. and Mothe J. and Perego R. and Potthast M. and Sebastiani F.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303072239-5},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 43rd European Conference on Information Retrieval, ECIR 2021; Conference date: 28 March 2021 through 1 April 2021; Conference code: 257189; All Open Access, Green Open Access}
}

@ARTICLE{Yin20211309,
	author = {Yin, Jiankai and Yu, Donglei and Li, Ziyi and Guo, Wei and Zhu, Hao},
	title = {Chinese Rose Flower Disease Recognition Method Based on Deep Learning},
	year = {2021},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {747},
	pages = {1309 – 1319},
	doi = {10.1007/978-981-16-0115-6_149},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122470600&doi=10.1007%2f978-981-16-0115-6_149&partnerID=40&md5=9b4299ff71b048c5cd0e68b3740e69b3},
	affiliations = {Beijing Forestry University, Beijing, China},
	abstract = {In order to solve the problem of low recognition rate and weak robustness of small sample disease leaves in natural environment, taking Chinese rose disease leaves as the research object, a disease leaf recognition model based on deep convolution neural network and compound feature dictionary was proposed. First of all, the transfer learning technique is used to train the FasterR-CNN model to detect the patch area of the diseased leaves. Then, the color feature and SIFT feature are extracted from the whole patch region set by high-density sampling method, and the color feature and SIFT feature vocabulary are established. Finally, the feature extracted from the disease region is mapped in the compound dictionary to obtain the feature histogram, and the support vector machine is used to train the disease recognition model. The experimental results show that when the number of visual words in the compound dictionary is 50, the robustness and real-time performance of disease recognition are good, the average recognition accuracy is 90.83%, and the time of single-frame image is 1.68 s. Under the combination of color features and SIFT features, the average disease recognition accuracy of this method is the highest under natural light conditions, up to 84.16%. At the same time, compared with the traditional word bag method, the average recognition accuracy of this method is improved by 25.43% under the same dataset. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Chinese rose disease; Compound dictionary; FasterR-CNN; Recognition model},
	keywords = {Color; Support vector machines; Chinese rose; Chinese rose disease; Color features; Compound dictionary; Fasterr-CNN; Recognition accuracy; Recognition methods; Recognition models; Rose flower; SIFT Feature; Deep learning},
	correspondence_address = {J. Yin; Beijing Forestry University, Beijing, China; email: bjfuyinjiankai@eiwhy.com},
	editor = {Chang J. and Yen N. and Hung J.C.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18761100},
	isbn = {978-981160114-9},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 10th International Conference on Frontier Computing, FC 2020; Conference date: 10 July 2020 through 13 July 2020; Conference code: 270399}
}

@CONFERENCE{Sachar2021,
	author = {Sachar, S. and Kumar, A.},
	title = {Automatic plant identification using transfer learning},
	year = {2021},
	journal = {IOP Conference Series: Materials Science and Engineering},
	volume = {1022},
	number = {1},
	doi = {10.1088/1757-899X/1022/1/012086},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100799512&doi=10.1088%2f1757-899X%2f1022%2f1%2f012086&partnerID=40&md5=56545252aa7c6e70980cc791668dbff5},
	affiliations = {Department of Computer Science and Applications, Panjab University, Chandigarh, India},
	abstract = {Plant identification is a widely researched area in the field of computer vision. Many attempts have been made to automate the process of plant identification using an image of a part of plant including flower, leaf and bark. Leaf has proven be the most reliable source of information. After exhaustive experiments, we chose to apply transfer learning to compare the feature extraction capabilities of VGG-16, Xception, MobileNetV2 Convolutional Neural Network (CNN) and DenseNet121 architectures to freely available Swedish, Flavia and MalayaKew leaf image datasets. Random Forest is used as classifier to identify the species of given leaf. The evaluations and comparisons of the specified feature extractor models are provided. DenseNet121 achieved maximum accuracy of 100%, 99% and 92.4% respectively in the three datasets. © 2021 Institute of Physics Publishing. All rights reserved.},
	author_keywords = {CNN; DenseNet; Feature extraction; Random Forest; Transfer learning},
	correspondence_address = {S. Sachar; Department of Computer Science and Applications, Panjab University, Chandigarh, India; email: silky.sachar@gmail.com},
	editor = {Khamparia A. and Gupta D. and Manocha A.K. and Khanna A.},
	publisher = {IOP Publishing Ltd},
	issn = {17578981},
	language = {English},
	abbrev_source_title = {IOP Conf. Ser. Mater. Sci. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 1st International Conference on Computational Research and Data Analytics, ICCRDA 2020; Conference date: 24 October 2020; Conference code: 166870; All Open Access, Bronze Open Access}
}

@ARTICLE{Işık2021105,
	author = {Işık, Şahin and Özkan, Kemal},
	title = {Overview of handcrafted features and deep learning models for leaf recognition},
	year = {2021},
	journal = {Journal of Engineering Research (Kuwait)},
	volume = {9},
	number = {1},
	pages = {105 – 116},
	doi = {10.36909/JER.V9I1.7737},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102712189&doi=10.36909%2fJER.V9I1.7737&partnerID=40&md5=92514f63308d47d6cbe4a8b20f2eaa35},
	affiliations = {Department of Computer Engineering, Eskisehir Osmangazi University, Meselik Campus, Eskisehir, 26480, Turkey},
	abstract = {In this study, an automated system for classification of leaf species based on the global and local features is presented by concentrating on a smart and unorthodox decision system. The utilized global features consist of 11 features and are separated into two categories: gross shape features (7) and moment based features (4), respectively. In case of local features, only the curve points on Bézier curves are accepted as discriminative features. With the purpose of reducing the search space and improving the performance of the system, firstly, the class label of leaf object is determined by conducting the global features with respect to predefined threshold values. Once the target class is determined, the local features have been performed on in order to validate the label of leaf sample. After conducting experiments on the K-Nearest Neighbor (K-NN) with Hausdorff distance, this system provides valuable accuracy rate as achieving the 96.78% performance on Flavia and the 94.66% on Swedish dataset. Moreover, by applying a deep learning model, namely, Inception-v3 architecture, the superior results were recorded as 99.11% and 98.95% when compared to state-of-the-art methods. It turns out that one can use our feature extraction and classification technique or Inception-v3 model by considering compromises and commutations about efficiency and effectiveness. © 2021 University of Kuwait. All rights reserved.},
	author_keywords = {Bézier Curve Points; Deep Learning; Gross Shape Features; Hausdorff Distance; Leaf Classification; Moment Based Features},
	correspondence_address = {Ş. Işık; Department of Computer Engineering, Eskisehir Osmangazi University, Meselik Campus, Eskisehir, 26480, Turkey; email: sahini@ogu.edu.tr},
	publisher = {University of Kuwait},
	issn = {23071885},
	language = {English},
	abbrev_source_title = {J. Eng. Res. (Kuwait)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@CONFERENCE{Savindya Somakeerthi20201,
	author = {Savindya Somakeerthi, D.C. and Udani De Silva, G.W.I. and Thenu De Silva, L.D. and Chandrasiri, S. and Joseph, J.K.},
	title = {Amazon biology: An augmented reality-based e-book for biology},
	year = {2020},
	journal = {ICAC 2020 - 2nd International Conference on Advancements in Computing, Proceedings},
	pages = {1 – 6},
	doi = {10.1109/ICAC51239.2020.9357165},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102434197&doi=10.1109%2fICAC51239.2020.9357165&partnerID=40&md5=bbe7fa2dc92b8f64493c9c3417b44a12},
	affiliations = {Sri Lanka Institute of Information Technology, Department of Information Technology, Malabe, Colombo, Sri Lanka},
	abstract = {Biology is a conventionally struggling subject to learn from both high school and college students due to its complexity. Students are used to learning Biology from various methods such as reading textbooks, attending lectures. Biology is based on more practical and most of the schools not available proper lab facilities, anatomic structures, and resources to learn the module easily. And teachers who teach the module face a considerable number of issues when delivering the concepts. Some of them face unavailability of teaching aids, time-consuming, lack of lecture materials. Apart from that, the nature of the topic and the teaching style are the main learning problems faced by the students. Therefore, students do not learn the concepts perfectly and interest in the module has been reduced day by day. To overcome these difficulties 'Amazon Biology, ' mobile application has been proposed. The application consists of three major modules including image processing for the plant classification, augmented reality for human anatomy, and gamification. The proposed application has used the techniques in augmented reality and game-based learning. The developed system delivers nearly 85% level of accuracy and provides more advantages for students. They are effective and efficient learning, teaching via visual materials, and practical. © 2020 IEEE.},
	author_keywords = {Augment reality; Biology; Gamification; Image processing; Mobile application},
	keywords = {Augmented reality; Image processing; Plants (botany); Anatomic structures; College students; Efficient learning; Game-based Learning; Learning problem; Lecture materials; Mobile applications; Plant classification; Students},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818412-8},
	language = {English},
	abbrev_source_title = {ICAC - Int. Conf. Adv. Comput., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2nd International Conference on Advancements in Computing, ICAC 2020; Conference date: 10 December 2020 through 11 December 2020; Conference code: 167441}
}

@ARTICLE{Dorai2021333,
	author = {Dorai, Kavita},
	title = {Nmr spectroscopy and the plant metabolome},
	year = {2021},
	journal = {eMagRes},
	volume = {9},
	number = {4},
	pages = {333 – 348},
	doi = {10.1002/9780470034590.emrstm1629},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100582868&doi=10.1002%2f9780470034590.emrstm1629&partnerID=40&md5=4b59c75b699a1778b3c79c70b1a4606a},
	affiliations = {Indian Institute of Science Education & Research Mohali, Manauli, Punjab, India},
	abstract = {Nuclear magnetic resonance (NMR)-based metabolomics has led to several pathbreaking developments in the characterization of the plant metabolome. New techniques for data processing and analysis of NMR spectra have improved the identification and quantitation of primary and secondary plant metabolites. The exciting possibility of using machine learning algorithms and artificial-intelligence tools such as deep learning and neural networks for multivariate statistical analysis of NMR data has opened up new avenues of research in integrating ‘Big Data’ methods with plant metabolomics. Quantitative metabolite fingerprinting using two-dimensional (2D) ultrafast and high-resolution magic angle spinning (HR-MAS) NMR experiments has provided unique perspectives on the interactions of plant metabolic networks and their responses to external environment stresses. Plant NMR metabolomic studies have contributed significantly to the understanding of plant classification and taxonomy, the interaction of plant metabolic networks, bioactivity and mechanism of action of significant metabolites in medicinally important plants, genetically modified plants and their ecological implications, plant–organism interactions, plant defense against herbivore and pathogen attacks, and plant metabolome response to abiotic stress. © 2021 John Wiley & Sons, Ltd.},
	author_keywords = {2D ultrafast and HR-MAS NMR methods; Big data and statistical analysis; NMR data processing; Plant NMR metabolomics; Plant stress response; Plant–organism interactions},
	keywords = {Big data; Biomolecules; Data handling; Deep learning; Learning algorithms; Magic angle spinning; Metabolism; Metabolites; Nuclear magnetic resonance spectroscopy; 2d ultrafast and high-resolution magic angle spinning nuclear magnetic resonance method; Big data and statistical analyse; High resolution magic angle spinning; Magic angle spinning nuclear magnetic resonance; Metabolomics; Nuclear magnetic resonance data; Nuclear magnetic resonance data processing; Plant nuclear magnetic resonance metabolomic; Plant stress response; Plant–organism interaction; Resonance methods; Ultra-fast; Multivariant analysis},
	publisher = {Blackwell Publishing Ltd},
	issn = {20556101},
	language = {English},
	abbrev_source_title = {eMagRes},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Senevirathne202097,
	author = {Senevirathne, L.P.D.S. and Pathirana, D.P.D.S. and Silva, A.L. and Dissanayaka, M.G.S.R. and Nawinna, D.P. and Ganegoda, D.},
	title = {Mobile-based assistive tool to identify learn medicinal herbs},
	year = {2020},
	journal = {ICAC 2020 - 2nd International Conference on Advancements in Computing, Proceedings},
	pages = {97 – 102},
	doi = {10.1109/ICAC51239.2020.9357247},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102419053&doi=10.1109%2fICAC51239.2020.9357247&partnerID=40&md5=d12ab122a5fa1e9de83753e320b4fa4a},
	affiliations = {SLIIT, Faculty of Computing, Malabe, Sri Lanka},
	abstract = {Sri Lanka is recognized and valued globally due to its rich heritage of tropical plants, herbs and trees. A need for the valuation of valuable herbs are identified among both Sri Lankans as well as tourists. This paper brings forth a solution in distinguishing medicinal herbs through leaves and flowers using deep learning and image processing algorithms via a mobile application. The proposed mobile application identifies a flower and leaf by its morphological features, such as shape, color, texture. The perspective is to achieve highest accuracy for plant identification using image processing. The proposed model revealed an accuracy of 92.5% in the classification of leaves and flowers. Accuracy of 6 different plants are identified using this method. This application also provides Sinhala virtual assistant which enables user to search herbs using the name, which is popular among people, to obtain information about herbs. The main outcome of the virtual assistant of the research is to develop an information retrieval method on medicinal herbs in a more accurate, easy and efficient way. In addition. this application also provides 3D structure of the selected medicinal herb in augmented reality (AR). © 2020 IEEE.},
	author_keywords = {AR; Deep learning; Image Processing; Machine learning; Medicinal Herbs; Morphological; Scientific; Sinhala; TensorFlow; Text Summarizing},
	keywords = {Augmented reality; Deep learning; Mobile computing; Plants (botany); Textures; 3D Structure; Assistive tool; Image processing algorithm; Medicinal herb; Mobile applications; Morphological features; Plant identification; Virtual assistants; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818412-8},
	language = {English},
	abbrev_source_title = {ICAC - Int. Conf. Adv. Comput., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2nd International Conference on Advancements in Computing, ICAC 2020; Conference date: 10 December 2020 through 11 December 2020; Conference code: 167441}
}

@CONFERENCE{Veramendi2021393,
	author = {Veramendi, Wilbur N. Chiuyari and Cruvinel, Paulo E.},
	title = {Algorithm for the Countering Maize Plants Based on UAV, Digital Image Processing and Semantic Modeling},
	year = {2021},
	journal = {Proceedings - 2021 IEEE 15th International Conference on Semantic Computing, ICSC 2021},
	pages = {393 – 397},
	doi = {10.1109/ICSC50631.2021.00072},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102640332&doi=10.1109%2fICSC50631.2021.00072&partnerID=40&md5=27f306f0fa46103fe17eaa4c20549995},
	affiliations = {Embrapa Instrumentation, São Carlos, SP, Brazil; Federal University of Sao Carlos, Post Graduation Program in Computer Science, São Carlos, SP, Brazil},
	abstract = {With the need to increase agricultural production and to avoid loss, this paper presents the development of a new method for counting plants of maize in an agricultural field using spectral images obtained by an UAV, as well as digital processing and semantic modeling techniques. The method is based on the use of the Circular Hough Transform (CHT) in conjunction with the techniques of Backmapping, neighborhood analysis, and a classification of patterns. Both the supper vector machines (SVM) and the neural networks (NN) methods have been evaluated for the classification procedure. Besides, using a computational environment for simulation, previous results have been obtained, i.e., showing not only the usefulness of the direct measures but also an automatic way for the plants identification, counting and height determination of the planted maize. Also, the establishment of a friendly interface has been carried out, which allows the monitoring of the phenological phases involved in the stages of the maize cultivation.  © 2021 IEEE.},
	author_keywords = {Hough Transform; Image Processing; Pest Control; Risk Management; Semantic Decision Making; UAVs},
	keywords = {Agricultural robots; Cultivation; Hough transforms; Semantics; Spectroscopy; Support vector machines; Unmanned aerial vehicles (UAV); Agricultural fields; Agricultural productions; Circular Hough transforms; Classification procedure; Computational environments; Height determination; Neighborhood analysis; Neural network (nn); Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818899-7},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. Semant. Comput., ICSC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 15th IEEE International Conference on Semantic Computing, ICSC 2021; Conference date: 27 January 2021 through 29 January 2021; Conference code: 167666}
}

@ARTICLE{Salve2021313,
	author = {Salve, Pradip and Sardesai, Milind and Yannawar, Pravin},
	title = {Combining Multiple Classifiers Using Hybrid Votes Technique with Leaf Vein Angle, CNN and Gabor Features for Plant Recognition},
	year = {2021},
	journal = {Communications in Computer and Information Science},
	volume = {1381 CCIS},
	pages = {313 – 331},
	doi = {10.1007/978-981-16-0493-5_28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104430326&doi=10.1007%2f978-981-16-0493-5_28&partnerID=40&md5=5e9a7e83031d3dfab7b453ce7c336b71},
	affiliations = {Vision and Intelligence Laboratory, Department of Computer Science and IT, Dr. Babasaheb Ambedkar Marathwada University, Aurangabad, Maharashtra, India; Floristic Research Laboratory, Department of Botany, Savitribai Phule Pune University, Pune, India},
	abstract = {Modern plant identification system highly depends upon robust features and classification algorithms possibly the combination of several modalities. In this paper, we have used deep CNN features as well as we introduce venation angles present between two veins as a features to classify plants. We have employed 5 types of classifiers to classification and results have been compared using evaluation measure Mean Reciprocal Rank (MRR) and F-measure. VISLeaf dataset have been used to perform the experiments, the system achieved the accuracy of 96.67%, 92.76%, 80.00%, 53.89%, 96.67% for SVM, KNN and Naïve Bayes, Tree classifier, Neural Networks, respectively and 97.22% for proposed hybrid votes classifier. © 2021, Springer Nature Singapore Pte Ltd.},
	author_keywords = {Leaf recognition; Multimodal system; Plant classification; Vein angle},
	keywords = {Barium compounds; Image processing; Pattern recognition; Sodium compounds; Support vector machines; Classification algorithm; Evaluation measures; Gabor feature; Mean reciprocal ranks; Multiple classifiers; Plant identification systems; Plant recognition; Tree classifiers; Classification (of information)},
	correspondence_address = {P. Salve; Vision and Intelligence Laboratory, Department of Computer Science and IT, Dr. Babasaheb Ambedkar Marathwada University, Aurangabad, Maharashtra, India; email: pradipslv@gmail.com},
	editor = {Santosh K.C. and Gawali B.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18650929},
	isbn = {978-981160492-8},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 3rd International Conference on Recent Trends in Image Processing and Pattern Recognition, RTIP2R 2020; Conference date: 3 January 2020 through 4 January 2020; Conference code: 255849}
}

@CONFERENCE{Aggarwal2021,
	author = {Aggarwal, Shilpi and Bhatia, Madhulika and Madaan, Rosy and Pandey, Hari Mohan},
	title = {Optimized sequential model for plant recognition in Keras},
	year = {2021},
	journal = {IOP Conference Series: Materials Science and Engineering},
	volume = {1022},
	number = {1},
	doi = {10.1088/1757-899X/1022/1/012118},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100783004&doi=10.1088%2f1757-899X%2f1022%2f1%2f012118&partnerID=40&md5=2fa2a01a04adf321e501b08ae22db1bd},
	affiliations = {Faculty of Engineering and Technology, Manav Rachna International Institute of Research and Studies, Faridabad, (Haryana), India; Department of Computer Science Engineering, Amity University, Noida, (Uttar Pradesh), India; Department of Computer Science, Edge Hill University, Lancashire, United Kingdom},
	abstract = {There are huge varieties of floras in the world. Lots of varieties of species are beneficial to the human's life. Plant recognition is a very important task to segregate the huge amount of floras which belong to various categories. Various researchers have applied different approaches to recognize the plant family. Deep learning is a subset of machine learning. This is one of the accepted technologies that automatically extracts features, processes them and yields the best results. Keras is a widely used deep learning framework which is employed in this work. Five different plant species are chosen as samples among the Indian species, namely OcimumTenuiflorum, Sansevieriatrifasciata, Chlorophytumcomosum, Azadirachtaindica, Aloe Vera. These samples are rich in oxygen. From these samples the features like shape, color, texture, corners are extracted. One hot encoding is also applied onto the target values to optimize the results of recognition. The extracted features are fed into the sequential keras model which recognizes the plant species. The accuracy of the training set is 100 percent and the testing set is 96.7percent.Confusion matrix is drawn to show the correctly classified and misclassified samples. © 2021 Institute of Physics Publishing. All rights reserved.},
	author_keywords = {Keras; Oxygen; Plant Recognition; Plants},
	correspondence_address = {S. Aggarwal; Faculty of Engineering and Technology, Manav Rachna International Institute of Research and Studies, Faridabad, (Haryana), India; email: shilpi.16.g@gmail.com},
	editor = {Khamparia A. and Gupta D. and Manocha A.K. and Khanna A.},
	publisher = {IOP Publishing Ltd},
	issn = {17578981},
	language = {English},
	abbrev_source_title = {IOP Conf. Ser. Mater. Sci. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 1st International Conference on Computational Research and Data Analytics, ICCRDA 2020; Conference date: 24 October 2020; Conference code: 166870; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Chung20211,
	author = {Chung, Yi and Chou, Chih-Ang and Li, Chih-Yang},
	title = {Central attention and a dual path convolutional neural network in real-world tree species recognition},
	year = {2021},
	journal = {International Journal of Environmental Research and Public Health},
	volume = {18},
	number = {3},
	pages = {1 – 29},
	doi = {10.3390/ijerph18030961},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099713264&doi=10.3390%2fijerph18030961&partnerID=40&md5=2bc392f84b82cd1504d401b026626f7c},
	affiliations = {College of Human Development and Health, National Taipei University of Nursing and Health Sciences, Taipei, 11219, Taiwan; Xin Ji International Company, New Taipei, 234014, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, 10617, Taiwan},
	abstract = {Identifying plants is not only the job of professionals, but also useful or essential for the plant lover and the general public. Although deep learning approaches for plant recognition are promising, driven by the success of convolutional neural networks (CNN), their performances are still far from the requirements of an in-field scenario. First, we propose a central attention concept that helps focus on the target instead of backgrounds in the image for tree species recognition. It could prevent model training from confused vision by establishing a dual path CNN deep learning framework, in which the central attention model combined with the CNN model based on Incep-tionV3 were employed to automatically extract the features. These two models were then learned together with a shared classification layer. Experimental results assessed the effectiveness of our proposed approach which outperformed each uni-path alone, and existing methods in the whole plant recognition system. Additionally, we created our own tree image database where each photo contained a wealth of information on the entire tree instead of an individual plant organ. Lastly, we developed a prototype system of an online/offline available tree species identification working on a consumer mobile platform that can identify the tree species not only by image recognition, but also detection and classification in real-time remotely. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Deep learning; Dual path convolutional neural network; Mobile application; Plant recognition; Visual atten-tion},
	keywords = {Attention; Databases, Factual; Neural Networks, Computer; Recognition, Psychology; Trees; artificial neural network; classification; conceptual framework; detection method; identification method; machine learning; pattern recognition; tree; article; attention; comparative effectiveness; consumer; convolutional neural network; deep learning; human; human experiment; mobile application; species identification; vision; attention; factual database; tree},
	correspondence_address = {Y. Chung; College of Human Development and Health, National Taipei University of Nursing and Health Sciences, Taipei, 11219, Taiwan; email: m9306009@gmail.com},
	publisher = {MDPI AG},
	issn = {16617827},
	pmid = {33499249},
	language = {English},
	abbrev_source_title = {Int. J. Environ. Res. Public Health},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Perera2021831,
	author = {Perera, P.S.T. and Arudchelvam, T.},
	title = {Leaf Based Plant Identification System for Sri Lankan Medicinal Plant},
	year = {2021},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {1364 AISC},
	pages = {831 – 836},
	doi = {10.1007/978-3-030-73103-8_59},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105912632&doi=10.1007%2f978-3-030-73103-8_59&partnerID=40&md5=7e9bbd3e09169f71d6eec8316b1ff738},
	affiliations = {Department of Computing and Information Systems, Wayamba University of Sri Lanka, Kuliyapitiya, Sri Lanka},
	abstract = {Plant identification is an interesting and challenging research topic due to the variety of plant species. Identification of medicinal plants is very useful to botanists, industrialists, food engineers and physicians. Among different parts of a plant, leaf is widely used for plant identification because it is usually the most abundant type of data available in botanical reference collections and the easiest to obtain in the field studies. A number of research works have been carried out for plant leaf identification. In this work, a system is developed to identify medicinal plants from Sri Lanka using leaf images. Images of leaves of different plants are obtained from the Flavia dataset. In order to get proper results, leaf images with a white background are used. Pre-processing and feature extraction techniques are used before using a pattern matcher to compare the information from this image with the ones in the database to get potential matches. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author_keywords = {Leaf identification; Medicinal plant; Plant identification},
	keywords = {Computer programming; Computer science; Feature extraction techniques; Medicinal plants; Plant identification; Plant identification systems; Plant species; Pre-processing; Reference collections; Research topics; Plants (botany)},
	correspondence_address = {P.S.T. Perera; Department of Computing and Information Systems, Wayamba University of Sri Lanka, Kuliyapitiya, Sri Lanka; email: psandunitharakap@gmail.com},
	editor = {Arai K.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {21945357},
	isbn = {978-303073102-1},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: Future of Information and Communication Conference, FICC 2021; Conference date: 29 April 2021 through 30 April 2021; Conference code: 257959}
}

@ARTICLE{Tavakoli2021,
	author = {Tavakoli, H. and Alirezazadeh, P. and Hedayatipour, A. and Banijamali Nasib, A.H. and Landwehr, N.},
	title = {Leaf image-based classification of some common bean cultivars using discriminative convolutional neural networks},
	year = {2021},
	journal = {Computers and Electronics in Agriculture},
	volume = {181},
	doi = {10.1016/j.compag.2020.105935},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098686666&doi=10.1016%2fj.compag.2020.105935&partnerID=40&md5=15f5084f0b07761f40adbc353a4330f0},
	affiliations = {Department of Engineering for Crop Production, Leibniz Institute for Agricultural Engineering and Bioeconomy e.V. (ATB), Max-Eyth-Allee 100, Potsdam, 14469, Germany; JRG Data Science in Agriculture, Leibniz Institute for Agricultural Engineering and Bioeconomy e.V. (ATB), Max-Eyth-Allee 100, Potsdam, 14469, Germany; Agricultural and Natural Resources Research Center of Markazi Province, Arak, 38135-889, Iran; Department of Mechanical Engineering of Biosystems, Faculty of Agriculture, Arak University, Arak, 38156-8-8349, Iran; Department of Computer Science, University of Potsdam, August-Bebel-Str. 89, Potsdam, 14482, Germany},
	abstract = {In recent years, many efforts have been made to apply image processing techniques for plant leaf identification. However, categorizing leaf images at the cultivar/variety level, because of the very low inter-class variability, is still a challenging task. In this research, we propose an automatic discriminative method based on convolutional neural networks (CNNs) for classifying 12 different cultivars of common beans that belong to three various species. We show that employing advanced loss functions, such as Additive Angular Margin Loss and Large Margin Cosine Loss, instead of the standard softmax loss function for the classification can yield better discrimination between classes and thereby mitigate the problem of low inter-class variability. The method was evaluated by classifying species (level I), cultivars from the same species (level II), and cultivars from different species (level III), based on images from the leaf foreside and backside. The results indicate that the performance of the classification algorithm on the leaf backside image dataset is superior. The maximum mean classification accuracies of 95.86, 91.37 and 86.87% were obtained at the levels I, II and III, respectively. The proposed method outperforms the previous relevant works and provides a reliable approach for plant cultivars identification. © 2020 Elsevier B.V.},
	author_keywords = {Bean; Digital image analysis; Loss functions; Plant identification; VGG16},
	keywords = {Classification (of information); Convolution; Image classification; Plants (botany); Classification algorithm; Discriminative methods; Image datasets; Image processing technique; Large margins; Loss functions; Mean classification; Plant cultivars; artificial neural network; automation; cultivar; discriminant analysis; image classification; leaf; legume; Convolutional neural networks},
	correspondence_address = {H. Tavakoli; Department of Engineering for Crop Production, Leibniz Institute for Agricultural Engineering and Bioeconomy e.V. (ATB), Potsdam, Max-Eyth-Allee 100, 14469, Germany; email: htavakoli@atb-potsdam.de},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24}
}

@ARTICLE{Ahila Priyadharshini2021268,
	author = {Ahila Priyadharshini, R. and Arivazhagan, S. and Arun, M.},
	title = {Ayurvedic Medicinal Plants Identification: A Comparative Study on Feature Extraction Methods},
	year = {2021},
	journal = {Communications in Computer and Information Science},
	volume = {1377 CCIS},
	pages = {268 – 280},
	doi = {10.1007/978-981-16-1092-9_23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107376167&doi=10.1007%2f978-981-16-1092-9_23&partnerID=40&md5=ab7b20b309ff8c85571baa4d9613efde},
	affiliations = {Centre for Image Processing and Pattern Recognition, Mepco Schlenk Engineering College, Sivakasi, Tamil Nadu, India},
	abstract = {Proper identification of medicinal plants is essential for agronomists, ayurvedic medicinal practitioners and for ayurvedic medicines industry. Even though many plant leaf databases are available publicly, no specific standardized database is available for Indian Ayurvedic Plant species. In this paper, we introduce a publicly available annotated database of Indian medicinal plant leaf images named as MepcoTropicLeaf. The research work also presents the preliminary results on recognizing the plant species based on the spatial, spectral and machine learnt features on the selected set of 50 species from the database. To attain the machine learnt features, we propose a six level convolutional neural network (CNN) and report an accuracy of 87.25% using machine learnt features. © 2021, Springer Nature Singapore Pte Ltd.},
	author_keywords = {CNN; DWT; HOG; Medicinal plant; Moments},
	keywords = {Convolutional neural networks; Database systems; Plants (botany); Annotated database; Ayurvedic medicine; Comparative studies; Feature extraction methods; Medicinal plants; Plant leaf; Plant species; Computer vision},
	correspondence_address = {R. Ahila Priyadharshini; Centre for Image Processing and Pattern Recognition, Mepco Schlenk Engineering College, Sivakasi, Tamil Nadu, India; email: rahila@mepcoeng.ac.in},
	editor = {Singh S.K. and Roy P. and Raman B. and Nagabhushan P.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18650929},
	isbn = {978-981161091-2},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 5th International Conference on Computer Vision and Image Processing, CVIP 2020; Conference date: 4 December 2020 through 6 December 2020; Conference code: 256959}
}

@CONFERENCE{Li2021343,
	author = {Li, Guangye and Kong, Mengfan and Wang, Shihao},
	title = {Research on Plant Recognition Algorithm Based on YOLOV3 in Complex Scenes},
	year = {2021},
	journal = {Proceedings - 2021 International Conference on Computer Information Science and Artificial Intelligence, CISAI 2021},
	pages = {343 – 347},
	doi = {10.1109/CISAI54367.2021.00072},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127414368&doi=10.1109%2fCISAI54367.2021.00072&partnerID=40&md5=088a7ae4011df936c05a38d53b80bd3a},
	affiliations = {Xi'An Jiaotong-Liverpool University School of Advanced Technology, Suzhou, China; Jilin University, School of Management, Changchun, China; Central South University, School of Computer Science and Engineering, Changsha, China},
	abstract = {Recent advances in hardware capabilities and machine learning technologies have made it possible to monitor plants and their environments. In this work, the paper applies deep learning methods to detect and identify plants in digital images and report experiments conducted at the experimental workstation. Yolov3 was used to detect and classify some plants from 9051 digital images, with an average accuracy of80.15%.  © 2021 IEEE.},
	author_keywords = {Complex Scenes; Deep Learning; Image Processing; Plant Recognition},
	keywords = {Image processing; Complex scenes; Deep learning; Digital image; Images processing; Learning methods; Machine learning technology; Plant recognition; Recognition algorithm; Deep learning},
	correspondence_address = {G. Li; Xi'An Jiaotong-Liverpool University School of Advanced Technology, Suzhou, China; email: liguangye0816@163.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166540692-5},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Comput. Inf. Sci. Artif. Intell., CISAI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 4th International Conference on Computer Information Science and Artificial Intelligence, CISAI 2021; Conference date: 17 September 2021 through 19 September 2021; Conference code: 177736}
}

@ARTICLE{Wilf202193,
	author = {Wilf, Peter and Wing, Scott L. and Meyer, Herbert W. and Rose, Jacob A. and Saha, Rohit and Serre, Thomas and Cúneo, N. Rubén and Donovan, Michael P. and Erwin, Diane M. and Gandolfo, María A. and González-Akre, Erika and Herrera, Fabiany and Hu, Shusheng and Iglesias, Ari and Johnson, Kirk R. and Karim, Talia S. and Zou, Xiaoyu},
	title = {An image dataset of cleared, x-rayed, and fossil leaves vetted to plant family for human and machine learning},
	year = {2021},
	journal = {PhytoKeys},
	volume = {187},
	pages = {93 – 128},
	doi = {10.3897/PHYTOKEYS.187.72350},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123891749&doi=10.3897%2fPHYTOKEYS.187.72350&partnerID=40&md5=919d2139b54c72105b082ad934e678ec},
	affiliations = {Department of Geosciences and Earth and Environmental Systems Institute, Pennsylvania State University, University Park, PA, 16802, United States; Department of Paleobiology, Smithsonian Institution, Washington, 20013, DC, United States; Florissant Fossil Beds National Monument, National Park Service, Florissant, 80816, CO, United States; School of Engineering, Brown University, Providence, 02912, RI, United States; Department of Cognitive, Linguistic and Psychological Sciences, Carney Institute for Brain Science, Brown University, Providence, 02912, RI, United States; CONICET-Museo Paleontológico Egidio Feruglio, Chubut, Trelew, 9100, Argentina; Department of Paleobotany and Paleoecology, Cleveland Museum of Natural History, Cleveland, 44106, OH, United States; University of California-Berkeley, Museum of Paleontology, Berkeley, 94720, CA, United States; LH Bailey Hortorium, Plant Biology Section, School of Integrative Plant Science, Cornell University, Ithaca, 14853, NY, United States; Conservation Ecology Center, Smithsonian Conservation Biology Institute, National Zoological Park, Front Royal, 22630, VA, United States; Negaunee Integrative Research Center, Field Museum of Natural History, Chicago, 60605, IL, United States; Division of Paleobotany, Peabody Museum of Natural History, Yale University, New Haven, 06520, CT, United States; Instituto de Investigaciones en Biodiversidad y Ambiente INIBIOMA, CONICET-UNComa, San Carlos de Bariloche 8400, Río Negro, Argentina; University of Colorado Museum of Natural History, Boulder, 80503, CO, United States},
	abstract = {Leaves are the most abundant and visible plant organ, both in the modern world and the fossil record. Identifying foliage to the correct plant family based on leaf architecture is a fundamental botanical skill that is also critical for isolated fossil leaves, which often, especially in the Cenozoic, represent extinct genera and species from extant families. Resources focused on leaf identification are remarkably scarce; however, the situation has improved due to the recent proliferation of digitized herbarium material, live-plant identification applications, and online collections of cleared and fossil leaf images. Nevertheless, the need remains for a specialized image dataset for comparative leaf architecture. We address this gap by assembling an open-access database of 30,252 images of vouchered leaf specimens vetted to family level, primarily of angiosperms, including 26,176 images of cleared and x-rayed leaves representing 354 families and 4,076 of fossil leaves from 48 families. The images maintain original resolution, have user-friendly filenames, and are vetted using APG and modern paleobotanical standards. The cleared and x-rayed leaves include the Jack A. Wolfe and Leo J. Hickey contributions to the National Cleared Leaf Collection and a collection of high-resolution scanned x-ray negatives, housed in the Division of Paleobotany, Department of Paleobiology, Smithsonian National Museum of Natural History, Washington D.C.; and the Daniel I. Axelrod Cleared Leaf Collection, housed at the University of California Museum of Paleontology, Berkeley. The fossil images include a sampling of Late Cretaceous to Eocene paleobotanical sites from the Western Hemisphere held at numerous institutions, especially from Florissant Fossil Beds National Monument (late Eocene, Colorado), as well as several other localities from the Late Cretaceous to Eocene of the Western USA and the early Paleogene of Colombia and southern Argentina. The dataset facilitates new research and education opportunities in paleobotany, comparative leaf architecture, systematics, and machine learning. © 2021 Peter Wilf et al. All Rights Reserved.},
	author_keywords = {Angiosperms; cleared leaves; data science; fossil leaves; leaf architecture; paleobotany},
	publisher = {Pensoft Publishers},
	issn = {13142011},
	language = {English},
	abbrev_source_title = {PhytoKeys},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Hassan2021205,
	author = {Hassan, Sk Mahmudul and Maji, Arnab Kumar},
	title = {Comparison of automated leaf recognition techniques},
	year = {2021},
	journal = {International Journal of Intelligent Enterprise},
	volume = {8},
	number = {2-3},
	pages = {205 – 214},
	doi = {10.1504/IJIE.2021.114503},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105207115&doi=10.1504%2fIJIE.2021.114503&partnerID=40&md5=331e5a23c74c064f79525d1a28ed486b},
	affiliations = {Department of Information Technology, North Eastern Hill University, Shillong, India},
	abstract = {Plant plays an important role in different ways in human life and atmosphere. There are large numbers of plant species in the world. Plant species plays a vital role in many domains such as preventing some of the diseases, farming, environment, discovery of new drug and other related areas. Recognition of plant species without expert understanding is a huge task. There has been great demand for applying automatic computer vision technologies to increase botanical knowledge. Using leaf features and traits, the classification and identification of plant is carried out. Leaf features like shape, texture and venation are the features most frequently used to differentiate the plant species. Different methodologies are there to extract the feature and to classify the leaf images using a classifier. In this paper we are going to discuss on different leaf recognition approaches along with feature extraction methods and their performances. © 2021 Inderscience Enterprises Ltd.. All rights reserved.},
	author_keywords = {ANN; Artificial neural network; CNN deep learning; Convolution neural network; PNN; Probabilistic neural network; Support vector machine; SVM},
	correspondence_address = {S.M. Hassan; Department of Information Technology, North Eastern Hill University, Shillong, India; email: hassanmahmudul89@gmail.com},
	publisher = {Inderscience Publishers},
	issn = {17453232},
	language = {English},
	abbrev_source_title = {Int. J. Intell. Enterp.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Donesh2020369,
	author = {Donesh, S. and Piumi Ishanka, U.A.},
	title = {Plant leaf recognition: Comparing contour-based and region-based feature extraction},
	year = {2020},
	journal = {ICAC 2020 - 2nd International Conference on Advancements in Computing, Proceedings},
	pages = {369 – 373},
	doi = {10.1109/ICAC51239.2020.9357152},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102472317&doi=10.1109%2fICAC51239.2020.9357152&partnerID=40&md5=da54c2d13fae49c5ef1055e5c5a9c988},
	affiliations = {Sabaragamuwa University of Sri Lanka, Department of Computing and Information Systems, Faculty of Applied Sciences, Belihuloya, Sri Lanka},
	abstract = {Plants play a vital role in the environment. Identifying them and classifying them is an important task for botanists. This study briefly points out- how to recognize plant species using image processing techniques that can help botanists and scientists, the appropriate features for plant species recognition in feature extraction, how can a classification help to increase the accuracy of the plant leaf classification. There are four major phases used in here for the recognition, and they are image input, image pre-processing, feature extraction, and SVM classification. This automatic recognition system is developed using python with Jupyter Notebook environment gives higher accuracy for the plant recognition for the botanists and comparing the feature extractions such as Contour-based and Region-based to get down more accurate results than previous researches is the main purpose of the proposed study. Contour-based and Region-based features were calculated through equations. SVM classification is used for both feature extraction methods. For individual feature extraction the Contour-based feature extraction is more efficient with 72.25% accuracy than Region-based feature extraction with 70.41% accuracy, and for combining both feature extraction SVM classification gives 68.58% accuracy. Contour-based feature is the most appropriate feature for a plant species recognition. © 2020 IEEE.},
	author_keywords = {Contour-based; Feature extraction; Region-based; SVM classification},
	keywords = {Extraction; Feature extraction; Image segmentation; Support vector machines; Automatic recognition system; Feature extraction methods; Image preprocessing; Image processing technique; Individual features; Plant leaf classifications; Plant recognition; SVM classification; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818412-8},
	language = {English},
	abbrev_source_title = {ICAC - Int. Conf. Adv. Comput., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2nd International Conference on Advancements in Computing, ICAC 2020; Conference date: 10 December 2020 through 11 December 2020; Conference code: 167441}
}

@CONFERENCE{Todorov2021516,
	author = {Todorov, Pavel and Christoff, Nicole},
	title = {Vessels Detection Based on Neural Network with Application in Wood Recognition},
	year = {2021},
	journal = {Proceedings of the 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications, IDAACS 2021},
	volume = {1},
	pages = {516 – 520},
	doi = {10.1109/IDAACS53288.2021.9660958},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124799362&doi=10.1109%2fIDAACS53288.2021.9660958&partnerID=40&md5=ccacb31f0e2fa9575ba148d83faf2a9b},
	affiliations = {Technical University of Sofia, Faculty of Telecommunications, 8 Kl. Ohridski Blvd, Sofia, 1756, Bulgaria},
	abstract = {For the conservation of plant species and the control of timber commerce across the world, tree identification is becoming increasingly crucial. It relies heavily on anatomical traits such as vessels, fibers, parenchyma, and rays. The cell structure of each hardwood species differs greatly across intraspecific species and defines their differences. In this article, we propose a method for the detection of vessels through two steps: feature extraction and classification. They are based on grayscale image entropy and classification using a neural network.  © 2021 IEEE.},
	author_keywords = {Classification; Feature extraction; Vessel detection},
	keywords = {Classification (of information); Conservation; Extraction; Cell structure; Feature extraction and classification; Features extraction; Gray-scale images; Hardwood species; Neural-networks; Plant species; Tree identification; Vessel detection; Wood recognition; Feature extraction},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166542605-3},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Intell. Data Acquis. Adv. Comput. Sys.: Technol. Appl., IDAACS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications, IDAACS 2021; Conference date: 22 September 2021 through 25 September 2021; Conference code: 176232}
}

@ARTICLE{Ramesh20211151,
	author = {Ramesh, K. and Samraj, Andrews},
	title = {Simplified SVD Feature Construction in Multiangle Images to Identify Plant Varieties and Weed Infestation},
	year = {2021},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {1133},
	pages = {1151 – 1164},
	doi = {10.1007/978-981-15-3514-7_85},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090523954&doi=10.1007%2f978-981-15-3514-7_85&partnerID=40&md5=e607aaaba6c4b631bf720645c52d2b4a},
	affiliations = {Department of CS, CA & IT, Karpagam Academy of Higher Education, Coimbatore, India; Department of Computer Science and Engineering, Mahendra Engineering College, Namakkal, Tamilnadu, India},
	abstract = {It is essential to identify the weed growth in plant infection at an earlier stage, which is an important procedure of precision agriculture. In this research paper, we found that the optimum methods by positioning appropriate optical sensors to acquire the most relevant images to interpret the plant, identification of weed infections in the plant bed, and accurate identification of intrusion through the images. We introduced a variant of singular value decomposition for this purpose to achieve the best possible results. The performance of this modified singular value decomposition by our work is found better than the conventional singular value decomposition-based feature extraction. Comparison of aerial and portrait images was done to identify the best choice according to the required identification. © 2021, Springer Nature Singapore Pte Ltd.},
	author_keywords = {Analysis of variance; Image processing; Modified singular value decomposition; Threshold classification},
	keywords = {Agricultural robots; Antennas; Artificial intelligence; Image processing; Best choice; Feature construction; Multi-angle; Optimum method; Portrait image; Research papers; Weed growth; Weed infestation; Singular value decomposition},
	correspondence_address = {K. Ramesh; Department of CS, CA & IT, Karpagam Academy of Higher Education, Coimbatore, India; email: krmca86@gmail.com},
	editor = {Chiplunkar N.N. and Fukao T.},
	publisher = {Springer},
	issn = {21945357},
	isbn = {978-981153513-0},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: International Conference on Artificial Intelligence and Data Engineering, AIDE 2019; Conference date: 23 May 2019 through 24 May 2019; Conference code: 243769}
}

@CONFERENCE{Ohnemuller2021172,
	author = {Ohnemuller, Laurenz and Briassouli, Alexia},
	title = {Improving accuracy and efficiency in plant detection on a novel, benchmarking real-world dataset},
	year = {2021},
	journal = {2021 IEEE International Workshop on Metrology for Agriculture and Forestry, MetroAgriFor 2021 - Proceedings},
	pages = {172 – 176},
	doi = {10.1109/MetroAgriFor52389.2021.9628717},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123398865&doi=10.1109%2fMetroAgriFor52389.2021.9628717&partnerID=40&md5=66fa0ef94730927c80f46b101b73088e},
	affiliations = {Maastricht University, Department of Data Science and Knowledge Engineering, Maastricht, Netherlands},
	abstract = {Detecting plants in images is central in precision agriculture, but can be challenging due to their small size, similarities in appearance, varying lighting and environmental conditions. Moreover, computational capacity in real-world settings may be limited. This work examines how accurate, computationally efficient real-time plant detection can be achieved on the large and varied benchmarking Open Plant Phenotyping Database, by building upon the State-of-the-Art (SoA) Scaled YOLO v4 real-time object detection model. The effect of pre-processing, namely cropping unnecessary information and increasing contrast, is examined and experimentally shown to improve both accuracy and efficiency. Transfer learning is also leveraged for the deployment of Scaled YOLO v4, using pre-trained weights from the MS COCO data set, and shown to lead to a moderate improvement in accuracy. The proposed final model results in approximately 10% higher accuracy than the existing baseline model, on a representative subset of about half of images in the Open Plant Phenotyping Database. Experiments show that plant detection accuracy is improved for most well represented samples, with errors appearing in particularly challenging cases or caused by data imbalance. This shows the proposed method has significant potential for highly accurate and computationally efficient plant detection in real-world environments.  © 2021 IEEE.},
	author_keywords = {object detection; plant classification; precision farming; transfer learning; YOLO},
	keywords = {Agriculture; Benchmarking; Classification (of information); Computational efficiency; Computer vision; Efficiency; Large dataset; Object detection; Computationally efficient; Plant classification; Plant detections; Plant phenotyping; Precision Agriculture; Precision-farming; Real- time; Real-world datasets; Transfer learning; YOLO; Object recognition},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166540533-1},
	language = {English},
	abbrev_source_title = {IEEE Int. Workshop Metrol. Agric. For., MetroAgriFor - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 3rd IEEE International Workshop on Metrology for Agriculture and Forestry, MetroAgriFor 2021; Conference date: 3 November 2021 through 5 November 2021; Conference code: 175226; All Open Access, Green Open Access}
}

@ARTICLE{Qin2021302,
	author = {Qin, Xiao and Shi, Yu and Huang, Xiao and Li, Huiting and Huang, Jiangtao and Yuan, Changan and Liu, Chunxia},
	title = {Attention-Based Deep Multi-scale Network for Plant Leaf Recognition},
	year = {2021},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12836 LNCS},
	pages = {302 – 313},
	doi = {10.1007/978-3-030-84522-3_24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113638418&doi=10.1007%2f978-3-030-84522-3_24&partnerID=40&md5=930e41d3768b8991f2c3481c82678653},
	affiliations = {Nanning Normal University, Nanning, 530299, China; Guangxi Academy of Science, Nanning, 530025, China; Guangxi Technological College of Machinery and Electricity, Nanning, 530007, China},
	abstract = {Plant leaf recognition is a computer vision task used to identify plant species. To address the problem that current plant leaf recognition algorithms have difficulty in recognizing fine-grained leaf classification between classes, this paper proposes a DMSNet (Deep Multi-Scale Network) model, a plant leaf classification algorithm based on multi-scale feature extraction. In order to improve the extraction ability of different fine-grained features of the model, the model is improved on the basis of Multi-scale Backbone Architecture model. In order to achieve better plant leaf classification, a visual attention mechanism module to DMSNet is added and ADMSNet (Attention-based Deep Multi-Scale Network), which makes the model focus more on the plant leaf itself, is proposed, essential features are enhanced, and useless features are suppressed. Experiments on real datasets show that the classification accuracy of the DMSNet model reaches 96.43%. In comparison, the accuracy of ADMSNet with the addition of the attention module reaches 97.39%, and the comparison experiments with ResNet-50, ResNext, Res2Net-50 and Res2Net-101 models on the same dataset show that DMSNet improved the accuracy by 4.6%, 18.57%, 3.72% and 3.84%, respectively. The experimental results confirm that the DMSNet and ADMSNet plant leaf recognition models constructed in this paper can accurately recognize plant leaves and have better performance than the traditional models. © 2021, Springer Nature Switzerland AG.},
	author_keywords = {ECANet; Multi-scale backbone; Plant leaf classification},
	keywords = {Behavioral research; Classification (of information); Computation theory; Extraction; Intelligent computing; Architecture modeling; Classification accuracy; Essential features; Leaf classification; Multi-scale features; Plant leaf classifications; Traditional models; Visual attention mechanisms; Plants (botany)},
	correspondence_address = {J. Huang; Nanning Normal University, Nanning, 530299, China; email: Jiangtao@nnnu.edu.cn},
	editor = {Huang D. and Jo K. and Li J. and Gribova V. and Premaratne P.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303084521-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 17th International Conference on Intelligent Computing, ICIC 2021; Conference date: 12 August 2021 through 15 August 2021; Conference code: 263609}
}

@CONFERENCE{Kaur202178,
	author = {Kaur, Priya Pinder and Singh, Sukhdev},
	title = {Analysis of Multiple Classifiers for Herbal Plant Recognition},
	year = {2021},
	journal = {Proceedings of IEEE International Conference on Signal Processing,Computing and Control},
	volume = {2021-October},
	pages = {78 – 83},
	doi = {10.1109/ISPCC53510.2021.9609426},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123004969&doi=10.1109%2fISPCC53510.2021.9609426&partnerID=40&md5=1980a632ef8f4cb4b6fc261a81ddcebb},
	affiliations = {Punjabi University, Department Of Computer Science, Patiala, India},
	abstract = {In this paper multiple classifiers are used for automatic plant recognition based on the shape of leaf features which are extracted from the leaf images using different herbal plants. Four different classifiers have been used namely SVM, KNN, Random Forest, and Logistic regression. Shape feature is considered as an important feature among all other features like- color, texture, vein structure, etc. as a leaf is available throughout the year and leaf shape contains more features to extract. Geometric shape features that are calculated as leaf's length, width, area, perimeter, area of leaf enclosed in a rectangle, percentage of leaf in the rectangle, calculated pixels of leaf in four different quadrants are extracted during feature extraction. Among all cases, LR (Logistic regression) performed best in 7 cases while RF performed in 5 cases. SVM and LR model classifiers performed best with 95% accuracy for feature 6. KNN and RF model classifiers performed best with 90% and 93% accuracy respectively for feature 3.  © 2021 IEEE.},
	author_keywords = {Feature extraction; Herbal Training dataset; Multiple Classifiers; Plant recognition},
	keywords = {Classification (of information); Decision trees; Extraction; Logistic regression; Support vector machines; Textures; Features extraction; Herbal plants; Herbal training dataset; Important features; Leaf images; Multiple-classifiers; Plant recognition; Random forests; Shape features; Training dataset; Feature extraction},
	editor = {Kumar R. and Jain S. and Sohal H.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {26438615},
	isbn = {978-166542552-0},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Signal Process., Comput. Control, ISPCC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 6th IEEE International Conference on Signal Processing, Computing and Control, ISPCC 2021; Conference date: 7 October 2021 through 9 October 2021; Conference code: 174517}
}

@ARTICLE{Desai202139,
	author = {Desai, Vishwad and Savani, Vijay and Patel, Rutul},
	title = {Plant Classification based on Leaves using Artificial Neural Network},
	year = {2021},
	journal = {International Journal of Integrated Engineering},
	volume = {13},
	number = {6},
	pages = {39 – 49},
	doi = {10.30880/ijie.2021.13.06.003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115933723&doi=10.30880%2fijie.2021.13.06.003&partnerID=40&md5=75d9664a1d9e07fec843b4609cd167fb},
	affiliations = {Alumani EC Dept, Institute of Technology, Nirma University, Ahmedabad, 382481, India; Department of Electronics and Communication Engg, Institute of Technology, Nirma University, Ahmedabad, 382481, India},
	abstract = {Abstract: Manual methods to examine leaf for plant classification can be tedious, therefore, automation is desired. Existing methods try distinctive approaches to accomplish this task. Nowadays, Convolution Neural Networks (CNN) are widely used for such application which achieves higher accuracy. However, CNN's are computationally expensive and require extensive dataset for training. Other existing methods are far less resource expensive but they also have their shortcomings for example, some features cannot be processed accurately with automation, some necessary differentiators are left out. To overcome this, we have proposed a simple Artificial Neural Network (ANN) for automatic classification of plants based on their leaf features. Experimental results show that the proposed algorithm able to achieve an accuracy of 96% by incorporating only a single hidden layer of ANN. Hence, our approach is computationally efficient compared to existing CNN based methods. © 2021. UTHM Publisher. All rights reserved.},
	author_keywords = {Artificial neural network; classification; dimensionality reduction; feature extraction; plant},
	correspondence_address = {V. Savani; Department of Electronics and Communication Engg, Institute of Technology, Nirma University, Ahmedabad, 382481, India; email: vijay.savani@nirmauni.ac.in},
	publisher = {Penerbit UTHM},
	issn = {2229838X},
	language = {English},
	abbrev_source_title = {Int. J. Integr. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Vadlamani2021911,
	author = {Vadlamani, Raghuram and Kramer, Virginia and Schmidt, Karsten},
	title = {Automatic watering of plants in a pot using plant recognition with CNN},
	year = {2021},
	journal = {Proceedings of the 5th International Conference on Electronics, Communication and Aerospace Technology, ICECA 2021},
	pages = {911 – 919},
	doi = {10.1109/ICECA52323.2021.9675994},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125382279&doi=10.1109%2fICECA52323.2021.9675994&partnerID=40&md5=54702535b3a77d06e43fdb4fe382fbd7},
	affiliations = {Frankfurt University of Applied Sciences, Dept. Computer Science and Engineering, Frankfurt, Germany},
	abstract = {The plant's growth is dependent on the soil moisture, but the required soil moisture threshold level is not the same for all the plant species. In the modern era with the combination of advanced Embedded and Computer Vision technologies, it is possible to detect the plant species on an Embedded platform and watering automatically. This paper discusses the implementation and working model which can detect the plant in a pot and water the plant automatically based on the threshold of the soil moisture and plant species. The complete model is built on Jetson Nano Nvidia GPU and a PCB which is de-signed according to the requirements, to control the motor pump and read sensor values. Python is used for the implementation of API in more customized way. The plant species and threshold values can be customized based on the climatic condition of any country. As the model is implemented for automatic watering of the plant in a pot, the calibration of the pot width is also done using the MobileNet SSD V2 algorithm to suggest the required soil moisture content and the water level in the tank.  © 2021 IEEE.},
	author_keywords = {Automatic watering model; Embedded Computer vision; Jetson NanoGPU; Mars rover; MobileNet SSD v2; PCB design; Python},
	keywords = {Computer vision; High level languages; Organic pollutants; Polychlorinated biphenyls; Soil moisture; Water levels; Automatic watering model; Embedded computer vision; Jetson NanoGPU; Mars Rovers; Mobilenet SSD v2; PCB design; Plant growth; Plant recognition; Plant species; Threshold levels; Python},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166543524-6},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Electron., Commun. Aerosp. Technol., ICECA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 5th International Conference on Electronics, Communication and Aerospace Technology, ICECA 2021; Conference date: 2 December 2021 through 4 December 2021; Conference code: 176530}
}

@CONFERENCE{Chulif20211526,
	author = {Chulif, Sophia and Chang, Yang Loong},
	title = {Improved herbarium-field triplet network for cross-domain plant identification: NEUON submission to LifeCLEF 2021 plant},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {2936},
	pages = {1526 – 1539},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113552671&partnerID=40&md5=4c5c619b50e748d4059ae0e222f5b950},
	affiliations = {Department of Artificial Intelligence, NEUON AI, Sarwak, 94300, Malaysia},
	abstract = {This paper presents the submissions made by our team to PlantCLEF 2021. The challenge's goal was to identify plant species based on the test set made from only plant images in the field, given a training dataset consisting of primarily herbarium images. We implemented a two-streamed Herbarium-Field Triplet Loss Network to evaluate the similarity between herbarium and field pairs, thereby matching species from both herbarium and field domains. The network is made from two convolutional neural networks taking herbarium and field images as input, respectively. The network employed is a similar but improved version of our submission to the previous year's challenge [1]. In addition, we trained a one-streamed network taking both herbarium and field images as input to enable the learning of the features of each species irrespective of their domains. We found that an ensemble of these networks performed better than the Herbarium-Field Triplet Loss Network alone. We achieved a Mean Reciprocal Rank (MRR) of 0.181 for the primary metric, which focused on the whole test set. Comparably, we achieved an MRR of 0.158 for the secondary metric, which focused on the subset of species with fewer field training images. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
	author_keywords = {Computer vision; Convolutional neural networks; Cross-domain plant identification; Herbarium; Triplet loss},
	keywords = {Statistical tests; Cross-domain; Loss networks; Mean reciprocal ranks; Plant identification; Plant species; Previous year; Training dataset; Training image; Convolutional neural networks},
	correspondence_address = {S. Chulif; Department of Artificial Intelligence, Sarwak, NEUON AI, 94300, Malaysia; email: sophiadouglas@neuon.ai},
	editor = {Faggioli G. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Ferro N. and University of Padova, Department of Information Engineering, Via Gradenigo 6/b, Padova and Joly A. and University of Montpellier, LIRMM, Inria ZENITH, 161 Rue Ada, Montpellier Cedex 5 and Maistro M. and University of Copenhagen, Department of Computer Science, Universitetsparken 1, Copenhagen and Piroi F. and Vienna University of Technology (TU), Institute of Information Systems Engineering, Favoritenstrasse 9-11/188, Vienna},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2021 Working Notes of CLEF - Conference and Labs of the Evaluation Forum, CLEF-WN 2021; Conference date: 21 September 2021 through 24 September 2021; Conference code: 171327}
}

@ARTICLE{Jabir2021387,
	author = {Jabir, Brahim and Rabhi, Loubna and Falih, Noureddine},
	title = {RNN- and CNN-based weed detection for crop improvement: An overview},
	year = {2021},
	journal = {Foods and Raw Materials},
	volume = {9},
	number = {2},
	pages = {387 – 396},
	doi = {10.21603/2308-4057-2021-2-387-396},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121485200&doi=10.21603%2f2308-4057-2021-2-387-396&partnerID=40&md5=e771bae506a732f1474afc9189df851d},
	affiliations = {Sultan Moulay Slimane University, Beni Mellal, Morocco},
	abstract = {Introduction. Deep learning is a modern technique for image processing and data analysis with promising results and great potential. Successfully applied in various fields, it has recently entered the field of agriculture to address such agricultural problems as disease identification, fruit/plant classification, fruit counting, pest identification, and weed detection. The latter was the subject of our work. Weeds are harmful plants that grow in crops, competing for things like sunlight and water and causing crop yield losses. Traditional data processing techniques have several limitations and consume a lot of time. Therefore, we aimed to take inventory of deep learning networks used in agriculture and conduct experiments to reveal the most efficient ones for weed control. Study objects and methods. We used new advanced algorithms based on deep learning to process data in real time with high precision and efficiency. These algorithms were trained on a dataset containing real images of weeds taken from Moroccan fields. Results and discussion. The analysis of deep learning methods and algorithms trained to detect weeds showed that the Convolutional Neural Network is the most widely used in agriculture and the most efficient in weed detection compared to others, such as the Recurrent Neural Network. Conclusion. Since the Convolutional Neural Network demonstrated excellent accuracy in weed detection, we adopted it in building a smart system for detecting weeds and spraying them in place. © 2021 Jabir et al. All Rights Reserved.},
	author_keywords = {Convolutional Neural Network (CNN); deep learning; Digital agriculture; machine learning; Recurrent Neural Network (RNN); weed detection},
	publisher = {Kemerovo State University},
	issn = {23084057},
	language = {English},
	abbrev_source_title = {Foods Raw Mater.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access}
}

@ARTICLE{Wang2021,
	author = {Wang, Xianfeng and Zhang, Chuanlei and Zhang, Shanwen},
	title = {Multiscale Convolutional Neural Networks with Attention for Plant Species Recognition},
	year = {2021},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2021},
	doi = {10.1155/2021/5529905},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111072029&doi=10.1155%2f2021%2f5529905&partnerID=40&md5=6b338b5166eebbca7e8267e601a8a74c},
	affiliations = {School of Information Engineering, Xijing University, Xi'an, 710123, China; College of Artificial Intelligence, Tianjin University of Science and Technology, Tianjin, 300222, China},
	abstract = {Plant species recognition is a critical step in protecting plant diversity. Leaf-based plant species recognition research is important and challenging due to the large within-class difference and between-class similarity of leaves and the rich inconsistent leaves with different sizes, colors, shapes, textures, and venations. Most existing plant leaf recognition methods typically normalize all leaf images to the same size and then recognize them at one scale, which results in unsatisfactory performances. A novel multiscale convolutional neural network with attention (AMSCNN) model is constructed for plant species recognition. In AMSCNN, multiscale convolution is used to learn the low-frequency and high-frequency features of the input images, and an attention mechanism is utilized to capture rich contextual relationships for better feature extraction and improving network training. Extensive experiments on the plant leaf dataset demonstrate the remarkable performance of AMSCNN compared with the hand-crafted feature-based methods and deep-neural network-based methods. The maximum accuracy attained along with AMSCNN is 95.28%.  © 2021 Xianfeng Wang et al.},
	keywords = {Neural Networks, Computer; Plant Leaves; Convolution; Deep neural networks; Image enhancement; Plants (botany); Textures; Attention mechanisms; Class similarities; Contextual relationships; Feature-based method; High frequency HF; Maximum accuracies; Network training; Plant diversity; plant leaf; Convolutional neural networks},
	correspondence_address = {C. Zhang; College of Artificial Intelligence, Tianjin University of Science and Technology, Tianjin, 300222, China; email: a17647@gmail.com},
	publisher = {Hindawi Limited},
	issn = {16875265},
	pmid = {34285692},
	language = {English},
	abbrev_source_title = {Comput. Intell. Neurosci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Akter2020,
	author = {Akter, Raisa and Hosen, Md Imran},
	title = {CNN-based Leaf Image Classification for Bangladeshi Medicinal Plant Recognition},
	year = {2020},
	journal = {ETCCE 2020 - International Conference on Emerging Technology in Computing, Communication and Electronics},
	doi = {10.1109/ETCCE51779.2020.9350900},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102067203&doi=10.1109%2fETCCE51779.2020.9350900&partnerID=40&md5=893a9fcbf6e75f580e858b413705b5a6},
	affiliations = {Manarat International University, Dept. of Computer Science and Engineering, Savar, Dhaka, Bangladesh},
	abstract = {Classifying plant species has taken much attention in the research area to help people recognizing plants easily. In recent years, the convolutional neural networks (CNN) have achieved tremendous computer vision results, especially in image classification. Usually, humans find it difficult to recognize proper medicinal plants. It requires the intuition of an expert botanist, which is a time consuming manual task. In this research, we proposed an automated system for the medicinal plant classification, which will help people identify useful plant species quickly. A new dataset of 10 medicinal plants of Bangladesh is introduced, collected from different regions across the country, and some state-of-the images collected from different sources. After that, a three-layer convolutional neural network is employed to extract the high-level features for the classification trained with the data augmentation technique. The training process was done on 34123 images, and the experimental result on another 3570 images proved that this method is quite feasible and effective, which gave by a 71.3% accuracy rate. © 2020 IEEE.},
	author_keywords = {Computer Vision; Convolutional Neural Network; Image Recognition; Leaf Classification},
	keywords = {Automation; Classification (of information); Convolution; Convolutional neural networks; Multilayer neural networks; Network layers; Plant extracts; Accuracy rate; Automated systems; Data augmentation; High-level features; Medicinal plants; Plant species; Three-layer; Training process; Image classification},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166541962-8},
	language = {English},
	abbrev_source_title = {ETCCE - Int. Conf. Emerg. Technol. Comput., Commun. Electron.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; Conference name: 2020 International Conference on Emerging Technology in Computing, Communication and Electronics, ETCCE 2020; Conference date: 21 December 2020 through 22 December 2020; Conference code: 167272}
}

@ARTICLE{Kanda2021162590,
	author = {Kanda, Paul Shekonya and Xia, Kewen and Sanusi, Olanrewaju Hazzan},
	title = {A Deep Learning-Based Recognition Technique for Plant Leaf Classification},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {162590 – 162613},
	doi = {10.1109/ACCESS.2021.3131726},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120575513&doi=10.1109%2fACCESS.2021.3131726&partnerID=40&md5=9352b41164283ad80351d803a5e97d69},
	affiliations = {School of Electronic and Information Engineering, Hebei University of Technology, Beichen District, Tianjin, 300401, China; College of Intelligence and Computing, Tianjin University, Jinnan District, Tianjin, 300350, China},
	abstract = {In the practice of plant classification, the design of hand-crafted features is more dependent on the ability of computer vision experts to encode morphological characters that are predefined by botanists. However, the distinct features that each plant has as demonstrated by its leaves can be automatically learned based on the end-to-end advantage of Deep Learning algorithms. Therefore, Deep Learning based plant leaf recognition methods is an important approach nowadays. In this article, we are applying three technologies to achieve a model with high accuracy for plant classification. A Conditional Generative Adversarial Network was used to generate synthetic data, a Convolutional Neural Network was used for feature extraction and the rich extracted features were fed into a Logistic Regression classifier for efficient classification of the plant species. The effectiveness of this method can be seen in the wealth of plant datasets that it was tested on. The paper contains results on seven datasets with different modalities. We utilized both Deep Learning and Logistic regression in effectively classifying the plants using their leaf images with accuracies averaging 96.1% for about eight datasets used, but greater for the individual datasets from 99.0 to 100% on some individual datasets. Extensive experiments on each of the datasets demonstrate the superiority of our method compared with others and are highlighted in our results. © 2013 IEEE.},
	author_keywords = {cGAN; classification; convolutional neural networks; deep learning; feature extraction; leaf images},
	keywords = {Convolution; Data mining; Deep neural networks; Extraction; Feature extraction; Generative adversarial networks; Job analysis; Learning algorithms; Plants (botany); Regression analysis; CGAN; Convolutional neural network; Deep learning; Features extraction; Leaf images; Shape; Task analysis; Vein; Classification (of information)},
	correspondence_address = {K. Xia; School of Electronic and Information Engineering, Hebei University of Technology, Tianjin, Beichen District, 300401, China; email: kwxia@hebut.edu.cn},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access}
}

@ARTICLE{Okada202176,
	author = {Okada, Karla and Santos, Eulanda M. and Carvalho, José Reginaldo H. and Nunes-Silva, Carlos Gustavo and Vasconcelos, Max},
	title = {An application for automatic classification of unconventional food plants},
	year = {2021},
	journal = {Smart Innovation, Systems and Technologies},
	volume = {198 SIST},
	pages = {76 – 85},
	doi = {10.1007/978-3-030-55374-6_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089717155&doi=10.1007%2f978-3-030-55374-6_8&partnerID=40&md5=e79798c99141bc9b8dbcc5aba23f00fc},
	affiliations = {Instituto Ambiental e Tecnológico da Amazônia, Rua Tucanos, 49 - Coroado, Manaus, Brazil; Institute of Computer, Federal University of Amazonas, Av. General Rodrigo Octavio Jordão Ramos, 1200 - Coroado I, Manaus, Brazil; Federal University of Amazonas, Av. General Rodrigo Octavio Jordão Ramos, 1200 - Coroado I, Manaus, Brazil},
	abstract = {Unconventional food plants are eatable species, having one or more parts with nutritional potential, but are not commonly used. These plants have attracted considerable attention, as more and more people have become interested and resorted to these natural resources. In order to be actually consumed, unconventional food plants should be known and disseminated. However, although there are many scientific researches dealing with plants identification in the literature, none of them addresses the automatic identification of these species. This work presents a study focused on the identification of unconventional food plants by means of two different strategies: 1) the classical combination of digital image processing-based feature generation and machine learning; and 2) CNN (Convolutional Neural Network) for feature representation and classification. To do so, the authors generated a database of selected species. The paper also details the process of database collecting, its constitution and representativeness, as well as the experiments performed on comparing the two investigated strategies. In the first strategy, we studied 17 features, including shape and texture features and employed Random Forest as classifier. Since we extracted the features from segmented leaves, the paper also details the segmentation process. Finally, the second strategy applies a CNN pre-trained on ImageNet. The comparative study showed that CNNs achieved lower false positive rates and higher and significant accuracy rates. These results show that computer-aided unconventional food plants identification systems are feasible, which may be important tools to allow non-experts to have access to such a valuable information, since the interested public is large and diverse, ranging from professionals to the general public. © Springer Nature Switzerland AG 2021.},
	author_keywords = {Classification; Deep networks; Digital image processing; Unconventional food plants},
	keywords = {Automation; Convolutional neural networks; Decision trees; Image processing; Plants (botany); Textures; Automatic classification; Comparative studies; False positive rates; Feature generation; Feature representation; Scientific researches; Segmentation process; Shape and textures; Classification (of information)},
	correspondence_address = {K. Okada; Instituto Ambiental e Tecnológico da Amazônia, Manaus, Rua Tucanos, 49 - Coroado, Brazil; email: karla.okada@iatecam.org.br},
	editor = {Pereira L. and Carvalho J.R.H. and Krus P. and Klofsten M. and De Negri V.J.},
	publisher = {Springer},
	issn = {21903018},
	isbn = {978-303055373-9},
	language = {English},
	abbrev_source_title = {Smart Innov. Syst. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: Interdisciplinary Conference on Innovation, Design, Entrepreneurship, And Sustainable Systems, IDEAS 2019; Conference date: 17 June 2019 through 19 June 2019; Conference code: 243609}
}

@ARTICLE{Rathor2021536,
	author = {Rathor, Sandeep},
	title = {Ensemble Based Plant Species Recognition System Using Fusion of Hog and Kaze Approach},
	year = {2021},
	journal = {Communications in Computer and Information Science},
	volume = {1395 CCIS},
	pages = {536 – 545},
	doi = {10.1007/978-981-16-1480-4_48},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107381644&doi=10.1007%2f978-981-16-1480-4_48&partnerID=40&md5=f7cc83255aa3fc06f9591dd6d1a5beb6},
	affiliations = {Department of Computer Engineering and Applications, GLA University, Mathura, India},
	abstract = {Recognition of plant species based on images of leaves has been one of the important challenges faced in the field of machine learning. In this paper, we propose a machine learning-based model which can automatically extract relevant features like edges, shape, etc. using two feature extraction algorithms: Histogram of Oriented Gradients and Kaze. The feature vectors are combined to form a final vector which is then fed into 3 classification algorithms, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Random Forest algorithm. The result of these classifiers is used to create an ensemble, based on the probabilistic voting approach. The accuracy of our proposed system is more than any single classifier i.e. 92.30%. © 2021, Springer Nature Singapore Pte Ltd.},
	author_keywords = {Ensemble based machine learning system; Hog and Kaze approach; KNN; Plant recognition; SVM},
	keywords = {Decision trees; Learning algorithms; Nearest neighbor search; Plants (botany); Support vector machines; Turing machines; Classification algorithm; Feature extraction algorithms; Feature vectors; Histogram of oriented gradients; K nearest neighbor (KNN); Random forest algorithm; Relevant features; Voting approach; Learning systems},
	correspondence_address = {S. Rathor; Department of Computer Engineering and Applications, GLA University, Mathura, India; email: sandeep.rathor@gla.ac.in},
	editor = {Singh P.K. and Veselov G. and Pljonkin A. and Kumar Y. and Paprzycki M. and Zachinyaev Y.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18650929},
	isbn = {978-981161479-8},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Conference on Futuristic Trends in Network and Communication Technologies, FTNCT 2020; Conference date: 14 October 2020 through 16 October 2020; Conference code: 257249}
}

@ARTICLE{Chulif2021173,
	author = {Chulif, Sophia and Chang, Yang Loong},
	title = {Herbarium-Field Triplet Network for Cross-Domain Plant Identification},
	year = {2021},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12880 LNCS},
	pages = {173 – 188},
	doi = {10.1007/978-3-030-85251-1_14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115874300&doi=10.1007%2f978-3-030-85251-1_14&partnerID=40&md5=d0b20836a6a5853d42a97b48afb89698},
	affiliations = {Department of Artificial Intelligence, NEUON AI, Kota Samarahan, Sarawak, 94300, Malaysia},
	abstract = {This paper presents the implementation and performance of a Herbarium-Field Triplet Loss Network to evaluate the herbarium-field similarity of plants which corresponds to the cross-domain plant identification challenge in PlantCLEF 2020. The challenge was designed to assess the use of digitized herbarium specimens on the automated plant identification of data deficient flora. The training data consisted of mainly herbarium images, while the test images were solely photos taken in the field. We trained a two-streamed triplet loss network to maximize the embedding distance of different plant species and at the same time minimize the embedding distance of the same plant species given herbarium-field pairs. The objective is to bring the embedding distance of the same herbarium-field pair closer together while moving the embedding distance of different herbarium-field pairs apart. We achieved a similar result in the test sets regardless of whether the species has many or very few field training data. We obtained a Mean Reciprocal Rank (MRR) of 0.121 on the whole test set and an MRR of 0.108 on the subset of species with fewer field training images, or in other words, rare species. Our main contribution is designing a triplet network for plant recognition based on herbarium-field pairs, which were empirically proven to be more effective in classifying species with fewer or no herbarium-field pairs. © 2021, Springer Nature Switzerland AG.},
	author_keywords = {Computer vision; Convolutional neural networks; Cross-domain plant identification; Triplet loss},
	keywords = {Computer vision; Convolutional neural networks; Convolutional neural network; Cross-domain; Cross-domain plant identification; Embeddings; Loss networks; Plant identification; Plant species; Test sets; Training data; Triplet loss; Embeddings},
	correspondence_address = {Y.L. Chang; Department of Artificial Intelligence, NEUON AI, Kota Samarahan, Sarawak, 94300, Malaysia; email: yangloong@neuon.ai},
	editor = {Candan K.S. and Ionescu B. and Goeuriot L. and Larsen B. and Müller H. and Joly A. and Maistro M. and Piroi F. and Faggioli G. and Ferro N.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303085250-4},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 12th International Conference of the Cross-Language Evaluation Forum for European Languages, CLEF 2021; Conference date: 21 September 2021 through 24 September 2021; Conference code: 265349}
}

@CONFERENCE{Lin2021,
	author = {Lin, Ruikai and Ma, Junwei and Yu, Huiling and Zhang, Yizhuo},
	title = {Accurate recognition method of plant leaves based on multi-feature fusion},
	year = {2021},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {11928},
	doi = {10.1117/12.2611757},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120486824&doi=10.1117%2f12.2611757&partnerID=40&md5=932866c18297ae4b522445326c317138},
	affiliations = {Northeast Forestry University, Heilongjiang Province, Harbin, 150000, China},
	abstract = {During the use of a convolutional neural network to train a recognition model of plant leaves, the convolutional layers focus on the appearance of leaves in learning the features of them, while ignoring their internal texture features, thereby resulting in the misclassification of plant leaves with similar appearance. Aiming at this problem, this paper proposes an accurate identification method of plant leaves based on multi-feature fusion, which can be applied to extract the appearance and texture features of leaves simultaneously, and to conduct fusion and summation for these two types of features. The experimental results indicate that compared with the accuracy of the ordinary convolutional neural network recognition method and traditional machine learning method, the accuracy of this method has been improved substantially.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
	author_keywords = {Appearance feature; Convolutional neural network; Leaf recognition; Texture feature},
	keywords = {Convolutional neural networks; Learning systems; Multilayer neural networks; Plants (botany); Textures; Appearance feature; Convolutional neural network; Identification method; Leaf recognition; Misclassifications; Multi-feature fusion; Plant leaves; Recognition methods; Recognition models; Texture features; Convolution},
	editor = {Wu F. and Cen F.},
	publisher = {SPIE},
	issn = {0277786X},
	isbn = {978-151064724-4},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2021 International Conference on Image Processing and Intelligent Control, IPIC 2021; Conference date: 30 July 2021 through 1 August 2021; Conference code: 174453}
}

@ARTICLE{Raina20211,
	author = {Raina, Akshay and Mahajan, Shubham and Vanipriya, Ch. and Bhardwaj, Anil and Pandit, Amit Kant},
	title = {Classification and Detection of Leaves Using Different Image Processing Techniques},
	year = {2021},
	journal = {Lecture Notes in Networks and Systems},
	volume = {202 LNNS},
	pages = {1 – 5},
	doi = {10.1007/978-981-16-0695-3_1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111254242&doi=10.1007%2f978-981-16-0695-3_1&partnerID=40&md5=bfc13f2520fad0477c8f5f74c6cb0581},
	affiliations = {Shri Mata Vaishno Devi University, Katra, Reasi, Jammu and Kashmir, India; Sir M. Visvesvaraya Institute of Technology, Bangalore, India},
	abstract = {Diversity of plants are present in earth’s nature where each type has its individual exceptional highlights. Because of their gigantic advantages to humankind, many plant species are utilized in everyday lifetime. Hence, precise plant leaf acknowledgment over computer vision techniques has cleared its approach to a few fields such as ayurvedic and analysis of wellbeing matters. Innovation has consistently assumed a fundamental job in all parts of human turn of events. Accomplishing exact acknowledgment and arrangement of plant leaf is consistently a test to specialists. In this work, advance different procedures that are received for pre-preparing include extraction furthermore, characterization of leaf, in view of shape and surface highlights of leaves test. The paper further presents test outcomes did on Flavia dataset in request to perceive, arrange leaves utilizing dim level co-event network and various leveled centroid-based strategies. In this work, about 300 leaves are tested by 30 unique modules with end goal of examination. Abstract should summarize the contents of the paper and should contain at least 70 and at most 150 words. It should be set in 9-point font size and should be inset 1.0 cm from the right and left margins. There should be two blank (10-point) lines before and after the abstract. This document is in the required format. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Gray level co-occurrence network; Image processing; Leaves; Plant classification},
	correspondence_address = {S. Mahajan; Shri Mata Vaishno Devi University, Katra, Reasi, Jammu and Kashmir, India; email: mahajanshubham2232579@gmail.com},
	editor = {Das S. and Mohanty M.N.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {23673370},
	isbn = {978-981160694-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Networks Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: International Conference on Intelligent Computing and Advances in Communication, ICAC 2020; Conference date: 25 November 2020 through 26 November 2020; Conference code: 260599}
}

@CONFERENCE{Zhao202110,
	author = {Zhao, Fan and Zhu, Zheng and Huang, Penggui and Deng, Xiaofan and Sun, Yongke and Mu, Yanxia},
	title = {Research on Rapid Wood Recognition Based on Gray Level Co-occurrence Matrix and ELM},
	year = {2021},
	journal = {Proceedings of 2021 IEEE International Conference on Data Science and Computer Application, ICDSCA 2021},
	pages = {10 – 16},
	doi = {10.1109/ICDSCA53499.2021.9650128},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124179135&doi=10.1109%2fICDSCA53499.2021.9650128&partnerID=40&md5=bd2edbaebf0c8ecd8e1ce21d12d93040},
	affiliations = {Southwest Forestry University, College of Big Data and Intelligent Engineering, Kunming, China; Southwest Forestry University, College of Landscape Architecture and Horticulture, Kunming, China},
	abstract = {Wood recognition based on computer technology relies on a large number of computing services provided by cloud services, which makes it unable to be used in recognition scenarios without a network and cannot run on some low performance devices. These shortcomings limit the application of computer algorithms in wood recognition. The photographs of 13 types of wood that are in circulation in Yunnan Province are taken as the research objects. Based on the gray-level co-occurrence matrix, the wood texture features are extracted, and the extreme learning machine (ELM) is used as the feature classifier, and a method of quickly identifying wood is proposed. Experiments show that compared with traditional machine vision algorithms and deep learning-based wood recognition methods, this method has the characteristics of strong generalization and low energy consumption, and can quickly recognize wood on low-performance hardware. This method can make the recognition model run on some low-performance devices, such as Arduino and Raspberry Pi, which provides a solution for some wood recognition scenarios where the cloud platform cannot be used without a network. © 2021 IEEE.},
	author_keywords = {Extreme learning machine; Gray level co-occurrence matrix; Quick identification; wood recognition},
	keywords = {Deep learning; Knowledge acquisition; Matrix algebra; Textures; Wood; Cloud services; Computer technology; Computing services; Gray-level co-occurrence matrix; Grey-level co-occurrence matrixes; Performance; Quick identification; Research object; Wood recognition; Yunnan province; Energy utilization},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166544053-0},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Data Sci. Comput. Appl., ICDSCA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2021 IEEE International Conference on Data Science and Computer Application, ICDSCA 2021; Conference date: 29 October 2021 through 31 October 2021; Conference code: 175884}
}

@ARTICLE{Hazra2021227,
	author = {Hazra, Dipankar and Bhattacharyya, Debnath and Kim, Tai-hoon},
	title = {A Random Forest-Based Leaf Classification Using Multiple Features},
	year = {2021},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {1280},
	pages = {227 – 239},
	doi = {10.1007/978-981-15-9516-5_20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104899546&doi=10.1007%2f978-981-15-9516-5_20&partnerID=40&md5=0a4bf956a9a4642b96bdaba39793add8},
	affiliations = {Department of Computer Science & Engineering, OmDayal Group of Institutions, Uluberia, Howrah, West Bengal, 711316, India; Department of Computer Science & Engineering, K L Deemed to be University, KLEF, Guntur, 522502, India; School of Economics & Managements, Beijing Jiaotong University, Beijing, China},
	abstract = {A novel method of resultant radial distances, leaf perimeter-based features, and RGB color moments-based leaf classification is proposed in this paper. In the training stage, shape features of plant leaf images are extracted by the resultant radial distances and leaf perimeter-based features; and color features are extracted using RGB color moments. Random forest is constructed with the leaf features where leaf names are the class attribute. In the testing stage, shape feature consisting of resultant radial distances and perimeter-based features, and color feature of the query image is extracted by means of same method. The query leaf image is recognized by the already created random forest in the training stage. The proposed method gives 98% recognition rate, which is similar to state-of-the-art leaf recognition methods. This is mainly due to the resultant radial distances for calculating accurate shape features. The smooth and jagged edges of the leaves are perfectly distinguished by leaf perimeter-based features. RGB moments help to distinguish different colored leaves. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	author_keywords = {Leaf recognition; Perimeter-based features; Random forest; Resultant radial distances; RGB color moments},
	keywords = {Artificial intelligence; Color; Decision trees; Plants (botany); Random forests; Soft computing; Color features; Leaf classification; Leaf recognition; Multiple features; Plant leaf images; Radial distance; Shape features; State of the art; Color image processing},
	correspondence_address = {D. Hazra; Department of Computer Science & Engineering, OmDayal Group of Institutions, Howrah, West Bengal, Uluberia, 711316, India; email: dipankar1998@rediffmail.com},
	editor = {Bhattacharyya D. and Thirupathi Rao N.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {21945357},
	isbn = {978-981159515-8},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: International Conference on Machine Intelligence and Soft Computing, ICMISC 2020; Conference date: 3 September 2020 through 4 September 2020; Conference code: 254739}
}

@CONFERENCE{Mommert20216391,
	author = {Mommert, Michael and Scheibenreif, Linus and Hanna, Joëlle and Borth, Damian},
	title = {POWER PLANT CLASSIFICATION FROM REMOTE IMAGING WITH DEEP LEARNING},
	year = {2021},
	journal = {International Geoscience and Remote Sensing Symposium (IGARSS)},
	volume = {2021-July},
	pages = {6391 – 6394},
	doi = {10.1109/IGARSS47720.2021.9553219},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126015248&doi=10.1109%2fIGARSS47720.2021.9553219&partnerID=40&md5=8b394b99316969f3006a63e20aa6f68d},
	affiliations = {University of St. Gallen, Institute of Computer Science},
	abstract = {Satellite remote imaging enables the detailed study of land use patterns on a global scale. We investigate the possibility to improve the information content of traditional land use classification by identifying the nature of industrial sites from medium-resolution remote sensing images. In this work, we focus on classifying different types of power plants from Sentinel-2 imaging data. Using a ResNet-50 deep learning model, we are able to achieve a mean accuracy of 90.0% in distinguishing 10 different power plant types and a background class. Furthermore, we are able to identify the cooling mechanisms utilized in thermal power plants with a mean accuracy of 87.5%. Our results enable us to qualitatively investigate the energy mix from Sentinel-2 imaging data, and prove the feasibility to classify industrial sites on a global scale from freely available satellite imagery. © 2021 IEEE.},
	author_keywords = {Classification; Deep learning; Land use classification; Remote sensing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166540369-6},
	coden = {IGRSE},
	language = {English},
	abbrev_source_title = {Dig Int Geosci Remote Sens Symp (IGARSS)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2021 IEEE International Geoscience and Remote Sensing Symposium, IGARSS 2021; Conference date: 12 July 2021 through 16 July 2021; Conference code: 176845; All Open Access, Green Open Access}
}

@ARTICLE{Sun20211451,
	author = {Sun, Yongke and Lin, Qizhao and He, Xin and Zhao, Youjie and Dai, Fei and Qiu, Jian and Cao, Yong},
	title = {Wood species recognition with small data: A deep learning approach},
	year = {2021},
	journal = {International Journal of Computational Intelligence Systems},
	volume = {14},
	number = {1},
	pages = {1451 – 1460},
	doi = {10.2991/ijcis.d.210423.001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107896998&doi=10.2991%2fijcis.d.210423.001&partnerID=40&md5=3649af78ecd334d3720b6237b4f3c29d},
	affiliations = {Yunnan Provincial Key Laboratory of Wood Adhesives and Glued Products, Southwest Forestry University, Kunming, Yunnan Province, 650224, China; College of Material Science and Engineering, Southwest Forestry University, Kunming, Yunnan Province, 650224, China; College of Big Data and Intelligent Engineering, Southwest Forestry University, Kunming, Yunnan Province, 650224, China},
	abstract = {Wood species recognition is an important work in the wood trade and wood commercial activities. Although many recognition methods were presented in recent years, the existing wood species recognition methods mainly use shallow recognition models with low accuracy and are still unsatisfying for many real-world applications. Besides, their generalization ability is not strong. In this paper, a novel deep-learning-based wood species recognition method was proposed, which improved the accuracy and generalization greatly. The method uses 20X amplifying glass to acquire wood images, extracts the image features with ResNet50 neural network, refines the features with linear discriminant analysis (LDA), and recognizes the wood species with a KNN classifier. Our data was small, but we adopted transfer learning to improve our method. About 3000 wood images were used in our wood species recognition experiments and our method was executed in 25 rare wood species and the results showed our method had better generalization performance and accuracy. Compared with traditional deep learning our results were obtained from a small amount of data, which just confirmed the effectiveness of our method. © 2021 The Authors. Published by Atlantis Press B.V.},
	author_keywords = {Feature extraction; Generalization performance; KNN; Linear discriminant analysis; ResNet50; Transfer learning; Wood recognition},
	keywords = {Discriminant analysis; Learning systems; Transfer learning; Wood; Generalization ability; Generalization performance; K-NN classifier; Learning approach; Linear discriminant analysis; Recognition methods; Recognition models; Species recognition; Deep learning},
	correspondence_address = {Y. Cao; College of Material Science and Engineering, Southwest Forestry University, Kunming, Yunnan Province, 650224, China; email: cn_caoyong@126.com},
	publisher = {Atlantis Press},
	issn = {18756891},
	language = {English},
	abbrev_source_title = {Int. J. Comput. Intell. Syst.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access}
}@CONFERENCE{Jayalath2019125,
	author = {Jayalath, A.D.A.D.S. and Amarawanshaline, T.G.A.G.D. and Nawinna, D.P. and Nadeeshan, P.V.D. and Jayasuriya, H.P.},
	title = {Identification of Medicinal Plants by Visual Characteristics of Leaves and Flowers},
	year = {2019},
	journal = {2019 IEEE 14th International Conference on Industrial and Information Systems: Engineering for Innovations for Industry 4.0, ICIIS 2019 - Proceedings},
	pages = {125 – 129},
	doi = {10.1109/ICIIS47346.2019.9063275},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084118957&doi=10.1109%2fICIIS47346.2019.9063275&partnerID=40&md5=2cf4452060550f7f5d60dab3494a6ab9},
	affiliations = {SLIIT, Faculty of Computing, Malabe, Sri Lanka},
	abstract = {In Ayurveda medicine, correct identification of medicinal plants is of great importance. Plants are identified by human experts using their visual features and aroma. Incorrect identification of medicinal plants may lead to adverse results. Plant identification can be automated using visual morphological characteristics such as the shape, color, and texture of the leaves and flowers. This paper presents how rare medicinal plants were identified with high accuracy by applying image processing and machine learning capabilities. For this study, a database was created from scanned images of leaves and flowers of rare medicinal plants used in Sri Lankan Ayurveda medicine. Both the front and back sides of leaves and flowers were captured. The leaves are classified based on the unique feature combination. Identification rates up to 98% have been obtained when tested over 10 plants. © 2019 IEEE.},
	author_keywords = {Ayurveda; Classification; Feature extraction; Indigenous medicine; Morphological features; Neural network; Plant identification},
	keywords = {Image processing; Information systems; Information use; Textures; Adverse result; Identification rates; Medicinal plants; Morphological characteristic; Plant identification; Scanned images; Unique features; Visual feature; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172813706-3},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Ind. Inf. Sys.: Eng. Innov. Ind. 4.0, ICIIS - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 14th IEEE International Conference on Industrial and Information Systems, ICIIS 2019; Conference date: 18 December 2019 through 20 December 2019; Conference code: 159233}
}

@ARTICLE{Machhour2020149,
	author = {Machhour, Abderrahmane and Zouhri, Amal and El Mallahi, Mostafa and Lakhliai, Zakia and Tahiri, Ahmed and Chenouni, Driss},
	title = {Plants classification using neural shifted legendre-fourier moments},
	year = {2020},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {684 LNEE},
	pages = {149 – 153},
	doi = {10.1007/978-3-030-53187-4_18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089718659&doi=10.1007%2f978-3-030-53187-4_18&partnerID=40&md5=d9a03c211d2e363083c80c55366c8710},
	affiliations = {Laboratory of Computer Science and Interdisciplinary Physics LIPI, Sidi Mohamed Ben Abdellah University, Fez, Morocco},
	abstract = {Plants are the primary food source of humans. They are the raw material of most medicines. Therefore, it was necessary to use artificial intelligence to help those interested in plants to classify and identify various plants types quickly and accurately. In this article we present Neural Shifted Legendre-Fourier Moments, we used shifted Legendre-Fourier moments to extract features from leaves images and build descriptor vectors. These vectors are the inputs of the artificial neural network. We tested this model on MalayaKew (MK) Leaf dataset and we got important results. The validity of this proposed method has been provided under different transformations. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2020.},
	author_keywords = {Artificial intelligence; Artificial neural network; Deep learning; Descriptor vector; Features extraction; Image classification; Plants classification; Shifted Legendre-Fourier moments},
	keywords = {Artificial intelligence; Fourier transforms; Descriptors; Food sources; Fourier moments; Legendre; Plants (botany)},
	correspondence_address = {A. Machhour; Laboratory of Computer Science and Interdisciplinary Physics LIPI, Sidi Mohamed Ben Abdellah University, Fez, Morocco; email: machhour007@hotmail.com},
	editor = {El Moussati A. and Kpalma K. and Ghaouth Belkasmi M. and Saber M. and Guégan S.},
	publisher = {Springer},
	issn = {18761100},
	isbn = {978-303053186-7},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 1st International Conference on Smart Information and Communication Technologies, SmartICT 2019; Conference date: 26 September 2019 through 28 September 2019; Conference code: 243359}
}

@CONFERENCE{Narvekar2020660,
	author = {Narvekar, Chhaya and Rao, Madhuri},
	title = {Flower classification using CNN and transfer learning in CNN-Agriculture Perspective},
	year = {2020},
	journal = {Proceedings of the 3rd International Conference on Intelligent Sustainable Systems, ICISS 2020},
	pages = {660 – 664},
	doi = {10.1109/ICISS49785.2020.9316030},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100778472&doi=10.1109%2fICISS49785.2020.9316030&partnerID=40&md5=8cf8626afe0c04bcec22701376e62f44},
	affiliations = {Xavier Institute of Engineering, Department of Information Technology, Mumbai, India; TSEC, Department of Information Technology, Bandra, Mumbai, India},
	abstract = {Classiücation of flowers is a difficult task because of the huge number of flowering plant species, which are similar in shape, color and appearance. A flower classification can be used in various applications such as field monitoring, plant identification, medicinal plant, floriculture industry, research in plant taxonomy. In this study, the authors have demonstrated and analyzed the recent developments in deep learning methods such as CNN and transfer learning in CNN. Prototype CNN model architecture proposed and transfer learning approach as well examined on VGG16, MobileNet2 and Resnet50 architecture for flower classification on publicly available flower dataset. © 2020 IEEE.},
	author_keywords = {CNN; Deep Learning; MobileNet; ResNet; Transfer Learning; VGG},
	keywords = {Classification (of information); Deep learning; Learning systems; CNN models; Field monitoring; Flowering plants; Learning methods; Medicinal plants; Plant identification; Plant taxonomy; Transfer learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172817089-3},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Intell. Sustain. Syst., ICISS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27; Conference name: 3rd International Conference on Intelligent Sustainable Systems, ICISS 2020; Conference date: 3 December 2020 through 5 December 2020; Conference code: 166600}
}

@ARTICLE{Joly2020542,
	author = {Joly, Alexis and Goëau, Hervé and Kahl, Stefan and Botella, Christophe and Ruiz De Castaneda, Rafael and Glotin, Hervé and Cole, Elijah and Champ, Julien and Deneu, Benjamin and Servajean, Maximillien and Lorieul, Titouan and Vellinga, Willem-Pier and Stöter, Fabian-Robert and Durso, Andrew and Bonnet, Pierre and Müller, Henning},
	title = {LifeCLEF 2020 teaser: Biodiversity identification and prediction challenges},
	year = {2020},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12036 LNCS},
	pages = {542 – 549},
	doi = {10.1007/978-3-030-45442-5_70},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084172472&doi=10.1007%2f978-3-030-45442-5_70&partnerID=40&md5=0bfc98ebe832fdd0e3970518a3f0c7d7},
	affiliations = {Inria, LIRMM, Montpellier, France; CIRAD, UMR AMAP, Montpellier, France; INRA, UMR AMAP, Montpellier, France; Aix Marseille Univ, Université de Toulon, CNRS, LIS, DYNI, Marseille, France; Xeno-canto Foundation, Amsterdam, Netherlands; HES-SO, Sierre, Switzerland; Chemnitz University of Technology, Chemnitz, Germany; LIRMM, Université Paul Valéry, University of Montpellier, CNRS, Montpellier, France; University of Geneva, Geneva, Switzerland; Caltech, Pasadena, United States},
	abstract = {Building accurate knowledge of the identity, the geographic distribution and the evolution of species is essential for the sustainable development of humanity, as well as for biodiversity conservation. However, the difficulty of identifying plants and animals in the field is hindering the aggregation of new data and knowledge. Identifying and naming living plants or animals is almost impossible for the general public and is often difficult even for professionals and naturalists. Bridging this gap is a key step towards enabling effective biodiversity monitoring systems. The LifeCLEF campaign, presented in this paper, has been promoting and evaluating advances in this domain since 2011. The 2020 edition proposes four data-oriented challenges related to the identification and prediction of biodiversity: (i) PlantCLEF: cross-domain plant identification based on herbarium sheets, (ii) BirdCLEF: bird species recognition in audio soundscapes, (iii) GeoLifeCLEF: location-based prediction of species based on environmental and occurrence data, and (iv) SnakeCLEF: image-based snake identification. © Springer Nature Switzerland AG 2020.},
	author_keywords = {Biodiversity; Bird identification; IA; Machine learning; Plant identification; Snake identification; Species distribution model; Species identification; Species prediction},
	keywords = {Animals; Biodiversity; Forecasting; Geographical distribution; Historic preservation; Optical character recognition; Sustainable development; Biodiversity conservation; Biodiversity monitoring; Bird species; Cross-domain; General publics; Location based; Plant identification; Snake identifications; Conservation},
	correspondence_address = {A. Joly; Inria, LIRMM, Montpellier, France; email: alexis.joly@inria.fr},
	editor = {Jose J.M. and Yilmaz E. and Magalhães J. and Martins F. and Castells P. and Ferro N. and Silva M.J.},
	publisher = {Springer},
	issn = {03029743},
	isbn = {978-303045441-8},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 42nd European Conference on IR Research, ECIR 2020; Conference date: 14 April 2020 through 17 April 2020; Conference code: 239129; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Lottes202020,
	author = {Lottes, Philipp and Behley, Jens and Chebrolu, Nived and Milioto, Andres and Stachniss, Cyrill},
	title = {Robust joint stem detection and crop-weed classification using image sequences for plant-specific treatment in precision farming},
	year = {2020},
	journal = {Journal of Field Robotics},
	volume = {37},
	number = {1},
	pages = {20 – 34},
	doi = {10.1002/rob.21901},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070665803&doi=10.1002%2frob.21901&partnerID=40&md5=2e0df32ed1c50cdb268582ff5ff9cb3e},
	affiliations = {Photogrammetry & Robotics Lab, University of Bonn, Bonn, Germany},
	abstract = {Conventional farming still relies on large quantities of agrochemicals for weed management which have several negative side-effects on the environment. Autonomous robots offer the potential to reduce the amount of chemicals applied, as robots can monitor and treat each plant in the field individually and thereby circumventing the uniform chemical treatment of the whole field. Such agricultural robots need the ability to identify individual crops and weeds in the field using sensor data and must additionally select effective treatment methods based on the type of weed. For example, certain types of weeds can only be effectively treated mechanically due to their resistance to herbicides, whereas other types can be treated trough selective spraying. In this article, we present a novel system that provides the necessary information for effective plant-specific treatment. It estimates the stem location for weeds, which enables the robots to perform precise mechanical treatment, and at the same time provides the pixel-accurate area covered by weeds for treatment through selective spraying. The major challenge in developing such a system is the large variability in the visual appearance that occurs in different fields. Thus, an effective classification system has to robustly handle substantial environmental changes including varying weed pressure, various weed types, different growth stages, changing visual appearance of the plants and the soil. Our approach uses an end-to-end trainable fully convolutional network that simultaneously estimates plant stem positions as well as the spatial extent of crop plants and weeds. It jointly learns how to detect the stems and the pixel-wise semantic segmentation and incorporates spatial information by considering image sequences of local field strips. The jointly learned feature representation for both tasks furthermore exploits the crop arrangement information that is often present in crop fields. This information is considered even if it is only observable from the image sequences and not a single image. Such image sequences, as typically provided by robots navigating over the field along crop rows, enable our approach to robustly estimate the semantic segmentation and stem positions despite the large variations encountered in different fields. We implemented and thoroughly tested our approach on images from multiple farms in different countries. The experiments show that our system generalizes well to previously unseen fields under varying environmental conditions—a key capability to deploy such systems in the real world. Compared to state-of-the-art approaches, our approach generalizes well to unseen fields and not only substantially improves the stem detection accuracy, that is, distinguishing crop and weed stems, but also improves the semantic segmentation performance. © 2019 Wiley Periodicals, Inc.},
	author_keywords = {agricultural robotics; machine learning; plant classification; precision farming},
	keywords = {Agricultural chemicals; Air navigation; Crops; Image classification; Indicators (chemical); Learning systems; Pixels; Robots; Semantics; Visualization; Agricultural robotics; Convolutional networks; Different growth stages; Environmental conditions; Feature representation; Plant classification; Precision farming; State-of-the-art approach; Image segmentation},
	correspondence_address = {P. Lottes; Photogrammetry & Robotics Lab, University of Bonn, Bonn, Germany; email: philipp.lottes@igg.uni-bonn.de},
	publisher = {John Wiley and Sons Inc.},
	issn = {15564959},
	language = {English},
	abbrev_source_title = {J. Field. Rob.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 48; All Open Access, Bronze Open Access}
}

@CONFERENCE{Yang202037,
	author = {Yang, Li and Ding, Jiao and Jiang, Liheng and Han, Renrui and Bi, Yingchun and Zheng, Shangzhi},
	title = {A Novel Method for Leaf Recognition Based on D-LLE and Polar Coordinate Feature Extraction},
	year = {2020},
	journal = {Proceedings of 2020 IEEE 3rd International Conference on Information Systems and Computer Aided Education, ICISCAE 2020},
	pages = {37 – 41},
	doi = {10.1109/ICISCAE51034.2020.9236850},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096363831&doi=10.1109%2fICISCAE51034.2020.9236850&partnerID=40&md5=f4ae79492a1857970d6cc0a9d547598b},
	affiliations = {School of Medical Information, Wannan Medical College, Wuhu City, China; Anhui Institute of Information Technology, Wuhu City, China; College of Information Engineering, Chaohu University, Chaohu City, China},
	abstract = {By extracting low-level features under the rectangular coordinate system, traditional leaf recognition methods typically have properties such as high dimensionality of extracted features, high-computational requirement and weak generalization performance. Based on the manifold learning algorithm D-LLE, we proposed a novel leaf recognition method under the polar coordinate system. The method first extracts from the leaf images high-dimensional features associated with polar coordinate as the preprocessing. Consequently, D-LLE is harnessed to reduce the features' dimensionality. In the low-dimensional space, we use the nearest neighbor classifier to make final determination. Experimental results exhibit higher effectiveness and efficiency of our method compared with classical traditional methods. © 2020 IEEE.},
	author_keywords = {Feature extraction; Image recognition; Nearest neighbor classifier; Polar coordinates; Supervised LLE algorithm},
	keywords = {Information systems; Information use; Learning algorithms; Computational requirements; Effectiveness and efficiencies; Generalization performance; High dimensional feature; Manifold learning algorithm; Nearest Neighbor classifier; Polar coordinate systems; Rectangular coordinates; Computer aided instruction},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818303-9},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Inf. Syst. Comput. Aided Educ., ICISCAE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 3rd IEEE International Conference on Information Systems and Computer Aided Education, ICISCAE 2020; Conference date: 27 September 2020 through 29 September 2020; Conference code: 164646}
}

@CONFERENCE{Yustika Manik2020,
	author = {Yustika Manik, Fuzy and Kana Saputra, S. and Sartika Br Ginting, Dewi},
	title = {Plant Classification Based on Extraction Feature Gray Level Co-Occurrence Matrix Using k-nearest Neighbour},
	year = {2020},
	journal = {Journal of Physics: Conference Series},
	volume = {1566},
	number = {1},
	doi = {10.1088/1742-6596/1566/1/012107},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087779235&doi=10.1088%2f1742-6596%2f1566%2f1%2f012107&partnerID=40&md5=e8f3923f7591683dc1f28845ce332eca},
	affiliations = {Program Studi Ilmu Komputer, Fakultas Ilmu Komputer Dan Teknologi Informasi, Universitas Sumatera Utara, Medan, Indonesia; Program Studi Ilmu Komputer, Fakultas Matematika Dan Ilmu Pengetahuan Alam, Universitas Negeri Medan, Medan, Indonesia},
	abstract = {Indonesia is one of the countries with high plant diversity. Almost every region in Indonesia has distinctive plants and may not be present in other countries. Based on these facts required a strategic step to record and identify plants in Indonesia. One method that can be used to leaf image feature extraction is the Gray Level Co-occurrence Matrix (GLCM). This research will implement k-Nearest Neighbor (k-NN) method to classify type of plants based on leaf texture. The classification result based on GLCM using k-NN classifier showed that the accuracy using k = 3 was 83%. The use of parameter k influence classification results, the greater the value of k then the accuracy would be smaller. Classification errors for some types of leaf images occurred because the value extraction traits generated by GLCM was very similar and had a small range of values. © Published under licence by IOP Publishing Ltd.},
	keywords = {Extraction; Matrix algebra; Nearest neighbor search; Text processing; Textures; Classification errors; Classification results; Gray level co occurrence matrix(GLCM); Gray level co-occurrence matrix; K-nearest neighbors; K-nearest neighbours; k-NN classifier; Plant classification; Image processing},
	publisher = {Institute of Physics Publishing},
	issn = {17426588},
	language = {English},
	abbrev_source_title = {J. Phys. Conf. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 4th International Conference on Computing and Applied Informatics 2019, ICCAI 2019; Conference date: 26 November 2019 through 27 November 2019; Conference code: 161570; All Open Access, Bronze Open Access}
}

@ARTICLE{Lee2020,
	author = {Lee, Sue Han and Goëau, Hervé and Bonnet, Pierre and Joly, Alexis},
	title = {New perspectives on plant disease characterization based on deep learning},
	year = {2020},
	journal = {Computers and Electronics in Agriculture},
	volume = {170},
	doi = {10.1016/j.compag.2020.105220},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078973748&doi=10.1016%2fj.compag.2020.105220&partnerID=40&md5=ff34cc4dafcf3a33cadf4a68cef79ffd},
	affiliations = {AMAP, Univ Montpellier, CIRAD, CNRS, INRA, IRD, Montpellier, France; CIRAD, UMR AMAP, Montpellier, France; INRIA Sophia-Antipolis – ZENITH team, LIRMM, Montpellier, France},
	abstract = {The control of plant diseases is a major challenge to ensure global food security and sustainable agriculture. Several recent studies have proposed to improve existing procedures for early detection of plant diseases through modern automatic image recognition systems based on deep learning. In this article, we study these methods in detail, especially those based on convolutional neural networks. We first examine whether it is more relevant to fine-tune a pre-trained model on a plant identification task rather than a general object recognition task. In particular, we show, through visualization techniques, that the characteristics learned differ according to the approach adopted and that they do not necessarily focus on the part affected by the disease. Therefore, we introduce a more intuitive method that considers diseases independently of crops, and we show that it is more effective than the classic crop-disease pair approach, especially when dealing with disease involving crops that are not illustrated in the training database. This finding therefore encourages future research to rethink the current de facto paradigm of crop disease categorization. © 2020 The Authors},
	author_keywords = {Automated visual crops analysis; Deep learning; Plant diseases; Transfer learning},
	keywords = {Crops; Disease control; Food supply; Image enhancement; Image recognition; Neural networks; Object recognition; Automated visual crops analysis; Convolutional neural network; Image recognition system; Plant disease; Plant identification; Sustainable agriculture; Transfer learning; Visualization technique; artificial neural network; automation; crop plant; disease incidence; food security; learning; visual analysis; Deep learning},
	correspondence_address = {S.H. Lee; AMAP, Univ Montpellier, CIRAD, CNRS, INRA, IRD, Montpellier, France; email: sue-han.lee@cirad.fr},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 137; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Valarmathi20203684,
	author = {Valarmathi, G. and Suganthi, S.U. and Subashini, V. and Janaki, R. and Sivasankari, R. and Dhanasekar, S.},
	title = {CNN algorithm for plant classification in deep learning},
	year = {2020},
	journal = {Materials Today: Proceedings},
	volume = {46},
	pages = {3684 – 3689},
	doi = {10.1016/j.matpr.2021.01.847},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112703644&doi=10.1016%2fj.matpr.2021.01.847&partnerID=40&md5=67fa3cf1d6929149f944a794a2ea3f5b},
	affiliations = {Ece, Sri Sairam Institute of Technology, Chennai, 600044, India; Ece, Sri Eshwar College of Engineering, Coimbatore, 641202, India},
	abstract = {The prior methodology of characterizing the plants for dependent on surface based order and another strategy depends on KNN classifier. This paper presents qualities examination of plants utilizing picture preparing methods for robotized vision framework utilized at horticultural field. In farming examination, the programmed plant attributes recognition is fundamental one in observing huge field. The proposed dynamic framework uses picture content portrayal and regulated classifier sort of neural organization. This will naturally distinguish the plant species when we import its picture as info. Picture preparing strategies for this sort of choice investigation includes pre-processing and characterization stage. At Processing, an info picture will be resized and commotion expulsion procedure is applied. At definite stage the neural organization orders the pictures as farming plant, harmful plant and therapeutic plant separately. At that point it will show the attributes of each plant. © 2020 Elsevier Ltd. All rights reserved.},
	author_keywords = {CNN algorithm; Feature extraction; kNN algorithm; Plant classification; SVM classifier},
	keywords = {Classification (of information); Learning algorithms; Support vector machines; Attribute recognition; CNN algorithm; Dynamic framework; Features extraction; KNN algorithm; Neural organization; Plant classification; Surface-based; SVM classifiers; Vision frameworks; Deep learning},
	correspondence_address = {G. Valarmathi; Ece, Sri Sairam Institute of Technology, Chennai, 600044, India; email: Valarmathi.ece@sairamit.edu.in},
	editor = {Palanikumar K.},
	publisher = {Elsevier Ltd},
	issn = {22147853},
	language = {English},
	abbrev_source_title = {Mater. Today Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 2020 International Conference on Materials, Manufacturing and Mechanical Engineering for Sustainable Developments, ICMSD 2020; Conference date: 19 December 2020 through 20 December 2020; Conference code: 170457}
}

@CONFERENCE{Paulson202057,
	author = {Paulson, Anu and Ravishankar, S.},
	title = {AI Based Indigenous Medicinal Plant Identification},
	year = {2020},
	journal = {Proceedings - 2020 Advanced Computing and Communication Technologies for High Performance Applications, ACCTHPA 2020},
	pages = {57 – 63},
	doi = {10.1109/ACCTHPA49271.2020.9213224},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094627876&doi=10.1109%2fACCTHPA49271.2020.9213224&partnerID=40&md5=3d0c2b26e3b5e1e0334f97ddf14a4c87},
	affiliations = {Vidya Academy of Science and Technology, Department of Computer Science and Engineering, Thrissur, India},
	abstract = {In preserving the physical and psychological state of persons, ayurvedic medicines have an important role. The research aims to identify indigenous ayurvedic medicinal plant species using deep learning techniques. The social relevance of the proposal is so high as it would solve the problems of a wide range of stakeholders like physicians, pharmacy, government, and public. The identification of rare plant species may lead to a significant impact on the research associated with medical and other related areas. Another application can be the identification of plant species in forest and remote areas, where access to humans is limited. In such cases, the image of a particular plant species may be captured using drones and further analyzed. Currently, a lot of research work has been going on in the area of plant species identification using machine learning algorithms. The performance of Convolutional Neural Network (CNN), and pretrained models VGG16, and VGG19 has been compared for leaf identification problem. The dataset proposed in this research work contains indigenous medicinal plants of Kerala. The dataset consists of leaf images of 64 medicinal plants. CNN obtained a classification accuracy of 95.79%. VGG16 and VGG19 achieve an accuracy of 97.8% and 97.6% respectively, outperforms basic CNN.  © 2020 IEEE.},
	author_keywords = {Ayurveda; Convolutional Neural Network; Dataset; Deep Learning; Medicinal Plants; Plant Classification; Transfer Learning; VGG16; VGG19; VGGNet},
	keywords = {Convolutional neural networks; Deep learning; Learning systems; Medicine; Plants (botany); Ayurvedic medicine; Classification accuracy; Leaf identification; Learning techniques; Medicinal plants; Plant species; Plant species identification; Psychological state; Learning algorithms},
	editor = {Joy J. and Sreeraj M.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172816453-3},
	language = {English},
	abbrev_source_title = {Proc. - Adv. Comput. Commun. Technol. High Perform. Appl., ACCTHPA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; Conference name: 2020 Advanced Computing and Communication Technologies for High Performance Applications, ACCTHPA 2020; Conference date: 2 July 2020 through 4 July 2020; Conference code: 163772}
}

@ARTICLE{Pankaja2020557,
	author = {Pankaja, K. and Suma, V.},
	title = {Mango Leaves Recognition Using Deep Belief Network with MFO and Multi-feature Fusion},
	year = {2020},
	journal = {Smart Innovation, Systems and Technologies},
	volume = {160},
	pages = {557 – 565},
	doi = {10.1007/978-981-32-9690-9_61},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075566101&doi=10.1007%2f978-981-32-9690-9_61&partnerID=40&md5=94bc00c0eccbc8ce02da1f59fe51d768},
	affiliations = {Computer Science and Engineering, Cambridge Institute of Technology, VTU, Bengaluru, India; Information Science and Engineering, Dayanand Sagar College of Engineering, VTU, Bengaluru, India},
	abstract = {In automatic plant classification, plant identification based on digital leaf images is a challenging task. This paper proposes a Moth-Flame Optimization (MFO) based Deep Belief Network (DBN) method for plant leaf recognition. Initially, a combination of texture and shape features is applied for extracting features from the preprocessed image. Further, for leaf classification, the MFO optimizes the DBN parameters to minimize error and are used as classifiers. The classifier has been applied to five different sets of mango leaf images and achieved an accuracy of 98.5%. The experimental result indicates that it is feasible to automatically classify plants by using multi-feature extraction of plant leaf images in combination with MFO-based DBN. © 2020, Springer Nature Singapore Pte Ltd.},
	author_keywords = {DBNs; Feature extraction; MFO; Plant leaf; Texture feature},
	keywords = {Extraction; Feature extraction; Fruits; Intelligent computing; Plants (botany); Textures; DBNs; Deep belief network (DBN); Deep belief networks; Multi-feature fusion; Plant classification; Plant identification; Plant leaf; Texture features; Image processing},
	correspondence_address = {K. Pankaja; Computer Science and Engineering, Cambridge Institute of Technology, VTU, Bengaluru, India; email: pankaja.osr@gmail.com},
	editor = {Satapathy S.C. and Bhateja V. and Mohanty J.R. and Udgata S.K.},
	publisher = {Springer},
	issn = {21903018},
	isbn = {978-981329689-3},
	language = {English},
	abbrev_source_title = {Smart Innov. Syst. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 3rd International Conference on Smart Computing and Informatics, SCI 2018; Conference date: 21 December 2018 through 22 December 2018; Conference code: 232679}
}

@ARTICLE{Huixian202068828,
	author = {Huixian, Jiang},
	title = {The Analysis of Plants Image Recognition Based on Deep Learning and Artificial Neural Network},
	year = {2020},
	journal = {IEEE Access},
	volume = {8},
	pages = {68828 – 68841},
	doi = {10.1109/ACCESS.2020.2986946},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083896578&doi=10.1109%2fACCESS.2020.2986946&partnerID=40&md5=a00844d037002e8d5c705263e3bbc6ae},
	affiliations = {School of Geographical Sciences, Fujian Normal University, Fuzhou, 350007, China; Fujian Provincial Engineering Research Center for Monitoring and Assessing Terrestrial Disasters, Fuzhou, 350007, China},
	abstract = {Classification and identification of plants are helpful for people to effectively understand and protect plants. The leaves of plants are the most important recognition organs. With the development of artificial intelligence and machine vision technology, plant leaf recognition technology based on image analysis is used to improve the knowledge of plant classification and protection. Deep learning is the abbreviation of deep neural network learning method and belongs to neural network structure. It can automatically learn features from big data and use artificial neural network based on back propagation algorithm to train and classify plant leaf samples. The main content of this paper is to extract plant leaf features and identify plant species based on image analysis. Firstly, plant leaf images are segmented by various methods, and then feature extraction algorithm is used to extract leaf shape and texture features from leaf sample images. Then the comprehensive characteristic information of plant leaves is formed according to the comprehensive characteristic information. In this paper, 50 plant leaf databases are tested and compared with KNN-based neighborhood classification, Kohonen network based on self-organizing feature mapping algorithm and SVM-based support vector machine. At the same time, the leaves of 7 different plants were compared and it was found that ginkgo leaves were easier to identify. For leaf images under complex background, good recognition effect has been achieved. Image samples of the test set are input into the learning model to obtain reconstruction errors. The class label of the test set can be obtained by reconstructing the deep learning model with the smallest error set. The results show that this method has the shortest recognition time and the highest correct recognition rate. © 2013 IEEE.},
	author_keywords = {artificial neural network; deep learning; feature extraction; leaf image segmentation; Plant identification},
	keywords = {Backpropagation; Classification (of information); Conformal mapping; Deep neural networks; Image analysis; Image enhancement; Image recognition; Learning systems; Nearest neighbor search; Plant extracts; Self organizing maps; Support vector machines; Textures; Classification and identifications; Feature extraction algorithms; Machine vision technologies; Neural network learning; Neural network structures; Plant classification; Reconstruction error; Self-organizing feature mapping; Deep learning},
	correspondence_address = {J. Huixian; School of Geographical Sciences, Fujian Normal University, Fuzhou, 350007, China; email: Jhx155@163.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 44; All Open Access, Gold Open Access}
}

@ARTICLE{Zhang2020,
	author = {Zhang, Yaonan and Cui, Jing and Wang, Zhaobin and Kang, Jianfang and Min, Yufang},
	title = {Leaf image recognition based on bag of features},
	year = {2020},
	journal = {Applied Sciences (Switzerland)},
	volume = {10},
	number = {15},
	doi = {10.3390/app10155177},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088800136&doi=10.3390%2fapp10155177&partnerID=40&md5=85e74db2f7bb1a476eced1724457713b},
	affiliations = {Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Lanzhou, 730000, China; National Cryosphere Desert Data Center, Lanzhou, 730000, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China},
	abstract = {Plants are ubiquitous in human life. Recognizing an unknown plant by its leaf image quickly is a very interesting and challenging research. With the development of image processing and pattern recognition, plant recognition based on image processing has become possible. Bag of features (BOF) is one of the most powerful models for classification, which has been used for many projects and studies. Dual-output pulse-coupled neural network (DPCNN) has shown a good ability for texture features in image processing such as image segmentation. In this paper, a method based on BOF and DPCNN (BOF_DP) is proposed for leaf classification. BOF_DP achieved satisfactory results in many leaf image datasets. As it is hard to get a satisfactory effect on the large dataset by a single feature, a method (BOF_SC) improved from bag of contour fragments is used for shape feature extraction. BOF_DP and LDA (linear discriminant analysis) algorithms are, respectively, employed for textual feature extraction and reducing the feature dimensionality. Finally, both features are used for classification by a linear support vector machine (SVM), and the proposed method obtained higher accuracy on several typical leaf datasets than existing methods. © 2020 by the authors.},
	author_keywords = {BOF; DPCNN; Feature extraction; Plant recognition; Shape context},
	correspondence_address = {Z. Wang; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; email: wangzhb@lzu.edu.cn},
	publisher = {MDPI AG},
	issn = {20763417},
	language = {English},
	abbrev_source_title = {Appl. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access, Gold Open Access}
}

@CONFERENCE{Haryono2020338,
	author = {Haryono and Anam, Khairul and Saleh, Azmi},
	title = {A Novel Herbal Leaf Identification and Authentication Using Deep Learning Neural Network},
	year = {2020},
	journal = {CENIM 2020 - Proceeding: International Conference on Computer Engineering, Network, and Intelligent Multimedia 2020},
	pages = {338 – 342},
	doi = {10.1109/CENIM51130.2020.9297952},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099647362&doi=10.1109%2fCENIM51130.2020.9297952&partnerID=40&md5=fe74e98a943fbe209008ffcff2679aa3},
	affiliations = {University of Jember, Dept. of Electrical Engineering, Jember, Indonesia},
	abstract = {Herbal plants are plants that can be used as an alternative to the natural healing of diseases. The existence of herbal plants is still not widely known by the public. It is due to many types of medicinal plants so it requires special knowledge to recognize them. A smart and accurate herbal leaf recognition system is needed to overcome this. This study aims to identify and authenticate herbal leaves using the convolutional neural network and Long Short-Term Memory (CNN-LSTM) methods. Identification was carried out on nine types of herbal leaves divided into two-Thirds of training data and one-Third of testing data. The results of the identification process were validated by other data not included in training data and testing data, as well as leaf data other than the nine types of leaves identified. The CNN-LSTM method shows good results in the identification process, with an accuracy of 94.96%. © 2020 IEEE.},
	author_keywords = {authentication; CNN; herbal leaf; Identification; LSTM},
	keywords = {Authentication; Convolutional neural networks; Deep neural networks; Plants (botany); Well testing; CNN; Convolutional neural network; Herbal leaf; Herbal plants; Identification; Identification process; Leaf identification; LSTM; Testing data; Training data; Long short-term memory},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818283-4},
	language = {English},
	abbrev_source_title = {CENIM - Proceeding: Int. Conf. Comput. Eng., Network, Intell. Multimed.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2020 International Conference on Computer Engineering, Network, and Intelligent Multimedia, CENIM 2020; Conference date: 17 November 2020 through 18 November 2020; Conference code: 166112}
}

@CONFERENCE{Shobana202096,
	author = {Shobana, K.B. and Perumal, P.},
	title = {Plants Classification Using Machine Learning Algorithm},
	year = {2020},
	journal = {2020 6th International Conference on Advanced Computing and Communication Systems, ICACCS 2020},
	pages = {96 – 100},
	doi = {10.1109/ICACCS48705.2020.9074416},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084651180&doi=10.1109%2fICACCS48705.2020.9074416&partnerID=40&md5=4666728a6119793719ba80f2ac4885de},
	affiliations = {Sri Ramakrishna Engineering College, Computer Science and Engineering Department, Coimbatore, 641022, India},
	abstract = {Water is the principle content in a plant. Along these lines, the development of plants are enormously reliant on the adjustments in plant water content. To advance plant development in water short and dry spell pressure conditions, numerous methods have been considered. The target of this paper is to structure and build up an astute framework, in light of Support Vector Machine (SVM) and machine vision that would advance plant development in a restricted water condition. At long last shading, morphological and textural highlights were separated from a lot of turf grass, wheat, rice plant pictures under dry season pressure conditions. At that point an information arrangement process was overseen utilizing an SVM and ANN. Results indicated that the general arrangement exactness of ANN was 92% and higher correct nesses were acquired when the SVM was utilized as the classifier with a general precision of 98.00% for plant condition (Fresh and Wilted). © 2020 IEEE.},
	author_keywords = {ANN; imaging; plant classification; plant segmentation; Support Vector Machine},
	keywords = {Drought; Learning algorithms; Dry seasons; General arrangement; Plant development; Plant water content; Pressure conditions; Rice plants; Turf-grasses; Water conditions; Support vector machines},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172815197-7},
	language = {English},
	abbrev_source_title = {Int. Conf. Adv. Comput. Commun. Syst., ICACCS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 6th International Conference on Advanced Computing and Communication Systems, ICACCS 2020; Conference date: 6 March 2020 through 7 March 2020; Conference code: 159486}
}

@ARTICLE{Natesan2020310,
	author = {Natesan, Sowmya and Armenakis, Costas and Vepakomma, Udayalakshmi},
	title = {Individual tree species identification using dense convolutional network (Densenet) on multitemporal RGB images from UAV},
	year = {2020},
	journal = {Journal of Unmanned Vehicle Systems},
	volume = {8},
	number = {4},
	pages = {310 – 333},
	doi = {10.1139/juvs-2020-0014},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096898851&doi=10.1139%2fjuvs-2020-0014&partnerID=40&md5=e8c077c8a74668c56e2f079049cd3495},
	affiliations = {Geomatics Engineering, Department of Earth and Space Science and Engineering, Lassonde School of Engineering, York University, Toronto, M3J 1P3, ON, Canada; FPInnovations, Pointe-Claire, H9R 3J9, QC, Canada},
	abstract = {Tree species identification at the individual tree level is crucial for forest operations and management, yet its automated mapping remains challenging. Emerging technology, such as the high-resolution imagery from unmanned aerial vehicles (UAV) that is now becoming part of every forester’s surveillance kit, can potentially provide a solution to better characterize the tree canopy. To address this need, we have developed an approach based on a deep Convolutional Neural Network (CNN) to classify forest tree species at the individual tree-level that uses high-resolution RGB images acquired from a consumer-grade camera mounted on a UAV platform. This work explores the ability of the Dense Convolutional Network (DenseNet) to classify commonly available economic coniferous tree species in eastern Canada. The network was trained using multitemporal images captured under varying acquisition parameters to include seasonal, temporal, illumination, and angular variability. Validation of this model using distinct images over a mixed-wood forest in Ontario, Canada, showed over 84% classification accuracy in distinguishing five predominant species of coniferous trees. The model remains highly robust even when using images taken during different seasons and times, and with varying illumination and angles. © 2020, Canadian Science Publishing. All rights reserved.},
	author_keywords = {Coniferous species; Deep learning; DenseNet CNN; Tree species identification; UAV RGB images},
	keywords = {Antennas; Convolution; Convolutional neural networks; Deep neural networks; Image acquisition; Unmanned aerial vehicles (UAV); Acquisition parameters; Automated mapping; Classification accuracy; Convolutional networks; Emerging technologies; High resolution imagery; Multi-temporal image; Tree species identifications; Forestry},
	correspondence_address = {C. Armenakis; Geomatics Engineering, Department of Earth and Space Science and Engineering, Lassonde School of Engineering, York University, Toronto, M3J 1P3, Canada; email: armenc@yorku.ca},
	publisher = {Canadian Science Publishing},
	issn = {22913467},
	language = {English},
	abbrev_source_title = {J. Unmanned Veh. Sys.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 22; All Open Access, Bronze Open Access}
}

@ARTICLE{Martínez20202668,
	author = {Martínez, Fredy H.S. and Edwar Jacinto, G. and Fernando Martínez, S.},
	title = {Model for the identification of diseases in the banana plant using a convolutional neural network},
	year = {2020},
	journal = {Allergy, Asthma and Immunology Research},
	volume = {13},
	number = {10},
	pages = {2668 – 2673},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096025266&partnerID=40&md5=878caeb872961cc5553185c2633c5b43},
	affiliations = {Facultad Tecnológica, Universidad Distrital Francisco, José de Caldas, Colombia},
	abstract = {Banana cultivation is an important source of economic income for several developing countries, with economies heavily dependent on their agricultural production. Such is the case of Colombia, a banana export country in which much of its production is still carried out by small peasant families without a high degree of modernization. The plant is quite sensitive to diseases caused by environmental conditions, bacteria, and viruses. Infections can spread rapidly and cause great damage to the plantations. For this reason, it is necessary to develop high performance and very low-cost technologies capable of quickly identifying the damage, to control it and reduce losses. In this article, we propose a convolutional model based on a deep neural network for the classification of banana plant leaf damage from images. The model is trained specifically for this problem with real images captured in different states of affectation of leaves of the plant. The model is suitable to be propagated on a very low-cost embedded system, and therefore suitable to be used in small plantations. The performance of the model demonstrates a high capacity to differentiate the damage to the leaves and the cause of it, making it possible to quickly formulate a treatment strategy. © 2020 Korean Academy of Asthma, Allergy and Clinical Immunology. All rights reserved.},
	author_keywords = {Bacterial Wilt; Banana; Black Sigatoka; Deep Neural Network; Image Processing},
	keywords = {Article; bacterial wilt; banana; convolutional neural network; deep neural network; embedding; plant identification; plant leaf; preliminary data},
	publisher = {Korean Academy of Asthma, Allergy and Clinical Immunology},
	issn = {20927355},
	language = {English},
	abbrev_source_title = {Allergy Asthma Immunol. Res.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Gokhale2020272,
	author = {Gokhale, Abhishek and Babar, Sayali and Gawade, Srushti and Jadhav, Shubhankar},
	title = {Identification of medicinal plant using image processing and machine learning},
	year = {2020},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {1155},
	pages = {272 – 282},
	doi = {10.1007/978-981-15-4029-5_27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089613596&doi=10.1007%2f978-981-15-4029-5_27&partnerID=40&md5=081d784e97cdefaee58622be4a43f5af},
	affiliations = {Department of Computer Engineering, Pimpri Chinchwad College of Engineering, Pune, India},
	abstract = {Medicinal plants are the backbone of the system of medicines; they are the richest bioresource of drugs of traditional systems of medicine, modern medicines, nutraceuticals, food supplements, folk medicines, pharmaceutical intermediates, and chemical entities for synthetic drugs. These plants are classified according to their medicinal values. Classification of medicinal plants is acknowledged as a significant activity in the production of medicines along with the knowledge of its use in the medicinal industry. Medicinal plant classification based on parts such as leaves has shown significant results. An automated system for the identification of medicinal plants from leaves using Image processing and Machine Learning techniques has been presented. This paper provides knowledge of the process of identification of medicinal plants from features extracted from the images of leaves and different preprocessing techniques used for feature extraction from a leaf. Many features were extracted from each leaf such as its length, width, perimeter, area, color, rectangularity, and circularity. It is expected that for the automatic identification of medicinal plants, a web-based or mobile computer system will help the community people to develop their knowledge on medicinal plants, help taxonomists to develop more efficient species identification techniques and also participate significantly in the pharmaceutical drug manufacturing. © Springer Nature Singapore Pte Ltd. 2020.},
	author_keywords = {Feature extraction; Image processing; Leaf recognition; Machine learning; Medicinal plants; OpenCV},
	keywords = {Automation; Drug products; Machine learning; Plants (botany); Automated systems; Machine learning techniques; Mobile computer systems; Pharmaceutical drugs; Pharmaceutical intermediates; Preprocessing techniques; Species identification; Traditional systems; Image processing},
	correspondence_address = {S. Gawade; Department of Computer Engineering, Pimpri Chinchwad College of Engineering, Pune, India; email: srushtigawade1999@gmail.com},
	editor = {Iyer B. and Rajurkar A.M. and Gudivada V.},
	publisher = {Springer},
	issn = {21945357},
	isbn = {978-981154028-8},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 5th International Conference on Computing in Engineering and Technology,ICCET 2020; Conference date: 9 January 2020 through 11 January 2020; Conference code: 243079}
}

@ARTICLE{He20201123,
	author = {He, Tuo and Lu, Yang and Jiao, Lichao and Zhang, Yonggang and Jiang, Xiaomei and Yin, Yafang},
	title = {Developing deep learning models to automate rosewood tree species identification for CITES designation and implementation},
	year = {2020},
	journal = {Holzforschung},
	volume = {74},
	number = {12},
	pages = {1123 – 1133},
	doi = {10.1515/hf-2020-0006},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094898795&doi=10.1515%2fhf-2020-0006&partnerID=40&md5=ccc84f285d1e6afe9264e46c802385f0},
	affiliations = {Department of Wood Anatomy and Utilization, Chinese Research Institute of Wood Industry, Chinese Academy of Forestry, Beijing, 100091, China; Wood Collections (WOODPEDIA), Chinese Academy of Forestry, Beijing, 100091, China},
	abstract = {The implementation of Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) to combat illegal logging and associated trade necessitates accurate and efficient field screening of wood species. In this study, a total of 10,237 images of 15 Dalbergia and 11 Pterocarpus species were collected from the transverse surfaces of 417 wood specimens. Three deep learning models were then constructed, trained, and tested with these images to discriminate between timber species. The optimal parameters of the deep learning model were analyzed, and the representative wood anatomical features that were activated by the deep learning models were visualized. The results demonstrated that the overall accuracies of the 26-class, 15-class, and 11-class models were 99.3, 93.7, and 88.4%, respectively. It is suggested that at least 100 high-quality images per species with minimum patch sizes of 1000 × 1000 from more than 10 wood specimens were needed to train reliable and applicable deep learning models. The feature visualization indicated that the vessel groupings and axial parenchyma were the main wood anatomical features activated by the deep learning models. The combination of the state-of-the-art deep learning models, parameter configuration, and feature visualization provide a time- and cost-effective tool for the field screening of wood species to support effective CITES designation and implementation. © 2020 Walter de Gruyter GmbH, Berlin/Boston 2020.},
	author_keywords = {CITES; Dalbergia; deep learning models; Pterocarpus; rosewood tree species; wood identification},
	keywords = {Conservation; Cost effectiveness; International trade; Learning systems; Visualization; Wood; Anatomical features; Endangered species; High quality images; Illegal logging; Optimal parameter; Overall accuracies; State of the art; Tree species identifications; Deep learning},
	correspondence_address = {Y. Yin; Department of Wood Anatomy and Utilization, Chinese Research Institute of Wood Industry, Chinese Academy of Forestry, Beijing, 100091, China; email: yafang@caf.ac.cn},
	publisher = {De Gruyter Open Ltd},
	issn = {00183830},
	coden = {HOLZA},
	language = {English},
	abbrev_source_title = {Holzforschung},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18}
}

@CONFERENCE{Chompookham202072,
	author = {Chompookham, Thipwimon and Gonwirat, Sarayuth and Lata, Siriwiwat and Phiphiphatphaisit, Sirawan and Surinta, Olarik},
	title = {Plant Leaf Image Recognition using Multiple-grid Based Local Descriptor and Dimensionality Reduction Approach},
	year = {2020},
	journal = {ACM International Conference Proceeding Series},
	pages = {72 – 77},
	doi = {10.1145/3388176.3388180},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092145877&doi=10.1145%2f3388176.3388180&partnerID=40&md5=31e57e2b9b673264220aa393554370b1},
	affiliations = {Department of Information Technology, Faculty of Informatics, Mahasarakham University, Thailand; Multi-agent Intelligent Simulation Laboratory (MISL), Faculty of Informatics, Mahasarakham University, Maha Sarakham, Thailand},
	abstract = {The identification process of plant species is one of the significant and challenging problems. In this research area, many researchers have focused on identifying the plant leaf images because the leaves of a plant are found almost all year round. The achieve method of the plant leaf image recognition is based on unique extraction features from the plant leaf and using the well-known machine learnings as a classification method. As a result, recognition accuracy was often not very high. In order to improve recognition accuracy, we proposed a multiple grids technique based on the local descriptors and dimensionality reduction. Firstly, we divided the plant leaf image according to grid size and calculated the local descriptors from each grid. Secondly, the dimensionality reduction is proposed to transform and decrease the correlated variables of the feature vector. Finally, the feature vector with a relatively low-dimensional is transferred to the machine learning techniques, which are the support vector machine and multi-layer perceptron algorithms. We have evaluated and compared the proposed algorithm with the bag of visual words method and the deep convolutional neural network (including AlexNet and GoogLeNet architectures) on the Folio leaf image dataset. The experiments show that the proposed algorithm has improved and obtained very high accuracy on plant leaf image recognition.  © 2020 ACM.},
	author_keywords = {Dimensionality reduction; Local descriptor; Multi-layer perceptron; Multiple grids approach; Plant leaf recognition; Support vector machine},
	keywords = {Convolutional neural networks; Deep neural networks; Image enhancement; Image recognition; Learning systems; Multilayer neural networks; Plants (botany); Support vector machines; Bag-of-visual-words; Classification methods; Correlated variables; Identification process; Local descriptors; Machine learning techniques; Multi layer perceptron; Recognition accuracy; Dimensionality reduction},
	publisher = {Association for Computing Machinery},
	isbn = {978-145037725-6},
	language = {English},
	abbrev_source_title = {ACM Int. Conf. Proc. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 3rd International Conference on Information Science and System, ICISS 2020; Conference date: 19 March 2020 through 22 March 2020; Conference code: 163062}
}

@ARTICLE{Pushpa2020763,
	author = {Pushpa, B.R. and Anusha, R. and Brundavi, N. and Divya, L. and Ishwarya, M.P.},
	title = {Comparisons of various image preprocessing approaches for enhancing medicinal leaves},
	year = {2020},
	journal = {International Journal of Pharmaceutical Research},
	volume = {12},
	number = {3},
	pages = {763 – 770},
	doi = {10.31838/ijpr/2020.12.03.108},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083298841&doi=10.31838%2fijpr%2f2020.12.03.108&partnerID=40&md5=84233efe8ca1a9ff3a35f47d9c9af4f9},
	affiliations = {Department of Computer Science, Amrita School of Arts and Sciences, Amrita Vishwa Vidyapeetham, Mysuru Campus, India},
	abstract = {Plants are widely used in foodstuff, medicines and industry and it’s also essential for environmental protection. However, it’s a crucial task to recognizes plant species on earth. Plants are depreciated due to pollution, diseased or nutrient deficiency. The current way of identification and determination of the type of medicinal leaves is still being done manually and prone to human errors. Misclassification of plant species results in huge loss on the production of crops and economical value of market. Plant recognition requires huge amount of labor, knowledge about the plants and also requires more time interval. Image processing is the method of study and manipulation of digitalized images especially to enhance the quality of the images. During image acquisition there is a chance of corruption due to noise images are corrupted by the noise that are applied on medicinal leaves. Filtering and smoothing of images are the important task to scale back or remove the effect of noise. This paper represents analysis of various image preprocessing techniques that are applied on medicinal leaf images. These techniques are used to remove the noise based on their potential. Image preprocessing is a main step for noise removal and for enhancing the standard of an original images. The quality of images is analyzed in support with PSNR (peek signal to noise ratio) values that are applied on various filtering and smoothening methods to get the better-quality images. Further these preprocessing methods are very useful for the segmentation process. © 2020, Advanced Scientific Research. All rights reserved.},
	author_keywords = {Average Filter; Gaussian Filter; Histogram Equalization; Median Filter; Poisson Noise; Preprocessing Methods; PSNR; Salt and Pepper; Weiner Filter},
	keywords = {Article; Azadirachta indica; bean; comparative study; environmental protection; feature extraction; histogram; human; image analysis; image processing; image quality; image segmentation; information processing; lemon; mathematical analysis; medicinal plant; noise reduction; nonhuman; nutritional deficiency; Ocimum tenuiflorum; papaya; plant leaf; signal noise ratio; species identification; squash; support vector machine},
	publisher = {Advanced Scientific Research},
	issn = {09752366},
	language = {English},
	abbrev_source_title = {Int. J. Pharm. Res.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Banerjee2020251,
	author = {Banerjee, Somnath and Pamula, Rajendra},
	title = {Random forest boosted cnn: An empirical technique for plant classification},
	year = {2020},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {1112},
	pages = {251 – 261},
	doi = {10.1007/978-981-15-2188-1_20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084115309&doi=10.1007%2f978-981-15-2188-1_20&partnerID=40&md5=fb117a786fb47800956b6788631b3c66},
	affiliations = {Pamula Indian Institute of Technology (ISM), Dhanbad, India},
	abstract = {Plant identification and classification is one of the toughest jobs for inexperienced botanists. This paper proposes a hybrid model by combining random forest and along with the convolutional neural network (CNN) to classify the images. Our proposed method comprises of two phases; feature extraction using CNN and training the random forest model. The layers in convolutional neural network are useful to extract essential features. In this work, we have used PlantCLEF 2019 dataset (Amazonian Rainforest) to train and evaluate our model. We compare our method with the state-of-the-art methods for plant classification. The experimented method produces relatively higher accuracy than earlier methods. © Springer Nature Singapore Pte Ltd. 2020.},
	author_keywords = {CNN; Plant classification; PlantCLEF 2019; Random forest},
	keywords = {Convolution; Decision trees; Multilayer neural networks; Random forests; Amazonian rainforest; Empirical techniques; Essential features; Hybrid model; Plant classification; Plant identification; Random forest modeling; State-of-the-art methods; Convolutional neural networks},
	correspondence_address = {S. Banerjee; Pamula Indian Institute of Technology (ISM), Dhanbad, India; email: mailto_somnath.16kt000102@cse.ism.ac.in},
	editor = {Mandal J.K. and Mukhopadhya S.},
	publisher = {Springer},
	issn = {21945357},
	isbn = {978-981152187-4},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 1st International Conference on Global AI Congress, GAIC 2019; Conference date: 12 September 2019 through 14 September 2019; Conference code: 238999}
}

@ARTICLE{Vilasini20201445,
	author = {Vilasini, M. and Ramamoorthy, P.},
	title = {CNN approaches for classification of Indian leaf species using smartphones},
	year = {2020},
	journal = {Computers, Materials and Continua},
	volume = {62},
	number = {3},
	pages = {1445 – 1472},
	doi = {10.32604/cmc.2020.08857},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082297542&doi=10.32604%2fcmc.2020.08857&partnerID=40&md5=b8b4c455efcf005a518297da2e7fa752},
	affiliations = {KPR Institute of Engineering and Technology, Coimbatore, 641407, India; Adithiya Institute of Engineering and Technology, Coimbatore, 641107, India},
	abstract = {Leaf species identification leads to multitude of societal applications. There is enormous research in the lines of plant identification using pattern recognition. With the help of robust algorithms for leaf identification, rural medicine has the potential to reappear as like the previous decades. This paper discusses CNN based approaches for Indian leaf species identification from white background using smartphones. Variations of CNN models over the features like traditional shape, texture, color and venation apart from the other miniature features of uniformity of edge patterns, leaf tip, margin and other statistical features are explored for efficient leaf classification. © 2020 Tech Science Press. All rights reserved.},
	author_keywords = {Classification; CNN; Deep learning; Prewitt edge detection; Transfer learning},
	keywords = {Classification (of information); Deep learning; mHealth; Pattern recognition; Smartphones; Textures; Edge patterns; Leaf classification; Leaf identification; Plant identification; Prewitt edge detection; Robust algorithm; Species identification; Statistical features; Transfer learning},
	correspondence_address = {M. Vilasini; KPR Institute of Engineering and Technology, Coimbatore, 641407, India; email: vilasiniaddress@gmail.com},
	publisher = {Tech Science Press},
	issn = {15462218},
	language = {English},
	abbrev_source_title = {Comput. Mater. Continua},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access, Gold Open Access}
}

@ARTICLE{Muthireddy2020439,
	author = {Muthireddy, Vamsidhar and Jawahar, C.V.},
	title = {Indian Plant Recognition in the Wild},
	year = {2020},
	journal = {Communications in Computer and Information Science},
	volume = {1249},
	pages = {439 – 449},
	doi = {10.1007/978-981-15-8697-2_41},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097286025&doi=10.1007%2f978-981-15-8697-2_41&partnerID=40&md5=48de94c78265de74a0aa1424ace84abd},
	affiliations = {IIIT Hyderabad, Hyderabad, India},
	abstract = {Conservation efforts to protect biodiversity rely on an accurate identification process. In the case of plant identification, traditional methods used are manual, time-consuming and require a degree of expertise to operate. As a result, there is an increasing interest today for an automated plant identification system. Such a system can help in aiding plant-related education, promoting ecotourism, creating a digital heritage for plant species among many others. We propose a solution using modern convolutional neural network architectures which achieves state-of-the-art performance for plant classification in the wild. An exhaustive set of experiments are performed to classify 112 species of plants from the challenging Indic-Leaf dataset. The best performing model gives Top-1 precision of 90.08 and Top-5 precision of 96.90. © 2020, Springer Nature Singapore Pte Ltd.},
	keywords = {Biodiversity; Classification (of information); Convolutional neural networks; Network architecture; Digital heritage; Identification process; Plant classification; Plant identification; Plant identification systems; Plant recognition; Plant species; State-of-the-art performance; Computer vision},
	correspondence_address = {V. Muthireddy; IIIT Hyderabad, Hyderabad, India; email: vamsidhar.muthireddy@research.iiit.ac.in},
	editor = {Babu R.V. and Prasanna M. and Namboodiri V.P.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {18650929},
	isbn = {978-981158696-5},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 7th National Conference on Computer Vision, Pattern Recognition, Image Processing, and Graphics, NCVPRIPG 2019; Conference date: 22 December 2019 through 24 December 2019; Conference code: 251849}
}

@CONFERENCE{Nguyen Quoc202025,
	author = {Nguyen Quoc, Trung and Truong Hoang, Vinh},
	title = {Medicinal Plant identification in the wild by using CNN},
	year = {2020},
	journal = {International Conference on ICT Convergence},
	volume = {2020-October},
	pages = {25 – 29},
	doi = {10.1109/ICTC49870.2020.9289480},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098945856&doi=10.1109%2fICTC49870.2020.9289480&partnerID=40&md5=24c0cd2bbb80ce4db61351b799ee7493},
	affiliations = {Faculty of Information Technology, Ho Chi Minh Open Universty, Viet Nam},
	abstract = {Plant identification based on deep learning received many attention and effort from the research community with many promising results. It becomes an active trend in the recent years. We apply Convolutional Neural Network (CNN) to recognize Vietnamese medicinal plant images in this paper. Different frameworks are evaluated such as: VGG16, Resnet50, InceptionV3, DenseNet121, Xception and MobileNet. The highest accuracy reached by Xception with 88.26%. We might think this approach will greatly contribute to the discovery and conservation of valuable medicinal plants. © 2020 IEEE.},
	author_keywords = {Deep learning; DenseNet121; InceptionV3; Mo-bileNet; Resnet50; VGG16; VietNam medicinal plant; VNPlant-200; Xception},
	keywords = {Deep learning; Plants (botany); Medicinal plants; Plant identification; Research communities; Vietnamese; Convolutional neural networks},
	publisher = {IEEE Computer Society},
	issn = {21621233},
	isbn = {978-172816758-9},
	language = {English},
	abbrev_source_title = {Int. Conf. ICT Convergence},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; Conference name: 11th International Conference on Information and Communication Technology Convergence, ICTC 2020; Conference date: 21 October 2020 through 23 October 2020; Conference code: 166030}
}

@ARTICLE{Beck2020,
	author = {Beck, Michael A. and Liu, Chen-Yi and Bidinosti, Christopher P. and Henry, Christopher J. and Godee, Cara M. and Ajmani, Manisha},
	title = {An embedded system for the automated generation of labeled plant images to enable machine learning applications in agriculture},
	year = {2020},
	journal = {PLoS ONE},
	volume = {15},
	number = {12 December},
	doi = {10.1371/journal.pone.0243923},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098214985&doi=10.1371%2fjournal.pone.0243923&partnerID=40&md5=1afdb481ac76ed5a2cbe70656a59c695},
	affiliations = {Department of Physics, University of Winnipeg, Winnipeg, MB, Canada; Department of Applied Computer Science, University of Winnipeg, Winnipeg, MB, Canada; Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, MB, Canada; Department of Biology, University of Winnipeg, Winnipeg, MB, Canada},
	abstract = {A lack of sufficient training data, both in terms of variety and quantity, is often the bottleneck in the development of machine learning (ML) applications in any domain. For agricultural applications, ML-based models designed to perform tasks such as autonomous plant classification will typically be coupled to just one or perhaps a few plant species. As a consequence, each crop-specific task is very likely to require its own specialized training data, and the question of how to serve this need for data now often overshadows the more routine exercise of actually training such models. To tackle this problem, we have developed an embedded robotic system to automatically generate and label large datasets of plant images for ML applications in agriculture. The system can image plants from virtually any angle, thereby ensuring a wide variety of data; and with an imaging rate of up to one image per second, it can produce lableled datasets on the scale of thousands to tens of thousands of images per day. As such, this system offers an important alternative to time- and cost-intensive methods of manual generation and labeling. Furthermore, the use of a uniform background made of blue keying fabric enables additional image processing techniques such as background replacement and image segementation. It also helps in the training process, essentially forcing the model to focus on the plant features and eliminating random correlations. To demonstrate the capabilities of our system, we generated a dataset of over 34,000 labeled images, with which we trained an ML-model to distinguish grasses from non-grasses in test data from a variety of sources. We now plan to generate much larger datasets of Canadian crop plants and weeds that will be made publicly available in the hope of further enabling ML applications in the agriculture sector. © 2020 Beck et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	keywords = {Agriculture; Algorithms; Canada; Deep Learning; Humans; Image Processing, Computer-Assisted; Machine Learning; Plant Development; Plants; agriculture; article; image processing; machine learning; nonhuman; Poaceae; weed; agriculture; algorithm; anatomy and histology; Canada; classification; human; image processing; machine learning; plant; plant development},
	correspondence_address = {M.A. Beck; Department of Physics, University of Winnipeg, Winnipeg, Canada; email: m.beck@uwinnipeg.ca},
	publisher = {Public Library of Science},
	issn = {19326203},
	coden = {POLNC},
	pmid = {33332382},
	language = {English},
	abbrev_source_title = {PLoS ONE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Tiwari202044,
	author = {Tiwari, Shamik},
	title = {A comparative study of deep learning models with handcraft features and non-handcraft features for automatic plant species identification},
	year = {2020},
	journal = {International Journal of Agricultural and Environmental Information Systems},
	volume = {11},
	number = {2},
	pages = {44 – 57},
	doi = {10.4018/IJAEIS.2020040104},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083731345&doi=10.4018%2fIJAEIS.2020040104&partnerID=40&md5=bc32696eec906f43c5ee5787f6a5d720},
	affiliations = {UPES University, India},
	abstract = {The classification of plants is one of the most important aims for botanists since plants have a significant part in the natural life cycle. In this work, a leaf-based automatic plant classification framework is investigated. The aim is to compare two different deep learning approaches named Deep Neural Network (DNN) and deep Convolutional Neural Network (CNN). In the case of deep neural network, hybrid shapes and texture features are utilized as hand-crafted features while in the case of the convolution non-handcraft, features are applied for classification. The offered frameworks are evaluated with a public leaf database. From the simulation results, it is confirmed that the deep CNN-based deep learning framework demonstrates superior classification performance than the handcraft feature based approach. Copyright © 2020 IGI Global.},
	author_keywords = {CNN; Deep learning; Feature extraction; Plant classification},
	keywords = {Automatic identification; Classification (of information); Convolution; Convolutional neural networks; Deep neural networks; Life cycle; Textures; Classification performance; Comparative studies; Feature based approaches; Learning approach; Learning frameworks; Plant classification; Plant species identification; Texture features; comparative study; conceptual framework; image classification; life cycle analysis; machine learning; numerical model; species diversity; Deep learning},
	publisher = {IGI Global},
	issn = {19473192},
	language = {English},
	abbrev_source_title = {Int. J. Agric. Environ. Inf. Syst.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; All Open Access, Bronze Open Access}
}

@CONFERENCE{Nhan2020,
	author = {Nhan, Nguyen Thi Thanh and Le, Thi-Lan and Hai, Vu},
	title = {Do we need multiple organs for plant identification?},
	year = {2020},
	journal = {2020 International Conference on Multimedia Analysis and Pattern Recognition, MAPR 2020},
	doi = {10.1109/MAPR49794.2020.9237787},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096355555&doi=10.1109%2fMAPR49794.2020.9237787&partnerID=40&md5=cfd44d96b19515ea414ac5aefdf645eb},
	affiliations = {University of Information and Communication Technology, Nguyen University, Thai Nguyen, Viet Nam; School of Electronics and Telecommunications, Hanoi University of Science and Technology, Hanoi, Viet Nam; International Research Institute Mica, Hanoi University of Science and Technology, Hanoi, Viet Nam},
	abstract = {Plant identification that aims at determining the name of plant species from images has attracted a lot of research attentions. Previous studies have confirmed that the plant identification based on multiple organs outperforms that of single organ. However, majority of recent works mainly use transformation-based approach and focus on two organs. Inspired from the work [1] that proposed a fusion scheme named robust hybrid fusion method (RHF) for two organs-based plant identification, in this paper, we propose to extend this scheme for more than two organs plant identification. 57 experiments have been conducted on a dataset of 50 species extracted from ImageCLEF 2015 dataset. The experimental results have shown that the more organs used for plant identification are, the better the identification accuracy is. Using all 6 organs obtain 98.8% accuracy at rank-1. © 2020 IEEE.},
	author_keywords = {convolutional neural network; late fusion; multi-organ plant identification; plant identification},
	keywords = {Information analysis; Hybrid fusions; Identification accuracy; ImageCLEF; Plant identification; Plant species; Transformation based; Pattern recognition},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172816555-4},
	language = {English},
	abbrev_source_title = {Int. Conf. Multimed. Anal. Pattern Recognit., MAPR},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 3rd International Conference on Multimedia Analysis and Pattern Recognition, MAPR 2020; Conference date: 8 October 2020 through 9 October 2020; Conference code: 164647}
}

@ARTICLE{Miyoshi2020,
	author = {Miyoshi, Gabriela Takahashi and Arruda, Mauro dos Santos and Osco, Lucas Prado and Junior, José Marcato and Gonçalves, Diogo Nunes and Imai, Nilton Nobuhiro and Tommaselli, Antonio Maria Garcia and Honkavaara, Eija and Gonçalves, Wesley Nunes},
	title = {A novel deep learning method to identify single tree species in UAV-based hyperspectral images},
	year = {2020},
	journal = {Remote Sensing},
	volume = {12},
	number = {8},
	doi = {10.3390/RS12081294},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084533046&doi=10.3390%2fRS12081294&partnerID=40&md5=7b2ccb11aadaafa9dabe06239f7afbcd},
	affiliations = {Graduate Program in Cartographic Sciences, São Paulo State University (UNESP), Presidente Prudente, SP, 19060-900, Brazil; Graduate Program in Computer Sciences, Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Av. Costa e Silva, Campo Grande, 79070-900, Brazil; Faculty of Engineering and Architecture and Urbanism, University ofWestern São Paulo (UNOESTE), Cidade Universitária, R. José Bongiovani, Presidente Prudente, SP, 19050-920, Brazil; Faculty of Engineering, Architecture, and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Av. Costa e Silva, Campo Grande, 79070-900, Brazil; Department of Cartography, São Paulo State University (UNESP), Presidente Prudente, SP, 19060-900, Brazil; Finnish Geospatial Research Institute, National Land Survey of Finland, Geodeetinrinne 2, Masala, 02430, Finland},
	abstract = {Deep neural networks are currently the focus of many remote sensing approaches related to forest management. Although they return satisfactory results in most tasks, some challenges related to hyperspectral data remain, like the curse of data dimensionality. In forested areas, another common problem is the highly-dense distribution of trees. In this paper, we propose a novel deep learning approach for hyperspectral imagery to identify single-tree species in highly-dense areas. We evaluated images with 25 spectral bands ranging from 506 to 820 nm taken over a semideciduous forest of the Brazilian Atlantic biome. We included in our network's architecture a band combination selection phase. This phase learns from multiple combinations between bands which contributed the most for the tree identification task. This is followed by a feature map extraction and a multi-stage model refinement of the confidence map to produce accurate results of a highly-dense target. Our method returned an f-measure, precision and recall values of 0.959, 0.973, and 0.945, respectively. The results were superior when compared with a principal component analysis (PCA) approach. Compared to other learning methods, ours estimate a combination of hyperspectral bands that most contribute to the mentioned task within the network's architecture. With this, the proposed method achieved state-of-the-art performance for detecting and geolocating individual tree-species in UAV-based hyperspectral images in a complex forest. © 2020 by the authors.},
	author_keywords = {Band selection; Convolutional neural network; Data-reduction; High-density object; Tree species identification},
	keywords = {Aircraft detection; Deep neural networks; Forestry; Learning systems; Network architecture; Principal component analysis; Remote sensing; Spectroscopy; Unmanned aerial vehicles (UAV); Data dimensionality; Hyper-spectral imageries; Hyperspectral Data; Multi stage modeling; Precision and recall; Remote sensing approaches; State-of-the-art performance; Tree identification; Deep learning},
	correspondence_address = {G.T. Miyoshi; Graduate Program in Cartographic Sciences, São Paulo State University (UNESP), Presidente Prudente, SP, 19060-900, Brazil; email: gabriela.t.miyoshi@unesp.br},
	publisher = {MDPI AG},
	issn = {20724292},
	language = {English},
	abbrev_source_title = {Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 47; All Open Access, Gold Open Access}
}

@ARTICLE{Han2020224,
	author = {Han, Jing-Hua and Jin, Chen and Wu, Li-Sha},
	title = {Research on Accuracy of Flower Recognition Application Based on Convolutional Neural Network},
	year = {2020},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {965},
	pages = {224 – 232},
	doi = {10.1007/978-3-030-20454-9_22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067639419&doi=10.1007%2f978-3-030-20454-9_22&partnerID=40&md5=79d339f38591c4c259722304c0e14ec8},
	affiliations = {Beijing Forestry University, Beijing, China},
	abstract = {Compared with traditional flower recognition methods, the existing flower recognition applications on the market use advanced deep learning technology to improve the accuracy of plant recognition and solve the problem of plant recognition. The article studied the five applications that users commonly use, comparing and analyzing their recognition accuracy, and finally putting forward the feasibility advice for further improvement of flower recognition applications. The method of sampling survey was adopted, this paper divides the garden flowers and wild flowers into different levels according to their common degrees. Each type of flower was shot from 5 different angles and scenes, and recognized by these five applications separately. The results showed that the rankings of the five applications evaluated were Hua Bangzhu, Hua Banlv, Xing Se, Microsoft’s Flower Recognition, and Find Flower Recognition. At pre-sent, it is necessary to continuously improve from the aspects of technology, products and plant libraries. © 2020, Springer Nature Switzerland AG.},
	author_keywords = {Accuracy; Convolutional neural network; Deep learning; Flower recognition},
	keywords = {Convolution; Deep learning; Human engineering; Neural networks; Systems engineering; Accuracy; Convolutional neural network; Flower recognition; Learning technology; MicroSoft; Plant recognition; Recognition accuracy; Sampling survey; Engineering education},
	correspondence_address = {J.-H. Han; Beijing Forestry University, Beijing, China; email: hanjing013@126.com},
	editor = {Ahram T.},
	publisher = {Springer Verlag},
	issn = {21945357},
	isbn = {978-303020453-2},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: AHFE International Conference on Human Factors in Artificial Intelligence and Social Computing, the AHFE International Conference on Human Factors, Software, Service and Systems Engineering, and the AHFE International Conference of Human Factors in Energy, 2019; Conference date: 24 July 2019 through 28 July 2019; Conference code: 227189}
}

@ARTICLE{Boho2020,
	author = {Boho, David and Rzanny, Michael and Wäldchen, Jana and Nitsche, Fabian and Deggelmann, Alice and Wittich, Hans Christian and Seeland, Marco and Mäder, Patrick},
	title = {Flora Capture: a citizen science application for collecting structured plant observations},
	year = {2020},
	journal = {BMC Bioinformatics},
	volume = {21},
	number = {1},
	doi = {10.1186/s12859-020-03920-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097586692&doi=10.1186%2fs12859-020-03920-9&partnerID=40&md5=a4e4076162c94df9d4a0684964163261},
	affiliations = {Institute for Computer and Systems Engineering, Technische Universität Ilmenau, Helmholtzplatz 5, Ilmenau, 98693, Germany; Department Biogeochemical Integration, Max-Planck-Institute for Biogeochemistry, Hans-Knöll-Str. 10, Jena, 07745, Germany},
	abstract = {Background: Digital plant images are becoming increasingly important. First, given a large number of images deep learning algorithms can be trained to automatically identify plants. Second, structured image-based observations provide information about plant morphological characteristics. Finally in the course of digitalization, digital plant collections receive more and more interest in schools and universities. Results: We developed a freely available mobile application called Flora Capture allowing users to collect series of plant images from predefined perspectives. These images, together with accompanying metadata, are transferred to a central project server where each observation is reviewed and validated by a team of botanical experts. Currently, more than 4800 plant species, naturally occurring in the Central European region, are covered by the application. More than 200,000 images, depicting more than 1700 plant species, have been collected by thousands of users since the initial app release in 2016. Conclusion: Flora Capture allows experts, laymen and citizen scientists to collect a digital herbarium and share structured multi-modal observations of plants. Collected images contribute, e.g., to the training of plant identification algorithms, but also suit educational purposes. Additionally, presence records collected with each observation allow contribute to verifiable records of plant occurrences across the world. © 2020, The Author(s).},
	author_keywords = {Citizen science; Digital herbariumn; Digital plant collection; Mobile app; Multi-organ plant identification; Structured plant observations},
	keywords = {Flowers; Image Processing, Computer-Assisted; Neural Networks, Computer; Plants; Software; Deep learning; Citizen science; European regions; Mobile applications; Naturally occurring; Plant identification; Plant morphological; Plant species; Structured images; algorithm; article; citizen science; citizen scientist; flora; human; layperson; metadata; mobile application; plant identification; anatomy and histology; flower; image processing; plant; software; Learning algorithms},
	correspondence_address = {J. Wäldchen; Department Biogeochemical Integration, Max-Planck-Institute for Biogeochemistry, Jena, Hans-Knöll-Str. 10, 07745, Germany; email: jwald@bgc-jena.mpg.de},
	publisher = {BioMed Central Ltd},
	issn = {14712105},
	coden = {BBMIC},
	pmid = {33317442},
	language = {English},
	abbrev_source_title = {BMC Bioinform.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Zin20202198,
	author = {Zin, Izwan Asraf Md and Ibrahim, Zaidah and Isa, Dino and Aliman, Sharifah and Sabri, Nurbaity and Mangshor, Nur Nabilah Abu},
	title = {Herbal plant recognition using deep convolutional neural network},
	year = {2020},
	journal = {Bulletin of Electrical Engineering and Informatics},
	volume = {9},
	number = {5},
	pages = {2198 – 2205},
	doi = {10.11591/eei.v9i5.2250},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087114927&doi=10.11591%2feei.v9i5.2250&partnerID=40&md5=927c5ca7fb5af08a4c997361a3292c1f},
	affiliations = {Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Malaysia; CONNECT Initiative, Crops for the Future, Malaysia},
	abstract = {This paper investigates the application of deep Convolutional Neural Network (CNN) for herbal plant recognition through leaf identification. Traditional plant identification is often time-consuming due to varieties as well as similarities possessed within the plant species. This study shows that a deep CNN model can be created and enhanced using multiple parameters to boost recognition accuracy performance. This study also shows the significant effects of the multi-layer model on small sample sizes to achieve reasonable performance. Furthermore, data augmentation provides more significant benefits on the overall performance. Simple augmentations such as resize, flip and rotate will increase accuracy significantly by creating invariance and preventing the model from learning irrelevant features. A new dataset of the leaves of various herbal plants found in Malaysia has been constructed and the experimental results achieved 99% accuracy. © 2020, Institute of Advanced Engineering and Science. All rights reserved.},
	author_keywords = {Convolutional neural network; Data augmentation; Deep learning; Herbal plant recognition},
	correspondence_address = {I.A.M. Zin; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Malaysia; email: izwanzin@gmail.com},
	publisher = {Institute of Advanced Engineering and Science},
	issn = {20893191},
	language = {English},
	abbrev_source_title = {Bull. Electr. Eng. Inform.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Almeida2020,
	author = {Almeida, Brianna K. and Garg, Manish and Kubat, Miroslav and Afkhami, Michelle E.},
	title = {Not that kind of tree: Assessing the potential for decision tree–based plant identification using trait databases},
	year = {2020},
	journal = {Applications in Plant Sciences},
	volume = {8},
	number = {7},
	doi = {10.1002/aps3.11379},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087175794&doi=10.1002%2faps3.11379&partnerID=40&md5=880c46732c027bee5bad9a675ce88c9d},
	affiliations = {Department of Biology, University of Miami, 1301 Memorial Drive, Coral Gables, 33143, FL, United States; Department of Electrical and Computer Engineering, University of Miami, 1251 Memorial Drive, Coral Gables, 33143, FL, United States},
	abstract = {Premise: Advancements in machine learning and the rise of accessible “big data” provide an important opportunity to improve trait-based plant identification. Here, we applied decision-tree induction to a subset of data from the TRY plant trait database to (1) assess the potential of decision trees for plant identification and (2) determine informative traits for distinguishing taxa. Methods: Decision trees were induced using 16 vegetative and floral traits (689 species, 20 genera). We assessed how well the algorithm classified species from test data and pinpointed those traits that were important for identification across diverse taxa. Results: The unpruned tree correctly placed 98% of the species in our data set into genera, indicating its promise for distinguishing among the species used to construct them. Furthermore, in the pruned tree, an average of 89% of the species from the test data sets were properly classified into their genera, demonstrating the flexibility of decision trees to also classify new species into genera within the tree. Closer inspection revealed that seven of the 16 traits were sufficient for the classification, and these traits yielded approximately two times more initial information gain than those not included. Discussion: Our findings demonstrate the potential for tree-based machine learning and big data in distinguishing among taxa and determining which traits are important for plant identification. © 2020 Almeida et al. Applications in Plant Sciences is published by Wiley Periodicals LLC on behalf of the Botanical Society of America.},
	author_keywords = {decision tree; information gain; machine learning; plant identification; TRY plant trait database},
	correspondence_address = {B.K. Almeida; Department of Biology, University of Miami, Coral Gables, 1301 Memorial Drive, 33143, United States; email: b.almeida@miami.edu},
	publisher = {John Wiley and Sons Inc.},
	issn = {21680450},
	language = {English},
	abbrev_source_title = {Appl. Plant Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Prajakta202081,
	author = {Prajakta, Sadgir and Varsha, Ratnaparkhe},
	title = {Plant Species identification by utilization of Width Plot Analysis Feature Extraction Technique},
	year = {2020},
	journal = {Research Journal of Chemistry and Environment},
	volume = {24},
	number = {7},
	pages = {81 – 85},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095837143&partnerID=40&md5=d9f1c846550f126091caf9a2c24ce30e},
	affiliations = {Department of Electronics Engineering, Government College of Engineering, Osmanpura, Station Road, Aurangabad, Maharashtra, 431005, India},
	abstract = {Every flora and fauna in the ecosystem has its peculiar role. In order to preserve ecosystem, conservation of the every species is of paramount importance. Identification of species aids the conservation and plays a vital role in preservation of enviornment. This work proposes method for precise plant identification with limited domain knowledge. Plants: Aegle Marmelos L. (bel), Aloe Vera, Ricinus Communis Linn. (castor), Ocimum Tenuiflorum (tulasi), Azadirachta Indica (neem), Datura Stramonium (Datura), Calotropis Gigantea (rui), Vitex Negundo (nirgundi) with applications as analgesic and anti-inflammatory properties were selected for the study. Hue discrimination algorithm is utilized for primary classification of trees based on maturity. A novel technique “Width Plot Analysis” is used for feature extraction which achieves 94% accuracy. Tree classifier implemented gives improved accuracy with reduced operating time. © 2020 World Research Association. All rights reserved.},
	author_keywords = {Ayurvedic; Digital image processing; Hue Discrimination; Local Normalization; Ridge Filter; Tree Classification; Width Plot Analysis},
	keywords = {Aegle marmelos; Aloe vera; Azadirachta indica; Calotropis gigantea; Castor; Datura; Datura stramonium; Ocimum tenuiflorum; Ricinus communis; Vitex negundo; algorithm; classification; conservation planning; herb; identification method; medicinal plant; medicine; plant extract},
	correspondence_address = {S. Prajakta; Department of Electronics Engineering, Government College of Engineering, Aurangabad, Maharashtra, Osmanpura, Station Road, 431005, India; email: psadgir@yahoo.in},
	publisher = {World Research Association},
	issn = {09720626},
	language = {English},
	abbrev_source_title = {Res. J. Chem. Environ.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yang2020,
	author = {Yang, Kunlong and Zhong, Weizhen and Li, Fengguo},
	title = {Leaf segmentation and classification with a complicated background using deep learning},
	year = {2020},
	journal = {Agronomy},
	volume = {10},
	number = {11},
	doi = {10.3390/agronomy10111721},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098651572&doi=10.3390%2fagronomy10111721&partnerID=40&md5=6073da79f1a4961f85e18cf296c339e8},
	affiliations = {Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, Guangdong Provincial Engineering Technology Research Center for Quantum Precision Measurement, National Demonstration Center for Experimental Physics Education, SPTE, South China Normal University, Guangzhou, 510006, China},
	abstract = {The segmentation and classification of leaves in plant images are a great challenge, especiallywhen several leaves are overlapping in images with a complicated background. In this paper, the segmentation and classification of leaf images with a complicated background using deep learning are studied. First, more than 2500 leaf images with a complicated background are collected and artificially labeled with target pixels and background pixels. Two-thousand of them are fed into a Mask Region-based Convolutional Neural Network (Mask R-CNN) to train a model for leaf segmentation. Then, a training set that contains more than 1500 training images of 15 species is fed into a very deep convolutional network with 16 layers (VGG16) to train a model for leaf classification. The best hyperparameters for these methods are found by comparing a variety of parameter combinations. The results show that the averageMisclassification Error (ME) of 80 test images usingMask R-CNN is 1.15%. The average accuracy value for the leaf classification of 150 test images using VGG16 is up to 91.5%. This indicates that these methods can be used to segment and classify the leaf image with a complicated background effectively. It could provide a reference for the phenotype analysis and automatic classification of plants. © 2020 by the authors.},
	author_keywords = {Deep learning; Image segmentation; Mask R-CNN; Plant classification; VGG16},
	correspondence_address = {F. Li; Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, Guangdong Provincial Engineering Technology Research Center for Quantum Precision Measurement, National Demonstration Center for Experimental Physics Education, SPTE, South China Normal University, Guangzhou, 510006, China; email: lifengguo@m.scnu.edu.cn},
	publisher = {MDPI AG},
	issn = {20734395},
	language = {English},
	abbrev_source_title = {Agronomy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 48; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Chen20201,
	author = {Chen, Xueshen and Mao, Yuanyang and Ma, Xu and Qi, Long},
	title = {A tactile method for rice plant recognition based on machine learning},
	year = {2020},
	journal = {Sensors (Switzerland)},
	volume = {20},
	number = {18},
	pages = {1 – 16},
	doi = {10.3390/s20185135},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090429531&doi=10.3390%2fs20185135&partnerID=40&md5=234ac6fced7cd3f45362a17da39f7c71},
	affiliations = {College of Engineering, South China Agricultural University, Guangzhou, 510642, China},
	abstract = {Accurate and real-time recognition of rice plants is the premise underlying the implementation of precise weed control. However, achieving desired results in paddy fields using the traditional visual method is difficult due to the occlusion of rice leaves and the interference of weeds. The objective of this study was to develop a novel rice plant recognition sensor based on a tactile method which acquires tactile information through physical touch. The tactile sensor would be mounted on the paddy field weeder to provide identification information for the actuator. First, a flexible gasbag filled with air was developed, where vibration features produced by tactile and sliding feedback were acquired when this apparatus touched rice plants or weeds, allowing the subtle vibration data with identification features to be reflected through the voltage value of an air-pressured sensor mounted inside the gasbag. Second, voltage data were preprocessed by three algorithms to optimize recognition features, including dimensional feature, dimensionless feature, and fractal dimension. The three types of features were used to train and test a neural network classifier. To maximize classification accuracy, an optimum set of features (b (variance), f (kurtosis), h (waveform factor), l (box dimension), and m (Hurst exponent)) were selected using a genetic algorithm. Finally, the feature-optimized classifier was trained, and the actual performances of the sensor at different contact positions were tested. Experimental results showed that the recognition rates of the end, middle, and root of the sensor were 90.67%, 98%, and 96% respectively. A tactile-based method with intelligence could produce high accuracy for rice plant recognition, as demonstrated in this study. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {ANN; Recognition; Rice; Tactile; Weeds},
	keywords = {Algorithms; Machine Learning; Neural Networks, Computer; Oryza; Touch; Fractal dimension; Genetic algorithms; Machine learning; Weed control; Classification accuracy; Contact position; Neural network classifier; Real time recognition; Recognition features; Tactile information; Tactile sensors; Waveform factor; algorithm; machine learning; Oryza; touch; Plants (botany)},
	correspondence_address = {X. Ma; College of Engineering, South China Agricultural University, Guangzhou, 510642, China; email: maxu1959@scau.edu.cn},
	publisher = {MDPI AG},
	issn = {14248220},
	pmid = {32916874},
	language = {English},
	abbrev_source_title = {Sensors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Varghese2020800,
	author = {Varghese, Baizel Kurian and Augustine, Albin and Babu, Jubil Maria and Sunny, Deepa and Cherian, Er.Sijo},
	title = {INFOPLANT: Plant Recognition using Convolutional Neural Networks},
	year = {2020},
	journal = {Proceedings of the 4th International Conference on Computing Methodologies and Communication, ICCMC 2020},
	pages = {800 – 807},
	doi = {10.1109/ICCMC48092.2020.ICCMC-000149},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084646059&doi=10.1109%2fICCMC48092.2020.ICCMC-000149&partnerID=40&md5=a0a035ad9f957a32a0aa870939f8120d},
	affiliations = {Computer Science and Engineering Saintgits, College of Engineering},
	abstract = {The identification of a large number of trees around us is a complex task. The people who study plants can easily identify the plants based on the features of the leaf. Machine learning techniques are used to group leaf types automatically. But the deep learning techniques bring a lot of research improvements that make this classification more practical. Here we propose a system to identify the plants using the CNN model, which involves image processing. A perfect insight on plants is necessary to enable recognizing the novel or the species that are uncommon. The variations on leaf characteristics help to conduct a comparative study on plants. © 2020 IEEE.},
	author_keywords = {CNN; Deep Learning; tflite; Transfer learning},
	keywords = {Convolutional neural networks; Deep learning; Image processing; Learning algorithms; CNN models; Comparative studies; Complex task; Leaf characteristics; Learning techniques; Machine learning techniques; Number of trees; Plant recognition; Learning systems},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172814889-2},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Comput. Methodol. Commun., ICCMC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 4th International Conference on Computing Methodologies and Communication, ICCMC 2020; Conference date: 11 March 2020 through 13 March 2020; Conference code: 159543}
}

@CONFERENCE{Chulif2020,
	author = {Chulif, Sophia and Chang, Yang Loong},
	title = {Herbarium-Field Triplet Network for Cross-Domain Plant Identification NEUON Submission to LifeCLEF 2020 Plant},
	year = {2020},
	journal = {CEUR Workshop Proceedings},
	volume = {2696},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121787269&partnerID=40&md5=691155bbe8781c9ef9951009d44324bb},
	affiliations = {Department of Artificial Intelligence, NEUON AI, Sarawak, 94300, Malaysia},
	abstract = {This paper presents the implementation and performance of a Herbarium-Field triplet loss network to evaluate the herbarium-field similarity of plants which corresponds to the cross-domain plant identification challenge in PlantCLEF 2020. A two-streamed triplet loss network is trained to maximize the embedding distance of different plant species and at the same time minimize the embedding distance of the same plant species given herbarium-field pairs. The team submitted seven runs which achieved a Mean Reciprocal Rank score of 0.121 and 0.111 for the whole test set and the sub-set of the test set respectively. Copyright © 2020 for this paper by its authors.},
	author_keywords = {Computer vision; Convolutional neural networks; Cross-domain plant identification; Triplet loss},
	keywords = {Computer vision; Convolutional neural networks; Convolutional neural network; Cross-domain; Cross-domain plant identification; Embeddings; Loss networks; Performance; Plant identification; Plant species; Test sets; Triplet loss; Embeddings},
	editor = {De Carolis B. and Gena C. and Lieto A. and Rossi S. and Sciutti A.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 11th Conference and Labs of the Evaluation Forum, CLEF 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 163832}
}

@ARTICLE{Misra2020197,
	author = {Misra, Debaleena and Crispim-Junior, Carlos and Tougne, Laure},
	title = {Patch-Based CNN Evaluation for Bark Classification},
	year = {2020},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12540 LNCS},
	pages = {197 – 212},
	doi = {10.1007/978-3-030-65414-6_15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101390099&doi=10.1007%2f978-3-030-65414-6_15&partnerID=40&md5=020ef30c3605b4c8e8aa6647a19d5fa9},
	affiliations = {Univ Lyon, Lyon 2, LIRIS, Lyon, 69676, France},
	abstract = {The identification of tree species from bark images is a challenging computer vision problem. However, even in the era of deep learning today, bark recognition continues to be explored by traditional methods using time-consuming handcrafted features, mainly due to the problem of limited data. In this work, we implement a patch-based convolutional neural network alternative for analyzing a challenging bark dataset Bark-101, comprising of 2587 images from 101 classes. We propose to apply image re-scaling during the patch extraction process to compensate for the lack of sufficient data. Individual patch-level predictions from fine-tuned CNNs are then combined by classical majority voting to obtain image-level decisions. Since ties can often occur in the voting process, we investigate various tie-breaking strategies from ensemble-based classifiers. Our study outperforms the classification accuracy achieved by traditional methods applied to Bark-101, thus demonstrating the feasibility of applying patch-based CNNs to such challenging datasets. © 2020, Springer Nature Switzerland AG.},
	author_keywords = {Bark classification; Bicubic interpolation; Convolutional neural networks; Image re-scaling; Majority voting; Patch-based CNNs; Super-resolution networks; Transfer learning},
	keywords = {Computer vision; Convolutional neural networks; Deep learning; Bark classification; Classification accuracy; Computer vision problems; Extraction process; Limited data; Tie-breaking; Tree species; Voting process; Classification (of information)},
	correspondence_address = {C. Crispim-Junior; Univ Lyon, Lyon, Lyon 2, LIRIS, 69676, France; email: carlos.crispim-junior@liris.cnrs.fr},
	editor = {Bartoli A. and Fusiello A.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303065413-9},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: Workshops held at the 16th European Conference on Computer Vision, ECCV 2020; Conference date: 23 August 2020 through 28 August 2020; Conference code: 253939; All Open Access, Green Open Access}
}

@ARTICLE{Juadih2020573,
	author = {Juadih, Walla Rahim and Hashim, Kadhim M.},
	title = {Classification of plants based on the outer shape of the leaf},
	year = {2020},
	journal = {Journal of Advanced Research in Dynamical and Control Systems},
	volume = {12},
	number = {1 Special Issue},
	pages = {573 – 582},
	doi = {10.5373/JARDCS/V12SP1/20201106},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079464226&doi=10.5373%2fJARDCS%2fV12SP1%2f20201106&partnerID=40&md5=cf9ee78084496a0e3fb15f3875b7306f},
	affiliations = {Computer Science Department, College of Education for Pure Sciences, University of Thi-Qar, Nasiriya, Iraq},
	abstract = {Computer is very important in our life and they get into all fields. So, in this research we will use computer's application and it's importantly in recognition and classification plants' leaves, because of fewness of plants' classification experts. The data base used includes (35) types of different special plants after preprocessing on these pictures, the primal processing includes two practical's: (picture's enhancement and verges determine for pictures). In addition, we extract a group of geometrical features of plants' leaves depending on outwardness surround for leaf. We extract (10) features Euclidian for each leaf contains (surround, distance, center, main diameter, second diameter, and Extreme points). We used traditional distance for measuring distance between any two points. Then, the leaf of plant is recognized using algorithm C4.5. © 2020, Institute of Advanced Scientific Research, Inc.. All rights reserved.},
	author_keywords = {Algorithm C4.5; Data Base For Plants' Leaves; Features Extraction; Primal Processing},
	publisher = {Institute of Advanced Scientific Research, Inc.},
	issn = {1943023X},
	language = {English},
	abbrev_source_title = {J. Adv. Res. Dyn. Control. Syst.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Liu20201461,
	author = {Liu, Wei and Feng, Wenlong and Huang, Mengxing and Han, Guilai and Lin, Jialun},
	title = {Plant taxonomy in hainan based on deep convolutional neural network and transfer learning},
	year = {2020},
	journal = {Proceedings - 2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom 2020},
	pages = {1461 – 1466},
	doi = {10.1109/TrustCom50675.2020.00197},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101189522&doi=10.1109%2fTrustCom50675.2020.00197&partnerID=40&md5=76b1d035372401ff5af0863a132ba642},
	affiliations = {Institute of Medical Informatics, Hainan Medical University Institute of Information and Communication, Hainan University, Haikou, China; Institute of Information and Communication, Hainan University, Haikou, China; Institute of Medical Informatics, Hainan Medical University, Haikou, China},
	abstract = {Plant play a very important role in the protection of the ecological balance. Compared to manual identification of plants, automated plant identification enable experts to process significantly greater numbers of plants with higher efficiencies in shorter periods of time. In this study, we propose an effective deep Convolutional Neural Network (CNN)-based model that is capable of automatically identifying and classifying plant species in Hainan by studying the details of their leaves. We apply transfer learning based on CNN to fine-tune the pre-trained models. Further, the optimal values of associated hyperparameters that maximize the accuracy of the proposed method are determined. Finally, experiments are carried out on two available botanical datasets: the Flavia dataset with 32 classes and the HNPlant dataset with 10 classes. The results demonstrate that the highest classification accuracies exhibited by the proposed CNN-based model on the Flavia and HNPlant datasets are 89% and 95%, respectively, thus establishing their effectiveness. © 2020 IEEE.},
	author_keywords = {Convolutional Neural Network; Hainan Plant Classification; Plant Identification; Plant Leaves; Transfer Learning},
	keywords = {Classification (of information); Convolution; Deep learning; Deep neural networks; Plants (botany); Transfer learning; Classification accuracy; Ecological balance; Higher efficiency; Hyperparameters; Manual identification; Optimal values; Plant identification; Plant taxonomy; Convolutional neural networks},
	editor = {Wang G. and Ko R. and Bhuiyan M.Z.A. and Pan Y.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-166540392-4},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. Trust, Secur. Priv. Comput. Commun., TrustCom},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 19th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom 2020; Conference date: 29 December 2020 through 1 January 2021; Conference code: 167070}
}

@ARTICLE{Suto20201311,
	author = {Suto, Jozsef},
	title = {Plant leaf recognition with shallow and deep learning: A comprehensive study},
	year = {2020},
	journal = {Intelligent Data Analysis},
	volume = {24},
	number = {6},
	pages = {1311 – 1328},
	doi = {10.3233/IDA-194821},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098259626&doi=10.3233%2fIDA-194821&partnerID=40&md5=37e86e02bb0a2ee7ca58bb9493a6e3cd},
	affiliations = {Department of Informatics Systems and Networks, Faculty of Informatics, University of Debrecen, Kassai street, 26, Debrecen, 4028, Hungary},
	abstract = {Nowadays there are hundreds of thousands known plant species on the Earth and many are still unknown yet. The process of plant classification can be performed using different ways but the most popular approach is based on plant leaf characteristics. Most types of plants have unique leaf characteristics such as shape, color, and texture. Since machine learning and vision considerably developed in the past decade, automatic plant species (or leaf) recognition has become possible. Recently, the automated leaf classification is a standalone research area inside machine learning and several shallow and deep methods were proposed to recognize leaf types. From 2007 to present days several research papers have been published in this topic. In older studies the classifier was a shallow method while in current works many researchers applied deep networks for classification. During the overview of plant leaf classification literature, we found an interesting deficiency (lack of hyper-parameter search) and a key difference between studies (different test sets). This work gives an overall review about the efficiency of shallow and deep methods under different test conditions. It can be a basis to further research.  © 2020 - IOS Press and the authors. All rights reserved.},
	author_keywords = {Artificial neural network; deep learning; feature engineering; plant leaf recognition},
	keywords = {Classification (of information); Learning systems; Textures; Hyper-parameter; Leaf characteristics; Leaf classification; Plant classification; Plant leaf classifications; Plant species; Research papers; Test condition; Deep learning},
	correspondence_address = {J. Suto; Department of Informatics Systems and Networks, Faculty of Informatics, University of Debrecen, Debrecen, Kassai street, 26, 4028, Hungary; email: suto.jozsef@inf.unideb.hu},
	publisher = {IOS Press BV},
	issn = {1088467X},
	language = {English},
	abbrev_source_title = {Intell. Data Anal.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Wang2020218,
	author = {Wang, Chen and Zhou, Jun and Xu, Cheng-yuan and Bai, Xiao},
	title = {A Deep Object Detection Method for Pineapple Fruit and Flower Recognition in Cluttered Background},
	year = {2020},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12068 LNCS},
	pages = {218 – 227},
	doi = {10.1007/978-3-030-59830-3_19},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092934967&doi=10.1007%2f978-3-030-59830-3_19&partnerID=40&md5=6115424e4f1bd47e63ea92fe00094eb9},
	affiliations = {School of Computer Science and Engineering, Beihang University, Beijing, China; School of Information and Communication Technology, Griffith University, Brisbane, Australia; School of Health, Medical and Applied Sciences, Central Queensland University, Rockhampton, Australia},
	abstract = {Natural initiation of pineapple flowers is not synchronized, which yields difficulties in yield prediction and the decision of harvest. Computer vision based pineapple detection system is an automated solution to address this issue. However, it is faced with significant challenges, e.g. pineapple flowers and fruits vary in size at different growing stages, the images are influenced by camera viewpoint, illumination conditions, occlusion and so on. This paper presents an approach for pineapple fruit and flower recognition using a state-of-the-art deep object detection model. We collected images from pineapple orchard using three different cameras and selected suitable ones to create a dataset. The experimental results show promising detection performance, with an mAP of 0.64 and F1 score of 0.69. © 2020, Springer Nature Switzerland AG.},
	author_keywords = {Deep learning; Pineapple detection; YOLOv3},
	keywords = {Artificial intelligence; Cameras; Fruits; Object recognition; Automated solutions; Cluttered backgrounds; Detection performance; Detection system; Flower recognition; Illumination conditions; Object detection method; State of the art; Object detection},
	correspondence_address = {J. Zhou; School of Information and Communication Technology, Griffith University, Brisbane, Australia; email: jun.zhou@griffith.edu.au},
	editor = {Lu Y. and Vincent N. and Yuen P.C. and Zheng W.-S. and Cheriet F. and Suen C.Y.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303059829-7},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2nd International Conference on Pattern Recognition and Artificial Intelligence, ICPRAI 2020; Conference date: 19 October 2020 through 23 October 2020; Conference code: 249999}
}

@ARTICLE{Unal2020105587,
	author = {Unal, Zeynep},
	title = {Smart Farming Becomes even Smarter with Deep Learning - A Bibliographical Analysis},
	year = {2020},
	journal = {IEEE Access},
	volume = {8},
	pages = {105587 – 105609},
	doi = {10.1109/ACCESS.2020.3000175},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086712805&doi=10.1109%2fACCESS.2020.3000175&partnerID=40&md5=d75f1a2310ade71c24e980bfa651ec1c},
	affiliations = {Niǧtaş Company, Niǧde, Turkey},
	abstract = {Smart farming is a new concept that makes agriculture more efficient and effective by using advanced information technologies. The latest advancements in connectivity, automation, and artificial intelligence enable farmers better to monitor all procedures and apply precise treatments determined by machines with superhuman accuracy. Farmers, data scientists and, engineers continue to work on techniques that allow optimizing the human labor required in farming. With valuable information resources improving day by day, smart farming turns into a learning system and becomes even smarter. Deep learning is a type of machine learning method, using artificial neural network principles. The main feature by which deep learning networks are distinguished from neural networks is their depth and that feature makes them capable of discovering latent structures within unlabeled, unstructured data. Deep learning networks that do not need human intervention while performing automatic feature extraction have a significant advantage over previous algorithms. The focus of this study is to explore the advantages of using deep learning in agricultural applications. This bibliography reviews the potential of using deep learning techniques in agricultural industries. The bibliography contains 120 papers from the database of the Science Citation Index on the subject that were published between 2016 and 2019. These studies have been retrieved from 39 scientific journals. The papers are classified into the following categories as disease detection, plant classification, land cover identification, precision livestock farming, pest recognition, object recognition, smart irrigation, phenotyping, and weed detection. © 2013 IEEE.},
	author_keywords = {artificial neural networks; internet of things; Machine learning; precision agriculture},
	keywords = {Agricultural robots; Agriculture; Bibliographies; Learning systems; Neural networks; Object recognition; Weed control; Agricultural industries; Automatic feature extraction; Information resource; Land cover identifications; Machine learning methods; Plant classification; Precision livestock farming; Science citation index; Deep learning},
	correspondence_address = {Z. Unal; Niǧtaş Company, Niǧde, Turkey; email: zeynepunal1010@hotmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 59; All Open Access, Gold Open Access}
}

@CONFERENCE{Kusumawardani2019,
	author = {Kusumawardani, Wahyu and Muzzazinah and Ramli, Murni},
	title = {Plant taxonomy learning and research: A systematics review},
	year = {2019},
	journal = {AIP Conference Proceedings},
	volume = {2194},
	doi = {10.1063/1.5139783},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077717749&doi=10.1063%2f1.5139783&partnerID=40&md5=3567605bd4e7b92d278fce64f7d10753},
	affiliations = {Master Program of Biology Education, Universitas Sebelas Maret, Surakarta, Indonesia},
	abstract = {Concepts of plant identification and classification were important basic knowledge to be mastered by biology students. The research was aimed to find out what concepts and methods of learning plant taxonomy, and find out the objects and methods in plant taxonomy research. Seventeen articles published from 2005-2019 were selected as the review materials. Nine articles were about learning the plant taxonomy, and eight articles were about research on plant taxonomy. The articles were obtained from Journal of Biological Education and Science Direct. The results showed the common concepts learned about plant identification and classification. The prominent plant groups used in the learning were: the Bryophytes, Pteridophytes, Gymnosperms, and Angiosperm with the example of the native species and focal species. The learning methods and approaches were varied, including: using real plant specimens, dichotomous key method, word association exercise based on mnemonics approach, or pictorial card games for identification native plants. Others use an electronic multi-access key, iOS app on the iPod for plant identification guide, interactive multimedia dichotomous key for plant identification, labeled drawing and descriptive writing of native plant identification. Various aspects used as the object of the research on plants taxonomy, one of them was the leaves. Various methods used in the research on plant taxonomy, such as: FRT, LDC Linear, kNN, SIFT, Color moments, SFTA, ANNs, Deep learning techniques, hierarchical approach - NFC, and AIT. © 2019 Author(s).},
	author_keywords = {learning; plant taxonomy; research},
	correspondence_address = {W. Kusumawardani; Master Program of Biology Education, Universitas Sebelas Maret, Surakarta, Indonesia; email: wahyukusumawardani@student.uns.ac.id},
	editor = {Indriyanti N.Y. and Ramli M. and Nurhasanah F.},
	publisher = {American Institute of Physics Inc.},
	issn = {0094243X},
	isbn = {978-073541945-2},
	language = {English},
	abbrev_source_title = {AIP Conf. Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 2nd International Conference on Science, Mathematics, Environment, and Education, ICoSMEE 2019; Conference date: 26 July 2019 through 28 July 2019; Conference code: 156217; All Open Access, Bronze Open Access}
}

@ARTICLE{Kamath2020,
	author = {Kamath, Radhika and Balachandra, Mamatha and Prabhu, Srikanth},
	title = {Paddy Crop and Weed Discrimination: A Multiple Classifier System Approach},
	year = {2020},
	journal = {International Journal of Agronomy},
	volume = {2020},
	doi = {10.1155/2020/6474536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088694360&doi=10.1155%2f2020%2f6474536&partnerID=40&md5=576209b566b8c65487766684a68f7ef1},
	affiliations = {Department of Computer Science and Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, India},
	abstract = {Weeds are unwanted plants that grow among crops. These weeds can significantly reduce the yield and quality of the farm output. Unfortunately, site-specific weed management is not followed in most of the cases. That is, instead of treating a field with a specific type of herbicide, the field is treated with a broadcast herbicide application. This broadcast application of the herbicide has resulted in herbicide-resistant weeds and has many ill effects on the natural environment. This has prompted many research studies to seek the most effective weed management techniques. One such technique is computer vision-based automatic weed detection and identification. Using this technique, weeds can be detected and identified and a suitable herbicide can be recommended to the farmers. Therefore, it is important for the computer vision technique to successfully identify and classify the crops and weeds from the digital images. This paper investigates the multiple classifier systems built using support vector machines and random forest classifiers for plant classification in classifying paddy crops and weeds from digital images. Digital images of paddy crops and weeds from the paddy fields were acquired using three different cameras fixed at different heights from the ground. Texture, color, and shape features were extracted from the digital images after background subtraction and used for classification. A simple and new method was used as a decision function in the multiple classifier systems. An accuracy of 91.36% was obtained by the multiple classifier systems and was found to outperform single classifier systems. © 2020 Radhika Kamath et al.},
	correspondence_address = {M. Balachandra; Department of Computer Science and Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, India; email: mamtha.bc@manipal.edu},
	publisher = {Hindawi Limited},
	issn = {16878159},
	language = {English},
	abbrev_source_title = {Int. J. Agron},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access, Green Open Access}
}

@ARTICLE{Bonnet2020,
	author = {Bonnet, Pierre and Joly, Alexis and Faton, Jean-Michel and Brown, Susan and Kimiti, David and Deneu, Benjamin and Servajean, Maximilien and Affouard, Antoine and Lombardo, Jean-Christophe and Mary, Laura and Vignau, Christel and Munoz, François},
	title = {How citizen scientists contribute to monitor protected areas thanks to automatic plant identification tools},
	year = {2020},
	journal = {Ecological Solutions and Evidence},
	volume = {1},
	number = {2},
	doi = {10.1002/2688-8319.12023},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100405898&doi=10.1002%2f2688-8319.12023&partnerID=40&md5=031c51c172dabef7f4eccd386def6f41},
	affiliations = {CIRAD, UMR AMAP, Occitanie, Montpellier, France; AMAP, University of Montpellier, CIRAD, CNRS, INRAE, IRD, Occitanie, Montpellier, France; INRIA Sophia-Antipolis - ZENITH team, LIRMM - UMR 5506 - CC 477, Occitanie, Montpellier, France; Réserve naturelle nationale des Ramières du Val de Drôme, Allex, France; Lewa House Pimbi Ltd., Isiolo, Kenya; Lewa Wildlife Conservancy, Isiolo, Kenya; LIRMM UMR 5506, CNRS, University of Montpellier, Occitanie, Montpellier, France; AMIS, Paule Valery University - Montpellier 3, Occitanie, Montpellier, France; Tela Botanica, Occitanie, Montpellier, France; Laboratoire d'Ecologie Alpine, Université Grenoble Alpes, Grenoble, France},
	abstract = {1. Successful monitoring and management of plant resources worldwide needs the involvement of civil society to support natural reserve managers. Because it is difficult to correctly and quickly identify plant species for non-specialists, the development of recent techniques based on automatic visual identification should facilitate and increase public engagement in citizen science initiatives. 2. Automatic identification platforms are new to most citizen scientists and land managers. Pl@ntNet is such a platform, available since 2013 on web and mobile environments, and now included in several workflows such as invasive alien species management, endemic species monitoring, educational activities and eco-tourism practices. The successful development of such platforms needs to identify their strengths and weaknesses in order to improve and facilitate their use in all aspects of ecosystem management. 3. Here we present two Pl@ntNet citizen science initiatives used by conservation practitioners in Europe (France) and Africa (Kenya). We discuss various perspectives, including benefits and limitations. Based on the experiences of field managers, we formulate several recommendations for future initiatives. The recommendations are aimed at a diverse group of conservation managers and citizen science practitioners. © 2020 The Authors. Ecological Solutions and Evidence published by John Wiley & Sons Ltd on behalf of British Ecological Society.},
	author_keywords = {artificial intelligence; automatic plant identification; citizen science; deep learning technologies; données opportunistes; engagement du public; identification automatique des plantes; intelligence artificielle; opportunistic data; plant biodiversity monitoring; public engagement; science citoyenne; surveillance de la biodiversité végétale; technologies d'apprentissage profond; volontaires; volunteers},
	correspondence_address = {P. Bonnet; CIRAD, UMR AMAP, Montpellier, Occitanie, France; email: pierre.bonnet@cirad.fr},
	publisher = {Blackwell Publishing Ltd},
	issn = {26888319},
	language = {English},
	abbrev_source_title = {Ecol. Solut. Evid.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Alamoudi2020,
	author = {Alamoudi, Shadi and Hong, Xia and Wei, Hong},
	title = {Plant Leaf Recognition Using Texture Features and Semi-Supervised Spherical K-means Clustering},
	year = {2020},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	doi = {10.1109/IJCNN48605.2020.9207386},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093829310&doi=10.1109%2fIJCNN48605.2020.9207386&partnerID=40&md5=94b778df99724a23fa3001f211b53a4f},
	affiliations = {University of Reading, Department of Computer Science School of Mathematical, Physical and Computational Sciences, Reading, RG6 6AY, United Kingdom},
	abstract = {Automatic plant leave recognition using digital images and machine learning techniques is an important task. The disadvantage of supervised learning techniques is that they are limited to learn from labelled datasets which are often expensive to obtain. In this paper, a novel decision fusion framework is proposed by combining semi-supervised clustering with the well known image features analysis methods in computer vision. Initially the leave image features are generated by applying the Grey Level Co-occurrence Matrix analysis to the processed leave images transformed by Gabor or Laplacian of Gaussian filters. Then an on-line spherical k-means clustering technique, guided by a minimum number of labelled leaves, is used to train the base classifiers. The final decision of classification is produced by selecting classifier which produces the max-cosine value amongst the baseline classifiers. Comparative experiments have been carried out to demonstrate that proposed approaches are suited for automatic leave type recognition. © 2020 IEEE.},
	keywords = {Image analysis; Learning systems; Neural networks; Plants (botany); Supervised learning; Textures; Base classifiers; Comparative experiments; Grey level co-occurrence matrixes; K-means clustering techniques; Laplacian of gaussian filters; Machine learning techniques; Semi-supervised Clustering; Texture features; K-means clustering},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172816926-2},
	coden = {85OFA},
	language = {English},
	abbrev_source_title = {Proc Int Jt Conf Neural Networks},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 2020 International Joint Conference on Neural Networks, IJCNN 2020; Conference date: 19 July 2020 through 24 July 2020; Conference code: 163566}
}

@ARTICLE{Sindic20201770,
	author = {Sindic, Caleb M.T. and Riday, Heathcliffe},
	title = {Using image object recognition to increase biomass in red clover (Trifolium pratense L.) breeding},
	year = {2020},
	journal = {Crop Science},
	volume = {60},
	number = {4},
	pages = {1770 – 1781},
	doi = {10.1002/csc2.20028},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084828821&doi=10.1002%2fcsc2.20028&partnerID=40&md5=0041b4c38a28c19a1b60cafdc82d8a61},
	affiliations = {US Dairy Forage Research Center, USDA-ARS, Madison, 53706, WI, United States},
	abstract = {Phenotyping in forage legume breeding can be time consuming and resource intensive. With computation advances and computational power cost reductions, utilizing artificial intelligence in automated phenotyping is becoming feasible for even resource limited forage legume breeding programs. Here we report on the use of machine learning to train a neural network to identify and isolate red clover plants from digital images of space planted red clover nurseries. Challenges to red clover plant identification included: 1) plants were grown with a grass companion; and 2) plants were close enough to each other so that plants often overlapped each other in the digital images. To estimate biomass yield a second neural network was trained using machine learning to count leaves per plant among identified red clover plants from the plant classification neural network. The two neural networks were validated on six red clover digital image sets taken on red clover space plant nurseries. Average neural network plant classification success rates were measured at 94.6% across the six digital image sets. Neural network red clover leaf counts were correlated with human visual biomass scores at r2 = 0.528. We conclude that automated phenotyping based on digital image analysis of red clover breeding nurseries is currently feasible. We further conclude that additional phenotypic traits could be obtained on identified red clover plants from such images sets. © 2019 The Authors. Crop Science © 2019 Crop Science Society of America},
	correspondence_address = {H. Riday; US Dairy Forage Research Center, USDA-ARS, Madison, 53706, United States; email: Heathcliffe.Riday@ars.usda.gov},
	publisher = {John Wiley and Sons Inc.},
	issn = {0011183X},
	coden = {CRPSA},
	language = {English},
	abbrev_source_title = {Crop Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{Zhao202056,
	author = {Zhao, Yue and Zheng, Yili and Shi, Honglei and Zhang, Lu},
	title = {Transfer learning-based convolutional neural network image recognition method for plant leaves},
	year = {2020},
	journal = {International Journal of Circuits, Systems and Signal Processing},
	volume = {14},
	pages = {56 – 62},
	doi = {10.46300/9106.2020.14.9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085515869&doi=10.46300%2f9106.2020.14.9&partnerID=40&md5=3b304a14fc3182afdcdfd17f701748ce},
	affiliations = {Beijing Forestry University, Beijing, 100083, China},
	abstract = {To improve the accuracy of plant leaf image recognition with a small dataset of plant leaves, a convolution neural network (CNN) plant leaf image recognition method based on transfer learning is proposed. First, a plant leaf image database was expanded by pre-processing the original plant leaf images through random horizontal and vertical rotation and random zooming. The expanded dataset was then processed by mean removal and divided into training and testing sets at a ratio of 4:1. Second, transfer learning training was performed on the plant leaf dataset using existing models (AlexNet and InceptionV3) that were pre-trained on a large dataset. To ensure these models can be adapted to image recognition for plant leaves, the original parameters of the last fully connected layer were replaced, whereas those of all other convolution layers were retained. Finally, the method proposed in this paper was compared to support vector machine, deep belief network, and CNN through testing on the ICL database. A Tensorflow training network model was used in the comparison test, and the results were visualized by Tensorboard. The testing results showed a considerable improvement in recognition accuracy when using the pre-trained AlexNet and InceptionV3 models, where the training dataset accuracies were 95.31% and 95.4%, respectively. © 2020, North Atlantic University Union. All rights reserved.},
	author_keywords = {Convolutional neural network; Leaf recognition; Transfer learning},
	keywords = {Convolution; Convolutional neural networks; Deep learning; Image enhancement; Image recognition; Large dataset; Learning systems; Plants (botany); Statistical tests; Support vector machines; Convolution neural network; Deep belief networks; Plant leaf images; Recognition accuracy; Recognition methods; Training and testing; Training network; Vertical rotation; Transfer learning},
	correspondence_address = {Y. Zheng; Beijing Forestry University, Beijing, 100083, China; email: zhengyili@bjfu.edu.cn},
	publisher = {North Atlantic University Union},
	issn = {19984464},
	language = {English},
	abbrev_source_title = {Int. J. Circuit Syst. Signal Process.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Bronze Open Access}
}

@ARTICLE{Forero202035,
	author = {Forero, Manuel G. and Beltrán, Carlos E. and Troncoso, Armando and González-Santos, Christian},
	title = {Classification of cattleya trianae and its varieties by using colorimetry},
	year = {2020},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12088 LNCS},
	pages = {35 – 44},
	doi = {10.1007/978-3-030-49076-8_4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087279217&doi=10.1007%2f978-3-030-49076-8_4&partnerID=40&md5=d6a17477482c8efde772ef80c9865fed},
	affiliations = {Semillero Lún, Facultad de Ingeniería, Universidad de Ibagué, Ibagué, Colombia; Semillero Lún, Facultad de Ciencias Naturales y Matemáticas, Universidad de Ibagué, Ibagué, Colombia},
	abstract = {Orchids in general, like Cattleya trianae, have been characterized mostly by taxonomic and visual studies. However, colour is not used for classification. Here, a new method for identifying and classifying orchids of different varieties of Cattleya trianae is introduced. This method is not subjective and uses the colour information obtained from the central axis of the Cattleya trianae lip. To this end, a new acquisition protocol was established, which uses a new device for image acquisition of the labellum’s central axis from the hippocampus to the epicentre. The colour patterns found between samples of the same variety were adjusted and it was verified by using correlation they can be employed to identify each variety. Finally, a support vector machine was used to classify and identify four Cattleya trianae varieties, finding that a linear kernel was enough to classify them with an accuracy of 100%. © Springer Nature Switzerland AG 2020.},
	author_keywords = {Cattleya trianae; Orchid varieties; Orchids; Plant classification},
	keywords = {Biodiversity; Color; Colorimetry; Support vector machines; Acquisition protocols; Central axis; Colour informations; Epicentre; Linear kernel; New devices; Pattern recognition},
	correspondence_address = {M.G. Forero; Semillero Lún, Facultad de Ingeniería, Universidad de Ibagué, Ibagué, Colombia; email: manuel.forero@unibague.edu.co},
	editor = {Figueroa Mora K.M. and Anzurez Marín J. and Cerda J. and Carrasco-Ochoa J.A. and Martínez-Trinidad J.F. and Olvera-López J.A.},
	publisher = {Springer},
	issn = {03029743},
	isbn = {978-303049075-1},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 12th Mexican Conference on Pattern Recognition, MCPR 2020; Conference date: 24 June 2020 through 27 June 2020; Conference code: 241229; All Open Access, Bronze Open Access}
}

@ARTICLE{Kattenborn2019,
	author = {Kattenborn, Teja and Eichel, Jana and Fassnacht, Fabian Ewald},
	title = {Convolutional Neural Networks enable efficient, accurate and fine-grained segmentation of plant species and communities from high-resolution UAV imagery},
	year = {2019},
	journal = {Scientific Reports},
	volume = {9},
	number = {1},
	doi = {10.1038/s41598-019-53797-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075712641&doi=10.1038%2fs41598-019-53797-9&partnerID=40&md5=36d9812dc9d6244a6775590f843f7ab0},
	affiliations = {Institute of Geography and Geoecology (IFGG), Karlsruhe Institute of Technology (KIT), Kaiserstr. 12, Karlsruhe, 76131, Germany},
	abstract = {Recent technological advances in remote sensing sensors and platforms, such as high-resolution satellite imagers or unmanned aerial vehicles (UAV), facilitate the availability of fine-grained earth observation data. Such data reveal vegetation canopies in high spatial detail. Efficient methods are needed to fully harness this unpreceded source of information for vegetation mapping. Deep learning algorithms such as Convolutional Neural Networks (CNN) are currently paving new avenues in the field of image analysis and computer vision. Using multiple datasets, we test a CNN-based segmentation approach (U-net) in combination with training data directly derived from visual interpretation of UAV-based high-resolution RGB imagery for fine-grained mapping of vegetation species and communities. We demonstrate that this approach indeed accurately segments and maps vegetation species and communities (at least 84% accuracy). The fact that we only used RGB imagery suggests that plant identification at very high spatial resolutions is facilitated through spatial patterns rather than spectral information. Accordingly, the presented approach is compatible with low-cost UAV systems that are easy to operate and thus applicable to a wide range of users. © 2019, The Author(s).},
	keywords = {Algorithms; Data Collection; Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer; Plant Physiological Phenomena; Plants; Remote Sensing Technology; Satellite Imagery; article; computer vision; convolutional neural network; deep learning; human; human experiment; image analysis; imagery; plant identification; vegetation; algorithm; image processing; information processing; plant; plant physiology; procedures; remote sensing; satellite imagery},
	correspondence_address = {T. Kattenborn; Institute of Geography and Geoecology (IFGG), Karlsruhe Institute of Technology (KIT), Karlsruhe, Kaiserstr. 12, 76131, Germany; email: teja.kattenborn@kit.edu},
	publisher = {Nature Research},
	issn = {20452322},
	pmid = {31776370},
	language = {English},
	abbrev_source_title = {Sci. Rep.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 123; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Safar2020463,
	author = {Safar, Amna and Safar, Maytham},
	title = {Intelligent flower detection system using machine learning},
	year = {2020},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {1038},
	pages = {463 – 472},
	doi = {10.1007/978-3-030-29513-4_33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072828272&doi=10.1007%2f978-3-030-29513-4_33&partnerID=40&md5=f3e71bc8e609667a1e8e6be0aa532c44},
	affiliations = {Kuwait University, Kuwait City, Kuwait},
	abstract = {It is a very hard and a challenging mission to identify different types of flowers as they are very similar. Even expert botanists and gardeners cannot identify some of them accurately. The idea of automating flowers recognition is bewildering as the flowers are not rigid objects and their images can be affected by many External influences. The proposed system use machine learning algorithms to fully automate and increase the accuracy of flower classification. Machine learning model will be used to extract flower’s features automatically, process through different layers of the neural network and finally classify the flower class. The proposed work is based on “Resnet” model, which is used for classification task. Resnet won the first place on ILSVRC 2015. Many enhancements have been made on Resnet model to improve the accuracy. Fine tuning, dropout ratio and class weight are some of the proposed model enhancements. The proposed model reaches 92% accuracy, which is the highest percent till the moment. © Springer Nature Switzerland AG 2020.},
	author_keywords = {Flower classification; Machine learning; ResNet model},
	keywords = {Intelligent systems; Learning algorithms; Learning systems; Multilayer neural networks; Classification tasks; Different layers; External influences; Fine tuning; Flower detections; Machine learning models; Rigid objects; System use; Machine learning},
	correspondence_address = {A. Safar; Kuwait University, Kuwait City, Kuwait; email: eng.safar@gmail.com},
	editor = {Bi Y. and Bhatia R. and Kapoor S.},
	publisher = {Springer Verlag},
	issn = {21945357},
	isbn = {978-303029512-7},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: Intelligent Systems Conference, IntelliSys 2019; Conference date: 5 September 2019 through 6 September 2019; Conference code: 231009}
}

@ARTICLE{Pawara2020,
	author = {Pawara, Pornntiwa and Okafor, Emmanuel and Groefsema, Marc and He, Sheng and Schomaker, Lambert R.B. and Wiering, Marco A.},
	title = {One-vs-One classification for deep neural networks},
	year = {2020},
	journal = {Pattern Recognition},
	volume = {108},
	doi = {10.1016/j.patcog.2020.107528},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087787971&doi=10.1016%2fj.patcog.2020.107528&partnerID=40&md5=ae8c184c73ec1270f184a84bdc882e5b},
	affiliations = {Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence, University of Groningen, 9747 AG Groningen, Netherlands; Department of Computer Engineering, Ahmadu Bello University, Zaria, Nigeria; Boston Children's Hospital, Harvard Medical School, United States},
	abstract = {For performing multi-class classification, deep neural networks almost always employ a One-vs-All (OvA) classification scheme with as many output units as there are classes in a dataset. The problem of this approach is that each output unit requires a complex decision boundary to separate examples from one class from all other examples. In this paper, we propose a novel One-vs-One (OvO) classification scheme for deep neural networks that trains each output unit to distinguish between a specific pair of classes. This method increases the number of output units compared to the One-vs-All classification scheme but makes learning correct decision boundaries much easier. In addition to changing the neural network architecture, we changed the loss function, created a code matrix to transform the one-hot encoding to a new label encoding, and changed the method for classifying examples. To analyze the advantages of the proposed method, we compared the One-vs-One and One-vs-All classification methods on three plant recognition datasets (including a novel dataset that we created) and a dataset with images of different monkey species using two deep architectures. The two deep convolutional neural network (CNN) architectures, Inception-V3 and ResNet-50, are trained from scratch or pre-trained weights. The results show that the One-vs-One classification method outperforms the One-vs-All method on all four datasets when training the CNNs from scratch. However, when using the two classification schemes for fine-tuning pre-trained CNNs, the One-vs-All method leads to the best performances, which is presumably because the CNNs had been pre-trained using the One-vs-All scheme. © 2020 The Authors},
	author_keywords = {Computer vision; Deep learning; Multi-class classification; One-vs-One classification; Plant recognition},
	keywords = {Classification (of information); Convolutional neural networks; Deep neural networks; Encoding (symbols); Learning systems; Signal encoding; State assignment; Classification methods; Classification scheme; Complex decision; Decision boundary; Deep architectures; Label encoding; Multi-class classification; Plant recognition; Network architecture},
	correspondence_address = {P. Pawara; Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence, University of Groningen, 9747 AG Groningen, Netherlands; email: p.pawara@rug.nl},
	publisher = {Elsevier Ltd},
	issn = {00313203},
	coden = {PTNRA},
	language = {English},
	abbrev_source_title = {Pattern Recogn.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 40; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Huang20191426,
	author = {Huang, Zhi-Kai and He, Cui-Qun and Wang, Zhen-Ning and Xi, Jun-Mei and Wang, Huan and Hou, Ling-Ying},
	title = {Cinnamomum Camphora Classification Based on Leaf Image Using Transfer Learning},
	year = {2019},
	journal = {Proceedings of 2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference, IAEAC 2019},
	pages = {1426 – 1429},
	doi = {10.1109/IAEAC47372.2019.8997791},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081181162&doi=10.1109%2fIAEAC47372.2019.8997791&partnerID=40&md5=51a2d93f6f53553d1fedfb8890a58eae},
	affiliations = {Nanchang Institute of Technology, College of Mechanical and Electrical Engineering, Nanchang, 330099, China},
	abstract = {Plants play a very important role in our daily life providing us with food and oxygen. It is necessary to build an aut omatic system for recognizing plant Convolutional neural netw orks (CNNs) have become effective identification method especial ly in computer vision in recent years because they achieved outst anding performance on different tasks, such as image classificatio ns. In this paper, we have shown performance comparison of usin g the seven pretrained deep convolutional neural network(Alexne t, Googlenet, Resnet50, Resnet101, Inceptionv3, Inceptionresnetv 2, Squeezenet) that employed fine-tuning technique to retain the l ast three full connection layers and get the classication model. We applied our method to Cinnamomum Camphora leaf dataset clas sification,.the experimental results showed that the proposed met hod is quite effective and feasible. © 2019 IEEE.},
	author_keywords = {Cinnamomum Camphora tree; Convolutional neural network; Deep learning; Plant classification; Transfer learn ing},
	keywords = {Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Image classification; Multilayer neural networks; Cinnamomum camphora; Classication; Daily lives; Fine tuning; Identification method; Leaf images; Performance comparison; Plant classification; Transfer learning},
	editor = {Xu B. and Mou K.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172811907-6},
	language = {English},
	abbrev_source_title = {Proc. IEEE Adv. Inf. Technol., Electron. Autom. Control Conf., IAEAC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th IEEE Advanced Information Technology, Electronic and Automation Control Conference, IAEAC 2019; Conference date: 20 December 2019 through 22 December 2019; Conference code: 157790}
}

@ARTICLE{Tan2020,
	author = {Tan, Chaoqun and Wu, Chong and Huang, Yongliang and Wu, Chunjie and Chen, Hu},
	title = {Identification of different species of Zanthoxyli Pericarpium based on convolution neural network},
	year = {2020},
	journal = {PLoS ONE},
	volume = {15},
	number = {4},
	doi = {10.1371/journal.pone.0230287},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083391313&doi=10.1371%2fjournal.pone.0230287&partnerID=40&md5=15e7452d11819f2393952b8029332743},
	affiliations = {National Key Laboratory of Fundamental Science on Synthetic Vision, College of Computer Science, Sichuan University, Chengdu, China; College of Pharmacy, Chengdu University of Traditional Chinese Medicine, Chengdu, China; Affiliated Hospital of Chengdu, University of Traditional Chinese Medicine, Chengdu, China},
	abstract = {Zanthoxyli Pericarpium (ZP) are the dried ripe peel of Zanthoxylum schinifolium Sieb. et Zucc (ZC) or Zanthoxylum bungeanum Maxim (ZB). It has wide range of uses both medicine and food, and favorable market value. The diverse specifications of components of ZP is exceptional, and the common aims of adulteration for economic profit is conducted. In this work, a novel method for the identification different species of ZP is proposed using convolutional neural networks (CNNs). The data used for the experiment is 5 classes obtained from camera and mobile phones. Firstly, the data considering 2 categories are trained to detect the labels by YOLO. Then, the multiple deep learning including VGG, ResNet, Inception v4, and DenseNet are introduced to identify the different species of ZP (HZB, DZB, OZB, ZA and JZC). In order to assess the performance of CNNs, compared with two traditional identification models including Support Vector Machines (SVM) and Back Propagation (BP). The experimental results demonstrate that the CNN model have a better performance to identify different species of ZP and the highest identification accuracy is 99.35%. The present study is proved to be a useful strategy for the discrimination of different traditional Chinese medicines (TCMs). © 2020 Tan et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	keywords = {Deep Learning; Medicine, Chinese Traditional; Mobile Applications; Smartphone; Zanthoxylum; Article; back propagation; Chinese medicine; convolutional neural network; Derong Zanthoxylum bungeanum; experimental study; Hanyuan Zanthoxylum bungeanum; Jinyang Zanthoxylum schinifolium; measurement accuracy; nonhuman; pericarp; plant identification; Sichuan Zanthoxylum bungeanum; support vector machine; Zanthoxyli Pericarpium; Zanthoxylum; Zanthoxylum armatum; Chinese medicine; classification; mobile application; smartphone},
	correspondence_address = {H. Chen; National Key Laboratory of Fundamental Science on Synthetic Vision, College of Computer Science, Sichuan University, Chengdu, China; email: huchen@scu.edu.cn},
	publisher = {Public Library of Science},
	issn = {19326203},
	coden = {POLNC},
	pmid = {32282810},
	language = {English},
	abbrev_source_title = {PLoS ONE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Yang2020378,
	author = {Yang, Hongwei and Wu, Di and Yuan, Changan and Qin, Xiao and Wu, Hongjie and Zhao, Xingming and Zhao, Zhongqiu},
	title = {Plant Leaf Recognition Network Based on Feature Learning and Metric Learning},
	year = {2020},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12463 LNCS},
	pages = {378 – 386},
	doi = {10.1007/978-3-030-60799-9_33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093872561&doi=10.1007%2f978-3-030-60799-9_33&partnerID=40&md5=d505ebfb984b0b9880c00e4553a78aa8},
	affiliations = {Institute of Machine Learning and Systems Biology, School of Electronics and Information Engineering, Tongji University, Shanghai, China; Guangxi Academy of Science, Nanning, 530025, China; School of Computer and Information Engineering, Nanning Normal University, Nanning, 530299, China; School of Computer Science and Technology, Soochow University, Suzhou, 215006, China; School of Electronic and Information Engineering, Suzhou University of Science and Technology, Suzhou, 215009, China; Institute of Science and Technology for Brain Inspired Intelligence (ISTBI), Fudan University, Shanghai, 200433, China; Key Laboratory of Computational Neuro-Science and Brain-Inspired Intelligence, Ministry of Education, Shanghai, China; College of Computer Science and Information Engineering, Hefei University of Technology, Hefei, 230009, China},
	abstract = {Plant image recognition is an important thing for protecting plants, protecting the environment, and protecting nature. Recently, most models in the field of plant leaf recognition make classification after extracting global features. In this paper, we propose a plant leaf recognition model based on metric learning. Metric learning calculates the similarity of the extracted feature vectors to obtain the distance between different sample features, so as to determine whether similar pictures belong to the same category, and then achieve the classification effect. In this study, feature triplet are used for metric learning, and the loss function we used is triplet-loss. © 2020, Springer Nature Switzerland AG.},
	author_keywords = {Global feature; Metric-learning; Multi-task; ResNet; Triplet-loss},
	keywords = {Computation theory; Image recognition; Machine learning; Feature learning; Feature vectors; Global feature; Loss functions; Metric learning; Network-based; Plant leaf; Sample features; Intelligent computing},
	correspondence_address = {H. Yang; Institute of Machine Learning and Systems Biology, School of Electronics and Information Engineering, Tongji University, Shanghai, China; email: 1830805@tongji.edu.cn},
	editor = {Huang D.-S. and Bevilacqua V. and Hussain A.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {03029743},
	isbn = {978-303060798-2},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 16th International Conference on Intelligent Computing, ICIC 2020; Conference date: 2 October 2020 through 5 October 2020; Conference code: 250169}
}

@ARTICLE{Thanikkal202013103,
	author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M.T.},
	title = {Unique shape descriptor algorithm for medicinal plant identification (SDAMPI) with abridged image database},
	year = {2020},
	journal = {IEEE Sensors Journal},
	volume = {20},
	number = {21},
	pages = {13103 – 13109},
	doi = {10.1109/JSEN.2020.3002909},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092537689&doi=10.1109%2fJSEN.2020.3002909&partnerID=40&md5=ccade0781c90fce9e5b72405eb28929d},
	affiliations = {Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida, 201313, India; Department of Electronics and Communication Engineering, Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida, 201313, India; Department of Botany, St. Thomas College Thrissur, Thrissur, 680001, India},
	abstract = {In image processing, leaf shape recognition requires a huge database of similar images. Normal leaf image database construction requires more time and space. On the other hand, mobile offline applications are not able to contain huge image database with high pixel ratio. To solve this problem, mainly in medicinal plant identification, small leaf descriptors are necessary to completely provide the required plant information. Presently, the Botanists use shape reference table to recognize the following shapes of the leaf: ovate, cordate, elliptical, oblong, lanceolate and linear. As the shape numbers are invariant under scale, rotation and translation, which is highly desirable property for object recognition and chain code techniques preserve data by allowing large data reduction. Hence, in the proposed Shape Descriptor Algorithm for Medicinal Plant Identification (SDAMPI), we developed a descriptor to resolve the pixel selection issue of Freeman chain code and generate a unique leaf shape number. This leaf shape digital descriptor will act as a reference table for medicinal plant leaf shape identification. The performance of the proposed descriptor is evaluated through Jaccard similarity index graph and Levenshtein distance (LD) graph. From the results, it is confirming that, SDAMPI descriptor can detect Medicinal plant leaf shapes more accurately than existing methods. © 2001-2012 IEEE.},
	author_keywords = {Chain code; image processing; plant leaves; shape features; shape number},
	keywords = {Database systems; Object recognition; Pixels; Freeman chain code; Levenshtein distance; Medicinal plants; Off-line applications; Pixel selection; Plant information; Shape descriptors; Similarity indices; Plants (botany)},
	correspondence_address = {A.K. Dubey; Department of Electronics and Communication Engineering, Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida, 201313, India; email: dubey1ak@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {1530437X},
	language = {English},
	abbrev_source_title = {IEEE Sensors J.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@ARTICLE{Visu20205355,
	author = {Visu, P. and Sivakumar, Nagarajan and Kumaresan, P. and Yokesh Babu, Sundaresan and Ramesh, P.S.},
	title = {Removing leaf petioles and auto locating apex-base points using straight line interpolation and bisection},
	year = {2020},
	journal = {Multimedia Tools and Applications},
	volume = {79},
	number = {7-8},
	pages = {5355 – 5369},
	doi = {10.1007/s11042-018-6579-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053263916&doi=10.1007%2fs11042-018-6579-z&partnerID=40&md5=48d2f204e6a943012cecee5199335146},
	affiliations = {Department of IT, Velammal Engineering College, Affiliated to Anna University, Chennai, India; School of Information Technology & Engineering, Vellore Institute of Technology, Vellore, India; School of Computing Science & Engineering, Vellore Institute of Technology, Vellore, India; CIIS, Vellore Institute of Technology, Vellore, India},
	abstract = {Plants are one of the long lasting species on Earth and are used for various purposes such as medicine, food and organics. Apex point and base point, coined here, are very important points of leaf facilitating extraction of various leaf shape based features. The main challenge in automatic plant recognition system is automatically identifying apex-base points and removing petiole of the leaf. Here, we propose a novel methodology, using straight line interpolation and bisection, for automatically identifying leaf apex point and base point. The coordinates of apex point and base point are successfully retrieved and the methodology is rotation and scale invariant. Finally, a logistic regression is used for classification. The proposed methodology is tested in several datasets like Leaflia, flavia and the results are satisfactory. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Apex point; Base point; Bisection; Pattern recognition; Petiole; Plant recognition system; Straight line interpolation},
	keywords = {Interpolation; Pattern recognition; Apex point; Base points; Bisection; Line interpolation; Petiole; Plant recognition; Pattern recognition systems},
	correspondence_address = {P. Visu; Department of IT, Velammal Engineering College, Affiliated to Anna University, Chennai, India; email: pandu.visu@gmail.com},
	publisher = {Springer},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Luo20201554,
	author = {Luo, Xianxian and Xu, Songya and Yan, Hong},
	title = {Application of deep belief network in forest type identification using hyperspectral data},
	year = {2020},
	journal = {Advances in Science, Technology and Engineering Systems},
	volume = {5},
	number = {6},
	pages = {1554 – 1559},
	doi = {10.25046/aj0506186},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099712224&doi=10.25046%2faj0506186&partnerID=40&md5=740dd49380f82954a1ef2b3d9366c3cd},
	affiliations = {Faculty of Mathematics and Computer Science, Quanzhou Normal University, Quanzhou, 362000, China; Fujian Provincial Key Laboratory of Data Intensive Computing, Quanzhou, 362000, China; Faculty of Educational Science, Quanzhou Normal University, Quanzhou, 362000, China; Fujian Forest Inventory and Planning Institute, Fuzhou, 350000, China},
	abstract = {Forest mapping by remote sensing is a hot topics in forestry. At present, many researchers focus on the research of forest type classification or tree species identification using different machine learning methods and try to improve the accuracy of classification of satellite image. However, forest type classification using deep belief network (DBN) is still limited in previous literatures. Our research focuses on forest mapping in the western part of Dehua county in southern China. Most important objective was to assess the feasibility of forest mapping from hyperspectral data using deep learning. The HJ-1A hyperspectral data was adopted in this paper. We applied deep belief network and got a thematic map of four forest types, such as coniferous forest, broad-leaved forest, mixed forest and non-forest. Our finding shows that optimal network depth of DBN model is 3 and best node in each layer is 256 in our experiment. Overall accuracy is 85.8% and kappa coefficient is 0.785 with best-fit parameters in DBN model, while for SVM is 73% and 0.6447 respectively. DBN obtain better performance compared with support vector machine. Furthermore, network depth and number of nodes in each hidden layer in DBN model has a significant effect on overall accuracy and Kappa coefficient. In general, DBN is promised to be dominant method of forest mapping by hyperspectral data. © 2020 ASTES Publishers. All rights reserved.},
	author_keywords = {Deep Belief Network; Deep Learning; Forest Mapping; Hyperspectral Data},
	correspondence_address = {S. Xu; Faculty of Educational Science, Quanzhou Normal University, Quanzhou, 362000, China; email: 29974817@qq.com},
	publisher = {ASTES Publishers},
	issn = {24156698},
	language = {English},
	abbrev_source_title = {Adv.  Sci., Technol.  Eng.  Syst.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Latif20208103,
	author = {Latif, Ghazanfar and Alghazo, Jaafar and Maheswar, R. and Vijayakumar, V. and Butt, Mohsin},
	title = {Deep learning based intelligence cognitive vision drone for automatic plant diseases identification and spraying},
	year = {2020},
	journal = {Journal of Intelligent and Fuzzy Systems},
	volume = {39},
	number = {6},
	pages = {8103 – 8114},
	doi = {10.3233/JIFS-189132},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097598042&doi=10.3233%2fJIFS-189132&partnerID=40&md5=d10e29a4a2728a6e8528253023db9e55},
	affiliations = {College of Computer Engineering and Sciences, Prince Mohammad Bin Fahd University, Saudi Arabia; Dean - Research (Assistant) School of EEE, VIT Bhopal University, India; Cloud Computing Consultant, MIT Square, United Kingdom; College of Applied and Supporting Studies, King Fahd University of Petroleum and Minerals, Saudi Arabia},
	abstract = {The agriculture industry is of great importance in many countries and plays a considerable role in the national budget. Also, there is an increased interest in plantation and its effect on the environment. With vast areas suitable for farming, countries are always encouraging farmers through various programs to increase national farming production. However, the vast areas and large farms make it difficult for farmers and workers to continually monitor these broad areas to protect the plants from diseases and various weather conditions. A new concept dubbed Precision Farming has recently surfaced in which the latest technologies play an integral role in the farming process. In this paper, we propose a SMART Drone system equipped with high precision cameras, high computing power with proposed image processing methodologies, and connectivity for precision farming. The SMART system will automatically monitor vast farming areas with precision, identify infected plants, decide on the chemical and exact amount to spray. Besides, the system is connected to the cloud server for sending the images so that the cloud system can generate reports, including prediction on crop yield. The system is equipped with a user-friendly Human Computer Interface (HCI) for communication with the farm base. This multidrone system can process vast areas of farmland daily. The Image processing technique proposed in this paper is a modified ResNet architecture. The system is compared with deep CNN architecture and other machine learning based systems. The ResNet architecture achieves the highest average accuracy of 99.78% on a dataset consisting of 70,295 leaf images for 26 different diseases of 14 plants. The results obtained were compared with the CNN results applied in this paper and other similar techniques in previous literature. The comparisons indicate that the proposed ResNet architecture performs better compared to other similar techniques. © 2020 - IOS Press and the authors. All rights reserved.},
	author_keywords = {Automatic plant identification; automatic spraying; cognitive vision drone; Convolutional Neural Networks (CNN); deep learning; plant diseases; residual networks; smart devices},
	keywords = {Agricultural robots; Agriculture; Budget control; Computer architecture; Drones; Image processing; Agriculture industries; Cognitive vision; Computing power; Human computer interfaces; Image processing technique; Latest technology; National budget; Precision farming; Deep learning},
	correspondence_address = {G. Latif; College of Computer Engineering and Sciences, Prince Mohammad Bin Fahd University, Saudi Arabia; email: glatif@pmu.edu.sa},
	publisher = {IOS Press BV},
	issn = {10641246},
	language = {English},
	abbrev_source_title = {J. Intelligent Fuzzy Syst.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12}
}

@CONFERENCE{Tang2020,
	author = {Tang, Zhixuan},
	title = {Leaf image recognition and classification based on GBDT-probabilistic neural network},
	year = {2020},
	journal = {Journal of Physics: Conference Series},
	volume = {1592},
	number = {1},
	doi = {10.1088/1742-6596/1592/1/012061},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092461673&doi=10.1088%2f1742-6596%2f1592%2f1%2f012061&partnerID=40&md5=afe007fa45f23bbd0a8a46e6578f5bc2},
	affiliations = {School of Science, Wuhan University of Technology, Wuhan, 430070, China},
	abstract = {In this paper, the binary images of 100 kinds of leaves are used for leaf recognition. Firstly, we screen 35 important features and use the grey clustering analysis to establish the quantitative feature system of leaves. Then we use the gradient descent tree algorithm (GBDT) to select core features and use probabilistic neural network (PNN) to recognize and classify leaves, constructing a hybrid GBDT-PNN model. In the end, we obtain the classification results of leaves to evaluate model performance and the influence of core features on the model. The results show that the accuracy rate of GBDT-PNN model using 12 core features is 92.75%. And the accuracy rate with all 35 features is 93.5%. It illustrates that the model has great performance and core features have high influence on the model. By comparing with other commonly used deep learning algorithms and models, it is verified that the GBDT-PNN image recognition and classification model is effective and has high accuracy.  © Published under licence by IOP Publishing Ltd.},
	keywords = {Binary images; Deep learning; Gradient methods; Image classification; Image recognition; Inductively coupled plasma mass spectrometry; Learning algorithms; Trees (mathematics); Classification models; Classification results; Gradient descent; Grey clustering analysis; Important features; Model performance; Probabilistic neural networks; Quantitative features; Neural networks},
	correspondence_address = {Z. Tang; School of Science, Wuhan University of Technology, Wuhan, 430070, China; email: 422970498@qq.com},
	editor = {Dai W.},
	publisher = {IOP Publishing Ltd},
	issn = {17426588},
	language = {English},
	abbrev_source_title = {J. Phys. Conf. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 3rd International Conference on Physics, Mathematics and Statistics, ICPMS 2020; Conference date: 20 May 2020 through 22 May 2020; Conference code: 163185; All Open Access, Gold Open Access}
}

@ARTICLE{Mustafa202011419,
	author = {Mustafa, M.S. and Husin, Z. and Tan, W.K. and Mavi, M.F. and Farook, R.S.M.},
	title = {Development of automated hybrid intelligent system for herbs plant classification and early herbs plant disease detection},
	year = {2020},
	journal = {Neural Computing and Applications},
	volume = {32},
	number = {15},
	pages = {11419 – 11441},
	doi = {10.1007/s00521-019-04634-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075950965&doi=10.1007%2fs00521-019-04634-7&partnerID=40&md5=3545ef48c95046e0f1e37187cea5e771},
	affiliations = {School of Computer and Communication Engineering, Universiti Malaysia Perlis, Arau, Malaysia; Faculty of Engineering Technology, Universiti Malaysia Perlis, Arau, Malaysia},
	abstract = {Plants such as herbs are widely used in the medical and cosmetic industry. Recognizing a species and detecting an early disease of a plant are quite challenging and difficult to implement as an automated device. The manual identification process is a lengthy process and requires a prior understanding about the plant itself, such as shape, odour, and texture. Thus, this research aimed to realize the computerized method to recognize the species and detect early disease of the herbs by referring to these characteristics. This research has been developed a system for recognizing the species and detecting the early disease of the herbs using computer vision and electronic nose, which focus on odour, shape, colour and texture extraction of herb leaves, together with a hybrid intelligent system that are involved fuzzy inference system, naïve Bayes (NB), probabilistic neural network (PNN) and support vector machine (SVM) classifier. These techniques were used to perform a convenient and effective herb species recognition and early disease detection on ten different herb species samples. The species recognition accuracy rate among ten different species using computer vision and electronic nose is archived 97% and 96%, respectively, in SVM, 98% and 98%, respectively, in PNN and both 94% in NB. In the early disease detection, the detection rate among ten different herb’s species using computer vision and electronic nose are 98% and 97%, respectively, in SVM, both 98% in PNN, 95% and 94%, respectively, in NB. Integrated three machine learning approaches have successfully achieved almost 99% for recognition and detection rate. © 2019, Springer-Verlag London Ltd., part of Springer Nature.},
	author_keywords = {Computer vision; Early herb disease detection; Electronic nose; Herb species recognition; Hybrid intelligent system},
	keywords = {Barium compounds; Electronic nose; Fuzzy inference; Fuzzy neural networks; Intelligent systems; Plants (botany); Support vector machines; Textures; Disease detection; Early disease detection; Fuzzy inference systems; Herb species; Hybrid intelligent system; Machine learning approaches; Manual identification; Probabilistic neural networks; Computer vision},
	correspondence_address = {Z. Husin; School of Computer and Communication Engineering, Universiti Malaysia Perlis, Arau, Malaysia; email: zulhusin@unimap.edu.my},
	publisher = {Springer},
	issn = {09410643},
	language = {English},
	abbrev_source_title = {Neural Comput. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32}
}

@ARTICLE{Santos2020139,
	author = {Santos, Luís and Santos, Filipe N. and Oliveira, Paulo Moura and Shinde, Pranjali},
	title = {Deep Learning Applications in Agriculture: A Short Review},
	year = {2020},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {1092 AISC},
	pages = {139 – 151},
	doi = {10.1007/978-3-030-35990-4_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082125818&doi=10.1007%2f978-3-030-35990-4_12&partnerID=40&md5=5e45d9287f29513ad499ad94a7f55927},
	affiliations = {INESC TEC - INESC Technology and Science, Porto, Portugal; UTAD - University of Trás-os-Montes e Alto Douro, Vila Real, Portugal},
	abstract = {Deep learning (DL) incorporates a modern technique for image processing and big data analysis with large potential. Deep learning is a recent tool in the agricultural domain, being already successfully applied to other domains. This article performs a survey of different deep learning techniques applied to various agricultural problems, such as disease detection/identification, fruit/plants classification and fruit counting among other domains. The paper analyses the specific employed models, the source of the data, the performance of each study, the employed hardware and the possibility of real-time application to study eventual integration with autonomous robotic platforms. The conclusions indicate that deep learning provides high accuracy results, surpassing, with occasional exceptions, alternative traditional image processing techniques in terms of accuracy. © 2020, Springer Nature Switzerland AG.},
	author_keywords = {Agriculture; Deep learning; Image processing; Survey},
	keywords = {Agriculture; Data handling; Fruits; Image processing; Robotics; Surveying; Surveys; Autonomous robotics; Disease detection; High-accuracy; Image processing technique; Learning techniques; Modern techniques; Paper analysis; Real-time application; Deep learning},
	correspondence_address = {L. Santos; INESC TEC - INESC Technology and Science, Porto, Portugal; email: luis.c.santos@inesctec.pt},
	editor = {Silva M.F. and Luís Lima J. and Reis L.P. and Sanfeliu A. and Tardioli D.},
	publisher = {Springer},
	issn = {21945357},
	isbn = {978-303035989-8},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 44; Conference name: 4th Iberian Robotics Conference, ROBOT 2019; Conference date: 20 November 2019 through 22 November 2019; Conference code: 236649}
}

@CONFERENCE{Pushpa2020,
	author = {Pushpa, B.R. and Megha, N. and Amaljith, K.B.},
	title = {Comparision and classification of medicinal plant leaf based on texture feature},
	year = {2020},
	journal = {2020 International Conference for Emerging Technology, INCET 2020},
	doi = {10.1109/INCET49848.2020.9154155},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090578017&doi=10.1109%2fINCET49848.2020.9154155&partnerID=40&md5=52cf3168d59896bce795c542fa656940},
	affiliations = {Mysuru Campus Amrita Vishwa Vidyapeetham, Department of Computer Science, Amrita School of Arts and Sciences, India},
	abstract = {Automated classification system for medicinal plants is very rigorous and it's challenging problem. India is a country where we can find numerous plants species and each plant contains its own medicinal value. Manual identification and classification process requires prior knowledge and it very difficult for human beings to remember all plant species name and its uses. Conservation of these medicinal plants is important as it is will be beneficial to many sectors such as in the field of medicine, botanic researches and plant taxonomy study. Existing systems are not capable of modelling Indian medicinal plants species. The proposed system helps in classifying the medicinal plants by utilizing the texture features that majorly contributes in leaf recognition. The main phases in proposed methodology are image enhancement, feature extraction and classification. The leaf images are captured by using smart phones further by digital image processing techniques the features are extracted and comparison is performed among them. Finally K-nearest neighbor (KNN) classifier is implemented to design an automatic classifier.  © 2020 IEEE.},
	author_keywords = {GLCM; GLDM; KNN; WAVELET},
	keywords = {Image enhancement; mHealth; Nearest neighbor search; Smartphones; Textures; Automated classification systems; Automatic classifiers; Classification process; Digital image processing technique; Feature extraction and classification; K-nearest neighbor classifiers (KNN); Manual identification; Medicinal plants; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172816221-8},
	language = {English},
	abbrev_source_title = {Int. Conf. for Emerg. Technol., INCET},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 2020 International Conference for Emerging Technology, INCET 2020; Conference date: 5 June 2020 through 7 June 2020; Conference code: 162255}
}

@ARTICLE{Chakravarthy20209492,
	author = {Chakravarthy, S Midhun and Chokkalingam, S.P. and Cynthia, J Sybi},
	title = {Plant leaf recognition using shape features and KNN classifiers},
	year = {2020},
	journal = {Journal of Green Engineering},
	volume = {10},
	number = {10},
	pages = {9492 – 9505},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096590245&partnerID=40&md5=2640c203814c64e502d1bbd477ef1af6},
	affiliations = {Department of computer science and Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, India},
	abstract = {This paper proposes an ideal methodology of differentiating and identifying the leaves by using its shape features. The Shape features of the leaf is modelled using Gray level co-occurrence matrix (GLCM). Here, a pre-processing step is done before the feature extraction and feature recognization using K-nearest neighboring algorithm (KNN) is implemented to correct differing translation, rotation and scaling factor, as these features are usually sensitive to the alignment and size of the leaf image. Then the Efficacy of our methods is utilized by using k-NN neural classifiers. This features have been applied exclusively just as in mix to examine how recognition correctness can be improved. Experimental results exhibit that the implemented approach is compelling in perceiving leaves with fluctuating surface, shape, size and directions adequately. © 2020 Alpha Publishers. All rights reserved.},
	author_keywords = {GLCM; K-NN; Leaf features; Neural classifiers; Plant leaf},
	publisher = {Alpha Publishers},
	issn = {19044720},
	language = {English},
	abbrev_source_title = {J. Green Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Pankaja2020597,
	author = {Pankaja, K. and Suma, V.},
	title = {Plant Leaf Recognition and Classification Based on the Whale Optimization Algorithm (WOA) and Random Forest (RF)},
	year = {2020},
	journal = {Journal of The Institution of Engineers (India): Series B},
	volume = {101},
	number = {5},
	pages = {597 – 607},
	doi = {10.1007/s40031-020-00470-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088588211&doi=10.1007%2fs40031-020-00470-9&partnerID=40&md5=25cc38ce068066755336c538f9cbbac7},
	affiliations = {Computer Science and Engineering, Cambridge Institute of Technology, VTU, Bengaluru, India; Information Science and Engineering, Dayananda Sagar College of Engineering, VTU, Bengaluru, India},
	abstract = {Image processing has a vital role to play in current day scenario due to its wide band of advantages and applications such as healthcare, military, scientific and business applications. As such, plant species identification through leaf image is one of the computer vision challenges. In this paper, a method for recognizing and classifying the plant leaves by hybridizing whale optimization algorithm (WOA) and random forest (RF) is proposed. This work is carried out on Swedish and Flavia leaf datasets. Initially, pre-processing is applied to remove noises in data or to enhance its quality, prior to feature extraction. WOA is used to overcome dimensionality problem. Further, the classifier of RF is used to identify the leaf. The proposed method shows a high accuracy of 97.58% with a reduced execution time when compared with other approaches. This investigation ensures better plant leaf classification and recognition for the medical purposes. © 2020, The Institution of Engineers (India).},
	author_keywords = {Feature extraction; Feature selection; Plant leaf classification; Random forest; Whale optimization algorithm},
	keywords = {Decision trees; Military applications; Military photography; Optimization; Plants (botany); Random forests; Business applications; High-accuracy; Optimization algorithms; Plant leaf classifications; Plant leaves; Plant species identification; Pre-processing; REmove noise; Image processing},
	correspondence_address = {K. Pankaja; Computer Science and Engineering, Cambridge Institute of Technology, VTU, Bengaluru, India; email: pankaja.osr@gmail.com},
	publisher = {Springer},
	issn = {22502106},
	language = {English},
	abbrev_source_title = {J. Inst. Eng. Ser. B},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17}
}

@ARTICLE{Goyal2020,
	author = {Goyal, Neha and Kumar, Nitin and Kapil},
	title = {On solving leaf classification using linear regression},
	year = {2020},
	journal = {Multimedia Tools and Applications},
	doi = {10.1007/s11042-020-09899-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091733552&doi=10.1007%2fs11042-020-09899-y&partnerID=40&md5=399a36ab17af676ec81cf03601f2d4e2},
	affiliations = {NIT, Kurukshetra, Uttarakhand, India},
	abstract = {Plant’s conservation is getting close attention nowadays. It requires awareness about ecology among masses. Plant species identification has been proved as a primary step in literature for biodiversity conservation. It is a sequential process from leaf images as input followed by image enhancement algorithms, and feature extraction phase to classification. The complete process of identifying a leaf image requires substantial time. The article focuses on introducing a simpler and computationally inexpensive framework with a performance at par or better as compared to the existing framework. The article covers several findings and results while transforming the proposed framework for plant identification to a parameter specific optimized framework. The findings include optimizing the leaf image dimension, the impact of RGB to grayscale conversion method, and comparative analysis of the proposed framework for classification from images with other frameworks that first extract specific features and then classify. It also represents the whole framework as a regression problem. Further, improvement is incorporated by integrating the benefits of kernel trick in linear regression. Our finding confirms that the framework not only recognizing the leaf images with comparable accuracy but also reduces the computational time significantly to identify leaf images as compared to other frameworks. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Color to gray-scale conversion; Image down-sampling; Image projection; Kernel function; Linear regression},
	keywords = {Biodiversity; Classification (of information); Conservation; Biodiversity conservation; Comparative analysis; Conversion methods; Image enhancement algorithm; Leaf classification; Plant identification; Plant species identification; Regression problem; Image enhancement},
	correspondence_address = {N. Goyal; NIT, Kurukshetra, Uttarakhand, India; email: neha.goyal2309@gmail.com},
	publisher = {Springer},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Sujee20201433,
	author = {Sujee, R. and Thangavel, Senthil Kumar},
	title = {Plant Leaf Recognition Using Machine Learning Techniques},
	year = {2020},
	journal = {New Trends in Computational Vision and Bio-Inspired Computing - Selected Works Presented at the ICCVBIC 2018},
	pages = {1433 – 1444},
	doi = {10.1007/978-3-030-41862-5_147},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125660240&doi=10.1007%2f978-3-030-41862-5_147&partnerID=40&md5=bb24600493c1a8edc338386594ce3b28},
	affiliations = {Department of Computer Science and Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India},
	correspondence_address = {R. Sujee; Department of Computer Science and Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; email: r_sujee@cb.amrita.edu},
	editor = {Smys S. and Iliyasu A.M. and Bestak R. and Shi F.},
	publisher = {Springer Nature},
	isbn = {978-303041861-8},
	language = {English},
	abbrev_source_title = {New Trends Comput. Vis. Bio-Inspired Comput. - Sel. Works Present. ICCVBIC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2018 International Conference on Computational Vision and Bio-Inspired Computing, ICCVBIC 2018; Conference date: 29 November 2018 through 30 November 2018; Conference code: 177223}
}

@ARTICLE{Pankaja202023,
	author = {Pankaja, K. and Suma, V.},
	title = {Mango Leaves Recognition Using Deep Belief Network with Moth-Flame Optimization and Multi-feature Fusion},
	year = {2020},
	journal = {Smart Innovation, Systems and Technologies},
	volume = {159},
	pages = {23 – 31},
	doi = {10.1007/978-981-13-9282-5_3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075748597&doi=10.1007%2f978-981-13-9282-5_3&partnerID=40&md5=7022958b9000f9d62d4d07c8d3b55e18},
	affiliations = {Cambridge Institute of Technology, Computer Science and Engineering, VTU, Bengaluru, India; Dayanand Sagar College of Engineering, Information Science and Engineering, VTU, Bengaluru, India},
	abstract = {In automatic plant classification, plant identification based on digital leaf images is a challenging task. This paper proposes a Moth-Flame Optimization (MFO)-based deep belief network (DBN) method for plant leaf recognition. Initially, a combination of texture and shape features is applied for extracting features from preprocessed image. Further, for leaf classification, the MFO optimizes the DBN parameters to minimize error and is used as classifier. The classifier has been applied to five different sets of mango leaf images and achieved an accuracy of 98.5%. The experimental result indicates that it is feasible to automatically classify plants by using multi-feature extraction of plant leaf images in combination with MFO-based DBN. © 2020, Springer Nature Singapore Pte Ltd.},
	author_keywords = {DBNs; Feature extraction; MFO; Plant leaf; Texture feature},
	keywords = {Extraction; Feature extraction; Fruits; Intelligent computing; Plants (botany); Textures; DBNs; Deep belief network (DBN); Deep belief networks; Multi-feature fusion; Plant classification; Plant identification; Plant leaf; Texture features; Image processing},
	correspondence_address = {K. Pankaja; Cambridge Institute of Technology, Computer Science and Engineering, VTU, Bengaluru, India; email: pankaja.osr@gmail.com},
	editor = {Satapathy S.C. and Bhateja V. and Mohanty J.R. and Udgata S.K.},
	publisher = {Springer},
	issn = {21903018},
	isbn = {978-981139281-8},
	language = {English},
	abbrev_source_title = {Smart Innov. Syst. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 3rd International Conference on Smart Computing and Informatics, SCI 2018; Conference date: 21 December 2019 through 22 December 2019; Conference code: 232679}
}

@ARTICLE{Fan202010212,
	author = {Fan, Boyuan and Yang, Rutong and Wu, Weihong and Li, Linfang and Wang, Shu'an and Li, Ya and Wang, Peng and Gao, Lulu and Fu, Li and Zhu, Jiangwei and Karimi-Maleh, Hassan and Zheng, Yuhong},
	title = {Development of an electrochemical technology for ten Clematis spp varieties identification},
	year = {2020},
	journal = {International Journal of Electrochemical Science},
	volume = {15},
	pages = {10212 – 10220},
	doi = {10.20964/2020.10.79},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092033844&doi=10.20964%2f2020.10.79&partnerID=40&md5=de4e5243e936cae72e185c26da9903d1},
	affiliations = {College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; Institute of Botany, Jiangsu Province and Chinese Academy of Sciences (Nanjing Botanical Garden Mem. Sun Yat-Sen), Nanjing, 210014, China; Co-Innovation Center for Sustainable Forestry in Southern China, Nanjing Forestry University, Nanjing, 210037, China; School of Resources and Enviroment, University of Electronic Science and Technology of China, P.O. Box 611731, Xiyuan Ave, Chengdu, China; Department of Chemical Sciences, University of Johannesburg, P.O. Box 17011, Doornfontein Campus, Johannesburg, 2028, South Africa},
	abstract = {The identification of ornamental plants has always been a challenge. Due to the complexity of breeding development, ornamental plants often have very complex genetic relationships with native plants. In this work, we proposed the use of glassy carbon electrodes to perform voltammetric scanning of extracts of plant leaves. Ten specific varieties of Clematis were selected as research targets. We recorded the voltammograms of these Clematis varieties under different conditions and found that different extraction solvents and buffer solutions can present different profiles. Integrating these voltammograms can be used to quickly identify varieties of Clematis. In addition to scatter plots and 2D density maps we previously proposed, we propose a new pattern recognition method in this work. © 2020 The Authors.},
	author_keywords = {Clematis spp; Electrochemical sensor; Pattern recognition; Plant identification; Voltammograms},
	correspondence_address = {W. Wu; College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; email: whwu@hdu.edu.cn; L. Fu; College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; email: fuli@hdu.edu.cn},
	publisher = {Electrochemical Science Group},
	issn = {14523981},
	language = {English},
	abbrev_source_title = {Int.J.Electrochem.Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Su2020,
	author = {Su, Wen-Hao and Slaughter, David C. and Fennimore, Steven A.},
	title = {Non-destructive evaluation of photostability of crop signaling compounds and dose effects on celery vigor for precision plant identification using computer vision},
	year = {2020},
	journal = {Computers and Electronics in Agriculture},
	volume = {168},
	doi = {10.1016/j.compag.2019.105155},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076701447&doi=10.1016%2fj.compag.2019.105155&partnerID=40&md5=b04e10846db7b99d6b334d8b9f267a09},
	affiliations = {Department of Biological and Agricultural Engineering, University of California, Davis (UCD), One Shields Avenue, Davis, 95616-5270, CA, United States; Department of Plant Sciences, University of California, Davis (UCD), Salinas, CA, United States},
	abstract = {Vegetable crops grown in the field are particularly vulnerable to competition from weeds at the seedling stage. To avoid yield loss, weeds must be removed early in the crop cycle. Shortages of farm laborers for hand weeding and lack of effective vegetable herbicides is stimulating the development of intelligent weeders. One challenge for development of effective intelligent weeders is the precision differentiation of vegetable crops and weeds. Crop signaling is a technique that enables rapid and accurate identification of target crops. In this study, an appropriate dose of fluorescent compounds allowed the vegetable crop locations in the field to be reliably detectable by smart machines. The study examined protocols for crop root treatment, computer vision system development, crop signal detection and biomass evaluation. Rhodamine B (Rh-B) is a fluorescent compound with unique optical properties. Different doses of Rh-B were applied to the celery roots for various durations prior to transplantation to assess Rh-B transport in the plant, its photostability and potential impact on seedling growth in the natural outdoor environment of a farm. Systemic Rh-B absorbed via the roots moved throughout the celery plant in 24 h. Compared with 60 ppm solution of Rh-B, higher doses of Rh-B including 90, 180, and 270 ppm resulted in greater absorption by the plant, but such three concentrations injured the plant. The biomass test results showed that the treatment of celery roots for two days with a 60 ppm solution of Rh-B was safe for celery plants. This Rh-B dosage had good photostability in celery seedlings for about 5 weeks after transplanting. An effective dose of systemic Rh-B allowed for the rapid identification of celery plants at early growth stages, thereby facilitating the automatic differentiation of weeds and crops by a robotic machine vision system. © 2019 Elsevier B.V.},
	author_keywords = {Celery biomass; Crop signaling; Photobleaching; Rhodamine B; Robot-plant interaction},
	keywords = {Apium graveolens var. dulce; Biomass; Computer vision; Crops; Ecology; Fluorescence; Nondestructive examination; Optical properties; Photobleaching; Plants (botany); Vegetables; Automatic differentiations; Computer vision system; Fluorescent compounds; Machine vision systems; Non destructive evaluation; Plant interactions; Rhodamine B; Weeks after transplanting; absorption; biomass; bleaching; computer vision; concentration (composition); dose-response relationship; dye; growth rate; herb; identification method; injury; robotics; seedling; Rhodium compounds},
	correspondence_address = {W.-H. Su; UC Davis, Davis, One Shields Avenue, 95616, United States; email: whssu@ucdavis.edu},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; All Open Access, Bronze Open Access}
}

@ARTICLE{Hao2020,
	author = {Hao, Xia and Jia, Jingdun and Mateen Khattak, Abdul and Zhang, Li and Guo, Xuchao and Gao, Wanlin and Wang, Minjuan},
	title = {Growing period classification of Gynura bicolor DC using GL-CNN},
	year = {2020},
	journal = {Computers and Electronics in Agriculture},
	volume = {174},
	doi = {10.1016/j.compag.2020.105497},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085562911&doi=10.1016%2fj.compag.2020.105497&partnerID=40&md5=2d93d8a28cdcc18c7f0ff498804c1e77},
	affiliations = {Key Laboratory of Agricultural Informatization Standardization, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Department of Horticulture, The University of Agriculture, Peshawar, 25120, Pakistan; College of Information Science and Engineering, Shandong Agriculture and Engineering University, Jinan, 251100, China},
	abstract = {Determining the optimal growth stage of leafy vegetables suitable for harvesting is of great importance. Despite the availability of many intelligent programs designed for plant classification, identifying the growth stage of a vegetable from its leaf features remains a major challenge. Gynura bicolor DC (G. bicolor) is an important vegetable, and its leaves are harvested for culinary use at a specific growth stage. The produce quality of leaves is compromised when they are harvested at improper stage. A classification model named GL-CNN was proposed based on convolutional neural networks to address this issue. The proposed model merges the features using a network fusion strategy to expand the feature representation on the basis of the intact leaf and leaf patch image sets. The networks were validated using a new dataset of G. bicolor planted and collected by ourselves. “Early fusion” and “late fusion” networks were designed and compared with GL-CNN to verify the rationality of network fusion location. The test accuracy of GL-CNN reaches 95.63%, which is the best in the classification task. © 2020 Elsevier B.V.},
	author_keywords = {Classification; Deep learning; Fusion strategy; Multi-scale information},
	keywords = {Gynura bicolor; Harvesting; Plants (botany); Vegetables; Classification models; Classification tasks; Feature representation; Growing period; Intelligent programs; Leafy vegetables; Optimal growth; Plant classification; accuracy assessment; artificial neural network; harvesting; image analysis; leafy vegetable; numerical model; Convolutional neural networks},
	correspondence_address = {W. Gao; Key Laboratory of Agricultural Informatization Standardization, Ministry of Agriculture and Rural Affairs, China Agricultural University, Beijing, 10083, China; email: wanlin_cau@163.com},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18}
}

@CONFERENCE{Pantano2020298,
	author = {Pantano, Matteo and Kamps, Tobias and Pizzocaro, Solomon and Pantano, Giorgio and Corno, Matteo and Savaresi, Sergio},
	title = {Methodology for Plant Specific Cultivation through a Plant Identification pipeline},
	year = {2020},
	journal = {2020 IEEE International Workshop on Metrology for Agriculture and Forestry, MetroAgriFor 2020 - Proceedings},
	pages = {298 – 302},
	doi = {10.1109/MetroAgriFor50201.2020.9277567},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099029424&doi=10.1109%2fMetroAgriFor50201.2020.9277567&partnerID=40&md5=648ba4a649c6aea77f682d32b7331b58},
	affiliations = {Technology Siemens Ag, Munich, Germany; Politecnico di Milano, Move Research Team, Milan, Italy; Azienda Agricola Giorgio Pantano, Candiana, Italy},
	abstract = {Agriculture needs to optimization for satisfying the rising demands of food due to world population growth. An approach to this problem is digitization of agriculture through IT tools by creating digital twins. However, in the digital twin creation the uniqueness of the plant is lost, therefore, plant based agricultural cultivation cannot be performed. Hence, this paper proposes a methodology to assign an identification marker to plants in a crop using an image analysis pipeline. To show the effectiveness of the algorithm the proposed method is evaluated on the Rovitis robotic platform and compared with the crop ontology. The outcome of this work can be used in robotic agricultural platforms to address plants singularly thus optimizing their cultivation.  © 2020 IEEE.},
	author_keywords = {autonomous robotics; deep learning; digital farming; digital twin; plant specific cultivation; precision viticulture},
	keywords = {Agricultural robots; Crops; Digital twin; Forestry; Pipelines; Population statistics; Robotics; Timber; Agricultural cultivation; Plant identification; Robotic platforms; World population; Cultivation},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818783-9},
	language = {English},
	abbrev_source_title = {IEEE Int. Workshop Metrol. Agric. For., MetroAgriFor - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd IEEE International Workshop on Metrology for Agriculture and Forestry, MetroAgriFor 2020; Conference date: 4 November 2020 through 6 November 2020; Conference code: 165730}
}

@ARTICLE{Espejo-Garcia2020,
	author = {Espejo-Garcia, Borja and Mylonas, Nikos and Athanasakos, Loukas and Fountas, Spyros and Vasilakoglou, Ioannis},
	title = {Towards weeds identification assistance through transfer learning},
	year = {2020},
	journal = {Computers and Electronics in Agriculture},
	volume = {171},
	doi = {10.1016/j.compag.2020.105306},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081018738&doi=10.1016%2fj.compag.2020.105306&partnerID=40&md5=1e0e7ed422c179b66f10134706ffc5cd},
	affiliations = {Agricultural University of Athens, Athens, Greece; Institute of Thessaly, Thessaly, Greece},
	abstract = {Reducing the use of pesticides through selective spraying is an important component towards a more sustainable computer-assisted agriculture. Weed identification at early growth stage contributes to reduced herbicide rates. However, while computer vision alongside deep learning have overcome the performance of approaches that use hand-crafted features, there are still some open challenges in the development of a reliable automatic plant identification system. These type of systems have to take into account different sources of variability, such as growth stages and soil conditions, with the added constraint of the limited size of usual datasets. This study proposes a novel crop/weed identification system that relies on a combination of fine-tuning pre-trained convolutional networks (Xception, Inception-Resnet, VGNets, Mobilenet and Densenet) with the “traditional” machine learning classifiers (Support Vector Machines, XGBoost and Logistic Regression) trained with the previously deep extracted features. The aim of this approach was to avoid overfitting and to obtain a robust and consistent performance. To evaluate this approach, an open access dataset of two crop [tomato (Solanum lycopersicum L.) and cotton (Gossypium hirsutum L.)] and two weed species [black nightshade (Solanum nigrum L.) and velvetleaf (Abutilon theophrasti Medik.)] was generated. The pictures were taken by different production sites across Greece under natural variable light conditions from RGB cameras. The results revealed that a combination of fine-tuned Densenet and Support Vector Machine achieved a micro F1 score of 99.29% with a very low performance difference between train and test sets. Other evaluated approaches also obtained repeatedly more than 95% F1 score. Additionally, our results analysis provides some heuristics for designing transfer-learning based systems to avoid overfitting without decreasing performance. © 2020 Elsevier B.V.},
	author_keywords = {Deep learning; Open data; Precision agriculture; Transfer learning; Weed identification},
	keywords = {Greece; Abutilon theophrasti; Gossypium hirsutum; Lycopersicon esculentum; Solanum nigrum; Automatic identification; Cotton; Crops; Deep learning; Learning systems; Logistic regression; Open Data; Precision agriculture; Support vector machines; Support vector regression; Computer assisted; Consistent performance; Convolutional networks; Cotton (Gossypium hirsutum L.); Plant identification systems; Solanum lycopersicum; Sources of variability; Weed identification; computer simulation; computer vision; identification method; machine learning; performance assessment; pesticide application; precision agriculture; weed; Transfer learning},
	correspondence_address = {B. Espejo-Garcia; Agricultural University of Athens, Athens, Greece; email: borjaeg@aua.gr},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 116}
}

@ARTICLE{Yusof20201225,
	author = {Yusof, Rubiyah and Ahmad, Azlin and Khairuddin, Anis Salwa Mohd and Khairuddin, Uswah and Azmi, Nik Mohamad Aizuddin Nik and Rosli, Nenny Ruthfalydia},
	title = {Transfer Learning Approach in Automatic Tropical Wood Recognition System},
	year = {2020},
	journal = {Mechanisms and Machine Science},
	volume = {75},
	pages = {1225 – 1233},
	doi = {10.1007/978-3-030-27053-7_104},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075582125&doi=10.1007%2f978-3-030-27053-7_104&partnerID=40&md5=be5fab97a59eded1717e928043980d03},
	affiliations = {Centre of Artificial Intelligence and Robotic (CAIRO), Malaysia-Japan International Institute of Technology, Universiti Teknologi Malaysia, Kuala Lumpur, 50300, Malaysia; Faculty of Computer and Mathematical Sciences, Advanced Analytics Engineering Centre (AAEC), Universiti Teknologi MARA, Shah Alam, 40450, Selangor, Malaysia; Department of Electrical Engineering, Faculty of Engineering, Universiti Malaya, Kuala Lumpur, Malaysia},
	abstract = {Automatic recognition of tropical wood species is a very challenging task due to the lack of discriminative features among intra wood species and very discriminative features among inter class species. While many conventional pattern recognition algorithms have been implemented and proven to solve wood image classification with 100% accuracy, when using deep learning however, the classification accuracy drops tremendously to only 36.3% due to small number of training samples. Deep learning requires large number of samples in order to work well, unfortunately, wood samples provided by the national forest institute are limited. In this paper, we explore the use of transfer learning in deep neural network for the classification of tropical wood species based on image analysis. Several model of deep learning techniques are tested and results have shown that the classification performance after transfer learning was added reaches 100% accuracy. © Springer Nature Switzerland AG 2020.},
	author_keywords = {Deep learning; Transfer learning; Wood classification; Wood recognition system},
	keywords = {Deep neural networks; Learning systems; Pattern recognition; Transfer learning; Tropics; Wood; Automatic recognition; Classification accuracy; Classification performance; Discriminative features; Learning techniques; Pattern recognition algorithms; Tropical wood species; Wood recognition; Deep learning},
	correspondence_address = {R. Yusof; Centre of Artificial Intelligence and Robotic (CAIRO), Malaysia-Japan International Institute of Technology, Universiti Teknologi Malaysia, Kuala Lumpur, 50300, Malaysia; email: rubiyah.kl@utm.my},
	publisher = {Springer Science and Business Media B.V.},
	issn = {22110984},
	language = {English},
	abbrev_source_title = {Mech. Mach. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Gai202035,
	author = {Gai, Jingyao and Tang, Lie and Steward, Brian L.},
	title = {Automated crop plant detection based on the fusion of color and depth images for robotic weed control},
	year = {2020},
	journal = {Journal of Field Robotics},
	volume = {37},
	number = {1},
	pages = {35 – 52},
	doi = {10.1002/rob.21897},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074394125&doi=10.1002%2frob.21897&partnerID=40&md5=1fb7e765a5035e8039ef4fe6483cc1d4},
	affiliations = {Agricultural and Biosystems Engineering, Iowa State University, Ames, IA, United States},
	abstract = {Robotic weeding enables weed control near or within crop rows automatically, precisely and effectively. A computer-vision system was developed for detecting crop plants at different growth stages for robotic weed control. Fusion of color images and depth images was investigated as a means of enhancing the detection accuracy of crop plants under conditions of high weed population. In-field images of broccoli and lettuce were acquired 3–27 days after transplanting with a Kinect v2 sensor. The image processing pipeline included data preprocessing, vegetation pixel segmentation, plant extraction, feature extraction, feature-based localization refinement, and crop plant classification. For the detection of broccoli and lettuce, the color-depth fusion algorithm produced high true-positive detection rates (91.7% and 90.8%, respectively) and low average false discovery rates (1.1% and 4.0%, respectively). Mean absolute localization errors of the crop plant stems were 26.8 and 7.4 mm for broccoli and lettuce, respectively. The fusion of color and depth was proved beneficial to the segmentation of crop plants from background, which improved the average segmentation success rates from 87.2% (depth-based) and 76.4% (color-based) to 96.6% for broccoli, and from 74.2% (depth-based) and 81.2% (color-based) to 92.4% for lettuce, respectively. The fusion-based algorithm had reduced performance in detecting crop plants at early growth stages. © 2019 Wiley Periodicals, Inc.},
	author_keywords = {computer vision; crop detection; robotic weeding; sensor fusion},
	keywords = {Color; Computer control systems; Computer vision; Data handling; Extraction; Image enhancement; Image fusion; Image segmentation; Robotics; Weed control; Color and depth images; Computer vision system; Different growth stages; False discovery rate; Image processing pipeline; Localization errors; Robotic weeding; Sensor fusion; Crops},
	correspondence_address = {L. Tang; Agricultural and Biosystems Engineering, Iowa State University, Ames, United States; email: lietang@iastate.edu},
	publisher = {John Wiley and Sons Inc.},
	issn = {15564959},
	language = {English},
	abbrev_source_title = {J. Field. Rob.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 40; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Li2020,
	author = {Li, Yang and Chao, Xuewei},
	title = {Ann-based continual classification in agriculture},
	year = {2020},
	journal = {Agriculture (Switzerland)},
	volume = {10},
	number = {5},
	doi = {10.3390/agriculture10050178},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085351302&doi=10.3390%2fagriculture10050178&partnerID=40&md5=5ef08154d290481640208acd1fd77427},
	affiliations = {College of Mechanical and Electrical Engineering, Shihezi University, Xinjiang, 832003, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, 300072, China},
	abstract = {In the area of plant protection and precision farming, timely detection and classification of plant diseases and crop pests play crucial roles in the management and decision-making. Recently, there have been many artificial neural network (ANN) methods used in agricultural classification tasks, which are task specific and require big datasets. These two characteristics are quite different from how humans learn intelligently. Undoubtedly, it would be exciting if the models can accumulate knowledge to handle continual tasks. Towards this goal, we propose an ANN-based continual classification method via memory storage and retrieval, with two clear advantages: Few data and high flexibility. This proposed ANN-based model combines a convolutional neural network (CNN) and generative adversarial network (GAN). Through learning of the similarity between input paired data, the CNN part only requires few raw data to achieve a good performance, suitable for a classification task. The GAN part is used to extract important information from old tasks and generate abstracted images as memory for the future task. Experimental results show that the regular CNN model performs poorly on the continual tasks (pest and plant classification), due to the forgetting problem. However, our proposed method can distinguish all the categories from new and old tasks with good performance, owing to its ability of accumulating knowledge and alleviating forgetting. There are so many possible applications of this proposed approach in the agricultural field, for instance, the intelligent fruit picking robots, which can recognize and pick different kinds of fruits; the plant protection is achieved by automatic identification of diseases and pests, which can continuously improve the detection range. Thus, this work also provides a reference for other studies towards more intelligent and flexible applications in agriculture. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Deep learning; Memory; Metric; Similarity},
	correspondence_address = {Y. Li; College of Mechanical and Electrical Engineering, Shihezi University, Xinjiang, 832003, China; email: liyang328@shzu.edu.cn},
	publisher = {MDPI AG},
	issn = {20770472},
	language = {English},
	abbrev_source_title = {Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 59; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Tran202034,
	author = {Tran, An C. and Nhu Y, Nguyen Thi and Thoa, Phung Kim and Tran, Nghi C. and Duong-Trung, Nghia},
	title = {Real-Time Recognition of Medicinal Plant Leaves Using Bounding-box Based Models},
	year = {2020},
	journal = {Proceedings - 2020 International Conference on Advanced Computing and Applications, ACOMP 2020},
	pages = {34 – 41},
	doi = {10.1109/ACOMP50827.2020.00013},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102408948&doi=10.1109%2fACOMP50827.2020.00013&partnerID=40&md5=94ed3c2de39eaa3dd6b978d539464281},
	affiliations = {Can Tho University, Can Tho city, Viet Nam; National Central University, Taoyuan City, Taiwan; FPT University, Can Tho city, Viet Nam},
	abstract = {It has recently been demonstrated that leaf recognition systems opened to an exciting challenge for computer vision and machine learning. These systems' actual benefit depends on the recognition capacity of models in unconstrained environments and application scenarios. In this paper, the authors collect a realworld dataset containing 1700 images with 34 types of medicinal plants for the evaluation of object recognition algorithms. The images have been taken in several botanic gardens in much different exposure, distance, and rotation. Then we evaluate several off-The-shelf deep architectures to recognize medicinal plants and take into account the recognition accuracy. An excellent average Fl-score of 0.98 is achieved. Finally, we integrate the best approach into our self-developed mobile application that (i) recognizes the medicinal plants in real-Time and (ii) proposes their healthcare's uses and remedies.  © 2020 IEEE.},
	author_keywords = {Bounding Box; Convolutional Networks; Image Recognition; Medicinal Plants},
	keywords = {Image segmentation; Object recognition; Application scenario; Deep architectures; Mobile applications; Object recognition algorithm; Real time recognition; Recognition accuracy; Recognition capacity; Unconstrained environments; Plants (botany)},
	editor = {Le L.-S. and Marchese M. and Dao B. and Toulouse M. and Tran K.D.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172818167-7},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Adv. Comput. Appl., ACOMP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 14th International Conference on Advanced Computing and Applications, ACOMP 2020; Conference date: 25 November 2020 through 27 November 2020; Conference code: 167321}
}

@ARTICLE{Wang2020495,
	author = {Wang, Zhaobin and Cui, Jing and Zhu, Ying},
	title = {Plant recognition based on Jaccard distance and BOW},
	year = {2020},
	journal = {Multimedia Systems},
	volume = {26},
	number = {5},
	pages = {495 – 508},
	doi = {10.1007/s00530-020-00657-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086726582&doi=10.1007%2fs00530-020-00657-6&partnerID=40&md5=648db93ee7b6957cbe87fd152940e292},
	affiliations = {School of Infomation Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Key Laboratory of Microbial Resources Exploitation and Application of Gansu Province, Institute of Biology, Gansu Academy of Sciences, Lanzhou, China},
	abstract = {Plant recognition is a meaningful research that has attracted many researchers. Due to the variety of plants, it is difficult for the existing identification methods to identify their species efficiently. We proposes a plant recognition method based on Jaccard distance and Bag of words (BOW). Firstly, Jaccard distance is employed to calculate the similarity between the test sample and part of the training samples of all species, C1 species with the highest similarity are selected as candidate species of the test image, which not only reduce the amount of computation but also shorten the time consumption. Secondly, BOW is employed to extract features from texture image and contour image, and support vector machine is used for training and classification. In our method, the texture and contour features of leaf images are extracted by Laws texture measure and Sobel operators respectively. The local and global features of the leaf can be described well. Some representative datasets are used to evaluate the proposed method and obtain high accuracy. Comparison with existing methods proves that the proposed method not only has a high accuracy, but also has robustness in noise environment. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.},
	author_keywords = {Bag of words; Feature extraction; Jaccard distance; Plant recognition},
	keywords = {Support vector machines; Textures; Contour features; Identification method; Jaccard distance; Noise environments; Plant recognition; Texture measures; Time consumption; Training sample; Image texture},
	correspondence_address = {Z. Wang; School of Infomation Science and Engineering, Lanzhou University, Lanzhou, 730000, China; email: zhaobin_wang@hotmail.com},
	publisher = {Springer},
	issn = {09424962},
	coden = {MUSYE},
	language = {English},
	abbrev_source_title = {Multimedia Syst},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Le2020,
	author = {Le, Vi Nguyen Thanh and Ahderom, Selam and Apopei, Beniamin and Alameh, Kamal},
	title = {A novel method for detecting morphologically similar crops and weeds based on the combination of contour masks and filtered Local Binary Pattern operators},
	year = {2020},
	journal = {GigaScience},
	volume = {9},
	number = {3},
	doi = {10.1093/gigascience/giaa017},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081041719&doi=10.1093%2fgigascience%2fgiaa017&partnerID=40&md5=2a4843db0fee098b804769ae8b798491},
	affiliations = {Electronic Science Research Institute, Edith Cowan University, 270 Joondalup Drive, Joondalup, 6027, WA, Australia},
	abstract = {Background: Weeds are a major cause of low agricultural productivity. Some weeds have morphological features similar to crops, making them difficult to discriminate. Results: We propose a novel method using a combination of filtered features extracted by combined Local Binary Pattern operators and features extracted by plant-leaf contour masks to improve the discrimination rate between broadleaf plants. Opening and closing morphological operators were applied to filter noise in plant images. The images at 4 stages of growth were collected using a testbed system. Mask-based local binary pattern features were combined with filtered features and a coefficient k. The classification of crops and weeds was achieved using support vector machine with radial basis function kernel. By investigating optimal parameters, this method reached a classification accuracy of 98.63% with 4 classes in the "bccr-segset" dataset published online in comparison with an accuracy of 91.85% attained by a previously reported method. Conclusions: The proposed method enhances the identification of crops and weeds with similar appearance and demonstrates its capabilities in real-time weed detection. © 2020 The Author(s) 2020.},
	author_keywords = {computer vision; contour masks; feature extraction; local binary patterns; morphological operators; precision agriculture; weed detection},
	keywords = {Crops, Agricultural; Pattern Recognition, Automated; Phenotype; Plant Leaves; Sensitivity and Specificity; Software; Weed Control; accuracy; analytic method; Article; binary classification; classification; crop; crop production; developmental stage; image analysis; intermethod comparison; mathematical analysis; noise reduction; nonhuman; plant growth; plant leaf; plant structures; priority journal; radial basis function; support vector machine; weed; weed control; classifier; computer vision; controlled study; feature extraction; filtered local binary pattern with contour mask method; image processing; imaging algorithm; plant identification; plant structures; anatomy and histology; automated pattern recognition; crop; phenotype; procedures; sensitivity and specificity; software},
	correspondence_address = {V.N.T. Le; Electronic Science Research Institute, Edith Cowan University, Joondalup, 270 Joondalup Drive, 6027, Australia; email: vlenguye@our.ecu.edu.au},
	publisher = {Oxford University Press},
	issn = {2047217X},
	pmid = {32129847},
	language = {English},
	abbrev_source_title = {GigaScience},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Islam2019,
	author = {Islam, Md. Khairul and Umme Habiba, Sultana and Masudul Ahsan, Sk. Md.},
	title = {Bangladeshi Plant Leaf Classification and Recognition Using YOLO Neural Network},
	year = {2019},
	journal = {ICIET 2019 - 2nd International Conference on Innovation in Engineering and Technology},
	doi = {10.1109/ICIET48527.2019.9290618},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099401258&doi=10.1109%2fICIET48527.2019.9290618&partnerID=40&md5=2a96330012a632f623a7b43af4346925},
	affiliations = {Khulna University of Engineering Technology, Department of Computer Science and Engineering, Khulna, Bangladesh; Khulna University of Engineering Technology, Department of Computer Science and Engineering, Khulna, Bangladesh; Khulna University of Engineering Technology, Department of Computer Science and Engineering, Khulna, Bangladesh},
	abstract = {Now a days, recognition of plant species and leaves holds a great importance in the field of medical science, environmental issues like maintaining ecological balance, preserving distinct plants etc. Using Deep Convolutional Neural Network (CNN) as a classifier, has shown tremendous success on the field of classification and detection task. In this paper, we have proposed to use the YOLOv2 model as a classifier through which we have trained the model using our leaf dataset. In this transfer learning approach our target leaf data set was used to classify leaves as our target task. Both recognition and localization of the leaves are done through this work. Multiple leaves detection and classification is also an achievement of this work. Approximately 96% classification accuracy was achieved to classify the leaves which had also shown a satisfactory localization accuracy also. Both recognition and localization of leaves will bring a success to the researchers in the field of botany, medicinal plant analysis also.  © 2019 IEEE.},
	author_keywords = {classifier; convolutional neural network; feature extraction; multiple leaves recognition; YOLOv2},
	keywords = {Classification (of information); Convolutional neural networks; Deep neural networks; Engineering research; Transfer learning; Classification accuracy; Ecological balance; Environmental issues; Localization accuracy; Medical science; Medicinal plants; Multiple leaves; Plant leaf classifications; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172816309-3},
	language = {English},
	abbrev_source_title = {ICIET - Int. Conf. Innov. Eng. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 2nd International Conference on Innovation in Engineering and Technology, ICIET 2019; Conference date: 23 December 2019 through 24 December 2019; Conference code: 166032}
}

@CONFERENCE{Jayalath2019220,
	author = {Jayalath, A.D.A.D.S. and Nadeeshan, P.V.D. and Amarawansh, T.G.A.G.D. and Jayasuriya, H.P. and Nawinna, D.P.},
	title = {Ayurvedic Knowledge Sharing Platform with Sinhala Virtual Assistant},
	year = {2019},
	journal = {2019 International Conference on Advancements in Computing, ICAC 2019},
	pages = {220 – 225},
	doi = {10.1109/ICAC49085.2019.9103413},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086463542&doi=10.1109%2fICAC49085.2019.9103413&partnerID=40&md5=d8f2283e601a0362712cbeaecbe6d714},
	affiliations = {Sri Lanka Institute of Information Technology, Faculty of Computing, Malabe, Sri Lanka},
	abstract = {Apart from western medicine methods Ayurveda medicinal system is a very huge and better resulting medicinal technique. In these Ayurveda methods identification of indigenous plants to predict the medicines is very important and must do very carefully. Generally main components that we use to identify a plant are leaf, flower, trunk and root etc. Among these features, we use images of leaves and flowers. To do this we are using deep learning based CNN approaches and machine learning and technologies. Those are OpenCV, and Tensorflow classification algorithm. According to the evidences that we gathered from surveys and interviews that we conducted with the responsible parties we could find out that lots of people don't have much knowledge about indigenous medicinal plants and their Ayurveda treatment methods. To overcome this problem we implemented Ayurveda information centralized chatbot which is able to answer user's questions relevant to the Ayurveda and indigenous medicinal plants. Chatbot will analyze the question that user asks and will provide answers according to that. Another useful feature of this system is it provides relevant information of Ayurveda doctors. So users can find doctors according to their needs and they are able to rate and give recommendations for the doctors. That will be help others to find doctors more easily and efficiently without any doubt. © 2019 IEEE.},
	author_keywords = {Ayurveda; Image Processing; Machine learning; Neural Network; Plant identification; Sinhala; Virtual Assistant},
	keywords = {Deep learning; Chatbot; Classification algorithm; Knowledge-sharing platform; Medicinal plants; Treatment methods; Virtual assistants; Western medicines; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172814170-1},
	language = {English},
	abbrev_source_title = {Int. Conf. Adv. Comput., ICAC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2019 International Conference on Advancements in Computing, ICAC 2019; Conference date: 5 December 2019 through 7 December 2019; Conference code: 160453}
}

@ARTICLE{Huynh2020197,
	author = {Huynh, Hiep Xuan and Truong, Bao Quoc and Nguyen Thanh, Kiet Tan and Truong, DInh Quoc},
	title = {Plant Identification Using New Architecture Convolutional Neural Networks Combine with Replacing the Red of Color Channel Image by Vein Morphology Leaf},
	year = {2020},
	journal = {Vietnam Journal of Computer Science},
	volume = {7},
	number = {2},
	pages = {197 – 208},
	doi = {10.1142/S2196888820500116},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105514264&doi=10.1142%2fS2196888820500116&partnerID=40&md5=d8719e8cab40c308a31ae1edff21321b},
	affiliations = {College of Information and Communication Technology, Can Tho University, Can Tho City, Viet Nam; College of Engineering Technology, Can Tho University, Can Tho City, Viet Nam},
	abstract = {The determination of plant species from field observation requires substantial botanical expertise, which puts it beyond the reach of most nature enthusiasts. Traditional plant species identification is almost impossible for the general public and challenging even for professionals who deal with botanical problems daily such as conservationists, farmers, foresters, and landscape architects. Even for botanists themselves, species identification is often a difficult task. This paper proposes a model deep learning with a new architecture Convolutional Neural Network (CNN) for leaves classifier based on leaf pre-processing extract vein shape data replaced for the red channel of colors. This replacement improves the accuracy of the model significantly. This model experimented on collector leaves data set Flavia leaf data set and the Swedish leaf data set. The classification results indicate that the proposed CNN model is effective for leaf recognition with the best accuracy greater than 98.22%.  © 2020 The Author(s).},
	author_keywords = {convolutional neural networks; Deep learning; leaf classification},
	publisher = {World Scientific},
	issn = {21968896},
	language = {English},
	abbrev_source_title = {Vietnam. J. Comput. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Gold Open Access}
}

@ARTICLE{Amend2019411,
	author = {Amend, Sandra and Brandt, David and Di Marco, Daniel and Dipper, Tobias and Gässler, Gabriel and Höferlin, Markus and Gohlke, Maurice and Kesenheimer, Katharina and Lindner, Peter and Leidenfrost, Roland and Michaels, Andreas and Mugele, Tobias and Müller, Arthur and Riffel, Tanja and Sampangi, Yeshwanth and Winkler, Jan},
	title = {Weed Management of the Future},
	year = {2019},
	journal = {KI - Kunstliche Intelligenz},
	volume = {33},
	number = {4},
	pages = {411 – 415},
	doi = {10.1007/s13218-019-00617-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088007437&doi=10.1007%2fs13218-019-00617-x&partnerID=40&md5=861fa08de4fb05f098341ace1492366d},
	affiliations = {Deepfield Robotics, A grow platform GmbH Bosch Start-Up, Grönerstr. 5, Ludwigsburg, 71636, Germany},
	abstract = {The methods used to protect agricultural products currently undergo drastic changes. Artificial Intelligence is a prime candidate to overcome two challenges faced by farmers around the world: The increasing cost and decreasing availability of human labor for weed control, and the growing global restriction of herbicides. Deep Learning is one of the most prominent approaches for applying AI to all kinds of use cases in industrial applications, entertainment, and security. Its latest field of application is plant classification that enables automated weed control and precise spot spraying of herbicides. While cheap, powerful platforms for deploying classification mechanisms are widely available, this comes at the cost of expensive and effort rich classifier training. This effectively makes Deep Learning-based approaches unavailable for the majority of the agricultural sector. Deepfield Robotics presents a systematic approach for deploying AI onto fields at large, including the learnings that led to their self-contained AI driven plant classification modules that relieve individuals from having to deploy their own AI solution. The same technology acts as enabler for more agricultural domains, such as targeted fertilization, nano irrigation, and automated phenotyping. This article documents Deepfield Robotics’ findings and vision on how AI can be the workhorse for agricultural weeding labor. © 2019, Gesellschaft für Informatik e.V. and Springer-Verlag GmbH Germany, part of Springer Nature.},
	author_keywords = {Agricultural robotics; Autonomous robots; Deep learning; Weed management},
	keywords = {Agricultural products; Deep learning; Herbicides; Robotics; Agricultural robotics; Classification mechanism; Classifier training; Deep learning; Human labor; Increasing costs; Learning-based approach; Plant classification; Spot spraying; Weed management; Weed control},
	correspondence_address = {J. Winkler; Deepfield Robotics, A grow platform GmbH Bosch Start-Up, Ludwigsburg, Grönerstr. 5, 71636, Germany; email: jan.winkler@de.bosch.com},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {09331875},
	language = {English},
	abbrev_source_title = {KI - Kunstl. Intell.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Cao2020,
	author = {Cao, Deng and Lowell, Cadance and Schluttenhofer, Craig M. and Morris, Augustus and Erdman, Austin R. and Johnson, Torry and Taylor, Jeffrey D.},
	title = {Undergraduate research: Deep learning based plant classifiers and their real- life research applications},
	year = {2020},
	journal = {ASEE Annual Conference and Exposition, Conference Proceedings},
	volume = {2020-June},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095793127&partnerID=40&md5=838e143f661fddfd017794f4785fd560},
	affiliations = {Central State University, United States},
	abstract = {Deep learning structures, such as Convolutional Neural Networks (CNNs), have been introduced to the undergraduate students in Central State University for the past three years. Funded by an 1890 Land Grant Evans-Allen research program and a USDA Capacity Building Grant, a number of students with minimum deep learning background were trained to develop customized CNNs. After training, the students were able to solve given plant classification problems and develop plant classification apps to showcase the performance of the customized CNNs. In particular, two students' research projects were discussed in details in this work. One project's goal was to identify Soybean (Glycine max) in its Cotyledon (VC) and 1st -5th trifoliate stages, the other project's goal was to identify Hemp (Cannabis sativa) in its three variations. The databases used in these projects were built from real field images, which contain 9 common weed species. The students' achievement, as well as discovered issues, are assessed and reported in this work. The students' projects will be further used to support our 1890 Land Grant and CBG research. © American Society for Engineering Education 2020.},
	keywords = {Amino acids; Convolutional neural networks; Hemp; Nitrogen fixation; Students; Capacity building; Learning structure; Plant classification; Research applications; Research programs; Students' projects; Undergraduate research; Undergraduate students; Deep learning},
	publisher = {American Society for Engineering Education},
	issn = {21535965},
	language = {English},
	abbrev_source_title = {ASEE Annu. Conf. Expos. Conf. Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2020 ASEE Virtual Annual Conference, ASEE 2020; Conference date: 22 June 2020 through 26 June 2020; Conference code: 164392}
}

@CONFERENCE{Pechebovicz2020674,
	author = {Pechebovicz, Denise and Premebida, Sthefanie and Soares, Vinicios and Camargo, Thiago and Bittencourt, Jakson L. and Baroncini, Virginia and Martins, Marcella},
	title = {Plants recognition using embedded convolutional neural networks on mobile devices},
	year = {2020},
	journal = {Proceedings of the IEEE International Conference on Industrial Technology},
	volume = {2020-February},
	pages = {674 – 679},
	doi = {10.1109/ICIT45562.2020.9067289},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084148070&doi=10.1109%2fICIT45562.2020.9067289&partnerID=40&md5=cd2e5d796e5b65ddec2ff96bbcde5446},
	affiliations = {Federal University of Technology, Paraná - Ponta Grossa - (UTFPR-PG), Brazil},
	abstract = {In this work we propose a mobile application capable of recognizing Brazilian medicinal plants to be used by universities, students that have not previous contact with the species and professionals working on health centers. We describe the database generation based on the Brazilian Ministry of Health list of medicinal and common toxic plants. We also implement artificial intelligence techniques to perform the recognition task using a class of convolutional neural networks (CNN) focused on lowering the computation resource necessary to run deep learning tasks and also optimizing the execution of the architectures on embedded and mobile devices. © 2020 IEEE.},
	author_keywords = {Convolutional Neural Networks; Embedded Applications; Image Classification; Plant Recognition},
	keywords = {Convolution; Deep learning; Artificial intelligence techniques; Brazilian medicinal plants; Computation resources; Database generation; Embedded and mobile devices; Health centers; Learning tasks; Mobile applications; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172815754-2},
	coden = {85RSA},
	language = {English},
	abbrev_source_title = {Proc IEEE Int Conf Ind Technol},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 21st IEEE International Conference on Industrial Technology, ICIT 2020; Conference date: 26 February 2020 through 28 February 2020; Conference code: 159293}
}

@CONFERENCE{Goëau2020,
	author = {Goëau, Hervé and Bonnet, Pierre and Joly, Alexis},
	title = {Overview of LifeCLEF Plant Identification task 2020},
	year = {2020},
	journal = {CEUR Workshop Proceedings},
	volume = {2696},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121770379&partnerID=40&md5=a93aea4f050a7ee4b17611608fd87063},
	affiliations = {CIRAD, UMR AMAP, France; AMAP, Univ Montpellier, CIRAD, CNRS, INRAE, IRD, Montpellier, France; Inria ZENITH team, France; LIRMM, Montpellier, France},
	abstract = {Automated identification of plants has improved considerably thanks to the recent progress in deep learning and the availability of training data with more and more photos in the field. However, this profusion of data only concerns a few tens of thousands of species, mostly located in North America and Western Europe, much less in the richest regions in terms of biodiversity such as tropical countries. On the other hand, for several centuries, botanists have collected, catalogued and systematically stored plant specimens in herbaria, particularly in tropical regions, and the recent efforts by the biodiversity informatics community made it possible to put millions of digitized sheets online. The LifeCLEF 2020 Plant Identification challenge (or”PlantCLEF 2020”) was designed to evaluate to what extent automated identification on the flora of data deficient regions can be improved by the use of herbarium collections. It is based on a dataset of about 1,000 species mainly focused on the South America’s Guiana Shield, an area known to have one of the greatest diversity of plants in the world. The challenge was evaluated as a cross-domain classification task where the training set consist of several hundred thousand herbarium sheets and few thousand of photos to enable learning a mapping between the two domains. The test set was exclusively composed of photos in the field. This paper presents the resources and assessments of the conducted evaluation, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of the main outcomes. Copyright © 2020 for this paper by its authors.},
	author_keywords = {Amazon rainforest; Benchmark; Cross-domain classification; Domain adaptation; Evaluation; Fine-grained classification; Guiana Shield; LifeCLEF; Plant; PlantCLEF; Species identification; Tropical flora},
	keywords = {Biodiversity; Classification (of information); Deep learning; Image enhancement; Plants (botany); Amazon rain forest; Benchmark; Cross-domain; Cross-domain classification; Domain adaptation; Evaluation; Fine grained; Fine-grained classification; Guiana shield; LifeCLEF; Plant; PlantCLEF; Species identification; Tropical flora; Tropics},
	editor = {De Carolis B. and Gena C. and Lieto A. and Rossi S. and Sciutti A.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 11th Conference and Labs of the Evaluation Forum, CLEF 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 163832}
}

@ARTICLE{Hieu202025,
	author = {Hieu, Nguyen Van and Hien, Ngo Le Huy},
	title = {Automatic plant image identification of Vietnamese species using deep learning models},
	year = {2020},
	journal = {International Journal of Engineering Trends and Technology},
	volume = {68},
	number = {4},
	pages = {25 – 31},
	doi = {10.14445/22315381/IJETT-V68I4P205S},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087054977&doi=10.14445%2f22315381%2fIJETT-V68I4P205S&partnerID=40&md5=305b8da847c9e5feefe7fa9321e8d52d},
	affiliations = {Faculty of Information Technology, University of Danang, University of Science and Technology, Viet Nam; Faculty of Computer Science and Engineering, University of Danang, VN-UK Institute for Research and Executive Education, Viet Nam},
	abstract = {It is complicated to distinguish among thousands of plant species in the natural ecosystem, and many efforts have been investigated to address the issue. In Vietnam, the task of identifying one from 12,000 species requires specialized experts in flora management, with thorough training skills and in-depth knowledge. Therefore, with the advance of machine learning, automatic plant identification systems have been proposed to benefit various stakeholders, including botanists, pharmaceutical laboratories, taxonomists, forestry services, and organizations. The concept has fueled an interest in research and application from global researchers and engineers in both fields of machine learning and computer vision. In this paper, the Vietnamese plant image dataset was collected from an online encyclopedia of Vietnamese organisms, together with the Encyclopedia of Life, to generate a total of 28,046 environmental images of 109 plant species in Vietnam. A comparative evaluation of four deep convolutional feature extraction models, which are MobileNetV2, VGG16, ResnetV2, and Inception Resnet V2, is presented. Those models have been tested on the Support Vector Machine (SVM) classifier to experiment with the purpose of plant image identification. The proposed models achieve promising recognition rates, and MobilenetV2 attained the highest with 83.9%. This result demonstrates that machine learning models are potential for plant species identification in the natural environment, and future works need to examine proposing higher accuracy systems on a larger dataset to meet the current application demand. © 2020 Innovare Academics Sciences Pvt. Ltd. All rights reserved.},
	author_keywords = {Convolutional neural network; Deep learning models; Plant identification; Support vector machine},
	publisher = {Seventh Sense Research Group},
	issn = {23490918},
	language = {English},
	abbrev_source_title = {Int. J. Eng. Trends Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access, Green Open Access}
}

@ARTICLE{Elstone2020,
	author = {Elstone, Lydia and How, Kin Yau and Brodie, Samuel and Ghazali, Muhammad Zulfahmi and Heath, William P. and Grieve, Bruce},
	title = {High speed crop and weed identification in lettuce fields for precision weeding},
	year = {2020},
	journal = {Sensors (Switzerland)},
	volume = {20},
	number = {2},
	doi = {10.3390/s20020455},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078004407&doi=10.3390%2fs20020455&partnerID=40&md5=2ffd3c6fdb09c92a85a92acb98bdf1a6},
	affiliations = {School of Electrical and Electronic Engineering, The University of Manchester, Oxford Rd, Manchester, M13 9PL, United Kingdom},
	abstract = {Precision weeding can significantly reduce or even eliminate the use of herbicides in farming. To achieve high-precision, individual targeting of weeds, high-speed, low-cost plant identification is essential. Our system using the red, green, and near-infrared reflectance, combined with a size differentiation method, is used to identify crops and weeds in lettuce fields. Illumination is provided by LED arrays at 525, 650, and 850 nm, and images are captured in a single-shot using a modified RGB camera. A kinematic stereo method is utilised to compensate for parallax error in images and provide accurate location data of plants. The system was verified in field trials across three lettuce fields at varying growth stages from 0.5 to 10 km/h. In-field results showed weed and crop identification rates of 56% and 69%, respectively. Post-trial processing resulted in average weed and crop identifications of 81% and 88%, respectively. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Kinetic stereo imaging; Multispectral imaging; Plant detection; Precision weeding},
	keywords = {Crops; Geometrical optics; Infrared devices; Crop identification; Differentiation methods; Multispectral imaging; Near infra-red reflectances; Plant detections; Plant identification; Precision weeding; Stereo imaging; article; crop; field study; growth curve; human; illumination; infrared radiation; lettuce; nonhuman; velocity; weed; Stereo image processing},
	correspondence_address = {L. Elstone; School of Electrical and Electronic Engineering, The University of Manchester, Manchester, Oxford Rd, M13 9PL, United Kingdom; email: lydia.elstone@postgrad.manchester.ac.uk},
	publisher = {MDPI AG},
	issn = {14248220},
	pmid = {31947520},
	language = {English},
	abbrev_source_title = {Sensors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Sujith2020220,
	author = {Sujith, A. and Aji, S.},
	title = {An Optimal Feature Set with LBP for Leaf Image Classification},
	year = {2020},
	journal = {Proceedings of the 4th International Conference on Computing Methodologies and Communication, ICCMC 2020},
	pages = {220 – 225},
	doi = {10.1109/ICCMC48092.2020.ICCMC-00042},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084656337&doi=10.1109%2fICCMC48092.2020.ICCMC-00042&partnerID=40&md5=38b8f5a6bfb2790b2ce608096ff31708},
	affiliations = {University of Kerala, Department of Computer Science, Thiruvananthapuram, Kerala, India},
	abstract = {Plants are the integral part of human life. The preparation of Ayurvedic medicine from medicinal plants began thousands of years ago, and most of the people depend on Ayurveda for the recovery of diseases. The awareness of plants from manuscript, ancestors and Botanist helps to protect the plant which maintain the balanced ecosystem. Identifying plants through wet lab experiment is a time consuming task. The advancements in computational techniques have given considerable contributions in plant identification domain and it is gradually becoming an alternative method. The proposed method gives an optimal feature set which has evolved from LBP, GLCM, and HOG features extraction techniques. The combined feature vector is optimized with the help of Neighborhood Component Analysis (NCA). The feature selection and dimensionality reduction techniques have helped to enhance the classification performance and computational efficiency. Three plant datasets - Flavia, D-Leaf and Swedish Leaves - are used in the experiments for evaluating the proposal. Experimental result shows that proposed method has an average classification accuracy of 97.63% in 291.24 seconds computational time. © 2020 IEEE.},
	author_keywords = {Gray Level Co-occurrence Matrix; Histogram Oriented Gradient; Local Binary Pattern; Neighborhood Components Analysis; Plant identification and Classification},
	keywords = {Computational efficiency; Dimensionality reduction; Image classification; Classification accuracy; Classification performance; Computational technique; Dimensionality reduction techniques; Neighborhood component analysis; Optimal feature sets; Plant identification; Time-consuming tasks; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172814889-2},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Comput. Methodol. Commun., ICCMC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 4th International Conference on Computing Methodologies and Communication, ICCMC 2020; Conference date: 11 March 2020 through 13 March 2020; Conference code: 159543}
}

@ARTICLE{Su2020208753,
	author = {Su, Jianyu and Wang, Meihua and Wu, Zhenxin and Chen, Qingliang},
	title = {Fast Plant Leaf Recognition Using Improved Multiscale Triangle Representation and KNN for Optimization},
	year = {2020},
	journal = {IEEE Access},
	volume = {8},
	pages = {208753 – 208766},
	doi = {10.1109/ACCESS.2020.3037649},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097648809&doi=10.1109%2fACCESS.2020.3037649&partnerID=40&md5=5f8e10d5502a80b711557133b8edbc20},
	affiliations = {Department of Computer Science, Jinan University, Guangzhou, 510632, China; College of Mathematics and Informatics, South China Agricultural of University, Guangzhou, 510642, China; Guangzhou Xuanyuan Research Institute Company, Ltd., Guangzhou, 510006, China},
	abstract = {Due to the complexity and similarity of plant leaves, it is very important to study an effective leaf-feature extraction method to improve the recognition rate of plant leaves. We study five multi-scale triangle representations: the triangle unsigned area representation (TUA), the triangle vertex angle representation (TVA) and three new representations, which we define as the gray average (TGA), the gray standard deviation (TGSD) and the side length integral (TSLI) on the triangle. In this method the curvature features of the contour, the texture features and the shape area feature are extracted to provide a multiscale leaf-feature description, and a new adaptive KNN for optimization method is proposed to improve the retrieval rate of leaf datasets. Experiments show that compared with the state-of-the-art methods, our method has higher accuracy on the Swedish and Flavia plant leaf datasets, which are respectively 99.35% and 99.43% with 84.76% Mean Average Precision (MAP) value and has comparable results on MPEG-7, kimia99 and kimia216 datasets. When our method is combined with KNN for optimization, the retrieval rate of the above datasets has been significantly improved, especially MAP on the Flavia dataset increases to 94.48%. © 2013 IEEE.},
	author_keywords = {adaptive KNN for optimization; multi-scale leaf-feature description; multi-scale triangle representation; Plant leaf recognition},
	keywords = {Curve fitting; Textures; Curvature features; Feature description; Feature extraction methods; Optimization method; Retrieval rate; Standard deviation; State-of-the-art methods; Texture features; Plants (botany)},
	correspondence_address = {M. Wang; College of Mathematics and Informatics, South China Agricultural of University, Guangzhou, 510642, China; email: wangmeihua@scau.edu.cn},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access, Gold Open Access}
}

@ARTICLE{Van Hieu2020904,
	author = {Van Hieu, Nguyen and Hien, Ngo Le Huy},
	title = {Recognition of plant species using deep convolutional feature extraction},
	year = {2020},
	journal = {International Journal on Emerging Technologies},
	volume = {11},
	number = {3},
	pages = {904 – 910},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087319701&partnerID=40&md5=ce74e0177c55dce489999f1cc8064e8f},
	affiliations = {Department of Information and Technology, The University of Danang, University of Science and Technology, Danang, Viet Nam; Department of Computer Science and Engineering, The University of Danang-VN-UK, Institute for Research and Executive Education, Danang, Viet Nam},
	abstract = {There are more than 391,000 plant species currently known to global science, and it is challenging to distinguish among them. The identification of plant species requires in-depth surveyors and botanists who possess a tremendous amount of knowledge on native plant species. Therefore, plant recognition has become an interdisciplinary concentration in both botanical taxonomy and machine learning for a faster identification process. In this paper, a convolutional neural network system has been proposed to perform feature extraction using different deep learning models in large-scale plant classification methods. The plant image dataset was collected from the PlantCLEF2003 dataset, which consists of 51,273 images from 609 plant species. Four deep convolutional feature extraction methods, including Resnet50V2, Inception Resnet V2, MobilenetV2, and VGG16, are used to extract features from the images. A comparative evaluation of four deep learning models using two classification methods, Support Vector Machine (SVN) and k-nearest neighbor (KNN), is presented. With the highest accuracy of 95.6%, MobilenetV2 performed better than the other deep learning models for plant recognition in both SVM and KNN classification methods. Moreover, the SVM classifier has outperformed the KNN in terms of accuracy in the plant image recognition system. The outcomes are promising for further applications and future work gears towards experiments on a larger dataset with high-performance computing facilities to propose a higher accuracy system of plant image identification in natural environments. © 2020, Research Trend. All rights reserved.},
	author_keywords = {Convolutional neural network; Deep features; Deep learning; K-nearest neighbors; Plant identification; Support vector machine},
	publisher = {Research Trend},
	issn = {09758364},
	language = {English},
	abbrev_source_title = {Int. J. Emerg. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@CONFERENCE{Riaz2020,
	author = {Riaz, Syeda Alleena and Naz, Saeeda and Razzak, Imran},
	title = {Multipath Deep Shallow Convolutional Networks for Large Scale Plant Species Identification in Wild Image},
	year = {2020},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	doi = {10.1109/IJCNN48605.2020.9207113},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093859259&doi=10.1109%2fIJCNN48605.2020.9207113&partnerID=40&md5=1ba9d573401abaaff7bae9de5f8312af},
	affiliations = {GGPGC, Department of Computer Science, Abbottabad, Pakistan; Deakin University, School of Information Technology, Geelong, Australia},
	abstract = {One out of five plants are threatened as evident from the IUCN Red List data. Such high rates of loss in plant species triggered to protect and conserve biodiversity. It needs extremely high identification skills obtained via intensive training and experience, even for experienced botanists it is sometimes impossible to provide a definite identification based on a single image. Automatic identification of plant species in natural scene images is one of the important however challenging research problem with various applications in the field of agriculture and botany. Recently, state-of-the-art Deep Convolutional Neural Networks have been applied to classify different species of plants however, they still suffer due to the complexity of the plant images. In this paper, we present multi-path multi deep convolutional network for the identification of plant species which feeds different versions of plant images, thus resultant model has better image presentation than traditional CNN. Comprehensive experimental evaluation on benchmark plant datasets showed that without using any pre-trained models, our proposed shallow network demonstrate very competitive performance for plant species identification. The experimental results proved that the multi-path multi CNN are highly effective for learning discriminative features. © 2020 IEEE.},
	author_keywords = {Ensemble Learning; Plant identification; Plant species identification; Shallow network},
	keywords = {Agricultural robots; Automation; Benchmarking; Biodiversity; Conservation; Convolution; Deep neural networks; Image processing; Plants (botany); Competitive performance; Convolutional networks; Discriminative features; Experimental evaluation; Image presentations; Natural scene images; Plant species identification; Research problems; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172816926-2},
	coden = {85OFA},
	language = {English},
	abbrev_source_title = {Proc Int Jt Conf Neural Networks},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 2020 International Joint Conference on Neural Networks, IJCNN 2020; Conference date: 19 July 2020 through 24 July 2020; Conference code: 163566}
}

@CONFERENCE{Chen2020,
	author = {Chen, Yunqiang and Wang, Qing and Chen, Hong and Li, Xiang and Song, Xiaoyu},
	title = {Research on tracking method of flower target},
	year = {2020},
	journal = {Journal of Physics: Conference Series},
	volume = {1453},
	number = {1},
	doi = {10.1088/1742-6596/1453/1/012155},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081160264&doi=10.1088%2f1742-6596%2f1453%2f1%2f012155&partnerID=40&md5=6f8bd89df9bb768f4fbef8a36f912dc4},
	affiliations = {College of Information and Electrical Engineering, China Agricultural University, Beijing, 10083, China},
	abstract = {Flowers are a common species in people's lives, but people's understanding of flowers is not comprehensive. The traditional UI display method lacks immersion and interactivity. This paper proposes a method based on the deep learning neural network flower recognition algorithm to recognize the flower target information and replace the traditional UI with the AR information to display the flower information. The tracking and positioning technology used in the floral AR display method is a very important underlying technology. This paper focuses on the tracking technology of flower target, and proposes a server-based flower recognition neural network algorithm recognition algorithm. After identifying the flower information and detection frame, it outputs the detection frame parameters of the flower target, and then combines the OpenCV KCF tracker pair. The method of performing AR display of the flower information has high tracking accuracy and stability. © Published under licence by IOP Publishing Ltd.},
	keywords = {Deep learning; Flower recognition; Learning neural networks; Neural network algorithm; Positioning technologies; Target information; Tracking accuracy; Tracking method; Tracking technology; Target tracking},
	correspondence_address = {Q. Wang; College of Information and Electrical Engineering, China Agricultural University, Beijing, 10083, China; email: wangqingait@sina.com},
	publisher = {Institute of Physics Publishing},
	issn = {17426588},
	language = {English},
	abbrev_source_title = {J. Phys. Conf. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2019 2nd International Conference on Computer Information Science and Artificial Intelligence, CISAI 2019; Conference date: 25 October 2019 through 27 October 2019; Conference code: 158105; All Open Access, Bronze Open Access}
}

@ARTICLE{Knoll2019,
	author = {Knoll, Florian J. and Czymmek, Vitali and Harders, Leif O. and Hussmann, Stephan},
	title = {Real-time classification of weeds in organic carrot production using deep learning algorithms},
	year = {2019},
	journal = {Computers and Electronics in Agriculture},
	volume = {167},
	doi = {10.1016/j.compag.2019.105097},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075534704&doi=10.1016%2fj.compag.2019.105097&partnerID=40&md5=7c0a9a78d024fe6f0679823da15d1116},
	affiliations = {Faculty of Engineering, West Coast University of Applied Sciences, Heide, Germany},
	abstract = {This paper proposes a real-time machine learning approach for carrot plants classification in organic farming using Convolutional Neural Network. Artificial neural networks become increasingly popular for image processing tasks, e.g. the classification of complex structures in images. The problem is not very often the accuracy of the classification, but the speed of calculation. The core of this paper presents a real-time calculation flow for the neural networks, in which all the individual steps are summarized. It also briefly discusses the used sensors, which are suitable for the Convolutional Neural Network and the pre-processing which extracts the plants from the background in order to keep the load on the neural network as low as possible. © 2019 Elsevier B.V.},
	author_keywords = {Colour room processing; Convolution Neural Network (CNN); Deep learning; Organic farming; Random forest classifier; Real-time performance; Visual sensors},
	keywords = {Daucus carota; Agriculture; Convolution; Decision trees; Deep learning; Deep neural networks; Image processing; Neural networks; Convolution neural network; Organic farming; Random forest classifier; Real time performance; Visual sensor; algorithm; artificial neural network; classification; color; image processing; machine learning; organic farming; real time; root vegetable; sensor; weed; Learning algorithms},
	correspondence_address = {F.J. Knoll; West Coast University of Applied Sciences, Faculty of Engineering, Heide, Fritz-Thiedemann Ring 20, 25746, Germany; email: knoll@fh-westkueste.de},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 30}
}

@CONFERENCE{Parashar20202635,
	author = {Parashar, J. and Bhandarkar, S.M. and Simon, J. and Hopkinson, B.M. and Pennings, S.C.},
	title = {Estimation of abundance and distribution of salt marsh plants from images using deep learning},
	year = {2020},
	journal = {Proceedings - International Conference on Pattern Recognition},
	pages = {2635 – 2642},
	doi = {10.1109/ICPR48806.2021.9412264},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110476214&doi=10.1109%2fICPR48806.2021.9412264&partnerID=40&md5=40c03252f9632ad477ebcbc009732626},
	affiliations = {Institute for AI, University of Georgia, Athens, 30602, GA, United States; Dept. of Computer Science, University of Georgia, Athens, 30602, GA, United States; Dept. of Marine Sciences, University of Georgia, Athens, 30602, GA, United States; Dept. of Biology and Biochemistry, University of Houston, Houston, 77204, TX, United States},
	abstract = {Recent advances in computer vision and machine learning, most notably deep convolutional neural networks (CNNs), are exploited to identify and localize various plant species in salt marsh images. Three different approaches are explored that provide estimations of abundance and spatial distribution at varying levels of granularity defined by spatial resolution. In the coarsest-grained approach, CNNs are tasked with identifying which of six plant species are present/absent in large patches within the salt marsh images. CNNs with diverse topological properties and attention mechanisms are shown capable of providing accurate estimations with > 90% precision and recall for the more abundant plant species and reduced performance for less common plant species. Estimation of percent cover of each plant species is performed at a finer spatial resolution, where smaller image patches are extracted and the CNNs tasked with identifying the plant species or substrate at the center of the image patch. For the percent cover estimation task, the CNNs are observed to exhibit a performance profile similar to that for the presence/absence estimation task, but with an ≈ 5%-10% reduction in precision and recall. Finally, fine-grained estimation of the spatial distribution of the various plant species is performed via semantic segmentation. The DeepLab-V3 semantic segmentation architecture is observed to provide very accurate estimations for abundant plant species, but with significant performance degradation for less abundant plant species; in extreme cases, rare plant classes are seen to be ignored entirely. Overall, a clear trade-off is observed between the CNN estimation quality and the spatial resolution of the underlying estimation thereby offering guidance for ecological applications of CNN-based approaches to automated plant identification and localization in salt marsh images. © 2020 IEEE},
	author_keywords = {Attention mechanism; Convolutional neural networks; Deep learning; Ecological monitoring; Network topology; Salt marsh monitoring},
	keywords = {Convolutional neural networks; Deep neural networks; Economic and social effects; Image processing; Image resolution; Pattern recognition; Population distribution; Semantics; Topology; Wetlands; Accurate estimation; Attention mechanisms; Performance degradation; Performance profile; Plant identification; Precision and recall; Semantic segmentation; Topological properties; Deep learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {10514651},
	isbn = {978-172818808-9},
	coden = {PICRE},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Pattern Recognit.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 25th International Conference on Pattern Recognition, ICPR 2020; Conference date: 10 January 2021 through 15 January 2021; Conference code: 169954}
}

@ARTICLE{Keivani202017,
	author = {Keivani, Mohammad and Mazloum, Jalil and Sedaghatfar, Ezatollah and Tavakoli, Mohammad Bagher},
	title = {Automated analysis of leaf shape, texture, and color features for plant classification},
	year = {2020},
	journal = {Traitement du Signal},
	volume = {37},
	number = {1},
	pages = {17 – 28},
	doi = {10.18280/ts.370103},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082456562&doi=10.18280%2fts.370103&partnerID=40&md5=5cb98a97ef7382735955ccb596a7ccba},
	affiliations = {Department of Electrical and Electronic Engineering, Faculty of Engineering, Islamic Azad University of Arak, Arak, 3836119131, Iran; Faculty of Engineering, Department of Electrical and Electronic Engineering, Shahid Sattari Aeronautical University of Science and Technology, Tehran, 1384663113, Iran; Faculty of Agriculture, Department of Plant Pathology, Islamic Azad University of Arak, Arak, 3836119131, Iran},
	abstract = {The main purpose of this research is to apply image processing for plant identification in agriculture. This application field has so far received less attention rather than the other image processing applications domains. This is called the plant identification system. In the plant identification system, the conventional technique is dealt with looking at the leaves and fruits of the plants. However, it does not take into account as a cost effective approach because of its time consumption. The image processing technique can lead to identify the specimens more quickly and classify them through a visual machine method. This paper proposes a methodology for identifying the plant leaf images through several items including GIST and Local Binary Pattern (LBP) features, three kinds of geometric features, as well as color moments, vein features, and texture features based on lacunarity. After completion of the processing phase, the features are normalized, and then Pbest-guide binary particle swarm optimization (PBPSO) is developed as a novel method for reduction of the features. In the next phase, these features are employed for classification of the plant species. Different machine learning classifiers are evaluated including k-nearest neighbor, decision tree, naïve Bayes, and multi-SVM. We tested our proposed technique on Flavia and Folio leaf datasets. The final results demonstrated that the decision tree has the best performance. The results of the experiments reveal that the proposed algorithm shows the accuracy of 98.58% and 90.02% for the "Flavia" and "Folio" datasets, respectively. © 2020 Lavoisier. All rights reserved.},
	author_keywords = {Best-guide binary particle swarm optimization; Geometrics; GIST; Machine learning; Plants},
	keywords = {Classification (of information); Cost effectiveness; Decision trees; Learning systems; Nearest neighbor search; Particle swarm optimization (PSO); Plants (botany); Support vector machines; Textures; Binary particle swarm optimization; Conventional techniques; Geometrics; GIST; Image processing applications; Image processing technique; Plant identification systems; Plants; Image processing},
	correspondence_address = {M. Keivani; Department of Electrical and Electronic Engineering, Faculty of Engineering, Islamic Azad University of Arak, Arak, 3836119131, Iran; email: Mkeivani94@iau-arak.ac.ir},
	publisher = {International Information and Engineering Technology Association},
	issn = {07650019},
	language = {English},
	abbrev_source_title = {Trait. Signal},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; All Open Access, Bronze Open Access}
}

@CONFERENCE{Wang2020,
	author = {Wang, Qiyao and He, Guiqing and Li, Feng and Zhang, Haixi},
	title = {A novel database for plant diseases and pests classification},
	year = {2020},
	journal = {ICSPCC 2020 - IEEE International Conference on Signal Processing, Communications and Computing, Proceedings},
	doi = {10.1109/ICSPCC50002.2020.9259502},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097946196&doi=10.1109%2fICSPCC50002.2020.9259502&partnerID=40&md5=7cf4cf4b901bab7e4b68fe68c23f06f2},
	affiliations = {Northwestern Polytechnical University, School of Electronics and Information, Xi'an, China},
	abstract = {In agricultural field, the research, detection, and treatment of plant diseases and pests play a very important role. Prevention or early treatment of diseases and pests can significantly increase crop yields. With rich variety, plants form a mature hierarchical structure based on taxonomic methods. Thus, in the process of computer vision, plant classification and identification have attracted many researchers, and then the detection system of plant diseases and pests came into being. In this paper, the keyword retrieval is used to obtain images of plant diseases and pests from the keyword search engine to build a novel database. After that, a hierarchical multitask learning is proposed to classify plant diseases and pests by leveraging the relationship between different plant species and pests. The experimental results confirmed the feasibility and reliability of the classification of plant diseases and pests using deep learning model.  © 2020 IEEE.},
	author_keywords = {Deep Learning; Hierarchical Multi-task Learning; Plant Diseases and Pests Database},
	keywords = {Agricultural robots; Agriculture; Classification (of information); Deep learning; Learning systems; Search engines; Signal processing; Agricultural fields; Detection system; Hierarchical structures; Keyword retrieval; Keyword search; Learning models; Plant classification; Plant species; Diseases},
	correspondence_address = {G. He; Northwestern Polytechnical University, School of Electronics and Information, Xi'an, China; email: guiqing_he@nwpu.edu.cn},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172817201-9},
	language = {English},
	abbrev_source_title = {ICSPCC - IEEE Int. Conf. Signal Process., Commun. Comput., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2020 IEEE International Conference on Signal Processing, Communications and Computing, ICSPCC 2020; Conference date: 21 August 2020 through 23 August 2020; Conference code: 165127}
}

@ARTICLE{He2020,
	author = {He, Guiqing and Huo, Yincheng and Ao, Zhen and Zhang, Haixi},
	title = {Toward plant organs in nature: A new database for plant organ system},
	year = {2020},
	journal = {Journal of Electronic Imaging},
	volume = {29},
	number = {6},
	doi = {10.1117/1.JEI.29.6.063009},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098687134&doi=10.1117%2f1.JEI.29.6.063009&partnerID=40&md5=6d8fea0977eac3b2ce987b7faaafafc1},
	affiliations = {Northwestern Polytechnical University, School of Electronics and Information, Xi'an, China; Northwest A and F University, The College of Information Engineering, Yangling, China},
	abstract = {The detection of plant organs is an important research field of plant recognition area. However, due to the lack of database of plant organs, the application of convolutional neural network-based object detection on plant species is very limited. A database of plant organs for deep learning-based object detection is constructed. A huge number of plant images are clawed using specific keywords through keyword search engines such as Baidu and Google. After that, an automatic junk image cleaning method is performed to remove junk images. Finally, artificial labeling is used to delineate plant organ regions. To evaluate the quality of the database, experiments in different object detection models are implemented. Results show that the established plant organ database has good performance in plant organs positioning and classification.  © 2020 SPIE and IS&T.},
	author_keywords = {convolutional neural network; deep learning; object detection; plant organ database},
	keywords = {Classification (of information); Convolutional neural networks; Database systems; Deep learning; Object detection; Object recognition; Cleaning methods; Keyword search; Plant organs; Plant recognition; Plant species; Research fields; Search engines},
	correspondence_address = {G. He; Northwestern Polytechnical University, School of Electronics and Information, Xi'an, China; email: guiqing_he@nwpu.edu.cn},
	publisher = {SPIE},
	issn = {10179909},
	coden = {JEIME},
	language = {English},
	abbrev_source_title = {J. Electron. Imaging},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}@CONFERENCE{Bodhwani2019186,
	author = {Bodhwani, Vinit and Acharjya, D.P. and Bodhwani, Umesh},
	title = {Deep residual networks for plant identification},
	year = {2019},
	journal = {Procedia Computer Science},
	volume = {152},
	pages = {186 – 194},
	doi = {10.1016/j.procs.2019.05.042},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068363463&doi=10.1016%2fj.procs.2019.05.042&partnerID=40&md5=2d4c46b4b347bdd1cb82b40892eef7f2},
	affiliations = {School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, 632014, India},
	abstract = {Advancing the knowledge and understanding of plants growing around us plays a very important and crucial role in medicinally, economically and in a sustainable agriculture. The identification of plant images in the field of computer vision has become an interdisciplinary focus. Taking benign conditions of quick advancement in computer vision and deep learning algorithms, convolutional neural networks (CNN) approach is taken into consideration to learn feature representation of 185 classes of leaves taken from Columbia University, the University of Maryland, and the Smithsonian Institution. For the large-scale classification of plants in the natural environment, a 50-layer deep residual learning framework consisting of 5 stages is designed. The proposed model achieves a recognition rate of 93.09 percent as the accuracy of testing on the LeafSnap dataset, illustrating that deep learning is a highly promising forestry technology. © 2019 The Authors. Published by Elsevier Ltd.},
	author_keywords = {Computer Vision; Convolution Neural Networks; Data Augmentation; Plant Classification},
	keywords = {Agricultural robots; Agriculture; Computer networks; Computer vision; Convolution; Convolutional neural networks; Deep learning; Plants (botany); Statistical tests; Ubiquitous computing; Convolution neural network; Data augmentation; Feature representation; Large scale classifications; Plant classification; Plant identification; Sustainable agriculture; University of Maryland; Learning algorithms},
	editor = {Bundele M. and Dey N. and Madria S.K.},
	publisher = {Elsevier B.V.},
	issn = {18770509},
	language = {English},
	abbrev_source_title = {Procedia Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 34; Conference name: 1st International Conference on Pervasive Computing Advances and Applications, PerCAA 2019; Conference date: 8 January 2019 through 10 January 2019; Conference code: 149073; All Open Access, Gold Open Access}
}

@CONFERENCE{Tian2019,
	author = {Tian, Mengxiao and Chen, Hong and Wang, Qing},
	title = {Flower identification based on Deep Learning},
	year = {2019},
	journal = {Journal of Physics: Conference Series},
	volume = {1237},
	number = {2},
	doi = {10.1088/1742-6596/1237/2/022060},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070320044&doi=10.1088%2f1742-6596%2f1237%2f2%2f022060&partnerID=40&md5=443dfbd8021467341b44d9a8d0e6d81c},
	affiliations = {China Agricultural University, No. 17 Qinghua East Road, 100083, China},
	abstract = {In the field of plant scientific research, agroforestry investigation and production and management, plant identification is crucial basic work, and flower identification is an important part of plant identification. Given the present artificial defects of labor cost, low efficiency and low accuracy in present artificial flower information query and traditional computer vision method, the study built a modified tiny darknet in flowers classification method. Seventeen types of flower datasets published by Oxford University are taken as the research objects and the input of the neural network model. The deep network classification model is trained to automatically extract the characteristics of flower images. Combined with softmax classifier, the flower test images are classified and identified. The experimental results show that the classification accuracy is 92% which is higher than the classification algorithm results of the original model and some current mainstream models. This model has a simple structure, few training parameters, and has achieved a good recognition effect. It is suitable for automatic classification and recognition in the field of flower planting and is convenient for the retrieval of agricultural plant information database. © 2019 IOP Publishing Ltd. All rights reserved.},
	keywords = {Agriculture; Classification (of information); Data mining; Intelligent computing; Query processing; Signal processing; Wages; Automatic classification; Classification accuracy; Classification algorithm; Classification methods; Network classification; Plant identification; Scientific researches; Traditional computers; Deep learning},
	publisher = {Institute of Physics Publishing},
	issn = {17426588},
	language = {English},
	abbrev_source_title = {J. Phys. Conf. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; Conference name: 2019 4th International Conference on Intelligent Computing and Signal Processing, ICSP 2019; Conference date: 29 March 2019 through 31 March 2019; Conference code: 149890; All Open Access, Bronze Open Access}
}

@CONFERENCE{Zheng2019,
	author = {Zheng, Yi and Zhao, Chunyu and Liao, Weiran and Ding, Jingjing and Huang, Zhenyu},
	title = {Identification of aquatic plants in shallow waters based on the sound absorption model combined with the deep learning method},
	year = {2019},
	journal = {Proceedings of the 26th International Congress on Sound and Vibration, ICSV 2019},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084011915&partnerID=40&md5=6f5aa43ffef0f04a3d2ec33c138adad4},
	affiliations = {Instrument Science and Engineering, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China},
	abstract = {Due to the eutrophication phenomena in shallow waters, e.g. lakes and rivers, aquatic plants may grow rapidly and cause serious ecological problems. Therefore, the measurement and evaluation of the underwater plants are important for the environmental protection and ecological restoration of shallow waters. Ultrasonic sensors are widely used to detect underwater objects due to the advantages of low cost and real-time monitoring. This paper develops an echo-location measurement system which includes the acoustic source, the propagation model of the water media and the sound absorption model of the aquatic plants and sediments. The proposed sound absorption model shows the relationship between sound absorption coefficients and the amount of plant. Experiments are then carried out to measure and identify the sound absorption coefficients of the plant samples. Considering variations of different kind of plants, a deep learning method is employed to identify different plants and eliminate the differences in experiments. By using the model of plants, the identification and quantity estimation of aquatic plants with conventional ultrasonic sensors are realized © Proceedings of the 26th International Congress on Sound and Vibration, ICSV 2019. All rights reserved.},
	author_keywords = {Aquatic plants identification; Deep learning; Sound absorption model},
	keywords = {Acoustic wave absorption; Eutrophication; Object detection; River pollution; Sound insulating materials; Ultrasonic applications; Ultrasonic sensors; Water absorption; Aquatic plants; Ecological problem; Ecological restoration; Propagation modeling; Real time monitoring; Sound absorption coefficients; Sound absorption models; Underwater objects; Deep learning},
	publisher = {Canadian Acoustical Association},
	isbn = {978-199918100-0},
	language = {English},
	abbrev_source_title = {Proc. Int. Congr. Sound Vib., ICSV},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 26th International Congress on Sound and Vibration, ICSV 2019; Conference date: 7 July 2019 through 11 July 2019; Conference code: 151920}
}

@ARTICLE{Vo2019363,
	author = {Vo, Anh H. and Dang, Hoa T. and Nguyen, Bao T. and Pham, Van-Huy},
	title = {Vietnamese herbal plant recognition using deep convolutional features},
	year = {2019},
	journal = {International Journal of Machine Learning and Computing},
	volume = {9},
	number = {3},
	pages = {363 – 367},
	doi = {10.18178/ijmlc.2019.9.3.811},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067069451&doi=10.18178%2fijmlc.2019.9.3.811&partnerID=40&md5=397be30e336ba6e6028fad5efbc111c5},
	affiliations = {Faculty of Information Technology, Ton Duc Thang University, Ho Chi Minh City, Viet Nam; Faculty of Information Technology, University of Education and Technology, Ho Chi Minh City, Viet Nam; AI Lab, Faculty of Information Technology, Ton Duc Thang University, Ho Chi Minh City, Viet Nam},
	abstract = {Herbal plant image identification is able to help users without specialized knowledge about botany and plan systematics to find out the information of herbal plans, thus it has become an interdisciplinary focus in both botanical taxonomy and computer vision. A computer vision aided herbal plan identification system has been developed to meet the demand of recognizing and identifying herbal plants rapidly. In this paper, the first herbal plant image dataset collected by mobile phone in natural scenes is presented, which contains 10,000 images of 10 herbal plant species in Vietnam. A VGG16-based deep learning model consisting of 5 residual building blocks is used to extract features from the images. A comparative evaluation of seven classification methods using the same deep convolutional feature extraction method is presented. Experiments on our collected dataset demonstrate that deep learning features worked well with LightGBM classification method for herbal plant recognition in the natural environment with a recognition rate of 93.6%. © 2019 International Association of Computer Science and Information Technology.},
	author_keywords = {Deep feature; Deep learning; Herbal plant; Plant identification},
	publisher = {International Association of Computer Science and Information Technology},
	issn = {20103700},
	language = {English},
	abbrev_source_title = {Int. J. Mach. Learn. Comput.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 22; All Open Access, Bronze Open Access}
}

@ARTICLE{Zhang201935313,
	author = {Zhang, Hehu and Wang, Xiushan and Jiang, Lintao and Xu, Yibo and Jiang, Guoqiang},
	title = {Near color recognition based on residual vector and SVM},
	year = {2019},
	journal = {Multimedia Tools and Applications},
	volume = {78},
	number = {24},
	pages = {35313 – 35328},
	doi = {10.1007/s11042-019-08164-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073931534&doi=10.1007%2fs11042-019-08164-1&partnerID=40&md5=4bcef13469a4857bbb77908881bd987b},
	affiliations = {Department of Electrical Engineering, College of Mechanical & Electrical Engineering of Henan Agricultural University, Zhengzhou, Henan, China},
	abstract = {With the extensive application of machine vision in Agriculture, plant recognition is becoming an important research territory. Due to illumination change, occlusion problem, green background and other factors, the image segmentation quality of plant is uneven. When there are significantly different colors, such as red, green and blue, between plant target and background, classical image processing methods are up to the task. However, in near color scene, for example dark green target and bright green background, plant target recognition is still a very challenging task. To segment plants in above scene, the near color recognition method based on residual vector and SVM has been developed. Firstly, the color vectors were projected to the plane that crosses the point Origin(0, 0, 0) and is perpendicular to reference color vector B0(255, 255, 255). After projection, the significantly different color vectors were distributed in different polar angle ranges, while the near color vectors were concentrated in the same polar angle range. For near color vectors, the small polar angle difference, namely roll, was regarded as redundant information. Then, the angle θ between target color vector A(r, g, b) and B0, along with the norm of A, namely ‖ A → ‖ was calculated. As a result, the three-dimensional target color vector A was converted to the two-dimensional residual vector R(‖ A → ‖ θ). Finally, SVM classifier is used to identify the residual vector. The results show that the linear recognition rate is 90.25%, the average recognition speed 0.243 s, the nonlinear recognition rate 87.47%, and the average recognition speed 0.254 s. This study provides theoretical reference for plant target recognition in the near color scene. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Machine vision; Near color; Residual vector; Support vector machine},
	keywords = {Color; Computer vision; Image segmentation; Support vector machines; Illumination changes; Image processing - methods; Occlusion problems; Red , green and blues; Residual vectors; Segmentation quality; Target and background; Target recognition; Vectors},
	correspondence_address = {X. Wang; Department of Electrical Engineering, College of Mechanical & Electrical Engineering of Henan Agricultural University, Zhengzhou, China; email: towxs@163.com},
	publisher = {Springer},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Dudi2019999,
	author = {Dudi, Bhanuprakash and Rajesh, V.},
	title = {Medicinal plant recognition based on CNN and machine learning},
	year = {2019},
	journal = {International Journal of Advanced Trends in Computer Science and Engineering},
	volume = {8},
	number = {4},
	pages = {999 – 1003},
	doi = {10.30534/ijatcse/2019/03842019},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073380467&doi=10.30534%2fijatcse%2f2019%2f03842019&partnerID=40&md5=e977fbc25cf3c23202e245b6719bebd6},
	affiliations = {Dept of ECE, K L Deemed to be university, Vaddeswaram Guntur, Andhra Pradesh, India; Dept of ECE, CVR college of Engineering, Ibrahimpatanam RangaReddy, Telangana, India},
	abstract = {In the recent days automated plant species recognition systems are developed to help the ordinary people in identification of the different species. But the automatic analysis of plant species by the computer is difficult as compared to the human interpretation. The research has been carried out in this field for the better recognition of plant species. Still these approaches lack with exact classification of the plant species. The problem is due to the inappropriate classification algorithm. Especially when we consider the recognition of medicinal plant species, the accuracy will be the primary criteria. The proposed system in this research adopts the deep learning method to obtain the high accuracy in classification and recognition process using computer vision techniques. This system uses the Convolutional Neural Network (CNN) and the machine learning algorithms for deep learning of medicinal plant images. This research work has been carried out on the leaf dataset of flavia from sourceforge website. This data is fed as the training dataset for the CNN and machine learning based proposed system. An accuracy of 98% has been achieved in the recognition of the medicinal plant species. All the performance metrics like precision, recall, F1-score and support are calculated. Also the achieved training and validation accuracies are nearly equal. © 2019, World Academy of Research in Science and Engineering. All rights reserved.},
	author_keywords = {Classification; CNN and Machine Learning; Image recognition; Medicinal plant},
	publisher = {World Academy of Research in Science and Engineering},
	issn = {22783091},
	language = {English},
	abbrev_source_title = {Int. J. Adv. Trends Comput. Sci. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 23; All Open Access, Bronze Open Access}
}

@CONFERENCE{Boston201915,
	author = {Boston, Tony and Van Dijk, Albert},
	title = {Some experiments in automated identification of Australian plants using convolutional neural networks},
	year = {2019},
	journal = {23rd International Congress on Modelling and Simulation - Supporting Evidence-Based Decision Making: The Role of Modelling and Simulation, MODSIM 2019},
	pages = {15 – 21},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086461023&partnerID=40&md5=7eac1839df60a31b9e09ae33d92416a6},
	affiliations = {Fenner School of Environment and Society, Australian National University, Australian Capital Territory, Australia},
	abstract = {Accurate plant identification is a skill that generally requires considerable knowledge and advanced training. However, plant identification is useful to a broad range of people within society, from conservationists and farmers to citizen scientists. Access to accurate, widely available knowledge about the identity and distribution of living species is critical for biodiversity conservation and sustainable development. Automated plant identification has undergone major advances since 2012 with the application of convolutional neural networks (CNNs) from the emerging field of deep learning. This branch of machine learning has shown remarkable accuracy in image classification and visual object recognition when applied to still images through competitions such as the ImageNet Large Scale Visual Recognition Challenge. This research project used transfer learning to fine-tune pre-trained deep learning CNNs originally developed for the ImageNet challenge, such as Inception and ResNet, which are publicly available through Tensorflow Hub. The models were applied to the automated identification of images of plants extracted from the Australian National Botanic Gardens Australian Plant Image Index and validated using additional images from the Atlas of Living Australia (ALA) and other Internet sources. A comparison of model performance was undertaken using three different datasets: Whole plant images (9,612 images of 392 species with at least 20 images per species), images of flowers (3,384 images of 271 species with at least 10 images per species) and scanning electron microscopy images of liverwort spores from Fossombronia spp. (322 images of 12 species with at least 10 images per species). To decrease the risk of overfitting and extend the training dataset, data augmentation techniques such as scaling and reflection were tested to identify a high performing method, which also improved overall model performance. The best performing model for the All-plants (80.6% accuracy) and Flower datasets (88.4% accuracy) was Inception-V3 pre-trained on the iNaturalist dataset of plants and animals. For the Fossombronia spp. dataset, the best performing model (81.2% accuracy) was ResNet-V2-50 pre-trained on ImageNet 2012, using the 50-layer implementation of ResNet-V2. The best performing flower identification model was also shown to have some proficiency in identifying the genus of an unknown species, where the genus but not species was represented in the dataset, with a Top-5 accuracy of 66%. The Flower dataset's best model performance was further tested using 1,000 images (20 images of 50 randomly selected species) downloaded from the Atlas of Living Australia and the Internet which produced a Top-1 accuracy of 85.9%. Questions that remain to be addressed include further testing of data augmentation approaches and more comprehensive analysis to exclude overfitting. An interesting future extension of this study would be to train the best performing model on a larger dataset of Australian plant images, which could be used to aid scientists and the general public in identifying unknown species through image upload using an online website or phone app. Copyright © 2019 The Modelling and Simulation Society of Australia and New Zealand Inc. All rights reserved.},
	author_keywords = {Convolutional neural networks; Deep learning; Plant identification; Transfer learning},
	keywords = {Automation; Behavioral research; Biodiversity; Convolution; Convolutional neural networks; Decision making; Deep learning; Object recognition; Scanning electron microscopy; Transfer learning; Automated identification; Biodiversity conservation; Comparison of models; Comprehensive analysis; Identification model; Plant identification; Scanning electron microscopy image; Visual object recognition; Conservation},
	correspondence_address = {T. Boston; Fenner School of Environment and Society, Australian National University, Australian Capital Territory, Australia; email: tony.boston@anu.edu.au},
	editor = {Elsawah S.},
	publisher = {Modelling and Simulation Society of Australia and New Zealand Inc. (MSSANZ)},
	isbn = {978-097584009-2},
	language = {English},
	abbrev_source_title = {Int. Congr. Model. Simul. - Support. Evidence-Based Decis. Making: Role Model. Simul., MODSIM},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 23rd International Congress on Modelling and Simulation - Supporting Evidence-Based Decision Making: The Role of Modelling and Simulation, MODSIM 2019; Conference date: 1 December 2019 through 6 December 2019; Conference code: 160153}
}

@ARTICLE{H'ng2019669,
	author = {H'ng, Choo Wooi and Loh, Wei Ping},
	title = {A prediction of leaf mechanical properties with data mining},
	year = {2019},
	journal = {Computers and Electronics in Agriculture},
	volume = {162},
	pages = {669 – 676},
	doi = {10.1016/j.compag.2019.05.006},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065506614&doi=10.1016%2fj.compag.2019.05.006&partnerID=40&md5=da12c609c92f80f41d4c22026787b434},
	affiliations = {School of Mechanical Engineering, Engineering Campus, Universiti Sains Malaysia, Nibong Tebal, 14300, Penang, Malaysia},
	abstract = {The mechanical properties of the leaf are typically determined by mechanical testing approaches to study the leaf lifespan, plant anti-herbivore defences and ecological functions by considering habitat influences, environmental resources variation and species diversity. While the leaf morphology features were commonly used for plant recognition and plant disease detection with the aid of an automated inspection system. However, the influence of morphology features on leaf mechanical properties is vague. In this research, we investigated the effect of various morphological features on mechanical properties of leaf, followed by proposed a novel leaf mechanical properties prediction model using data mining techniques. A 600 × 22 feature vector was collected and examined using Pearson correlation analysis and Welch's test to select the relevant features. The prediction on four mechanical properties indicators was performed with LinearRegression, KStar, DecisionTable and M5P algorithms in the Waikato Environment for Knowledge Analysis (WEKA). The experimental results show that numeric prediction for Tearing Force (FT) and Tearing Strength (ST) (RRSE ≈ 25%) were about two folds better as compared to Work-to-tear (WT) and Specific Work-to-tear (SWT) (RRSE ≈ 50%) in four algorithms tested. The best results achieved was the FT indicator prediction with M5P algorithms (RRSE = 23.12%). FT indicator prediction model adopted from M5P algorithms output was constructed. © 2019},
	author_keywords = {Data mining; Leaf geometry; Leaf morphology; Regression},
	keywords = {New Zealand; North Island; Waikato; Correlation methods; Feature extraction; Forecasting; Mechanical properties; Mechanical testing; Morphology; Automated inspection systems; Ecological functions; Environmental resources; Leaf morphology; Morphological features; Pearson correlation analysis; Properties prediction; Regression; algorithm; data mining; leaf; leaf morphology; mechanical property; regression analysis; species diversity; testing method; Data mining},
	correspondence_address = {W.P. Loh; School of Mechanical Engineering, Engineering Campus, Universiti Sains Malaysia, Nibong Tebal, 14300, Malaysia; email: meloh@usm.my},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Sahidan2019327,
	author = {Sahidan, Nurul Fatihah and Juha, Ahmad Khairi and Ibrahim, Zaidah},
	title = {Evaluation of basic convolutional neural network and bag of features for leaf recognition},
	year = {2019},
	journal = {Indonesian Journal of Electrical Engineering and Computer Science},
	volume = {14},
	number = {1},
	pages = {327 – 332},
	doi = {10.11591/ijeecs.v14.i1.pp327-332},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061127709&doi=10.11591%2fijeecs.v14.i1.pp327-332&partnerID=40&md5=58b4c995e5619069527b2c71dcee6ce2},
	affiliations = {Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia},
	abstract = {This paper presents the evaluation of basic Convolutional Neural Network (CNN) and Bag of Features (BoF) for Leaf Recognition. In this study, the performance of basic CNN and BoF for leaf recognition using a publicly available dataset called Folio dataset has been investigated. CNN has proven its powerful feature representation power in computer vision. The same goes with BoF where it has set new performance standards on popular image classification benchmarks and has achieved scalability breakthrough in image retrieval. The feature that is being utilized in the BoF is Speeded-Up Robust Feature (SURF) texture feature. The experimental results indicate that BoF achieves better accuracy compared to basic CNN. © 2019 Institute of Advanced Engineering and Science. All rights reserved.},
	author_keywords = {Bag of features; CNN; Deep learning; Leaf recognition},
	correspondence_address = {Z. Ibrahim; Faculty of Computer and Mathematical Sciences, UniversitiTeknologi MARA, Shah Alam, Malaysia; email: zaidah@tmsk.uitm.edu.my},
	publisher = {Institute of Advanced Engineering and Science},
	issn = {25024752},
	language = {English},
	abbrev_source_title = {Indones. J. Electrical Eng. Comput. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Kaya201920,
	author = {Kaya, Aydin and Keceli, Ali Seydi and Catal, Cagatay and Yalic, Hamdi Yalin and Temucin, Huseyin and Tekinerdogan, Bedir},
	title = {Analysis of transfer learning for deep neural network based plant classification models},
	year = {2019},
	journal = {Computers and Electronics in Agriculture},
	volume = {158},
	pages = {20 – 29},
	doi = {10.1016/j.compag.2019.01.041},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060714879&doi=10.1016%2fj.compag.2019.01.041&partnerID=40&md5=54ed8047dfdeae88636414705c11b980},
	affiliations = {Department of Computer Engineering, Hacettepe University, Ankara, Turkey; Information Technology Group, Wageningen University, Wageningen, Netherlands},
	abstract = {Plant species classification is crucial for biodiversity protection and conservation. Manual classification is time-consuming, expensive, and requires experienced experts who are often limited available. To cope with these issues, various machine learning algorithms have been proposed to support the automated classification of plant species. Among these machine learning algorithms, Deep Neural Networks (DNNs) have been applied to different data sets. DNNs have been however often applied in isolation and no effort has been made to reuse and transfer the knowledge of different applications of DNNs. Transfer learning in the context of machine learning implies the usage of the results of multiple applications of DNNs. In this article, the results of the effect of four different transfer learning models for deep neural network-based plant classification is investigated on four public datasets. Our experimental study demonstrates that transfer learning can provide important benefits for automated plant identification and can improve low-performance plant classification models. © 2019},
	author_keywords = {Convolutional neural networks; Deep neural networks; Fine-tuning; Plant classification; Transfer learning},
	keywords = {Biodiversity; Classification (of information); Conservation; Deep neural networks; Machine learning; Neural networks; Automated classification; Biodiversity protection; Convolutional neural network; Fine tuning; Manual classification; Multiple applications; Plant classification; Transfer learning; algorithm; artificial neural network; biodiversity; classification; conservation management; data set; experimental study; expert system; learning; machine learning; numerical model; plant; Learning algorithms},
	correspondence_address = {A. Kaya; Department of Computer Engineering, Hacettepe University, Ankara, Turkey; email: aydinkaya@cs.hacettepe.edu.tr},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 237}
}

@ARTICLE{Malik2019745,
	author = {Malik, Muzamil and Ikram, Amna and Batool, Syeda Naila and Aslam, Waqar},
	title = {A Performance Assessment of Rose Plant Classification Using Machine Learning},
	year = {2019},
	journal = {Communications in Computer and Information Science},
	volume = {932},
	pages = {745 – 756},
	doi = {10.1007/978-981-13-6052-7_64},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064058297&doi=10.1007%2f978-981-13-6052-7_64&partnerID=40&md5=30abb46c71c1d2be9dec77f300787b55},
	affiliations = {The Department of CS & IT, The Islamia University of Bahawalpur, Bahawalpur, Pakistan; The Department of CS & IT, The Government Sadiq College Women University, Bahawalpur, Pakistan},
	abstract = {Machine learning enriches the field of artificial intelligence that aims to make computers powerful by providing them information extracted from data. Flowers identification is highly significant and relevant for Plant Scientists. Carrying it out manually is not only a tedious task but also prone to errors due to a large number of flower types. Using machine learning algorithms to identify flowers is appealing. To this aim, two observations on flower leaves are relevant and leverage flower identification: one, flower plants have key knowledge in their leaves, thus enable distinctiveness; two, leaves have a much longer life on plants than flowers and fruits. In this paper, we have proposed a machine learning approach based on k Nearest Neighbor (k-NN) to identify rose types. Following steps are carried out during the identification process. First, rose plant images are taken using 23MP camera, ensuring temperature uniformity during the experiment. Second, texture and histogram features are extracted from the captured images. Third, k-NN algorithm is applied to these features with k taking values between 1 and 10. Our research brings to limelight the usefulness of selected features for rose type identification with histogram and texture features achieving maximum accuracies of 65% and 45.50% respectively. © 2019, Springer Nature Singapore Pte Ltd.},
	author_keywords = {Artificial intelligence; Flower classification; Machine learning; Nearest Neighbor},
	keywords = {Artificial intelligence; Graphic methods; Knowledge management; Learning systems; Lime; Machine learning; Nearest neighbor search; Pattern recognition; Plants (botany); Textures; Histogram features; Identification process; K-nearest neighbors; Machine learning approaches; Nearest neighbors; Performance assessment; Plant classification; Temperature uniformity; Learning algorithms},
	correspondence_address = {M. Malik; The Department of CS & IT, The Islamia University of Bahawalpur, Bahawalpur, Pakistan; email: muzmalik2013@gmail.com},
	editor = {Bajwa I.S. and Costa A. and Kamareddine F.},
	publisher = {Springer Verlag},
	issn = {18650929},
	isbn = {978-981136051-0},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 1st International Conference on Intelligent Technologies and Applications, INTAP 2018; Conference date: 23 October 2018 through 25 October 2018; Conference code: 224579}
}

@CONFERENCE{Sun201926,
	author = {Sun, Hongyu and Cui, Xining and Guo, Yinjing and Ding, Anqing},
	title = {Simplified Spike-timing Dependent Plasticity Learning Rule of Spiking Neural Networks for Unsupervised Clustering},
	year = {2019},
	journal = {Proceedings of 2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference, IMCEC 2019},
	pages = {26 – 30},
	doi = {10.1109/IMCEC46724.2019.8983810},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081166071&doi=10.1109%2fIMCEC46724.2019.8983810&partnerID=40&md5=e157847cb8cad472093467f65620e3e1},
	affiliations = {Shandong University of Science and Technology, College of Electronic Information Engineering, China},
	abstract = {Spike-Timing Dependent Plasticity (STDP) is widely believed as a family of learning mechanisms originally postulated in the context of artificial machine learning algorithms, and has been proven successful in training spiking neural networks (SNN). This paper presents a simplified model of STDP learning rule in artificial network based on spiking neurons for unsupervised clustering. Population encoder algorithm is used to convert the continuous real-number data to discrete input spike times. Subsequently, the generated spikes are submitted into a two-layered structure SNN which employed Leaky Integrate and Fire (LIF) neurons as nodes and trained by simplified STDP learning rule. The learning and clustering capabilities of simplified STDP were investigated by applying it to the nonlinearly separable data like the 2-Dimension ring data set and 4-Dimension Fisher iris plant classification problem. The results show that the SNN algorithm can achieve the best accuracy of 100% and 95.33% respectively, which demonstrates that the proposed simplified model of STDP learning rule is a viable training algorithm and the SNN approach is suitable for unsupervised clustering. © 2019 IEEE.},
	author_keywords = {Spike-Timing Dependent Plasticity; Spiking Neural Networks; Unsupervised Clustering},
	keywords = {Classification (of information); Clustering algorithms; Information management; Machine learning; Neural networks; Neurons; Population statistics; Timing circuits; Integrate and fires; Non-linearly separable data; Plant classification; Spike timing dependent plasticities; Spiking neural network(SNN); Spiking neural networks; Two-layered structures; Unsupervised clustering; Learning algorithms},
	editor = {Xu B.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172810513-0},
	language = {English},
	abbrev_source_title = {Proc. IEEE Adv. Inf. Manag., Commun., Electron. Autom. Control Conf., IMCEC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 3rd IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference, IMCEC 2019; Conference date: 11 October 2019 through 13 October 2019; Conference code: 157524}
}

@ARTICLE{Keskar20196705,
	author = {Keskar, Margesh and Maktedar, Dhananjay},
	title = {Enhancing classifier accuracy in ayurvedic medicinal plants using WO-DNN},
	year = {2019},
	journal = {International Journal of Engineering and Advanced Technology},
	volume = {9},
	number = {1},
	pages = {6705 – 6714},
	doi = {10.35940/ijeat.A2001.109119},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074635638&doi=10.35940%2fijeat.A2001.109119&partnerID=40&md5=8a061e9e48448b322f7d849eb92c983b},
	affiliations = {Visvesvaraya Technological University, Belagavi, Karnataka, India},
	abstract = {Identification of right medicinal plants that goes in to the formation of a medicine is significant in ayurvedic medicinal industry. This paper focuses around the automatic identification proof of therapeutic plants that are regularly utilized in Ayurveda. The fundamental highlights required to distinguish a medicinal plant is its leaf shape, color and texture. In this paper, we propose efficient accurate classifier for ayurvedic medical plant identification (EAC-AMP) utilizing using hybrid optimal machine learning techniques. In EAC-AMP, image corners detect first and top, bottom leaf edges are computed by the improved edge detection algorithm. After preprocessing, the segmentation can achieve using spider optimization neural network (SONN), which segments leaf regions from an image. The time and frequency domain features are computed by the symbolic accurate approximation (SAX); other features shape features, color features and tooth features are computed by the two-dimensional binary phase encoding (2DBPE). Finally, a whale optimization with deep neural network (DNN) classifier is used to characterize the type of plants. Accuracy in identification of any ayurvedic plant leaf is achieved by understanding and extracting the plant features. The main objective of the proposed EAC-AMP approach is to increase the accuracy of classifier. MATLAB experimental analysis showed better results such as accuracy, sensitivity and specificity. © BEIESP.},
	author_keywords = {2DBPE; EAC-AMP; Spider Optimization Neural Network; Symbolic Accurate Approximation; Whale Optimization with DNN},
	publisher = {Blue Eyes Intelligence Engineering and Sciences Publication},
	issn = {22498958},
	language = {English},
	abbrev_source_title = {Int. J. Eng. Adv. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Bronze Open Access}
}

@ARTICLE{Uemura2019,
	author = {Uemura, Takuya and Arimura, Gen-Ichiro},
	title = {Current opinions about herbivore-associated molecular patterns and plant intracellular signaling},
	year = {2019},
	journal = {Plant Signaling and Behavior},
	volume = {14},
	number = {9},
	doi = {10.1080/15592324.2019.1633887},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068037680&doi=10.1080%2f15592324.2019.1633887&partnerID=40&md5=922199149710fc7e86d3243e4eaddc04},
	affiliations = {Department of Biological Science & Technology, Faculty of Industrial Science & Technology, Tokyo University of Science, Tokyo, Japan},
	abstract = {Elicitor-associated compounds included in oral secretions of herbivorous arthropods, defined as herbivore-associated molecular patterns (HAMPs), induce defense responses in plants. Recognition of HAMPs by the host plants triggers the activation of downstream intracellular and intercellular signaling, resulting in the production of defensive secondary metabolites and volatile emissions to defend against herbivore attack. Thus far, several chemical classes of HAMPs, e.g., fatty acid-amino acid conjugates, peptides, enzymes, and oligosaccharides, have been characterized from not only plant-chewing arthropod herbivores but also plant-sucking arthropod herbivores. Here, we introduce the latest insights about HAMPs and the HAMPs-induced defense signaling network in host plants. © 2019, © 2019 Taylor & Francis Group, LLC.},
	author_keywords = {Elicitor; herbivore-associated molecular patterns (HAMPs); pattern recognition receptors (PRRs)},
	keywords = {Alarmins; Animals; Herbivory; Insecta; Intracellular Space; Plants; Receptors, Pattern Recognition; Signal Transduction; alarmin; pattern recognition receptor; animal; herbivory; insect; intracellular space; metabolism; physiology; plant; signal transduction},
	correspondence_address = {G.-I. Arimura; Department of Biological Science and Technology, Faculty of Industrial Science and Technology, Tokyo University of Science, Tokyo, 6-3-1 Niijuku, Katsushika-ku, 125-8585, Japan; email: garimura@rs.tus.ac.jp},
	publisher = {Taylor and Francis Inc.},
	issn = {15592316},
	pmid = {31230525},
	language = {English},
	abbrev_source_title = {Plant. Signal. Behav.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Li2019472,
	author = {Li, Pengxi and Gong, Xiaoqing and Hu, Xu and Shi, Lianqi and Xue, Xiaoting and Guo, Jun and Xu, Pengfei and Gan, Daguang},
	title = {Plant Identification Based on Multi-branch Convolutional Neural Network with Attention},
	year = {2019},
	journal = {Communications in Computer and Information Science},
	volume = {1043},
	pages = {472 – 481},
	doi = {10.1007/978-981-13-9917-6_45},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073906166&doi=10.1007%2f978-981-13-9917-6_45&partnerID=40&md5=673bd536b8e94a5d37a97641444d9a89},
	affiliations = {School of Information Science and Technology, Northwest University, Xi’an, 710127, China; School of Computer Science and Technology, Xidian University, Xi’an, 710071, China; Wanfang Data, Beijing, 100038, China},
	abstract = {The identification of plants has great significance in the study of plants, and it has applications in plant classification and medical research. With the development of computer vision, plants identification based on deep learning methods can effectively carry out. At present, most of the existing methods use the traditional features such as the shape and texture features for plant identification, and these methods are applicable to plants with large differences in their shapes. Therefore, we develop a multi-branch convolutional neural network with attention (MCNNA) to extract the effective features, and the first part of MCNNA is an attention block, which is used to reduce the influence of background, and the latter part is multi-branch convolutional neural network, which is used to extract the multi-view features through multi-channel. Experiments have shown that our proposed method has better performances for the classification of plants than the traditional methods. We tested our method on the dataset BJFU100 and obtained final accuracy of 97.89%, and we have the accuracy of 93.35% on our own image dataset. © 2019, Springer Nature Singapore Pte Ltd.},
	author_keywords = {Attention block; Fusion block; Multi-branch convolutional neural network; Plants identification},
	keywords = {Convolution; Deep learning; Textures; Attention block; Convolutional neural network; Image datasets; Learning methods; Medical research; Plant classification; Plant identification; Shape and textures; Neural networks},
	editor = {Wang Y. and Huang Q. and Peng Y.},
	publisher = {Springer Verlag},
	issn = {18650929},
	isbn = {978-981139916-9},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 14th Conference on Image and Graphics Technologies and Applications, IGTA 2019; Conference date: 19 April 2019 through 20 April 2019; Conference code: 229029}
}

@ARTICLE{Swapna20192544,
	author = {Swapna, C. and Shaji, R.S.},
	title = {Energy based wavelet and multilevel classifier for efficient leaf recognition},
	year = {2019},
	journal = {International Journal of Engineering and Advanced Technology},
	volume = {8},
	number = {5},
	pages = {2544 – 2550},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069904097&partnerID=40&md5=d7e4fdfb8c4c565be8f172120c755969},
	affiliations = {Department of Computer Science and Engineering, Noorul Islam Centre for Higher Education, Kumaracoil, Tamilnadu, India; Department of Computer Science and Engineering, St. Xavier’s Catholic College of Engineering, Chunkankadai, Kanyakumari, India},
	abstract = {Computer Aided Leaf Recognition (CALR) is an important field of research that provides tools for forestry, agriculture and pharmacy. Due to the deterioration of environment, rare species of plants are at the brim of extinction. Investigation of rare plants though CALR can subsidize to environmental protection. Generally, CALR system consists of four main steps, such as, enhancement, segmentation, leaf feature extraction and classification. Preprocessing step enhances the leaf image by removing noise, modifying contrast and highlighting boundaries. To separate leaf image from the background, the CALR system uses clustering combined with Energy Based Wavelet (EBW) segmentation. Optimized Principal Component Analysis (OPCA) is used to extracts 28 features falling under five categories, namely, geometry, color, texture, fractal and leaf specialization. A two-level classifier is used to improve the accuracy of recognition process. A refined training set is generated during the first level, and it is used to train the second level classifier. Standard leaf image dataset and real leaf image dataset are used to evaluate the performance of proposed algorithm. This leaf recognition model is effective in discriminating leaves and identifying plant. Hence, taxonomists can use this system to identify precious plant leaves in order to save them. © BEIESP.},
	author_keywords = {Computer aided leaf recognition (CALR); Energy based wavelet; Multilevel classifier; Optimized principal component analysis},
	publisher = {Blue Eyes Intelligence Engineering and Sciences Publication},
	issn = {22498958},
	language = {English},
	abbrev_source_title = {Int. J. Eng. Adv. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Pravin20191471,
	author = {Pravin, A. and Deepa, C.},
	title = {Observation on therapeutic plant identification based on deep learning technique},
	year = {2019},
	journal = {International Journal of Innovative Technology and Exploring Engineering},
	volume = {8},
	number = {11},
	pages = {1471 – 1475},
	doi = {10.35940/ijitee.J9808.0981119},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073328298&doi=10.35940%2fijitee.J9808.0981119&partnerID=40&md5=d87e3fc0b31bec90baa32a72c0612ea2},
	affiliations = {Sri Ramakrishna College of Arts and Science (Formerly SNR Sons College), Coimbatore, India},
	abstract = {Plants have been used for medicinal purposes long before recorded history. It plays a major role in medicines, food, perfumes and cosmetics industries. By knowing the herbal plants and its usage it can be used for above applications. In this digital era, people don’t have adequate knowledge to identify various herbal plants which are used by our ancestors for long time. Presently, the identification of herbal plants is purely based on the human perception or knowledge. There may be probability of human error occurring. In order to have an efficient herb species classification, there must be a complete model which should be automatic and convenient recognition system. This paper is reviewing the different leaf classification methodologies based on deep learning algorithms. The main aim of this research paper is to conclude the advanced technique for the leaf identification. © BEIESP.},
	author_keywords = {Deep learning algorithm; Herbal plants; Leaf identification; Machine learning; Plant recognition system},
	publisher = {Blue Eyes Intelligence Engineering and Sciences Publication},
	issn = {22783075},
	language = {English},
	abbrev_source_title = {Int. J. Innov. Technol. Explor. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Adam2019225,
	author = {Adam, Sukemi and Amir, Amirullah},
	title = {Fruit Plant Leaf Identification Feature Extraction Using Zernike Moment Invariant (ZMI) and Methods Backpropagation},
	year = {2019},
	journal = {Proceedings - 1st International Conference on Informatics, Multimedia, Cyber and Information System, ICIMCIS 2019},
	pages = {225 – 230},
	doi = {10.1109/ICIMCIS48181.2019.8985219},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081110947&doi=10.1109%2fICIMCIS48181.2019.8985219&partnerID=40&md5=775a005c6d11c8965320ee41a86f69b1},
	affiliations = {University of Sriwijaya, Palembang, Indonesia},
	abstract = {The concept of pattern recognition is often used to identify a wide range of objects. Due to the ability to recognize objects is needed by humans. One of them is for pattern recognition on the leaves as identification in determining the types of leaves. However, in the acquisition, very frequent disturbances called noise. Noise in the image is a region of pixel image intensity of unwanted or deemed to disturb the segmentation process until the introduction. The impact of noise can degrade the image quality when the segmentation process. Therefore, in this study, the researchers added a preprocessing stage to reduce noise modest invisible when the acquisition using the camera. Gaussian filter used as a technique to tackle the problem at last preprocessing. Aside from the noise, constraints at the time of feature extraction of natural researchers also because the study took shape characteristic based on the area of the image. So if the object changes the coordinates of the start pixel image was unrecognizable. Based on these problems do research to identify the leaves by using Zernike Moment invariant feature extraction (ZMI) and Backpropagation algorithm. Based on the testing that was done on 100 test data success rate Based on these problems do research to identify the leaves by using Zernike Moment invariant feature extraction (ZMI) and Backpropagation algorithm. Based on the testing that was done on 100 test data success rate78%. © 2019 IEEE.},
	author_keywords = {Backpropagation; Feature Extraction; Gaussian filter; Leaf Recognition; Zernike Moment invariant},
	keywords = {Backpropagation; Extraction; Image segmentation; Information systems; Information use; Pixels; Plants (botany); Pulse shaping circuits; Gaussian filters; Impact of noise; Leaf recognition; Pixel images; Plant leaf; Segmentation process; Shape characteristics; Zernike moment invariants; Feature extraction},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172812930-3},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Inf., Multimed., Cyber Inf. Syst., ICIMCIS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 1st International Conference on Informatics, Multimedia, Cyber and Information System, ICIMCIS 2019; Conference date: 24 October 2019 through 25 October 2019; Conference code: 157517}
}

@ARTICLE{Jiao2019312,
	author = {Jiao, Zhihao and Zhang, Lijun and Yuan, Chang-An and Qin, Xiao and Shang, Li},
	title = {Plant Leaf Recognition Based on Conditional Generative Adversarial Nets},
	year = {2019},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {11643 LNCS},
	pages = {312 – 319},
	doi = {10.1007/978-3-030-26763-6_30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070713491&doi=10.1007%2f978-3-030-26763-6_30&partnerID=40&md5=9c5f0f3fc3ac271dfbd5c689d29acc11},
	affiliations = {Institute of Machine Learning and Systems Biology, School of Electronics and Information Engineering, Tongji University, Shanghai, China; Collaborative Innovation Center of Intelligent New Energy Vehicle, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China; Science Computing and Intelligent Information Processing of GuangXi Higher Education Key Laboratory, Nanning Normal University, Nanning, Guangxi, China; Department of Communication Technology, College of Electronic Information Engineering, Suzhou Vocational University, Suzhou, 215104, Jiangsu, China},
	abstract = {Plants play an important role in human life, Identifying and protecting plants has far-reaching implications for the sustainable development of the ecological environment. Plant leaves can often reflect important characteristics of plants, so it is scientific and feasible to effectively identify plant species through plant leaves. With the rapid development of deep learning in recent years, it has been widely applied in plant leaf recognition. Compared with the traditional method, deep learning based plant leaf recognition algorithm can extract plant leaf features more effectively and can greatly improve the performance. Based on the detailed analysis of the structural characteristics of three classical convolutional neural network models, this paper comprehensively compares the recognition performance of three convolutional neural network models on ICL plant leaf datasets. The experimental results show that the Conditional Generative Adversarial Net with optimized output layer has better recognition results on the plant leaf dataset than other convolutional neural network models. © 2019, Springer Nature Switzerland AG.},
	author_keywords = {Convolutional neural network; Deep learning; Generative adversarial nets; Plant leaf recognition},
	keywords = {Computation theory; Convolution; Deep learning; Deep neural networks; Intelligent computing; Multilayer neural networks; Sustainable development; Convolutional neural network; Ecological environments; Generative adversarial nets; Output layer; Plant leaf; Plant leaves; Plant species; Structural characteristics; Plants (botany)},
	correspondence_address = {Z. Jiao; Institute of Machine Learning and Systems Biology, School of Electronics and Information Engineering, Tongji University, Shanghai, China; email: Jiaozhihao17@163.com},
	editor = {Huang D.-S. and Bevilacqua V. and Premaratne P.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-303026762-9},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 15th International Conference on Intelligent Computing, ICIC 2019; Conference date: 3 August 2019 through 6 August 2019; Conference code: 229289}
}

@CONFERENCE{Duong-Trung201983,
	author = {Duong-Trung, Nghia and Quach, Luyl-Da and Nguyen, Minh-Hoang and Nguyen, Chi-Ngon},
	title = {A combination of transfer learning and deep learning for medicinal plant classification},
	year = {2019},
	journal = {ACM International Conference Proceeding Series},
	volume = {Part F147957},
	pages = {83 – 90},
	doi = {10.1145/3321454.3321464},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066055340&doi=10.1145%2f3321454.3321464&partnerID=40&md5=8b9220b96c8d846ae7d86691102a0738},
	affiliations = {FPT University, Can Tho city, Viet Nam; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; College of Engineering Technology, Can Tho University, Can Tho city, Viet Nam},
	abstract = {Medicinal plants are an important element of indigenous medical systems in Viet Nam. These resources are usually regarded as a part of culture’s traditional knowledge. One of the prerequisites for any medical recommendation systems and/or applications is accurate identification and classification of medicinal plants. Hence, leveraging technology in automatic classification of these curative herbs has become essential. Unfortunately, building and training a machine learning model from scratch is next to impossible due to the lack of hardware infrastructure and finance support. It painfully restricts the requirements of rapid solutions to deal with the demand. For this purpose, this paper exploits the idea of transfer learning which is the improvement of learning in a new prediction task through the transferability of knowledge from a related prediction task that has already been learned. By utilizing state-of-the-art deep networks re-trained with our collected data, our extensive experiments show that the proposed combination performs perfectly and achieves the classification accuracy of 98.7% within the acceptable training time. Copyright ACM 2019.},
	author_keywords = {Deep learning; Medicinal plant classification; MobileNets; Transfer learning},
	keywords = {Knowledge management; Plants (botany); Transfer learning; Automatic classification; Classification accuracy; Machine learning models; Medicinal plants; MobileNets; Prediction tasks; State of the art; Traditional knowledge; Deep learning},
	publisher = {Association for Computing Machinery},
	isbn = {978-145036633-5},
	language = {English},
	abbrev_source_title = {ACM Int. Conf. Proc. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; Conference name: 4th International Conference on Intelligent Information Technology, ICIIT 2019; Conference date: 20 February 2019 through 23 February 2019; Conference code: 147957}
}

@ARTICLE{Amlekar2019127,
	author = {Amlekar, Manisha M. and Ali, Mouad M. H. and Gaikwad, Ashok T.},
	title = {Classification of plants using invariant features and a neural network},
	year = {2019},
	journal = {Smart Innovation, Systems and Technologies},
	volume = {107},
	pages = {127 – 136},
	doi = {10.1007/978-981-13-1747-7_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059101562&doi=10.1007%2f978-981-13-1747-7_13&partnerID=40&md5=739d21aca06361d029a0e853c950d75f},
	affiliations = {Dr. Babasaheb Ambedkar, Marathwada University, Aurangabad, Maharashtra, India; Information system & Technology Center, Hodeidah University, Hodeidah, Yemen; Institute of Management Studies and Information Technology, Aurangabad, Maharashtra, India},
	abstract = {This chapter presents leaf shape moment invariant features and a neural network approach to plant classification. Leaf image samples for plant species were processed in order to find leaf shape patterns. Further leaf shape moment invariant features were extracted and then, by using a regularized neural network, plant classification accuracy was studied. Leaf image samples of five different plants were taken for classification. Invariant features are significant for classifying plants with leaves of similar shapes. A regularized neural network was used for plant classification, based on leaf shape moment invariant features. The result of the neural network model was observed for invariant features and their combination with shape features. Eighteen shape features and seven of Hu’s invariant features were extracted from sample images of leaves from five different plant classes. © Springer Nature Singapore Pte Ltd. 2019.},
	author_keywords = {Leaf shape; Moment invariant features; Neural network; Plant classification},
	keywords = {Data communication systems; Image processing; Intelligent systems; Neural networks; Plants (botany); Invariant features; Leaf shape; Moment invariant; Neural network model; Plant classification; Plant species; Regularized neural networks; Shape features; Classification (of information)},
	correspondence_address = {M.M. Amlekar; Dr. Babasaheb Ambedkar, Marathwada University, Aurangabad, India; email: manishaak2012@gmail.com},
	editor = {Joshi A. and Satapathy S.C.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {21903018},
	isbn = {978-981131746-0},
	language = {English},
	abbrev_source_title = {Smart Innov. Syst. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Conference on Information and Communication Technology for Intelligent Systems, ICTIS 2018; Conference date: 6 April 2018 through 7 April 2018; Conference code: 222109}
}

@ARTICLE{George20192136,
	author = {George, Juby and Gladston Raj, S.},
	title = {Leaf identification using harris corner detection, SURF feature and FLANN matcher},
	year = {2019},
	journal = {International Journal of Innovative Technology and Exploring Engineering},
	volume = {8},
	number = {11},
	pages = {2136 – 2143},
	doi = {10.35940/ijitee.K2016.0981119},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073394058&doi=10.35940%2fijitee.K2016.0981119&partnerID=40&md5=997ee85168aa00c29565e82d5bbeb4b6},
	affiliations = {School of Computer Science, Mahatma Gandhi University, Kottayam, 686560, India; Department of Computer Science, Govt. College Nedumangad, Thiruvananthapuram, Kerala, India},
	abstract = {Leaf Recognition is very important in agriculture for identification of plants. Leaves of various plants have unique characteristics which are to be used for categorization. Out of the different features, leaf vein is one of the prominent biometric feature. Extracting leaf vein and perform classification based on these features leads to more accurate identification of plants. In practice, due to change in various lighting conditions and orientations, the extraction of leaf vein becomes difficult. This work focuses on extracting veins using ridge orientation and frequency estimation using region mask which brings out good quality vein structure under different conditions. The vein structure thus obtained is used for identifying keypoints using Harris corner detector. Features are extracted from the keypoints using SURF feature extraction method and finally the trained and query images are compared to identify the correct leaf species using FLANN matcher. Flavia leaf image database with 32 different species are used and an accuracy of 98.75% was resulted. The proposed methodology can be used for plant leaf identification in real world for identifying medicinal plants and other category of plants. This method can be used for identifying veins of dry leaves which can further extract the features and identify the species. © BEIESP.},
	author_keywords = {FLANN matcher; Frequency estimation; Harris Corner detector; Region mask; SURF Feature Descriptor},
	publisher = {Blue Eyes Intelligence Engineering and Sciences Publication},
	issn = {22783075},
	language = {English},
	abbrev_source_title = {Int. J. Innov. Technol. Explor. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Shao2019102,
	author = {Shao, Yu},
	title = {Supervised global-locality preserving projection for plant leaf recognition},
	year = {2019},
	journal = {Computers and Electronics in Agriculture},
	volume = {158},
	pages = {102 – 108},
	doi = {10.1016/j.compag.2019.01.022},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060932245&doi=10.1016%2fj.compag.2019.01.022&partnerID=40&md5=7728d53f71e26f50e83c588e36405fee},
	affiliations = {School of Electronic Information Engineering, Zhengzhou SIAS University, 451150, China},
	abstract = {Plant leaf-based species recognition is still a challenge due to the large intra-class differences and the inter-class similarity of nature plant leaves. A new manifold learning method namely supervised global-locality preserving projection (SGLP) is proposed for plant leaf recognition, including three stages. First, construct the local weighted inter-class and intra-class scatter matrices by local information and class information of the training samples, and then construct a global weighted inter-scatter matrix, finally design a multi-objection optimal solution to enhance the compactness of the intra-class points and inter-class-manifold separability. Compared with the classical manifold-based plant recognition methods, global weighted inter-scatter matrix is constructed to enlarge the distance between different classes of the data and then effectively reveal the intrinsic manifold structure for classification. Experiments on the ICL and Swedish leaf datasets validate that the proposed SGLP algorithm obtains higher classification results than other state-of-the-art methods. © 2019 Elsevier B.V.},
	author_keywords = {Leaf recognition; Locality preserving discriminant projections (LPDP); Manifold learning; supervised global-locality preserving projection (SGLP)},
	keywords = {Classification (of information); Structure (composition); Classification results; Leaf recognition; Locality preserving projections; Locality-preserving; Manifold learning; Manifold structures; Species recognition; State-of-the-art methods; algorithm; design method; discriminant analysis; identification method; information technology; leaf; machine learning; matrix; plant; Plants (botany)},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17}
}

@CONFERENCE{Pang2019,
	author = {Pang, Po Ken and Lim, King Hann},
	title = {Review on Automatic Plant Identification Using Computer Vision Approaches},
	year = {2019},
	journal = {IOP Conference Series: Materials Science and Engineering},
	volume = {495},
	number = {1},
	doi = {10.1088/1757-899X/495/1/012032},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067869289&doi=10.1088%2f1757-899X%2f495%2f1%2f012032&partnerID=40&md5=3ae1bf42cac47a36f76580a777ec208b},
	affiliations = {Department of Electrical and Computer Engineering Curtin University Malaysia, Sarawak, Miri, 98009, Malaysia},
	abstract = {Plants are crucial resources on the Earth for ecological living habitat. However, the rapid loss of plant species has alerted the globe with the rising awareness of biodiversity conservation. ReThe need of plant identification provides an essential biologist information for plant research and development. It has brought significant impact on environmental conservation and exploration. Nevertheless, it requires species identification skills, high time consumption on study the species and usage of specific botanical terms. The knowledge of plant identification is not only for botanist and plant ecologists, but it is also useful for society, from professionals to the general public. The challenges of plant identification is the complexity of gaining plant species knowledge. Currently, with relevant technologies (digital cameras, mobile devices and remote access to databases) and computer vision techniques, it have created an automated plant identification to ease the society in plant identification. The aim of this paper is to document an analysis and comparison of studies between two types computer vision approaches for plant species identification and the features, i.e., shape, texture, colour, margin, and vein structure. It is useful to researchers in the fields for ongoing researches and comparable analyses of applied methods. © Published under licence by IOP Publishing Ltd.},
	keywords = {Biodiversity; Conservation; Digital devices; Ecology; Mobile telecommunication systems; Textures; Biodiversity conservation; Computer vision techniques; Environmental conservation; Plant identification; Plant species identification; Research and development; Species identification; Time consumption; Computer vision},
	editor = {Prasanna M.V. and Chong L.H. and Nagarajan R. and Lau H.H.},
	publisher = {Institute of Physics Publishing},
	issn = {17578981},
	language = {English},
	abbrev_source_title = {IOP Conf. Ser. Mater. Sci. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 11th Curtin University Technology, Science and Engineering International Conference, CUTSE 2018; Conference date: 26 November 2018 through 28 November 2018; Conference code: 148783; All Open Access, Bronze Open Access}
}

@CONFERENCE{Cheng2019,
	author = {Cheng, Hsueh-Hung and Ke, Yan-Ling and Lin, Chu-Ping and Huang, Jin-Hsing and Chen, Shih-Fang and Kuo, Yan-Fu},
	title = {Identifying and localizing the disease spots of late blight on tomato leaves using deep convolutional neural networks},
	year = {2019},
	journal = {2019 ASABE Annual International Meeting},
	doi = {10.13031/aim.201900445},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084011591&doi=10.13031%2faim.201900445&partnerID=40&md5=8e7d654e1ecfc56ed4cd986272bb1d0c},
	affiliations = {Department of Bio-Industrial Mechatronics Engineering, National Taiwan University, Taiwan; Plant Pathology Division, Taiwan Agricultural Research Institute Council of Agriculture, Executive Yuan, Taiwan},
	abstract = {Tomato late blight is an infamous disease due to causing severe tomato yield loss. Phytophthora infestans, the causal pathogen of tomato late blight, could disseminate to all the cultivation regions in a suitable weather condition and destroy all the crop in weeks. In order to prevent severe disease spreading, early symptom identification of the disease is important to take actions for disease control. Late blight symptoms include from irregularly shaped water-soaked to brown necrotic lesions on plant leaves and stems. Conventionally, the identification of late blight deeply relies on the experience of tomato farmers. However, the symptoms of late blight might be confused with the atypical symptoms and lesions of some other diseases, confusing not only the well-experienced farmers but also the inexperienced plant pathologists. This study proposed to identify tomato late blight using leaf images and deep learning. A Navigator-teacher-scrutinizer network (NTS-Net) was developed to localize and identify the putative late blight lesions of tomato leaves. The developed NTS-Net model achieved a mean accuracy of 99.76% in diseased and healthy plant identification and also achieved a precision of 50% in lesions localization. © 2019 ASABE Annual International Meeting. All rights reserved.},
	author_keywords = {Deep learning; Disease identification; Navigator-teacher-scrutinizer network; Tomato late blight},
	keywords = {Deep learning; Deep neural networks; Disease control; Fruits; Neural networks; Convolutional neural network; Disease spreading; Late blight; Phytophthora infestans; Plant identification; Plant leaves; Tomato leaves; Tomato yield; Plants (botany)},
	publisher = {American Society of Agricultural and Biological Engineers},
	language = {English},
	abbrev_source_title = {ASABE Annu. Int. Meet.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2019 ASABE Annual International Meeting; Conference date: 7 July 2019 through 10 July 2019; Conference code: 151393}
}

@ARTICLE{Paco Ramos2019143,
	author = {Paco Ramos, Mery M. and Paco Ramos, Vanessa M. and Fabian, Arnold Loaiza and Osco Mamani, Erbert F.},
	title = {A Feature Extraction Method Based on Convolutional Autoencoder for Plant Leaves Classification},
	year = {2019},
	journal = {Communications in Computer and Information Science},
	volume = {1096 CCIS},
	pages = {143 – 154},
	doi = {10.1007/978-3-030-36211-9_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078478181&doi=10.1007%2f978-3-030-36211-9_12&partnerID=40&md5=79e457a72c0b1901cad9d446b0ca01d0},
	affiliations = {Universidad Nacional Jorge Basadre Grohmann, Tacna, Peru},
	abstract = {In this research, we present an approach based on Convolutional Autoencoder (CAE) and Support Vector Machine (SVM) for leaves classification of different trees. While previous approaches relied on image processing and manual feature extraction, the proposed approach operates directly on the image pixels, without any preprocessing. Firstly, we use multiple layers of CAE to learn the features of leaf image dataset. Secondly, the extracted features were used to train a linear classifier based on SVM. Experimental results show that the classifiers using these features can improve their predictive value, reaching an accuracy rate of 94.74%. © Springer Nature Switzerland AG 2019.},
	author_keywords = {Computer vision; Convolutional Autoencoder; Deep learning; Feature extraction; Image processing; Plant classification},
	keywords = {Artificial intelligence; Computer vision; Convolution; Deep learning; Extraction; Feature extraction; Plants (botany); Support vector machines; Accuracy rate; Auto encoders; Feature extraction methods; Linear classifiers; Multiple layers; Plant classification; Plant leaves classifications; Predictive values; Image processing},
	correspondence_address = {M.M. Paco Ramos; Universidad Nacional Jorge Basadre Grohmann, Tacna, Peru; email: mpacor@unjbg.edu.pe},
	editor = {Orjuela-Cañón A.D. and Figueroa-García J.C. and Arias-Londoño J.D.},
	publisher = {Springer},
	issn = {18650929},
	isbn = {978-303036210-2},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2nd IEEE Colombian Conference on Applications in Computational Intelligence, ColCACI 2019; Conference date: 5 June 2019 through 7 June 2019; Conference code: 235709}
}

@CONFERENCE{Kazerouni2019279,
	author = {Kazerouni, Masoud Fathi and Saeed, Nazeer T. Mohammed and Kuhnert, Klaus-Dieter},
	title = {Exploration of Autonomous Mobile Robots through Challenging Outdoor Environments for Natural Plant Recognition Using Deep Neural Network},
	year = {2019},
	journal = {Proceedings - 2019 IEEE 15th International Conference on Intelligent Computer Communication and Processing, ICCP 2019},
	pages = {279 – 285},
	doi = {10.1109/ICCP48234.2019.8959784},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079237668&doi=10.1109%2fICCP48234.2019.8959784&partnerID=40&md5=10a29bf0b7c8559966fa2276af60f741},
	affiliations = {Institute of Real-time Learning Systems, University of Siegen, Siegen, Germany},
	abstract = {Modernization of living environments and human activities have severe effects on many parameters and factors such as climate change and global warming, an increase of incidence and the severity of wildfires, land surfaces and ice sheets, ecological imbalance, change of fertility of the soil, flow of energy, food security, etc. In addition, human modernization has had a negative impact on biodiversity and the natural environment. An integral component of modernization is agriculture that associates with the outdoor environment and relevant issues. Automation of agricultural activities contributes to reducing the dependency on human labor and the harmful effects on the natural environment. The correct identification of plants in outdoor environments has been neglected and many critical environmental and non-environmental limits and factors, such as weather conditions, time, viewpoint, lighting conditions (illuminations and light intensity), distance, etc., have not been considered in existing plant recognition systems and technologies. Hence, there is a demand to develop mobile and real-time systems for plant recognition in natural environments. This paper addresses these challenges and introduces the application of autonomous mobile robot and semi-robots for recognition of natural plant species in outdoor environments. Furthermore, the proposed system presents the use of employing low-cost cameras, such as iPhone 6s, Canon EOS 600D and Samsung Galaxy Note 4, for plant recognition system in real-time. The performance of the system has been tested with a number of experiments in different years, 2017 and 2018, and at different times of day, morning and evening. The proposed system is a combination of new technologies involving deep learning concepts and an autonomous field robot to carry out precise plant recognition in challenging environments. The final accuracy of the mobile real-time system is 84.1666%. © 2019 IEEE.},
	author_keywords = {Agricultural Robot; Autonomous Mobile Robot; Deep Learning; Intelligent Agriculture Robotics; Natural Plant Recognition},
	keywords = {Agricultural robots; Agriculture; Biodiversity; Deep learning; Deep neural networks; Environmental technology; Food supply; Global warming; Intelligent robots; Interactive computer systems; Mobile robots; Modernization; Navigation; Agricultural activities; Autonomous Mobile Robot; Integral components; Lighting conditions; Living environment; Natural environments; Natural plants; Outdoor environment; Real time systems},
	editor = {Nedevschi S. and Potolea R. and Slavescu R.R.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172814914-1},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. Intell. Comput. Commun. Process., ICCP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 15th IEEE International Conference on Intelligent Computer Communication and Processing, ICCP 2019; Conference date: 5 September 2019 through 7 September 2019; Conference code: 156916}
}

@ARTICLE{Turkoglu20191,
	author = {Turkoglu, Muammer and Hanbay, Davut},
	title = {Recognition of plant leaves: An approach with hybrid features produced by dividing leaf images into two and four parts},
	year = {2019},
	journal = {Applied Mathematics and Computation},
	volume = {352},
	pages = {1 – 14},
	doi = {10.1016/j.amc.2019.01.054},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060939399&doi=10.1016%2fj.amc.2019.01.054&partnerID=40&md5=98147fd6eb80d13e2c948a539b64f99a},
	affiliations = {Department of Computer Engineering, Faculty of Engineering, Bingöl University, Bingöl, 12000, Turkey; Department of Computer Engineering, Faculty of Engineering, Inönü University, Malatya, 44280, Turkey},
	abstract = {Plants play a crucial role in the lives of all living things. A risk of extinction exists for many plants, hence many botanists and scientists are working in order to protect plants and plant diversity. Plant identification is the most important part of studies carried out for this purpose. In order to identify plants more accurately, different approaches have been used in the studies to date. One of these approaches is plant identification through leaf recognition, and is the basis of many conducted studies. It can be used for automatic plant recognition in the area of botany, the food sector, industry, medicine, and in many more areas too. In this study, image processing based on feature extraction methods such as color features, vein features, Fourier Descriptors (FD), and Gray-Level Co-occurrence Matrix (GLCM) methods are used. This study suggests the use of features extracted from leaves divided into two or four parts, instead of extracting for the whole leaf. Both the individual and combined performances of each feature extraction method are calculated by Extreme Learning Machines (ELM) classifier. The suggested approach has been applied to the Flavia leaf dataset. 10-fold cross-validation was used to evaluate the accuracy of the proposed method, which was then compared and tabulated with methods from other studies. The evaluated accuracy of the proposed method on the Flavia leaf dataset was calculated as 99.10%. © 2019 Elsevier Inc.},
	author_keywords = {ELM; Hybrid features; Image processing; Leaf recognition; Section process},
	keywords = {Extraction; Feature extraction; Learning systems; Plants (botany); 10-fold cross-validation; Extreme learning machine; Feature extraction methods; Fourier descriptors; Gray level co occurrence matrix(GLCM); Hybrid features; Leaf recognition; Plant identification; Image processing},
	correspondence_address = {M. Turkoglu; Department of Computer Engineering, Faculty of Engineering, Bingöl University, Bingöl, 12000, Turkey; email: mturkoglu@bingol.edu.tr},
	publisher = {Elsevier Inc.},
	issn = {00963003},
	coden = {AMHCB},
	language = {English},
	abbrev_source_title = {Appl. Math. Comput.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 43}
}

@ARTICLE{Rzanny2019,
	author = {Rzanny, Michael and Mäder, Patrick and Deggelmann, Alice and Chen, Minqian and Wäldchen, Jana},
	title = {Flowers, leaves or both? How to obtain suitable images for automated plant identification},
	year = {2019},
	journal = {Plant Methods},
	volume = {15},
	number = {1},
	doi = {10.1186/s13007-019-0462-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069762650&doi=10.1186%2fs13007-019-0462-4&partnerID=40&md5=1f92639a16f0d196df6c77e69001ba60},
	affiliations = {Department of Biogeochemical Integration, Max Planck Institute for Biogeochemistry, Hans-Knöll-Str. 10, Jena, Germany; Software Engineering for Safety-Critical Systems Group, Technische Universität Ilmenau, Ehrenbergstr. 20, Ilmenau, 98693, Germany},
	abstract = {Background: Deep learning algorithms for automated plant identification need large quantities of precisely labelled images in order to produce reliable classification results. Here, we explore what kind of perspectives and their combinations contain more characteristic information and therefore allow for higher identification accuracy. Results: We developed an image-capturing scheme to create observations of flowering plants. Each observation comprises five in-situ images of the same individual from predefined perspectives (entire plant, flower frontal- and lateral view, leaf top- and back side view). We collected a completely balanced dataset comprising 100 observations for each of 101 species with an emphasis on groups of conspecific and visually similar species including twelve Poaceae species. We used this dataset to train convolutional neural networks and determine the prediction accuracy for each single perspective and their combinations via score level fusion. Top-1 accuracies ranged between 77% (entire plant) and 97% (fusion of all perspectives) when averaged across species. Flower frontal view achieved the highest accuracy (88%). Fusing flower frontal, flower lateral and leaf top views yields the most reasonable compromise with respect to acquisition effort and accuracy (96%). The perspective achieving the highest accuracy was species dependent. Conclusions: We argue that image databases of herbaceous plants would benefit from multi organ observations, comprising at least the front and lateral perspective of flowers and the leaf top view. © 2019 The Author(s).},
	author_keywords = {Computer vision; Convolutional networks; Deep learning; Flower; Multi-organ plant classification; Object classification; Plant determination; Plant images; Plant leaf; Plant observation; Poaceae; Species identification},
	correspondence_address = {M. Rzanny; Department of Biogeochemical Integration, Max Planck Institute for Biogeochemistry, Jena, Hans-Knöll-Str. 10, Germany; email: mrzanny@bgc-jena.mpg.de},
	publisher = {BioMed Central Ltd.},
	issn = {17464811},
	language = {English},
	abbrev_source_title = {Plant Methods},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 39; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Raut20191579,
	author = {Raut, Swati P. and Bhalchandra, A.S.},
	title = {Plant Recognition System Based on Leaf Image},
	year = {2019},
	journal = {Proceedings of the 2nd International Conference on Intelligent Computing and Control Systems, ICICCS 2018},
	pages = {1579 – 1581},
	doi = {10.1109/ICCONS.2018.8663028},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063768787&doi=10.1109%2fICCONS.2018.8663028&partnerID=40&md5=0dd18ba596f2e410a93c35cd5f3cf5e9},
	affiliations = {Dept. Electronics, Government College of Engineering, Aurangabad, Maharashtra, India},
	abstract = {Nature has huge members of plants identifying them and classifying them is important task for botanists. Usually plants are recognized by leaf and its characteristics like shape, texture, vein structure, color etc. A system is developed which recognizes plants automatically based on leaf structure using image processing. Moreover evolutionary changes are also taking place in plants and it has impact on identification and classification. Image data base and related information is stored on cloud. Shape measure include area, perimeter, ratio of MajorAxisLength and MinorAxisLength, vein structure indicating angle of sub veins with major vein, texture are the major feature used. Botanists visit remote places, jungles where the plants need to be identified and classified. So an attempt is made to develop an automatic identification system where in an image of leaf is captured by any smart phone, uploaded on cloud, where image available and complete data base is trained. Captured image is processed features will be extracted and gives to classifies the plant classification result will be transmitted back to smart phone and related information. © 2018 IEEE.},
	author_keywords = {digital signature; leaf pattern; machine learning},
	keywords = {Automation; Classification (of information); Control systems; Electronic document identification systems; Intelligent computing; Learning systems; Smartphones; Textures; Automatic identification system; Evolutionary changes; Image database; leaf pattern; Leaf structure; Plant classification; Plant recognition; Shape measure; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153862842-3},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Intell. Comput. Control Syst., ICICCS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 2nd International Conference on Intelligent Computing and Control Systems, ICICCS 2018; Conference date: 14 June 2018 through 15 June 2018; Conference code: 146012}
}

@ARTICLE{Turkoglu2019,
	author = {Turkoglu, Muammer and Hanbay, Davut},
	title = {Leaf-based plant species recognition based on improved local binary pattern and extreme learning machine},
	year = {2019},
	journal = {Physica A: Statistical Mechanics and its Applications},
	volume = {527},
	doi = {10.1016/j.physa.2019.121297},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065520212&doi=10.1016%2fj.physa.2019.121297&partnerID=40&md5=17997f83e99f711ff0f4fa210cf881e5},
	affiliations = {Computer Engineering Department, Engineering Faculty, Bingol University, 12000, Bingol, Turkey; Computer Engineering Department, Engineering Faculty, Inonu University, 44280, Malatya, Turkey},
	abstract = {Over the past 15 years, many feature extraction methods have been used and developed for the recognition of plant species. These methods have mostly been performed using separation operations from the background based on a pre-processing stage. However, the Local Binary Patterns (LBP)method, which provides high performance in object recognition, is used to obtain textural features from images without need for a pre-processing stage. In this paper, we propose different approaches based on LBP for the recognition of plant leaves using extracted texture features from plant leaves. While the original LBP converts color images to gray tones, the proposed methods are applied by using the R and G color channel of images. In addition, we evaluate the robustness of the proposed methods against noise such as salt & pepper and Gaussian. Later, the obtained features from the proposed methods were classified and tested using the Extreme Learning Machine (ELM)method. The experimental works were performed using various plant leaf datasets such as Flavia, Swedish, ICL, and Foliage. According to the obtained performance results, the calculated accuracy values for Flavia, Swedish, ICL and Foliage datasets were 98.94%, 99.46%, 83.71%, and 92.92%, respectively. The results demonstrate that the proposed method was more successful when compared to the original LBP, improved LBP methods, and other image descriptors for both noisy and noiseless images. © 2019 Elsevier B.V.},
	author_keywords = {Extreme learning machine; Image descriptors; Local binary pattern; Plant classification; Region-overall LBP},
	keywords = {Knowledge acquisition; Machine learning; Object recognition; Plants (botany); Textures; Extreme learning machine; Image descriptors; Local binary patterns; Plant classification; Region-overall LBP; Image enhancement},
	correspondence_address = {M. Turkoglu; Computer Engineering Department, Engineering Faculty, Bingol University, 12000, Bingol, Turkey; email: mturkoglu@bingol.edu.tr},
	publisher = {Elsevier B.V.},
	issn = {03784371},
	coden = {PHYAD},
	language = {English},
	abbrev_source_title = {Phys A Stat Mech Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 39}
}

@CONFERENCE{Ozdil2019457,
	author = {Ozdil, Omer and Esin, Yunus Emre and Demirel, Berkan and Ozturk, Safak},
	title = {Generating spectral signature library for patterned object in hyperspectral images},
	year = {2019},
	journal = {Proceedings of 9th International Conference on Recent Advances in Space Technologies, RAST 2019},
	pages = {457 – 460},
	doi = {10.1109/RAST.2019.8767808},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073908653&doi=10.1109%2fRAST.2019.8767808&partnerID=40&md5=3241d2e281fda0481d21d13da2f3589d},
	affiliations = {Image and Video Processing Group, HAVELSAN Inc., Ankara, Turkey},
	abstract = {Small objects cover within a few pixels or even cover subpixels due to the low spatial resolution of hyperspectral cameras. This factor significantly reduces the detection performance for multi-pattern objects. For this reason, it is important to obtain signatures that can model the target in best way while creating spectral library signatures. In this study, a representative signature generation method for the patterned objects is proposed in order to determine the target in hyperspectral aerial images. The target detection performance, which was calculated by using the pattern signatures of the object is compared with the target detection performance using the Mean Signatures obtained by the average of the signatures from a selected area. In addition, the target detection performance of the average signatures obtained from randomly selected areas are also compared. According to the results, it is observed that the target detection performance of the Mean Signatures is higher than that of the the pattern signatures. © 2019 IEEE.},
	author_keywords = {crop detection; hyperspectral image processing; plant classification; spectral library},
	keywords = {Antennas; Hyperspectral imaging; Pixels; Detection performance; Hyper-spectral cameras; Pattern signature; Plant classification; Signature generation; Spatial resolution; Spectral libraries; Spectral signature; Spectroscopy},
	editor = {Menekay S. and Cetin O. and Alparslan O.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153869448-0},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Recent Adv. Space Technol., RAST},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 9th International Conference on Recent Advances in Space Technologies, RAST 2019; Conference date: 11 June 2019 through 14 June 2019; Conference code: 149874}
}

@ARTICLE{Kumar2019163912,
	author = {Kumar, Munish and Gupta, Surbhi and Gao, Xiao-Zhi and Singh, Amitoj},
	title = {Plant Species Recognition Using Morphological Features and Adaptive Boosting Methodology},
	year = {2019},
	journal = {IEEE Access},
	volume = {7},
	pages = {163912 – 163918},
	doi = {10.1109/ACCESS.2019.2952176},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075622487&doi=10.1109%2fACCESS.2019.2952176&partnerID=40&md5=d739a2c1a416b887ba4f20451943b47f},
	affiliations = {Department of Computational Sciences, Maharaja Ranjit Singh Punjab Technical University, Bathinda, 151001, India; Department of Computer Science and Engineering, Gokaraju Rangaraju Institute of Engineering and Technology, Hyderabad, 500090, India; School of Computing, University of Eastern Finland, Kuopio, 70211, Finland},
	abstract = {Plant species detection aims at the automatic identification of plants. Although a lot of aspects like leaf, flowers, fruits, seeds could contribute to the decision, but leaf features are the most significant. As a plant leaf is always more accessible as compared to other parts of the plants, it is obvious to study it for plant identification. The present paper introduced a novel plant species classifier based on the extraction of morphological features using a Multilayer Perceptron with Adaboosting. The proposed framework comprises pre-processing, feature extraction, feature selection, and classification. Initially, some pre-processing techniques are used to set up a leaf image for the feature extraction process. Various morphological features, i.e., centroid, major axis length, minor axis length, solidity, perimeter, and orientation are extracted from the digital images of various categories of leaves. Different classifiers, i.e., k-NN, Decision Tree and Multilayer perceptron are employed to test the accuracy of the algorithm. AdaBoost methodology is explored for improving the precision rate of the proposed system. Experimental results are obtained on a public dataset (FLAVIA) downloaded from http://flavia.sourceforge.net/. A precision rate of 95.42% has been achieved using the proposed machine learning classifier, which outperformed the state-of-the-art algorithms. © 2013 IEEE.},
	author_keywords = {AdaBoost; decision tree; feature extraction; k-NN; Leaf recognition; multilayer perceptron; plant leaf classification; plant species identification},
	keywords = {Adaptive boosting; Automation; Decision trees; Extraction; Feature extraction; Machine learning; Multilayer neural networks; Multilayers; Nearest neighbor search; Plants (botany); Trees (mathematics); Automatic identification; Leaf recognition; Major axis lengths; Morphological features; Plant identification; Plant leaf classifications; Plant species identification; State-of-the-art algorithms; Image processing},
	correspondence_address = {M. Kumar; Department of Computational Sciences, Maharaja Ranjit Singh Punjab Technical University, Bathinda, 151001, India; email: munishcse@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 51; All Open Access, Gold Open Access}
}

@ARTICLE{Harjanti2019163,
	author = {Harjanti, Trinugi Wira and Madenda, Sarifuddin},
	title = {Development of feature extraction on leaf image for medicinal plants identification},
	year = {2019},
	journal = {International Journal of Recent Technology and Engineering},
	volume = {8},
	number = {2 Special Issue 7},
	pages = {163 – 168},
	doi = {10.35940/ijrte.B1040.0782S719},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074681244&doi=10.35940%2fijrte.B1040.0782S719&partnerID=40&md5=c08da84348511c5261b343a23dc9318e},
	affiliations = {Information System, STTI NIIT, 13770, Indonesia; Computer Science & Information Technology, Gunadarma University, 16424, Indonesia},
	abstract = {The leaf image identification process depends on the feature extraction results. Each medicinal plant has different shapes and patterns of leaf venation. But for one type of medicinal plants have the same pattern of venation shape and pattern even though the size is different. One of the methods for extraction of leaf image form characteristics is by fractal-based feature extraction. Through fractal can be calculated the value of leaf dimensions and searched parts of leaves that have similarities between one part with other parts. As for the method of extracting the characteristics of venation pattern using B-Spline method.Benefits of research conducted is to help people identifying the types of medicinal plants found, knowing the benefits and ways of brewing. While the research contribution is prototype software application based on information technology that can be used by the people through mobile phones for the identification of medicinal plants. To identify or match the results of feature extraction on the leaf found whether included in the medicinal plant, conducted by Euclidean Distance method. In the experiments we used 1100 data consist of 55 variety of medicinal plants for each 20 samples.The experimental result show that the accuracy of identification using of fractal and b-spline is 85.30%. © BEIESP.},
	author_keywords = {B-spline; Euclidean distance; Feature extraction; Fractal; Medicinal plant identification},
	publisher = {Blue Eyes Intelligence Engineering and Sciences Publication},
	issn = {22773878},
	language = {English},
	abbrev_source_title = {Int. J. Recent Technol. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Bronze Open Access}
}

@CONFERENCE{Ganschow20191855,
	author = {Ganschow, L. and Thiele, T. and Deckers, N. and Reulke, R.},
	title = {Classification of tree species on the basis of tree bark texture},
	year = {2019},
	journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
	volume = {42},
	number = {2/W13},
	pages = {1855 – 1859},
	doi = {10.5194/isprs-archives-XLII-2-W13-1855-2019},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067507501&doi=10.5194%2fisprs-archives-XLII-2-W13-1855-2019&partnerID=40&md5=e04f46581544c079339e7a90810071e6},
	affiliations = {VINS 3D GmbH, Wegedornstrasse 32, Berlin, 12524, Germany; Humboldt-Universität zu Berlin, Unter den Linden 6, Berlin, 10099, Germany},
	abstract = {Forest inventory is an important topic in forestry and a digital solution which works on the basis of tree images is looked for. Implementing a system which automatically classifies tree species is the overall goal. In this paper the implementation of a convolutional neural net for solving this classification problem is executed and evaluated. The objective is creating a system which works well on unseen data and deriving guidelines and constraints to guarantee good accuracy results. Images including tree segmentation and the corresponding labels are provided as training data. The tree species classification takes the segmentation results of a stereo vision based image segmentation algorithm as input. The basic idea consists of cropping the tree images into quadratic boxes before feeding them into the neural net. First, each box is classified separately and then the results are evaluated to get a classification for the whole tree. Methods for result improvement include altering box size, using overlapping boxes, artificially enlarging the training set, pretraining and finetuning. Cropping a tree image into boxes of a specific size and accumulating the single results to get a classification of the whole tree leads to an accuracy of 96.7% provided that specific constraints like minimum box number and the projected size of the tree on image plane are considered. Finally, ways to further improve performance are pointed out. © Authors 2019.},
	author_keywords = {Convolutional Neural Network; Forest Inventory; Image Segmentation; Integrated Positioning System; Plant Classification; Pretraining and Finetuning},
	keywords = {Convolution; Forestry; Navigation systems; Neural networks; Stereo image processing; Stereo vision; Textures; Convolutional neural network; Forest inventory; Integrated positioning system; Plant classification; Pre-training; Image segmentation},
	correspondence_address = {R. Reulke; Humboldt-Universität zu Berlin, Berlin, Unter den Linden 6, 10099, Germany; email: ralf.reulke@hu-Berlin.de},
	editor = {Vosselman G. and Oude Elberink S.J. and Yang M.Y.},
	publisher = {International Society for Photogrammetry and Remote Sensing},
	issn = {16821750},
	language = {English},
	abbrev_source_title = {Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 4th ISPRS Geospatial Week 2019; Conference date: 10 June 2019 through 14 June 2019; Conference code: 148406; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Al-Qurran2019,
	author = {Al-Qurran, Raffi and Al-Ayyoub, Mahmoud and Shatnawi, Ali},
	title = {Plant Classification in the Wild: A Transfer Learning Approach},
	year = {2019},
	journal = {ACIT 2018 - 19th International Arab Conference on Information Technology},
	doi = {10.1109/ACIT.2018.8672694},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064107961&doi=10.1109%2fACIT.2018.8672694&partnerID=40&md5=438d3deabcae35fd1373b2d01a48fbf0},
	affiliations = {Jordan University of Science and Technology, Irbid, Jordan},
	abstract = {Datasets specialized in wildlife usually contain imbalanced classes of natural wild images such as, for instance, plant images, which are acquired from the surrounding environment with natural scene background. Deep neural networks have proven their efficiency in classifying such datasets. However, such an approach requires a workaround to approximately balance the classes in order to prevent the occurrence of overfitting during the training phase of the neural network. Many approaches exist to overcome this problem includes over-sampling, undersampling, generating synthetic samples, data augmentation, etc. The iNaturalist species classification and detection dataset represents a good example of vastly imbalanced datasets. It contains 13 superclasses. This work focuses on the Plantae superclass and builds a Convolutional Neural Network to distinguish a subset of the subclasses of Plantae. Our model benefits from cutting-edge techniques such as transfer learning and data augmentation to obtain a reasonably high level of accuracy (78.76%). © 2018 IEEE.},
	author_keywords = {Convolutional Neural Networks; Data Augmentation; Deep Learning; Plant classification; Transfer Learning},
	keywords = {Convolution; Deep learning; Deep neural networks; Neural networks; Convolutional neural network; Data augmentation; Imbalanced class; Imbalanced Data-sets; Plant classification; Species classification; Surrounding environment; Transfer learning; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172810385-3},
	language = {English},
	abbrev_source_title = {ACIT - Int. Arab Conf. Inf. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 19th International Arab Conference on Information Technology, ACIT 2018; Conference date: 28 November 2018 through 30 November 2018; Conference code: 146532}
}

@ARTICLE{Song2019163277,
	author = {Song, Yupeng and He, Fazhi and Zhang, Xiying},
	title = {To Identify Tree Species with Highly Similar Leaves Based on a Novel Attention Mechanism for CNN},
	year = {2019},
	journal = {IEEE Access},
	volume = {7},
	pages = {163277 – 163286},
	doi = {10.1109/ACCESS.2019.2951607},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075796513&doi=10.1109%2fACCESS.2019.2951607&partnerID=40&md5=62452363258d94eeb734999caf3e01bf},
	affiliations = {School of Computer Science, Wuhan University, Wuhan, 430072, China; School of Information and Computer Engineering, Northeast Forestry University, Harbin, 150040, China},
	abstract = {Image identification technology has great significance for forestry production and forestry management. Highly similar object identification tasks, such as tree species with similar leaves, are extremely challenging. Simply using typical Convolutional Neural Networks (CNNs) or simply adding more convolutional layers still performs poorly in the above tasks. In this paper, we present a novel attention mechanism to enhance the CNN for identification of tree species with highly similar leaves. This paper presents a highly discriminative network, namely attention branch based convolutional neural networks (ABCNN), to better distinguish the differences between leaves features. Firstly, we proposed a novel structure, in which an attention branch is added in all block layers of network besides the typical normal branch. Secondly, our attention branch adopts a condensation process to obtain a region of interest (ROI) from global information of input and designs a reconstruction process to amplify the features difference to focus on the ROI. Thirdly, we design a fusion process, which carefully combines the attention branch with a normal branch to improve the network performance in the training process. The proposed ABCNN is tested on special dataset of Leafsnap with highly similar tree leaves. Our approach achieved 91.43% classification accuracy, which is higher than previous methods. Furthermore, ABCNN is also tested on general data set of SVHN and obtains 98.27% classification accuracy, which is the most competitive when considering the lower computational resources for ordinary applications. Both above experiments demonstrate the discrimination and robustness of the proposed method. © 2013 IEEE.},
	author_keywords = {attention mechanism; convolutional neural network; deep learning; highly similar leaves; image classification; Tree species identification},
	keywords = {Classification (of information); Convolution; Deep learning; Deep neural networks; Forestry; Image classification; Image segmentation; Network layers; Timber; Attention mechanisms; Classification accuracy; Computational resources; Convolutional neural network; Discriminative networks; highly similar leaves; Reconstruction process; Tree species identifications; Multilayer neural networks},
	correspondence_address = {F. He; School of Computer Science, Wuhan University, Wuhan, 430072, China; email: fzhe@whu.edu.cn},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@CONFERENCE{Mamani Diaz20199,
	author = {Mamani Diaz, Carlos A. and Medina Castaneda, Edgar E. and Mugruza Vassallo, Carlos A.},
	title = {Deep Learning for Plant Classification in Precision Agriculture},
	year = {2019},
	journal = {2019 International Conference on Computer, Control, Informatics and its Applications: Emerging Trends in Big Data and Artificial Intelligence, IC3INA 2019},
	pages = {9 – 13},
	doi = {10.1109/IC3INA48034.2019.8949612},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078845969&doi=10.1109%2fIC3INA48034.2019.8949612&partnerID=40&md5=ffa0732543f3713a9e876bbba26d5314},
	affiliations = {South Lima (UNTELS), Department of Electronic and Telecommunication Engineering, Lima, Peru; Federal University of Rio de Janeiro, Department of Electrical Engineering, Rio de Janeiro, Brazil; National University of Technology, South Lima (UNTELS), Research group in Computing and Cognitive Neuroscience, Lima, Peru},
	abstract = {Deep learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multidisciplinary agriculture technologies domain. In this research, we present a deep learning classification system of diverse plants, in order to enable precision agriculture applications. This classification problem was achieved thanks to the public dataset 'Plant Seedlings Dataset', which contains images of approximately 960 unique plants belonging to 12 species at several growth stages. The database has been from Aarhus University Flakkebjerg Research Station in collaboration between the University of Southern Denmark and Aarhus University. A classification comparison was used to determinate which of three pre-trained models; InceptionV3, VGG16 and Xception; reach the best accuracy performance for the database used in this work. Results determined that (1) Xception was the best model for plant classification obtaining 86.21%, overcoming other networks in 7.37% with a time processing around 741 seconds. (2) GPU hardware changes the classification model results impacting strongly in their accuracy score. © 2019 IEEE.},
	author_keywords = {Deep Learning; Inception V3; Machine Learning; Precision Agriculture; VGG 16; Xception},
	keywords = {Artificial intelligence; Big data; Classification (of information); Engineering education; Learning systems; Precision agriculture; Seed; Agriculture applications; Agriculture technology; Classification models; Data intensive science; High performance computing; Inception V3; VGG 16; Xception; Deep learning},
	correspondence_address = {C.A. Mugruza Vassallo; National University of Technology, South Lima (UNTELS), Research group in Computing and Cognitive Neuroscience, Lima, Peru; email: cmugruza@yahoo.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172815540-1},
	language = {English},
	abbrev_source_title = {Int. Conf. Comput., Control, Inf. Appl.: Emerg. Trends Big Data Artif. Intell., IC3INA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 7th International Conference on Computer, Control, Informatics and its Applications, IC3INA 2019; Conference date: 23 October 2019 through 24 October 2019; Conference code: 156684}
}

@CONFERENCE{Skovsen2019639,
	author = {Skovsen, S. and Laursen, M.S. and Gislum, R. and Eriksen, J. and Dyrmann, M. and Mortensen, A.K. and Farkhani, S. and Karstoft, H. and Jensen, N.P. and Jørgensen, R.N.},
	title = {Species distribution mapping of grass clover leys using images for targeted nitrogen fertilization},
	year = {2019},
	journal = {Precision Agriculture 2019 - Papers Presented at the 12th European Conference on Precision Agriculture, ECPA 2019},
	pages = {639 – 645},
	doi = {10.3920/978-90-8686-888-9_79},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073756565&doi=10.3920%2f978-90-8686-888-9_79&partnerID=40&md5=ff53f2b13640fbb1f8721f111f9ae5f5},
	affiliations = {Aarhus University, Department of Engineering, Finlandsgade 22, Aarhus N, 8200, Denmark; Aarhus University, Department of Agroecology, Forsøgsvej 1, Slagelse, 4200, Denmark; Aarhus University, Department of Agroecology, Blichers Allé 20, Tjele, 8830, Denmark; I-GIS, Voldbjergvej 14A, Risskov, 8240, Denmark},
	abstract = {Targeted nitrogen fertilization of grass clover mixtures, based on the local distribution of grass and clover, can potentially increase yield, improve yield quality, and reduce nitrate leaching. In this work, the grass and clover distributions of 150 ha were mapped based on image recognition in colour images. With a high-speed down-facing image acquisition platform mounted on an ATV, the fields were systematically traversed in October 2018 to capture 17,759 geotagged and spatially distributed images. Using a trained deep convolutional neural network, the images were automatically analysed to classify every pixel and estimate the species distributions in each image. Interpolation of the clover distribution into maps showed a high variation of clover content between the fields, and frequently within them. Site-specific nitrogen fertilization strategies were then produced based on the maps. © Wageningen Academic Publishers 2019},
	author_keywords = {Deep learning; Plant classification; Remote sensing; Semantic segmentation; Species mapping},
	keywords = {Deep learning; Deep neural networks; Image recognition; Neural networks; Nitrogen; Precision agriculture; Remote sensing; Semantics; Convolutional neural network; Local distributions; Nitrate leaching; Nitrogen fertilization; Plant classification; Semantic segmentation; Species distributions; Species mapping; Population distribution},
	correspondence_address = {S. Skovsen; Aarhus University, Department of Engineering, Aarhus N, Finlandsgade 22, 8200, Denmark; email: ssk@eng.au.dk},
	editor = {Stafford J.V.},
	publisher = {Wageningen Academic Publishers},
	isbn = {978-908686337-2},
	language = {English},
	abbrev_source_title = {Precis. Agric. - Pap. Present. Eur. Conf. Precis. Agric., ECPA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 12th European Conference on Precision Agriculture, ECPA 2019; Conference date: 8 July 2019 through 11 July 2019; Conference code: 151756}
}

@ARTICLE{Zhang2019,
	author = {Zhang, Jingcheng and He, Yuhang and Yuan, Lin and Liu, Peng and Zhou, Xianfeng and Huang, Yanbo},
	title = {Machine learning-based spectral library for crop classification and status monitoring},
	year = {2019},
	journal = {Agronomy},
	volume = {9},
	number = {9},
	doi = {10.3390/agronomy9090496},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071431069&doi=10.3390%2fagronomy9090496&partnerID=40&md5=86766efc2ebebe0c7e03996c99be9255},
	affiliations = {College of Artificial Intelligence, Hangzhou Dianzi University, Hangzhou, 310018, China; School of Information Engineering and Art and Design, Zhejiang University of Water Resources and Electric Power, Hangzhou, 310018, China; Crop Production Systems Research Unit, United States Department of Agriculture, Agricultural Research Service, PO Box 350, Stoneville, 38776, MS, United States},
	abstract = {The establishment and application of a spectral library is a critical step in the standardization and automation of remote sensing interpretation and mapping. Currently, most spectral libraries are designed to support the classification of land cover types, whereas few are dedicated to agricultural remote sensing monitoring. Here, we gathered spectral observation data on plants in multiple experimental scenarios into a spectral database to investigate methods for crop classification (16 crop species) and status monitoring (tea plant and rice growth). We proposed a set of screening methods for spectral features related to plant classification and status monitoring (band reflectance, vegetation index, spectral differentiation, spectral continuum characteristics) that are based on ISODATA and JM distance. Next, we investigated the performance of different machine learning classifiers in the spectral library application, including K-nearest neighbor (KNN), Random Forest (RF), and a genetic algorithm coupled with a support vector machine (GA-SVM). The optimal combination of spectral features and the classifier with the highest classification accuracy were selected for crop classification and status monitoring scenarios. The GA-SVM classifier performed the best, which produced an accuracy of OAA = 0.94, Kappa = 0.93 for crop classification in a complex scenario (crops mixed with 71 non-crop plant species), and promising accuracies for tea plant growth monitoring (OAA = 0.98, Kappa = 0.97) and rice growth stage monitoring (OAA = 0.92, Kappa = 0.90). Therefore, the establishment of a plant spectral library combined with relevant feature extraction and a classification algorithm effectively supports agricultural monitoring by remote sensing. © 2019 by the authors.},
	author_keywords = {Crop classification; Hyperspectral; Machine learning; Spectral library; Status monitoring},
	correspondence_address = {X. Zhou; College of Artificial Intelligence, Hangzhou Dianzi University, Hangzhou, 310018, China; email: zhouxianfeng@hdu.edu.cn},
	publisher = {MDPI AG},
	issn = {20734395},
	language = {English},
	abbrev_source_title = {Agronomy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; All Open Access, Gold Open Access}
}

@CONFERENCE{Goyal2019405,
	author = {Goyal, Neha and Kapil and Kumar, Nitin},
	title = {Plant species identification using leaf image retrieval: A study},
	year = {2019},
	journal = {2018 International Conference on Computing, Power and Communication Technologies, GUCON 2018},
	pages = {405 – 411},
	doi = {10.1109/GUCON.2018.8675114},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064406012&doi=10.1109%2fGUCON.2018.8675114&partnerID=40&md5=5593e69f7da7a42a6b06561e842d8abf},
	affiliations = {Dept.of Computer Applications, National Institute of Technology, Kurukshetra, India; Dept. of Computer Science and Engineering, National Institute of Technology, Uttarakhand, India},
	abstract = {Human beings along with other living beings and their ecological system are completely inter-dependent. In the past few decades, the technological development has affected the environment more radically than ever before. It has posed grave threats to the natural resources including habitat loss and degradation, over-exploitation of resources and change in climatic condition. Most of plant species are on the verge of extinction. In the present circumstances, it is essential to conserve ecological system. Plant identification is a crucial step towards ecosystem diversity conservation. It is time consuming and requires lots of efforts, specialized knowledge and in-depth training. Recent technological advancement in the field of imaging, data analysis, and plant morphology has improved the decision such as yield prediction, crop management, veterinary diet, improving climate and many more. This has also made it possible to develop plant species identification system. In this paper, we present a comprehensive study about plant species identification methods based on various feature extraction methods, classification and other challenges. Furthermore, the performance of two widely used classifiers viz. Support Vector Machine (SVM) and Probabilistic Neural Network (PNN) on Flavia dataset is examined in terms of Precision, Recall and F-Score. It is observed that SVM performs better than PNN for plant species identification.. © 2018 IEEE.},
	author_keywords = {Classification; Feature extraction; Morphological feature; Pattern recognition; Plant Morphology},
	keywords = {Crops; Ecology; Extraction; Feature extraction; Morphology; Neural networks; Pattern recognition; Support vector machines; Feature extraction methods; Morphological features; Plant morphology; Plant species identification; Probabilistic neural networks; Specialized knowledge; Technological advancement; Technological development; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153864491-1},
	language = {English},
	abbrev_source_title = {Int. Conf. Comput., Power Commun. Technol., GUCON},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 2018 International Conference on Computing, Power and Communication Technologies, GUCON 2018; Conference date: 28 September 2018 through 29 September 2018; Conference code: 146693}
}

@ARTICLE{Gyires-Tóth201988,
	author = {Gyires-Tóth, Bálint Pál and Osváth, Márton and Papp, Dávid and Szucs, Gábor},
	title = {Deep learning for plant classification and content-based image retrieval},
	year = {2019},
	journal = {Cybernetics and Information Technologies},
	volume = {19},
	number = {1},
	pages = {88 – 100},
	doi = {10.2478/CAIT-2019-0005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065139099&doi=10.2478%2fCAIT-2019-0005&partnerID=40&md5=118ee0440633787de8182357340a93e6},
	affiliations = {Budapest University of Technology and Economics, Department of Telecommunications and Media Informatics, 2nd Magyar Tudósok krt., Budapest, H-1117, Hungary},
	abstract = {The main goal of the present research is to classify images of plants to species with deep learning. We used convolutional neural network architectures for feature learning and fully connected layers with logsoftmax output for classification. Pretrained models on ImageNet were used, and transfer learning was applied. In the current research image sets published in the scope of the PlantCLEF 2015 challenge were used. The proposed system surpasses the results of all top competitors of the challenge by 8% and 7% at observation and image levels, respectively. Our secondary goal was to satisfy the users' needs in content-based image retrieval to give relevant hits during species search task. We optimized the length of the returned lists in order to maximize MAP (Mean Average Precision), which is critical to the performance of image retrieval. Thus, we achieved more than 50% improvement of MAP in the test set compared to the baseline. © 2019, Bulgarian Academy of Sciences.},
	author_keywords = {Convolutional neural networks; Deep learning; Image retrieval; Inception V3; MAP},
	publisher = {Sciendo},
	issn = {13119702},
	language = {English},
	abbrev_source_title = {Cybern. Inf. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access, Gold Open Access}
}

@ARTICLE{Sadeghi-Tehran2019,
	author = {Sadeghi-Tehran, Pouria and Angelov, Plamen and Virlet, Nicolas and Hawkesford, Malcolm J.},
	title = {Scalable database indexing and fast image retrieval based on deep learning and hierarchically nested structure applied to remote sensing and plant biology},
	year = {2019},
	journal = {Journal of Imaging},
	volume = {5},
	number = {3},
	doi = {10.3390/jimaging5030033},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067646205&doi=10.3390%2fjimaging5030033&partnerID=40&md5=8979b8a64669c62bbae20b6a2525d247},
	affiliations = {Department of Plant Sciences, Rothamsted Research, Harpenden, AL5 2JQ, United Kingdom; School of Computing and Communications, InfoLab21, Lancaster University, Lancaster, LA1 4WA, United Kingdom},
	abstract = {Digitalisation has opened a wealth of new data opportunities by revolutionizing how images are captured. Although the cost of data generation is no longer a major concern, the data management and processing have become a bottleneck. Any successful visual trait system requires automated data structuring and a data retrieval model to manage, search, and retrieve unstructured and complex image data. This paper investigates a highly scalable and computationally efficient image retrieval system for real-time content-based searching through large-scale image repositories in the domain of remote sensing and plant biology. Images are processed independently without considering any relevant context between sub-sets of images. We utilize a deep Convolutional Neural Network (CNN) model as a feature extractor to derive deep feature representations from the imaging data. In addition, we propose an effective scheme to optimize data structure that can facilitate faster querying at search time based on the hierarchically nested structure and recursive similarity measurements. A thorough series of tests were carried out for plant identification and high-resolution remote sensing data to evaluate the accuracy and the computational efficiency of the proposed approach against other content-based image retrieval (CBIR) techniques, such as the bag of visual words (BOVW) and multiple feature fusion techniques. The results demonstrate that the proposed scheme is effective and considerably faster than conventional indexing structures. © 2019 by the authors.},
	author_keywords = {Bag of visual words; Content-based image retrieval; Data indexing; Deep convolutional neural networks; Deep learning; Information retrieval; Recursive similarity measurement; Remote sensing},
	keywords = {Computational efficiency; Content based retrieval; Convolution; Data handling; Deep neural networks; Image processing; Indexing (of information); Information management; Plants (botany); Query processing; Search engines; Bag-of-visual-words; Content-Based Image Retrieval; Contents-based image retrievals; Convolutional neural network; Data indexing; Deep convolutional neural network; Deep learning; Recursive similarity measurement; Remote-sensing; Similarity measurements; Remote sensing},
	correspondence_address = {P. Sadeghi-Tehran; Department of Plant Sciences, Rothamsted Research, Harpenden, AL5 2JQ, United Kingdom; email: pouria.sadeghi-tehran@rothamsted.ac.uk},
	publisher = {MDPI},
	issn = {2313433X},
	language = {English},
	abbrev_source_title = {J. Imaging},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Tiwari2019165,
	author = {Tiwari, Utkarsh and Singh, Rohit Kumar and Wargia, Rohan Vijay and Vikashrao, P Uttareshwar},
	title = {Machine learning based flower recognition system},
	year = {2019},
	journal = {International Journal of Engineering and Advanced Technology},
	volume = {8},
	number = {5 SpecialIssue},
	pages = {165 – 169},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070463568&partnerID=40&md5=961b0317751d67fd71e24c47a2a984e2},
	affiliations = {REVA University, Bangalore, India},
	abstract = {Automatic flower plucking systems for smart agriculture are being studied for many years to support flower harvesting. Such systems require flower recognition task to be integrated as part of the system. This paper presents an approach for classification of flowers using a machine learning algorithm. The method categorizes flowers into different species with the help of convolutional neural networks and deep learning techniques. The system uses a pre-trained CNN model to improve the accuracy rate. Concepts such as Feedforward, back-propagation and transfer learning are used to create the neural network model. Different hyper-parameter values have been tested on the model which provides maximum accuracy of 85.0 percentage on the testing dataset. The result is visualized in the form of bar-plots which provides the top 5 predictions of flower species for the given input image of a flower. © BEIESP.},
	author_keywords = {CNN; Feedforward; Image recognition; Machine learning},
	publisher = {Blue Eyes Intelligence Engineering and Sciences Publication},
	issn = {22498958},
	language = {English},
	abbrev_source_title = {Int. J. Eng. Adv. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wu2019,
	author = {Wu, Danzi and Han, Xue and Wang, Guan and Sun, Yu and Zhang, Haiyan and Fu, Hongping},
	title = {Deep Learning with Taxonomic Loss for Plant Identification},
	year = {2019},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2019},
	doi = {10.1155/2019/2015017},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076356317&doi=10.1155%2f2019%2f2015017&partnerID=40&md5=04fb986aa8a8ad3f2f48a135530d369e},
	affiliations = {School of Landscape Architecture, Beijing Forestry University, Beijing, 100083, China; School of Information Science and Technology, Beijing Forestry University, Beijing, 100083, China; School of Cyber Science and Technology, Beihang University, Beijing, 100191, China},
	abstract = {Plant identification is a fine-grained classification task which aims to identify the family, genus, and species according to plant appearance features. Inspired by the hierarchical structure of taxonomic tree, the taxonomic loss was proposed, which could encode the hierarchical relationships among multilevel labels into the deep learning objective function by simple group and sum operation. By training various neural networks on PlantCLEF 2015 and PlantCLEF 2017 datasets, the experimental results demonstrated that the proposed loss function was easy to implement and outperformed the most commonly adopted cross-entropy loss. Eight neural networks were trained, respectively, by two different loss functions on PlantCLEF 2015 dataset, and the models trained by taxonomic loss led to significant performance improvements. On PlantCLEF 2017 dataset with 10,000 species, the SENet-154 model trained by taxonomic loss achieved the accuracies of 84.07%, 79.97%, and 73.61% at family, genus and species levels, which improved those of model trained by cross-entropy loss by 2.23%, 1.34%, and 1.08%, respectively. The taxonomic loss could further facilitate the fine-grained classification task with hierarchical labels. © 2019 Danzi Wu et al.},
	keywords = {Deep Learning; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Plants; Entropy; Classification tasks; Cross entropy; Hierarchical structures; Learning objective functions; Loss functions; Plant identification; Simple groups; Taxonomic trees; anatomy and histology; automated pattern recognition; classification; image processing; plant; procedures; Deep learning},
	correspondence_address = {Y. Sun; School of Information Science and Technology, Beijing Forestry University, Beijing, 100083, China; email: sunyv@buaa.edu.cn},
	publisher = {Hindawi Limited},
	issn = {16875265},
	pmid = {31871441},
	language = {English},
	abbrev_source_title = {Comput. Intell. Neurosci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Bronze Open Access, Green Open Access}
}

@CONFERENCE{Vaishnnave2019,
	author = {Vaishnnave, M.P. and Suganya Devi, K. and Srinivasan, P. and Arutperumjothi, G.},
	title = {Detection and classification of groundnut leaf diseases using KNN classifier},
	year = {2019},
	journal = {2019 IEEE International Conference on System, Computation, Automation and Networking, ICSCAN 2019},
	doi = {10.1109/ICSCAN.2019.8878733},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074755594&doi=10.1109%2fICSCAN.2019.8878733&partnerID=40&md5=de657dd72fe53af81b3b30d062c70473},
	affiliations = {Department of Information and Technology, University College of Engineering, Villupuram, Tamil Nadu, India; Department of Computer Science and Engineering, National Institute of Technology, Silchar Cachar, Assam, India; Department of Physics, National Institute of Technology, Silchar Cachar, Assam, India},
	abstract = {One of the ultimate key factors besttow less yield is disease attack. The groundnut plant disease such as fungi, soil borne and viruses. In this paper, we are given that software determination to robotically classify and categorize groundnut leaf diseases. This method will improve production of crops. It comprises of number of steps viz. image acquisition, image pre-processing, segmentation, features extraction and classifier using K Nearest Neighbor (KNN). To increase performance of existing algorithm the SVM classifier is replaced with KNN classification. In this paper we categorized only 4 different disease using KNN classifier algorithm. © 2019 IEEE.},
	author_keywords = {Early detection; Feature Extraction; Grountnut Disease Detection; KNN; Plant Classification},
	keywords = {Extraction; Feature extraction; Image segmentation; Nearest neighbor search; Support vector machines; Viruses; Disease detection; Features extraction; Image preprocessing; K nearest neighbor (KNN); K-NN classifications; K-NN classifier; Plant classification; SVM classifiers; Learning algorithms},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172811525-2},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Syst., Compu., Autom. Netw., ICSCAN},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 47; Conference name: 2019 IEEE International Conference on System, Computation, Automation and Networking, ICSCAN 2019; Conference date: 29 March 2019 through 30 March 2019; Conference code: 153212}
}

@CONFERENCE{Goëau2019,
	author = {Goëau, Hervé and Bonnet, Pierre and Joly, Alexis},
	title = {Overview of LifeCLEF Plant identification task 2019: Diving into data deficient tropical countries},
	year = {2019},
	journal = {CEUR Workshop Proceedings},
	volume = {2380},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070515552&partnerID=40&md5=d269dbfe070f7f8a68943a8d6c7ec6f9},
	affiliations = {CIRAD, UMR AMAP, France; AMAP, Univ Montpellier, CIRAD, CNRS, INRA, IRD, Montpellier, France; Inria ZENITH team, France; LIRMM, Montpellier, France},
	abstract = {Automated identification of plants has improved considerably thanks to the recent progress in deep learning and the availability of training data. However, this profusion of data only concerns a few tens of thousands of species, while the planet has nearly 369K. The LifeCLEF 2019 Plant Identification challenge (or”PlantCLEF 2019”) was designed to evaluate automated identification on the flora of data deficient regions. It is based on a dataset of 10K species mainly focused on the Guiana shield and the Northern Amazon rainforest, an area known to have one of the greatest diversity of plants and animals in the world. As in the previous edition, a comparison of the performance of the systems evaluated with the best tropical flora experts was carried out. This paper presents the resources and assessments of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of the main outcomes. © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CLEF 2019, 9-12 September 2019, Lugano, Switzerland.},
	author_keywords = {Amazon rainforest; Benchmark; Evaluation; Expert; Fine-grained classification; Guiana Shield leaves; LifeCLEF; Plant; PlantCLEF; Species identification; Tropical flora},
	keywords = {Benchmarking; Deep learning; Tropics; Amazon rain forest; Evaluation; Expert; Fine grained; Guiana Shield leaves; LifeCLEF; Plant; PlantCLEF; Species identification; Tropical flora; Plants (botany)},
	editor = {Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Losada D.E. and Muller H.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 20th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2019; Conference date: 9 September 2019 through 12 September 2019; Conference code: 149771}
}

@ARTICLE{FatihahSahidan2019737,
	author = {FatihahSahidan, Nurul and Juha, Ahmad Khairi and Mohammad, Norasiah and Ibrahim, Zaidah},
	title = {Flower and leaf recognition for plant identification using convolutional neural network},
	year = {2019},
	journal = {Indonesian Journal of Electrical Engineering and Computer Science},
	volume = {16},
	number = {2},
	pages = {737 – 743},
	doi = {10.11591/ijeecs.v16.i2.pp737-743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073571403&doi=10.11591%2fijeecs.v16.i2.pp737-743&partnerID=40&md5=6083277dfff15cbd4ba451deb85f3142},
	affiliations = {Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia},
	abstract = {This paper presents flower and leaf recognition for plant identification using Convolutional Neural Network (CNN). In this study, the performance of CNN for plant identification using images of the leaves, flowers and a combination of both are investigated. Two publicly available datasets, namely Folio leaf dataset and Flower Recognition dataset, have been used for the training and testing purposes. CNN has been proven to produce excellent results for object recognition but its performance can still be influenced by the type of images and the number of layers of the CNN architecture. Experimental results indicate that the utilization of leaf images only arrive to the highest accuracy for plant identification compared to the images of flowers only or the combination of both, that are 98%, 85% and 74%, respectively. Copyright © 2019 Institute of Advanced Engineering and Science. All rights reserved.},
	author_keywords = {CNN; Deep learning; Flower recognition; Leaf recognition},
	correspondence_address = {Z. Ibrahim; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; email: zaidah@tmsk.uitm.edu.my},
	publisher = {Institute of Advanced Engineering and Science},
	issn = {25024752},
	language = {English},
	abbrev_source_title = {Indones. J. Electrical Eng. Comput. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Huang20191951,
	author = {Huang, Sijiang and Liu, Weijie and Qi, Fei and Yang, Kepeng},
	title = {Development and validation of a deep learning algorithm for the recognition of plant disease},
	year = {2019},
	journal = {Proceedings - 21st IEEE International Conference on High Performance Computing and Communications, 17th IEEE International Conference on Smart City and 5th IEEE International Conference on Data Science and Systems, HPCC/SmartCity/DSS 2019},
	pages = {1951 – 1957},
	doi = {10.1109/HPCC/SmartCity/DSS.2019.00269},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073534137&doi=10.1109%2fHPCC%2fSmartCity%2fDSS.2019.00269&partnerID=40&md5=15219b8655b0f282157810993641f71c},
	affiliations = {School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, China; School of Software and Microelectronics, Peking University, China; Institute of Network Technology, Beijing University of Posts and Telecommunications, China},
	abstract = {Crop health is the foundation for agricultural development. In some area, due to the lack of professional botanical experts, it is difficult to correctly diagnose the plant disease in the work of planting. In this paper, we propose a novel deep neural network structure that can reliably classify plant types and plant disease using a single image of plant leaf which enables the end-to-end diagnosis of plant disease. Our proposed model consists of two sub-models, a leaf segmentation model that employs a U-Net to separate the leaves in the original image from the background which effectively eliminates interference, and a plant disease classification model based on our proposed Two-head network that classifies plant diseases with the features extracted by various popular pre-trained models. We verified our model using the plant disease dataset with 8 plant species and 19 plant diseases provided by AI Challenger 2019. Experimental results demonstrate that our final model achieves a 0.9807 accuracy of plant classification and a 0.8745 accuracy of disease recognition. We believe that the technology in our model has great potential to become the basis of fully automatic reliable plant disease classification system which can be embedded into portable devices to assist farmers in the future. © 2019 IEEE.},
	author_keywords = {deep neural network; diagnosis; disease recognition; leaf segmentation; plant disease},
	keywords = {Agriculture; Data communication systems; Deep neural networks; Diagnosis; Image segmentation; Smart city; Agricultural development; Neural network structures; Original images; Plant classification; Plant disease; Portable device; Segmentation models; Single images; Plants (botany)},
	editor = {Xiao Z. and Yang L.T. and Balaji P. and Li T. and Li K. and Zomaya A.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172812058-4},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. High Perform. Comput. Commun., IEEE Int. Conf. Smart City IEEE Int. Conf. Data Sci. Syst., HPCC/SmartCity/DSS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 25; Conference name: 21st IEEE International Conference on High Performance Computing and Communications, 17th IEEE International Conference on Smart City and 5th IEEE International Conference on Data Science and Systems, HPCC/SmartCity/DSS 2019; Conference date: 10 August 2019 through 12 August 2019; Conference code: 152364}
}

@ARTICLE{Seeland2019,
	author = {Seeland, Marco and Rzanny, Michael and Boho, David and Wäldchen, Jana and Mäder, Patrick},
	title = {Image-based classification of plant genus and family for trained and untrained plant species},
	year = {2019},
	journal = {BMC Bioinformatics},
	volume = {20},
	number = {1},
	doi = {10.1186/s12859-018-2474-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059494614&doi=10.1186%2fs12859-018-2474-x&partnerID=40&md5=06595bf89bd05af1a798a212ce879b92},
	affiliations = {Institute for Computer and Systems Engineering, Technische Universität Ilmenau, Helmholtzplatz 5, Ilmenau, 98693, Germany; Max-Planck-Institute for Biogeochemistry, Department Biogeochemical Integration, Hans-Knöll-Str. 10, Jena, 07745, Germany},
	abstract = {Background: Modern plant taxonomy reflects phylogenetic relationships among taxa based on proposed morphological and genetic similarities. However, taxonomical relation is not necessarily reflected by close overall resemblance, but rather by commonality of very specific morphological characters or similarity on the molecular level. It is an open research question to which extent phylogenetic relations within higher taxonomic levels such as genera and families are reflected by shared visual characters of the constituting species. As a consequence, it is even more questionable whether the taxonomy of plants at these levels can be identified from images using machine learning techniques. Results: Whereas previous studies on automated plant identification from images focused on the species level, we investigated classification at higher taxonomic levels such as genera and families. We used images of 1000 plant species that are representative for the flora of Western Europe. We tested how accurate a visual representation of genera and families can be learned from images of their species in order to identify the taxonomy of species included in and excluded from learning. Using natural images with random content, roughly 500 images per species are required for accurate classification. The classification accuracy for 1000 species amounts to 82.2% and increases to 85.9% and 88.4% on genus and family level. Classifying species excluded from training, the accuracy significantly reduces to 38.3% and 38.7% on genus and family level. Excluded species of well represented genera and families can be classified with 67.8% and 52.8% accuracy. Conclusion: Our results show that shared visual characters are indeed present at higher taxonomic levels. Most dominantly they are preserved in flowers and leaves, and enable state-of-the-art classification algorithms to learn accurate visual representations of plant genera and families. Given a sufficient amount and composition of training data, we show that this allows for high classification accuracy increasing with the taxonomic level and even facilitating the taxonomic identification of species excluded from the training process. © 2019 The Author(s).},
	author_keywords = {Computer vision; Deep learning; Plant identification; Taxonomy; Zero-shot classification},
	keywords = {Phylogeny; Plants; Computer vision; Deep learning; Image classification; Taxonomies; Classification algorithm; Image-based classification; Machine learning techniques; Morphological characters; Phylogenetic relationships; Plant identification; Shot classification; Taxonomic identifications; article; classification algorithm; flora; flower; genus; machine learning; ordo; plant identification; plant leaf; vision; Western Europe; classification; phylogeny; plant; Plants (botany)},
	correspondence_address = {M. Seeland; Institute for Computer and Systems Engineering, Technische Universität Ilmenau, Ilmenau, Helmholtzplatz 5, 98693, Germany; email: marco.seeland@tu-ilmenau.de},
	publisher = {BioMed Central Ltd.},
	issn = {14712105},
	coden = {BBMIC},
	pmid = {30606100},
	language = {English},
	abbrev_source_title = {BMC Bioinform.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Feng2019598,
	author = {Feng, Jing and Wang, Zhiwen and Zha, Min and Cao, Xinliang},
	title = {Flower recognition based on transfer learning and ADam deep learning optimization algorithm},
	year = {2019},
	journal = {ACM International Conference Proceeding Series},
	pages = {598 – 604},
	doi = {10.1145/3366194.3366301},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076641233&doi=10.1145%2f3366194.3366301&partnerID=40&md5=6c41c1ae881896be91b1353b4d53ff94},
	affiliations = {Electrical and Information Engineering, Guangxi University of Science and Technology, Liu Zhou Guangxi, China},
	abstract = {Due to the complex background of flowers and the similarity between their own categories, the traditional method of image recognition is to extract features manually, which can not solve this problem well. With the development and progress of science and technology, deep learning has gradually entered the image recognition problem and achieved good results. This paper proposes the flower recognition based on transfer learning and Adam deep learning optimization algorithm for the defects of the current mainstream convolutional neural network with deep depth and long parameters, long training time and slow convergence. The VGG16 model is modified and supplemented. At the same time, the transfer learning method and the Adam optimization algorithm are used to accelerate network convergence. Thirty kinds of flower image data sets were established by 102 Category Flower Dataset partial images and 17 Category Flower Dataset. The experimental results show that the accuracy of the test set in this paper is 98.99%. Compared with the traditional image recognition algorithm, it has the characteristics of fast convergence and high recognition accuracy. © 2019 Association for Computing Machinery.},
	author_keywords = {Deep learning; Flower recognition; Transfer learning; VGG16},
	keywords = {Convolutional neural networks; Image recognition; Learning algorithms; Learning systems; Optimization; Robotics; Transfer learning; Flower recognition; Learning optimizations; Optimization algorithms; Recognition accuracy; Recognition algorithm; Science and Technology; Transfer learning methods; VGG16; Deep learning},
	publisher = {Association for Computing Machinery},
	isbn = {978-145037298-5},
	language = {English},
	abbrev_source_title = {ACM Int. Conf. Proc. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; Conference name: 2019 International Conference on Robotics, Intelligent Control and Artificial Intelligence, RICAI 2019; Conference date: 20 September 2019 through 22 September 2019; Conference code: 155401}
}

@ARTICLE{Nguyen201926,
	author = {Nguyen, Thi Thanh-Nhan and Le, Thi-Lan and Vu, Hai and Hoang, Van-Sam},
	title = {Towards an automatic plant identification system without dedicated dataset},
	year = {2019},
	journal = {International Journal of Machine Learning and Computing},
	volume = {9},
	number = {1},
	pages = {26 – 34},
	doi = {10.18178/ijmlc.2019.9.1.761},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061298200&doi=10.18178%2fijmlc.2019.9.1.761&partnerID=40&md5=367d71a423a6e80565e145f2adeb06ee},
	affiliations = {International Research Institute MICA, HUST-CNRS/UMI-2594-GRENOBLE INP, Hanoi, Viet Nam; University of Information and Communication Technology, Thainguyen University, Thainguyen, Viet Nam; Vietnam Forestry University, Hanoi, Viet Nam},
	abstract = {A large image dataset plays a crucial role in building automatic vision recognition system. However, collecting and labeling data are tedious, laborious and time-consuming tasks. In some cases, it is chicken and egg problem: it is only possible to get application data after the system deployment. In our study, we are interested in building automatic plant identification systems from images. As plants distribution on the world is not uniform and may change in response to the availability of resources, the availability of species in different areas is different. That is why some species are very abundant in one region and non-existing in others regions. Even the distribution of plant species is diverse, plant species in the planet share common features. They all have organ types such as leaf, flower, etc. Taking into this observation, in this paper, we propose a new approach for building an image-based plant identification without an available image database based on the combination of deep learning, transfer learning, and crowd-sourcing. The proposed approach consists of four main steps: plant organ detection, plant image collection, data validation and plant identification. Plant organ detection aims to learn organ type characteristic from available image datasets of plants while the purpose of the data collection step is to crawl dataset from crowd-sourced sources. Then, plant organ detection will be used in data validation in order to remove the unwanted/invalid images while keeping the valid ones. Finally, plant identification method will be developed and evaluated from the new image dataset. We illustrate and demonstrate the use of the proposed approach for building a Vietnamese medicinal plant retrieval system. © 2019 International Association of Computer Science and Information Technology.},
	author_keywords = {Convolutional neural network; Deep learning; Organ detection; Plant identification},
	publisher = {International Association of Computer Science and Information Technology},
	issn = {20103700},
	language = {English},
	abbrev_source_title = {Int. J. Mach. Learn. Comput.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access, Bronze Open Access}
}

@ARTICLE{Tenhunen2019677,
	author = {Tenhunen, Henri and Pahikkala, Tapio and Nevalainen, Olli and Teuhola, Jukka and Mattila, Heta and Tyystjärvi, Esa},
	title = {Automatic detection of cereal rows by means of pattern recognition techniques},
	year = {2019},
	journal = {Computers and Electronics in Agriculture},
	volume = {162},
	pages = {677 – 688},
	doi = {10.1016/j.compag.2019.05.002},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065440674&doi=10.1016%2fj.compag.2019.05.002&partnerID=40&md5=f970634db7095a30df0a20baea11628d},
	affiliations = {Department of Future Technologies, Computer Science, University of Turku, Finland; Department of Biochemistry, Molecular Plant Biology, University of Turku, Finland},
	abstract = {Automatic locating of weeds from fields is an active research topic in precision agriculture. A reliable and practical plant identification technique would enable the reduction of herbicide amounts and lowering of production costs, along with reducing the damage to the ecosystem. When the seeds have been sown row-wise, most weeds may be located between the sowing rows. The present work describes a clustering-based method for recognition of plantlet rows from a set of aerial photographs, taken by a drone flying at approximately ten meters. The algorithm includes three phases: segmentation of green objects in the view, feature extraction, and clustering of plants into individual rows. Segmentation separates the plants from the background. The main feature to be extracted is the center of gravity of each plant segment. A tentative clustering is obtained piecewise by applying the 2D Fourier transform to image blocks to get information about the direction and the distance between the rows. The precise sowing line position is finally derived by principal component analysis. The method was able to find the rows from a set of photographs of size 1452×969 pixels approximately in 0.11 s, with the accuracy of 94 per cent. © 2019 Elsevier B.V.},
	author_keywords = {Computer vision; Fourier transform; Pattern recognition; Precision agriculture; Principal component analysis},
	keywords = {Antennas; Computer vision; Damage detection; Fourier transforms; Photography; Precision agriculture; Principal component analysis; 2d fourier transforms; Aerial Photographs; Automatic Detection; Automatic locating; Center of gravity; Pattern recognition techniques; Plant identification; Research topics; aerial photograph; algorithm; automation; cereal; computer vision; detection method; Fourier transform; pattern recognition; precision agriculture; principal component analysis; segmentation; weed; Pattern recognition},
	correspondence_address = {E. Tyystjärvi; Department of Biochemistry, Molecular Plant Biology, University of Turku, Finland; email: esatyy@utu.fi},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@CONFERENCE{Ratajczak2019240,
	author = {Ratajczak, Rémi and Bertrand, Sarah and Crispim-Junior, Carlos and Tougne, Laure},
	title = {Efficient bark recognition in the wild},
	year = {2019},
	journal = {VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
	volume = {4},
	pages = {240 – 248},
	doi = {10.5220/0007361902400248},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068214369&doi=10.5220%2f0007361902400248&partnerID=40&md5=517a466e2adabe96dd0e731617567d00},
	affiliations = {Univ Lyon, LIRIS, Lyon 2, Lyon, F-69676, France; Unité Cancer et Environnement, Centre Léon Bérard, Lyon, France; Agence de l’Environnement et de la Maîtrise de l’Energie, Angers, France},
	abstract = {In this study, we propose to address the difficult task of bark recognition in the wild using computationally efficient and compact feature vectors. We introduce two novel generic methods to significantly reduce the dimensions of existing texture and color histograms with few losses in accuracy. Specifically, we propose a straightforward yet efficient way to compute Late Statistics from texture histograms and an approach to iteratively quantify the color space based on domain priors. We further combine the reduced histograms in a late fusion manner to benefit from both texture and color cues. Results outperform state-of-the-art methods by a large margin on four public datasets respectively composed of 6 bark classes (BarkTex, NewBarkTex), 11 bark classes (AFF) and 12 bark classes (Trunk12). In addition to these experiments, we propose a baseline study on Bark-101, a new challenging dataset including manually segmented images of 101 bark classes that we release publicly. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
	author_keywords = {Bark Recognition; Color Quantification; Data Fusion; Dimensionality Reduction; Texture Classification},
	keywords = {Color; Computer graphics; Data fusion; Graphic methods; Iterative methods; Large dataset; Textures; Bark Recognition; Baseline studies; Compact Features; Computationally efficient; Dimensionality reduction; Segmented images; State-of-the-art methods; Texture classification; Computer vision},
	editor = {Kerren A. and Hurter C. and Braz J.},
	publisher = {SciTePress},
	isbn = {978-989758354-4},
	language = {English},
	abbrev_source_title = {VISIGRAPP - Proc. Int. Jt. Conf. Comput. Vis., Imaging Comput. Graph. Theory Appl.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 14th International Conference on Computer Vision Theory and Applications, VISAPP 2019 - Part of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP  2019; Conference date: 25 February 2019 through 27 February 2019; Conference code: 146941; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Joly2019275,
	author = {Joly, Alexis and Goëau, Hervé and Botella, Christophe and Kahl, Stefan and Poupard, Marion and Servajean, Maximillien and Glotin, Hervé and Bonnet, Pierre and Vellinga, Willem-Pier and Planqué, Robert and Schlüter, Jan and Stöter, Fabian-Robert and Müller, Henning},
	title = {LifeCLEF 2019: Biodiversity identification and prediction challenges},
	year = {2019},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {11438 LNCS},
	pages = {275 – 282},
	doi = {10.1007/978-3-030-15719-7_37},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064880395&doi=10.1007%2f978-3-030-15719-7_37&partnerID=40&md5=340a19f8837eb7aaceb6af05d3f7dbac},
	affiliations = {Inria, LIRMM, Montpellier, France; CIRAD, UMR AMAP, Montpellier, France; INRA, UMR AMAP, Montpellier, France; AMU, Univ. Toulon, CNRS, ENSAM, LSIS UMR 7296, IUF, Toulon, France; Xeno-canto Foundation, Groningen, Netherlands; HES-SO, Sierre, Switzerland; Chemnitz University of Technology, Chemnitz, Germany; LIRMM, Université Paul Valéry, University of Montpellier, CNRS, Montpellier, France},
	abstract = {Building accurate knowledge of the identity, the geographic distribution and the evolution of living species is essential for a sustainable development of humanity, as well as for biodiversity conservation. However, the burden of the routine identification of plants and animals in the field is strongly penalizing the aggregation of new data and knowledge. Identifying and naming living plants or animals is actually almost impossible for the general public and often a difficult task for professionals and naturalists. Bridging this gap is a key challenge towards enabling effective biodiversity information retrieval systems. The LifeCLEF evaluation campaign, presented in this paper, aims at boosting and evaluating the advances in this domain since 2011. In particular, the 2019 edition proposes three data-oriented challenges related to the identification and prediction of biodiversity: (i) an image-based plant identification challenge, (ii) a bird sounds identification challenge and (iii) a location-based species prediction challenge based on spatial occurrence data and environmental tensors. © Springer Nature Switzerland AG 2019.},
	author_keywords = {Biodiversity; Bird identification; Informatics; Machine learning; Plant identification; Species distribution model; Species identification; Species prediction},
	keywords = {Biodiversity; Birds; Conservation; Forecasting; Geographical distribution; Historic preservation; Informatics; Information retrieval systems; Learning systems; Population distribution; Sustainable development; Biodiversity conservation; General publics; Image-based; Location based; Plant identification; Species distribution modeling; Species identification; Search engines},
	correspondence_address = {H. Goëau; CIRAD, UMR AMAP, Montpellier, France; email: herve.goeau@cirad.fr},
	editor = {Fuhr N. and Mayr P. and Azzopardi L. and Hauff C. and Stein B. and Hiemstra D.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-303015718-0},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 41st European Conference on Information Retrieval, ECIR 2019; Conference date: 14 April 2019 through 18 April 2019; Conference code: 225189; All Open Access, Green Open Access}
}

@CONFERENCE{Habiba20196,
	author = {Habiba, Umme and Howlader, Md. Rasel and Islam, Md. Aminul and Faisal, Rahat Hossain and Rahman, Md. Mostafijur},
	title = {Automatic medicinal plants classification using multi-channel modified local gradient pattern with SVM Classifier},
	year = {2019},
	journal = {2019 Joint 8th International Conference on Informatics, Electronics and Vision, ICIEV 2019 and 3rd International Conference on Imaging, Vision and Pattern Recognition, icIVPR 2019 with International Conference on Activity and Behavior Computing, ABC 2019},
	pages = {6 – 11},
	doi = {10.1109/ICIEV.2019.8858527},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074075413&doi=10.1109%2fICIEV.2019.8858527&partnerID=40&md5=9ad6e76619dfd02c77bb83d26e36fd18},
	affiliations = {Dept. of Computer Science and Engineering, University of Barisal, Barisal, Bangladesh},
	abstract = {Most of the people in the world rely on traditional medicine which is made from medicinal plants. However, very few works concentrate on automatic classification. Therefore, the automatic classification of medicinal plants demands more investigation which is an important issue for conservation, authentication, and production of medicines. In this paper, for automatically classifying medicinal plants, we present a Multi-channel Modified Local Gradient Pattern (MCMLGP), a new texture-based feature descriptor that uses different channels of color images for extracting more significant features to improve the performance of classification. We have trained our proposed approach using SVM classifier with various kernels such as linear, polynomial and HI. In addition, we have used different feature descriptors for comparative experimental analysis with MCMLGP by conducting the rigorous experiment on our own medicinal plants dataset. The proposed approach gain higher accuracy (96.11%) than other techniques, and significantly valuable for exploration and evolution of medicinal plants classification. © 2019 IEEE.},
	author_keywords = {Adaptive Threshold; Image Classification; LGP; Medicinal plant; SVM},
	keywords = {Image classification; Image enhancement; Medical applications; Pattern recognition; Plants (botany); Textures; Adaptive thresholds; Automatic classification; Experimental analysis; Feature descriptors; Local gradient patterns; Medicinal plants; Multi channel; SVM classifiers; Support vector machines},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172810786-8},
	language = {English},
	abbrev_source_title = {Jt. Int. Conf. Inf., Electron. Vis. ICIEV Int. Conf. Imaging, Vis. Pattern Recognit., icIVPR Int. Conf. Act. Behav. Comput., ABC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: Joint 8th International Conference on Informatics, Electronics and Vision and 3rd International Conference on Imaging, Vision and Pattern Recognition, ICIEV and icIVPR 2019; Conference date: 30 May 2019 through 2 June 2019; Conference code: 152554}
}

@CONFERENCE{Chouvatut2019375,
	author = {Chouvatut, Varin and Wattanapairotrat, Supawit},
	title = {Feature reduction from correlation matrix for classification of two basil species in common genus},
	year = {2019},
	journal = {JCSSE 2019 - 16th International Joint Conference on Computer Science and Software Engineering: Knowledge Evolution Towards Singularity of Man-Machine Intelligence},
	pages = {375 – 380},
	doi = {10.1109/JCSSE.2019.8864221},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074229388&doi=10.1109%2fJCSSE.2019.8864221&partnerID=40&md5=cbb3bee6e0cf62618d9c2521edfcf6f5},
	affiliations = {Faculty of Science, Chiang Mai University, Department of Computer Science, Chiang Mai, Thailand},
	abstract = {This research proposed ways with comparison results for feature selection and reduction for plant's leaf classification based on a key concept that features in a data set may include weakly relevant or redundant features. Six classifiers of support vector machine (SVM) model are demonstrated with ten features of about 320 leaves of two basil species sharing common genus. Plant species in a common genus typically have various aspects of similarity in their leaf features and this is our challenge in the way whether feature reduction should be done. Feature reduction provides the decrease in processing time in many cases, but it can easily reduce classification performance in terms of accuracy rate. According to our proposed techniques, an optimal feature reduction can still obtain while we still gain a perfect classification of 100 percent of accuracy. © 2019 IEEE.},
	author_keywords = {common genus; correlation matrix; feature reduction; feature selection; plant classification},
	keywords = {Feature extraction; Plants (botany); Software engineering; Support vector machines; Classification performance; common genus; Comparison result; Correlation matrix; Feature reduction; Leaf classification; Plant classification; Redundant features; Classification (of information)},
	correspondence_address = {V. Chouvatut; Faculty of Science, Chiang Mai University, Department of Computer Science, Chiang Mai, Thailand; email: varinchouv@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172810719-6},
	language = {English},
	abbrev_source_title = {JCSSE - Int. Jt. Conf. Comput. Sci. Softw. Eng.: Knowl. Evol. Towar. Singul. Man-Machine Intell.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 16th International Joint Conference on Computer Science and Software Engineering, JCSSE 2019; Conference date: 10 July 2019 through 12 July 2019; Conference code: 152760}
}

@CONFERENCE{Adedoja2019,
	author = {Adedoja, Adedamola and Owolawi, Pius Adewale and Mapayi, Temitope},
	title = {Deep learning based on NASNet for plant disease recognition using leave images},
	year = {2019},
	journal = {icABCD 2019 - 2nd International Conference on Advances in Big Data, Computing and Data Communication Systems},
	doi = {10.1109/ICABCD.2019.8851029},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073531434&doi=10.1109%2fICABCD.2019.8851029&partnerID=40&md5=9ec86f2018f2c838e690d85a302704d4},
	affiliations = {Department of Computer Systems Engineering, Tshwane University of Technology, Pretoria, South Africa},
	abstract = {Global food security has become a very important research focus. This is due to the fact that food is a basic need of human beings and its adequate supply to meet the need of humans must be ensured. Plant diseases have, however, been one of the major problems threatening the adequate supply of food to humans. The early detection of these diseases can assist in their efficient management, thus making huge differences between survival and destruction of crops in farmlands affected by these plant diseases. Deep neural networks have been successfully applied in the field of artificial intelligence. This has inspired an increased research into the use of deep learning in the domains of image processing and computer vision. This paper present a study on the use of deep learning-based approach to identify diseased plants using leaf images by transfer learning. The study uses NASNet architeure for the convolutionary neural networks (CNN). The model is then trained and tested using a publicly available PlantVillage project dataset that contains varied images of plant leaves with multiple variations in infection status and location in the plants. Using the model, an accuracy rate of 93.82% was achieved. © 2019 IEEE.},
	author_keywords = {Convolutional Neural Networks; Deep Learning; Diseases; Image Processing; Plant Recognition},
	keywords = {Big data; Convolutional codes; Deep learning; Deep neural networks; Diseases; Food supply; Image processing; Neural networks; Plants (botany); Convolutional neural network; Efficient managements; Global food security; Image processing and computer vision; Learning-based approach; Plant recognition; Research focus; Transfer learning; Data communication systems},
	editor = {Maharaj M. and Singh U.G.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153869236-3},
	language = {English},
	abbrev_source_title = {icABCD - Int. Conf. Adv. Big Data, Comput. Data Commun. Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32; Conference name: 2nd International Conference on Advances in Big Data, Computing and Data Communication Systems, icABCD 2019; Conference date: 5 August 2019 through 6 August 2019; Conference code: 152287}
}

@ARTICLE{Anubha Pearline20191997,
	author = {Anubha Pearline, S. and Sathiesh Kumar, V. and Harini, S.},
	title = {A study on plant recognition using conventional image processing and deep learning approaches},
	year = {2019},
	journal = {Journal of Intelligent and Fuzzy Systems},
	volume = {36},
	number = {3},
	pages = {1997 – 2004},
	doi = {10.3233/JIFS-169911},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063459322&doi=10.3233%2fJIFS-169911&partnerID=40&md5=95c2b15ac82441789521a337ca876b1e},
	affiliations = {Department of Electronics Engineering, Madras Institute of Technology, Anna University, Chennai, India},
	abstract = {Plant species recognition from images or videos is challenging due to a large diversity of plants, variation in orientation, viewpoint, background clutter, etc. In this paper, plant species recognition is carried out using two approaches, namely, traditional method and deep learning approach. In traditional method, feature extraction is carried out using Hu moments (shape features), Haralick texture, local binary pattern (LBP) (texture features) and color channel statistics (color features). The extracted features are classified using different classifiers (linear discriminant analysis, logistic regression, classification and regression tree, naïve Bayes, k-nearest neighbor, random forest and bagging classifier). Also, different deep learning architectures are tested in the context of plant species recognition. Three standard datasets (Folio, Swedish leaf and Flavia) and one real-time dataset (Leaf12) is used. It is observed that, in traditional method, feature vector obtained by the combination of color channel statistics+LBP+Hu+Haralick with Random Forest classifier for Leaf12 dataset resulted in a plant recognition accuracy (rank-1) of 82.38%. VGG 16 Convolutional Neural Network (CNN) architecture with logistic regression resulted in an accuracy of 97.14% for Leaf12 dataset. An accuracy of 96.53%, 96.25% and 99.41% is obtained for Folio, Flavia and Swedish leaf datasets using VGG 19 CNN architecture with logistic regression as a classifier. It is also observed that the VGG (Very large Convolutional Neural Network) CNN models provided a higher accuracy rate compared to traditional methods. © 2019 - IOS Press and the authors.},
	author_keywords = {convolutional neural network; deep learning; machine learning classification; Plant species recognition},
	keywords = {Color; Convolution; Decision trees; Discriminant analysis; Image processing; Intelligent systems; Nearest neighbor search; Network architecture; Neural networks; Regression analysis; Soft computing; Textures; Classification and regression tree; Convolutional neural network; Learning architectures; Linear discriminant analysis; Local binary patterns; Machine learning classification; Plant species; Random forest classifier; Deep learning},
	correspondence_address = {S. Anubha Pearline; Department of Electronics Engineering, Madras Institute of Technology, Anna University, Chennai, India; email: anubhapearl@gmail.com},
	publisher = {IOS Press},
	issn = {10641246},
	language = {English},
	abbrev_source_title = {J. Intelligent Fuzzy Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 53}
}

@ARTICLE{Syed2019334,
	author = {Syed, Azhar Talha and Merugu, Suresh and Koppula, Vijaya Kumar},
	title = {Plant recognition using spatial transformer network},
	year = {2019},
	journal = {International Journal of Recent Technology and Engineering},
	volume = {7},
	number = {5},
	pages = {334 – 336},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063695888&partnerID=40&md5=e1721645a1d7e4b0d488c98d80140c80},
	abstract = {Agriculture is one of the most prominent work sectors in countries like India. However, the majority of farmers are unaware of the modern plant diseases and the methods are to be followed to expect a better yield from their crops. Data science and Machine Learning have made a great progress in recent years for providing a solution to problems like these. Findings: By developing a system which will help the farmers in getting aware about the different species of plants without having a need for definite education would be very helpful to them. Objective: In this paper, we propose an efficient way of recognizing plants using cell phone cameras, as it will be very easy for the farmers and also other people who have their work involving plants, to get information about a plant which will help them in their work. We also provide a performance analysis on our solution and the previous work in this paper. Methods/Statistical Analysis: In Machine Learning terminology this is a multiclass classification problem where the input is an image and the expected output is the class of which the plant in the image belongs to. There are several ways of solving a multi-class classification problem such as using K nearest neighbors, Multiclass Support Vector Machines, Neural Networks, and Convolutional Neural Networks. But for this problem, we also take user convenience into consideration and we suggest the use of Spatial Transformer Network as the classification will still be accurate whilst the image is not properly aligned and has a lot of noise in it. © BEIESP.},
	author_keywords = {Convolutional neural networks; Deep learning; Plant recognition; Spatial transformer network},
	publisher = {Blue Eyes Intelligence Engineering and Sciences Publication},
	issn = {22773878},
	language = {English},
	abbrev_source_title = {Int. J. Recent Technol. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Pavel2019299,
	author = {Pavel, Monirul Islam and Kamruzzaman, Syed Mohammad and Hasan, Sadman Sakib and Sabuj, Saifur Rahman},
	title = {An IoT based plant health monitoring system implementing image processing},
	year = {2019},
	journal = {2019 IEEE 4th International Conference on Computer and Communication Systems, ICCCS 2019},
	pages = {299 – 303},
	doi = {10.1109/CCOMS.2019.8821783},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072977874&doi=10.1109%2fCCOMS.2019.8821783&partnerID=40&md5=ae375d8139ac854a3f9e8c5df94c9842},
	affiliations = {Department of Computer Science and Engineering, BRAC University, Dhaka, 1212, Bangladesh; Department of Electrical and Electronic Engineering, BRAC University, Dhaka, 1212, Bangladesh},
	abstract = {The combination of internet of things (IoT) with environmental sensing and image processing device has opened a new era to monitor the health of plants. Classification of plant diseases in early stages using image processing and analyzing environmental sensing data not only helps farmers to get healthy plants but also maximize the production. To monitor and classify plant diseases IoT is essential to send images and give feedback on it. In this paper, a raspberry pi based IoT device is proposed which sends images of plants to classify diseases and updates environmental parameters like air temperature, humidity, soil moisture and pH in MySQL database in real-time. To segment the affected part of plant, k-mean cluster algorithm is used after performing preprocessing stage and converting into L*a*b color space. Multi-class support vector Machine (SVM) is applied to categorize disease using fourteen types of features of color, texture and shape obtained when implementing gray level co-occurrence matrix where the system was able to classify with an accuracy of 97.33%. Thus, classifying diseases and analyzing environment parameters help farms to monitor plant growth efficiently for better production. © 2019 IEEE.},
	author_keywords = {Environmental sensing; Image processing; Internet of things; Multi-class SVM; Plant disease classification},
	keywords = {Classification (of information); Color; Internet of things; K-means clustering; Soil moisture; Support vector machines; Textures; Environmental parameter; Environmental sensing; Gray level co-occurrence matrix; Internet of Things (IOT); K-mean cluster algorithms; Multi-class support vector machines; Multiclass SVM; Plant disease; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172811322-7},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Comput. Commun. Syst., ICCCS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; Conference name: 4th IEEE International Conference on Computer and Communication Systems, ICCCS 2019; Conference date: 23 February 2019 through 25 February 2019; Conference code: 151726}
}

@ARTICLE{Ferentinos2019134,
	author = {Ferentinos, Konstantinos P. and Barda, Myrto and Damer, Dave},
	title = {An image-based deep learning model for cannabis diseases, nutrient deficiencies and pests identification},
	year = {2019},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {11804 LNAI},
	pages = {134 – 145},
	doi = {10.1007/978-3-030-30241-2_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072901726&doi=10.1007%2f978-3-030-30241-2_12&partnerID=40&md5=808c911c9553ad5fb95e389a0b0a8a5e},
	affiliations = {Department of Agricultural Engineering Soil and Water Resources Institute, Hellenic Agricultural Organization “Demeter”, Athens, Greece; Testfire Labs, Edmonton, Canada},
	abstract = {In this work, a deep learning system for cannabis plants disease, nutrient deficiencies and pests identification is developed, based on image data processed by convolutional neural network models. Training of the models was performed using image data available on the Internet, while database development included data cleansing by expert agronomists, basic image editing, and data augmentation techniques commonly used in deep learning applications in order to expand the rather limited amount of available data. Three fungi diseases, two pests and three nutrient deficiencies were included in the identification system, together with healthy plants identification. The final model reached a performance of 90.79% in successfully identifying cannabis diseases (or healthy plants) in previously “unseen” plant images. The most difficult cannabis problems to be identified were powdery mildew and potassium deficiency. Results showed that transfer learning from existing models specialized in similar tasks to the one under development, is more successful than using transfer learning from more general models. Finally, even though the amount of training images in some of the considered problems was significantly small, no correlation between model performance and the size of the training dataset for each category was found. © Springer Nature Switzerland AG 2019.},
	author_keywords = {Cannabis; Convolutional Neural Networks; Disease detection; Disease diagnosis},
	keywords = {Convolution; Diagnosis; Fungi; Image processing; Neural networks; Nutrients; Cannabis; Convolutional neural network; Database development; Disease detection; Disease diagnosis; Nutrient deficiency; Potassium deficiency; Transfer learning; Deep learning},
	correspondence_address = {K.P. Ferentinos; Department of Agricultural Engineering Soil and Water Resources Institute, Hellenic Agricultural Organization “Demeter”, Athens, Greece; email: k.ferentinos@swri.gr},
	editor = {Moura Oliveira P. and Novais P. and Reis L.P.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-303030240-5},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 19th EPIA Conference on Artificial Intelligence, EPIA 2019; Conference date: 3 September 2019 through 6 September 2019; Conference code: 231339}
}

@CONFERENCE{Dileep2019321,
	author = {Dileep, M.R. and Pournami, P.N.},
	title = {AyurLeaf: A Deep Learning Approach for Classification of Medicinal Plants},
	year = {2019},
	journal = {IEEE Region 10 Annual International Conference, Proceedings/TENCON},
	volume = {2019-October},
	pages = {321 – 325},
	doi = {10.1109/TENCON.2019.8929394},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077685822&doi=10.1109%2fTENCON.2019.8929394&partnerID=40&md5=ec56e965d6418b2bddc238654bb16f91},
	affiliations = {National Institute of Technology, Department of Computer Science and Engineering, Calicut, India},
	abstract = {Ayurvedic medicines have a vital role in preserving physical and mental health of human beings. Identification and classification of medicinal plants are essential for better treatment. Lack of experts in this field makes proper identification and classification of medicinal plants a tedious task. Hence, a fully automated system for medicinal plant classification is highly desirable. This work proposes AyurLeaf, a Deep Learning based Convolutional Neural Network (CNN) model, to classify medicinal plants using leaf features such as shape, size, color, texture etc. This research work also proposes a standard dataset for medicinal plants, commonly seen in various regions of Kerala, the state on southwestern coast of India. The proposed dataset contains leaf samples from 40 medicinal plants. A deep neural network inspired from Alexnet is utilised for the efficient feature extraction from the dataset. Finally, the classification is performed using Softmax and SVM classifiers. Our model, upon 5-cross validation, achieved a classification accuracy of 96.76% on AyurLeaf dataset. AyurLeaf helps us to preserve the traditional medicinal knowledge carried by our ancestors and provides an easy way to identify and classify medicinal plants. © 2019 IEEE.},
	author_keywords = {Classification; Convolutional Neural Network; Deep Learning; Leaf features; Medicinal plant},
	keywords = {Automation; Convolution; Deep learning; Deep neural networks; Neural networks; Plants (botany); Support vector machines; Textures; Ayurvedic medicine; Classification accuracy; Convolutional neural network; Fully automated; Leaf features; Learning approach; Medicinal plants; SVM classifiers; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21593442},
	isbn = {978-172811895-6},
	coden = {85QXA},
	language = {English},
	abbrev_source_title = {IEEE Reg 10 Annu Int Conf Proc TENCON},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 31; Conference name: 2019 IEEE Region 10 Conference: Technology, Knowledge, and Society, TENCON 2019; Conference date: 17 October 2019 through 20 October 2019; Conference code: 155997}
}

@CONFERENCE{Villaruz2019,
	author = {Villaruz, Jolitte A. and Salido, Julie Ann A. and Barrios, Dennis M. and Felizardo, Rogelio L.},
	title = {Philippine indigenous plant seedlings classification using deep learning},
	year = {2019},
	journal = {2018 IEEE 10th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment and Management, HNICEM 2018},
	doi = {10.1109/HNICEM.2018.8666412},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064137936&doi=10.1109%2fHNICEM.2018.8666412&partnerID=40&md5=e32753d071288c405e92a116cc16d8d8},
	affiliations = {College of Industrial Technology, Aklan Sate University, Aklan, Philippines; College of Agriculture Forestry and Environmental Sciences, Aklan Sate University, Aklan, Philippines},
	abstract = {Plant taxonomists have specialized knowledge on a specific plant species. The shortage of these experts and their unbalanced distribution throughout the globe remains to be a serious problem worldwide. Deep learning technology can be used to create tools that will help them speed up the process of plant classification. This study implements three deep learning models to classify images of Philippine indigenous plant seedlings into five species. AlexNet, GoogLeNet, and ResNet50 were fine-tuned for this purpose. In the three pre-trained models, the weight and bias learning rate factors of the fully connected layers were both increased to 20 to speed up learning. Augmentation techniques such as rotation, random flip, and random horizontally and vertically translations were performed to training images to avoid the effects of overfitting. The resulting deep learning models can be implemented to classify Philippine indigenous plant seedlings as they all achieved above 90% accuracy on the validation set, with ResNet50 exhibited the highest with 98.93%. © 2018 IEEE.},
	author_keywords = {Deep learning; Deep neural networks; Philippine indigenous plant; Plant classification; Transfer learning},
	keywords = {Deep learning; Deep neural networks; Environmental management; Nanotechnology; Augmentation techniques; Learning models; Learning technology; Philippine indigenous plant; Plant classification; Specialized knowledge; Transfer learning; Unbalanced distribution; Seed},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153867767-4},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Humanoid, Nanotechnol., Inf. Technol., Commun. Control, Environ. Manag., HNICEM},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 10th IEEE International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment and Management, HNICEM 2018; Conference date: 29 November 2018 through 2 December 2018; Conference code: 146034}
}

@ARTICLE{Yang2019178108,
	author = {Yang, Chengzhuan and Wei, Hui},
	title = {Plant Species Recognition Using Triangle-Distance Representation},
	year = {2019},
	journal = {IEEE Access},
	volume = {7},
	pages = {178108 – 178120},
	doi = {10.1109/ACCESS.2019.2958416},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077181557&doi=10.1109%2fACCESS.2019.2958416&partnerID=40&md5=4cdd287ec19a5aa8cf2dbaf046e437b3},
	affiliations = {School of Information, Zhejiang University of Finance and Economics, Hangzhou, 310018, China; Laboratory of Cognitive Algorithm and Model, Shanghai Key Laboratory of Data Science, School of Computer Science_aor4 Fudan University, Shanghai, 201203, China},
	abstract = {Plant species recognition using leaf images is a highly important and challenging issue in botany and pattern recognition. A center problem of this task is how to accurately extract leaf image characteristics and quickly calculate the similarity between them. This article presents a new shape description approach called triangle-distance representation (TDR) for plant leaf recognition. The TDR descriptor is represented by two matrices: a sign matrix and a triangle center distance matrix. The sign matrix is used to characterize the convex/concave property of a shape contour, while the triangle center distance matrix is used to represent the bending degree and spatial information of a shape contour. This method can effectively capture the detailed and global characteristics of a leaf shape while keeping the similarity transformations (translation, rotation, and scaling) unchanged. In addition, this method is quite compact and has low computational complexity. We tested our method on four standard plant leaf datasets, including the famous Swedish, Smithsonian, Flavia, and ImageCLEF2012 datasets. The results confirm that our approach exceeds the prior state-of-the-art shape-based plant leaf recognition approaches. An extra experiment on the MPEG-7 shape dataset further shows that our method can be applied to general shape recognition. © 2013 IEEE.},
	author_keywords = {Plant species recognition; shape descriptor; shape matching; triangle-distance representation},
	keywords = {Motion Picture Experts Group standards; Object recognition; Low computational complexity; Plant species; Shape description; Shape descriptors; Shape matching; Similarity transformation; Spatial informations; triangle-distance representation; Plants (botany)},
	correspondence_address = {C. Yang; School of Information, Zhejiang University of Finance and Economics, Hangzhou, 310018, China; email: chengzhuanyang@zufe.edu.cn},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21693536},
	language = {English},
	abbrev_source_title = {IEEE Access},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access, Gold Open Access}
}

@ARTICLE{Goyal201927785,
	author = {Goyal, Neha and Gupta, Kapil and Kumar, Nitin},
	title = {Multiclass Twin Support Vector Machine for plant species identification},
	year = {2019},
	journal = {Multimedia Tools and Applications},
	volume = {78},
	number = {19},
	pages = {27785 – 27808},
	doi = {10.1007/s11042-019-7588-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068208248&doi=10.1007%2fs11042-019-7588-2&partnerID=40&md5=e1319c746890c29f21e3e1913757377b},
	affiliations = {NIT, Kurukshetra, India; NIT, Uttarakhand, India},
	abstract = {Automatic plant species identification is one of the recent and fascinating research area as plants are crucial element of ecosystem. Several plant species exist with significant importance but most of us are unaware of the diversity of plant species available on earth. Their utility to humans starts as oxygen provider, food source, and medicinal compounds essential for medicines that are difficult to develop in right proportions. Being the first living habitants of earth, they have roots far deeper in the ecosystem than any living being. Hence, it is utmost important to develop automatic plant species identification system in which the digital image of the plant is given as input and the label of the plant is determined by the system. In this paper, we have focused on three different aspects (i) Significance of threshold (ii) Feature descriptor that can best describe the leaf images and (iii) Proposed a novel classification method called Multi class Twin Support Vector Machine which in an extension of widely used Twin Support Vector Machine classifier. The performance of the proposed method is compared with SVM, Multi Birth SVM and Probabilistic Neural Network. It is observed that the proposed classifier outperforms all the aforementioned classifiers on publicly available datasets. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Feature extraction; Image segmentation; Multiclass classification; Plant recognition; TWSVM},
	keywords = {Ecosystems; Feature extraction; Image segmentation; Neural networks; Support vector machines; Classification methods; Feature descriptors; Multi-class classification; Plant recognition; Plant species identification; Probabilistic neural networks; Twin support vector machines; TWSVM; Classification (of information)},
	correspondence_address = {N. Goyal; NIT, Kurukshetra, India; email: neha.goyal2309@gmail.com},
	publisher = {Springer New York LLC},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Bouchahma2019129,
	author = {Bouchahma, Majed and Al-Balushi, Mohsin and Al-Hosni, Sheikha and Al Wardi, Hamood},
	title = {Decision aid system for Omani medical herb leaves recognition using computer vision and artificial intelligence},
	year = {2019},
	journal = {International Journal of Information and Decision Sciences},
	volume = {11},
	number = {2},
	pages = {129 – 140},
	doi = {10.1504/IJIDS.2019.101142},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069803226&doi=10.1504%2fIJIDS.2019.101142&partnerID=40&md5=2d1fe913ae448aab756f3546cb9418fe},
	affiliations = {Department of Information Technology, College of Applied Sciences, Sultanate of Oman, P.O. Box 10, P.C: 329, Rustaq, Oman; Laboratoire SIIVA, Institut Supérieur d’Informatique, Université de Tunis, El Manar, Tunisia; College of Applied Sciences, P.O. Box 10, P.C: 329, Rustaq, Oman},
	abstract = {Herbs have been widely used in food preparation, medicine and cosmetic industry. Knowing which herbs to be used would be very critical in these applications. This research aims to define a method to classify the herbs plants based on their leaves colours and shapes. An open source decision aid system is designed and developed especially for helping scientist. The proposed system employs artificial and image processing techniques to perform recognition on a number of Omani species of medical herbs. © 2019 Inderscience Enterprises Ltd.},
	author_keywords = {Artificial intelligence; Computer vision; DAS; Decision aid system; Medical herbs; Speeded up robust features; SURF},
	correspondence_address = {M. Bouchahma; Department of Information Technology, College of Applied Sciences, Sultanate of Oman, Rustaq, P.O. Box 10, P.C: 329, Oman; email: dr.majed.rus@cas.edu.om},
	publisher = {Inderscience Publishers},
	issn = {17567017},
	language = {English},
	abbrev_source_title = {Int. J. Inf. Decis. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hu20192135,
	author = {Hu, Mingyue and Fen, Hailin and Yang, Yinhui and Xia, Kai and Ren, Lijin},
	title = {Tree Species Identification Based on the Fusion of Multiple Deep Learning Models Transfer Learning},
	year = {2019},
	journal = {Proceedings 2018 Chinese Automation Congress, CAC 2018},
	pages = {2135 – 2140},
	doi = {10.1109/CAC.2018.8623484},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062789393&doi=10.1109%2fCAC.2018.8623484&partnerID=40&md5=967b095e835a1cf1e4bfb1f1374fc305},
	affiliations = {School of Information Engineering, Zhejiang AandF University, Hangzhou, China; Key Laboratory of Forestry Intelligent Monitoring and Information Technology of Zhejiang Province, Hangzhou, China},
	abstract = {The automatic identification of tree species images is of great value in practical application. The conventional identification algorithm based on hand-crafted features has a complex process of feature extraction, and it is difficult to make an adaptive optimal adjustment based on actual data and specific identification tasks, which is not suitable for the accurate identification of tree species images with a complex background. A tree species identification method based on the fusion of multiple deep learning models transfer learning is proposed to solve the problem of tree species images identification with a complex background in natural scenes. A new tree image dataset called TreesNet was built. Based on the TreesNet dataset, a variety of experiments were designed, and the method proposed was compared with the conventional image classification methods. The experimental results show that the image identification accuracy of the tree species in the complex background with the method proposed in this paper reaches 93.75%, which is much better than the conventional machine learning method. © 2018 IEEE.},
	author_keywords = {deep learning; image identification; transfer learning; tree species identification},
	keywords = {Automation; Classification (of information); Forestry; Image understanding; Trees (mathematics); Automatic identification; Classification methods; Complex background; Conventional identification; Conventional machines; Image identification; Transfer learning; Tree species identifications; Deep learning},
	correspondence_address = {H. Fen; School of Information Engineering, Zhejiang AandF University, Hangzhou, China; email: hlfeng@zafu.edu.cn},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172811312-8},
	language = {English},
	abbrev_source_title = {Proc. Chin. Autom. Congr., CAC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2018 Chinese Automation Congress, CAC 2018; Conference date: 30 November 2018 through 2 December 2018; Conference code: 144512}
}

@ARTICLE{Chen20191122,
	author = {Chen, Hui and Zhu, Haodong and Chai, Xufeng},
	title = {Plant leaves recognition combined PCA with AdaBoost.M1},
	year = {2019},
	journal = {International Journal of Performability Engineering},
	volume = {15},
	number = {4},
	pages = {1122 – 1130},
	doi = {10.23940/ijpe.19.04.p7.11221130},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065844289&doi=10.23940%2fijpe.19.04.p7.11221130&partnerID=40&md5=92f65463a24fcb4f369f870c80ababd4},
	affiliations = {Engineering Training Centre, Zhengzhou University of Light Industry, Zhengzhou, 450002, China; School of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zhengzhou, 450002, China; Sida Foreign Language Primary School, Henan Experimental High School, Zhengzhou, 450000, China},
	abstract = {In order to improve the overall performance of plant leaves recognition, this paper proposed a novel method combining PCA with AdaBoost.M1to recognize plant leaves. The proposed method firstly carries out the image preprocessing, which includes the image gray processing, the image binarization, and the edge extraction; extracts the 13 features of plant leaf with the characteristics of rotation invariance, proportion invariance, and translation invariance; subsequently employs PCA to reduce the dimensions of these feature parameters; and finally adopts the AdaBoost.M1 classifier to train and recognize the reduced-dimension plant leaf images. Simulation experiment results indicate that the proposed method is able to improve the overall performance effectively of plant leaves recognition. © 2019 Totem Publisher, Inc. All rights reserved.},
	author_keywords = {AdaBoost.M1; Image processing; PCA; Performance improvement; Plant leaves recognition},
	keywords = {Image enhancement; Image processing; AdaBoost.M1; Feature parameters; Image binarization; Image preprocessing; Performance improvement; Plant leaves; Rotation invariance; Translation invariance; Plants (botany)},
	correspondence_address = {H. Zhu; School of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zhengzhou, 450002, China; email: zhuhaodong80@163.com},
	publisher = {Totem Publishers Ltd},
	issn = {09731318},
	language = {English},
	abbrev_source_title = {Int. J. Perform. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Bronze Open Access}
}

@ARTICLE{Liu2019310,
	author = {Liu, Peng and Zhang, Jingcheng and Wang, Bin and Zhang, Xuexue and Wu, Kaihua},
	title = {Study on vegetation classification based on spectral knowledge base},
	year = {2019},
	journal = {IFIP Advances in Information and Communication Technology},
	volume = {546},
	pages = {310 – 320},
	doi = {10.1007/978-3-030-06179-1_32},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060244826&doi=10.1007%2f978-3-030-06179-1_32&partnerID=40&md5=f2e4f41c25fd262f07fdbc2e4bdd10f6},
	affiliations = {College of Life Information Science and Instrument Engineering, Hangzhou Dianzi University, Hangzhou, 310018, Zhejiang, China},
	abstract = {A framework about spectral based vegetation classification was proposed, which serves as a core methodology of the vegetation spectral knowledge base. The hyperspectral reflectances of 13 types of plants were measured by an ASD FieldSpec 4 spectroradiometer. Two forms of spectral features were used for representing the key spectral characteristics of plants, including Vegetation index (VI) and spectral shape features. Based on these spectral features, a sensitivity analysis was performed to identify the most important features for establishing the classifier. The analysis of variance (ANOVA) and the cross-correlation analysis were applied to derive the sensitivity of features and remove features that have high correlations. Then, a classification method for differentiating plants was established by coupling some spectral similarity measures (e.g., ED) with some classification methods (e.g., BPANN and SVM). The results of discrimination analysis showed that a highest accuracy was produced by SVM with the OAA over 99% when using 7 sensitive VIs. The results suggested the framework about spectral based vegetation classification can form a basis for spectral knowledge base and application technology and further achieve a wide range of plant classification based on remote sensing. © IFIP International Federation for Information Processing 2019.},
	author_keywords = {Classification algorithm; Feature extraction; Hyperspectral; Vegetation classification},
	keywords = {Agriculture; Analysis of variance (ANOVA); Feature extraction; Knowledge based systems; Remote sensing; Sensitivity analysis; Vegetation; Application technologies; Classification algorithm; Cross-correlation analysis; Discrimination analysis; HyperSpectral; Hyperspectral reflectance; Spectral characteristics; Vegetation classification; Classification (of information)},
	correspondence_address = {J. Zhang; College of Life Information Science and Instrument Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; email: zhangjc_rs@163.com},
	editor = {Li D. and Zhao C.},
	publisher = {Springer New York LLC},
	issn = {18684238},
	isbn = {978-303006178-4},
	language = {English},
	abbrev_source_title = {IFIP Advances in Information and Communication Technology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 11th IFIP WG 5.14 International Conference on Computer and Computing Technologies in Agriculture, CCTA 2017; Conference date: 12 August 2017 through 15 August 2017; Conference code: 222899; All Open Access, Green Open Access}
}

@CONFERENCE{Habiba2019,
	author = {Habiba, Sultana Umme and Islam, Md. Khairul and Ahsan, Sk. Md. Masudul},
	title = {Bangladeshi Plant Recognition using Deep Learning based Leaf Classification},
	year = {2019},
	journal = {5th International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering, IC4ME2 2019},
	doi = {10.1109/IC4ME247184.2019.9036515},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082987621&doi=10.1109%2fIC4ME247184.2019.9036515&partnerID=40&md5=bc13304fd591c5f2b9010920cc154892},
	affiliations = {Khulna University of Engineering Technology, Department of Computer Science and Engineering, Khulna, 9203, Bangladesh},
	abstract = {At present deep learning-based object recognition approaches have placed a tremendous effect for classifying different objects. Leaves recognition using supervised learning has shown satisfying performance which may help in various research purposes also. In our work, we have used a deep convolutional neural network as a classifier. We have used a transfer learning approach. We have prepared our work dataset based on Bangladeshi plants which contains eight different classes of leaves. We have experimented with VGG16, VGG19, Resnet50, InceptionV3, Inception-Resnetv2 and Xception deep convolutional neural network models where we have found the highest value in VGG 16 which shows almost 96% classification accuracy. Recognition of useful plants using leaf image will be greatly helpful in the research of ayurvedic and endangered plants. © 2019 IEEE.},
	author_keywords = {deep convolutional neural network; leaf classification; transfer learning},
	keywords = {Conservation; Convolution; Convolutional neural networks; Deep neural networks; Object recognition; Plants (botany); Transfer learning; Classification accuracy; Different class; Endangered plants; Leaf classification; Leaf images; Plant recognition; Research purpose; Deep learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172813060-6},
	language = {English},
	abbrev_source_title = {Int. Conf. Comput., Commun., Chem., Mater. Electron. Eng., IC4ME2},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 5th International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering, IC4ME2 2019; Conference date: 11 July 2019 through 12 July 2019; Conference code: 158616}
}

@CONFERENCE{Neforawati2019,
	author = {Neforawati, Indri and Herman, Nanna Suryana and Mohd, Othman},
	title = {Precision agriculture classification using convolutional neural networks for paddy growth level},
	year = {2019},
	journal = {Journal of Physics: Conference Series},
	volume = {1193},
	number = {1},
	doi = {10.1088/1742-6596/1193/1/012026},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065606114&doi=10.1088%2f1742-6596%2f1193%2f1%2f012026&partnerID=40&md5=67e6abbe01d23eeb57e3a4ca4d14ccaf},
	affiliations = {Politeknik Negeri Jakarta, Jakarta, Indonesia; Universiti Teknikal Malaysia Melaka, Malaysia},
	abstract = {Precision Agricultural is a key component of modern agricultural. Several researchers tried to use various machine learning models as precision agricultural classification and recognition model, but surprisingly merely few researchers use Deep learning models to solve precision agriculture problems like Paddy Classification, Plant Classification or Fruit Classification. In this research, Precision Agriculture Classification on Paddy Image Dataset was performed using Convolutional Neural Networks. Paddy should be catered well in order tomonitor time to harvest, time to watering, and other tasks. The result of classification, we obtained 82% overall accuracy. © Published under licence by IOP Publishing Ltd.},
	keywords = {Convolution; Deep learning; Neural networks; Precision agriculture; Classification and recognition; Convolutional neural network; Image datasets; Learning models; Machine learning models; Overall accuracies; Plant classification; Classification (of information)},
	correspondence_address = {I. Neforawati; Politeknik Negeri Jakarta, Jakarta, Indonesia; email: indri.neforawati@tik.pnj.ac.id},
	publisher = {Institute of Physics Publishing},
	issn = {17426588},
	language = {English},
	abbrev_source_title = {J. Phys. Conf. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 2018 International Conference of Computer and Informatics Engineering, IC2IE 2018; Conference date: 12 September 2018 through 13 September 2018; Conference code: 147784; All Open Access, Bronze Open Access}
}

@ARTICLE{Imanov2019586,
	author = {Imanov, Elbrus and Alzouhbi, Abdallah Khaled},
	title = {Machine learning comparative analysis for plant classification},
	year = {2019},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {896},
	pages = {586 – 593},
	doi = {10.1007/978-3-030-04164-9_77},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059739577&doi=10.1007%2f978-3-030-04164-9_77&partnerID=40&md5=16c042b8b61c7b959a036b13b922621e},
	affiliations = {Department of Computer Engineering, Near East University, Mersin 10, North Cyprus, Turkey; Department of Water Recycling, Machha, Akkar, 1032, Lebanon},
	abstract = {Nowadays, digital image processing, artificial neural network and machine visualization have been pettishly progressing, and they cover a significant side of artificial cleverness and the rule among human beings and electro-mechanical devices. These technologies have been utilized in a wide range of agricultural operations, medicine and manufacturing. By this research the preparation of some functions has been conducted. In this paper we introduce the classification of maize leaves from pictures that reveal many conditions, opening among pictures, by pre-processing, taking out, plant feature recognition, matching and training, and lastly getting the outcomes executed by Matlab, neural network pattern recognition application. These given features are separated to leaf maturity and picture interpretations, rotary motions and calibration, and they are calculated to develop an approach that gives us better classification algorithm results. A plant scientist may be introduced with a plant for recognition of its classes revealed in its natural home ground, to gather an in-depth recognition. © 2019, Springer Nature Switzerland AG.},
	author_keywords = {Artificial neural network; Digital image processing; Machine learning; Machine visualization classification K-nearest neighbor; Support vector machine},
	keywords = {Computation theory; Digital devices; Electromechanical devices; Fuzzy systems; Learning systems; MATLAB; Nearest neighbor search; Neural networks; Pattern recognition; Plants (botany); Soft computing; Support vector machines; Visualization; Agricultural operations; Classification algorithm; Comparative analysis; Feature recognition; Given features; K-nearest neighbors; Plant classification; Rotary motions; Image processing},
	correspondence_address = {E. Imanov; Department of Computer Engineering, Near East University, Mersin 10, Turkey; email: elbrus.imanov@neu.edu.tr},
	editor = {Kacprzyk J. and Pedrycz W. and Jamshidi M. and Sadikoglu F.M. and Aliev R.A.},
	publisher = {Springer Verlag},
	issn = {21945357},
	isbn = {978-303004163-2},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 13th International Conference on Application of Fuzzy Systems and Soft Computing, ICAFS 2018; Conference date: 27 August 2018 through 28 August 2018; Conference code: 222439}
}

@ARTICLE{Saleem2019270,
	author = {Saleem, G. and Akhtar, M. and Ahmed, N. and Qureshi, W.S.},
	title = {Automated analysis of visual leaf shape features for plant classification},
	year = {2019},
	journal = {Computers and Electronics in Agriculture},
	volume = {157},
	pages = {270 – 280},
	doi = {10.1016/j.compag.2018.12.038},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059538505&doi=10.1016%2fj.compag.2018.12.038&partnerID=40&md5=254c4ce25414fdf9ff04a1eff3b203da},
	affiliations = {Department of Computer and Software Engineering, National University of Sciences & Technology H-12, Islamabad, Pakistan; School of Civil and Environmental Engineering, Research Centre for Integrated Transport Innovation, University of New South Wales, Sydney, 2052, NSW, Australia; Department of Computer Science and Engineering, University of Engineering and Technology, Lahore, Pakistan; Department of Mechatronics Engineering, National University of Sciences & Technology H-12, Islamabad, Pakistan},
	abstract = {A large number of studies have been performed during the past few years to automatically identify the plant type in a given image. Besides common object recognition difficulties arising mainly due to light, pose and orientation variations, the plant type identification problem is further complicated by the differences in leaf shape overage and changing leaf color under different weather conditions. The limited accuracy of existing approaches can be improved using an appropriate selection of representative leaf based features. This study evaluates different handcrafted visual leaf features, their extraction techniques, and classification methods. Towards this end, a new five-step algorithm is presented (comprising image pre-processing, segmentation, feature extraction, dimensionality reduction, and classification steps) for recognition of plant type through leaf images. The proposed algorithm is evaluated on a publicly available standard dataset ‘Flavia’ of 1600 leaf images and on a self-collected dataset of 625 leaf images. With the proposed algorithm, different classifiers such as k-nearest neighbor (KNN), decision tree, naïve Bayes, and multi-support vector machines (SVM) are tested. The best performing KNN, claimed for the final results, reveals that the proposed algorithm gives precision and recall values of 97.6% and 98.8% respectively when tested on ‘Flavia’ dataset. The proposed technique is also tested on our self-collected dataset, giving respectively 96.1% and 97.3% precision and recall measure results. Results confirm that our approach, when augmented with efficient segmentation techniques on raw leaf images, can be a significantly accurate plant type recognition method in practical situations. AlexNet, a Convolutional Neural Network (CNN) based approach is also compared for classification on the datasets as oppose to handcrafted feature-based approach and it is found that the later outperforms the former in robustness when the training dataset is small. © 2019 Elsevier B.V.},
	author_keywords = {Dimensionality reduction; Feature extraction; K-nearest neighbours; Naive Bayes; Segmentation},
	keywords = {Data mining; Decision trees; Extraction; Feature extraction; Image segmentation; Learning algorithms; Nearest neighbor search; Neural networks; Object recognition; Support vector machines; Trees (mathematics); Convolutional Neural Networks (CNN); Dimensionality reduction; Feature based approaches; Identification problem; K nearest neighbor (KNN); K-nearest neighbours; Naive bayes; Segmentation techniques; accuracy assessment; algorithm; artificial neural network; Bayesian analysis; climate conditions; data set; detection method; identification method; image analysis; image classification; leaf; numerical method; plant; precision; segmentation; Classification (of information)},
	correspondence_address = {W.S. Qureshi; Department of Mechatronics Engineering, National University of Sciences & Technology H-12, Islamabad, Pakistan; email: waqar.shahid@alumni.ait.asia},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 83}
}

@ARTICLE{Fricker2019,
	author = {Fricker, Geoffrey A. and Ventura, Jonathan D. and Wolf, Jeffrey A. and North, Malcolm P. and Davis, Frank W. and Franklin, Janet},
	title = {A convolutional neural network classifier identifies tree species in mixed-conifer forest from hyperspectral imagery},
	year = {2019},
	journal = {Remote Sensing},
	volume = {11},
	number = {19},
	doi = {10.3390/rs11192326},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073458685&doi=10.3390%2frs11192326&partnerID=40&md5=b4e5bbd4ade66c2e9b67326f460a0045},
	affiliations = {Department of Social Sciences, California Polytechnic State University, San Luis Obispo, 93407, CA, United States; Department of Botany and Plant Sciences, University of California, Riverside, 92521, CA, United States; Department of Computer Science and Software Engineering, California Polytechnic State University, San Luis Obispo, 93407, CA, United States; AmazonWeb Services, Amazon Corp., Seattle, 98109, WA, United States; U.S. Forest Service, PSW Research Station, Mammoth Lakes, 93546, CA, United States; Bren School of Environmental Science and oManagement, University of California, Santa Barbara, 93106, CA, United States},
	abstract = {In this study, we automate tree species classification and mapping using field-based training data, high spatial resolution airborne hyperspectral imagery, and a convolutional neural network classifier (CNN). We tested our methods by identifying seven dominant trees species as well as dead standing trees in a mixed-conifer forest in the Southern Sierra Nevada Mountains, CA (USA) using training, validation, and testing datasets composed of spatially-explicit transects and plots sampled across a single strip of imaging spectroscopy. We also used a three-band 'Red-Green-Blue' pseudo true-color subset of the hyperspectral imagery strip to test the classification accuracy of a CNN model without the additional non-visible spectral data provided in the hyperspectral imagery. Our classifier is pixel-based rather than object based, although we use three-dimensional structural information from airborne Light Detection and Ranging (LiDAR) to identify trees (points > 5 m above the ground) and the classifier was applied to image pixels that were thus identified as tree crowns. By training a CNN classifier using field data and hyperspectral imagery, we were able to accurately identify tree species and predict their distribution, as well as the distribution of tree mortality, across the landscape. Using a window size of 15 pixels and eight hidden convolutional layers, a CNN model classified the correct species of 713 individual trees from hyperspectral imagery with an average F-score of 0.87 and F-scores ranging from 0.67-0.95 depending on species. The CNN classification model performance increased from a combined F-score of 0.64 for the Red-Green-Blue model to a combined F-score of 0.87 for the hyperspectral model. The hyperspectral CNN model captures the species composition changes across ~700 meters (1935 to 2630 m) of elevation from a lower-elevation mixed oak conifer forest to a higher-elevation fir-dominated coniferous forest. High resolution tree species maps can support forest ecosystem monitoring and management, and identifying dead trees aids landscape assessment of forest mortality resulting from drought, insects and pathogens. We publicly provide our code to apply deep learning classifiers to tree species identification from geospatial imagery and field training data. © 2019 by the authors.},
	author_keywords = {Convolutional neural networks; Deep learning; Hyperspectral imagery; Species distribution modeling},
	keywords = {Classification (of information); Convolution; Deep learning; Deep neural networks; Ecosystems; Image classification; Neural networks; Optical radar; Pixels; Population distribution; Remote sensing; Spectroscopy; Well testing; Classification accuracy; Convolutional neural network; High spatial resolution; Hyper-spectral imageries; Light detection and ranging; Species distribution modeling; Structural information; Tree species identifications; Forestry},
	correspondence_address = {G.A. Fricker; Department of Social Sciences, California Polytechnic State University, San Luis Obispo, 93407, United States; email: africker@calpoly.edu},
	publisher = {MDPI AG},
	issn = {20724292},
	language = {English},
	abbrev_source_title = {Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 97; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Xue2019123,
	author = {Xue, Jin Ru and Fuentes, Sigfredo and Poblete-Echeverria, Carlos and Viejo, Claudia Gonzalez and Tongson, Eden and Du, He Juan and Su, Baofeng},
	title = {Automated Chinese medicinal plants classification based on machine learning using leaf morpho-colorimetry, fractal dimension and visible/near infrared spectroscopy},
	year = {2019},
	journal = {International Journal of Agricultural and Biological Engineering},
	volume = {12},
	number = {2},
	pages = {123 – 131},
	doi = {10.25165/j.ijabe.20191202.4637},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065200667&doi=10.25165%2fj.ijabe.20191202.4637&partnerID=40&md5=92f91fe2c62325475c0294e52314bf8b},
	affiliations = {College of Mechanical and Electronic Engineering, Northwest A & F University, Yangling, 712100, Shaanxi, China; School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, The University of Melbourne, 3010, Australia; Faculty of AgriSciences, Stellenbosch University, Stellenbosch, 7602, South Africa; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, Shaanxi, China; Key Laboratory of Agricultural Perception and Intelligent Services, Yangling, 712100, Shaanxi Province, China; College of Information Engineering, Tibet Nationality University, Xianyang, 712089, Shaanxi Province, China},
	abstract = {The identification of Chinese medicinal plants was used to rely on ampelographic manual assessment by experts. More recently, machine learning algorithms for pattern recognition have been successfully applied to leaf recognition in other plant species. These new tools make the classification of Chinese medicinal plants easier, more efficient and cost effective. This study showed comparative results between machine learning models obtained from two methods: i) a morpho-colorimetric method and ii) a visible (VIS)/Near Infrared (NIR) spectral analysis from sampled leaves of 20 different Chinese medicinal plants. Specifically, the automated image analysis and VIS/NIR spectral based parameters obtained from leaves were used separately as inputs to construct customized artificial neural network (ANN) models. Results showed that the ANN model developed using the morpho-colorimetric parameters as inputs (Model A) had an accuracy of 98.3% in the classification of leaves for the 20 medicinal plants studied. In the case of the model based on spectral data from leaves (Model B), the ANN model obtained using the averaged VIS/NIR spectra per leaf as inputs showed 92.5% accuracy for the classification of all medicinal plants used. Model A has the advantage of being cost effective, requiring only a normal document scanner as measuring instrument. This method can be adapted for non-destructive assessment of leaves in-situ by using portable wireless scanners. Model B combines the fast, non-destructive advantages of VIS/NIR spectroscopy, which can be used for rapid and non-invasive identification of Chinese medicinal plants and other applications by analyzing specific light spectra overtones from leaves to assess concentration of pigments such as chlorophyll, anthocyanins and others that are related active compounds from the medicinal plants. © 2019, Chinese Society of Agricultural Engineering. All rights reserved.},
	author_keywords = {Ampelography; Artificial neural networks; Computer vision; Pattern recognition},
	correspondence_address = {B. Su; College of Mechanical and Electronic Engineering, Northwest A & F University, Yangling, 712100, China; email: bfs@nwsuaf.edu.cn},
	publisher = {Chinese Society of Agricultural Engineering},
	issn = {19346344},
	language = {English},
	abbrev_source_title = {Int. J. Agric. Biol. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; All Open Access, Gold Open Access}
}

@CONFERENCE{Syahputra2019,
	author = {Syahputra, Hermawan and Indra, Zulfahmi and Febrian, Didi and Putri Adriani, Dhea},
	title = {Leaf feature extraction using glcm, moment invariant and shape morphology for indonesian medicinal plants recognition},
	year = {2019},
	journal = {Journal of Physics: Conference Series},
	volume = {1317},
	number = {1},
	doi = {10.1088/1742-6596/1317/1/012008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075329050&doi=10.1088%2f1742-6596%2f1317%2f1%2f012008&partnerID=40&md5=fb386b0989369cb9968912a5d55cf1ab},
	affiliations = {Department of Mathematics, Faculty of Mathematics and Natural Science, Universitas Negeri Medan, Indonesia},
	abstract = {This study aims to determine the extraction of GLCM texture features, shape morphology and moment invariant features on the leaf image of medicinal plants and determine the accuracy of plant recognition based on these three features by using Artificial Neural Network Classifiers. The procedure performed to classify medicinal plants based on their leaf image is image acquisition, image pre-processing, feature extraction, image classification and calculating the accuracy of test results. The introduction had tested for ten Indonesian medicinal plant samples, namely: Bangun-Bangun, Binahong, Jarak, Kemuning, Mangkokan, Mengkudu, Pegagan, Sambiloto, Sambung Nyawa, and Sirih. Based on the test results, obtained 97% accuracy with GLCM features, 69% with Shape Morphological features, 86% with GLCM and Shape Morphological features and 79% with moment invariant features. © 2019 IOP Publishing Ltd.},
	keywords = {Extraction; Feature extraction; Image acquisition; Morphology; Neural networks; Plants (botany); Textures; Artificial neural network classifiers; Image preprocessing; Medicinal plants; Moment invariant; Morphological features; Plant recognition; Shape morphology; Texture features; Classification (of information)},
	editor = {Ramli null and Universitas Negeri Padang, Faculty of Mathematics and Natural Sciences, Department of Physics, Jl. Prof. Dr. Hamka, Air Tawar, Padang and Khair M. and Universitas Negeri Padang, Kampus FMIPA UNP, Jl. Prof. Dr. Hamka, Air Tawar, Padang and Alizar null and Universitas Negeri Padang, Kampus FMIPA UNP, Jl. Prof. Dr. Hamka, Air Tawar, Padang and Sumarmin R. and Universitas Negeri Padang, Kampus FMIPA UNP, Jl. Prof. Dr. Hamka, Air Tawar, Padang and Putri D.H. and Universitas Negeri Padang, Kampus FMIPA UNP, Jl. Prof. Dr. Hamka, Air Tawar, Padang and Yohandri null and Universitas Negeri Padang, Faculty of Mathematics and Natural Sciences, Department of Physics, Jl. Prof. Dr. Hamka, Air Tawar, Padang and Festiyed null and Universitas Negeri Padang, Faculty of Mathematics and Natural Sciences, Department of Physics, Jl. Prof. Dr. Hamka, Air Tawar, Padang and Permana D. and Universitas Negeri Padang, Kampus FMIPA UNP, Jl. Prof. Dr. Hamka, Air Tawar, Padang},
	publisher = {Institute of Physics Publishing},
	issn = {17426588},
	language = {English},
	abbrev_source_title = {J. Phys. Conf. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Conference on Mathematics, Sciences, Education, and Technology, ICOMSET 2018; Conference date: 4 October 2018 through 5 October 2018; Conference code: 154491; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Kounalakis2019,
	author = {Kounalakis, Tsampikos and Triantafyllidis, Georgios A. and Nalpantidis, Lazaros},
	title = {Deep learning-based visual recognition of rumex for robotic precision farming},
	year = {2019},
	journal = {Computers and Electronics in Agriculture},
	volume = {165},
	doi = {10.1016/j.compag.2019.104973},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071398904&doi=10.1016%2fj.compag.2019.104973&partnerID=40&md5=5d779e33dadc915f0f6b3f841eaa227a},
	affiliations = {Service Robotics, Department of Robot Technology, Danish Technological Institute, Odense, Denmark; Department of Architecture, Design and Media Technology, Aalborg University, Copenhagen, Denmark; Department of Electrical Engineering, Technical University of Denmark, Kgs. Lyngby, Denmark},
	abstract = {In this paper we address the problem of recognising the Broad-leaved dock (Rumex obtusifolius L.) in grasslands from high-resolution 2D images. We discuss and present the determining factors for developing and implementing weed visual recognition algorithms using deep learning. This analysis, leads to the formulation of the proposed algorithm. Our implementation exploits Transfer Learning techniques for deep learning-based feature extraction, in combination with a classifier for weed recognition. A prototype robotic platform has been used to make available an image dataset from a dairy farm containing broad-leaved docks. The evaluation of the proposed algorithm on this dataset shows that it outperforms competing weed/plant recognition methods in recognition accuracy, while producing low false-positive rates under real-world operation conditions. © 2019 Elsevier B.V.},
	author_keywords = {Agricultural robotics; Deep learning visual recognition; Precision farming; Weed recognition},
	keywords = {Rumex; Rumex obtusifolius; Agriculture; Docks; Hydraulic structures; Robotics; Agricultural robotics; False positive rates; Learning-based feature extractions; Precision farming; Real world operations; Recognition accuracy; Visual recognition; Weed recognition; agricultural technology; algorithm; data set; image analysis; learning; precision agriculture; robotics; visual analysis; visualization; weed; Deep learning},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 53; All Open Access, Green Open Access}
}

@ARTICLE{Hamid201937,
	author = {Hamid, Laith Emad and Al-Haddad, S.A.R.},
	title = {Automated leaf alignment and partial shape feature extraction for plant leaf classification},
	year = {2019},
	journal = {Electronic Letters on Computer Vision and Image Analysis},
	volume = {18},
	number = {1},
	pages = {37 – 51},
	doi = {10.5565/rev/elcvia.1143},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068453122&doi=10.5565%2frev%2felcvia.1143&partnerID=40&md5=a9e51701d988b5da9e17c841cf94ba59},
	affiliations = {Department of Computer and Communication System Engineering, Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Malaysia},
	abstract = {The last few decades have witnessed various approaches to automate the process of plant classification using the characteristics of the leaf. Several approaches have been proposed, and the majority focused on global shape features. However, one challenge that faces this task is the high interclass similarity amongst the leaves of different species in terms of the global shape. Furthermore, there always has been an obstacle against full automation as several approaches require user intervention to align the leaf. Therefore, a new set of Quartile Features (QF) is proposed in this paper to describe the partial shape of the leaf, in addition to an automated alignment approach to automate the system. The QF are extracted from the horizontal and vertical leaf quartiles to describe the partial shape of the leaf and the relations among its parts. The well-known Flavia dataset has been selected for the evaluation of the proposed system. The experimental results indicate the ability of the proposed alignment algorithm to align leaves with different shapes and maintain a correct classification accuracy regardless of the orientation of the input leaf samples. Furthermore, the proposed QF indicated promising results by increasing the accuracy of the classification by a range of approximately 26% to 30% when combined with Hu's Moment Invariants, using k-fold cross-validation technique. © 2019 Computer Vision Center / Universitat Autonoma de Barcelona, Barcelona, Spain.},
	author_keywords = {Feature extraction; Leaf alignment; Partial shape; Plant leaf classification; Quartile features},
	correspondence_address = {L.E. Hamid; Department of Computer and Communication System Engineering, Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Malaysia; email: laithemad@gmail.com},
	publisher = {Universitat Autonoma de Barcelona},
	issn = {15775097},
	language = {English},
	abbrev_source_title = {Electron. Lett. Comput. Vis. Image Anal.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Fu2019199,
	author = {Fu, Li and Zheng, Yuhong and Zhang, Pengchong and Zhang, Haoyang and Wu, Mengyao and Zhang, Huaiwei and Wang, Aiwu and Su, Weitao and Chen, Fei and Yu, Jinhong and Cai, Wen and Lin, Cheng-Te},
	title = {An electrochemical method for plant species determination and classification based on fingerprinting petal tissue},
	year = {2019},
	journal = {Bioelectrochemistry},
	volume = {129},
	pages = {199 – 205},
	doi = {10.1016/j.bioelechem.2019.06.001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066984239&doi=10.1016%2fj.bioelechem.2019.06.001&partnerID=40&md5=c6420c8b0d9af368f21b1f65e25b70b7},
	affiliations = {College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; Jiangsu Key Laboratory for the Research and Utilization of Plant Resources, Institute of Botany, Chinese Academy of Sciences, Nanjing, Jiangsu Province, China; Hangzhou Botanical Garden, Hangzhou, 310013, China; Center for Advanced Material Diagnostic Technology, Shenzhen Technology University, Shenzhen, 518118, China; Key Laboratory of Marine Materials and Related Technologies, Zhejiang Key Laboratory of Marine Materials and Protective Technologies, Ningbo Institute of Materials Technology and Engineering, Chinese Academy of Sciences, Ningbo, 315201, China; Institute of Medical Engineering, Department of Biophysics, School of Basic Medical Sciences, Health Science Center, Xi'an Jiaotong University, Xi'an, 710061, Shaanxi, China},
	abstract = {The identification of plant species not only is a hobby but also has important application value in plant resources science. Traditional plant identification often relies on the experience of botanists. The infrageneric identification of plants is easily mistaken due to similarities in organ features. In this work, we propose an electrochemical method to obtain fingerprints of plant petal tissue. Fourteen species of Lycoris were used as a model for validating this methodology. Pattern and color recognition were established for visualization of electrochemical fingerprints recorded after various solvent extractions. In addition, the infrageneric relationships of these Lycoris species were deduced from the electrochemical fingerprints since the type and content of electroactive compounds in plants are controlled by genes. The results indicate that the electrochemical fingerprints of Lycoris petals are correlated with the infrageneric relationships of native Lycoris species. © 2019},
	author_keywords = {Electrochemical fingerprint; Graphene; Lycoris; Plant determination; Plant taxonomy},
	keywords = {Adsorption; Electrochemical Techniques; Flowers; Graphite; Lycoris; Plant Extracts; Solvents; Species Specificity; Graphene; Pattern recognition; Tissue; graphite; plant extract; solvent; Color recognition; Electroactive compounds; Electrochemical fingerprint; ELectrochemical methods; Lycoris; Plant determination; Plant identification; Plant taxonomy; Article; chemical fingerprinting; color; controlled study; electrochemical analysis; Lycoris; nonhuman; pattern recognition; petal; plant identification; plant taxonomy; plant tissue; species identification; stamen; adsorption; chemistry; classification; electrochemical analysis; flower; procedures; species difference; Tissue engineering},
	correspondence_address = {L. Fu; College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; email: fuli@hdu.edu.cn},
	publisher = {Elsevier B.V.},
	issn = {15675394},
	coden = {BIOEF},
	pmid = {31200249},
	language = {English},
	abbrev_source_title = {Bioelectrochemistry},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 66}
}

@ARTICLE{Lu2019216,
	author = {Lu, Zhihai and Lu, Siyuan},
	title = {Petal-image based flower classification via glcm and rbf-svm},
	year = {2019},
	journal = {Communications in Computer and Information Science},
	volume = {1138 CCIS},
	pages = {216 – 227},
	doi = {10.1007/978-981-15-1925-3_16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076899250&doi=10.1007%2f978-981-15-1925-3_16&partnerID=40&md5=0b9d6cc8bfe910541db666624831836c},
	affiliations = {School of Education Science, Nanjing Normal University, Nanjing, 210023, Jiangsu, China; School of Informatics, University of Leicester, Leicester, LE1 7RH, United Kingdom},
	abstract = {Flower identification is a difficult problem in practice. Because there are over 250,000 different kinds of species worldwide so far. Even an experienced flower expert needs reference book to categorize a flower because of the high intra-class variation and inter-class similarity. In this study, an automatic flower recognition method was proposed based on digital image processing and artificial intelligence for petal image. Gray level co-occurrence matrix was employed as the image feature and a support vector machine was trained as the classifier. Three different kernel functions were tested and radial basis function performed best. Experimental results revealed that our approach can achieve state-of-the-art classification performance. © Springer Nature Singapore Pte Ltd. 2019.},
	author_keywords = {Flower classification; Gray level co-occurrence matrix; Pattern recognition; Radial basis function; Support vector machine},
	keywords = {Computers; Functions; Image classification; Pattern recognition; Radial basis function networks; Support vector machines; Classification performance; Flower recognition; Gray level co-occurrence matrix; Image features; Intra-class variation; Kernel function; Radial basis functions; State of the art; Matrix algebra},
	correspondence_address = {S. Lu; School of Education Science, Nanjing Normal University, Nanjing, 210023, China; email: sl672@le.ac.uk},
	editor = {Ning H.},
	publisher = {Springer},
	issn = {18650929},
	isbn = {978-981151924-6},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Conference on Cyberspace Data and Intelligence, Cyber DI 2019, and the International Conference on Cyber-Living, Cyber-Syndrome, and Cyber-Health, CyberLife 2019; Conference date: 16 December 2019 through 18 December 2019; Conference code: 235039}
}

@ARTICLE{Fathi Kazerouni2019,
	author = {Fathi Kazerouni, Masoud and Mohammed Saeed, Nazeer T. and Kuhnert, Klaus-Dieter},
	title = {Fully-automatic natural plant recognition system using deep neural network for dynamic outdoor environments},
	year = {2019},
	journal = {SN Applied Sciences},
	volume = {1},
	number = {7},
	doi = {10.1007/s42452-019-0785-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079211768&doi=10.1007%2fs42452-019-0785-9&partnerID=40&md5=179c0246e82555dcab23a18560ec38e5},
	affiliations = {Institute of Real-Time Learning Systems, University of Siegen, Hölderlinstr. 3, Siegen, 57076, Germany},
	abstract = {The leaf, as the vital part of plants, can be affected by physical and physiological factors which might lead to changes in its shape, color and size. These unique parts play an essential role in the design and implementation of plant recognition systems, as the shapes of leaves vary among different plants. Weather type and related factors, such as light intensity, humidity, temperature and wind-speed, may have effects on the number of leaves that grow on a plant, the amount of fresh and dried leaves, the deformation of leaves, color of leaves, positions of leaves on branches, etc. In addition, photographing in outdoor environments with different weather conditions has undesired impacts on images. For instance, light scattering changes severely when there are water droplets during photographing which influences the images captured in rainy weather. Despite the importance of the proposed factors and the relevant effects on the plants, leaves and images taken from plants, a plant recognition system should be independent from all environmental and non-environmental factors to be practically useful and applicable for identifying plant species in uncontrolled outdoor environments. Moreover, changes in the time of day (morning, noon and evening), the distance between camera and plant species as well as the angle and illumination of the object and environment when the images are taken should be considered in the process of plant recognition. For instance, in a windy weather condition, for an observer even from a short distance, it is challenging to distinguish all single leaves and identify the plant type from the shape of its leaf. Furthermore, the unstructured background of the images is another challenge which affects the images captured in fields and outdoor environments. These are only some difficulties for identification of plants in natural environments such as farms, forests, etc. A consideration of mentioned factors contributes to developing a novel, efficient and accurate system for plant recognition that can be used in various uncontrolled outdoor environments. In the real-life, the images of the plants captured with the presence of these factors are very challenging for recognizing plant species and it is a desire to develop an efficient and accurate system that can be used in various natural conditions and uncontrolled situations. This paper presents the development and implementation of a convolutional neural network for automatic plant recognition in uncontrolled outdoor environments. The deep network has been used to recognize four different natural plant species. The proposed system brings the opportunity and transformative potential of deep neural networks to the plant recognition field. This fully-automatic system is efficient and generalized for recognition of plants in outdoor environments like forests and farms, and its the accuracy is 99.5%. Meanwhile, the final system has the functionality of being applied as a real-time one. © 2019, Springer Nature Switzerland AG.},
	author_keywords = {Convolutional neural networks; Deep learning; Dynamic environment; Fully-automatic system; Natural plant recognition},
	keywords = {Convolutional neural networks; Deep neural networks; Forestry; Light scattering; Meteorology; Wind; Automatic systems; Design and implementations; Environmental factors; Natural conditions; Natural environments; Outdoor environment; Physiological factors; Plant recognition; Plants (botany)},
	correspondence_address = {M. Fathi Kazerouni; Institute of Real-Time Learning Systems, University of Siegen, Siegen, Hölderlinstr. 3, 57076, Germany; email: masoud.fathi@uni-siegen.de},
	publisher = {Springer Nature},
	issn = {25233971},
	language = {English},
	abbrev_source_title = {SN Appl. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access, Bronze Open Access}
}

@CONFERENCE{Lim2019183,
	author = {Lim, Marcus Guozong and Chuah, Joon Huang},
	title = {Durian types recognition using deep learning techniques},
	year = {2019},
	journal = {2018 9th IEEE Control and System Graduate Research Colloquium, ICSGRC 2018 - Proceeding},
	pages = {183 – 187},
	doi = {10.1109/ICSGRC.2018.8657535},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063446266&doi=10.1109%2fICSGRC.2018.8657535&partnerID=40&md5=1edd95b1a6398ef93474b539591d1d93},
	affiliations = {Department of Electrical Engineering, Faculty of Engineering, University of Malaya, VIP Research Laboratory, Kuala Lumpur, 50603, Malaysia},
	abstract = {Fruit or plant recognition is a very pragmatic and specific application of deep-learning technique. As compared to conventional method, the technique requires a larger quantity of data for training while at the same time promises a higher level of accuracy. Among various classes of neural network, convolutional neural network (CNN) is arguably the most commonly used method in image classification. The aim of this research work is to develop an effective method to classify the various cultivars of Durio zibethinus (or commonly known as durian) based on the crop's visual features via the application of CNN to improve the accuracy and speed of the cultivars recognition. Meanwhile, a reliable database consisting of labelled durian cultivars has been created. A total of 800 images consisting of the bottom view of 3 classes of cultivars and non-durian images are used during the training process of the neural network. The research work starts with the pre-processing and conversion of the images then followed by one-hot labelling of the data, construction of the network architecture, training and validation of the model then lastly exporting the trained model for general application. Important system parameters and prediction accuracy are obtained, including the graphs of loss function and accuracy against the number of epochs, confusion matrix, miss-classified images, the effect of network architecture on prediction performance, etc. The prediction accuracy of the trained model on the perfect bottom-view images of Durio zibethinus is 82.50%. With the addition of non-durian images, the prediction accuracy is slightly dropped to 81.25%. © 2018 IEEE.},
	author_keywords = {Convolutional Neural Network (CNN); Deep Learning; Durian Classification; Durian Types Recognition; Durio zibethinus; Image Processing; Machine Learning},
	keywords = {Computer architecture; Convolution; Forecasting; Image classification; Image processing; Learning algorithms; Learning systems; Network architecture; Neural networks; Conventional methods; Convolutional neural network; Durian Types Recognition; Durio zibethinus; General applications; Learning techniques; Prediction accuracy; Prediction performance; Deep learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153866321-9},
	language = {English},
	abbrev_source_title = {IEEE Control Syst. Grad. Res. Colloq., ICSGRC - Proceeding},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; Conference name: 9th IEEE Control and System Graduate Research Colloquium, ICSGRC 2018; Conference date: 3 August 2018 through 4 August 2018; Conference code: 145774}
}

@ARTICLE{Rajagopal2019967,
	author = {Rajagopal, Heshalini and Khairuddin, Anis Salwa Mohd and Mokhtar, Norrima and Ahmad, Azlin and Yusof, Rubiyah},
	title = {Application of image quality assessment module to motion-blurred wood images for wood species identification system},
	year = {2019},
	journal = {Wood Science and Technology},
	volume = {53},
	number = {4},
	pages = {967 – 981},
	doi = {10.1007/s00226-019-01110-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068110578&doi=10.1007%2fs00226-019-01110-2&partnerID=40&md5=2567c34a5c62669064b8fd98602baf7c},
	affiliations = {Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, Malaysia; Malaysia Japan International Institute of Technology, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia},
	abstract = {Despite tighter conservation regulations, demand for timber products has continued to increase due to growing population. Normally, experts identify the wood species based on the pattern of the wood surface texture. However, manual inspection on wood texture is tedious, time-consuming, impractical and cost-ineffective for a human to analyze a large number of timber species. Therefore, a reliable automatic wood recognition system is needed in order to classify the wood species efficiently. The proposed system includes image acquisition, image quality assessment module (IQA), image deblurring, feature extraction and classification. In this research, the wood images are motion-blurred due to imperfections in the imaging and capturing process. Hence, an IQA module is proposed to monitor the quality of images before proceeding to the next stage which is the feature extraction process. The IQA module will determine whether the image has to undergo the image deblurring process based on the image quality value. If the image is of low quality based on the image quality value obtained, then the image will be deblurred before the feature extraction procedure. A reliable motion deblurring technique, which is based on Lucy–Richardson algorithm, is employed to enhance the motion-blurred images before proceeding to the next stage, which is the feature extraction process. Then, a statistical feature extraction technique is proposed to extract 24 features from each wood image. Finally, a support vector machine is used to classify the 20 tropical wood species. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.},
	keywords = {Extraction; Image Quality; Images; Motion; Processes; Species Identification; Systems; Wood Species; Extraction; Feature extraction; Image acquisition; Image quality; Support vector machines; Textures; Timber; Extraction procedure; Feature extraction and classification; Image quality assessment; Motion blurred image; Motion deblurring; Species identification; Statistical feature extractions; Tropical wood species; Image enhancement},
	correspondence_address = {A.S.M. Khairuddin; Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, Malaysia; email: anissalwa@um.edu.my},
	publisher = {Springer Verlag},
	issn = {00437719},
	coden = {WOSTB},
	language = {English},
	abbrev_source_title = {Wood Sci Technol},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24}
}

@CONFERENCE{Lee2019,
	author = {Lee, Jun Woo and Chan Yoon, Yeo},
	title = {Fine-Grained Plant Identification using wide and deep learning model 1},
	year = {2019},
	journal = {2019 International Conference on Platform Technology and Service, PlatCon 2019 - Proceedings},
	doi = {10.1109/PlatCon.2019.8669407},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063904571&doi=10.1109%2fPlatCon.2019.8669407&partnerID=40&md5=59c1333e9d69d1ff48e13ff5187a712b},
	affiliations = {SW/Content Research Laboratory, Electronics and Telecommunications Research Institute, Daejeon, South Korea},
	abstract = {In recent years, with the evolution of deep learning technology, the performance of plant image recognition has improved remarkably. In this paper, we propose a model to address the fine-grained plant image classification task by using the wide and deep learning framework which combines a linear model and a deep learning model. Proposed method sums the result of the wide and deep learning model using a logistic function so that discrete features can be considered simultaneously with continuous image content. Our works used metadata such as the date of flowering and locational information for the wide model. Our experiment shows that the proposed method gives better performance than a baseline method. © 2019 IEEE.},
	author_keywords = {deep neural network; fine-grained image classification; plant image classification},
	keywords = {Image classification; Image enhancement; Image recognition; Baseline methods; Fine grained; Learning frameworks; Learning models; Learning technology; Linear modeling; Logistic functions; Plant identification; Deep neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172811288-6},
	language = {English},
	abbrev_source_title = {Int. Conf. Platf. Technol. Serv., PlatCon - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; Conference name: 6th International Conference on Platform Technology and Service, PlatCon 2019; Conference date: 28 January 2019 through 30 January 2019; Conference code: 146400}
}

@CONFERENCE{Chouhan2019700,
	author = {Chouhan, Siddharth Singh and Singh, Uday Pratap and Kaul, Ajay and Jain, Sanjeev},
	title = {A Data Repository of Leaf Images: Practice towards Plant Conservation with Plant Pathology},
	year = {2019},
	journal = {2019 4th International Conference on Information Systems and Computer Networks, ISCON 2019},
	pages = {700 – 707},
	doi = {10.1109/ISCON47742.2019.9036158},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082990749&doi=10.1109%2fISCON47742.2019.9036158&partnerID=40&md5=9f62210789f5f3d72cc777a1c61a1a3b},
	affiliations = {School of Computer Science and Engineering, Shri Mata Vaishno Devi University, Katra, Jammu and Kashmir, 182320, India; School of Mathematics, Shri Mata Vaishno Devi University, Katra, Jammu and Kashmir, 182320, India; Indian Institute of Information Technology Design and Management, Department of Computer Science and Engineering, Jabalpur, 482005, India},
	abstract = {The relationship between the plants and the environment is multitudinous and complex. They help in nourishing the atmosphere with diverse elements. Plants are also a substantial element in regulating carbon emission and climate change. But in the past, we have destroyed them without hesitation. For the reason that not only we have lost a number of species located in them, but also a severe result has also been encountered in the form of climate change. However, if we choose to give them time and space, plants have an astonishing ability to recover and re-cloth the earth with varied plants and species that we have, so recently, stormed. Therefore, a contribution has been made in this article towards the study of plant growth and its management. Twelve economically and environmentally beneficial plants have been selected for this purpose. Leaf images of these plants in healthy and unhealthy conditions have been acquired and alienated among two separate classes. We have collected about 4503 images of which contain 2278 images of healthy leaf and 2225 images of the diseased leaf. Further, we hope that this study can be beneficial for researchers and academicians in developing methods for plant identification, plant classification, plant growth monitoring, leaf disease diagnosis, etc. Finally, the anticipated impression is towards a better understanding of the plants to be planted and their appropriate management. © 2019 IEEE.},
	author_keywords = {Computer Vision; Deforestation; Forestation; Horticulture; Image Processing; Plant Conservation; Plant Pathology},
	keywords = {Computer aided diagnosis; Computer networks; Computer vision; Deforestation; Earth (planet); Image processing; Information systems; Information use; Pathology; Data repositories; Forestation; Horticulture; Number of species; Plant classification; Plant conservation; Plant identification; Plant pathology; Climate change},
	correspondence_address = {S.S. Chouhan; School of Computer Science and Engineering, Shri Mata Vaishno Devi University, Katra, Jammu and Kashmir, 182320, India; email: siddharth.lnct@gmail.com},
	editor = {Sharman D.K. and Kumar D.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172813651-6},
	language = {English},
	abbrev_source_title = {Int. Conf. Inf. Syst. Comput. Networks, ISCON},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 30; Conference name: 4th International Conference on Information Systems and Computer Networks, ISCON 2019; Conference date: 21 November 2019 through 22 November 2019; Conference code: 158562}
}

@CONFERENCE{Zhao2019286,
	author = {Zhao, Jipeng and Wang, Zhaobin},
	title = {A Leaf Recognition Method Based on Integration of Multi-migration Model},
	year = {2019},
	journal = {2019 IEEE 4th International Conference on Image, Vision and Computing, ICIVC 2019},
	pages = {286 – 290},
	doi = {10.1109/ICIVC47709.2019.8981399},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084304255&doi=10.1109%2fICIVC47709.2019.8981399&partnerID=40&md5=be67a3b37982b58b65e32db4dafefbc8},
	affiliations = {Lanzhou University, School of Information Science and Engineering, Lanzhou, China},
	abstract = {Plants are important to humans, and the plant classification based on leaf images has also attracted more and more attention. As a concrete implementation of deep learning in the field of image recognition, convolutional neural network has achieved many satisfactory results. In this paper, in order to solve the problem that the model recognition accuracy is not high due to the rare sample size, we integrate multiple migration models to recognize and classify the leaf images. A total of three models were selected, Squeezenet, Googlenet and Resnet for transfer learning and integration. We first pre-trained three networks on the Imagenet dataset. Then, the three models were fine-tuned on the LZU blade dataset. Finally, the three models are integrated by an integration strategy based on the voting results and prediction probabilities of each model. Through experiments, our leaf recognition method based on migration model has achieved 99.49% recognition accuracy on LZU blade dataset, which has certain advantages compared with other methods or networks. © 2019 IEEE.},
	author_keywords = {deep learning; neural network; object recognition; transfer learning},
	keywords = {Convolutional neural networks; Deep learning; Image recognition; Integration; Transfer learning; Integration strategy; Leaf recognition; Migration model; Model recognition; Plant classification; Prediction probabilities; Recognition accuracy; Three networks; Image classification},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172812325-7},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Image, Vis. Comput., ICIVC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th IEEE International Conference on Image, Vision and Computing, ICIVC 2019; Conference date: 5 July 2019 through 7 July 2019; Conference code: 157452}
}

@CONFERENCE{Ido2019,
	author = {Ido, Junya and Saitoh, Takeshi},
	title = {CNN-based tree species identification from bark image},
	year = {2019},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {11069},
	doi = {10.1117/12.2524213},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065800899&doi=10.1117%2f12.2524213&partnerID=40&md5=c45288787018dddfb53bb334d75eaf18},
	affiliations = {Kyushu Institute of Technology, 680-4 Kawazu, Iizuka, Fukuoka, 820-8502, Japan},
	abstract = {This paper proposes a convolutional neural network (CNN)-based tree species identification method from bark image. The proposed method uses the well-known CNN model. The difficulty of our problem is to use a special tree image in which a colorful tag is stick on the bark. The tag is irrelevant to the species. In order to recognize with CNN, it is necessary to extract a region (ROI) excluding the tag. Thus, this paper proposes a ROI extraction method. Extracted ROI is fed to CNN. We evaluated the proposed method with six tree species. We carried out the evaluation experiment with various conditions, and found an optimal condition for our problem. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
	author_keywords = {Bark image; Convolutional neural network; Tree species identification},
	keywords = {Convolution; Forestry; Neural networks; Bark image; Convolutional neural network; Evaluation experiments; Optimal conditions; ROI extraction; Tree images; Tree species; Tree species identifications; Image processing},
	editor = {Pu Y. and Li C. and Pan Z. and Yu H.},
	publisher = {SPIE},
	issn = {0277786X},
	isbn = {978-151062828-1},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 10th International Conference on Graphics and Image Processing, ICGIP 2018; Conference date: 12 December 2018 through 14 December 2018; Conference code: 147854}
}

@CONFERENCE{Arrasco2019,
	author = {Arrasco, Carlos and Khlebnikov, Sofia and Oncevay, Arturo and Castanon, Cesar Beltran},
	title = {Leaf Venation Enhancing for Texture Feature Extraction in a Plant Classification Task},
	year = {2019},
	journal = {2018 IEEE Latin American Conference on Computational Intelligence, LA-CCI 2018},
	doi = {10.1109/LA-CCI.2018.8625221},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062535809&doi=10.1109%2fLA-CCI.2018.8625221&partnerID=40&md5=9bd8e1deb8c947ef8dd575e9f0344b96},
	affiliations = {Artificial Intelligence Research Group (IA-PUCP), Pontificia Universidad Catolica Del Peru, Lima, Peru},
	abstract = {In a computer science approach, the plant classification task focuses on the extraction of many leaf attributes, such as texture or veins, which are closely related and commonly analyzed together. Thereby, this study proposes a method to enhance the venation patterns over the leaf area, in order to improve the texture feature extraction in windows areas in the plant species identification. Regarding the experimentation, two types of texture features are contrasted, and it is performed over an own dataset with high-resolution image of 10 plant species. The obtained results demonstrate that the veins enhancing process improve the species classification task significantly for a texture descriptor based on the analysis of relation between neighboring pixels. © 2018 IEEE.},
	author_keywords = {Haralick's descriptors; leaf veins extraction; leaf venation enhancing; leaf-based plant classification; multi-scale fractal dimension; texture feature extraction},
	keywords = {Artificial intelligence; Extraction; Feature extraction; Fractal dimension; Textures; Descriptors; High resolution image; Leaf venation; Plant classification; Plant species identification; Species classification; Texture descriptor; Texture feature extraction; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153864625-0},
	language = {English},
	abbrev_source_title = {IEEE Lat. Am. Conf. Comput. Intell., LA-CCI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2018 IEEE Latin American Conference on Computational Intelligence, LA-CCI 2018; Conference date: 6 November 2018 through 9 November 2018; Conference code: 144525}
}

@CONFERENCE{Barosa2019,
	author = {Barosa, Roysing and Hassen, Sayed Issamuddine Sayed and Nagowah, Leckraj},
	title = {Smart Aquaponics with Disease Detection},
	year = {2019},
	journal = {2nd International Conference on Next Generation Computing Applications 2019, NextComp 2019 - Proceedings},
	doi = {10.1109/NEXTCOMP.2019.8883437},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074992452&doi=10.1109%2fNEXTCOMP.2019.8883437&partnerID=40&md5=8b1df214491942eb2be23e4eef23f485},
	affiliations = {Software and Information Systems, University of Mauritius, Réduit, Mauritius},
	abstract = {The vision of the Mauritian's Ministry of Agro- Industry and Food security is to further develop the agricultural sector and to promote the agro-industry by focusing on safety, supply, quality, innovation and new technologies. This research work attempts to develop a smart aquaponics system that combines conventional aquaculture with hydroponics. The system uses the Internet of Things to continuously sense and control the environment and to provide real-time feedback and alerts to the owner through a mobile application. It features a camera surveillance system that enables live streaming of the aquaponics setup and also allows for further image processing. Through the captured images, the system performs leaf recognition identifying the type of the leaf. Moreover, the system also performs disease detection that identifies if the leaf, and consequently the plant, is suffering from a specific disease. If the system detects a disease, it automatically generates a report which is sent to the owner through the mobile application. The prototype has been tested and looks very promising. We hope that this work paves the way towards a smart aquaponics system that detects, identifies and notifies the owner of possible diseases at early stage before their propagation. © 2019 IEEE.},
	author_keywords = {disease detection; image processing; Internet of Things; leaf identification; smart aquaponics},
	keywords = {Accident prevention; Food supply; Internet of things; Mobile computing; Network security; Security systems; Agricultural sector; Camera surveillance; Disease detection; Leaf identification; Leaf recognition; Mobile applications; Real-time feedback; smart aquaponics; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172811460-6},
	language = {English},
	abbrev_source_title = {Int. Conf. Next Gener. Comput. Appl,, NextComp - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 2nd International Conference on Next Generation Computing Applications, NextComp 2019; Conference date: 19 September 2019 through 21 September 2019; Conference code: 153531}
}

@CONFERENCE{Chulif2019,
	author = {Chulif, Sophia and Heng, Kiat Jing and Chan, Teck Wei and Al Monnaf, Abdullah and Chang, Yang Loong},
	title = {Plant identification on Amazonian and Guiana shield flora: NEUON submission to LifeCLEF 2019 Plant},
	year = {2019},
	journal = {CEUR Workshop Proceedings},
	volume = {2380},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070531149&partnerID=40&md5=e82c4ce3ab62bb236a2ea1c4b999978f},
	affiliations = {Department of Artificial Intelligence, NEUON AI, Sarawak, 94300, Malaysia},
	abstract = {This paper will look into the use of fine-tuned Inception-v4 and Inception-ResNet-v2 models to automate the classification of 10,000 plant species. Prior to training the networks, the training dataset was pre-processed to remove the noisy data. The team submitted three runs which achieved comparable performances to human experts on the test dataset comprising 745 observations for all the evaluation metrics. For the trained systems to generalise better, the systems were trained for multi-task classification and is able to classify plant images based on their species, with support of their genus and family labels. In particular, an ensemble of Inception-v4 and Inception-ResNet-v2 networks achieved a Top-1 accuracy of 0.316 and 0.246 for the test set identified by experts and the whole test set respectively. © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
	author_keywords = {Computer vision; Convolutional neural networks; Data cleaning; Plant identification},
	keywords = {Computer vision; Neural networks; Convolutional neural network; Data cleaning; Evaluation metrics; Human expert; Noisy data; Plant identification; Plant species; Training dataset; Statistical tests},
	editor = {Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Losada D.E. and Muller H.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 20th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2019; Conference date: 9 September 2019 through 12 September 2019; Conference code: 149771}
}

@CONFERENCE{Wu2019562,
	author = {Wu, Yong and Qin, Xiao and Pan, Yonghua and Yuan, Changan},
	title = {Convolution neural network based transfer learning for classification of flowers},
	year = {2019},
	journal = {2018 IEEE 3rd International Conference on Signal and Image Processing, ICSIP 2018},
	pages = {562 – 566},
	doi = {10.1109/SIPROCESS.2018.8600536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061780183&doi=10.1109%2fSIPROCESS.2018.8600536&partnerID=40&md5=b6091e002a8a505bfb362a824276666c},
	affiliations = {College of Computer and Information Engineering, Guangxi Teachers Education University, Nanning, China},
	abstract = {Flower plays an extremely important role in our life, which has high research value and application value. The traditional methods of flower classification is mainly based on shape, color or texture features, and this methods needs people to select features for flower classification lead to the accuracy of classification is not very high. This paper aims to develop an effective flower classification approach using convolution neural network and transfer learning. In this paper, based on VGG-16, VGG-19, Inception-v3 and ResNet50 models were used to compare the network initialization model with the transfer learning model. The results show that transfer learning can effectively avoid deep convolution networks are prone to local optimal problems and over-fitting problems. Compared with the traditional methods, the accuracy of flower recognition on Oxford flowers dataset is obviously improved, and has better robustness and generalization ability. © 2018 IEEE.},
	author_keywords = {Convolution neural network; Deep learning; Flowers classification; Transfer learning},
	keywords = {Convolution; Image processing; Textures; Accuracy of classifications; Classification approach; Convolution neural network; Generalization ability; Local optimal problems; Network initialization; Over fitting problem; Transfer learning; Deep learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153866394-3},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Signal Image Process., ICSIP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 51; Conference name: 2018 IEEE 3rd International Conference on Signal and Image Processing, ICSIP 2018; Conference date: 13 July 2018 through 15 July 2018; Conference code: 144205}
}

@CONFERENCE{Picek2019,
	author = {Picek, Lukáš and Šulc, Milan and Matas, Jiří},
	title = {Recognition of the Amazonian flora by Inception Networks with Test-time Class Prior Estimation CMP submission to PlantCLEF 2019},
	year = {2019},
	journal = {CEUR Workshop Proceedings},
	volume = {2380},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070527849&partnerID=40&md5=7bea042e37a5f7aa0c865fdae6d905bb},
	affiliations = {Dept. of Cybernetics, Faculty of Applied Sciences, University of West Bohemia, Czech Republic; Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic},
	abstract = {The paper describes an automatic system for recognition of 10,000 plant species, with focus on species from the Guiana shield and the Amazon rain forest. The proposed system achieves the best results on the PlantCLEF 2019 test set with 31.9% accuracy. Compared against human experts in plant recognition, the system performed better than 3 of the 5 participating human experts and achieved 41.0% accuracy on the subset for expert evaluation. The proposed system is based on the Inception-v4 and Inception-ResNet-v2 Convolutional Neural Network (CNN) architectures. Performance improvements were achieved by: adjusting the CNN predictions according to the estimated change of the class prior probabilities, replacing network parameters with their running averages, test-time data augmentation, filtering the provided training set and adding additional training images from GBIF. © 2019 CEUR-WS. All rights reserved.},
	author_keywords = {Class Prior Estimation; Classification; Computer Vision; Convolutional Neural Networks; Fine-grained; Machine Learning; Plant Recognition},
	keywords = {Classification (of information); Computer vision; Convolution; Learning systems; Neural networks; Amazon rain forest; Automatic systems; Convolutional neural network; Expert evaluation; Fine grained; Network parameters; Plant recognition; Prior probability; Image enhancement},
	editor = {Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Losada D.E. and Muller H.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 20th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2019; Conference date: 9 September 2019 through 12 September 2019; Conference code: 149771}
}

@CONFERENCE{Zhang2019,
	author = {Zhang, Qi and Zeng, Shaoning and Zhang, Bob},
	title = {Initial investigation of different classifiers for plant leaf classification using multiple features},
	year = {2019},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {11179},
	doi = {10.1117/12.2539654},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072607246&doi=10.1117%2f12.2539654&partnerID=40&md5=e663961d21c8dc39cf703ce56e63729f},
	affiliations = {PAMI Research Group, Department of Computer and Information Science, University of Macau, Taipa, Macao},
	abstract = {Plant leaf species classification is an active research area at present with many scientists attempting to use different classifiers with different leaf features to solve it. In this paper we evaluate 10 common classifiers: k-Nearest Neighbors (KNN), support vector machine (SVM), nu-SVM, decision tree, random forest, naïve bayes, linear discriminant analysis (LDA), logistic regression, quadratic discriminant analysis (QDA) and sparse representation in leaf species classification with different leaf features such as shape, texture and margin. Besides this, different numbers of leaf species and training samples for different classifiers were also evaluated in this study. The comprehensive results indicate that random forest, followed by LDA, logistic regression and sparse representation are the most robust and accurate classifiers in leaf recognition using various features. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
	author_keywords = {Leaf classification; Leaf features; Pattern recognition},
	keywords = {Decision trees; Discriminant analysis; Image processing; Nearest neighbor search; Pattern recognition; Regression analysis; Textures; K nearest neighbor (KNN); Leaf classification; Leaf features; Linear discriminant analysis; Plant leaf classifications; Quadratic discriminant analysis; Sparse representation; Species classification; Support vector machines},
	editor = {Hwang J.-N. and Jiang X.},
	publisher = {SPIE},
	issn = {0277786X},
	isbn = {978-151063075-8},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 11th International Conference on Digital Image Processing, ICDIP 2019; Conference date: 10 May 2019 through 13 May 2019; Conference code: 151673}
}

@CONFERENCE{Alimboyong2019217,
	author = {Alimboyong, Catherine R. and Hernandez, Alexander A.},
	title = {An Improved Deep Neural Network for Classification of Plant Seedling Images},
	year = {2019},
	journal = {Proceedings - 2019 IEEE 15th International Colloquium on Signal Processing and its Applications, CSPA 2019},
	pages = {217 – 222},
	doi = {10.1109/CSPA.2019.8696009},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065492983&doi=10.1109%2fCSPA.2019.8696009&partnerID=40&md5=ff8121807e0ff8319f1738ad15f2fb39},
	affiliations = {Graduate Programs, Technological Institute of the Philippines, Quezon City, Philippines; College of Information Technology Education, Technological Institute of the Philippines, Manila, Philippines},
	abstract = {This scientific pursuit aimed to develop a deep learning architecture tailored to classify plant seedling images. Our architecture encompasses seven learned layers - five convolutions and two fully connected. We performed full training on the network using 4, 234 plant seedling images belonging to twelve plant species from Aarhus University Signal Processing group. The system is fine-tuned for the architecture to have greater processing time and low memory consumption. The architecture was evaluated using different network parameters. Furthermore, we used training loss function, accuracy, sensitivity, and specificity to evaluate the system performance. Experimental results proved that the developed architecture has reached excellent performance with overall accuracy of 90.15%. Results were achieved in 111 minutes and 36 seconds. Future work includes, first, use the model with greater amount of datasets through data augmentation and compare the results to other existing deep learning architectures using same datasets. Second, authors will consider CNN and RNN architectures together using several other plant datasets. Third, create a portable mobile application for plant seedling images classification utilizing the developed model. © 2019 IEEE.},
	author_keywords = {CNN; Deep learning; Plant classification},
	keywords = {Deep learning; Deep neural networks; Image classification; Network architecture; Data augmentation; Developed model; Images classification; Learning architectures; Mobile applications; Network parameters; Overall accuracies; Plant classification; Image enhancement},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153867563-2},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Colloq. Signal Process. Appl., CSPA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; Conference name: 15th IEEE International Colloquium on Signal Processing and its Applications, CSPA 2019; Conference date: 8 March 2019 through 9 March 2019; Conference code: 147633}
}

@ARTICLE{Salve2019453,
	author = {Salve, Pradip and Yannawar, Pravin and Sardesai, Milind},
	title = {Evaluation and Analysis of Plant Classification System Based on Feature Level Fusion and Score Level Fusion},
	year = {2019},
	journal = {Communications in Computer and Information Science},
	volume = {1037},
	pages = {453 – 470},
	doi = {10.1007/978-981-13-9187-3_41},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070191810&doi=10.1007%2f978-981-13-9187-3_41&partnerID=40&md5=442ef8ed02af35d4575862af35d6c1e3},
	affiliations = {Vision and Intelligence Lab, Department of Computer Science and IT, Dr. Babasaheb Ambedkar Marathwada University, Aurangabad, MS, India; Floristic Research Lab, Department of Botany, Savitribai Phule Pune University, Pune, India},
	abstract = {This paper describes the automatic leaf recognition based on feature level fusion and score level fusion of vein orientation angles, GLCM, SIFT, SURF as a features. However, to obtain the sophisticated leaf recognition, the system must be undergo through numerous difficulties such as intra and inter-class variations in plants and defining the proper local and global image descriptors which can deal with the color, shape and textual, information for the classification. Selection of the meticulous features plays key role in designing the best classification system. In this paper we proposed multi-modal plant classification where several components are fused together for a more precise classification. The results shows that the proposed system for feature level fusion achieved 93.72% GAR with 6.27% of EER and for score level fusion system achieves 97.13% GAR and 2.86% EER. It is found that the performance of the classification has been increased by 3.79% of EER when score level fusion applied to the system. © 2019, Springer Nature Singapore Pte Ltd.},
	author_keywords = {Leaf features; Leaf veins; Plant classification; Plant recognition},
	keywords = {Image processing; Pattern recognition; Classification system; Evaluation and analysis; Feature level fusion; Leaf features; Leaf veins; Plant classification; Plant recognition; Score-level fusion; Classification (of information)},
	correspondence_address = {P. Yannawar; Vision and Intelligence Lab, Department of Computer Science and IT, Dr. Babasaheb Ambedkar Marathwada University, Aurangabad, India; email: pravinyannawar@gmail.com},
	editor = {Santosh K.C. and Hegadi R.S.},
	publisher = {Springer Verlag},
	issn = {18650929},
	isbn = {978-981139186-6},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2nd International Conference on Recent Trends in Image Processing and Pattern Recognition, RTIP2R 2018; Conference date: 21 December 2018 through 22 December 2018; Conference code: 228899}
}

@ARTICLE{Duong-Trung201958,
	author = {Duong-Trung, Nghia and Quach, Luyl-Da and Nguyen, Chi-Ngon},
	title = {Learning deep transferability for several agricultural classification problems},
	year = {2019},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {10},
	number = {1},
	pages = {58 – 67},
	doi = {10.14569/IJACSA.2019.0100107},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062980513&doi=10.14569%2fIJACSA.2019.0100107&partnerID=40&md5=f3a491fb9525c30e905ba10864dacdab},
	affiliations = {FPT University, Can Tho City, Viet Nam; Tay Do University, Can Tho City, Viet Nam; Can Tho University, Can Tho City, Viet Nam},
	abstract = {This paper addresses several critical agricultural classification problems, e.g. grain discoloration and medicinal plants identification and classification, in Vietnam via combining the idea of knowledge transferability and state-of-the-art deep convolutional neural networks. Grain discoloration disease of rice is an emerging threat to rice harvest in Vietnam as well as all over the world and it acquires specific attention as it results in qualitative loss of harvested crop. Medicinal plants are an important element of indigenous medical systems. These resources are usually regarded as a part of culture's traditional knowledge. Accurate classification is preliminary to any kind of intervention and recommendation of services. Hence, leveraging technology in automatic classification of these problems has become essential. Unfortunately, building and training a machine learning model from scratch is next to impossible due to the lack of hardware infrastructure and finance support. It painfully restricts the requirements of rapid solutions to deal with the demand. For this purpose, the authors have exploited the idea of transfer learning which is the improvement of learning in a new prediction task through the transferability of knowledge from a related prediction task that has already been learned. By utilizing state-of-the-art deep networks re-trained upon our collected data, our extensive experiments show that the proposed combination performs perfectly and achieves the classification accuracy of 98:7% and 98:5% on our collected datasets within the acceptable training time on a normal laptop. A mobile application is also deployed to facilitate further integrated recommendation and services. © 2018 The Science and Information (SAI) Organization Limited.},
	author_keywords = {Deep learning; Grain discoloration classification; Medicinal plant classification; Transfer learning},
	keywords = {Classification (of information); Deep neural networks; Discoloration; Grain (agricultural product); Deep learning; Grain discoloration classification; Medicinal plant classification; Medicinal plants; Plant classification; Plant identification; Prediction tasks; State of the art; Transfer learning; Viet Nam; Plants (botany)},
	publisher = {Science and Information Organization},
	issn = {2158107X},
	language = {English},
	abbrev_source_title = {Intl. J. Adv.  Comput. Sci. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access, Gold Open Access}
}

@CONFERENCE{Pacifico2019741,
	author = {Pacifico, Luciano D.S. and Britto, Larissa F.S. and Oliveira, Emilia G. and Ludermir, Teresa},
	title = {Automatic classification of medicinal plant species based on color and texture features},
	year = {2019},
	journal = {Proceedings - 2019 Brazilian Conference on Intelligent Systems, BRACIS 2019},
	pages = {741 – 746},
	doi = {10.1109/BRACIS.2019.00133},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077093978&doi=10.1109%2fBRACIS.2019.00133&partnerID=40&md5=2fb5b0894d58fdb324814f34ca828f4b},
	affiliations = {Departamento de Computacao - DC, Universidade Federal Rural de Pernambuco - UFRPE, Recife, Pernambuco, Brazil; Centro de Informatica - CIn, Universidade Federal de Pernambuco - UFPE, Recife, Pernambuco, Brazil},
	abstract = {Plants play an important role in nature, providing sustenance, oxygen and shelter for animals. Many plant and herb species have a long history of use as sources for medicines and medical treatments in a field known as traditional medicine. Traditional medicine is still adopted in many countries (such as India and Brazil) for a variety of reasons: plants and herbs are cheap in comparison to other kinds of pharmaceutical drugs, plants are nontoxic and do not impact any side effect when properly used. But the correct identification of medicinal plant species is still a challenging task in machine learning and computer vision. Many automatic systems for plant species recognition have been proposed on the past few years, but most of the proposed systems only present acceptable accuracies for a specific and limited set of plant species. In this work, we develop a new medicinal plant data set based on the extraction of texture and color features from plant leaf images. A complete automatic plant recognition system is proposed, and five well-known machine learning classifiers are tested as the recognition module. Experimental results showed that the best classifiers are able to obtain average accuracies over 97% on the proposed data set. © 2019 IEEE.},
	author_keywords = {Image Features Extraction; Image Understanding; Leaf Image Analysis; Medicinal Plant Recognition; Supervised Learning},
	keywords = {Classification (of information); Extraction; Image processing; Image understanding; Intelligent systems; Machine learning; Supervised learning; Textures; Automatic classification; Automatic systems; Color and texture features; Image features; Leaf images; Medical treatment; Medicinal plants; Pharmaceutical drugs; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172814253-1},
	language = {English},
	abbrev_source_title = {Proc. - Braz. Conf. Intell. Syst., BRACIS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; Conference name: 8th Brazilian Conference on Intelligent Systems, BRACIS 2019; Conference date: 15 October 2019 through 18 October 2019; Conference code: 155757}
}

@CONFERENCE{Yusof20192034,
	author = {Yusof, Rubiyah and Khairuddin, Uswah and Rosli, Nenny Ruthfalydia and Abdul Ghafar, Hafizza and Aizuddin Nik Azmi, Nik Mohamad and Ahmad, Azlin and Khairuddin, Anis Salwa},
	title = {A Study of Feature Extraction and Classifier Methods for Tropical Wood Recognition System},
	year = {2019},
	journal = {IEEE Region 10 Annual International Conference, Proceedings/TENCON},
	volume = {2018-October},
	pages = {2034 – 2039},
	doi = {10.1109/TENCON.2018.8650411},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063187501&doi=10.1109%2fTENCON.2018.8650411&partnerID=40&md5=d0ec4ffd2963174d8ab5c696ebd7c087},
	affiliations = {Centre of Artificial Intelligence and Robotics Malaysia-Japan International Institute of Technology, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Advanced Analytics Engineering Center (AAEC) Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; Department of Electrical Engineering, Universiti Malaya, Kuala Lumpur, Malaysia},
	abstract = {Tropical wood recognition is a very challenging task due to the lack of discriminative features among some species of the wood, and also some very discriminative features among inter class species. Moreover, noises due to illuminations, or the uncontrolled environment as well as the wood features such as the size of pores, the density of pores, etc., which depend very much on the age, weather and other factors, contributing to the irregularities of the features. In this paper, we explore the use of feature extraction techniques, classification techniques for better accuracy of the system. In particular, we explore the use of one of the deep learning method residual network based CNN (Res-Net), noting the capability of the network to learn the features of images and its ability of generalization. Results have shown that good feature extraction methods can give a much better accuracy for all the datasets tested, and Res-Net performed badly due to lack of data, which cause the problem of overfitting. © 2018 IEEE.},
	author_keywords = {Backpropagation Neural Network; Classification; Deep Learning; Wood Recognition System; Wood Species},
	keywords = {Backpropagation; Deep learning; Extraction; Feature extraction; Neural networks; Tropics; Wood; Back propagation neural networks; Classification technique; Density of pores; Discriminative features; Feature extraction methods; Feature extraction techniques; Learning methods; Wood recognition; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21593442},
	isbn = {978-153865457-6},
	coden = {85QXA},
	language = {English},
	abbrev_source_title = {IEEE Reg 10 Annu Int Conf Proc TENCON},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2018 IEEE Region 10 Conference, TENCON 2018; Conference date: 28 October 2018 through 31 October 2018; Conference code: 145614}
}

@ARTICLE{Madsen2019147,
	author = {Madsen, Simon L. and Dyrmann, Mads and Jørgensen, Rasmus N. and Karstoft, Henrik},
	title = {Generating artificial images of plant seedlings using generative adversarial networks},
	year = {2019},
	journal = {Biosystems Engineering},
	volume = {187},
	pages = {147 – 159},
	doi = {10.1016/j.biosystemseng.2019.09.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072256511&doi=10.1016%2fj.biosystemseng.2019.09.005&partnerID=40&md5=9982c51ebdb62168092c67ffb2c9d26f},
	affiliations = {Department of Engineering, Aarhus University, Finlandsgade 22, Aarhus N, 8200, Denmark},
	abstract = {Plants seedlings are a part of a domain with low inter-class and relatively high intra-class variance with respect to visual appearance. This paper presents an approach for generating artificial image samples of plant seedlings using generative adversarial networks (GAN) to alleviate for the lack of training data for deep learning systems in this domain. We show that it is possible to use GAN to produce samples that are visually distinct across nine different plants species and maintain a high amount variance within each species. The generated samples resemble the intended species with an average recognition accuracy of 58.9±9.2%, evaluated using a state-of-the-art classification model. The observed errors are related to samples representing species which are relatively anonymous at the dicotyledonous growth stage and to the model's incapability to reproduce small shape details. The artificial plant samples are also used for pretraining a classification model, which is finetuned using real data. The pretrained model achieves 62.0±5.3% accuracy on classifying real plant seedlings prior to any finetuning, thus providing a strong basis for further training. However, finetuning the pretrained models show no performance increase compared to models trained without finetuning, as both approaches are capable of achieving near perfect classification on the dataset applied in this work. © 2019 IAgrE},
	author_keywords = {GAN; Image synthesis; Low inter-class and high intra-class variance; Plant classification; Plant seedlings},
	keywords = {Classification (of information); Deep learning; Adversarial networks; Classification models; Image synthesis; Intra class; Plant classification; Plant seedlings; Recognition accuracy; Without fine-tuning; Seed},
	correspondence_address = {S.L. Madsen; Department of Engineering, Aarhus University, Aarhus N, Finlandsgade 22, 8200, Denmark; email: slm@eng.au.dk},
	publisher = {Academic Press},
	issn = {15375110},
	coden = {BEINB},
	language = {English},
	abbrev_source_title = {Biosyst. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18}
}@ARTICLE{Remeš201822,
	author = {Remeš, Václav and Haindl, Michal},
	title = {Rotationally invariant bark recognition},
	year = {2018},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {11004 LNCS},
	pages = {22 – 31},
	doi = {10.1007/978-3-319-97785-0_3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052215669&doi=10.1007%2f978-3-319-97785-0_3&partnerID=40&md5=a18dd519a73f4d6b97061a2021b9a604},
	affiliations = {The Institute of Information Theory and Automation, Czech Academy of Sciences, Prague, Czech Republic},
	abstract = {An efficient bark recognition method based on a novel wide-sense Markov spiral model textural representation is presented. Unlike the alternative bark recognition methods based on various gray-scale discriminative textural descriptions, we benefit from fully descriptive color, rotationally invariant bark texture representation. The proposed method significantly outperforms the state-of-the-art bark recognition approaches in terms of the classification accuracy. © Springer Nature Switzerland AG 2018.},
	author_keywords = {Bark recognition; Spiral markov random field model; Tree taxonomy classification},
	keywords = {Image segmentation; Markov processes; Syntactics; Bark recognition; Classification accuracy; Markov Random Field model; Recognition methods; Spiral models; State of the art; Taxonomy classifications; Texture representation; Pattern recognition},
	correspondence_address = {M. Haindl; The Institute of Information Theory and Automation, Czech Academy of Sciences, Prague, Czech Republic; email: haindl@utia.cz},
	editor = {Hancock E.R. and Ho T.K. and Biggio B. and Wilson R.C. and Robles-Kelly A. and Bai X.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331997784-3},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: Joint IAPR International Workshops on Structural and Syntactic Pattern Recognition, SSPR 2018 and Statistical Techniques in Pattern Recognition, SPR 2018; Conference date: 17 August 2018 through 19 August 2018; Conference code: 217049}
}

@CONFERENCE{Pavaloiu2017599,
	author = {Pavaloiu, Ionel-Bujorel and Ancuceanu, Robert and Enache, Claudia-Maria and Vasilateanu, Andrei},
	title = {Important shape features for Romanian medicinal herb identification based on leaf image},
	year = {2017},
	journal = {2017 E-Health and Bioengineering Conference, EHB 2017},
	pages = {599 – 602},
	doi = {10.1109/EHB.2017.7995495},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028536379&doi=10.1109%2fEHB.2017.7995495&partnerID=40&md5=ca4aff2eb42262c8c120cbe08ca794fb},
	affiliations = {Department of Engineering in Foreign Languages, University POLITEHNICA of Bucharest, Bucharest, Romania; Carol Davila University of Medicine and Pharmacy, Bucharest, Romania; Highschool Stefan Demetrescu, Bucharest, Romania},
	abstract = {Herbal medicines can be used in treating health conditions under qualified medical observation. Various advantages are perceived by many consumers as being associated with using herbal medicines compared to conventional pharmaceutical products, such as the reduced risk of side effects, effectiveness with chronic conditions, lower cost and large availability, although robust evidence supporting such perceptions is most often not available. When the plant collection is done by persons without appropriate training, the risks of misidentification may be serious, even fatal. There are launched several applications for plant identification from photographs and a large number of papers dedicated to plant recognition using leaves images. There are mobile applications now which can be used for plant recognition and we want to create one that can be able to recognize the Romanian medicinal plants, allowing the user a safe collection. We present in this paper the current methods for plant identification using leaf image, as well as the parameters which are used for the classification. We continue the study with the feasibility of Romanian medicinal plant recognition from leaf images and perform a feature selection procedure on the parameters proposed in literature. © 2017 IEEE.},
	author_keywords = {feature selection; leaf identification; Romanian herbal medicines},
	keywords = {Feature extraction; Health; Image processing; Medicine; Plant extracts; Risk perception; Chronic conditions; Herbal medicines; Leaf identification; Mobile applications; Pharmaceutical products; Plant identification; Plant recognition; Selection procedures; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153860358-1},
	language = {English},
	abbrev_source_title = {E-Heal. Bioeng. Conf., EHB},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 6th IEEE International Conference on E-Health and Bioengineering, EHB 2017; Conference date: 22 June 2017 through 24 June 2017; Conference code: 129704}
}

@CONFERENCE{Sabu2018574,
	author = {Sabu, Amala and Sreekumar, K. and Nair, Rahul R},
	title = {Recognition of ayurvedic medicinal plants from leaves: A computer vision approach},
	year = {2018},
	journal = {2017 4th International Conference on Image Information Processing, ICIIP 2017},
	volume = {2018-January},
	pages = {574 – 578},
	doi = {10.1109/ICIIP.2017.8313782},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046966353&doi=10.1109%2fICIIP.2017.8313782&partnerID=40&md5=9bf2898bd670f576a1d6373d606bdebe},
	affiliations = {Department of Computer Science, College of Engineering Poonjar, Kerala, India; Robotics and Cognitive Systems Group, Tata Consultancy Services, Kochi, Kerala, India},
	abstract = {Plants are an indispensable part of our ecosystem and India has a long history of using plants as a source of medicines. Since the advent of modern allopathic medicine, the use of traditional medicine declined to a considerable extent. However, in recent years, traditional medicine has made a comeback for a variety of reasons like they are inexpensive, nontoxic and does not impact any side effect. Different kind of medicinal plant species are available on earth but it is very difficult to identify the plant. Considerable knowledge accumulated by the villagers and tribal on medicine from plants remains unknown to the scientists and urban people. This kind of knowledge is usually handed down through generations. Our immediate concern is to preserve this knowledge in digital form through the concepts of machine learning, pattern recognition and computer vision. A machine can identify a medicinal plant through the features extracted from the leaf images, together with a classification algorithm. This paper proposes a computer vision approach for the recognition of ayurvedic medicinal plant species found in Western Ghats of India. The proposed system uses a combination of SURF and HOG features extracted from leaf images and a classification using k-NN classifier. Our experiments show results which seem to be sufficient for building apps for real life use. © 2017 IEEE.},
	author_keywords = {Computer Vision; HOG; Leaf Recognition; Machine Learning; Plant Recognition; SURF},
	keywords = {Artificial intelligence; Computer vision; Learning systems; Medical applications; Nearest neighbor search; Classification algorithm; Digital forms; k-NN classifier; Leaf recognition; Medicinal plants; Plant recognition; SURF; Traditional medicines; Plants (botany)},
	editor = {Tyagi V. and Ghrera S.P. and Singh A.K. and Gupta P.K.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150906734-3},
	language = {English},
	abbrev_source_title = {Int. Conf. Image Inf. Process., ICIIP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24; Conference name: 4th International Conference on Image Information Processing, ICIIP 2017; Conference date: 21 December 2017 through 23 December 2017; Conference code: 135241}
}

@ARTICLE{Blomley2017142,
	author = {Blomley, Rosmarie and Hovi, Aarne and Weinmann, Martin and Hinz, Stefan and Korpela, Ilkka and Jutzi, Boris},
	title = {Tree species classification using within crown localization of waveform LiDAR attributes},
	year = {2017},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	volume = {133},
	pages = {142 – 156},
	doi = {10.1016/j.isprsjprs.2017.08.013},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031500536&doi=10.1016%2fj.isprsjprs.2017.08.013&partnerID=40&md5=f19c74ac837fe0035309f743aad92612},
	affiliations = {Karlsruhe Institute of Technology (KIT), Institute of Photogrammetry and Remote Sensing, Englerstraße 7, Karlsruhe, 76131, Germany; Aalto University, School of Engineering, Department of Built Environment, POB. 15800, Aalto, 00076, Finland; University of Helsinki, Department of Forest Sciences, POB. 24, 00014 University of Helsinki, Finland},
	abstract = {Since forest planning is increasingly taking an ecological, diversity-oriented perspective into account, remote sensing technologies are becoming ever more important in assessing existing resources with reduced manual effort. While the light detection and ranging (LiDAR) technology provides a good basis for predictions of tree height and biomass, tree species identification based on this type of data is particularly challenging in structurally heterogeneous forests. In this paper, we analyse existing approaches with respect to the geometrical scale of feature extraction (whole tree, within crown partitions or within laser footprint) and conclude that currently features are always extracted separately from the different scales. Since multi-scale approaches however have proven successful in other applications, we aim to utilize the within-tree-crown distribution of within-footprint signal characteristics as additional features. To do so, a spin image algorithm, originally devised for the extraction of 3D surface features in object recognition, is adapted. This algorithm relies on spinning an image plane around a defined axis, e.g. the tree stem, collecting the number of LiDAR returns or mean values of returns attributes per pixel as respective values. Based on this representation, spin image features are extracted that comprise only those components of highest variability among a given set of library trees. The relative performance and the combined improvement of these spin image features with respect to non-spatial statistical metrics of the waveform (WF) attributes are evaluated for the tree species classification of Scots pine (Pinus sylvestris L.), Norway spruce (Picea abies (L.) Karst.) and Silver/Downy birch (Betula pendula Roth/Betula pubescens Ehrh.) in a boreal forest environment. This evaluation is performed for two WF LiDAR datasets that differ in footprint size, pulse density at ground, laser wavelength and pulse width. Furthermore, we evaluate the robustness of the proposed method with respect to internal parameters and tree size. The results reveal, that the consideration of the crown-internal distribution of within-footprint signal characteristics captured in spin image features improves the classification results in nearly all test cases. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
	author_keywords = {Classification; Feature design; Geometric features; Multi-scale; Tree species; WF-recording LiDAR},
	keywords = {Betula pendula; Picea abies; Pinus sylvestris; Classification (of information); Extraction; Image enhancement; Image processing; Object recognition; Optical radar; Plants (botany); Remote sensing; Classification results; Geometric feature; Light detection and ranging; Multi-scale; Picea Abies (L.) Karst; Remote sensing technology; Tree species; Tree species identifications; algorithm; boreal forest; canopy architecture; coniferous tree; deciduous tree; design; ecological approach; geometry; height; image analysis; image classification; lidar; pixel; prediction; remote sensing; signal; waveform analysis; wavelength; Forestry},
	correspondence_address = {R. Blomley; Karlsruhe Institute of Technology (KIT), Institute of Photogrammetry and Remote Sensing, Karlsruhe, Englerstraße 7, 76131, Germany; email: rosmarie.blomley@kit.edu},
	publisher = {Elsevier B.V.},
	issn = {09242716},
	coden = {IRSEE},
	language = {English},
	abbrev_source_title = {ISPRS J. Photogramm. Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28}
}

@CONFERENCE{Polonen2018,
	author = {Polonen, Ilkka and Annala, Leevi and Rahkonen, Samuli and Nevalainen, Olli and Honkavaara, Eija and Tuominen, Sakari and Viljanen, Niko and Hakala, Teemu},
	title = {Tree Species Identification Using 3D Spectral Data and 3D Convolutional Neural Network},
	year = {2018},
	journal = {Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing},
	volume = {2018-September},
	doi = {10.1109/WHISPERS.2018.8747253},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073904317&doi=10.1109%2fWHISPERS.2018.8747253&partnerID=40&md5=f4074eda3243ea59d83e1a072fa0dcf1},
	affiliations = {Faculty of Information Technology, University of Jyväskylä, P.O. Box 35, Jyväskylä, FI-40014, Finland; National Land Survey of Finland, Finnish Geospatial Research Insititute, Geodeetinrinne 2, Masala, 02430, Finland; Natural Resources Institute Finland, Helsinki, PL 2 00791, Finland},
	abstract = {In this study we apply 3D convolutional neural network (CNN) for tree species identification. Study includes the three most common Finnish tree species. Study uses a relatively large high-resolution spectral data set, which contains also a digital surface model for the trees. Data has been gathered using an unmanned aerial vehicle, a framing hyperspectral imager and a regular RGB camera. Achieved classification results are promising by with overall accuracy of 96.2 % for the classification of the validation data set. © 2018 IEEE.},
	author_keywords = {3D; convolutional neural network; spectral imaging; Tree species; UAV},
	keywords = {Antennas; Classification (of information); Convolution; Forestry; Image processing; Neural networks; Remote sensing; Spectroscopy; Unmanned aerial vehicles (UAV); Classification results; Convolutional neural network; Digital surface models; Hyperspectral imagers; Overall accuracies; Spectral imaging; Tree species; Tree species identifications; Trees (mathematics)},
	publisher = {IEEE Computer Society},
	issn = {21586276},
	isbn = {978-172811581-8},
	language = {English},
	abbrev_source_title = {Workshop Hyperspectral Image Signal Proces.: Evol. Remote Sens.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; Conference name: 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing, WHISPERS 2018; Conference date: 23 September 2018 through 26 September 2018; Conference code: 149100; All Open Access, Green Open Access}
}

@ARTICLE{Lee20184287,
	author = {Lee, Sue Han and Chan, Chee Seng and Remagnino, Paolo},
	title = {Multi-Organ Plant Classification Based on Convolutional and Recurrent Neural Networks},
	year = {2018},
	journal = {IEEE Transactions on Image Processing},
	volume = {27},
	number = {9},
	pages = {4287 – 4301},
	doi = {10.1109/TIP.2018.2836321},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046998909&doi=10.1109%2fTIP.2018.2836321&partnerID=40&md5=a778a5ef6649307fd831dbfbd2fa8da4},
	affiliations = {Center of Image and Signal Processing, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, 50603, Malaysia; Faculty of Science, Engineering and Computing, Kingston University, Kingston-Upon-Thames, KT1 2EE, United Kingdom},
	abstract = {Classification of plants based on a multi-organ approach is very challenging. Although additional data provide more information that might help to disambiguate between species, the variability in shape and appearance in plant organs also raises the degree of complexity of the problem. Despite promising solutions built using deep learning enable representative features to be learned for plant images, the existing approaches focus mainly on generic features for species classification, disregarding the features representing plant organs. In fact, plants are complex living organisms sustained by a number of organ systems. In our approach, we introduce a hybrid generic-organ convolutional neural network (HGO-CNN), which takes into account both organ and generic information, combining them using a new feature fusion scheme for species classification. Next, instead of using a CNN-based method to operate on one image with a single organ, we extend our approach. We propose a new framework for plant structural learning using the recurrent neural network-based method. This novel approach supports classification based on a varying number of plant views, capturing one or more organs of a plant, by optimizing the contextual dependencies between them. We also present the qualitative results of our proposed models based on feature visualization techniques and show that the outcomes of visualizations depict our hypothesis and expectation. Finally, we show that by leveraging and combining the aforementioned techniques, our best network outperforms the state of the art on the PlantClef2015 benchmark. The source code and models are available at https://github.com/cs-chan/Deep-Plant. © 1992-2012 IEEE.},
	author_keywords = {deep learning; Plant classification},
	keywords = {Algorithms; Databases, Factual; Deep Learning; Image Processing, Computer-Assisted; Plants; Biological systems; Biology; Complex networks; Computer vision; Convolution; Deep learning; Feature extraction; Job analysis; Learning systems; Recurrent neural networks; Visualization; Convolutional neural network; Degree of complexity; Plant classification; Recurrent neural network (RNN); Shape; Species classification; Structural learning; Task analysis; algorithm; classification; factual database; image processing; plant; procedures; Classification (of information)},
	correspondence_address = {C.S. Chan; Center of Image and Signal Processing, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, 50603, Malaysia; email: cs.chan@um.edu.my},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {10577149},
	coden = {IIPRE},
	pmid = {29870348},
	language = {English},
	abbrev_source_title = {IEEE Trans Image Process},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 68; All Open Access, Green Open Access}
}

@ARTICLE{Gao2018,
	author = {Gao, Zhi-Yong and Xie, Heng-Xing and Li, Ji-Feng and Liu, Shi-Li},
	title = {Spatial-Structure Siamese Network for Plant Identification},
	year = {2018},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	volume = {32},
	number = {11},
	doi = {10.1142/S0218001418500350},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047915526&doi=10.1142%2fS0218001418500350&partnerID=40&md5=eedd87a56ff1b91875014b226178b5c9},
	affiliations = {School of Chemistry and Environment, Weinan Normal University, Weinan, Shaanxi, 714099, China; Key Laboratory for Ecology and Environment of River Wetlands in Shaanxi Province, Weinan, Shaanxi, 714099, China},
	abstract = {Plant identification is now attracting considerable attention due to its important applications in agriculture automation and ecosystems. Recently, deep learning-based plant identification methods have drawn increasing interest and shown favorable performance. However, existing methods do not consider plant spatial structure and their similarities explicitly. In this paper, we propose a robust spatial-structure siamese network (3SN) for plant identification, which has the following advantages: (1) It models the spatial structure of a plant by recurrent neural networks exploiting their capability to capture long-range dependencies among sequential data, which enables it to capture even a slight difference between a specific plant and distractors. (2) The plant similarity modeling is achieved effectively by a siamese network with large numbers of image pairs. In this way, the plant classification task and siamese learning task are learned jointly in a unified framework, where both can enhance and complement each other. Extensive experimental results show that the proposed 3SN method outperforms the state-of-the-art methods consistently. © 2018 World Scientific Publishing Company.},
	author_keywords = {deep learning; Plant identification; RNNs; siamese network},
	keywords = {Recurrent neural networks; Long-range dependencies; Plant classification; Plant identification; RNNs; Similarity models; Spatial structure; State-of-the-art methods; Unified framework; Deep learning},
	publisher = {World Scientific Publishing Co. Pte Ltd},
	issn = {02180014},
	coden = {IJPIE},
	language = {English},
	abbrev_source_title = {Int J Pattern Recognit Artif Intell},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Yigit2019369,
	author = {Yigit, Enes and Sabanci, Kadir and Toktas, Abdurrahim and Kayabasi, Ahmet},
	title = {A study on visual features of leaves in plant identification using artificial intelligence techniques},
	year = {2019},
	journal = {Computers and Electronics in Agriculture},
	volume = {156},
	pages = {369 – 377},
	doi = {10.1016/j.compag.2018.11.036},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057566780&doi=10.1016%2fj.compag.2018.11.036&partnerID=40&md5=e4012ad0de2ccdb6e09d72b0d3dadf86},
	affiliations = {Depertment of Electrical Electronics Engineering, Engineering Faculty, Karamanoglu Mehmetbey Unıversity, Karaman, 70100, Turkey},
	abstract = {In this paper, artificial intelligence techniques (AIT) such as artificial neural network, naive bayes algorithm, random forest algorithm, K-nearest neighborhood (KNN) and support vector machine (SVM) are implemented to design an automatic identifier for the plant leaves. For this purpose, data of 637 healthy leaves consisting of 32 different plant species are used. 22 visual features (VF) of each leaf are extracted by using image processing techniques. These 22 VF are considered in 4 groups including dimension (D#6), color (C#6), texture (T#5) and pattern (P#5). In order to investigate the effects of these groups on the classifying performance, 15 possible different combinations from the 4 groups are constituted. The models are then trained via the data of 510 leaves, and their accuracy are tested through the data of 127 leaves. From the results of the test, SVM model with the accuracy of 92.91% is found to be the most successful identifier for combination including all groups. The next best result is achieved with the accuracy of 87.40% for the combination of D#6, C#6 and P#5 groups. Since the most important issue in the classification process is the use of the minimum number of VF, 16 most effective VF on the identification are determined by means of correlation-based feature selection (CFS) method. The best result for these 16 VF is also achieved with the accuracy of 94.49% by the SVM model. Then the performance of the proposed method is tested to identify the diseased and defected leaves. Therefore, 637 healthy and 33 diseased/defected leaves are put together. Randomly selected 536 leaves corresponding to 80% of all leaves are used for training and the remaining 134 leaves are used for testing, and identified with the accuracy of 92.53% by the SVM model. With this study, it is numerically revealed that the P#5 is the most effective feature group. Moreover, it has been determined that the most effective feature in the P#5 group is the feature of edge Fourier transform. The results point out that, if AIT models are properly modelled and trained, they can be successfully and effectively applied to the identification of the plants even if there are diseased and defected samples. © 2018 Elsevier B.V.},
	author_keywords = {Artificial intelligence techniques; Fourier Transform; Identification; Image processing technique; Plant species},
	keywords = {Decision trees; Defects; Fourier transforms; Identification (control systems); Image processing; Nearest neighbor search; Neural networks; Support vector machines; Artificial intelligence techniques; Classification process; Correlation based feature selections (CFS); Image processing technique; K-nearest neighborhoods; Naive-Bayes algorithm; Plant species; Random forest algorithm; accuracy assessment; algorithm; artificial intelligence; Fourier transform; identification method; image processing; numerical method; performance assessment; plant; Plants (botany)},
	correspondence_address = {E. Yigit; Depertment of Electrical Electronics Engineering, Engineering Faculty, Karamanoglu Mehmetbey Unıversity, Karaman, 70100, Turkey; email: enesyigit@kmu.edu.tr},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 56}
}

@ARTICLE{Wang201825,
	author = {Wang, Xiushan and Zhang, Hehu and Chen, Ying and Jiang, Guoqiang and Zou, Caihong and Li, Jian},
	title = {Plant canopy center recognition based on bionics principle of insect eating leaves},
	year = {2018},
	journal = {Paper Asia},
	volume = {COMPENDIUM 1},
	number = {7},
	pages = {25 – 28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059132877&partnerID=40&md5=a19e4fe39bfb6874e4b237187cc7da17},
	affiliations = {College of Mechanical & Electrical Engineering, Henan Agricultural University, Zhengzhou, 450002, China; College of Humanities and Law, Henan Agricultural University, Zhengzhou, 450002, China; Henan MingXinYuan Technology Co., LTD, Zhengzhou, 450002, China},
	abstract = {Machine vision technology is widely used in agriculture plant recognition and ecosystem assessment. However, it is still difficult to recognize the center of plant canopy due to the similar color background and occlusion problems. The main difficult is the interference of excess leaves in the canopy. Learn from this inspiration that insects first gnaw away the excess leaves around the canopy thus the plant canopy center recognition based on bionics principle of insect eating leaves was put forward. First of all, color index method is used to extract green vegetation; then design insect eating leaf algorithm eliminating excess leaves around canopy; finally, the gray level threshold algorithm is used to extract the canopy center. The result show that the algorithm based on bionics can successfully eliminate the excess canopy leaves, and leave the leaf base. Canopy center identification rate is 92.65 and the standard deviation of the segmentation was 1.67%. This study provided a machine vision method for plant canopy center recognition as well as a theoretical reference for further plant canopy research. © 2018, SHPMedia Sdn Bhd. All rights reserved.},
	author_keywords = {Bionics; Computer vision; Ecosystem assessment; Plant canopy; Weed},
	keywords = {Bionics; Color; Ecosystems; Insects; Leaves; Machinery; Plants; Biomimetics; Bionics; Biophysics; Computer vision; Ecosystems; Bionics principles; Ecosystem assessment; Gray level thresholds; Identification rates; Machine vision technologies; Plant canopies; Standard deviation; Weed; Plants (botany)},
	publisher = {SHPMedia Sdn Bhd},
	issn = {02184540},
	language = {English},
	abbrev_source_title = {Paper Asia},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Feng2018364,
	author = {Feng, Qingchun and Wang, Xiu and Li, Cuiling and Chen, Jian},
	title = {Color correction of seedling leaf image under various sunlight illuminations in greenhouse},
	year = {2018},
	journal = {International Agricultural Engineering Journal},
	volume = {27},
	number = {3},
	pages = {364 – 370},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059307965&partnerID=40&md5=3349d1fa7ef7687e974d5732af587b0d},
	affiliations = {College of Engineering, China Agricultural University, Beijing, 100085, China; National Research Center of Intelligent Equipment for Agriculture, Beijing Key Laboratory of Intelligent Equipment Technology for Agriculture, Beijing Research Center of Intelligent Equipment for Agriculture, Beijing, 100097, China},
	abstract = {In view of the need for monitoring vegetable seedlings’ growth in greenhouse based on machine vision, the method for correcting image color to overcome color distortion generated from the various sunlight in greenhouse, was studied in the paper, which was supposed to stabilize the image color and simplify leaf recognition. Through analyzing the correlation between the image RGB gray value and the sunlight intensity, the illumination variation was parsed based on image color information, and the linearity correction method for seedling image RGB value was proposed to compensate sunlight fluctuation. The L*a*b* chromatism value was adopted to evaluate distortion of the image color under various illumination, and the image color of ColorChecker’s Foliage color was referred as the standard color value. The linear correction factors of seedling image RGB value was calculated as the ratio of Foliage’s timely image RGB and the standard color RGB values. As is shown in field test result, after being corrected, the maximum distortion value of seedling image color decreased to 14.96, 38.03% of the maximum value before correction, under naturally various sunlight of 6450 lx to 59900 lx intensity during the time 7:00 to 17:00 in greenhouse, and the variances of seedling R, G, B value respectively reduce to 4.90, 4.73 and 3.11. The research was supposed to advance the study on agricultural target’s visual information acquisition under the natural sunlight. © 2018, Asian Association for Agricultural Engineering. All rights reserved.},
	author_keywords = {Color correction; Seedling growth monitoring; Sunlight variation},
	keywords = {Color; Greenhouses; Color correction; Illumination variation; Linear corrections; Linearity corrections; Monitoring vegetables; Seedling growth; Sunlight variation; Visual information; Color image processing},
	correspondence_address = {J. Chen; China Agricultural University, Beijing, 100085, China; email: jchen@cau.edu.cn},
	publisher = {Asian Association for Agricultural Engineering},
	issn = {08582114},
	coden = {IAEJE},
	language = {English},
	abbrev_source_title = {Int Agric Eng J},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Carranza-Rojas2017181,
	author = {Carranza-Rojas, Jose and Goeau, Herve and Bonnet, Pierre and Mata-Montero, Erick and Joly, Alexis},
	title = {Going deeper in the automated identification of Herbarium specimens},
	year = {2017},
	journal = {BMC evolutionary biology},
	volume = {17},
	number = {1},
	pages = {181},
	doi = {10.1186/s12862-017-1014-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030693039&doi=10.1186%2fs12862-017-1014-z&partnerID=40&md5=b704b7dc5c467cb6a74ccf087d56cc9c},
	affiliations = {INRIA, Montpellier, France; INRIA, Montpellier, France; INRIA, Montpellier, France; INRIA, Montpellier, France},
	abstract = {CONCLUSIONS: This is, to our knowledge, the first study that uses deep learning to analyze a big dataset with thousands of species from herbaria. Results show the potential of Deep Learning on herbarium species identification, particularly by training and testing across different datasets from different herbaria. This could potentially lead to the creation of a semi, or even fully automated system to help taxonomists and experts with their annotation, classification, and revision works.; BACKGROUND: Hundreds of herbarium collections have accumulated a valuable heritage and knowledge of plants over several centuries. Recent initiatives started ambitious preservation plans to digitize this information and make it available to botanists and the general public through web portals. However, thousands of sheets are still unidentified at the species level while numerous sheets should be reviewed and updated following more recent taxonomic knowledge. These annotations and revisions require an unrealistic amount of work for botanists to carry out in a reasonable time. Computer vision and machine learning approaches applied to herbarium sheets are promising but are still not well studied compared to automated species identification from leaf scans or pictures of plants in the field.; RESULTS: In this work, we propose to study and evaluate the accuracy with which herbarium images can be potentially exploited for species identification with deep learning technology. In addition, we propose to study if the combination of herbarium sheets with photos of plants in the field is relevant in terms of accuracy, and finally, we explore if herbarium images from one region that has one specific flora can be used to do transfer learning to another region with other species; for example, on a region under-represented in terms of collected data.},
	author_keywords = {Biodiversity informatics; Computer vision; Deep learning; Herbaria; Plant identification},
	keywords = {Algorithms; Automation; Image Processing, Computer-Assisted; Plant Leaves; Plants; Specimen Handling; algorithm; anatomy and histology; automation; classification; image processing; plant; plant leaf; specimen handling},
	issn = {14712148},
	pmid = {28797242},
	language = {English},
	abbrev_source_title = {BMC Evol. Biol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 136; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Pang20187851,
	author = {Pang, Cheng and Yao, Hongxun and Sun, Xiaoshuai and Zhao, Sicheng and Yu, Wei},
	title = {Rediscover flowers structurally},
	year = {2018},
	journal = {Multimedia Tools and Applications},
	volume = {77},
	number = {7},
	pages = {7851 – 7863},
	doi = {10.1007/s11042-017-4679-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017465838&doi=10.1007%2fs11042-017-4679-9&partnerID=40&md5=fd902cd7604ffc38e65ca2431d5c5252},
	affiliations = {School of Computer Science and Technology, Harbin Institute of Technology, No.92 West Dazhi Street, Harbin, 150001, Heilongjiang, China},
	abstract = {Existing methods for flower classification are usually focused on segmentation of the foreground, followed by extraction of features. After extracting the features from the foreground, global pooling is performed for final classification. Although this pipeline can be applied to many recognition tasks, however, these approaches have not explored structural cues of the flowers due to the large variation in their appearances. In this paper, we argue that structural cues are essential for flower recognition. We present a novel approach that explores structural cues to extract features. The proposed method encodes the structure of flowers into the final feature vectors for classification by operating on salient regions, which is robust to appearance variations. In our framework, we first segment the flower accurately by refining the existing segmentation method, and then we generate local features using our approach. We combine our local feature with global-pooled features for classification. Evaluations on the Oxford Flower dataset shows that by introducing the structural cues and locally pooling of some off-the-shelf features, our method outperforms the state-of-the-arts which employ specific designed features and metric learning. © 2017, Springer Science+Business Media New York.},
	author_keywords = {Feature extraction; Fine-grained classification; Image classification; Saliency detection},
	keywords = {Extraction; Feature extraction; Image classification; Feature vectors; Fine grained; Flower recognition; Metric learning; Saliency detection; Salient regions; Segmentation methods; State of the art; Classification (of information)},
	publisher = {Springer New York LLC},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Pankaja2019667,
	author = {Pankaja, K. and Suma, V.},
	title = {Leaf recognition and classification using Chebyshev moments},
	year = {2019},
	journal = {Smart Innovation, Systems and Technologies},
	volume = {105},
	pages = {667 – 678},
	doi = {10.1007/978-981-13-1927-3_70},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056901667&doi=10.1007%2f978-981-13-1927-3_70&partnerID=40&md5=7a78a37ee8c1b1978e8b91dae96ca0fc},
	affiliations = {Cambridge Institute of Technology, Computer Science and Engineering, VTU, Bengaluru, India; Dayanand Sagar College of Engineering, Information Science and Engineering, VTU, Bengaluru, India},
	abstract = {Earth contains millions of plants; each of this plant leaf has its own unique features. On related to their unique features plants leaf is used in different sectors in day to day life. Hence, proper identification of each leaf exhibits good result. According to the survey 50% of plant leaf is used in medical sector making medication for respective disease treatment. So plant leaf recognition plays a significant role. Many researches are conducted on leaf identification using different technology. This paper put forth an automatic leaf image identification model using image processing techniques. The proposed paper has been presented on leaf identification model, by using several feature extraction schemes. Feature extraction technique is carried out based on the texture, color and shape of the leaf images. The proposed model considered thirty classes of Flavia dataset with a total of 270 leaf images. The application of four different schemes for feature extraction increases the accuracy of the system up to 96.29%. © Springer Nature Singapore Pte Ltd. 2019.},
	author_keywords = {Chebyshev moments and roundness; DWT texture; Flavia database; HSV color moments; Support vector machine (SVM) classifier},
	keywords = {Classification (of information); Extraction; Feature extraction; Intelligent computing; Support vector machines; Chebyshev moments; Color moments; Disease treatment; Feature extraction techniques; Image processing technique; Leaf identification; Leaf recognition; Unique features; Image processing},
	correspondence_address = {K. Pankaja; Cambridge Institute of Technology, Computer Science and Engineering, VTU, Bengaluru, India; email: pankaja.osr@gmail.com},
	editor = {Das S. and Bhateja V. and Satapathy S.C.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	issn = {21903018},
	isbn = {978-981131926-6},
	language = {English},
	abbrev_source_title = {Smart Innov. Syst. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 2nd International Conference on Smart Computing and Informatics, SCI 2018; Conference date: 27 January 2018 through 28 January 2018; Conference code: 219289}
}

@ARTICLE{Riegler-Nurscher2018188,
	author = {Riegler-Nurscher, Peter and Prankl, Johann and Bauer, Thomas and Strauss, Peter and Prankl, Heinrich},
	title = {A machine learning approach for pixel wise classification of residue and vegetation cover under field conditions},
	year = {2018},
	journal = {Biosystems Engineering},
	volume = {169},
	pages = {188 – 198},
	doi = {10.1016/j.biosystemseng.2018.02.011},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043599646&doi=10.1016%2fj.biosystemseng.2018.02.011&partnerID=40&md5=720b5794dc4fad25063ff0b88e6bf2a6},
	affiliations = {Josephinum Research, Rottenhauser Straße 1, Wieselburg, Austria; Inst. for Land and Water Management Research, Federal Agency for Water Management, Petzenkirchen, Austria},
	abstract = {Soil cover is a crucial factor for sustainable cultivation of arable land. A certain degree of residue and vegetation cover reduces erosion significantly and has positive effects on plant development. In order to accomplish these positive effects, it is necessary to measure and control the amount of soil cover on fields. Manual measurement methods are time consuming and/or subjective. Available image analysis methods often lack of generalisation and accuracy. Many approaches only focus on residue or on vegetation cover and do not consider different camera hardware. Recent advancements in machine learning techniques are promising to overcome these issues. The proposed method, the entangled random forest, a variant of a random decision forest, classifies individual pixels into soil, residue, living plants and stones. Simple and efficient pixel-wise comparisons to neighbouring pixels are integrated as decision-features into the random forest. To validate our method, the result of the automatic classification was compared with results of manual classifications from evaluators on image grid points. The classification of soil results in a regression equation between the results of the new introduced method and a manual image classification of y = 0.99x + 2.02 (R2 = 0.93). Living plant classification results in a regression between both methods in y = 0.94x − 0.70 with (R2 = 0.98) and for dead residues in y = 1.04x − 0.64 (R2 = 0.84). It is possible to access a demo of the algorithm by using a web and a mobile application on https://soilcover.josephinum.at. © 2018 IAgrE},
	author_keywords = {2D image classification; Random forest; Soil cover estimation},
	keywords = {Artificial intelligence; Decision trees; HTTP; Learning algorithms; Learning systems; Pixels; Soils; Vegetation; 2D images; Automatic classification; Machine learning approaches; Machine learning techniques; Manual classification; Manual measurement methods; Random forests; Soil cover; Image classification},
	correspondence_address = {P. Riegler-Nurscher; Josephinum Research, Wieselburg, Rottenhauser Straße 1, Austria; email: p.riegler-nurscher@josephinum.at},
	publisher = {Academic Press},
	issn = {15375110},
	coden = {BEINB},
	language = {English},
	abbrev_source_title = {Biosyst. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 23}
}

@ARTICLE{Hidayat20185389,
	author = {Hidayat, Taufik and Nilawati, Asyaroh Ramadona},
	title = {Identification of plant types by leaf textures based on the backpropagation neural network},
	year = {2018},
	journal = {International Journal of Electrical and Computer Engineering},
	volume = {8},
	number = {6},
	pages = {5389 – 5398},
	doi = {10.11591/ijece.v8i6.pp.5389-5398},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057395135&doi=10.11591%2fijece.v8i6.pp.5389-5398&partnerID=40&md5=6f4266033c5fde6c55f0500ff5cb3bdb},
	affiliations = {Department of Informatics, Gunadarma University, Margonda Raya Street 100, Pondok Cina, Depok, Indonesia},
	abstract = {The number of species of plants or flora in Indonesia is abundant. The wealth of Indonesia's flora species is not to be doubted. Almost every region in Indonesia has one or some distinctive plant(s) which may not exist in other countries. In enhancing the potential diversity of tropical plant resources, good management and utilization of biodiversity is required. Based on such diversity, plant classification becomes a challenge to do. The most common way to recognize between one plant and another is to identify the leaf of each plant. Leaf-based classification is an alternative and the most effective way to do because leaves will exist all the time, while fruits and flowers may only exist at any given time. In this study, the researchers will identify plants based on the textures of the leaves. Leaf feature extraction is done by calculating the area value, perimeter, and additional features of leaf images such as shape roundness and slenderness. The results of the extraction will then be selected for training by using the backpropagation neural network. The result of the training (the formation of the training set) will be calculated to produce the value of recognition accuracy with which the feature value of the dataset of the leaf images is then to be matched. The result of the identification of plant species based on leaf texture characteristics is expected to accelerate the process of plant classification based on the characteristics of the leaves. Copyright © 2018 Institute of Advanced Engineering and Science. All rights reserved.},
	author_keywords = {Feature; Leaf; Perimeter; Ratio; Roundness; Slenderness},
	correspondence_address = {T. Hidayat; Department of Informatics, Gunadarma University, Pondok Cina, Depok, Margonda Raya Street 100, Indonesia; email: thidayat@staff.gunadarma.ac.id},
	publisher = {Institute of Advanced Engineering and Science},
	issn = {20888708},
	language = {English},
	abbrev_source_title = {Int. J. Electr. Comput. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Figueroa-Mata2018,
	author = {Figueroa-Mata, Geovanni and Mata-Montero, Erick and Valverde-Otarola, Juan Carlos and Arias-Aguilar, Dagoberto},
	title = {Using Deep Convolutional Networks for Species Identification of Xylotheque Samples},
	year = {2018},
	journal = {2018 IEEE International Work Conference on Bioinspired Intelligence, IWOBI 2018 - Proceedings},
	doi = {10.1109/IWOBI.2018.8464216},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054521882&doi=10.1109%2fIWOBI.2018.8464216&partnerID=40&md5=4981daa4f764187528bff837cee693ff},
	affiliations = {School of Mathematics, Costa Rica; School of Forestry Engineering, Costa Rica Institute of Technology, Cartago, Costa Rica},
	abstract = {Forest species identification is critical to scientifically support many environmental, commercial, forensic, archaeological, and paleontological actividades. Therefore, it is very important to develop fast and accurate identification systems. We present a deep CNN for automated forest species identification based on macroscopic images of wood cuts. We first implement and study a modified version of the LeNet convolutional network, which is trained from scratch with a database of macroscopic images of 41 forest species of the Brazilian flora. With this network we achieve a top-1 accuracy of 93.6%. Additionally, we fine-tune the Resnet50 model with pre-trained weights on Imagenet to reach a top-1 accuracy of 98.03%, which improves previous published results of research on the same image database. © 2018 IEEE.},
	author_keywords = {Automated plant identification; Biodiversity informatics; Convolutional neural networks; Deep learning; Forest species identification; Image processing},
	keywords = {Biodiversity; Convolution; Deep learning; Forestry; Image processing; Neural networks; Convolutional networks; Convolutional neural network; Forest species; Image database; Informatics; Plant identification; Species identification; Image enhancement},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153867506-9},
	language = {English},
	abbrev_source_title = {IEEE Int. Work Conf. Bioinspired Intell., IWOBI - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; Conference name: 2018 IEEE International Work Conference on Bioinspired Intelligence, IWOBI 2018; Conference date: 18 July 2018 through 20 July 2018; Conference code: 139802}
}

@ARTICLE{Fu201839,
	author = {Fu, Li and Zheng, Yuhong and Zhang, Pengchong and Zhu, Jiangwei and Zhang, Haoyang and Zhang, Luxi and Su, Weitao},
	title = {Embedding leaf tissue in graphene ink to improve signals in electrochemistry-based chemotaxonomy},
	year = {2018},
	journal = {Electrochemistry Communications},
	volume = {92},
	pages = {39 – 42},
	doi = {10.1016/j.elecom.2018.05.018},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047452477&doi=10.1016%2fj.elecom.2018.05.018&partnerID=40&md5=d033f22580452c48a7bed1dea0612d28},
	affiliations = {College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; Institute of Botany, Jiangsu Province and Chinese Academy of Sciences, Nanjing Botanical Garden, Mem. Sun Yat-Sen, Nanjing, 210014, China; Hangzhou Botanical Garden, 310013, Hangzhou, China; Collaborative Innovation Center of Sustainable Forestry in Southern China of Jiangsu Province, Nanjing Forestry University, Nanjing, 210037, China},
	abstract = {We propose a method for the electrochemical identification of plants by embedding leaf tissue into graphene deposited on a screen-printed electrode (SPE). The embedding process significantly enhanced the electrochemical signals, which made the SPE sufficiently sensitive to record information about electro-active compounds in plants. In this work, five Lycoris herbs have been used as examples to evaluate the feasibility of the proposed technique. Multidimensional pattern recognition was successfully established for plant identification. In addition, the recorded “electrochemical fingerprints” provided valuable taxonomic information, demonstrating the enormous potential of the technique for plant chemotaxonomy. © 2018 Elsevier B.V.},
	author_keywords = {Chemotaxonomy; Graphene ink; Multivariate chemometric analysis; Plant identification; Screen printed electrode},
	keywords = {Biochemistry; Electrochemical electrodes; Electrochemistry; Graphene; Multivariant analysis; Pattern recognition; Tissue; Chemometric analysis; Chemotaxonomy; Graphene inks; Plant identification; Screen printed electrodes; Tissue engineering},
	correspondence_address = {L. Fu; College of Materials and Environmental Engineering, Hangzhou Dianzi University, Hangzhou, 310018, China; email: fuli@hdu.edu.cn},
	publisher = {Elsevier Inc.},
	issn = {13882481},
	coden = {ECCMF},
	language = {English},
	abbrev_source_title = {Electrochem. Commun.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 37}
}

@CONFERENCE{Adinugroho2018350,
	author = {Adinugroho, Sigit and Sari, Yuita Arum},
	title = {Leaves classification using neural network based on ensemble features},
	year = {2018},
	journal = {2018 5th International Conference on Electrical and Electronics Engineering, ICEEE 2018},
	pages = {350 – 354},
	doi = {10.1109/ICEEE2.2018.8391360},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050037667&doi=10.1109%2fICEEE2.2018.8391360&partnerID=40&md5=73da031c0c6ee10eae1057d53ff12e9f},
	affiliations = {Computer Vision Research Group, Faculty of Computer Science, Brawijaya University, Malang, Indonesia},
	abstract = {An automated plant identification is necessary to identify plants, especially rarely seen ones. In this paper a framework to identify plant species based on leaf's characteristics is introduced. First, 31 features of leaves from 13 species are extracted that represents color, shape and texture of the leaves. Then, the features are selected according to their correlation to the class label. The data with 25.8% pruned features are then used to train a feedforward neural network. The network is trained and tested using 975 images by implementing 10-fold mechanism yields 95.54% accuracy. © 2018 IEEE.},
	author_keywords = {feature selection; feedforward neural network; leaf recognition; plant identification},
	keywords = {Feature extraction; Feedforward neural networks; Class labels; Leaf recognition; Plant identification; Plant species; Shape and textures; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153866392-9},
	language = {English},
	abbrev_source_title = {Int. Conf. Electr. Electron. Eng., ICEEE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 5th International Conference on Electrical and Electronics Engineering, ICEEE 2018; Conference date: 3 May 2018 through 5 May 2018; Conference code: 137377}
}

@CONFERENCE{Šulc2018,
	author = {Šulc, Milan and Picek, Lukáš and Matas, Jiří},
	title = {Plant recognition by inception networks with test-time class prior estimation},
	year = {2018},
	journal = {CEUR Workshop Proceedings},
	volume = {2125},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051056834&partnerID=40&md5=0c4c6180d7fda5d19dbe0c348afd4982},
	affiliations = {Center for Machine Perception, Dept. of Cybernetics, Faculty of Electrical Engineering, Czech Technical University, Prague, Czech Republic; Dept. of Cybernetics, Faculty of Applied Sciences, University of West Bohemi, Pilsen, Czech Republic},
	abstract = {The paper describes an automatic system for recognition of 10, 000 plant species from one or more images. The system finished 1st in the ExpertLifeCLEF 2018 plant identification challenge with 88.4% accuracy and performed better than 5 of the 9 participating plant identification experts. The system is based on the Inception-ResNet-v2 and Inception-v4 Convolutional Neural Network (CNN) architectures. Performance improvements were achieved by: adjusting the CNN predictions according to the estimated change of the class prior probabilities, replacing network parameters with their running averages, and test-time data augmentation.},
	author_keywords = {Class Prior Estimation; Classification; Computer Vision; Convolutional Neural Networks; Fine-grained; Machine Learning; Plant Identification; Plant Recognition},
	keywords = {Classification (of information); Computer vision; Convolution; Neural networks; Convolutional neural network; Convolutional Neural Networks (CNN); Data augmentation; Fine grained; Network parameters; Performance improvements; Plant identification; Plant recognition; Learning systems},
	editor = {Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Nie J.-Y. and Soulier L. and Soulier L. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 19th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2018; Conference date: 10 September 2018 through 14 September 2018; Conference code: 138100}
}

@CONFERENCE{Manoj Kumar2017231,
	author = {Manoj Kumar, P. and Surya, C.M. and Gopi, Varun P.},
	title = {Identification of ayurvedic medicinal plants by image processing of leaf samples},
	year = {2017},
	journal = {Proceedings - 2017 3rd IEEE International Conference on Research in Computational Intelligence and Communication Networks, ICRCICN 2017},
	volume = {2017-December},
	pages = {231 – 238},
	doi = {10.1109/ICRCICN.2017.8234512},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048965837&doi=10.1109%2fICRCICN.2017.8234512&partnerID=40&md5=c847c46ff00bb5af21768eec7aa64e5b},
	affiliations = {Department of Electronics and Communication Engineering, Government Engineering College, Wayanad, Kerala, India},
	abstract = {Identification of the correct medicinal plants that goes in to the preparation of a medicine is very important in ayurvedic medicinal industry. The main features required to identify a medicinal plant is its leaf shape, colour and texture. Colour and texture from both sides of the leaf contain deterministic parameters to identify the species. This paper explores feature vectors from both the front and back side of a green leaf along with morphological features to arrive at a unique optimum combination of features that maximizes the identification rate. A database of medicinal plant leaves is created from scanned images of front and back side of leaves of commonly used ayurvedic medicinal plants. The leaves are classified based on the unique feature combination. Identification rates up to 99% have been obtained when tested over a wide spectrum of classifiers. The above work has been extended to include identification by dry leaves and a combination of feature vectors is obtained, using which, identification rates exceeding 94% have been achieved. © 2017 IEEE.},
	author_keywords = {classification; feature extraction; morphological features; optimization; Plant identification; texture features},
	keywords = {Artificial intelligence; Classification (of information); Feature extraction; Image processing; Optimization; Feature vectors; Identification rates; Medicinal plants; Morphological features; Optimum combination; Plant identification; Texture features; Unique features; Plants (botany)},
	editor = {Piuri V. and Mukherjee A. and Mondal A. and Das A. and Maulik U. and Bhattacharjee D. and Bhaumik H. and Panigrahi B.K. and Gandhi T. and Bhattacharyya S. and Pan I.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153861931-5},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. Res. Comput. Intel. Commun. Networks, ICRCICN},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24; Conference name: 3rd IEEE International Conference on Research in Computational Intelligence and Communication Networks, ICRCICN 2017; Conference date: 3 November 2017 through 5 November 2017; Conference code: 134056}
}

@CONFERENCE{Lee20184462,
	author = {Lee, Sue Han and Chang, Yang Loong and Chan, Chee Seng and Remagnino, Paolo},
	title = {HGO-CNN: Hybrid generic-organ convolutional neural network for multi-organ plant classification},
	year = {2018},
	journal = {Proceedings - International Conference on Image Processing, ICIP},
	volume = {2017-September},
	pages = {4462 – 4466},
	doi = {10.1109/ICIP.2017.8297126},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045341171&doi=10.1109%2fICIP.2017.8297126&partnerID=40&md5=880aa0207e3f39c6e13a8f9838edc05e},
	affiliations = {Centre of Image and Signal Processing, Fac. Comp. Sci. and Info. Tech., University of Malaya, Malaysia; Robot Vision Team, Comp. Sci. Dept., Kingston University, United Kingdom},
	abstract = {Classification of plants based on a multi-organ approach is very challenging. Although additional data provides more information that might help to disambiguate between species, the variability in shape and appearance in plant organs also raises the degree of complexity of the problem. Existing approaches focus mainly on generic features for species classification, disregarding the features representing the organs. In fact, plants are complex entities sustained by a number of organ systems. In our approach, we exploit the PlantClef2015 benchmark, and introduce a hybrid generic-organ convolutional neural network (HGO-CNN), which takes into account both organ and generic information, combining them using a new feature fusion scheme for species classification. We show that our proposed method outperforms the state-of-the-art results. © 2017 IEEE.},
	author_keywords = {Deep learning; Plant classification},
	keywords = {Complex networks; Convolution; Deep learning; Image processing; Neural networks; Additional datum; Convolutional neural network; Degree of complexity; Generic features; Generic information; Plant classification; Species classification; State of the art; Classification (of information)},
	publisher = {IEEE Computer Society},
	issn = {15224880},
	isbn = {978-150902175-8},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Image Process. ICIP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; Conference name: 24th IEEE International Conference on Image Processing, ICIP 2017; Conference date: 17 September 2017 through 20 September 2017; Conference code: 134723}
}

@ARTICLE{Taghavi Namin2018,
	author = {Taghavi Namin, Sarah and Esmaeilzadeh, Mohammad and Najafi, Mohammad and Brown, Tim B. and Borevitz, Justin O.},
	title = {Deep phenotyping: Deep learning for temporal phenotype/genotype classification},
	year = {2018},
	journal = {Plant Methods},
	volume = {14},
	number = {1},
	doi = {10.1186/s13007-018-0333-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051059434&doi=10.1186%2fs13007-018-0333-4&partnerID=40&md5=0920e024113e46d9947ce9196e18ae91},
	affiliations = {Australian National University, Research School of Biology, Canberra, Australia; Australian National University, Research School of Engineering and Computer Science, Canberra, Australia},
	abstract = {Background: High resolution and high throughput genotype to phenotype studies in plants are underway to accelerate breeding of climate ready crops. In the recent years, deep learning techniques and in particular Convolutional Neural Networks (CNNs), Recurrent Neural Networks and Long-Short Term Memories (LSTMs), have shown great success in visual data recognition, classification, and sequence learning tasks. More recently, CNNs have been used for plant classification and phenotyping, using individual static images of the plants. On the other hand, dynamic behavior of the plants as well as their growth has been an important phenotype for plant biologists, and this motivated us to study the potential of LSTMs in encoding these temporal information for the accession classification task, which is useful in automation of plant production and care. Methods: In this paper, we propose a CNN-LSTM framework for plant classification of various genotypes. Here, we exploit the power of deep CNNs for automatic joint feature and classifier learning, compared to using hand-crafted features. In addition, we leverage the potential of LSTMs to study the growth of the plants and their dynamic behaviors as important discriminative phenotypes for accession classification. Moreover, we collected a dataset of time-series image sequences of four accessions of Arabidopsis, captured in similar imaging conditions, which could be used as a standard benchmark by researchers in the field. We made this dataset publicly available. Conclusion: The results provide evidence of the benefits of our accession classification approach over using traditional hand-crafted image analysis features and other accession classification frameworks. We also demonstrate that utilizing temporal information using LSTMs can further improve the performance of the system. The proposed framework can be used in other applications such as in plant classification given the environment conditions or in distinguishing diseased plants from healthy ones. © 2018 The Author(s).},
	author_keywords = {Accession classification; Deep features; Deep learning; Temporal information},
	correspondence_address = {S. Taghavi Namin; Australian National University, Research School of Biology, Canberra, Australia; email: sarah.taghavi-namin@anu.edu.au},
	publisher = {BioMed Central Ltd.},
	issn = {17464811},
	language = {English},
	abbrev_source_title = {Plant Methods},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 108; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Franklin2018195,
	author = {Franklin, Steven E.},
	title = {Pixel-and object-based multispectral classification of forest tree species from small unmanned aerial vehicles},
	year = {2018},
	journal = {Journal of Unmanned Vehicle Systems},
	volume = {6},
	number = {4},
	pages = {195 – 211},
	doi = {10.1139/juvs-2017-0022},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065876510&doi=10.1139%2fjuvs-2017-0022&partnerID=40&md5=893001af2b89b5a1262db055476dceb4},
	affiliations = {School of Environment, Trent University, Peterborough, K9J 7B8, ON, Canada},
	abstract = {Forest inventory, monitoring, and assessment requires accurate tree species identification and mapping. Recent experiences with multispectral data from small fixed-wing and rotary blade unmanned aerial vehicles (UAVs) suggest a role for this technology in the emerging paradigm of enhanced forest inventory (EFI). In this paper, pixel-based and object-based image analysis (OBIA) methods were compared in UAV-based tree species classification of nine commercial tree species in mature eastern Ontario mixedwood forests. Unsupervised clustering and supervised classification of tree crown pixels yielded approximately 50%–60% classification accuracy overall; OBIA with image segmentation to delineate tree crowns and machine learning yielded up to 80% classification accuracy overall. Spectral response patterns and tree crown shape and geometric differences were interpreted in context of their ability to separate tree species of interest with these classification methods. Accuracy assessment was based on field-based forest inventory tree species identification. The paper provides a brief summary of future research issues that will influence the growth of this geomatics innovation in forest tree species classification and forest inventory. © Canadian Science Publishing. All rights reserved.},
	author_keywords = {Forest inventory; Multispectral imagery; Tree species classification},
	keywords = {Antennas; Fixed wings; Image segmentation; Pixels; Supervised learning; Unmanned aerial vehicles (UAV); Classification accuracy; Commercial tree species; Multispectral classification; Object based image analysis (OBIA); Small unmanned aerial vehicles; Spectral response patterns; Supervised classification; Tree species identifications; Forestry},
	correspondence_address = {S.E. Franklin; School of Environment, Trent University, Peterborough, K9J 7B8, Canada; email: sfranklin@trentu.ca},
	publisher = {Canadian Science Publishing},
	issn = {22913467},
	language = {English},
	abbrev_source_title = {J. Unmanned Veh. Sys.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32}
}

@CONFERENCE{Prasad20172722,
	author = {Prasad, Shitala and Singh, Pankaj P.},
	title = {Medicinal plant leaf information extraction using deep features},
	year = {2017},
	journal = {IEEE Region 10 Annual International Conference, Proceedings/TENCON},
	volume = {2017-December},
	pages = {2722 – 2726},
	doi = {10.1109/TENCON.2017.8228324},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044193869&doi=10.1109%2fTENCON.2017.8228324&partnerID=40&md5=dc3bb20ad6f075791b83b264f3d36cfa},
	affiliations = {Biometrics and Forensics Lab, NTU, Singapore, Singapore; Computer Science and Engineering, CIT, Kokrajahr, Assam, India},
	abstract = {In today's digital world of ubiquitous and Internet of thinks, medicinal plant identification is a challenging but very useful task in computer vision (CV) helping agro-community to recognize the unknown species more rapidly. The tchnological improvements in feature representation deep convolutional neural network (DCNN) is promisingly used in several applications like object recognitions, natural language processing and computer graphics. In this paper, we propose a knowledge transfer from object identification to plant species identification where the raw plant leaf image is represented into deep features. These deep features are experimentally proved to out-perform the state-of-the-art in plant species recognition. These paper presents a new and efficient technique for leaf acquisition. Secondly, the image is transformed to device independent laß color space that is further used to compute VGG-16 feature map. This feature map is re-projected to PCA subspace to optimize the performance for species recognition. To prove the robustness, the paper uses two different types of plant leaf datasets. © 2017 IEEE.},
	keywords = {Computer graphics; Knowledge management; Natural language processing systems; Neural networks; Object recognition; Plants (botany); Deep convolutional neural networks; Feature representation; Knowledge transfer; Medicinal plants; Object identification; Plant leaf images; Plant species identification; Species recognition; Deep neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21593442},
	isbn = {978-150901133-9},
	coden = {85QXA},
	language = {English},
	abbrev_source_title = {IEEE Reg 10 Annu Int Conf Proc TENCON},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; Conference name: 2017 IEEE Region 10 Conference, TENCON 2017; Conference date: 5 November 2017 through 8 November 2017; Conference code: 133992}
}

@ARTICLE{Zhou2018,
	author = {Zhou, Tan and Popescu, Sorin C. and Lawing, A. Michelle and Eriksson, Marian and Strimbu, Bogdan M. and Bürkner, Paul C.},
	title = {Bayesian and classical machine learning methods: A Comparison for tree species classification with LiDAR waveform signatures},
	year = {2018},
	journal = {Remote Sensing},
	volume = {10},
	number = {1},
	doi = {10.3390/rs10010039},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040699889&doi=10.3390%2frs10010039&partnerID=40&md5=a882bae04d7833eb9166a9455fd85216},
	affiliations = {LiDAR Applications for the Study of Ecosystems with Remote Sensing (LASERS) Laboratory, Department of Ecosystem Science and Management, Texas A and M University, College Station, 77843, TX, United States; Department of Ecosystem Science and Management, Texas A and M University, College Station, 77843, TX, United States; Department of Forest Engineering, Resources and Management, Oregon State University, Corvallis, 97331, OR, United States; Institute of Psychology, University of Münster, Münster, 48149, Germany},
	abstract = {A plethora of information contained in full-waveform (FW) Light Detection and Ranging (LiDAR) data offers prospects for characterizing vegetation structures. This study aims to investigate the capacity of FW LiDAR data alone for tree species identification through the integration of waveform metrics with machine learning methods and Bayesian inference. Specifically, we first conducted automatic tree segmentation based on the waveform-based canopy height model (CHM) using three approaches including TreeVaW, watershed algorithms and the combination of TreeVaW and watershed (TW) algorithms. Subsequently, the Random forests (RF) and Conditional inference forests (CF) models were employed to identify important tree-level waveform metrics derived from three distinct sources, such as raw waveforms, composite waveforms, the waveform-based point cloud and the combined variables from these three sources. Further, we discriminated tree (gray pine, blue oak, interior live oak) and shrub species through the RF, CF and Bayesian multinomial logistic regression (BMLR) using important waveform metrics identified in this study. Results of the tree segmentation demonstrated that the TW algorithms outperformed other algorithms for delineating individual tree crowns. The CF model overcomes waveform metrics selection bias caused by the RF model which favors correlated metrics and enhances the accuracy of subsequent classification. We also found that composite waveforms are more informative than raw waveforms and waveform-based point cloud for characterizing tree species in our study area. Both classical machine learning methods (the RF and CF) and the BMLR generated satisfactory average overall accuracy (74% for the RF, 77% for the CF and 81% for the BMLR) and the BMLR slightly outperformed the other two methods. However, these three methods suffered from low individual classification accuracy for the blue oak which is prone to being misclassified as the interior live oak due to the similar characteristics of blue oak and interior live oak. Uncertainty estimates from the BMLR method compensate for this downside by providing classification results in a probabilistic sense and rendering users with more confidence in interpreting and applying classification results to real-world tasks such as forest inventory. Overall, this study recommends the CF method for feature selection and suggests that BMLR could be a superior alternative to classical machining learning methods.},
	author_keywords = {Bayesian multinomial logistic regression; Composite waveform; Conditional inference forests; Decomposition; Random forests; Tree segmentation; Watershed; Waveform signatures},
	keywords = {Artificial intelligence; Barium compounds; Bayesian networks; Decision trees; Decomposition; Inference engines; Learning systems; Lithium compounds; Optical radar; Regression analysis; Uncertainty analysis; Watersheds; Conditional inference; Multinomial logistic regression; Random forests; Tree segmentation; Wave forms; Forestry},
	correspondence_address = {T. Zhou; LiDAR Applications for the Study of Ecosystems with Remote Sensing (LASERS) Laboratory, Department of Ecosystem Science and Management, Texas A and M University, College Station, 77843, United States; email: tankwin0@tamu.edu},
	publisher = {MDPI AG},
	issn = {20724292},
	language = {English},
	abbrev_source_title = {Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 25; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Dyrmann2018202,
	author = {Dyrmann, Mads and Christiansen, Peter and Midtiby, Henrik Skov},
	title = {Estimation of plant species by classifying plants and leaves in combination},
	year = {2018},
	journal = {Journal of Field Robotics},
	volume = {35},
	number = {2},
	pages = {202 – 212},
	doi = {10.1002/rob.21734},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021363427&doi=10.1002%2frob.21734&partnerID=40&md5=3351b6aab5812da789b76c766e866ce7},
	affiliations = {The Maersk Mc-Kinney Moller Institute, University of Southern Denmark, Odense, Denmark; Department of Engineering, Aarhus University, Aarhus, Denmark},
	abstract = {Information on which weed species are present within agricultural fields is a prerequisite when using robots for site-specific weed management. This study proposes a method of improving robustness in shape-based classifying of seedlings toward natural shape variations within each plant species. To do so, leaves are separated from plants and classified individually together with the classification of the whole plant. The classification is based on common, rotation-invariant features. Based on previous classifications of leaves and plants, confidence in correct assignment is created for the plants and leaves, and this confidence is used to determine the species of the plant. By using this approach, the classification accuracy of eight plants species at early growth stages is increased from 93.9% to 96.3%. © 2017 Wiley Periodicals, Inc.},
	author_keywords = {automated weed control; Bayes belief integration; classifier fusion; computer vision; excessive green; phenotyping; plant classification},
	keywords = {Computer vision; Green computing; Seed; Weed control; Agricultural fields; Classification accuracy; Classifier fusion; Excessive green; Phenotyping; Plant classification; Plants and leaves; Rotation invariant features; Plants (botany)},
	correspondence_address = {M. Dyrmann; The Maersk Mc-Kinney Moller Institute, University of Southern Denmark, Odense, Denmark; email: mady@mmmi.sdu.dk},
	publisher = {John Wiley and Sons Inc.},
	issn = {15564959},
	language = {English},
	abbrev_source_title = {J. Field. Rob.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12}
}

@ARTICLE{Isnanto20181920,
	author = {Isnanto, R. Rizal and Riyadi, Munawar Agus and Awaj, Muhammad Fahmi},
	title = {Herb leaves recognition using gray level co-occurrence matrix and five distance-based similarity measures},
	year = {2018},
	journal = {International Journal of Electrical and Computer Engineering},
	volume = {8},
	number = {3},
	pages = {1920 – 1932},
	doi = {10.11591/ijece.v8i3.pp1920-1932},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047217446&doi=10.11591%2fijece.v8i3.pp1920-1932&partnerID=40&md5=56d72bfc1bb07183069d8ff188094d30},
	affiliations = {Department of Computer Engineering, Diponegoro University, Jl. Prof. Soedarto, S.H., Tembalang, Semarang, 50275, Indonesia; Department of Electrical Engineering, Diponegoro University, Indonesia},
	abstract = {Herb medicinal products derived from plants have long been considered as an alternative option for treating various diseases. In this paper, the feature extraction method used is Gray Level Co-occurrence Matrix (GLCM), while for its recognition using the metric calculations of Chebyshev, Cityblock, Minkowski, Canberra, and Euclidean distances. The method of determining the GLCM Analysis based on the texture analysis resulting from the extraction of this feature is Angular Second Moment, Contrast, Inverse Different Moment, Entropy as well as its Correlation. The recognition system used 10 leaf test images with GLCM method and Canberra distance resulted in the highest accuracy of 92.00%. While the use of 20 and 30 test data resulted in a recognition rate of 50.67% and 60.00%. Copyright © 2018 Institute of Advanced Engineering and Science. All rights reserved.},
	author_keywords = {Canberra distance; Chebyshev distance; City-block distance; Euclidean distance; Gray-level cooccurrence matrix; Minkowski distance},
	correspondence_address = {R.R. Isnanto; Department of Computer Engineering, Diponegoro University, Tembalang, Semarang, Jl. Prof. Soedarto, S.H., 50275, Indonesia; email: rizal_isnanto@yahoo.com},
	publisher = {Institute of Advanced Engineering and Science},
	issn = {20888708},
	language = {English},
	abbrev_source_title = {Int. J. Electr. Comput. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Shah2018860,
	author = {Shah, Meet P. and Singha, Sougata and Awate, Suyash P.},
	title = {Leaf classification using marginalized shape context and shape+texture dual-path deep convolutional neural network},
	year = {2018},
	journal = {Proceedings - International Conference on Image Processing, ICIP},
	volume = {2017-September},
	pages = {860 – 864},
	doi = {10.1109/ICIP.2017.8296403},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045321691&doi=10.1109%2fICIP.2017.8296403&partnerID=40&md5=bd20261069165ba1c444f85576e7fe52},
	affiliations = {Electrical Engineering Department, Indian Institute of Technology (IIT) Bombay, India; Yodlee, India; Computer Science and Engineering Department, Indian Institute of Technology (IIT) Bombay, India},
	abstract = {Identifying plant species based on photographs of their leaves is an important problem in computer vision and biology. Previous approaches for leaf image classification typically rely on hand-crafted shape features or texture features. In contrast, we propose a dual-path deep convolutional neural network (CNN) to (i) learn joint feature representations for leaf images, exploiting their shape and texture characteristics, and (ii) optimize these features for the classification task. We compare our CNN approach against (i) vanilla CNN classifiers and (ii) popular hand-crafted shape features, including a novel shape-context based feature that is extremely computationally efficient, which we call the marginalized shape context. Our results on three large public datasets demonstrate that our dual-path CNN leads to higher accuracy and consistency than the state of the art. © 2017 IEEE.},
	author_keywords = {Dual-path deep convolutional neural net; Leaf recognition; Marginalized shape context; Shape; Texture},
	keywords = {Convolution; Image texture; Neural networks; Plants (botany); Tellurium compounds; Textures; Classification tasks; Computationally efficient; Deep convolutional neural networks; Dual path; Feature representation; Leaf recognition; Shape; Shape contexts; Deep neural networks},
	publisher = {IEEE Computer Society},
	issn = {15224880},
	isbn = {978-150902175-8},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Image Process. ICIP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28; Conference name: 24th IEEE International Conference on Image Processing, ICIP 2017; Conference date: 17 September 2017 through 20 September 2017; Conference code: 134723}
}

@ARTICLE{Kala2018119,
	author = {Kala, Jules R. and Viriri, Serestina},
	title = {Plant specie classification using sinuosity coefficients of leaves},
	year = {2018},
	journal = {Image Analysis and Stereology},
	volume = {37},
	number = {2},
	pages = {119 – 126},
	doi = {10.5566/ias.1821},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050126674&doi=10.5566%2fias.1821&partnerID=40&md5=9a9a4919b5c76b67ee48173744794661},
	affiliations = {School of Mathematics, Statistics and Computer Science, University of KwaZulu-Natal, Durban, 4000, South Africa},
	abstract = {Forests are the lungs of our planet. Conserving the plants may require the development of an automated system that will identify plants using leaf features such as shape, color, and texture. In this paper, a leaf shape descriptor based on sinuosity coefficients is proposed. The sinuosity coefficients are defined using the sinuosity measure, which is a measure expressing the degree of meandering of a curve. The initial empirical experiments performed on the LeafSnap dataset on the usage of four sinuosity coefficients to characterize the leaf images using the Radial Basis Function Neural Network (RBF) and Multilayer Perceptron (MLP) classifiers achieved accurate classification rates of 88% and 65%, respectively. The proposed feature extraction technique is further enhanced through the addition of leaf geometrical features, and the accurate classification rates of 93% and 82% were achieved using RBF and MLP, respectively. The overall results achieved showed that the proposed feature extraction technique based on the sinuosity coefficients of leaves, complemented with geometrical features improve the accuracy rate of plant classification using leaf recognition. © 2018, Slovenian Society for Stereology and Quantitative Image Analysis.},
	author_keywords = {Leaf recognition; Plant classification; Sinuosity coefficients; Sinuosity measure},
	correspondence_address = {J.R. Kala; School of Mathematics, Statistics and Computer Science, University of KwaZulu-Natal, Durban, 4000, South Africa; email: raymondkala1@gmail.com},
	publisher = {Slovenian Society For Stereology And Quantitative Image Analysis},
	issn = {15803139},
	language = {English},
	abbrev_source_title = {Image Anal. Stereol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Gold Open Access}
}

@CONFERENCE{Akila20181,
	author = {Akila, I.S. and Sivakumar, A. and Swaminathan, S.},
	title = {Automation in plant growth monitoring using high-precision image classification and virtual height measurement techniques},
	year = {2018},
	journal = {Proceedings of 2017 International Conference on Innovations in Information, Embedded and Communication Systems, ICIIECS 2017},
	volume = {2018-January},
	pages = {1 – 4},
	doi = {10.1109/ICIIECS.2017.8275862},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046935034&doi=10.1109%2fICIIECS.2017.8275862&partnerID=40&md5=d6133844520a6582879589a172cbaaa4},
	affiliations = {Department of Electronics and Communication Engineering, Coimbatore Institute of Technology, Coimbatore, India},
	abstract = {Agriculture is the backbone of our Indian economy and almost 80 percentage of our population depend on it, but it is becoming a strenuous practice due to irregular weather pattern and over usage of underground water. So there is a need for automatic monitoring and advisory system for increase in productivity. Image processing is a powerful technique that has now been widely used in various fields for solving complex problems and its application in the field of agriculture has been proved a useful contribution. Thus the proposed work aims at extracting plant color or texture using image processing technique and the resultant image is subjected to virtual height measurement scheme. Then plant height is verified for normal growth in comparison with the plant growth chart. If plant growth is not well, an advisory system may be created which can alert the user. © 2017 IEEE.},
	author_keywords = {agriculture; color extraction; Conditional Random Field Temporal Search (CRFT); plant identification; virtual height measurement},
	keywords = {Agriculture; Embedded systems; Groundwater; Image classification; Advisory systems; Automatic monitoring; Color extraction; Conditional random field; Height Measurement; Image processing technique; Plant identification; Weather patterns; Data communication systems},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150903294-5},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Innov. Inf., Embed. Commun. Syst., ICIIECS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 2017 International Conference on Innovations In Information, Embedded and Communication Systems, ICIIECS 2017; Conference date: 17 March 2017 through 18 March 2017; Conference code: 134503}
}

@ARTICLE{Rzanny2017,
	author = {Rzanny, Michael and Seeland, Marco and Wäldchen, Jana and Mäder, Patrick},
	title = {Acquiring and preprocessing leaf images for automated plant identification: Understanding the tradeoff between effort and information gain},
	year = {2017},
	journal = {Plant Methods},
	volume = {13},
	number = {1},
	doi = {10.1186/s13007-017-0245-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033381800&doi=10.1186%2fs13007-017-0245-8&partnerID=40&md5=1f5d9690b6ddf3d0e603fb00f358300f},
	affiliations = {Max-Planck-Institute for Biogeochemistry, Department Biogeochemical Integration, Hans-Knöll-Str. 10, Jena, 07745, Germany; Technische Universität Ilmenau, Institute for Computer and Systems Engineering, Helmholtzplatz 5, Ilmenau, 98693, Germany},
	abstract = {Background: Automated species identification is a long term research subject. Contrary to flowers and fruits, leaves are available throughout most of the year. Offering margin and texture to characterize a species, they are the most studied organ for automated identification. Substantially matured machine learning techniques generate the need for more training data (aka leaf images). Researchers as well as enthusiasts miss guidance on how to acquire suitable training images in an efficient way. Methods: In this paper, we systematically study nine image types and three preprocessing strategies. Image types vary in terms of in-situ image recording conditions: perspective, illumination, and background, while the preprocessing strategies compare non-preprocessed, cropped, and segmented images to each other. Per image type-preprocessing combination, we also quantify the manual effort required for their implementation. We extract image features using a convolutional neural network, classify species using the resulting feature vectors and discuss classification accuracy in relation to the required effort per combination. Results: The most effective, non-destructive way to record herbaceous leaves is to take an image of the leaf's top side. We yield the highest classification accuracy using destructive back light images, i.e., holding the plucked leaf against the sky for image acquisition. Cropping the image to the leaf's boundary substantially improves accuracy, while precise segmentation yields similar accuracy at a substantially higher effort. The permanent use or disuse of a flash light has negligible effects. Imaging the typically stronger textured backside of a leaf does not result in higher accuracy, but notably increases the acquisition cost. Conclusions: In conclusion, the way in which leaf images are acquired and preprocessed does have a substantial effect on the accuracy of the classifier trained on them. For the first time, this study provides a systematic guideline allowing researchers to spend available acquisition resources wisely while yielding the optimal classification accuracy. © 2017 The Author(s).},
	author_keywords = {Back light; Background; CNN; Computer vision; Cropping; Effort; Image acquisition; Leaf image; Leaf side; Preprocessing; Segmentation},
	correspondence_address = {J. Wäldchen; Max-Planck-Institute for Biogeochemistry, Department Biogeochemical Integration, Jena, Hans-Knöll-Str. 10, 07745, Germany; email: jwald@bgc-jena.mpg.de},
	publisher = {BioMed Central Ltd.},
	issn = {17464811},
	language = {English},
	abbrev_source_title = {Plant Methods},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 42; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Beikmohammadi201821,
	author = {Beikmohammadi, Ali and Faez, Karim},
	title = {Leaf Classification for Plant Recognition with Deep Transfer Learning},
	year = {2018},
	journal = {Proceedings - 2018 4th Iranian Conference of Signal Processing and Intelligent Systems, ICSPIS 2018},
	pages = {21 – 26},
	doi = {10.1109/ICSPIS.2018.8700547},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065617810&doi=10.1109%2fICSPIS.2018.8700547&partnerID=40&md5=703a4f9516729bb635710e6bb167e0c7},
	affiliations = {Department of Electrical Engineering, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran},
	abstract = {Plant recognition systems that developed by computer vision researchers, help botanists in faster recognition and detection of unknown plant species. Until now, multiple studies focused on the process or algorithms that maximize use of botanical datasets for plants prediction modeling, but this method depends on leaf characteristics which can be changed with botanical data and different feature extraction techniques. On the other hand, recently, due to the popularity and successful implementation of deep learning-based methods in various areas such as image classification, object detection, and speech recognition, the researchers directed from traditional feature-based methods to deep learning. In this research, one more efficient method presented that use transfer learning to recognize plant for leaf classification, which first uses a pre-trained deep neural network model for learning useful leaf characteristics directly from the input data representation. Then use a logistic regression classifier for leaf classification. It is seen that transfer learning from a large dataset to limited botanical dataset in plant recognition task is well done. The proposed method is evaluated on two well-known botanical datasets, i.e., Flavia with 32 classes and Leafsnap with 184 classes, and has succeeded in achieving an accuracy of 99.6% and 90.54%, respectively. The results show that despite the large change in the number of classes in these two datasets, the proposed method, have a good performance and show the better result than methods based on the hand-crafted feature and other methods based on the deep learning in terms of memory and precision. © 2018 IEEE.},
	author_keywords = {deep learning; leaf classification; MobileNet; plant recognition; transfer learning},
	keywords = {Deep learning; Feature extraction; Intelligent systems; Large dataset; Object detection; Object recognition; Speech recognition; Feature extraction techniques; Leaf classification; Learning-based methods; Logistic regression classifier; MobileNet; Neural network model; Plant recognition; Transfer learning; Deep neural networks},
	editor = {Taheri H.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172811194-0},
	language = {English},
	abbrev_source_title = {Proc. - Iranian Conf. Signal Process. Intell. Syst., ICSPIS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27; Conference name: 4th Iranian Conference of Signal Processing and Intelligent Systems, ICSPIS 2018; Conference date: 25 December 2018 through 27 December 2018; Conference code: 147670}
}

@CONFERENCE{Haupt2018,
	author = {Haupt, Josef and Kahl, Stefan and Kowerko, Danny and Eibl, Maximilian},
	title = {Large-scale plant classification using deep convolutional neural networks},
	year = {2018},
	journal = {CEUR Workshop Proceedings},
	volume = {2125},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051055760&partnerID=40&md5=66775f096ec60a0b899a085516edff70},
	affiliations = {Department of Media Informatics, Chemnitz University of Technology, Chemnitz, D-09107, Germany; Department of Media Computing, Chemnitz University of Technology, Chemnitz, D-09107, Germany},
	abstract = {Deep learning techniques have significantly improved plant species classification in recent years. The goal of the 2018 ExpertLife-CLEF challenge was to compare the performance of human experts to machines trained on the PlantCLEF 2017 dataset containing 10.000 classes. We used the Inception, ResNet and DenseNet architectures to solve this complex task. In our experiments, complex neural net layouts yield strong results, comparable to human performance. We further push the overall accuracy through iterative adjustment of class weights. An ensemble consisting of a ResNet50 and two DenseNet201 with fine-tuned class weights reached a topl-accuracy of 77% on the test set.},
	author_keywords = {Convolutional Neural Networks; Deep Learning; Plant Classification},
	keywords = {Complex networks; Convolution; Deep learning; Iterative methods; Neural networks; Complex task; Convolutional neural network; Deep convolutional neural networks; Human performance; Learning techniques; Overall accuracies; Plant classification; Plant species; Deep neural networks},
	editor = {Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Nie J.-Y. and Soulier L. and Soulier L. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 19th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2018; Conference date: 10 September 2018 through 14 September 2018; Conference code: 138100}
}

@ARTICLE{Patrón20181563,
	author = {Patrón, José Salgado and Sendoya-Losada, Diego and Robayo-Betancourt, Faiber},
	title = {Design and implementation of an algorithm for plants identification and classification based on physical characteristics of their leaves using computer vision},
	year = {2018},
	journal = {ARPN Journal of Engineering and Applied Sciences},
	volume = {13},
	number = {5},
	pages = {1563 – 1569},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044136598&partnerID=40&md5=e10abddaafa3f6e09255defa1ca3692a},
	affiliations = {Department of Electronic Engineering, Faculty of Engineering, Surcolombiana University, Neiva, Huila, Colombia},
	abstract = {In this paper, it's described an algorithm for plants identification based on leaves physical features using image processing techniques and the feed forward neural network specialized on patterns recognition. As fundamental step, the sobel operator is used to highlight the leaves boundaries and veins. After obtaining those features, the new image is transfer to the frequency space through the wavelet transform as basis for the main vector of every sample. Finally, the results are evaluated according to percentage of samples correctly identified. The algorithm is adapted to a visual interface that allows the user to observe the steps of the image processing and get the leaf information. © 2006-2018 Asian Research Publishing Network (ARPN).},
	author_keywords = {Computer vision; Feed forward net; Image processing; Leaves; Neural networks; Wavelet transform},
	correspondence_address = {J.S. Patrón; Department of Electronic Engineering, Faculty of Engineering, Surcolombiana University, Neiva, Huila, Colombia; email: josesalgadop@usco.edu.co},
	publisher = {Asian Research Publishing Network},
	issn = {18196608},
	language = {English},
	abbrev_source_title = {ARPN J. Eng. Appl. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lee2018169,
	author = {Lee, Sue Han and Chang, Yang Loong and Chan, Chee Seng and Alexis, Joly and Bonnet, Pierre and Goeau, Herve},
	title = {Plant classification based on gated recurrent unit},
	year = {2018},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {11018 LNCS},
	pages = {169 – 180},
	doi = {10.1007/978-3-319-98932-7_16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052830799&doi=10.1007%2f978-3-319-98932-7_16&partnerID=40&md5=6ad58b22b9155d701911510a1e021f66},
	affiliations = {Center of Image and Signal Processing, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; INRIA, Montpellier, France; CIRAD-Amap, Montpellier, France},
	abstract = {Classification of plants based on a multi-organ approach is very challenging due to the variability in shape and appearance in plant organs. Despite promising solutions built using convolutional neural network (CNN) for plant classification, the existing approaches do not consider the correspondence between different views captured of a plant. In fact, botanists usually observe and study simultaneously a plant from different vintage points, as a whole and also analyse different organs in order to disambiguate species. Driven by this insight, we introduce a new framework for plant structural learning using the recurrent neural network (RNN) approach. This novel approach supports classification based on a varying number of plant views composed of one or more organs of a plant, by optimizing the dependencies between them. We also present the qualitative results of our proposed models by visualizing the learned attention maps. To our knowledge, this is the first study to venture into such dependencies modeling and interpret the respective neural net for plant classification. Finally, we show that our proposed method outperforms the conventional CNN approach on the PlantClef2015 benchmark. The source code and models are available at https://github.com/cs-chan/Deep-Plant. © Springer Nature Switzerland AG 2018.},
	author_keywords = {Deep learning; Plant classification; Recurrent neural network},
	keywords = {Association reactions; Recurrent neural networks; Convolutional Neural Networks (CNN); Plant classification; Plant organs; Recurrent neural network (RNN); Source codes; Structural learning; Deep learning},
	correspondence_address = {S.H. Lee; Center of Image and Signal Processing, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; email: leesuehan@siswa.um.edu.my},
	editor = {SanJuan E. and Murtagh F. and Nie J.Y. and Soulier L. and Cappellato L. and Bellot P. and Mothe J. and Trabelsi C. and Ferro N.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331998931-0},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 9th International Conference of the CLEF Association, CLEF 2018; Conference date: 10 September 2018 through 14 September 2018; Conference code: 217239}
}

@CONFERENCE{Thanikkal2018404,
	author = {Thanikkal, Jibi G and Kumar Dubey, Ashwani and Thomas, M.T.},
	title = {Whether color, shape and texture of leaves are the key features for image processing based plant recognition? An analysis!},
	year = {2018},
	journal = {2017 Recent Developments in Control, Automation and Power Engineering, RDCAPE 2017},
	pages = {404 – 409},
	doi = {10.1109/RDCAPE.2017.8358305},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048056562&doi=10.1109%2fRDCAPE.2017.8358305&partnerID=40&md5=d31a8b8bf26ed587a31f46060463a2b2},
	affiliations = {Dept. of Computer Science and Engineering, Amity University Uttar Pradesh, Noida, India; Dept. of Botany, St. Thomas' College, Thrissur, Kerala, India},
	abstract = {Studies on plant identification through image processing consider shape, color and texture features of leafs. But botanist's uses leaf morphology, leaf arrangement, types of venation, leave shapes, leave bases, leaf margins and leaf apices for recognizing a plant. This paper introduces the leaf venation, leaf margin, leaf apies, and leaf bases models for improving plant leaf identification. These new features along with shape, color and feature increases the accuracy of the plant identification. © 2017 IEEE.},
	author_keywords = {Image processing; Leaf image; Morphological features; Plant recognition; Shape matching},
	keywords = {Color; Image processing; Image texture; Plants (botany); Power control; Color and texture features; Leaf images; Leaf morphology; Morphological features; Plant identification; Plant recognition; Shape and textures; Shape matching; Color image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150903978-4},
	language = {English},
	abbrev_source_title = {Recent Dev. Control, Autom. Power Eng., RDCAPE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; Conference name: 2017 Recent Developments in Control, Automation and Power Engineering, RDCAPE 2017; Conference date: 26 October 2017 through 27 October 2017; Conference code: 136445}
}

@CONFERENCE{Ali2018431,
	author = {Ali, Redha and Hardie, Russell and Essa, Almabrok},
	title = {A Leaf Recognition Approach to Plant Classification Using Machine Learning},
	year = {2018},
	journal = {Proceedings of the IEEE National Aerospace Electronics Conference, NAECON},
	volume = {2018-July},
	pages = {431 – 434},
	doi = {10.1109/NAECON.2018.8556785},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059915301&doi=10.1109%2fNAECON.2018.8556785&partnerID=40&md5=3a9241b779443c0a2d1a7fd682518955},
	affiliations = {Department of Electrical and Computer Engineering, University of Dayton, 300 College Park, Dayton, 45469, OH, United States; Department of Electrical Engineering and Computer Science, Cleveland State University, 2121 Euclid Ave, Cleveland, 44115, OH, United States},
	abstract = {The identification of plants is a very important component of workflows in plant ecological research. This paper presents an automated leaf recognition method for plant identification. The proposed technique is simple and computationally efficient. It is based on a combination of two types of texture features, named Bag-of-features (BOF) and Local Binary Pattern (LBP). These features are utilized as inputs to a decision-making model that is based on a multiclass Support Vector Machine (SVM) classifier. The introduced method is evaluated on a publicly available leaf image database. The experimental results demonstrate that our proposed method is the highly efficient technique for plant recognition. © 2018 IEEE.},
	author_keywords = {component; formatting; insert; style; styling},
	keywords = {Decision making; component; formatting; insert; style; styling; Support vector machines},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {05473578},
	isbn = {978-153866557-2},
	language = {English},
	abbrev_source_title = {Proc. IEEE Natl. Aerosp. Electron. Conf., NAECON},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; Conference name: 2018 IEEE National Aerospace and Electronics Conference, NAECON 2018; Conference date: 23 July 2018 through 26 July 2018; Conference code: 143466}
}

@CONFERENCE{Aráujo20171880,
	author = {Aráujo, Voncarlos and Britto, Alceu S. and Brun, André L. and Koerich, Alessandro L. and Falate, Rosane},
	title = {Multiple classifier system for plant leaf recognition},
	year = {2017},
	journal = {2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017},
	volume = {2017-January},
	pages = {1880 – 1885},
	doi = {10.1109/SMC.2017.8122891},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044191540&doi=10.1109%2fSMC.2017.8122891&partnerID=40&md5=db335b299f1d8609e1bcd1f429e3396b},
	affiliations = {Pontifical Catholic University of Parańa (PUCPR), Curitiba, PR, Brazil; Ecole de Technologie Supérieure (ÉTS), Montreal, QC, Canada; State University of Ponta Grossa (UEPG), Ponta Grossa, PR, Brazil},
	abstract = {This paper presents a multiple classifier system (MCS) to identify plants species based on the texture and shape features extracted from leaf images. A diverse pool of SVM and Neural Network classifiers is trained on four different feature sets, namely, Local Binary Pattern (LBP), Histogram of Gradients (HOG), Speed of Robust Features (SURF) and Zernike Moments (ZM). Then, a static classifier selection method is used to search for the ensembles that maximize the average classification score. Experimental results on ImageCLEF 2011 and 2012 datasets have shown that combining different kind of classifiers trained on shape and texture features is an effective strategy for the plant automatic identification. The MCS improves the identification performance in up to 28% relative to the monolithic approach. Furthermore, the proposed approach also compares favourably with the best results reported in the literature for those datasets. © 2017 IEEE.},
	keywords = {Automation; Cybernetics; Feature extraction; Automatic identification; Histogram of gradients (HOG); Local binary pattern (LBP); Monolithic approach; Multiple classifier systems; Neural network classifier; Shape and textures; Static classifier; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153861645-1},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Syst., Man, Cybern., SMC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017; Conference date: 5 October 2017 through 8 October 2017; Conference code: 133297}
}

@ARTICLE{Liu2018513,
	author = {Liu, Huajian and Lee, Sang-Heon and Chahl, Javaan Singh},
	title = {Registration of multispectral 3D points for plant inspection},
	year = {2018},
	journal = {Precision Agriculture},
	volume = {19},
	number = {3},
	pages = {513 – 536},
	doi = {10.1007/s11119-017-9536-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029589709&doi=10.1007%2fs11119-017-9536-3&partnerID=40&md5=f6275812167e5d014d41a6105848f5a1},
	affiliations = {School of Engineering, University of South Australia, Adelaide, SA, Australia; Joint and Operations Analysis Division, Defence Science and Technology Organisation, ACT, Australia},
	abstract = {Machine vision technologies have shown advantages for efficient and accurate plant inspection in precision agriculture. Regarding the balance between accuracy of inspection and compactness for infield applications, multispectral imaging systems would be more suitable than RGB colour cameras or hyperspectral imaging systems. Multispectral image registration (MIR) is a key issue for multispectral imaging systems, however, this task is challenging. First of all, in many cases, two images needing registration do not have a one-to-one linear mapping in 2D space and therefore they cannot be aligned in 2D images. Furthermore, the general MIR algorithms are limited to images with uniform intensity and are incapable of registering images with rich features. This study developed a machine vision system (MVS) and a MIR method which replaces 2D-2D image registration by 3D-3D point cloud registration. The system can register 3D point clouds of ultraviolet (UV), blue, green, red and near-infrared (NIR) spectra in 3D space. It was found that the point clouds of general plants created by images of different spectral bands have a complementary property, and therefore a combined point cloud, called multispectral 3D point cloud, is denser than any cloud created by a single spectral band. Intensity information of each spectral band is available in a multispectral 3D point cloud and therefore image fusion and 3D morphological analysis can be conducted in the cloud. The MVS could be used as a sensor of a robotic system to fulfil on-the-go infield plant inspection tasks. © 2017, Springer Science+Business Media, LLC.},
	author_keywords = {3D point registration; 3D reconstruction; Computer vision; Image fusion; Machine vision; Multimodal image registration; Multispectral image processing; Multispectral image registration; Plant identification},
	keywords = {algorithm; computer vision; image analysis; image processing; machine learning; multispectral image; plant; precision agriculture; sensor; two-dimensional modeling},
	correspondence_address = {H. Liu; School of Engineering, University of South Australia, Adelaide, Australia; email: huajian.liu@mymail.unisa.edu.au},
	publisher = {Springer New York LLC},
	issn = {13852256},
	coden = {PREAF},
	language = {English},
	abbrev_source_title = {Precis. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@CONFERENCE{Pankaja20181190,
	author = {Pankaja, K. and Suma, V.},
	title = {Leaf Recognition and Classification Using GLCM and Hierarchical Centroid Based Technique},
	year = {2018},
	journal = {Proceedings of the International Conference on Inventive Research in Computing Applications, ICIRCA 2018},
	pages = {1190 – 1194},
	doi = {10.1109/ICIRCA.2018.8597184},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061507466&doi=10.1109%2fICIRCA.2018.8597184&partnerID=40&md5=dcae359f00a4c7d44561d4fb33209b85},
	affiliations = {Dept. of Computer Science and Engineering, CITECH, VTU, Bengaluru, India; Dept of Information Science and Engineering, DSCE, VTU, Bengaluru, India},
	abstract = {Variety of plants exists in earth's ecology where each plant species has its own unique features. Due to their immense benefits to mankind, many plant species are used in day to day life. Therefore, accurate plant leaf recognition through computer vision methods has paved its way to several fields like ayurvedic and diagnosis of health issues. Technology has always played a vital role in all aspects of human development. Achieving accurate recognition and categorization of plant leaf is always a challenge to researchers. This paper, therefore aims to put forth various techniques that are adopted for pre-processing, feature extraction and classification of leaf, based on shape and texture features of leaf sample. The paper further presents experimental results carried out on flavia dataset in order to recognize and classify leaves using gray level co-occurrence matrix and hierarchical centroid based technique. This work has considered 300 leaf samples with 30 different classes for the purpose of investigation. Experimental results indicate that from the aforementioned technique, it is possible to attain accuracy up to 96.66%. © 2018 IEEE.},
	author_keywords = {Feature Extraction Techniques; Flavia Dataset; Image Processing; PCA Classifier; Preprocessing},
	keywords = {Extraction; Feature extraction; Image processing; Plants (botany); Textures; Feature extraction and classification; Feature extraction techniques; Flavia Dataset; Gray level co-occurrence matrix; Human development; Leaf recognition; Preprocessing; Shape and textures; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153862456-2},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Inven. Res. Comput. Appl., ICIRCA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 2018 International Conference on Inventive Research in Computing Applications, ICIRCA 2018; Conference date: 11 July 2018 through 12 July 2018; Conference code: 144196}
}

@ARTICLE{Kganyago20175608,
	author = {Kganyago, Mahlatse and Odindi, John and Adjorlolo, Clement and Mhangara, Paidamoyo},
	title = {Selecting a subset of spectral bands for mapping invasive alien plants: A case of discriminating parthenium hysterophorus using field spectroscopy data},
	year = {2017},
	journal = {International Journal of Remote Sensing},
	volume = {38},
	number = {20},
	pages = {5608 – 5625},
	doi = {10.1080/01431161.2017.1343510},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044250281&doi=10.1080%2f01431161.2017.1343510&partnerID=40&md5=8061de2db145a97f84ccc877f2e613ec},
	affiliations = {School of Agricultural, Earth and Environmental Sciences, University of KwaZulu-Natal, Pietermaritzburg, South Africa; South African National Space Agency, Earth Observations, Pretoria, South Africa},
	abstract = {Parthenium hysterophorus is considered one of the top seven most problematic and devastating weeds in the world. It compromises the integrity of ecosystems, human health, agricultural production, and biodiversity. Therefore, its early detection and discrimination are critical for facilitating site-specific weed management. Recently, adoption of remote-sensing approaches has gained popularity for species-level mapping of vegetation. Specifically, the use of hyperspectral data has demonstrated reliable mapping accuracy. However, when working with hyperspectral data, feature selection is fundamental to achieving reliable classification accuracies. Moreover, challenges such as ‘the curse of dimensionality’ that cause unstable parameter estimates and high generalization errors when the number of observations (n) is less than the number of descriptive variables (p), i.e. n < p often compromise classification accuracy. In this study, we assessed the potential of a hybrid feature selection approach, based on statistical analysis and Support Vector Machines – Recursive Feature Elimination (SVM-RFE) for determining a subset of hyperspectral bands relevant for discriminating P. hysterophorus using field spectroscopy data. We compared the performance of SVM-RFE, Random Forest variable importance (RF VarImp), and entire spectral dat aset (p =1633) using SVM classifier with radial basis function (RBF) kernel. Results of SVM-RFE and RF VarImp generated lower classification accuracies (i.e. 76.19% and 66.67%, respectively) than the entire spectral data set, i.e. 78.57%. On the other hand, using a subset of 10 spectral bands, our hybrid approach yielded a superior overall accuracy of 80.19% in discriminating P. hysterophorus from its co-occurring species. The study showed that a subset consisting of two red-edge bands located at 685 and 707 nm, one near infrared band at 1115 nm, and seven short wave infrared bands at 1971, 1982, 1990, 1966, 2003, 2005, and 2013 nm had the greatest potential for discrimination of P. hysterophorus and co-occurring plant groups. Overall, the study suggests that the hybrid approach is effective for early detection and improvement of invasive alien plants classification accuracy, reducing data dimensionality and selecting a relevant spectral subset of bands. © 2017 Informa UK Limited, trading as Taylor & Francis Group.},
	keywords = {Parthenium hysterophorus; Agricultural machinery; Agriculture; Biodiversity; Classification (of information); Decision trees; Feature extraction; Infrared devices; Infrared radiation; Radial basis function networks; Remote sensing; Set theory; Support vector machines; Agricultural productions; Curse of dimensionality; Detection and discriminations; Hybrid feature selections; Radial Basis Function(RBF); Recursive feature elimination; Remote sensing approaches; Short wave infrared bands; accuracy assessment; angiosperm; image classification; introduced species; performance assessment; reliability analysis; spectral analysis; support vector machine; vegetation mapping; weed; Photomapping},
	correspondence_address = {M. Kganyago; School of Agricultural, Earth and Environmental Sciences, University of KwaZulu-Natal, Pietermaritzburg, P. Bag X01, Scottsville, 3209, South Africa; email: kganyagoml@gmail.com},
	publisher = {Taylor and Francis Ltd.},
	issn = {01431161},
	coden = {IJSED},
	language = {English},
	abbrev_source_title = {Int. J. Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17}
}

@ARTICLE{Qu2018141,
	author = {Qu, Yuan and Chen, Yixiang and Chen, Wenjie},
	title = {Co-design and implementation of image recognition based on ARM and FPGA},
	year = {2018},
	journal = {Communications in Computer and Information Science},
	volume = {857},
	pages = {141 – 153},
	doi = {10.1007/978-981-13-1026-3_11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050259653&doi=10.1007%2f978-981-13-1026-3_11&partnerID=40&md5=2efaa6c99a33ff24b5f3824de66d5463},
	affiliations = {School of Computer Science and Software Engineering, MOE Research Center for Software, Hardware Co-Design Engineering, East China Normal University, Shanghai, 200062, China},
	abstract = {With the development of the Internet of things, the image recognition system is widely required in many fields. It has very high requirement in real-time, but usually it has high complexity and large data. So the real-time, which improved by hardware acceleration, is the key of image recognition system. Traditional processors have the disadvantages of low flexibility and configurability for prototype of embedded system. The family of Xilinx Zynq7000 processors integrate dual-core ARM Cortext-A9 and low-power FPGA. It can improve the operating efficiency and dynamical configurability of developing image applications. It also can reduce the power consumption of image processing. In this paper, we present an ARM and FPGA Co-design architecture of image recognition system based on Zynq7000 processor. Then we validate this architecture by the leaf recognition system. This architecture is based on module designed at system-level and modeled at algorithm-level. After determining the algorithm option, we partite the ARM and FPGA of modules depending on algorithm simulation, and then implement them separately. Finally, ARM and FPGA modules are interconnected by interface or driver. When the joint debugging is completed, prototype development of the embedded application is finished. As the experiment shown, FPGA and ARM co-design is 1.84 times faster than the pure ARM. © 2018, Springer Nature Singapore Pte Ltd.},
	author_keywords = {ARM + FPGA; Co-design and implementation; Image recognition},
	keywords = {Computer architecture; Embedded systems; Field programmable gate arrays (FPGA); Image enhancement; Image recognition; Integrated circuit design; Program debugging; Software prototyping; Algorithm simulation; Co-designs; Embedded application; Hardware acceleration; Image applications; Image recognition system; Operating efficiency; Prototype development; ARM processors},
	correspondence_address = {Y. Chen; School of Computer Science and Software Engineering, MOE Research Center for Software, Hardware Co-Design Engineering, East China Normal University, Shanghai, 200062, China; email: yxchen@sei.ecnu.edu.cn},
	editor = {Bi Y. and Chen G. and Deng Q. and Wang Y.},
	publisher = {Springer Verlag},
	issn = {18650929},
	isbn = {978-981131025-6},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 15th National Conference on Embedded Systems Technology, ESTC 2017; Conference date: 17 November 2017 through 19 November 2017; Conference code: 216229}
}

@ARTICLE{Zhu201829779,
	author = {Zhu, Heyan and Liu, Qinglin and Qi, Yuankai and Huang, Xinyuan and Jiang, Feng and Zhang, Shengping},
	title = {Plant identification based on very deep convolutional neural networks},
	year = {2018},
	journal = {Multimedia Tools and Applications},
	volume = {77},
	number = {22},
	pages = {29779 – 29797},
	doi = {10.1007/s11042-017-5578-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054523161&doi=10.1007%2fs11042-017-5578-9&partnerID=40&md5=5a6e53b6109e82aa2e958440d5fdec41},
	affiliations = {School of Information, Beijing Forestry University, Beijing, China; School of Opto-Electronic Information, Yantai University, Yantai, China; School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Institute of Animation and digital art, Communication University of China, Beijing, China},
	abstract = {Plant identification is a critical step in protecting plant diversity. However, many existing identification systems prohibitively rely on hand-crafted features for plant species identification. In this paper, a deep learning method is employed to extract discriminative features from plant images along with a linear SVM for plant identification. To offer a self-learning feature representation for different plant organs, we choose a very deep convolutional neural networks (CNNs), which consists of sixteen convolutional layers followed by three Fully-Connected (FC) layers and a final soft-max layer. Five max-pooling layers are performed over a 2×2 pixel window with stride 2. Extensive experiments on several plant datasets demonstrate the remarkable performance of the very deep neural network compared to the hand-crafted features. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {CNN; Linear SVM; Plant identification},
	keywords = {Convolution; Neural networks; Deep convolutional neural networks; Discriminative features; Feature representation; Learning methods; Linear SVM; Plant diversity; Plant identification; Plant species identification; Deep neural networks},
	correspondence_address = {S. Zhang; School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China; email: s.zhang@hit.edu.cn},
	publisher = {Springer New York LLC},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 35}
}

@CONFERENCE{Boudra20181530,
	author = {Boudra, Safia and Yahiaoui, Itheri and Behloul, Ali},
	title = {Plant identification from bark: A texture description based on Statistical Macro Binary Pattern},
	year = {2018},
	journal = {Proceedings - International Conference on Pattern Recognition},
	volume = {2018-August},
	pages = {1530 – 1535},
	doi = {10.1109/ICPR.2018.8545798},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059775154&doi=10.1109%2fICPR.2018.8545798&partnerID=40&md5=b23ec6398722936c1f2b3f00fc107582},
	affiliations = {LaSTIC, University of Batna 2, Batna, Algeria; CReSTIC, Université de Reims Champagne-Ardenne, Reims, France},
	abstract = {This paper presents a novel, yet compact texture descriptor for plant species identification based on bark texture images. Termed Statistical Macro Binary Pattern (SMBP), the descriptor is informative, rotation invariant, and it is designed to encode texture information from a large support area. The main novelty of this approach is the use of statistical description to represent the intensity distribution in the large support area, and an LBP-like encoding scheme to derive a statistical macro pattern by thresholding it against its adaptive statistical prototype. We propose to test three neighborhood sampling schemes according to the angular quantization at each level of the macrostructure. The comprehensive experiments on three challenging bark datasets (BarkTex, Trunk12, AFF) show that our descriptor achieves high and more consistent identification rates when compared with LBP-like texture descriptors. © 2018 IEEE.},
	keywords = {Encoding (symbols); Statistics; Identification rates; Intensity distribution; Plant identification; Plant species identification; Statistical descriptions; Texture description; Texture descriptors; Texture information; Pattern recognition},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {10514651},
	isbn = {978-153863788-3},
	coden = {PICRE},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Pattern Recognit.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 24th International Conference on Pattern Recognition, ICPR 2018; Conference date: 20 August 2018 through 24 August 2018; Conference code: 143085}
}

@ARTICLE{Lottes20182870,
	author = {Lottes, Philipp and Behley, Jens and Milioto, Andres and Stachniss, Cyrill},
	title = {Fully convolutional networks with sequential information for robust crop and weed detection in precision farming},
	year = {2018},
	journal = {IEEE Robotics and Automation Letters},
	volume = {3},
	number = {4},
	pages = {2870 – 2877},
	doi = {10.1109/LRA.2018.2846289},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053624349&doi=10.1109%2fLRA.2018.2846289&partnerID=40&md5=183a86615c5e08d61f6ec5b5fd141e29},
	affiliations = {Institute of Geodesy and Geoinformation, University of Bonn, Bonn, 53113, Germany},
	abstract = {Reducing the use of agrochemicals is an important component toward sustainable agriculture. Robots that can perform targeted weed control offer the potential to contribute to this goal, for example, through specialized weeding actions such as selective spraying or mechanical weed removal. A prerequisite of such systems is a reliable and robust plant classification system that is able to distinguish crop and weed in the field. A major challenge in this context is the fact that different fields show a large variability. Thus, classification systems have to robustly cope with substantial environmental changes with respect to weed pressure and weed types, growth stages of the crop, visual appearance, and soil conditions. In this letter, we propose a novel crop-weed classification system that relies on a fully convolutional network with an encoder-decoder structure and incorporates spatial information by considering image sequences. Exploiting the crop arrangement information that is observable from the image sequences enables our system to robustly estimate a pixel-wise labeling of the images into crop and weed, i.e., a semantic segmentation. We provide a thorough experimental evaluation, which shows that our system generalizes well to previously unseen fields under varying environmental conditions - a key capability to actually use such systems in precision framing. We provide comparisons to other state-of-the-art approaches and show that our system substantially improves the accuracy of crop-weed classification without requiring a retraining of the model. © 2016 IEEE.},
	author_keywords = {Deep learning in robotics and automation; robotics in agriculture and forestry},
	keywords = {Agricultural chemicals; Classification (of information); Convolution; Crops; Deep learning; Forestry; Image segmentation; Petroleum reservoir evaluation; Robotics; Semantics; Classification system; Convolutional networks; Environmental conditions; Experimental evaluation; Semantic segmentation; Sequential information; State-of-the-art approach; Sustainable agriculture; Weed control},
	correspondence_address = {P. Lottes; Institute of Geodesy and Geoinformation, University of Bonn, Bonn, 53113, Germany; email: philipp.lottes@igg.uni-bonn.de},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {23773766},
	language = {English},
	abbrev_source_title = {IEEE Robot. Autom.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 154; All Open Access, Green Open Access}
}

@ARTICLE{Li2018596,
	author = {Li, Ji and Tang, Lie},
	title = {Crop recognition under weedy conditions based on 3D imaging for robotic weed control},
	year = {2018},
	journal = {Journal of Field Robotics},
	volume = {35},
	number = {4},
	pages = {596 – 611},
	doi = {10.1002/rob.21763},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048107836&doi=10.1002%2frob.21763&partnerID=40&md5=f7274de3db3462cf1ce94844edee51dc},
	affiliations = {Agricultural and Biosystem Engineering, Iowa State University, Ames, IA, United States},
	abstract = {A 3D time-of-flight camera was applied to develop a crop plant recognition system for broccoli and green bean plants under weedy conditions. The developed system overcame the previously unsolved problems caused by occluded canopy and illumination variation. An efficient noise filter was developed to remove the sparse noise points in 3D point cloud space. Both 2D and 3D features including the gradient of amplitude and depth image, surface curvature, amplitude percentile index, normal direction, and neighbor point count in 3D space were extracted and found effective for recognizing these two types of plants. Separate segmentation algorithms were developed for each of the broccoli and green bean plant in accordance with their 3D geometry and 2D amplitude characteristics. Under the experimental condition where the crops were heavily infested by various types of weed plants, detection rates over 88.3% and 91.2% were achieved for broccoli and green bean plant leaves, respectively. Additionally, the crop plants were segmented out with nearly complete shape. Moreover, the algorithms were computationally optimized, resulting in an image processing speed of over 30 frames per second. © 2017 Wiley Periodicals, Inc.},
	author_keywords = {3D point cloud; machine vision; plant recognition; robotic weed control},
	keywords = {Computer vision; Crops; Image segmentation; Robotics; Weed control; 3D point cloud; Amplitude characteristics; Experimental conditions; Illumination variation; Plant recognition; Segmentation algorithms; Surface curvatures; Time-of-flight cameras; Plants (botany)},
	correspondence_address = {L. Tang; Agricultural and Biosystem Engineering, Iowa State University, Ames, United States; email: lietang@iastate.edu},
	publisher = {John Wiley and Sons Inc.},
	issn = {15564959},
	language = {English},
	abbrev_source_title = {J. Field. Rob.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17}
}

@ARTICLE{Almogdady2018166,
	author = {Almogdady, Huthaifa and Manaseer, Saher and Hiary, Hazem},
	title = {A flower recognition system based on image processing and neural networks},
	year = {2018},
	journal = {International Journal of Scientific and Technology Research},
	volume = {7},
	number = {11},
	pages = {166 – 173},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059870758&partnerID=40&md5=a3944382c874307d4fa5b25290fab06f},
	abstract = {Recognition is one of computer vision high level processing, the recognition process is mainly based on classifying object by obtaining and analyzing their main distinguishable features. In this paper and as a benchmark dataset we have used Oxford 102 flowers dataset, as it consists of 8189 flowers images that belong to 102 flower species, each species contains 40 to 251 images that has been gathered using internet searching or directly from photographers. we are introducing a flower recognition system for the Oxford 102 flowers dataset using image processing techniques, combined with Artificial neural networks (ANN), based on our proposed methodology, this paper will be divided into 4 main steps; starting with image enhancement, cropping of images used to modify dataset images to create more suitable dataset for next stage. Then image segmentation introduced to separate the foreground (the flower object) from the background (rest of image) where chan-vese active contour has been used, and for the features extraction, all of color, texture and shape have been used, (HSV color descriptor, Gray Level Co-occurrence Matrix (GLCM) as texture descriptor, and Invariant Moments (IM) as a shape descriptor). Finally; the classification process where Back-Propagation Artificial Neural Network (ANN) used. We have achieved (81.19%) as an accuracy rate. © 2018, International Journal of Scientific and Technology Research. All rights reserved.},
	author_keywords = {(RSU); GPS; Location; VANET},
	publisher = {International Journal of Scientific and Technology Research},
	issn = {22778616},
	language = {English},
	abbrev_source_title = {Int. J. Sci. Technol. Res.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17}
}

@CONFERENCE{Imamoglu2018162,
	author = {Imamoglu, Nevrez and Martínez-Gómez, Pascual and Hamaguchi, Ryuhei and Sakurada, Ken and Nakamura, Ryosuke},
	title = {Exploring recurrent and feedback CNNs for multi-spectral satellite image classification},
	year = {2018},
	journal = {Procedia Computer Science},
	volume = {140},
	pages = {162 – 169},
	doi = {10.1016/j.procs.2018.10.325},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061965475&doi=10.1016%2fj.procs.2018.10.325&partnerID=40&md5=8becc355cebcac7a31fe2f2d32a8782f},
	affiliations = {Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; Amazon, Seattle, United States},
	abstract = {The emergence of deep learning applications such as convolutional neural networks (CNNs) have resulted in huge improvements on computer vision applications in a wide variety of fields. However, several works demonstrated that low-quality or noisy data (even including perceptually not visible noises) may have a huge impact on the accuracy of CNN models. Therefore, as inspired by biological perception systems, some recent works proposed the use of recurrent and feedback features in CNNs as an improvement to the existing feed-forward CNNs. These recent works on the integration of recurrence and/or feedback to CNNs mostly tested deep networks on natural scenes with relatively perceptually good resolution color images. In this work, we explored the effectiveness of CNNs with recurrent and feedback features for the solar-power plant classification task on mid-resolution (1 pixel - 30×30 square meters per pixel) multi-spectral satellite images. Experiments show promising results when using top-down signals (especially recurrent and feedback features together) on CNNs for multi-spectral image classification tasks, outperforming the baseline CNN model without any recurrent and feedback structure and other approaches in the literature including deep models. © 2018 The Authors. Published by Elsevier B.V.},
	author_keywords = {Classification; CNNs with Recurrent; Convolutional Neural Network; Feedback; Multi-Spectral Images; Remote Sensing},
	keywords = {Adaptive systems; Classification (of information); Complex networks; Convolution; Deep learning; Embedded systems; Feedback; Neural networks; Pixels; Remote sensing; Solar energy; Solar power plants; Solar power satellites; Spectroscopy; CNNs with Recurrent; Computer vision applications; Convolutional neural network; Multispectral image classification; Multispectral images; Multispectral satellite image; Perception systems; Plant classification; Image classification},
	correspondence_address = {N. Imamoglu; Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; email: nevrez.imamoglu@aist.go.jp},
	publisher = {Elsevier B.V.},
	issn = {18770509},
	language = {English},
	abbrev_source_title = {Procedia Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: Complex Adaptive Systems Conference with Theme: Cyber Physical Systems and Deep Learning, CAS 2018; Conference date: 5 November 2018 through 7 November 2018; Conference code: 140798; All Open Access, Gold Open Access}
}

@CONFERENCE{Elnemr201891,
	author = {Elnemr, Heba A.},
	title = {Feature selection for texture-based plant leaves classification},
	year = {2018},
	journal = {ACCS/PEIT 2017 - 2017 Intl Conf on Advanced Control Circuits Systems and 2017 Intl Conf on New Paradigms in Electronics and Information Technology},
	volume = {2018-February},
	pages = {91 – 96},
	doi = {10.1109/ACCS-PEIT.2017.8303025},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050663712&doi=10.1109%2fACCS-PEIT.2017.8303025&partnerID=40&md5=2101d680443558bcfec0e986c8ca6d9a},
	affiliations = {Computers and Systems Department, Electronics Research Institute, Giza, Egypt},
	abstract = {Automatic identification of plant species is an essentialfield of research that is required in several areas. Plant leaves recognition plays an influential part in plant identification due to leaves obtainability andstationary features. In this paper, an efficient automatic leave identification system based on texture features is presented. The proposed system is based on several steps. First, the leaf image is pre-processed to remove the noise, enhance the image appearance and extract the region of interest (ROI). Next, texture features, including curvelet transform descriptors (CTD), local binary pattern (LBP) and grey level co-occurrence matrix (GLCM) texture features, are extracted and normalized. Afterward, the Neighbourhood Component Feature Selection (NCFS) method is performed to reduce the feature space as well as select the significant features that are capable ofseparatingdifferent leaves classes. Finally, the selected features are fed to k-Nearest Neighbour (k-NN) classifier to categorize the plant leaves. A combination of 78 CTD, 17 GLCM texture features, and 59 LBP, with a total of 154, are reduced to 16 significant features using NCFS. The proposed technique is tested and evaluated on Flavia Leaves dataset. The selected 16 features achieved an accuracy of 98% using K-NN classifier. The system is evaluated using K-fold cross-validation method. © 2017 IEEE.},
	author_keywords = {Curvelet Transform; Grayscale Co-Occurrence Matrix; Leaf Classification; Local Binary Pattern; Neighborhood Component Feature Selection},
	keywords = {Automation; Image enhancement; Image segmentation; Nearest neighbor search; Plants (botany); Co-occurrence-matrix; Curve-let transforms; Grey-level co-occurrence matrixes; K nearest neighbours (k-NN); Leaf classification; Local binary patterns; Plant leaves classifications; The region of interest (ROI); Feature extraction},
	correspondence_address = {H.A. Elnemr; Computers and Systems Department, Electronics Research Institute, Giza, Egypt; email: heba@eri.sci.eg},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153866407-0},
	language = {English},
	abbrev_source_title = {ACCS/PEIT - Intl Conf Adv. Control Circuits Syst. Intl Conf New Paradig. Electron. Inf. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 2017 Intl Conf on Advanced Control Circuits Systems and 2017 Intl Conf on New Paradigms in Electronics and Information Technology, ACCS/PEIT 2017; Conference date: 5 November 2017 through 8 November 2017; Conference code: 135013}
}

@ARTICLE{Leibman-Markus20182313,
	author = {Leibman-Markus, Meirav and Pizarro, Lorena and Schuster, Silvia and Lin, Z.J. Daniel and Gershony, Ofir and Bar, Maya and Coaker, Gitta and Avni, Adi},
	title = {The intracellular nucleotide-binding leucine-rich repeat receptor (SlNRC4a) enhances immune signalling elicited by extracellular perception},
	year = {2018},
	journal = {Plant Cell and Environment},
	volume = {41},
	number = {10},
	pages = {2313 – 2327},
	doi = {10.1111/pce.13347},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050392811&doi=10.1111%2fpce.13347&partnerID=40&md5=a17c8edaee605476a56aeb29060c80bd},
	affiliations = {School of Plant Sciences and Food Security, Tel Aviv University, Tel Aviv, Israel; Department of Plant Pathology, University of California, Davis, CA, United States; Department of Plant Pathology and Weed Research ARO, The Volcani Center, Rishon LeZion, Israel; Donald Danforth Plant Science Center, Saint Louis, MO, United States},
	abstract = {Plant recognition and defence against pathogens employs a two-tiered perception system. Surface-localized pattern recognition receptors (PRRs) act to recognize microbial features, whereas intracellular nucleotide-binding leucine-rich repeat receptors (NLRs) directly or indirectly recognize pathogen effectors inside host cells. Employing the tomato PRR LeEIX2/EIX model system, we explored the molecular mechanism of signalling pathways. We identified an NLR that can associate with LeEIX2, termed SlNRC4a (NB-LRR required for hypersensitive response-associated cell death-4). Co-immunoprecipitation demonstrates that SlNRC4a is able to associate with different PRRs. Physiological assays with specific elicitors revealed that SlNRC4a generally alters PRR-mediated responses. SlNRC4a overexpression enhances defence responses, whereas silencing SlNRC4 reduces plant immunity. Moreover, the coiled-coil domain of SlNRC4a is able to associate with LeEIX2 and is sufficient to enhance responses upon EIX perception. On the basis of these findings, we propose that SlNRC4a acts as a noncanonical positive regulator of immunity mediated by diverse PRRs. Thus, SlNRC4a could link both intracellular and extracellular immune perceptions. © 2018 John Wiley & Sons Ltd},
	author_keywords = {EIX; endocytosis; immune receptors; LeEIX2; NB-LRR; NLR; pattern triggered immunity; SlNRC4},
	keywords = {Blotting, Western; CRISPR-Associated Protein 9; CRISPR-Cas Systems; Ethylenes; Gene Editing; Immunoprecipitation; Lycopersicon esculentum; Mass Spectrometry; Microscopy, Confocal; NLR Proteins; Plant Growth Regulators; Plant Immunity; Plant Proteins; Plants, Genetically Modified; Polymerase Chain Reaction; Reactive Oxygen Species; Receptors, Pattern Recognition; Signal Transduction; Lycopersicon esculentum; ethylene; ethylene derivative; nucleotide binding oligomerization domain like receptor; pattern recognition receptor; phytohormone; plant protein; reactive oxygen metabolite; amino acid; cell; chemical binding; defense mechanism; gene expression; immune response; immunity; molecular analysis; pathogen; recognition; confocal microscopy; CRISPR Cas system; gene editing; immunology; immunoprecipitation; mass spectrometry; metabolism; physiology; plant immunity; polymerase chain reaction; signal transduction; tomato; transgenic plant; Western blotting},
	correspondence_address = {A. Avni; School of Plant Sciences and Food Security, Tel Aviv University, Tel Aviv, Israel; email: lpavni@post.tau.ac.il},
	publisher = {Blackwell Publishing Ltd},
	issn = {01407791},
	coden = {PLCED},
	pmid = {29790585},
	language = {English},
	abbrev_source_title = {Plant Cell Environ.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26; All Open Access, Green Open Access}
}

@CONFERENCE{Kwok201887,
	author = {Kwok, Jessica and Sun, Yu},
	title = {A smart IoT-based irrigation system with automated plant recognition using deep learning},
	year = {2018},
	journal = {ACM International Conference Proceeding Series},
	pages = {87 – 91},
	doi = {10.1145/3177457.3177506},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049842571&doi=10.1145%2f3177457.3177506&partnerID=40&md5=8576bbe7fb25aa2c75a5bf2b93ac16ab},
	affiliations = {Claremont High School, 1601 N.Indian Blvd., Claremont, 91711, CA, United States; Department of Computer Science, California State Polytechnic University, Pomona, Pomona, 91768, CA, United States},
	abstract = {Machine Learning allows systems to learn and improve automatically from experiences without hand-coding. Thus, in recent years, many technology companies have been developing such application if Artificial Intelligence, from face recognition by Facebook, to the AlphaGo program by Google. The irrigation systems in the market nowadays mostly allow users to set them to a certain amount of water and at specific time intervals. However, there are usually more than one type of plants in a garden, and each species requires different amount of water. In order to resolve this issue, in this paper, we have developed an irrigation system, with the use of deep learning, that is able to adjust the amounts of water foe each type pf plant through plants recognition. There are two main parts of the solution, the software and the hardware. The prior is connected with cameras to undergo plant recognition, and utilizes database to find the suitable amount of water; the latter controls the amount of water that is able to flow out. © 2018 Association for Computing Machinery.},
	author_keywords = {Image Classification; Irrigation System; Machine Learning; Soil Moisture Content},
	keywords = {Application programs; Artificial intelligence; Face recognition; Image classification; Internet of things; Irrigation; Learning systems; Soil moisture; Facebook; Flow out; Hand coding; Irrigation systems; Plant recognition; Specific time; Technology companies; Deep learning},
	publisher = {Association for Computing Machinery},
	isbn = {978-145036339-6},
	language = {English},
	abbrev_source_title = {ACM Int. Conf. Proc. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 25; Conference name: 10th International Conference on Computer Modeling and Simulation, ICCMS 2018; Conference date: 8 January 2018 through 10 January 2018; Conference code: 137306}
}

@ARTICLE{Prasad201721339,
	author = {Prasad, Shitala and Peddoju, Sateesh Kumar and Ghosh, Debashis},
	title = {An adaptive plant leaf mobile informatics using RSSC},
	year = {2017},
	journal = {Multimedia Tools and Applications},
	volume = {76},
	number = {20},
	pages = {21339 – 21363},
	doi = {10.1007/s11042-016-4040-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992383120&doi=10.1007%2fs11042-016-4040-8&partnerID=40&md5=6ae7301cedb0677166ac8aa0a009d953},
	affiliations = {Department of Computer Science and Engineering, Indian Institute of Technology Roorkee, Uttarakhand, India; Department of Electronics and Communication Engineering, Indian Institute of Technology Roorkee, Uttarakhand, India},
	abstract = {An automated plant biometric system is now an important step in preserving nature’s biodiversity. This paper presents a novel Relative Sub-image Sparse Coefficient (RSSC) algorithm for mobile devices (MDs) representing plant leaves into a mathematically compact vector for its classification. The RSSC feature vector includes local Statistical Entropy Texture (SET) information inter-related to all the sub-images within a leaf. RSSC space is merged with Gray Level Co-occurrence Matrix (GLCM) feature to refine the outputs using best-Nearest Neighbor (best-NN), designed for MDs. The experiments were performed on three different types of leaf datasets: (i) Flavia, (ii) ICL and (iii) Diseased leaf datasets. The results proves our method more accurate and better compared to other existing plant identification systems. The proposed approach is also tolerant under shape distortion caused while capturing. The mobile machine learning system for leaf image informatics is deployed on Android devices which helps botanists, agriculturists and medical biologists to recognize ubiquitously the herbs and plant species anywhere-anytime. © 2016, Springer Science+Business Media New York.},
	author_keywords = {Best-NN; Human mobile interaction (HMI); Leaf image informatics; Mobile vision (MV); Relative sub-image sparse coefficients (RSSC); Shape descriptor},
	keywords = {Artificial intelligence; Biodiversity; Biometrics; Information science; Learning systems; Medical imaging; Mobile devices; Best-NN; Leaf images; Mobile interaction; Mobile vision; Shape descriptors; Subimages; Plants (botany)},
	correspondence_address = {S. Prasad; Department of Computer Science and Engineering, Indian Institute of Technology Roorkee, Uttarakhand, India; email: shitala@ieee.org},
	publisher = {Springer New York LLC},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Imah2018,
	author = {Imah, E.M. and Rahayu, Y.S. and Wintarti, A.},
	title = {Plant Leaf Recognition Using Competitive Based Learning Algorithm},
	year = {2018},
	journal = {IOP Conference Series: Materials Science and Engineering},
	volume = {288},
	number = {1},
	doi = {10.1088/1757-899X/288/1/012058},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041680923&doi=10.1088%2f1757-899X%2f288%2f1%2f012058&partnerID=40&md5=036bf2e1f8b17fd78a393ef88dc29fe0},
	affiliations = {Mathematics Department, Universitas Negeri Surabaya, Surabaya, East-Java, Indonesia; Biology Department, Universitas Negeri Surabaya, Surabaya, East-Java, Indonesia},
	abstract = {Plant recognition based on digital leaf image has received as particular attention in computer vision and intelligence system, due its important implication in automatic plant identification. Plant species have the unique leaf characteristics such as the shape, texture, margin, and colour, which different each other. This study presents a novel method for automation plant recognition using Generalized Relevance Learning Vector Quantization (GRLVQ). GRLVQ is a competitive based learning algorithm which is integrating features extraction and classification phases. The experimental result shows that GRLVQ has better performance than the predecessor algorithm. © Published under licence by IOP Publishing Ltd.},
	keywords = {Vector quantization; Features extraction; Generalized relevance learning vector quantizations; Intelligence systems; Leaf characteristics; Leaf images; Plant identification; Plant recognition; Plant species; Learning algorithms},
	correspondence_address = {E.M. Imah; Mathematics Department, Universitas Negeri Surabaya, Surabaya, East-Java, Indonesia; email: ellymatul@unesa.ac.id},
	editor = {Abdullah A.G. and Nandiyanto A.B.D. and Widiaty I.},
	publisher = {Institute of Physics Publishing},
	issn = {17578981},
	language = {English},
	abbrev_source_title = {IOP Conf. Ser. Mater. Sci. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2nd Annual Applied Science and Engineering Conference, AASEC 2017; Conference date: 24 August 2017 through 24 August 2017; Conference code: 134033; All Open Access, Gold Open Access}
}

@CONFERENCE{Durairajah20186,
	author = {Durairajah, Vickneswari and Gobee, Suresh and Muneer, Amgad},
	title = {Automatic Vision Based Classification System Using DNN and SVM Classifiers},
	year = {2018},
	journal = {Proceedings - 2018 3rd International Conference on Control, Robotics and Cybernetics, CRC 2018},
	pages = {6 – 14},
	doi = {10.1109/CRC.2018.00011},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070935752&doi=10.1109%2fCRC.2018.00011&partnerID=40&md5=a52f99adc5e84b028c15db7a423b123d},
	affiliations = {Asia Pacific University of Technology and Innovation, Malaysia},
	abstract = {In this paper, we construct an automatic classification vision system that is designed to recognize Malaysian herbs that are typically used for medical or culinary purposes. The proposed system employs two classifiers, Support Vector machine (SVM) and Deep Neural Network (DNN). The two classifiers have been implemented using OpenCV-Python. For the training test SVM achieved 86.63% recognition accuracy and DNN (TensorFlow) achieved 98% recognition accuracy. For the real life testing SVM achieved 74.63% recognition accuracy and DNN achieved 93% recognition accuracy. In the proposed system a total of 1000 leaves were used. A total of 50 samples of herbs were collected for each class and they were divided into two datasets. The first dataset which consisted 60% of the herbs samples were used for the training purpose and the other dataset with 40% of the herbs samples were used for the testing purpose. The time taken for each recognition process was 4 seconds for SVM and 5 seconds for DNN classifier. Also, the proposed system is capable of identifying the herbs leaves even though they are wet, dried and deformed with a recognition accuracy of 52.50%. Finally, based on the experiments that were done, the system proved to be very efficient and accurate with the highest recognition rate being 98%. The results indicate that the techniques used in the proposed system are significantly efficient when compared to the various techniques employed in the existing literature. © 2018 IEEE.},
	author_keywords = {DNN; feature extraction; GLCM technique; herb recognition; plant classification; segmentation; SVM; tensorflow; zernike moments},
	keywords = {Deep neural networks; Feature extraction; Image segmentation; Plants (botany); Robotics; Statistical tests; GLCM technique; herb recognition; Plant classification; tensorflow; Zernike moments; Support vector machines},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153867738-4},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Control, Robot. Cybern., CRC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; Conference name: 3rd International Conference on Control, Robotics and Cybernetics, CRC 2018; Conference date: 26 September 2018 through 28 September 2018; Conference code: 150264}
}

@ARTICLE{Zhang2018164,
	author = {Zhang, Shanwen and Zhang, Chuanlei and Wang, Zhen and Kong, Weiwei},
	title = {Combining sparse representation and singular value decomposition for plant recognition},
	year = {2018},
	journal = {Applied Soft Computing Journal},
	volume = {67},
	pages = {164 – 171},
	doi = {10.1016/j.asoc.2018.02.052},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045959409&doi=10.1016%2fj.asoc.2018.02.052&partnerID=40&md5=b4aa4751d78a221d8e46d69b8f2bbd88},
	affiliations = {Department of Information Engineering, XiJing University, Xi'an, 710123, China; School of Computer Science and Information Engineering, Tianjin University of Science and Technology, 1038 Da Gu Nan Lu, Tianjin, 300222, China},
	abstract = {Plant recognition is one of important research areas of pattern recognition. As plant leaves are extremely irregular, complex and diverse, many existing plant classification and recognition methods cannot meet the requirements of the automatic plant recognition system. A plant recognition approach is proposed by combining singular value decomposition (SVD) and sparse representation (SR) in this paper. The difference from the traditional plant classification methods is that, instead of establishing a classification model by extracting the classification features, the proposed method directly reduces the image dimensionality and recognizes the test samples based on the sparse coefficients, and uses the class-specific dictionary learning for sparse modeling to reduce the recognition time. The proposed method is verified on two plant leaf datasets and is compared with other four existing plant recognition methods The overall recognition accuracy of the proposed approach for the 6 kinds of plant leaves is over 96%, which is the best classification rate. The experimental results show the feasibility and effectiveness of the proposed method. © 2018},
	author_keywords = {Plant leaf image; Plant species recognition; Singular value decomposition (SVD); Sparse representation (SR)},
	keywords = {Classification (of information); Pattern recognition; Plants (botany); Class-specific dictionaries; Classification features; Classification models; Plant classification; Plant leaf images; Plant species; Recognition accuracy; Sparse representation; Singular value decomposition},
	correspondence_address = {C. Zhang; School of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, 1038 Da Gu Nan Lu, 300222, China; email: 97313114@tust.edu.cn},
	publisher = {Elsevier Ltd},
	issn = {15684946},
	language = {English},
	abbrev_source_title = {Appl. Soft Comput. J.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 29}
}

@CONFERENCE{Wang2018560,
	author = {Wang, Zhaobin and Chen, Lina and Li, Huale and Li, Jian and Zhu, Ying and Wang, Jie},
	title = {Comparative study on different PCNN models in plant leaf classification},
	year = {2018},
	journal = {2018 IEEE International Conference on Information and Automation, ICIA 2018},
	pages = {560 – 564},
	doi = {10.1109/ICInfA.2018.8812573},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072315355&doi=10.1109%2fICInfA.2018.8812573&partnerID=40&md5=2aa516e4d168f3867d067eb32552a82a},
	affiliations = {School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Institute of Biology Gansu, Academy of Sciences, Lanzhou, China},
	abstract = {Plant classification is an important area of botany research. Plant classification based on image processing is also currently one of the hot spots in the research. Pulse coupled neural network(PCNN) has good performance in image processing, especially in the feature extraction. There are a lot of improved models. In this paper, three kinds of important improved PCNN models such as pulse-coupled neural network and intersecting cortical model, spiking cortical model and two-output pulse coupled neural network are applied to plant leaf classification based on image processing so that the network model suitable for leaf classification is selected. We have improved the extraction method of entropy sequence features, and then conducted a comparative experiment. Experimental results show that each model has its own characteristics. However, due to its complete functions, the standard pulse-coupled neural network has slightly better feature extraction ability, followed by intersecting cortical model, spiking cortical model and dual-output pulsed coupled neural network. © 2018 IEEE.},
	author_keywords = {Feature extraction; Leaf classification.; PCNN; Pulse image},
	keywords = {Extraction; Feature extraction; Image enhancement; Neural networks; Plants (botany); Comparative experiments; Intersecting cortical models; Leaf classification; PCNN; Plant leaf classifications; Pulse coupled neural network; Pulse image; Pulsed coupled neural networks; Image classification},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153868069-8},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Inf. Autom., ICIA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2018 IEEE International Conference on Information and Automation, ICIA 2018; Conference date: 11 August 2018 through 13 August 2018; Conference code: 151193}
}

@ARTICLE{Gao201839,
	author = {Gao, Junfeng and Nuyttens, David and Lootens, Peter and He, Yong and Pieters, Jan G.},
	title = {Recognising weeds in a maize crop using a random forest machine-learning algorithm and near-infrared snapshot mosaic hyperspectral imagery},
	year = {2018},
	journal = {Biosystems Engineering},
	volume = {170},
	pages = {39 – 50},
	doi = {10.1016/j.biosystemseng.2018.03.006},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044615839&doi=10.1016%2fj.biosystemseng.2018.03.006&partnerID=40&md5=9d0139d5f1614ae16d6b39528713e3b4},
	affiliations = {Department of Biosystems Engineering, Ghent University, Coupure Links 653, Gent, 9000, Belgium; Technology and Food Science Unit, ILVO, Burg. Van Gansberghelaan 115, Merelbeke, 9820, Belgium; Plant Sciences Unit, ILVO, Caritasstraat 39, Melle, 9090, Belgium; College of Biosystems Engineering and Food Science, Zhejiang University, Yuhangtang Road 866, Hangzhou, 310058, Zhejiang, China},
	abstract = {This study explores the potential of a novel hyperspectral snapshot mosaic camera for weed and maize classification. The image processing, feature engineering and machine learning techniques were discussed when developing an optimal classification model for the three kinds of weeds and maize. A total set of 185 spectral features including reflectance and vegetation index features was constructed. Subsequently, the principal component analysis was used to reduce the redundancy of the constructed features, and the first 5 principal components, explaining over 95% variance ratio, were kept for further analysis. Furthermore, random forests as one of machine learning techniques were built for developing the classifier with three different combinations of features. Accuracy-oriented feature reduction was performed when choosing the optimal number of features for building the classification model. Moreover, hyperparameter tuning was explored for the optimal selection of random forest model. The results showed that the optimal random forest model with 30 important spectral features can achieve a mean correct classification rate of 1.0, 0.789, 0.691 and 0.752 for Zea mays, Convolvulus arvensis, Rumex and Cirsium arvense, respectively. The McNemar test showed an overall better performance of the optimal random forest model at the 0.05 significance level compared to the k-nearest neighbours (KNN) model. © 2018 IAgrE},
	author_keywords = {Cross validation; Feature selection; Hyperparameter tuning; Machine learning; Plant classification; Snapshot hyperspectral imaging},
	keywords = {Artificial intelligence; Classification (of information); Decision trees; Feature extraction; Grain (agricultural product); Hyperspectral imaging; Image processing; Infrared devices; Learning systems; Nearest neighbor search; Principal component analysis; Spectroscopy; Cross validation; Hyper-parameter; Hyper-spectral imageries; K nearest neighbours (k-NN); Machine learning techniques; Optimal classification; Plant classification; Snapshot hyperspectral imaging; Learning algorithms},
	correspondence_address = {Y. He; College of Biosystems Engineering and Food Science, Zhejiang University, Hangzhou, Yuhangtang Road 866, 310058, China; email: yhe@zju.edu.cn},
	publisher = {Academic Press},
	issn = {15375110},
	coden = {BEINB},
	language = {English},
	abbrev_source_title = {Biosyst. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 105}
}

@ARTICLE{Hu2018853,
	author = {Hu, Jing and Chen, Zhibo and Yang, Meng and Zhang, Rongguo and Cui, Yaji},
	title = {A multiscale fusion convolutional neural network for plant leaf recognition},
	year = {2018},
	journal = {IEEE Signal Processing Letters},
	volume = {25},
	number = {6},
	pages = {853 – 857},
	doi = {10.1109/LSP.2018.2809688},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042709417&doi=10.1109%2fLSP.2018.2809688&partnerID=40&md5=ceda666ac1fc6597be3e19cfeea0e702},
	affiliations = {School of Information Science and Technology, Beijing Forestry University, Taiyuan, 030024, China; College of Computer Science and Technology, Taiyuan University of Science and Technology, Taiyuan, 030024, China; School of Information Science and Technology, Beijing Forestry University, Beijing, 100083, China; International School, Beijing University of Posts and Telecommunications, Beijing, 100876, China},
	abstract = {Plant leaf recognition is a computer vision task used to automatically recognize plant species. It is very challenging since rich plant leaf morphological variations, such as sizes, textures, shapes, venation, and so on. Most existing plant leaf methods typically normalize all plant leaf images to the same size and recognize them at one scale, resulting in unsatisfactory performances. In this letter, a multiscale fusion convolutional neural network (MSF-CNN) is proposed for plant leaf recognition at multiple scales. First, an input image is down-sampled into multiples low resolution images with a list of bilinear interpolation operations. Then, these input images with different scales are step-by-step fed into the MSF-CNN architecture to learn discriminative features at different depths. At this stage, the feature fusion between two different scales is realized by a concatenation operation, which concatenates feature maps learned on different scale images from a channel view. Along with the depth of the MSF-CNN, multiscale images are progressively handled and the corresponding features are fused. Third, the last layer of the MSF-CNN aggregates all discriminative information to obtain the final feature for predicting the plant species of the input image. Experiments show the proposed MSF-CNN method is superior to multiple state-of-the art plant leaf recognition methods on the MalayaKew Leaf dataset and the LeafSnap Plant Leaf dataset. © 1994-2012 IEEE.},
	author_keywords = {Multiscale convolutional neural network (MSCNN); multiscale feature; plant leaf recognition},
	keywords = {Convolution; Neural networks; Plants (botany); Bilinear interpolation; Convolutional neural network; Discriminative features; Low resolution images; Morphological variation; Multi-scale features; Multiscale fusion; Plant leaf; Image segmentation},
	correspondence_address = {Z. Chen; School of Information Science and Technology, Beijing Forestry University, Beijing, 100083, China; email: zhibo@bjfu.edu.cn},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {10709908},
	coden = {ISPLE},
	language = {English},
	abbrev_source_title = {IEEE Signal Process Lett},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 92}
}

@ARTICLE{Zeng2017563,
	author = {Zeng, Shaoning and Zhang, Bob and Du, Yong},
	title = {Joint distances by sparse representation and locality-constrained dictionary learning for robust leaf recognition},
	year = {2017},
	journal = {Computers and Electronics in Agriculture},
	volume = {142},
	pages = {563 – 571},
	doi = {10.1016/j.compag.2017.11.013},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034017897&doi=10.1016%2fj.compag.2017.11.013&partnerID=40&md5=1d67505c0ee1dd12c34ce53db9b78460},
	affiliations = {Department of Computer and Information Science, University of Macau, Avenida da Universidade, Taipa, Macao; School of Information Science and Technology, Huizhou University, Guangdong, China; Department of Electrical and Information Engineering, Northeast Agricultural University, Harbin, China; Department of Computer Science and Technology, Tianjin University, Tianjin, China},
	abstract = {Plant species recognition has been a difficult and important task in agriculture, where computer techniques like image processing and pattern recognition can commendably facilitate plant recognition based on leaf images. The locality-constrained models produced by sparse representation and dictionary learning are a few of the prevailing feature models for leaf image recognition. Previous studies demonstrated that sparsity in representation plays an important role in the recognition, while sparsity constraints are the keys to solve the dictionary learning problems. Many of them focused on improving the sparsity, which is hard, but using large atoms in dictionary learning for high accuracy consumed more training time. Actually, sparse representation and dictionary learning are both based on distance calculation, e.g., Euclidean distance, which is also an aspect possible to obtain an improvement. On the premise of unchanged sparsity, this paper proposed a novel distance based method fusing Sparse Representation and Locality-Constrained Dictionary Learning (SRLC-DL) for robust leaf recognition. Integrating the distances obtained by dictionary learning and naive sparse representation can generate robust and high performance leaf recognition. In the fusion of distances, the number of atoms was not necessarily large as conventional methods, and even using smaller atoms produced more promising recognition at times. Therefore, not only has the leaf recognition accuracy by sparse representation been advanced, but the recognition speed also remains fast enough. A series of experiments had been conducted on five benchmark leaf datasets, including Caltech Leaves, Leaf, Herbarium, Swedish Leaf and Flavia. The experimental results demonstrated that SRLC-DL produced a higher accuracy in leaf image recognition and outperformed many other state-of-the-art methods. © 2017 Elsevier B.V.},
	author_keywords = {Dictionary learning; Feature integration; Leaf image recognition; Sparse representation},
	keywords = {Atoms; Image processing; Image recognition; Plants (botany); Conventional methods; Dictionary learning; Distance calculation; Distance-based methods; Feature integration; Sparse representation; Sparsity constraints; State-of-the-art methods; accuracy assessment; data set; image processing; leaf; learning; numerical model; pattern recognition; performance assessment; Pattern recognition},
	correspondence_address = {B. Zhang; Department of Computer and Information Science, University of Macau, Taipa, Avenida da Universidade, Macao; email: bobzhang@umac.mo},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@ARTICLE{Wang201899,
	author = {Wang, Zhaobin and Sun, Xiaoguang and Yang, Zekun and Zhang, Yaonan and Zhu, Ying and Ma, Yide},
	title = {Leaf Recognition Based on DPCNN and BOW},
	year = {2018},
	journal = {Neural Processing Letters},
	volume = {47},
	number = {1},
	pages = {99 – 115},
	doi = {10.1007/s11063-017-9635-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019626983&doi=10.1007%2fs11063-017-9635-1&partnerID=40&md5=be54a35954b66a553fb2ea69d003b298},
	affiliations = {School of Information Science and Engineering, Lanzhou University Lanzhou, Lanzhou, 730000, Gansu Province, China; Cold and Arid Regions Environmental and Engineering Research Institute, Chinese Academy of Sciences Lanzhou, Lanzhou, 730000, Gansu Province, China; Institute of Biology, Gansu Academy of Sciences, Lanzhou, 730000, China},
	abstract = {Leaf classification is an interesting and important research. Current work focuses mainly on feature extraction, especially on textural feature extraction. In this case, we propose a new method of leaf recognition based on bag of words (BOW) and entropy sequence (EnS). In our method, EnS is firstly obtained by dual-output pulse-coupled neural network and then it is improved by BOW. Locality-constrained linear coding method is used for sparse coding. Then, the classification system is built where the linear support vector machine is taken as classifier. Some representative datasets and existing methods are employed to evaluate the effect of the proposed method. Finally, experimental results show that the accuracy of our proposed method is better than existing methods. © 2017, Springer Science+Business Media New York.},
	author_keywords = {BOW; DPCNN; Feature extraction; Plant recognition; SVM},
	keywords = {Feature extraction; Neural networks; Support vector machines; Classification system; DPCNN; Entropy sequences; Leaf classification; Leaf recognition; Linear coding; Linear Support Vector Machines; Plant recognition; Extraction},
	correspondence_address = {Z. Wang; School of Information Science and Engineering, Lanzhou University Lanzhou, Lanzhou, 730000, China; email: zhaobin_wang@hotmail.com},
	publisher = {Springer New York LLC},
	issn = {13704621},
	coden = {NPLEF},
	language = {English},
	abbrev_source_title = {Neural Process Letters},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{Pacifico2018,
	author = {Pacifico, Luciano D. S. and Macario, Valmir and Oliveira, Joao F. L.},
	title = {Plant Classification Using Artificial Neural Networks},
	year = {2018},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	volume = {2018-July},
	doi = {10.1109/IJCNN.2018.8489701},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056521730&doi=10.1109%2fIJCNN.2018.8489701&partnerID=40&md5=0b440276d55331fa66ce120840997a4b},
	affiliations = {Departamento de Computacao - DC, Universidade Federal Rural de Pernambuco - UFRPE, Recife, Pernambuco, Brazil; Faculdade de Ciencias e Tecnologia de Garanhuns - FACETEG, Universidade de Pernambuco - UPE, Garanhuns, Pernambuco, Brazil},
	abstract = {Automatic plant species identification is a difficulty challenge and an interesting area of research for both botanical taxonomy and computer science. From the past few years, some attempts towards the development of automatic plant recognition systems have been proposed, but the performance of such systems is not satisfactory in terms of accuracy, and these systems are also task dependent, since they are strongly influenced by the set of characteristics extracted from plant samples, leading to the problem known as data set bias. In this work, we use a Multi-Layer Perceptron (MLP) artificial neural network trained with Backpropagation algorithm to perform automatic plant classification. To avoid data set bias problem, some plant data sets which use different plant features obtained by different feature extraction processes are employed. We compare MLP algorithm with several supervised learning methods from plant recognition literature using a statistical hypothesis test of type Friedman/Nemenyi test. The obtained results show the potential of MLP algorithm to deal with plant classification in a unbiased fashion. © 2018 IEEE.},
	keywords = {Backpropagation algorithms; Neural networks; Testing; Extraction process; Multi layer perceptron; Plant classification; Plant recognition; Plant samples; Plant species identification; Statistical hypothesis test; Supervised learning methods; Data mining},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150906014-6},
	coden = {85OFA},
	language = {English},
	abbrev_source_title = {Proc Int Jt Conf Neural Networks},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; Conference name: 2018 International Joint Conference on Neural Networks, IJCNN 2018; Conference date: 8 July 2018 through 13 July 2018; Conference code: 141067}
}

@CONFERENCE{Ozdil20182709,
	author = {Ozdil, Omer and Esin, Yunus Emre and Demirel, Berkan and Ozturk, Safak},
	title = {Representative signature generation for plant detection in hyperspectral images},
	year = {2018},
	journal = {International Geoscience and Remote Sensing Symposium (IGARSS)},
	volume = {2018-July},
	pages = {2709 – 2712},
	doi = {10.1109/IGARSS.2018.8517344},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063152211&doi=10.1109%2fIGARSS.2018.8517344&partnerID=40&md5=0dd029fe5de1d6eed7ae8fe08a30c41c},
	affiliations = {Sensors, Signal ve Image Processing Group, HAVELSAN Inc., Ankara, Turkey},
	abstract = {In this study, the effect of utilizing different type of signatures on plant detection success is evaluated on hyperspectral aerial images. Plant regions are tried to detect using spectral signatures of leaf, stem and tassel belonging to the plant separately and the plant representative signature (PRS) is created by averaging of signatures of selected region on the aerial images. The signatures used for detection are generated from hyperspectral images taken from 10m distance to target plant. The Spectral Angle Mapper(SAM) and Generalized Likelihood Ratio Test(GLRT) algorithms are used for target detection. Performance evaluation is made by Receiver Operating Characteristic (ROC) curves. When the results are evaluated, it is observed that the detection performance with the use of PRS is higher. © 2018 IEEE},
	author_keywords = {Corn detection; Hyperspectral image processing; Plant classification; Spectral library},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153867150-4},
	coden = {IGRSE},
	language = {English},
	abbrev_source_title = {Dig Int Geosci Remote Sens Symp (IGARSS)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 38th Annual IEEE International Geoscience and Remote Sensing Symposium, IGARSS 2018; Conference date: 22 July 2018 through 27 July 2018; Conference code: 141934}
}

@CONFERENCE{Gao201729,
	author = {Gao, Min and Lin, Yang and Sinnott, Richard O.},
	title = {A mobile application for plant recognition through deep learning},
	year = {2017},
	journal = {Proceedings - 13th IEEE International Conference on eScience, eScience 2017},
	pages = {29 – 38},
	doi = {10.1109/eScience.2017.15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043769253&doi=10.1109%2feScience.2017.15&partnerID=40&md5=0ad785d229c72d2109ebd9bc487073b7},
	affiliations = {School of Computing and Information Systems, University of Melbourne, Melbourne, Australia},
	abstract = {It is a simple task for humans to visually identify objects. However, computer-based image recognition remains challenging. In this paper we describe an approach for image recognition with specific focus on automated recognition of plants and flowers. The approach taken utilizes deep learning capabilities and unlike other approaches that focus on static images for feature classification, we utilize video data that compensates for the information that would otherwise be lost when comparing a static image with many others images of plants and flowers. We describe the steps taken in data collection, data cleaning and data purification, and the deep learning algorithms that were subsequently applied. We describe the mobile (iOS) application that was designed and finally we present the overall results that show that in the work undertaken thus far, the approach is able to identify 122/125 plants and 47/50 genera selected with degrees of confidence up to 95%. We also describe the performance speed up through the use of Cloud-based resources. © 2017 IEEE.},
	author_keywords = {Computer Vision; Convolutional neural networks; Data Mining; Deep Learning; Image Recognition},
	keywords = {Classification (of information); Computer vision; Data acquisition; Data mining; Image recognition; Learning algorithms; Neural networks; Automated recognition; Convolutional neural network; Data collection; Data purifications; Feature classification; Learning capabilities; Mobile applications; Plant recognition; Deep learning},
	correspondence_address = {Y. Lin; School of Computing and Information Systems, University of Melbourne, Melbourne, Australia; email: langl2@student.unimelb.edu.au},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153862686-3},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. eSci., eScience},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; Conference name: 13th IEEE International Conference on eScience, eScience 2017; Conference date: 24 October 2017 through 27 October 2017; Conference code: 132556}
}

@CONFERENCE{Joly2017551,
	author = {Joly, Alexis and Bonnet, Pierre and Affouard, Antoine and Lombardo, Jean-Christophe and Goeäu, Hervé},
	title = {Pl@ntNet - My business},
	year = {2017},
	journal = {MM 2017 - Proceedings of the 2017 ACM Multimedia Conference},
	pages = {551 – 555},
	doi = {10.1145/3123266.3129312},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035230294&doi=10.1145%2f3123266.3129312&partnerID=40&md5=49a17f6f9aa9fc2643085ae8064484df},
	affiliations = {Inria, LIRMM, Montpellier, France; Cirad, AMAP, Montpellier, France},
	abstract = {Pl@ntNet is a world-scale participatory platform and information system dedicated to the monitoring of plant biodiversity through image-based plant identification. Nowadays, the mobile front-end of Pl@ntNet has been downloaded by more than 4 millions users in about 170 countries and an active community of contributors produce and revise new observations everyday. This paper presents a business proposal allowing enterprises or organizations to set up their own private collaborative workflow within Pl@ntNet information system. The main added value is to allow them working on their own business object (e.g. plant disease diagnostic, deficiency measurements, railway lines maintenance, etc.) and with their own community of contributors and end-users (employees, sales representatives, clients, observers network, etc.). This business idea answers to a growing demand in agriculture and environmental economics. Actors in these domains begin to know that machine learning techniques are mature enough but the lack of training data and of efficient tools to collect them is a major breakthrough. A collaborative platform like Pl@ntNet extended with the technical innovations presented in this paper is the ideal tool to bridge this gap. It will initiate a powerful positive feedback loop boosting the production of training data while improving the work of the employees. © 2016 ACM.},
	keywords = {Biodiversity; Diagnosis; Economics; Information systems; Learning systems; Collaborative platform; Collaborative workflow; Environmental economics; Machine learning techniques; Plant identification; Positive feedback loop; Sales representatives; Technical innovation; Personnel training},
	publisher = {Association for Computing Machinery, Inc},
	isbn = {978-145034906-2},
	language = {English},
	abbrev_source_title = {MM - Proc. ACM Multimed. Conf.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 25th ACM International Conference on Multimedia, MM 2017; Conference date: 23 October 2017 through 27 October 2017; Conference code: 131373; All Open Access, Green Open Access}
}

@ARTICLE{Šulc2017,
	author = {Šulc, Milan and Matas, Jiří},
	title = {Fine-grained recognition of plants from images},
	year = {2017},
	journal = {Plant Methods},
	volume = {13},
	number = {1},
	doi = {10.1186/s13007-017-0265-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038942809&doi=10.1186%2fs13007-017-0265-4&partnerID=40&md5=8a06f596770bb16e82f9d3cc66567a45},
	affiliations = {FEE CTU in Prague, Department of Cybernetics, Karlovo namesti 13, Prague 2, 121 35, Czech Republic},
	abstract = {Background: Fine-grained recognition of plants from images is a challenging computer vision task, due to the diverse appearance and complex structure of plants, high intra-class variability and small inter-class differences. We review the state-of-the-art and discuss plant recognition tasks, from identification of plants from specific plant organs to general plant recognition "in the wild". Results: We propose texture analysis and deep learning methods for different plant recognition tasks. The methods are evaluated and compared them to the state-of-the-art. Texture analysis is only applied to images with unambiguous segmentation (bark and leaf recognition), whereas CNNs are only applied when sufficiently large datasets are available. The results provide an insight in the complexity of different plant recognition tasks. The proposed methods outperform the state-of-the-art in leaf and bark classification and achieve very competitive results in plant recognition "in the wild". Conclusions: The results suggest that recognition of segmented leaves is practically a solved problem, when high volumes of training data are available. The generality and higher capacity of state-of-the-art CNNs makes them suitable for plant recognition "in the wild" where the views on plant organs or plants vary significantly and the difficulty is increased by occlusions and background clutter. © 2017 The Author(s).},
	author_keywords = {Bark; Computer vision; Convolutional neural networks; Deep learning; Kernel maps; Leaves; Plants; SVM; Texture},
	correspondence_address = {M. Šulc; FEE CTU in Prague, Department of Cybernetics, Prague 2, Karlovo namesti 13, 121 35, Czech Republic; email: sulcmila@fel.cvut.cz},
	publisher = {BioMed Central Ltd.},
	issn = {17464811},
	language = {English},
	abbrev_source_title = {Plant Methods},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 44; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Faza2018,
	author = {Faza, S. and Nababan, E.B. and Efendi, S. and Basyuni, M. and Rahmat, R.F.},
	title = {An initial study of deep learning for mangrove classification},
	year = {2018},
	journal = {IOP Conference Series: Materials Science and Engineering},
	volume = {420},
	number = {1},
	doi = {10.1088/1757-899X/420/1/012093},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054831144&doi=10.1088%2f1757-899X%2f420%2f1%2f012093&partnerID=40&md5=9b0a3b0efa8c075644bdd2dbb5f3c5fb},
	affiliations = {Stud. in Faculty of Computer Science and Information Technology, Universitas Sumatera Utara, Medan, Indonesia; Faculty of Computer Science and Information Technology, Universitas Sumatera Utara, Medan, Indonesia; Faculty of Forestry, Universitas Sumatera Utara, Medan, Indonesia},
	abstract = {Deep Learning is a new breakthrough in the area of neural network. One of it methodology is Deep Neural Network. Usually, deep neural network is a method that can solve problems such as classification or prediction. Here in this study, we collect data for mangrove classification, and we see an opportunity to classify the type of mangrove based on its features using deep neural network. Research on mangrove plants related to classification is more widely used in mangrove dispersal spread through spectral and hyperspectral satellite images than using real value data such as morphological data, thus providing blank space for researchers to use mangrove morphological data. This initial study is to build a deep neural network architecture and analyze it in term of mangrove sprout plant classification. The result from our architectural methodology of this research is reaching the lowest training error of 0.1345 and the highest testing accuracy value of 98%. © Published under licence by IOP Publishing Ltd.},
	keywords = {Network architecture; Hyperspectral satellite; Morphological data; Plant classification; Real values; Testing accuracy; Training errors; Deep neural networks},
	publisher = {Institute of Physics Publishing},
	issn = {17578981},
	language = {English},
	abbrev_source_title = {IOP Conf. Ser. Mater. Sci. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 2nd Nommensen International Conference on Technology and Engineering, NICTE 2018; Conference date: 19 July 2018 through 20 July 2018; Conference code: 140093; All Open Access, Gold Open Access}
}

@CONFERENCE{Atito2018,
	author = {Atito, Sara and Yanikoglu, Berrin and Aptoula, Erchan and Ganiyusufolu, Ipek and Yildi, Aras and Yildirir, Kerem and Sevilmiş, Bariş and Şen, M. Umut},
	title = {Plant identification with deep learning ensembles in ExpertLifeCLEF 2018},
	year = {2018},
	journal = {CEUR Workshop Proceedings},
	volume = {2125},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051085509&partnerID=40&md5=ab63a69f5641603597e6150ac510749b},
	affiliations = {Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; Institute of Information Technologies, Gebze Technical University, Kocaeli, Turkey},
	abstract = {This work describes the plant identification system that we submitted to the ExpertLifeCLEF plant identification campaign in 2018. We fine-tuned two pre-trained deep learning architectures (SeNet and DensNetwork) using images shared by the CLEF organizers in 2017. Our main runs are 4 ensembles obtained with different weighted combinations of the 4 deep learning architectures. The fifth ensemble is based on deep learning features but uses Error Correcting Output Codes (ECOC) as the ensemble. Our best system has achieved a classification accuracy of 74.4%, while the best system obtained 86.7% accuracy, on the whole of the official test data. This system ranked 4th place among all the teams, but matched the accuracy of one of the human experts.},
	author_keywords = {Convolutional neural networks; Deep learning; Plant identification},
	keywords = {Network architecture; Neural networks; Classification accuracy; Convolutional neural network; Error correcting output code; Human expert; Learning architectures; Plant identification; Plant identification systems; Test data; Deep learning},
	editor = {Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Nie J.-Y. and Soulier L. and Soulier L. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 19th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2018; Conference date: 10 September 2018 through 14 September 2018; Conference code: 138100}
}

@ARTICLE{Cervantes2018376,
	author = {Cervantes, Jair and Garcia Lamont, Farid and Rodriguez Mazahua, Lisbeth and Zarco Hidalgo, Alfonso and Ruiz Castilla, José S.},
	title = {Complex Identification of Plants from Leaves},
	year = {2018},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10956 LNAI},
	pages = {376 – 387},
	doi = {10.1007/978-3-319-95957-3_41},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051860276&doi=10.1007%2f978-3-319-95957-3_41&partnerID=40&md5=b0ade1838ca1ca883e1bdd83f833c9f1},
	affiliations = {Posgrado e Investigación, UAEMEX (Autonomous University of Mexico State), Texcoco, 56259, Mexico; Division of Research and Postgraduate Studies, Instituto Tecnológico de Orizaba, Av. Oriente 9, 852. Col. Emiliano Zapata, Orizaba, 94320, Mexico},
	abstract = {The automatic identification of plant leaves is a very important current topic of research in vision systems. Several researchers have tried to solve the problem of identification from plant leaves proposing various techniques. The proposed techniques in the literature have obtained excellent results on data sets where the leaves have dissimilar features to each other. However, in cases where the leaves are very similar to each other, the classification accuracy falls significantly. In this paper, we proposed a system to deal with the performance problem of machine learning algorithms where the leaves are very similar. The results obtained show that combination of different features and features selection process can improve the classification accuracy. © 2018, Springer International Publishing AG, part of Springer Nature.},
	author_keywords = {Features selection; Plant identification; Vision system},
	keywords = {Automation; Feature extraction; Intelligent computing; Learning algorithms; Learning systems; Automatic identification; Classification accuracy; Features selection; Performance problems; Plant identification; Plant leaves; Vision systems; Plants (botany)},
	correspondence_address = {J. Cervantes; Posgrado e Investigación, UAEMEX (Autonomous University of Mexico State), Texcoco, 56259, Mexico; email: jcervantesc@uaemex.mx},
	editor = {Huang D. and Han K. and Hussain A. and Gromiha M.M.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331995956-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 14th International Conference on Intelligent Computing, ICIC 2018; Conference date: 15 August 2018 through 18 August 2018; Conference code: 216939; All Open Access, Green Open Access}
}

@ARTICLE{Tao201869,
	author = {Tao, Liu and Cheng, Cai},
	title = {Plant identification based on image set analysis},
	year = {2018},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10970 LNCS},
	pages = {69 – 82},
	doi = {10.1007/978-3-319-94361-9_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049359003&doi=10.1007%2f978-3-319-94361-9_6&partnerID=40&md5=646485471622ef323e5999d517f6c251},
	affiliations = {College of Engineering Information Engineering, Northwest A&F University, Yangling, 712100, China},
	abstract = {Plant identification is crucial in plant protection, crop breeding and agriculture research area. With the development of information technology, computer vision-based plant identification is an effective and efficient solution. Many tradition plant identification algorithms utilize only one type of plant images, such as images from leaves, flowers and stems etc., which may bring misclassifications. In order to recognize plant accurately, we propose a new plant identification scheme based on image set analysis. In this method, uses image set as a taxonomic unit, each image set consists of multiple images (whole plant, fruits, leaves, flowers, stems, branches, leaves scan), the characteristics of plants are fused together as a basis for plant identification. Compared to the traditional single feature, this study has more characteristics and provides more information in the classification process. A face recognition algorithm based on image collection is used in our research, for example: AHISD/CHISD (Affine/Convex Hull based Image Set Distance), PLRC (Pairwise Linear Regression Classification), SSDML (Set-to-Set Distance Metric Learning), SANP (Sparse Approximated Nearest Point), RNP (Regularized Nearest Points). Data set contains 64,150 plant images, it can be divided into 369 training set (each training set contains 50 pictures) and 914 test sets (each test set contains 50 picture). The results show that CHISD has the highest recognition rate, at 77.02%, which is more suitable for requiring higher accuracy. RNP identifies a plant class took an average of 0.75 s, the algorithm is more suitable for the occasion with higher time requirement. Therefore, the plant identification based on image set is feasible and low cost, which can be extended to agricultural production. © Springer International Publishing AG, part of Springer Nature 2018.},
	author_keywords = {Feature extraction; Image set; Plant identification},
	keywords = {Agriculture; Artificial intelligence; Classification (of information); Face recognition; Feature extraction; Image analysis; Mobile telecommunication systems; Statistical tests; Agricultural productions; Classification process; Face recognition algorithms; Image collections; Image sets; Misclassifications; Plant identification; Time requirements; Plants (botany)},
	correspondence_address = {C. Cheng; College of Engineering Information Engineering, Northwest A&F University, Yangling, 712100, China; email: chengcai@nwsuaf.edu.cn},
	editor = {Yang Y. and Zhang L. and Zou Y. and Aiello M.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331994360-2},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 7th International Conference on Artificial Intelligence and Mobile Services, AIMS 2018 Held as Part of the Services Conference Federation, SCF 2018; Conference date: 25 June 2018 through 30 June 2018; Conference code: 214859}
}

@CONFERENCE{Sommana2018,
	author = {Sommana, Benjaphan and Theeramunkong, Thanaruk},
	title = {Improving plant recognition using hybrid features from connectionist and knowledge-based approaches},
	year = {2018},
	journal = {KICSS 2018 - 2018 13th International Conference on Knowledge, Information and Creativity Support Systems, Proceedings},
	doi = {10.1109/KICSS45055.2018.8950646},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078914964&doi=10.1109%2fKICSS45055.2018.8950646&partnerID=40&md5=1c78028a15971a05610b31501b04470c},
	affiliations = {Sirindhorn International Institute of Technology, Thammasat University, Pathum Thani, Thailand},
	abstract = {Many connectionist approaches get promising result but lack of knowledge. In this paper, we proposed architecture that combined knowledge-based approach to improve the accuracy of plant recognition. Towards this, hybrid features are constructed by merging three types of knowledge-based features; morphological feature, texture feature and color feature with convolutional neural network extracted features. Our architecture consists of three main stages which are data pre-processing, feature extraction and classification. Before features are extracted, images will be resized and augmented in the pre-processing stage. To classify the species of leaf, we consider decision tree and artificial neural network as a classifier. We experiment on two datasets; Flavia and Swedish dataset. The experimental result shows that the proposed architecture can predict unseen images correctly more than existing models. © 2018 IEEE.},
	author_keywords = {Artificial neural network; Decision tree; Knowledge-based and Connectionist features; Leaf classification; Plant recognition},
	keywords = {Data handling; Data mining; Decision trees; Knowledge based systems; Network architecture; Neural networks; Textures; Connectionist approach; Convolutional neural network; Feature extraction and classification; Knowledge based; Knowledge-based approach; Leaf classification; Morphological features; Plant recognition; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-172810161-3},
	language = {English},
	abbrev_source_title = {KICSS - Int. Conf. Knowl., Inf. Creat. Support Syst., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 13th International Conference on Knowledge, Information and Creativity Support Systems, KICSS 2018; Conference date: 15 November 2018 through 17 November 2018; Conference code: 156774}
}

@CONFERENCE{Jose2018,
	author = {Jose, Carranza-Rojas and Erick, Mata-Montero and Herve, Goeau},
	title = {Hidden Biases in Automated Image-Based Plant Identification},
	year = {2018},
	journal = {2018 IEEE International Work Conference on Bioinspired Intelligence, IWOBI 2018 - Proceedings},
	doi = {10.1109/IWOBI.2018.8464187},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054474629&doi=10.1109%2fIWOBI.2018.8464187&partnerID=40&md5=00334dff966ed3613a35e8a7866ac8d2},
	affiliations = {School of Computing, Costa Rica Institute of Technology, Cartago, Costa Rica; CIRAD, UMR AMAP, Montpellier, France},
	abstract = {Plant identification is critical to support important biodiversity conservation actions such as biodiversity inventories, monitoring of populations of endangered organisms, and assessing climate change impact, among many others. Because deep learning has demonstrated impressive results in the field of computer vision in general, research on automatic plant identification has been shifting its attention towards deep learning approaches. However, some authors have noticed that an important methodological issue may have been overlooked in the design of many experiments, which may explain why, on one hand, some studies based on hand-crafted feature extraction approaches report very high accuracy levels, but, on the other hand, newer deep learning approaches used in events such as the PlantCLEF challenge report relatively lower accuracy levels. Because PlantCLEF uses same specimen photos exclusively in either the training dataset or the testing dataset, we postulate that this may explain the lower accuracies achieved. Specifically, we explore the following two questions: does using different images of the same specimen for training and testing introduce a significant bias in deep learning experiments as well as in those that use handcrafted features in classical computer vision techniques? Does it affect the accuracy of species identifications even in the more restricted domain of leaf-based automated species identifications? We also address the issue of scalability of accuracy results for both, a particular feature extraction approach and a deep learning approach. All experiments are conducted on a dataset of 7,262 photos of leaves of 255 species of plants from Costa Rica. © 2018 IEEE.},
	author_keywords = {Automated Plant Identification; Biodiversity Informatics; Data Biases; Deep Learning; Leaf Recognition},
	keywords = {Automation; Biodiversity; Climate change; Computer vision; Conservation; Extraction; Feature extraction; Plants (botany); Statistical tests; Biodiversity conservation; Biodiversity inventories; Computer vision techniques; Data Biases; Informatics; Leaf recognition; Plant identification; Species identification; Deep learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153867506-9},
	language = {English},
	abbrev_source_title = {IEEE Int. Work Conf. Bioinspired Intell., IWOBI - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 2018 IEEE International Work Conference on Bioinspired Intelligence, IWOBI 2018; Conference date: 18 July 2018 through 20 July 2018; Conference code: 139802}
}

@ARTICLE{Zhang201861,
	author = {Zhang, Shanwen and Wang, Zhen and Shi, Yun},
	title = {Multi-modal Plant Leaf Recognition Based on Centroid-Contour Distance and Local Discriminant Canonical Correlation Analysis},
	year = {2018},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10955 LNCS},
	pages = {61 – 66},
	doi = {10.1007/978-3-319-95933-7_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052127553&doi=10.1007%2f978-3-319-95933-7_8&partnerID=40&md5=b66c3872c4cb06b530423cd7402a8536},
	affiliations = {Department of Information Engineering, Xijing University, Xi’an, 710123, China},
	abstract = {Leaf based plant species recognition plays an important research, but it is a challenging work because of the complexity and diversity of plant leaves. A multi-modal plant leaf recognition method is proposed based on centroid-contour distance (CCD) and local discriminant canonical correlation analysis (LDCCA). First, the CCD feature vector is extracted from each leaf image. Second, the extracted feature vectors of any two within-class leaves are integrated by LDCCA. Final, K-nearest neighbor classifier is applied to plant recognition. The experiment results on a public dataset validated the effectiveness of the proposed method. © Springer International Publishing AG, part of Springer Nature 2018.},
	author_keywords = {Centroid-contour distance; Feature extraction; Local discriminant canonical correlation analysis (LDCCA); Plant recognition},
	keywords = {Correlation methods; Feature extraction; Intelligent computing; Nearest neighbor search; Plants (botany); Canonical correlation analysis; Centroid-contour distance; Feature vectors; K-nearest neighbor classifier; Plant recognition; Plant species; Public dataset; Within class; Modal analysis},
	correspondence_address = {Z. Wang; Department of Information Engineering, Xijing University, Xi’an, 710123, China; email: wjdw716@163.com},
	editor = {Jo K. and Huang D. and Zhang X.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331995932-0},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 14th International Conference on Intelligent Computing, ICIC 2018; Conference date: 15 August 2018 through 18 August 2018; Conference code: 216939}
}

@CONFERENCE{Jiang2018130,
	author = {Jiang, Ji-song and Kim, Hak-Jin and Cho, Woo-Jae},
	title = {On-the-go Image Processing System for Spatial Mapping of Lettuce Fresh Weight in Plant Factory},
	year = {2018},
	volume = {51},
	number = {17},
	pages = {130 – 134},
	doi = {10.1016/j.ifacol.2018.08.075},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053120976&doi=10.1016%2fj.ifacol.2018.08.075&partnerID=40&md5=4b5cf8ca9fed9d07409ec2867f88eb0a},
	affiliations = {Dept. of Biosystems Engineering, Seoul National University, Seoul, South Korea},
	abstract = {Real-time monitoring of crop growth parameters in plant factory can provide useful information about accurate assessment of their growth status for precision crop management. Plant weight is one of the most important biophysical properties used to determine the optimum time for harvesting. Conventional plant weight measurements are destructive and laborious. An on-the-go image processing system consisting of image acquisition and weight estimation was developed to generate a fresh weight map of plants grown in hydroponic solutions. Key technologies developed in this study are real-time image processing and spatial mapping methods that estimate the fresh weights of individual lettuces. Images were automatically captured with a low cost web camera and processed using a MYRIO-based embedded controller. The camera and embedded system moved along an XY axis frame above a plant growing bed (0.94 x 1.8 m) using two stepping motors and linear actuators. The image preprocessing algorithm consisted of two main subroutines, i.e., image segmentation and target plant recognition. For the image segmentation, the S channel of the HSV color space and Otsu's threshold were used to separate the plants from the background. The target plant was identified based on location information of the growing bed holes in captured images. The plant weight was estimated using calibration equations previously developed that relate the pixel numbers of lettuce images to their actual fresh weights in conjunction with the use of a two-point normalization method. The accuracy of the fresh weight determined by the developed embedded system was confirmed by a highly linear relationship with a slope near 1.0 and a coefficient of determination (R2) of 0.95 with a processing time of within 4 s. In addition, it was possible to generate a spatial map of the fresh weights of lettuces grown in a cultivation bed, which could be used to estimate their yields prior to harvesting. © 2018},
	author_keywords = {Fresh weight; Image processing; Overlapping leaf; Plant factory; Plant growth; Real-time sensing},
	keywords = {Cameras; Crops; Cultivation; Embedded systems; Image processing; Linear actuators; Mapping; Plant life extension; Stepping motors; Fresh weight; Overlapping leaf; Plant factory; Plant growth; Real time sensing; Image segmentation},
	publisher = {Elsevier B.V.},
	issn = {24058963},
	language = {English},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access, Bronze Open Access}
}

@CONFERENCE{Muthevi2017870,
	author = {Muthevi, Anilkumar and Uppu, Ravi Babu},
	title = {Leaf classification using completed local binary pattern of textures},
	year = {2017},
	journal = {Proceedings - 7th IEEE International Advanced Computing Conference, IACC 2017},
	pages = {870 – 874},
	doi = {10.1109/IACC.2017.0178},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027061895&doi=10.1109%2fIACC.2017.0178&partnerID=40&md5=536b3749389f50a7fbb53352c54c7bed},
	affiliations = {Acharya Nagarjuna University, Department of Computer Science and Engineering, Aditya College of Engineering and Technology, Surampalem, AP, India; Department of Computer Science and Engineering, Narsimha Reddy Engineering College, Secunderabad, TS, India},
	abstract = {This paper, introduces utilizing the magnitude component of Local Binary Pattern (LBP) apart from sign component (which is considered as conventional method). We applied this Completed Local Binary Pattern (CLBP) on plant leaf classification by randomly taken divergent blocks of each texture data set. This approach is also useful for the identification of quality leaves for the automation of grading process in commercial crops like Tobacco etc. By combining Center pixel CLBP (CCLBP), Signed component of CLBP (SCLBP) and magnitude part of CLBP (MCLBP) there is a considerable development can be achieved for rotationally invariant texture classification. © 2017 IEEE.},
	author_keywords = {Feature selection; Image processing; Leaf Recognition; Local Binary Pattern; Rotation invariance; Texture classification},
	keywords = {Bins; Classification (of information); Feature extraction; Grading; Plants (botany); Commercial crops; Conventional methods; Leaf classification; Leaf recognition; Local binary patterns; Plant leaf classifications; Rotation invariance; Texture classification; Image processing},
	editor = {PadmaSai Y. and Garg D.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150901560-3},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Adv. Comput. Conf., IACC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; Conference name: 7th IEEE International Advanced Computing Conference, IACC 2017; Conference date: 5 January 2017 through 7 January 2017; Conference code: 129044}
}

@ARTICLE{Lee20171,
	author = {Lee, Sue Han and Chan, Chee Seng and Mayo, Simon Joseph and Remagnino, Paolo},
	title = {How deep learning extracts and learns leaf features for plant classification},
	year = {2017},
	journal = {Pattern Recognition},
	volume = {71},
	pages = {1 – 13},
	doi = {10.1016/j.patcog.2017.05.015},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023164668&doi=10.1016%2fj.patcog.2017.05.015&partnerID=40&md5=2e5e4de011ff55072876613cb81b2472},
	affiliations = {Centre of Image & Signal Processing, Faculty of Computer Science and Information Technology, University of Malaya, Malaysia; Herbarium, Royal Botanic Gardens, TW9 3AE, United Kingdom; Faculty of Science, Engineering and Computing, Kingston University, KT1 2EE, United Kingdom},
	abstract = {Plant identification systems developed by computer vision researchers have helped botanists to recognize and identify unknown plant species more rapidly. Hitherto, numerous studies have focused on procedures or algorithms that maximize the use of leaf databases for plant predictive modeling, but this results in leaf features which are liable to change with different leaf data and feature extraction techniques. In this paper, we learn useful leaf features directly from the raw representations of input data using Convolutional Neural Networks (CNN), and gain intuition of the chosen features based on a Deconvolutional Network (DN) approach. We report somewhat unexpected results: (1) different orders of venation are the best representative features compared to those of outline shape, and (2) we observe multi-level representation in leaf data, demonstrating the hierarchical transformation of features from lower-level to higher-level abstraction, corresponding to species classes. We show that these findings fit with the hierarchical botanical definitions of leaf characters. Through these findings, we gained insights into the design of new hybrid feature extraction models which are able to further improve the discriminative power of plant classification systems. The source code and models are available at: https://github.com/cs-chan/Deep-Plant. © 2017 Elsevier Ltd},
	author_keywords = {Deep learning; Feature visualisation; Plant recognition},
	keywords = {Education; Extraction; Feature extraction; Metadata; Neural networks; Convolutional neural network; Discriminative power; Feature extraction techniques; Higher-level abstraction; Hybrid-feature extraction; Plant classification; Plant identification systems; Plant recognition; Deep learning},
	correspondence_address = {C.S. Chan; Centre of Image & Signal Processing, Faculty of Computer Science and Information Technology, University of Malaya, Malaysia; email: cs.chan@um.edu.my},
	publisher = {Elsevier Ltd},
	issn = {00313203},
	coden = {PTNRA},
	language = {English},
	abbrev_source_title = {Pattern Recogn.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 264; All Open Access, Green Open Access}
}

@ARTICLE{Yousefi201770,
	author = {Yousefi, Ehsan and Baleghi, Yasser and Sakhaei, Sayed Mahmoud},
	title = {Rotation invariant wavelet descriptors, a new set of features to enhance plant leaves classification},
	year = {2017},
	journal = {Computers and Electronics in Agriculture},
	volume = {140},
	pages = {70 – 76},
	doi = {10.1016/j.compag.2017.05.031},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020279289&doi=10.1016%2fj.compag.2017.05.031&partnerID=40&md5=d0cd8c50cab8539a63a1e891f18ca22e},
	affiliations = {Department of Electrical & Computer Engineering, Babol Noshirvani University of Technology, Babol, Iran},
	abstract = {Automatic plant leaf recognition can play an important role in plant classification due to leaf's availability, stable features and good potential to discriminate different kinds of species. Amongst many leaf features like leaf venation, margin, texture and lamina, leaf shape is the most important one due to its better discriminative power and ease of analysis. One of the most common leaf shape descriptors is Elliptic Fourier Descriptor (EFD). In this paper a new shape descriptor is introduced as “Rotation Invariant Wavelet Descriptor” (RIWD). The performance of RIWD is compared with IEFD using Flavia dataset. MLP neural network is used as the classifier in this work. Results analysis shows better performance of the proposed feature in classification accuracy. Furthermore, an optimum feature vector is constructed using a set of textural and morphological features and the RIWD that reached 97.5% classification accuracy with low computational cost in comparison with many reported results in Flavia dataset. © 2017 Elsevier B.V.},
	author_keywords = {Computer vision; Elliptic Fourier Descriptors; Feature extraction; Leaf recognition; Plant identification},
	keywords = {Computer vision; Feature extraction; Plants (botany); Textures; Classification accuracy; Elliptic Fourier descriptor; Elliptic fourier descriptors; Leaf recognition; Morphological features; Plant classification; Plant identification; Plant leaves classifications; accuracy assessment; agricultural technology; classification; computer vision; data set; leaf morphology; wavelet analysis; Classification (of information)},
	correspondence_address = {Y. Baleghi; email: y.baleghi@nit.ac.ir},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26}
}

@CONFERENCE{George20182216,
	author = {George, Juby and Raj, S. Gladston},
	title = {Leaf recognition using multi-layer perceptron},
	year = {2018},
	journal = {2017 International Conference on Energy, Communication, Data Analytics and Soft Computing, ICECDS 2017},
	pages = {2216 – 2221},
	doi = {10.1109/ICECDS.2017.8389846},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050113650&doi=10.1109%2fICECDS.2017.8389846&partnerID=40&md5=c12f61569e8600a2c4087f0309e01ca1},
	affiliations = {School of Computer Science, Mahatma Gandhi University, Kottayam Kottayam, 686560, India; Department of Computer Science Govt. College, Nedumangad, Thiruvananthapuram, Kerala, India},
	abstract = {In this work, we employ multi-layer feed forward network algorithm for leaf recognition using its shape, color and vein features. The leaf images are pre-processed and segmented. The shape features like area, convex area, diameter, length, width, perimeter, eccentricity, solidity, major axis length and Minor axis length are extracted by taking the features from the segmented leaf image. The mean, standard deviation, skewness and kurtosis for red, green and blue color features are extracted. Wiener filtering and canny edge detection are used to identify the vein features v1, v2, v3, v4 at different thresholds by calculating gray level value from the gray level histogram. The features are then exposed to Principal Component Analysis (PCA) for feature reduction and the resultant reduced feature set contains five shape features, six color features and three vein features. The features are then taken as input vectors for multi-layer neural network. Total 150 leaves from Columbia leaf image database are taken as samples. 150 leaves represent 10 different leaf species. The system was trained with 104 leaf images and was tested and validated using 23 leaf images each. The success rate of reduced feature set of leaf shapes and color was 84.7% and by combining leaf shape, color and vein features outputted a success rate of 95.3%. © 2017 IEEE.},
	author_keywords = {Color features; Feature extraction; Multilayer feed forward network; Principal Component Analysis; Shape features; Vein features; Wiener filter},
	keywords = {Color; Edge detection; Higher order statistics; Image processing; Network layers; Principal component analysis; Soft computing; Color features; Multi-layer feed-forward networks; Shape features; Vein features; WIENER filters; Feature extraction},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153861886-8},
	language = {English},
	abbrev_source_title = {Int. Conf. Energy, Commun., Data Anal. Soft Comput., ICECDS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 2017 International Conference on Energy, Communication, Data Analytics and Soft Computing, ICECDS 2017; Conference date: 1 August 2017 through 2 August 2017; Conference code: 137325}
}

@ARTICLE{Abas201890,
	author = {Abas, Mohamad Aqib Haqmi and Ismail, Nurlaila and Yassin, Ahmad Ihsan Mohd and Taib, Mohd Nasir},
	title = {VGG16 for plant image classification with transfer learning and data augmentation},
	year = {2018},
	journal = {International Journal of Engineering and Technology(UAE)},
	volume = {7},
	number = {4},
	pages = {90 – 94},
	doi = {10.14419/ijet.v7i4.11.20781},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054363713&doi=10.14419%2fijet.v7i4.11.20781&partnerID=40&md5=56bbc5f1af9f959f288f837cf76375ce},
	affiliations = {Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, 40450, Malaysia},
	abstract = {This paper discusses the potential of applying VGG16 model architecture for plant classification. Flower images are used instead of leaves as in other plant recognition model because the structure of shape and color of leaves are similar in nature. This might be disadvantageous when we want to use only leaves images as a sole feature of plants to classify the species. Previous work has demonstrated the effectiveness of using transfer learning, dropout and data augmentation as a method to reduce overfitting problem of convolutional neural network model when applied in limited amount of images data. We have successfully build and train the VGG16 model with 2800 flower images. The model able to achieve a classification accuracy score of 96.25% for training set, 93.93% for validation set and 89.96% for testing set. © 2018 Authors.},
	author_keywords = {Convolutional neural network; Data augmentation; Deep learning; Transfer learning, dropout},
	correspondence_address = {M.A.H. Abas; Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, 40450, Malaysia; email: Mohdaqib93@yahoo.com},
	publisher = {Science Publishing Corporation Inc},
	issn = {2227524X},
	language = {English},
	abbrev_source_title = {Int. J. Eng. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26; All Open Access, Bronze Open Access}
}

@ARTICLE{Tharwat201715,
	author = {Tharwat, Alaa and Gaber, Tarek and Hassanien, Aboul Ella},
	title = {One-dimensional vs. two-dimensional based features: Plant identification approach},
	year = {2017},
	journal = {Journal of Applied Logic},
	volume = {24},
	pages = {15 – 31},
	doi = {10.1016/j.jal.2016.11.021},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030544445&doi=10.1016%2fj.jal.2016.11.021&partnerID=40&md5=c98a932d61c6dcc73fd9a55db116ac86},
	affiliations = {Faculty of Engineering, Suez Canal University, Egypt; Scientific Research Group in Egypt (SRGE), Egypt; Faculty of Computers and Informatics, Suez Canal University, Ismailia, Egypt; Faculty of Computers and Information, Cairo University, Egypt; Faculty of Computers and Informatics, Suez Canal University, Ismailia, Egypt},
	abstract = {The number of endangered species has been increased due to shifts in the agricultural production, climate change, and poor urban planning. This has led to investigating new methods to address the problem of plant species identification/classification. In this paper, a plant identification approach using 2D digital leaves images was proposed. The approach used two features extraction methods based on one-dimensional (1D) and two-dimensional (2D) and the Bagging classifier. For the 1D-based methods, Principal Component Analysis (PCA), Direct Linear Discriminant Analysis (DLDA), and PCA + LDA techniques were applied, while 2DPCA and 2DLDA algorithms were used for the 2D-based method. To classify the extracted features in both methods, the Bagging classifier, with the decision tree as a weak learner was used. The five variants, i.e. PCA, PCA + LDA, DLDA, 2DPCA, and 2DLDA, of the approach were tested using the Flavia public dataset which consists of 1907 colored leaves images. The accuracy of these variants was evaluated and the results showed that the 2DPCA and 2DLDA methods were much better than using the PCA, PCA + LDA, and DLDA. Furthermore, it was found that the 2DLDA method was the best one and the increase of the weak learners of the Bagging classifier yielded a better classification accuracy. Also, a comparison with the most related work showed that our approach achieved better accuracy under the same dataset and same experimental setup. © 2016 Elsevier B.V.},
	author_keywords = {2DLDA; 2DPCA; Bagging classifier; Direct-LDA; Leaf image; Linear Discriminant Analysis (LDA); PCA + LDA; Plant identification; Principal component analysis; Small Sample Size (SSS)},
	keywords = {Agriculture; Climate change; Conservation; Decision trees; Discriminant analysis; Face recognition; Image processing; Learning systems; Plants (botany); 2DLDA; 2DPCA; Direct-LDA; Leaf images; Linear discriminant analysis; PCA+LDA; Plant identification; Small Sample Size; Principal component analysis},
	correspondence_address = {T. Gaber; Scientific Research Group in Egypt (SRGE), Egypt; email: tmgaber@gmail.com},
	publisher = {Elsevier Ltd},
	issn = {15708683},
	language = {English},
	abbrev_source_title = {J. Appl. Logic},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Bronze Open Access}
}

@ARTICLE{Liu2018,
	author = {Liu, Xuanxin and Xu, Fu and Sun, Yu and Zhang, Haiyan and Chen, Zhibo},
	title = {Convolutional recurrent neural networks for observation-centered plant identification},
	year = {2018},
	journal = {Journal of Electrical and Computer Engineering},
	volume = {2018},
	doi = {10.1155/2018/9373210},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046261473&doi=10.1155%2f2018%2f9373210&partnerID=40&md5=4361689b5d8d4e969fe31157c61d930f},
	affiliations = {School of Information Science and Technology, Beijing Forestry University, Beijing, 100083, China},
	abstract = {Traditional image-centered methods of plant identification could be confused due to various views, uneven illuminations, and growth cycles. To tolerate the significant intraclass variances, the convolutional recurrent neural networks (C-RNNs) are proposed for observation-centered plant identification to mimic human behaviors. The C-RNN model is composed of two components: The convolutional neural network (CNN) backbone is used as a feature extractor for images, and the recurrent neural network (RNN) units are built to synthesize multiview features from each image for final prediction. Extensive experiments are conducted to explore the best combination of CNN and RNN. All models are trained end-to-end with 1 to 3 plant images of the same observation by truncated back propagation through time. The experiments demonstrate that the combination of MobileNet and Gated Recurrent Unit (GRU) is the best trade-off of classification accuracy and computational overhead on the Flavia dataset. On the holdout test set, the mean 10-fold accuracy with 1, 2, and 3 input leaves reached 99.53%, 100.00%, and 100.00%, respectively. On the BJFU100 dataset, the C-RNN model achieves the classification rate of 99.65% by two-stage end-to-end training. The observation-centered method based on the C-RNNs shows potential to further improve plant identification accuracy. © 2018 Xuanxin Liu et al.},
	keywords = {Backpropagation; Behavioral research; Classification (of information); Convolution; Economic and social effects; Image processing; Plants (botany); Back-propagation through time; Classification accuracy; Classification rates; Computational overheads; Convolutional Neural Networks (CNN); Plant identification; Recurrent neural network (RNN); Uneven illuminations; Recurrent neural networks},
	correspondence_address = {F. Xu; School of Information Science and Technology, Beijing Forestry University, Beijing, 100083, China; email: xufu@bjfu.edu.cn},
	publisher = {Hindawi Limited},
	issn = {20900147},
	language = {English},
	abbrev_source_title = {J. Electr. Comput. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Khmag2018467,
	author = {Khmag, Asem and Al-Haddad, S.A.R. and Kamarudin, Noraziahtulhidayu},
	title = {Recognition system for leaf images based on its leaf contour and centroid},
	year = {2018},
	journal = {IEEE Student Conference on Research and Development: Inspiring Technology for Humanity, SCOReD 2017 - Proceedings},
	volume = {2018-January},
	pages = {467 – 472},
	doi = {10.1109/SCORED.2017.8305438},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048975702&doi=10.1109%2fSCORED.2017.8305438&partnerID=40&md5=42f8c554b4a35656b74db9569a516923},
	affiliations = {Computer System Engineering, Faculty of Engineering, University of Zawia, Az zawia, Libyan Arab Jamahiriya; Computer and Communication Engineering, Universiti Putra Malaysia, Serdang, Malaysia},
	abstract = {The recognition of plants is directly associated to society's life. Leaves from plants are proved to be a feasible source of information used to identify plant species. The recognition system of leaves is accomplished automatically using the experts of human being. Unfortunately, it has their loopholes that are a time consuming processes and low-effectiveness progression. The leaves classification using predictable process is quite complicated, time complexity, and as a result of using very long-Termed in botanical science for non-experts that make it more irritated operation. Thus, the prompt developments in digital images, computer vision and object detection and recognition systems encourage scientists to work towards plant species recognition according to image processing technology. In this study, an image processing algorithm in order to find out the shape structure of tested plants is presented. This technique exploits the variant to scaling shift, spin technique, scaling approach, and filtering processes. The leaf contours of the same plants are computed using Support Victor Machine (SVM) where the similar sequences of the same contours usually carry the same features while the different plants sequences have different contours. In this regard, SVM classifier is exploited to be applied as a classifier to the plant's leaf. In the Experiment part, the finding was taken from Flavia dataset and it demonstrated that the suggested technique has high recognition efficiency compared to state of the art methods and is shows better quality images especially in complicated features of digital images such as ridges, edges, lines, curves and complicated contours. © 2017 IEEE.},
	author_keywords = {Feature extraction; Image classification; Image contours; Leaf recognition; Support vector machine},
	keywords = {Feature extraction; Image classification; Object detection; Object recognition; Support vector machines; Image contour; Image processing algorithm; Image processing technology; Leaf recognition; Object detection and recognition; Recognition efficiency; Recognition systems; State-of-the-art methods; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153862126-4},
	language = {English},
	abbrev_source_title = {IEEE Stud. Conf. Res. Dev.: Inspiring Technol. Hum., SCOReD - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; Conference name: 15th IEEE Student Conference on Research and Development, SCOReD 2017; Conference date: 13 December 2017 through 14 December 2017; Conference code: 135011; All Open Access, Green Open Access}
}

@ARTICLE{Sabri2018141,
	author = {Sabri, Nurbaity and Aziz, Zalilah Abdul and Ibrahim, Zaidah and Rosni, Muhammad Akmal Rasydan Bin Mohd and Abd Ghapul, Abdul Hafiz bin},
	title = {Comparing Convolution Neural Network models for leaf recognition},
	year = {2018},
	journal = {International Journal of Engineering and Technology(UAE)},
	volume = {7},
	number = {3},
	pages = {141 – 144},
	doi = {10.14419/ijet.v7i3.15.17518},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067270877&doi=10.14419%2fijet.v7i3.15.17518&partnerID=40&md5=16a3595880852227e643d521234aaaeb},
	affiliations = {Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Campus Jasin, Melaka, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia},
	abstract = {This research compares the recognition performance between pretrained models, GoogLeNet and AlexNet, with basic Convolution Neural Network (CNN) for leaf recognition. Lately, CNN has gained a lot of interest in image processing applications. Numerous pretrained models have been introduced and the most popular pretrained models are GoogLeNet and AlexNet. Each model has its own layers of convolution and computational complexity. A great success has been achieved using these classification models in computer vision and this research investigates their performances for leaf recognition using MalayaKew (MK), an open access leaf dataset. GoogLeNet achieves a perfect 100% accuracy, outperforms both AlexNet and basic CNN. On the other hand, the processing time for GoogLeNet is longer compared to the other models due to the high number of layers in its architecture. © 2018 Authors.},
	author_keywords = {AlexNet; CNN; GoogLeNet; Leaf recognition},
	correspondence_address = {N. Sabri; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Campus Jasin, Melaka, Malaysia; email: nurbaity_sabri@melaka.uitm.edu.my},
	publisher = {Science Publishing Corporation Inc},
	issn = {2227524X},
	language = {English},
	abbrev_source_title = {Int. J. Eng. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Bronze Open Access}
}

@CONFERENCE{Lasseck2018,
	author = {Lasseck, Mario},
	title = {Machines vs. human experts: Contribution to the ExpertLifeCLEF 2018 plant identification task},
	year = {2018},
	journal = {CEUR Workshop Proceedings},
	volume = {2125},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051090004&partnerID=40&md5=7a4b52d9e9792dc372901a10a71c9e0f},
	affiliations = {Museum fuer Naturkunde Berlin, Germany},
	abstract = {This paper presents deep learning techniques for image-based plant identification at very large scale. Deep Convolutional Neural Networks (DCNNs) for image classification are fine-tuned to identify 10, 000 species. The proposed approach is evaluated in the ExpertLifeCLEF 2018 campaign. Results are compared with identifications by human experts and the best models of the LifeCLEF 2017 plant identification task. Performance comes close to the most advanced human expertise and surpasses previous state-of-the-art by 6 % on the official test set.},
	author_keywords = {Biodiversity; Data Augmentation; Deep Convolutional Neural Networks; Deep Learning; Image Classification; Plant Species Identification},
	keywords = {Biodiversity; Convolution; Deep learning; Image classification; Neural networks; Data augmentation; Deep convolutional neural networks; Human expert; Human expertise; Learning techniques; Plant identification; Plant species identification; State of the art; Deep neural networks},
	correspondence_address = {M. Lasseck; Museum fuer Naturkunde Berlin, Germany; email: Mario.Lasseck@mfn.Berlin},
	editor = {Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Nie J.-Y. and Soulier L. and Soulier L. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 19th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2018; Conference date: 10 September 2018 through 14 September 2018; Conference code: 138100}
}

@ARTICLE{Nasir201849,
	author = {Nasir, Ahmad Fakhri Ab and Rahman, M Nordin A and Mat, Nashriyah and Mamat, A Rasid and Ghani, Ahmad Shahrizan Abdul},
	title = {Image pre-processing algorithm for Ficus deltoidea Jack (Moraceae) varietal recognition: A repeated perpendicular line scanning approach},
	year = {2018},
	journal = {International Journal of Engineering and Technology(UAE)},
	volume = {7},
	number = {2},
	pages = {49 – 53},
	doi = {10.14419/ijet.v7i2.15.11211},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045425899&doi=10.14419%2fijet.v7i2.15.11211&partnerID=40&md5=24614d6a71b9b46822012d002185a06b},
	affiliations = {Faculty of Manufacturing Engineering, Universiti Malaysia Pahang, Pekan, Pahang, 26600, Malaysia; Faculy of Informatics and Computing, Universiti Sultan Zainal Abidin, Tembila Campus, Besut, Terengganu, 22000, Malaysia; Faculty of Bioresources and Food Industry, Universiti Sultan Zainal Abidin, Tembila Campus, Besut, Terengganu, 22000, Malaysia},
	abstract = {Image pre-processing task is always the first crucial step in plant species recognition system which is responsible to keep precision of feature measurement process. Some of researchers have developed the image pre-processing algorithm to remove petiole section. However, the algorithm was developed using semiautomatic algorithm which is strongly believed to give an inaccurate feature measurement. In this paper, a new technique of automatic petiole section removal is proposed based on repeated perpendicular petiole length scanning concept. Four phases of petiole removal technique involved are: i) binary image enhancement, ii) boundary binary image contour tracing, iii) petiole section scanning, and iv) optimal image size retaining and cropping. The experiments are conducted using six varieties of Ficus deltoidea Jack (Moraceae) leaves. The experimental results indicate that the segmentation results are acceptably good since the digital leaf images have less than 1% of segmentation errors on several ground truth images. © 2018 Authors.},
	author_keywords = {Ficus deltoidea Jack; Image pre-processing; Image processing; Leaf recognition; Plant species recognition},
	correspondence_address = {A.F.A. Nasir; Faculty of Manufacturing Engineering, Universiti Malaysia Pahang, Pekan, Pahang, 26600, Malaysia; email: afakhri@ump.edu.my},
	publisher = {Science Publishing Corporation Inc},
	issn = {2227524X},
	language = {English},
	abbrev_source_title = {Int. J. Eng. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Bronze Open Access}
}

@CONFERENCE{Do2017191,
	author = {Do, Thanh-Binh and Nguyen, Huy-Hoang and Nguyen, Thi-Thanh-Nhan and Vu, Hai and Tran, Thi-Thanh-Hai and Le, Thi-Lan},
	title = {Plant identification using score-based fusion of multi-organ images},
	year = {2017},
	journal = {Proceedings - 2017 9th International Conference on Knowledge and Systems Engineering, KSE 2017},
	volume = {2017-January},
	pages = {191 – 196},
	doi = {10.1109/KSE.2017.8119457},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043717956&doi=10.1109%2fKSE.2017.8119457&partnerID=40&md5=b137ceed289211143681bd44cdb4a03c},
	affiliations = {International Research Institute MICA, HUST, CNRS, UMI-2954, GRENOBLE INP, Hanoi, Viet Nam; School of Information and Communication Technology, HUST, Hanoi, Viet Nam; University of Information and Communication Technology, Thai Nguyen University, ThaiNguyen, Viet Nam},
	abstract = {This paper describes a fusion technique for species identification from images of different plant organs. Given a series of image organs such as branch, entire, flower or leaf, we firstly extract confidence scores for each single organ using a state-of-the-art deep convolutional neural network (CNN). After that, we deploy various schemes of the fusion approaches including not only conventional transformation-based approaches (sum rule, max rule, product rule) but also a classification-based approach (support vector machine). Then we proposed a hybrid fusion model. To measure the performances of the combination schemes, a large number of images of 50 species which are collected from two main resources: PlantCLEF 2015 dataset and the Internet resources. The experiment exhibits the dominant results of the fusion techniques compared with those of individual organs. At rank-1, the highest accuracy of a single organ is 73.0% for flower images, whereas by applying our proposed fusion technique for leaf and flower, the accuracy reaches to 89.8%. © 2017 IEEE.},
	author_keywords = {Convolutional Neural Network; Deep Learning; Fusion; Plant Identification},
	keywords = {Convolution; Deep learning; Deep neural networks; Fusion reactions; Image processing; Neural networks; Systems engineering; Convolutional neural network; Deep convolutional neural networks; Fusion approaches; Fusion techniques; Internet resources; Plant identification; Species identification; Transformation based; Image fusion},
	editor = {Phan X.-H. and Nguyen T.-T. and Tojo S. and Nguyen L.-M. and Le A.-P.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153863576-6},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Knowl. Syst. Eng., KSE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; Conference name: 9th International Conference on Knowledge and Systems Engineering, KSE 2017; Conference date: 19 October 2017 through 21 October 2017; Conference code: 132794}
}

@CONFERENCE{Selda201740,
	author = {Selda, Jesse Dave S. and Ellera, Roi Martin R. and Cajayon, Leandro C. and Linsangan, Noel B.},
	title = {Plant identification by image processing of leaf veins},
	year = {2017},
	journal = {ACM International Conference Proceeding Series},
	volume = {Part F131372},
	pages = {40 – 44},
	doi = {10.1145/3132300.3132315},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034452262&doi=10.1145%2f3132300.3132315&partnerID=40&md5=1186b92122565ce24d452f0172c84358},
	affiliations = {School of EECE, Mapua University, Manila, Philippines},
	abstract = {This study focuses on building a portable device capable of plant identification by image processing of leaf veins using Raspberry pi. The devise that the study will develop can help professionals in the field of Botany and Biology. It can be used by Pathologist, plant breeders and Doctors that specializes on medicinal plants. This study will concentrate on identifying the plant using its leaf veins. Its concentration is on simple leaf type. Twenty different kinds of plant leaves will be tested with 10 trials per leaf. Each leaf image will have its own labeled folder in the database that is created automatically after registration of leaf image. Series of algorithms were used for the leaf recognition method. Scale- Invariant Feature Transform (SIFT) extraction will be used to describe and detect the local features of the recognized leaf vein image. With the use of Support Vectors Machines (SVM), the matrix produced by the SIFT will be used to classify the correct plant to be shown on the Liquid Crystal Display (LCD) screen as the output containing the plant name, description and image. Initial results showed accuracy rate of 84.29% while the error rate was 15.71%. © 2017 Association for Computing Machinery.},
	author_keywords = {Leaf Veins; Python; SIFT; Simple Leaf; SVM},
	keywords = {Liquid crystal displays; Plants (botany); Signal processing; Leaf Veins; Liquid crystal display screens; Plant identification; Python; Scale invariant feature transforms; SIFT; Simple Leaf; Support vectors machine; Image processing},
	correspondence_address = {N.B. Linsangan; School of EECE, Mapua University, Manila, Philippines; email: nblinsangan@mapua.edu.ph},
	publisher = {Association for Computing Machinery},
	isbn = {978-145035289-5},
	language = {English},
	abbrev_source_title = {ACM Int. Conf. Proc. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; Conference name: 2017 International Conference on Imaging, Signal Processing and Communication, ICISPC 2017; Conference date: 26 July 2017 through 28 July 2017; Conference code: 131372}
}

@ARTICLE{Zhu2018223,
	author = {Zhu, Xiaolong and Zhu, Meng and Ren, Honge},
	title = {Method of plant leaf recognition based on improved deep convolutional neural network},
	year = {2018},
	journal = {Cognitive Systems Research},
	volume = {52},
	pages = {223 – 233},
	doi = {10.1016/j.cogsys.2018.06.008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050745236&doi=10.1016%2fj.cogsys.2018.06.008&partnerID=40&md5=96faced2ff1af3b2723817ddb40cf8f2},
	affiliations = {College of Information and Computer Engineering, Northeast Forestry University, Harbin, Heilongjiang, 150040, China; Forestry Intelligent Equipment Engineering Research Center, Harbin, Heilongjiang, 150040, China},
	abstract = {The identification of plant species mainly depends on the recognition of plant leaf characteristics. However, most recognition systems show the weak performance on detecting small objects like plant leaves in the complicated background. In order to improve the recognition ability of plant leaves in the complex environment, this paper proposes an improved deep convolutional neural network, which takes advantage of the Inception V2 with batch normalization (BN) instead of convolutional neural layers in the faster region convolutional neural network (Faster RCNN) offering multiscale image features to the region proposal network (RPN). In addition, the original images first are cut into the specified size according to the numerical order, and the segmented images are loaded into the proposed network sequentially. After the precise classification through softmax and bounding box regressor, the segmented images with identification labels are spliced together as final output images. The experimental results show that the proposed approach has higher recognition accuracy than Faster RCNN in recognizing leaf species in the complex background. © 2018 Elsevier B.V.},
	author_keywords = {Complex background; Convolutional neural network; Deep learning; Leaf recognition},
	keywords = {Complex networks; Convolution; Deep learning; Image enhancement; Image segmentation; Neural networks; Object detection; Plants (botany); Complex background; Complex environments; Convolutional neural network; Deep convolutional neural networks; Leaf recognition; Recognition abilities; Recognition accuracy; Recognition systems; article; learning; molecular recognition; nervous system; plant leaf; Deep neural networks},
	correspondence_address = {H. Ren; College of Information and Computer Engineering, Northeast Forestry University, Harbin, 150040, China; email: nefu_rhe@163.com},
	publisher = {Elsevier B.V.},
	issn = {13890417},
	coden = {CSROA},
	language = {English},
	abbrev_source_title = {Cogn. Sys. Res.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 61}
}

@CONFERENCE{Chang2018,
	author = {Chang, Siow-Wee and Tan, Jing Wei and Murat, Miraemiliana and Yap, Hwa Jen},
	title = {Automated tropical plant species identification using deep learning and leaf shape approach},
	year = {2018},
	journal = {Proceedings of International Conference on Computers and Industrial Engineering, CIE},
	volume = {2018-December},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061315518&partnerID=40&md5=49d85179b44de2842a47e72e0578cf9a},
	affiliations = {Institute of Biological Sciences, Faculty of Science, University of Malaya, Kuala Lumpur, Malaysia; Department of Mechanical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, Malaysia},
	abstract = {One of the main tasks for botanists is identifying the plant species, which is a matter of interest for laymen as well. The problems of plant species extinction are kept increasing day by day, thus it is crucial to prevent and reduce these matters in getting serious. Thus, either endangered or non-endangered plant species should be conserved and preserved through more efficient manners. The development of an automated plant identification system could aid the botanists and laymen in identifying plant species speedily and structurally in the era of biodiversity 4.0. In this study, 2,640 of leaf samples were collected from 88 tropical plant species (45 tropical shrubs and 43 tropical trees) in the University of Malaya, Malaysia. The leaf images were first pre-processed based on the feature extraction approaches which may include background noise removal, the region of interest segmentation and RGB to grey-scaled image conversion. An automated tropical plant species identification system is developed based on two feature extraction approaches, namely, deep learning approach and shape morphological descriptor, with Artificial Neural Network as the classifier. The results showed that the deep learning approach and shape morphological descriptor performed comparably well in both plain-background independent samples and complex-background independent samples, with both approaches are able to recognize the tropical plant species correctly within the top-10 most related species listed (accuracy>90%). In conclusion, the proposed automated tropical plant species identification system is able to identify the tropical plant species efficiently. It is hoped that this system could increase the interest of the public especially the young generation in the botanical study. Hence, this would help in contributing towards the conservation of tropical biodiversity. © 2018, Curran Associates Inc. All rights reserved.},
	author_keywords = {Artificial Neural Network; Deep learning; Feature extraction; Shape descriptor; Tropical plant species},
	keywords = {Automation; Biodiversity; Conservation; Deep neural networks; Extraction; Image segmentation; Tropics; Deep learning; Features extraction; Independent samples; Leaf shape; Learning approach; Morphological descriptors; Plant species; Plant species identification; Shape descriptors; Tropical plant species; Feature extraction},
	correspondence_address = {S.-W. Chang; Institute of Biological Sciences, Faculty of Science, University of Malaya, Kuala Lumpur, Malaysia; email: siowwee@um.edu.my},
	editor = {Xu X. and Zhong R.Y. and Dessouky M.I.},
	publisher = {Curran Associates Inc.},
	issn = {21648689},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Comput. Ind. Eng., CIE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 48th International Conference on Computers and Industrial Engineering, CIE 2018; Conference date: 2 December 2018 through 5 December 2018; Conference code: 144541}
}

@CONFERENCE{Gogul2017,
	author = {Gogul, I. and Kumar, V. Sathiesh},
	title = {Flower species recognition system using convolution neural networks and transfer learning},
	year = {2017},
	journal = {2017 4th International Conference on Signal Processing, Communication and Networking, ICSCN 2017},
	doi = {10.1109/ICSCN.2017.8085675},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039975634&doi=10.1109%2fICSCN.2017.8085675&partnerID=40&md5=11a8d4ff20b86b86316422fc9a682f6b},
	affiliations = {Department of Electronics Engineering, Madras Institute of Technology, Anna University, Chennai, 600044, India},
	abstract = {Automatic identification and recognition of medicinal plant species in environments such as forests, mountains and dense regions is necessary to know about their existence. In recent years, plant species recognition is carried out based on the shape, geometry and texture of various plant parts such as leaves, stem, flowers etc. Flower based plant species identification systems are widely used. While modern search engines provide methods to visually search for a query image that contains a flower, it lacks in robustness because of the intra-class variation among millions of flower species around the world. Hence in this proposed research work, a Deep learning approach using Convolutional Neural Networks (CNN) is used to recognize flower species with high accuracy. Images of the plant species are acquired using the built-in camera module of a mobile phone. Feature extraction of flower images is performed using a Transfer Learning approach (i.e. extraction of complex features from a pre-trained network). A machine learning classifier such as Logistic Regression or Random Forest is used on top of it to yield a higher accuracy rate. This approach helps in minimizing the hardware requirement needed to perform the computationally intensive task of training a CNN. It is observed that, CNN combined with Transfer Learning approach as feature extractor outperforms all the handcrafted feature extraction methods such as Local Binary Pattern (LBP), Color Channel Statistics, Color Histograms, Haralick Texture, Hu Moments and Zernike Moments. CNN combined with Transfer Learning approach yields impressive Rank-1 accuracies of 73.05%, 93.41% and 90.60% using OverFeat, Inception-v3 and Xception architectures, respectively as Feature Extractors on FLOWERS102 dataset. © 2017 IEEE.},
	author_keywords = {Artificial Intelligence; Convolutional Neural Networks; Deep Learning; Flower Recognition; Transfer Learning},
	keywords = {Artificial intelligence; Automation; Convolution; Decision trees; Deep learning; Extraction; Feature extraction; Learning systems; Neural networks; Plants (botany); Signal processing; Tellurium compounds; Automatic identification; Convolution neural network; Convolutional neural network; Feature extraction methods; Flower recognition; Plant species identification; Species recognition systems; Transfer learning; Search engines},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150904740-6},
	language = {English},
	abbrev_source_title = {Int. Conf. Signal Process., Commun. Netw., ICSCN},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 93; Conference name: 4th International Conference on Signal Processing, Communication and Networking, ICSCN 2017; Conference date: 16 March 2017 through 18 March 2017; Conference code: 131913}
}

@CONFERENCE{Carpentier20181075,
	author = {Carpentier, Mathieu and Giguere, Philippe and Gaudreault, Jonathan},
	title = {Tree Species Identification from Bark Images Using Convolutional Neural Networks},
	year = {2018},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	pages = {1075 – 1081},
	doi = {10.1109/IROS.2018.8593514},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062992968&doi=10.1109%2fIROS.2018.8593514&partnerID=40&md5=469da5c2587b75bbeeef9590d5ff1a55},
	abstract = {Tree species identification using bark images is a challenging problem that could prove useful for many forestry related tasks. However, while the recent progress in deep learning showed impressive results on standard vision problems, a lack of datasets prevented its use on tree bark species classification. In this work, we present, and make publicly available, a novel dataset called BarkNet 1.0 containing more than 23,000 high-resolution bark images from 23 different tree species over a wide range of tree diameters. With it, we demonstrate the feasibility of species recognition through bark images, using deep learning. More specifically, we obtain an accuracy of 93.88% on single crop, and an accuracy of 97.81 % using a majority voting approach on all of the images of a tree. We also empirically demonstrate that, for a fixed number of images, it is better to maximize the number of tree individuals in the training database, thus directing future data collection efforts. © 2018 IEEE.},
	keywords = {Classification (of information); Deep learning; Forestry; Intelligent robots; Neural networks; Convolutional neural network; Number of trees; Recent progress; Species classification; Species recognition; Training database; Tree species identifications; Voting approach; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {21530858},
	isbn = {978-153868094-0},
	coden = {85RBA},
	language = {English},
	abbrev_source_title = {IEEE Int Conf Intell Rob Syst},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 37; Conference name: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2018; Conference date: 1 October 2018 through 5 October 2018; Conference code: 144267; All Open Access, Green Open Access}
}

@ARTICLE{Gonçalves201812,
	author = {Gonçalves, Filipe Marcel Fernandes and Guilherme, Ivan Rizzo and Pedronette, Daniel Carlos Guimarães},
	title = {Semantic Guided Interactive Image Retrieval for plant identification},
	year = {2018},
	journal = {Expert Systems with Applications},
	volume = {91},
	pages = {12 – 26},
	doi = {10.1016/j.eswa.2017.08.035},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028510372&doi=10.1016%2fj.eswa.2017.08.035&partnerID=40&md5=913084620d4a4929529f7207795b0701},
	affiliations = {Department of Statistics, Applied Mathematics and Computing (DEMAC), State University of São Paulo (UNESP), Av. 24-A, 1515, Rio Claro, 13506-900, SP, Brazil},
	abstract = {A lot of images are currently generated in many domains, requiring specialized knowledge of identification and analysis. From one standpoint, many advances have been accomplished in the development of image retrieval techniques based on visual image properties. However, the semantic gap between low-level features and high-level concepts still represents a challenging scenario. On another standpoint, knowledge has also been structured in many fields by ontologies. A promising solution for bridging the semantic gap consists in combining the information from low-level features with semantic knowledge. This work proposes a novel graph-based approach denominated Semantic Interactive Image Retrieval (SIIR) capable of combining Content Based Image Retrieval (CBIR), unsupervised learning, ontology techniques and interactive retrieval. To the best of our knowledge, there is no approach in the literature that combines those diverse techniques like SIIR. The proposed approach supports expert identification tasks, such as the biologist's role in plant identification of Angiosperm families. Since the system exploits information from different sources as visual content, ontology, and user interactions, the user efforts required are drastically reduced. For the semantic model, we developed a domain ontology which represents the plant properties and structures, relating features from Angiosperm families. A novel graph-based approach is proposed for combining the semantic information and the visual retrieval results. A bipartite and a discriminative attribute graph allow a semantic selection of the most discriminative attributes for plant identification tasks. The selected attributes are used for formulating a question to the user. The system updates similarity information among images based on the user's answer, thus improving the retrieval effectiveness and reducing the user's efforts required for identification tasks. The proposed method was evaluated on the popular Oxford Flowers 17 and 102 Classes datasets, yielding highly effective results in both datasets when compared to other approaches. For example, the first five retrieved images for 17 classes achieve a retrieval precision of 97.07% and for 102 classes, 91.33%. © 2017 Elsevier Ltd},
	author_keywords = {Interactive image retrieval; Ontology; Semantic gap; Unsupervised learning},
	keywords = {Content based retrieval; Graphic methods; Image processing; Ontology; Search engines; Semantics; Unsupervised learning; Content-Based Image Retrieval; Expert identifications; Image retrieval techniques; Interactive image retrieval; Retrieval effectiveness; Semantic gap; Similarity informations; Specialized knowledge; Image retrieval},
	correspondence_address = {D.C.G. Pedronette; Department of Statistics, Applied Mathematics and Computing (DEMAC), State University of São Paulo (UNESP), Rio Claro, Av. 24-A, 1515, 13506-900, Brazil; email: daniel@rc.unesp.br},
	publisher = {Elsevier Ltd},
	issn = {09574174},
	coden = {ESAPE},
	language = {English},
	abbrev_source_title = {Expert Sys Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; All Open Access, Green Open Access}
}

@CONFERENCE{Srivastava2018626,
	author = {Srivastava, Vaibhava and Khunteta, Ajay},
	title = {Comparative Analysis of Leaf Classification and Recognition by Different SVM Classifiers},
	year = {2018},
	journal = {Proceedings of the International Conference on Inventive Research in Computing Applications, ICIRCA 2018},
	pages = {626 – 631},
	doi = {10.1109/ICIRCA.2018.8596813},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061482636&doi=10.1109%2fICIRCA.2018.8596813&partnerID=40&md5=d9bae72f3fc330203f1a5b8469bbc6e5},
	affiliations = {Department of Electronics and Communication, Rajasthan Technical University, Kota, India},
	abstract = {This paper presents the leaf shape extraction for the classification of plant. Leaf shape is an important parameter which are frequently used to identify and classify the plant species. Besides many challenges occurred in plant classification, it is very useful technique to study the livestock systems, conservation and ecology. The camera captures the image followed by segmenting the required leaves from the image and then a single leaf is segmented using OTSU's segmentation. A new approach is used to extract 14 features from the leaf using shape detector and classifying 16 different kind of plants with SVM classifier. Accuracies ranging from 89% to 91% are acquired which are similar to the earlier reported work. © 2018 IEEE.},
	author_keywords = {Leaf; Median filter; OTSU's segmentation; Pattern recognition; Shape feature; SVM},
	keywords = {Agriculture; Image segmentation; Median filters; Plants (botany); Comparative analysis; Leaf; Leaf classification; Livestock systems; New approaches; Plant classification; Shape features; SVM classifiers; Pattern recognition},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153862456-2},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Inven. Res. Comput. Appl., ICIRCA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; Conference name: 2018 International Conference on Inventive Research in Computing Applications, ICIRCA 2018; Conference date: 11 July 2018 through 12 July 2018; Conference code: 144196}
}

@CONFERENCE{Thanikkal2018518,
	author = {Thanikkal, Jibi G. and Dubey, Ashwani Kumar and Thomas, M.T.},
	title = {Advanced Plant Leaf Classification Through Image Enhancement and Canny Edge Detection},
	year = {2018},
	journal = {2018 7th International Conference on Reliability, Infocom Technologies and Optimization: Trends and Future Directions, ICRITO 2018},
	pages = {518 – 522},
	doi = {10.1109/ICRITO.2018.8748587},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069149429&doi=10.1109%2fICRITO.2018.8748587&partnerID=40&md5=1671d6b6c079380c91e6009a8a8b766b},
	affiliations = {Amity School of Engineering and Technology, Amity University, Noida, India; Department of Botany, St. Thomas College, Thrissur, Kerala, India},
	abstract = {Accuracy in identification of any plant is achieved by understanding and extracting the plant features. Image processing techniques has gained interest in identifying the plants in realist and accurate manner. Among them, Edge detection techniques has very important role in creation of database for plant identification. Edge filtering and optimization technique to create continuous edges makes Canny edge detection widely popular to retrieve image characteristics. The proposed contour based image segmentation process filter morphological features from plant leaves. Detailed vein and texture extraction of plant leave using Canny edge detector are explained. © 2018 IEEE.},
	author_keywords = {Canny edge detection; Image processing; Morphological features; Plants},
	keywords = {Edge detection; Image classification; Image processing; Image segmentation; Plants (botany); Textures; Canny edge detection; Image characteristics; Image processing technique; Image segmentation process; Morphological features; Optimization techniques; Plant leaf classifications; Plants; Image enhancement},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153864692-2},
	language = {English},
	abbrev_source_title = {Int. Conf. Reliab., Infocom Technol. Optim.: Trends Future Dir., ICRITO},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; Conference name: 7th International Conference on Reliability, Infocom Technologies and Optimization: Trends and Future Directions, ICRITO 2018; Conference date: 29 August 2018 through 31 August 2018; Conference code: 149167}
}

@CONFERENCE{Gulac2018,
	author = {Gulac, Fatih and Bayazit, Ulug},
	title = {Plant and Phenology Recognition from Field Images Using Texture and Color Features},
	year = {2018},
	journal = {2018 IEEE (SMC) International Conference on Innovations in Intelligent Systems and Applications, INISTA 2018},
	doi = {10.1109/INISTA.2018.8466300},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055530660&doi=10.1109%2fINISTA.2018.8466300&partnerID=40&md5=b245320b690b796cb859121de7e57c83},
	affiliations = {Computer Engineering Department, Istanbul Technical University, Istanbul, Turkey},
	abstract = {Determination of the phenological stages of plants is important for the growth of healthy and productive plants. The knowledge of transition times of phenological stages of a plant can provide valuable data for planning, organizing and timely execution of agricultural activities (spraying, irrigation etc.). TARBIL is an agricultural monitoring and information system that is founded and supported by Republic of Turkey Ministry of Food, Agriculture and Livestock. This system has a network of stations located in many parts of Turkey. Stations, that contain many sensors and cameras, periodically collect images and meteorological data from the agricultural fields. Previous works focus on either only about plant identification or only phenological stage recognition using only one texture analysis method. Our approachment to the problem is novel because not only the recognition of the plant type or the recognition of only the phenological stage, but also joint identification of the plant type and the phenological stages are provided with several texture and color feature analysis methods. In this work, a study is conducted to compare the use of several image texture features along with color features extracted from TARBIL field image data for the classification of the plants and their phenological stages. Experimental results show that HOG (Histograms of Oriented Gradients) yields the best performance among the texture features tested. © 2018 IEEE.},
	author_keywords = {agriculture; color; feature descriptors; image processing; plant phenology; texture},
	keywords = {Agriculture; Biology; Color; Color image processing; Forestry; Frost protection; Image processing; Intelligent systems; Meteorology; Textures; Agricultural activities; Agricultural monitoring; Feature descriptors; Histograms of oriented gradients; Joint identifications; Plant identification; Plant phenology; Texture analysis method; Image texture},
	editor = {Angelov P. and Yildirim T. and Iliadis L. and Manolopoulos Y.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153865150-6},
	language = {English},
	abbrev_source_title = {IEEE (SMC) Int. Conf. Innov. Intell. Syst. Appl., INISTA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2018 IEEE International Conference on Innovations in Intelligent Systems and Applications, INISTA 2018; Conference date: 3 July 2018 through 5 July 2018; Conference code: 139966}
}

@CONFERENCE{Wei20181350,
	author = {Wei, Liu and Li, Zhifang and Lin, Jialun and Liang, Danning},
	title = {The PSO-SVM-based method of the recognition of plant leaves},
	year = {2018},
	journal = {Proceedings - 15th IEEE International Symposium on Parallel and Distributed Processing with Applications and 16th IEEE International Conference on Ubiquitous Computing and Communications, ISPA/IUCC 2017},
	pages = {1350 – 1355},
	doi = {10.1109/ISPA/IUCC.2017.00205},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048359748&doi=10.1109%2fISPA%2fIUCC.2017.00205&partnerID=40&md5=cfe20113c8522dc2de8cc81c717887b8},
	affiliations = {Department of Medical Information, Hainan Medical University, Haikou, China},
	abstract = {Images of plant leaves are studied in the current paper, as a way to make recognition and classification of the plant species. First of all, the images are processed before extracting the shape features of the leaves. The leaves are then classified using the SVM (Support Vector Machine). After that, the SVM model's parameters are optimized using the PSO algorithm (Particle Swarm Optimization), which is also applied to design and optimize the classification algorithm as well as to make recognition and classification of the leaves. Finally, the Matlab simulation results prove that our method is valid. © 2017 IEEE.},
	author_keywords = {Feature extraction; Plant recognition; PSO; SVM},
	keywords = {Feature extraction; MATLAB; Particle swarm optimization (PSO); Support vector machines; Ubiquitous computing; Classification algorithm; Matlab simulations; Plant leaves; Plant recognition; Plant species; PSO algorithms; Shape features; SVM(support vector machine); Plants (botany)},
	editor = {Martinez G. and Hill R. and Fox G. and Mueller P. and Wang G.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153863790-6},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Symp. Parallel Distrib. Process. Appl. IEEE Int. Conf. Ubiquitous Comput. Commun., ISPA/IUCC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 15th IEEE International Symposium on Parallel and Distributed Processing with Applications and 16th IEEE International Conference on Ubiquitous Computing and Communications, ISPA/IUCC 2017; Conference date: 12 December 2017 through 15 December 2017; Conference code: 136745}
}

@CONFERENCE{Krause2018517,
	author = {Krause, Jonas and Sugita, Gavin and Baek, Kyungim and Lim, Lipyeow},
	title = {WTPlant (What's That Plant?): A deep learning system for identifying plants in natural images},
	year = {2018},
	journal = {ICMR 2018 - Proceedings of the 2018 ACM International Conference on Multimedia Retrieval},
	pages = {517 – 520},
	doi = {10.1145/3206025.3206089},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053920079&doi=10.1145%2f3206025.3206089&partnerID=40&md5=e7b923631ab906cc96497d27c52fef36},
	affiliations = {Department of Information and Computer Sciences, University of Hawai'i at Manoa, 1680 East-West Road, Honolulu, 96822, HI, United States},
	abstract = {Despite the availability of dozens of plant identification mobile applications, identifying plants from a natural image remains a challenging problem - most of the existing applications do not address the complexity of natural images, the large number of plant species, and the multi-scale nature of natural images. In this technical demonstration, we present the WTPlant system for identifying plants in natural images. WTPlant is based on deep learning approaches. Specifically, it uses stacked Convolutional Neural Networks for image segmentation, a novel preprocessing stage for multi-scale analyses, and deep convolutional networks to extract the most discriminative features. WTPlant employs different classification architectures for plants and flowers, thus enabling plant identification throughout all the seasons. The user interface also shows, in an interactive way, the most representative areas in the image that are used to predict each plant species. The first version of WTPlant is trained to classify 100 different plant species present in the campus of the University of Hawai'i at Manoa. First experiments support the hypothesis that an initial segmentation process helps guide the extraction of representative samples and, consequently, enables Convolutional Neural Networks to better recognize objects of different scales in natural images. Future versions aim to extend the recognizable species to cover the land-based flora of the Hawaiian Islands. © 2018 ACM.},
	author_keywords = {Convolutional neural network; Deep learning; Image processing; Multi-scale analysis; Plant taxonomy},
	keywords = {Convolution; Deep learning; Discrete cosine transforms; Image analysis; Image processing; Neural networks; Statistics; User interfaces; Convolutional networks; Convolutional neural network; Discriminative features; Initial segmentation; Multi scale analysis; Plant taxonomy; Pre-processing stages; Representative sample; Image segmentation},
	publisher = {Association for Computing Machinery, Inc},
	isbn = {978-145035046-4},
	language = {English},
	abbrev_source_title = {ICMR - Proc. ACM Int. Conf. Multimed. Retr.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; Conference name: 8th ACM International Conference on Multimedia Retrieval, ICMR 2018; Conference date: 11 June 2018 through 14 June 2018; Conference code: 137092}
}

@CONFERENCE{Chaisuk20184472,
	author = {Chaisuk, Phuchitsan and Phromsuthirak, Krisada and Areekul, Vutipong},
	title = {Leaf classification based on a quadratic curved axis},
	year = {2018},
	journal = {Proceedings - International Conference on Image Processing, ICIP},
	volume = {2017-September},
	pages = {4472 – 4476},
	doi = {10.1109/ICIP.2017.8297128},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045342033&doi=10.1109%2fICIP.2017.8297128&partnerID=40&md5=e96aee006017b881335271f6be90e4eb},
	affiliations = {Kasetsart Signal and Image Processing Laboratory (KSIP Lab), Department of Electrical Engineering, Kasetsart University, Bangkok, 10900, Thailand},
	abstract = {We introduce a new reference axis for leaf classification. The new reference axis, called a Mid-Leaf axis, is based on a quadratic curve that lies on the middle of a leaf. This curve is derived from three basic landmark points: an apex, a centroid, and a petiole. After mapping to a new plane based on this curve, leaf shape features are invariant under translation, rotation, scaling, and bending. We propose the leaf shape features based on partitioning the morphological features and the tangent's direction angle of the leaf contour. Using the ImageCLEF2012 database (Scan-type only), our experimental results show that the proposed method outperforms the state-of-the-arts for leaf classification in the accuracy metric. © 2017 IEEE.},
	author_keywords = {Leaf classification; Leaf retrieval; Plant identification},
	keywords = {Classification (of information); Image processing; Direction angle; Leaf classification; Leaf retrieval; Morphological features; Plant identification; Quadratic curves; Reference axis; State of the art; Mapping},
	publisher = {IEEE Computer Society},
	issn = {15224880},
	isbn = {978-150902175-8},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Image Process. ICIP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 24th IEEE International Conference on Image Processing, ICIP 2017; Conference date: 17 September 2017 through 20 September 2017; Conference code: 134723}
}

@CONFERENCE{Blumenthal2017211,
	author = {Blumenthal, Julie and Megherbi, Dalila B. and Lussier, Robert},
	title = {Supervised machine learning via Hidden Markov Models for accurate classification of plant stress levels & types based on imaged Chlorophyll fluorescence profiles & their rate of change in time},
	year = {2017},
	journal = {2017 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications, CIVEMSA 2017 - Proceedings},
	pages = {211 – 216},
	doi = {10.1109/CIVEMSA.2017.7995328},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028517574&doi=10.1109%2fCIVEMSA.2017.7995328&partnerID=40&md5=95688004c37e8912f4ad950886ec7181},
	affiliations = {CMINDS Research Center, University of Massachusetts, Lowell, United States; GrowTech Inc., Waltham, United States},
	abstract = {It has long been known that Chlorophyll fluorescence (ChlF), a plant response to stressors in time, is a useful tool in detecting plant stress. Accurate and early plant stress detection is imperative in enabling appropriate and timely intervention. One of the major limitations of prior work in ChIF-based plant classification is that, in general, only a few key inflection points of a localized selection of a chlorophyll fluorescence signal are used to calculate single index values for classification. These values yield limited insight into stress level and especially into stressor type. In this paper, we introduce and present a new method for plant stress classification that uses supervised learning, via Hidden Markov Models (HMMs), to build accurate class profiles using global (versus local) ChlF time-varying signal data acquired via video imaging. We show how creating increased-state supervised models can in particular, classify specific stressor types as well as achieve more granularity in stressor level classification. Experimental results are presented to show the value and potential of the proposed supervised method to enable more accurate and specific classification of plant stressor types and stressor levels. © 2017 IEEE.},
	author_keywords = {Chlorophyll Fluorescence Imaging; Classification; Computational Intelligence; Digital Image Processing; Hidden Markov Models; Machine Learning; OJIP Transient; Pattern Representation and Classification/Recognition; Plant Stress; PSMT Transient; Supervised Learning},
	keywords = {Artificial intelligence; Chlorophyll; Classification (of information); Fluorescence; Image classification; Image processing; Learning systems; Markov processes; Supervised learning; Virtual reality; Yield stress; Chlorophyll fluorescence; Chlorophyll fluorescence imaging; Hidden markov models (HMMs); Pattern representation; Plant classification; Plant stress; Supervised machine learning; Time varying signal; Hidden Markov models},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150904252-4},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Comput. Intell. Virtual Environ. Meas. Syst. Appl., CIVEMSA - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2017 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications, CIVEMSA 2017; Conference date: 26 June 2017 through 28 June 2017; Conference code: 129712}
}

@CONFERENCE{Yang20181082,
	author = {Yang, Meng-Meng and Nayeem, Arifur and Shen, Ling-Ling},
	title = {Plant classification based on stacked autoencoder},
	year = {2018},
	journal = {Proceedings of the 2017 IEEE 2nd Information Technology, Networking, Electronic and Automation Control Conference, ITNEC 2017},
	volume = {2018-January},
	pages = {1082 – 1086},
	doi = {10.1109/ITNEC.2017.8284906},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046675163&doi=10.1109%2fITNEC.2017.8284906&partnerID=40&md5=0b097bc88e5e15b56228a14b36aa3445},
	affiliations = {School of Computer Science and Technology, Nanjing Normal University, Nanjing, Jiangsu, 210023, China; Saidpur Government Technical School and College, Saidpur, 5310, Bangladesh; School of Overseas Education Nanjing, University of Post and Telecommunications, Nanjing, 210046, China; School of Business of Nanjing, Normal University, Nanjing, Jiangsu, 210023, China},
	abstract = {With the development of rapid technology, the similarity between plants is increasing, which will enhance the classified workload of botanists. Therefore, it is urge to find a quick automatic classification method. In recent years, the performance of autoencoder has become more and more prominent. Consequently, in this paper, we employ stacked autoencoder to classify three plants, including 630 images in total. The result of this experience shows that the accuracy of classification is 93.3%. © 2017 IEEE.},
	author_keywords = {deep learning; plant classification; stacked autoencoder},
	keywords = {Control engineering; Accuracy of classifications; Auto encoders; Automatic classification; Plant classification; Stacked autoencoder; Deep learning},
	correspondence_address = {M.-M. Yang; School of Computer Science and Technology, Nanjing Normal University, Nanjing, Jiangsu, 210023, China; email: 1320981274@qq.com},
	editor = {Xu B.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150906413-7},
	language = {English},
	abbrev_source_title = {Proc. IEEE Inf. Technol., Netwo., Electron. Autom. Control Conf., ITNEC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2nd IEEE Information Technology, Networking, Electronic and Automation Control Conference, ITNEC 2017; Conference date: 15 December 2017 through 17 December 2017; Conference code: 134593}
}

@CONFERENCE{Kang20181,
	author = {Kang, Euncheol and Oh, Il-Seok},
	title = {Weak constraint leaf image recognition based on convolutional neural network},
	year = {2018},
	journal = {International Conference on Electronics, Information and Communication, ICEIC 2018},
	volume = {2018-January},
	pages = {1 – 4},
	doi = {10.23919/ELINFOCOM.2018.8330637},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048485303&doi=10.23919%2fELINFOCOM.2018.8330637&partnerID=40&md5=788a53e8dc1d0bb09daa1e65af6bf530},
	affiliations = {Division of Computer Science and Engineering, Chonbuk National University, South Korea},
	abstract = {Recently the computer vision and machine learning research communities pay a great attention to the leaf image recognition problem. Our literature survey focusing on the user interaction aspect reveals that two schemes of image acquisition have been used, one with strong constraint and the other with no constraint. The strong constraint interaction asks users to capture images by placing a leaf on a uniform background such as white paper while the unconstrained interaction allows any form of image capturing. The former one gets a high performance sacrificing the user convenience while the latter one provides a great convenience sacrificing the recognition performance. Our scheme is weakly constrained in the middle of two extremes. The proposed interaction scheme only asks users to center the leaf on smartphone camera screen. The leaf may be on the tree or off the tree. When the leaf is picked off the tree, it is recommended to place it against rather uniform background such as sky, soil, or tree bark. By fine-tuning the pre-trained CNNs (Convolutional Neural Network), we obtained a practical performance, 96.08% top-1 and 99.81% top-5 accuracies. The dataset is publicly open and the recognition system is released as an Android App. © 2018 Institute of Electronics and Information Engineers.},
	author_keywords = {Automatic leaf recognition; convolutional neural network; deep learning; fine tuning},
	keywords = {Convolution; Forestry; Image recognition; Neural networks; Tuning; Convolutional neural network; Fine tuning; Interaction schemes; Leaf recognition; Literature survey; Machine learning research; Recognition systems; Smart-phone cameras; Deep learning},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153864754-7},
	language = {English},
	abbrev_source_title = {Int. Conf. Electron., Inf. Commun., ICEIC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 17th International Conference on Electronics, Information and Communication, ICEIC 2018; Conference date: 24 January 2018 through 27 January 2018; Conference code: 135724}
}@CONFERENCE{Joly2016958,
	author = {Joly, Alexis and Goëau, Hervé and Champ, Julien and Dufour-Kowalski, Samuel and Müller, Henning and Bonnet, Pierre},
	title = {Crowdsourcing biodiversity monitoring: How sharing your photo stream can sustain our planet},
	year = {2016},
	journal = {MM 2016 - Proceedings of the 2016 ACM Multimedia Conference},
	pages = {958 – 967},
	doi = {10.1145/2964284.2976762},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994615814&doi=10.1145%2f2964284.2976762&partnerID=40&md5=fedebd140caa080d67d6e6e7bb479d2f},
	affiliations = {Inria, ZENITH Team, LIRMM, Montpellier, 34095, France; IRD, UMR AMAP, Montpellier, France; HES-SO, Sierre, 3960, Switzerland; CIRAD, UMR, AMAP, Montpellier, France},
	abstract = {Large scale biodiversity monitoring is essential for sustainable development (earth stewardship). With the recent advances in computer vision, we see the emergence of more and more effective identification tools allowing to set-up large-scale data collection platforms such as the popular Pl@ntNet initiative that allow to reuse interaction data. Although it covers only a fraction of the world ora, this platform is already being used by more than 300K people who produce tens of thousands of validated plant observations each year. This explicitly shared and validated data is only the tip of the iceberg. The real potential relies on the millions of raw image queries submitted by the users of the mobile application for which there is no human validation. People make such requests to get information on a plant along a hike or something they find in their garden but not know anything about. Allowing the exploitation of such contents in a fully automatic way could scale up the world-wide collection of implicit plant observations by several orders of magnitude, which can complement the explicit monitoring efforts. In this paper, we first survey existing automated plant identification systems through a five-year synthesis of the Plant-CLEF benchmark and an impact study of the Pl@ntNet platform. We then focus on the implicit monitoring scenario and discuss related research challenges at the frontier of computer science and biodiversity studies. Finally, we discuss the results of a preliminary study focused on implicit monitoring of invasive species in mobile search logs. We show that the results are promising but that there is room for improvement before being able to automatically share implicit observations within international platforms. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
	keywords = {Computer vision; Sea ice; Biodiversity monitoring; Identification tools; International platforms; Large scale data; Mobile applications; Orders of magnitude; Plant identification systems; Research challenges; Biodiversity},
	publisher = {Association for Computing Machinery, Inc},
	isbn = {978-145033603-1},
	language = {English},
	abbrev_source_title = {MM - Proc. ACM Multimed. Conf.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; Conference name: 24th ACM Multimedia Conference, MM 2016; Conference date: 15 October 2016 through 19 October 2016; Conference code: 124107; All Open Access, Green Open Access}
}

@CONFERENCE{Winberg2016,
	author = {Winberg, Simon L},
	title = {Development of the Fynbos Leaf Optical Recognition Application (FLORA): An innovation journey of a tool to assist in identifying plants},
	year = {2016},
	journal = {International Symposium on Technology and Society, Proceedings},
	volume = {2016-March},
	doi = {10.1109/ISTAS.2015.7439398},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965163193&doi=10.1109%2fISTAS.2015.7439398&partnerID=40&md5=2ccd6ee11120899b617bf3fc4a475423},
	affiliations = {Department of Electrical Engineering, University of Cape Town, Cape Town, South Africa},
	abstract = {The Fynbos Leaf Optical Recognition Application (FLORA) is a software program to automatically identify fynbos plants using leaf photographs. While it is easier to classify fynbos when they are flowering, most fynbos flower for only short periods therefore FLORA was designed to identify plants by leaves instead of flowers. This paper presents the innovation journey of FLORA, highlighting transitions in development spaces, impact of requirements changes, and other significant challenges and lessons learned in the journey. The development was done out in a university research context and vacillated between being in a closed space and being a more open initiative. The project settled on being a collaborative and open innovation whereby the system supports a more diverse community of users and contributors. While the original requirements concerned a small scientific community of students and scientists botanists, the revised system, which the innovation journey lead towards, aims instead towards a wider community including tourists and schools pupils. It is hoped the innovation will have a broader societal influence in particular at schools level, where it is hoped that FLORA will both inspire young learns, and in particular tech savvy kids who spend too much time indoors, to spend time outdoors and to improve their awareness and appreciation of nature. This paper concludes with ways the project could have been streamlined from early on to better support the users and to facilitate the transition from a close to an open innovation. © 2015 IEEE.},
	author_keywords = {custom database design; image processing; innovation journey; plant identification},
	keywords = {Application programs; Image processing; Optical data processing; Database design; Diverse community; Optical recognition; Plant identification; Requirements change; Scientific community; Software program; University research; Plants (botany)},
	correspondence_address = {S.L. Winberg; Department of Electrical Engineering, University of Cape Town, Cape Town, South Africa; email: simon.winberg@uct.ac.za},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147998283-7},
	language = {English},
	abbrev_source_title = {Int Symp Technol Soc},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: IEEE International Symposium on Technology and Society, ISTAS 2015; Conference date: 11 November 2015 through 12 November 2015; Conference code: 121116}
}

@ARTICLE{Al-Otaibi2017469,
	author = {Al-Otaibi, Manar Bati and Ashour, Amira S. and Dey, Nilanjan and Al Quthami, Rahaf Abdullah and Al-Nufaei, Asrar Abdullah and Shi, Fuqian},
	title = {Statistical image analysis ased automated eaves classification},
	year = {2017},
	journal = {Frontiers in Artificial Intelligence and Applications},
	volume = {296},
	pages = {469 – 479},
	doi = {10.3233/978-1-61499-785-6-469},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028379806&doi=10.3233%2f978-1-61499-785-6-469&partnerID=40&md5=09c9b253eec09e71a31c26247e14452d},
	affiliations = {Computer Science Department, CIT College, Taif University, Saudi Arabia; Department of Electronics and Electrical Communications Engineering, Faculty of Engineering, Tanta University, Egypt; Department of Information Technology, Techno India College of Technology, Kolkata, India; College of Information and Engineering, Wenzhou Medical University, Wenzhou, China},
	abstract = {Plants recognition and classification is a challenging process due to the high variability in the plants' features and shapes. Numerous methodologies incorporating image processing were improved to tackle this process for early stage recognition of diseases. Leaf recognition has popular/wide range of agriculture practical applications. Consequently, the current work is interested in the recognition and classification of parsley and basil leaves along with the recognition of their infected parts. An image analysis is used to extract different statistical features from the leaves' dataset. From such statistical features a recognition/classification processes are performed to classify the fresh and infected leaves in each leaf type as well as to classify the two-leave species. The classification process was performed using neural network. The experimental results depicted that the classification accuracies for the three tested cases, namely fresh/infected basil, fresh/infected parsley, and fresh basil/parsley were 80%, 80.0%, and 100.0%; respectively. © 2017 The authors and IOS Press. All rights reserved.},
	author_keywords = {Classification; De-noising; Image analysis; Image processing; Leaf recognition; Neural network},
	keywords = {Classification (of information); Image analysis; Image classification; Intelligent systems; Intelligent vehicle highway systems; Neural networks; Plants (botany); Transportation; Classification accuracy; Classification process; De-noising; Leaf recognition; Statistical features; Statistical image analysis; Image processing},
	editor = {Balas V.E. and "Aurel Vlaicu" University of Arad, Faculty of Engineering, Department of Automation and Applied Informatics, 77 B-dul Revolutiei, Arad and Zhao X. and Chang'an University, School of Information Engineering, Nan Er Huan Zhong Duan, Xi'an and Shi F. and Jain L.C.},
	publisher = {IOS Press},
	issn = {09226389},
	isbn = {978-161499784-9},
	language = {English},
	abbrev_source_title = {Front. Artif. Intell. Appl.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 2nd International Conference on Information Technology and Intelligent Transportation Systems, ITITS 2017; Conference date: 10 June 2017; Conference code: 129700}
}

@ARTICLE{Almeida20165325,
	author = {Almeida, Jurandy and Pedronette, Daniel C. G. and Alberton, Bruna C. and Morellato, Leonor Patricia C. and Torres, Ricardo Da S.},
	title = {Unsupervised Distance Learning for Plant Species Identification},
	year = {2016},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	volume = {9},
	number = {12},
	pages = {5325 – 5338},
	doi = {10.1109/JSTARS.2016.2608358},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006172897&doi=10.1109%2fJSTARS.2016.2608358&partnerID=40&md5=07564dbcd9101153ae14c9c29e65e0ce},
	affiliations = {Institute of Science and Technology, Federal University of São Paulo (UNIFESP), São José dos Campos, 12247-014, Brazil; Department of Statistics, Applied Mathematics and Computation, São Paulo State University (UNESP), Rio Claro, 13506-900, Brazil; Department of Botany, São Paulo State University (UNESP), Rio Claro, 13506-900, Brazil; Institute of Computing, University of Campinas (UNICAMP), Campinas, 13083-852, Brazil},
	abstract = {Phenology is among the most trustworthy indicators of climate change effects on plants and animals. The recent application of repeated digital photographs to monitor vegetation phenology has provided accurate measures of plant life cycle changes over time. A fundamental requirement for phenology studies refers to the correct recognition of phenological patterns from plants by taking into account time series associated with their crowns. This paper presents a new similarity measure for identifying plants based on the use of an unsupervised distance learning scheme, instead of using traditional approaches based on pairwise similarities. We experimentally show that its use yields considerable improvements in time-series search tasks. In addition, we also demonstrate how the late fusion of different time series can improve the results on plant species identification. In some cases, significant gains were observed (up to +8.21% and +19.39% for mean average precision and precision at 10 scores, respectively) when compared with the use of time series in isolation. © 2016 IEEE.},
	author_keywords = {Image analysis; plant identification; remote phenology; time series; unsupervised distance learning},
	keywords = {Animalia; Biology; Climate change; Distance education; Image analysis; Life cycle; Pattern recognition; Time series; Digital photographs; Plant identification; Plant species identification; remote phenology; Similarity measure; Time series searches; Traditional approaches; Vegetation phenology; image analysis; phenology; plant; remote sensing; time series; unsupervised classification; Time series analysis},
	correspondence_address = {J. Almeida; Institute of Science and Technology, Federal University of São Paulo (UNIFESP), São José dos Campos, 12247-014, Brazil; email: jurandy.almeida@unifesp.br},
	publisher = {Institute of Electrical and Electronics Engineers},
	issn = {19391404},
	language = {English},
	abbrev_source_title = {IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16}
}

@ARTICLE{Wang2016899,
	author = {Wang, Zhaobin and Sun, Xiaoguang and Zhang, Yaonan and Ying, Zhu and Ma, Yide},
	title = {Leaf recognition based on PCNN},
	year = {2016},
	journal = {Neural Computing and Applications},
	volume = {27},
	number = {4},
	pages = {899 – 908},
	doi = {10.1007/s00521-015-1904-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928387329&doi=10.1007%2fs00521-015-1904-1&partnerID=40&md5=b814e5b9f32a23c995f11ba3e92eb65d},
	affiliations = {School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, Gansu, China; Cold and Arid Regions Environmental and Engineering Research Institute, Chinese Academy of Sciences, Lanzhou, 730000, Gansu, China; Institute of Biology, Gansu Academy of Sciences, Lanzhou, 730000, China},
	abstract = {Plant is closely related to humans. How to quickly recognize an unknown plant without related professional knowledge is a huge challenge. With the development of image processing and pattern recognition, it is available for plant recognition based on the technique of image processing. Pulse-coupled neural network is a powerful tool for image processing. It is widely applied in the field of image segmentation, image fusion, feature extraction, etc. Support vector machine is an excellent classifier, which can finish the complex task of data exploration. Based on these two techniques, a novel plant recognition method is proposed in this paper. The key feature is the entropy sequence obtained by pulse-coupled neural network. Other ancillary features can be computed directly by mathematical and morphological methods. Both key feature and ancillary features are employed to represent the unique feature of one plant. Support vector machine in our method is taken as the classifier, which can implement the multi-class classification. Experimental results show that the proposed method can finish the task of plant recognition effectively. Compared with the existing methods, our proposed method has better recognition rate. © 2015, The Natural Computing Applications Forum.},
	author_keywords = {Feature extraction; Image processing; PCNN; Plant recognition},
	keywords = {Classification (of information); Complex networks; Data fusion; Extraction; Feature extraction; Image fusion; Image segmentation; Learning systems; Mathematical morphology; Neural networks; Pattern recognition; Support vector machines; Data exploration; Entropy sequences; Leaf recognition; Multi-class classification; PCNN; Plant recognition; Professional knowledge; Pulse coupled neural network; Image processing},
	correspondence_address = {Y. Zhang; Cold and Arid Regions Environmental and Engineering Research Institute, Chinese Academy of Sciences, Lanzhou, 730000, China; email: yaonan@lzb.ac.cn},
	publisher = {Springer-Verlag London Ltd},
	issn = {09410643},
	language = {English},
	abbrev_source_title = {Neural Comput. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 44}
}

@ARTICLE{Zhang2017282,
	author = {Zhang, Shanwen and Zhang, Chuanlei},
	title = {Plant species recognition based on deep convolutional neural networks},
	year = {2017},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10361 LNCS},
	pages = {282 – 289},
	doi = {10.1007/978-3-319-63309-1_26},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027712803&doi=10.1007%2f978-3-319-63309-1_26&partnerID=40&md5=6c31347fb5f56491bebde1be5d46f667},
	affiliations = {Department of Information Engineering, Xijing University, Xi’an, 710123, China; Tianjin University of Science and Technology, Tianjin, 300222, China},
	abstract = {A number of the existing leaf based plan leaf recognition methods rely on the hand-crafted features of color, texture and shape, and other various features. One drawback of these methods is poor convergence and generalization. To overcome this problem, a deep convolutional neural network (DCNN) is applied to plant species recognition. The proposed method is different from the existing feature extraction based recognition approaches. The high-level features can be extracted by DCNN. The experimental results clearly demonstrate the effectiveness and efficiency of the proposed model in leaf identification in comparison with current state-of-the-art. © Springer International Publishing AG 2017.},
	author_keywords = {Automatic plant species identification; Deep convolutional neural networks (DCNN); Deep learning (DL); Feature extraction},
	keywords = {Computation theory; Convolution; Extraction; Feature extraction; Intelligent computing; Neural networks; Convolutional neural network; Effectiveness and efficiencies; High-level features; Leaf identification; Leaf recognition; Plant species; Plant species identification; State of the art; Deep neural networks},
	correspondence_address = {C. Zhang; Tianjin University of Science and Technology, Tianjin, 300222, China; email: a17647@gmail.com},
	editor = {Huang D.-S. and Premaratne P. and Bevilacqua V. and Gupta P.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331963308-4},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 13th International Conference on Intelligent Computing, ICIC 2017; Conference date: 7 August 2017 through 10 August 2017; Conference code: 195469}
}

@CONFERENCE{Yalcin2016,
	author = {Yalcin, Hulya and Razavi, Salar},
	title = {Plant classification using convolutional neural networks},
	year = {2016},
	journal = {2016 5th International Conference on Agro-Geoinformatics, Agro-Geoinformatics 2016},
	doi = {10.1109/Agro-Geoinformatics.2016.7577698},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994184244&doi=10.1109%2fAgro-Geoinformatics.2016.7577698&partnerID=40&md5=c3ddc8c5dd292998ea373e50a7ddbf91},
	affiliations = {Visual Intelligence Laboratory Electronics and Telecommunication Engineering, Istanbul Technical University, Turkey},
	abstract = {Application of the benefits of modern computing technology to improve the efficiency of agricultural fields is inevitable with growing concerns about increasing world population and limited food resources. Computing technology is crucial not only to industries related to food production but also to environmentalists and other related authorities. It is expected to increase the productivity, contribute to a better understanding of the relationship between environmental factors and healthy crops, reduce the labor costs for farmers and increase the operation speed and accuracy. Implementing machine learning methods such as deep neural networks on agricultural data has gained immense attention in recent years. One of the most important problems is automatic classification of plant species based on their types. Automatic plant type identification process could offer a great help for application of pesticides, fertilization and harvesting of different species on-time in order to improve the production processes of food and drug industries. In this paper, we propose a Convolutional Neural Network (CNN) architecture to classify the type of plants from the image sequences collected from smart agro-stations. First challenges introduced by illumination changes and deblurring are eliminated with some preprocessing steps. Following the preprocessing step, Convolutional Neural Network architecture is employed to extract the features of images. The construction of the CNN architecture and the depth of CNN are crucial points that should be emphasized since they affect the recognition capability of the architecture of neural networks. In order to evaluate the performance of the approach proposed in this paper, the results obtained through CNN model are compared with those obtained by employing SVM classifier with different kernels, as well as feature descriptors such as LBP and GIST. The performance of the approach is tested on dataset collected through a government supported project, TARBIL, for which over 1200 agro-stations are placed throughout Turkey. The experimental results on TARBIL dataset confirm that the proposed method is quite effective. © 2016 IEEE.},
	author_keywords = {agriculture; Computer vision; convolutional neural networks; deep learning; plant classification},
	keywords = {Agricultural machinery; Agriculture; Artificial intelligence; Chemical contamination; Compensation (personnel); Computer vision; Convolution; Image processing; Learning systems; Neural networks; Wages; Automatic classification; Convolutional neural network; Deep learning; Environmental factors; Identification process; Illumination changes; Machine learning methods; Plant classification; Network architecture},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150902350-9},
	language = {English},
	abbrev_source_title = {Int. Conf. Agro-Geoinformatics, Agro-Geoinformatics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 73; Conference name: 5th International Conference on Agro-Geoinformatics, Agro-Geoinformatics 2016; Conference date: 18 July 2016 through 20 July 2016; Conference code: 124077}
}

@ARTICLE{Wu2017505,
	author = {Wu, Huisi and Xiang, Yongkui and Liu, Jingjing and Wen, Zhenkun},
	title = {Automatic Leaf Recognition Based on Deep Convolutional Networks},
	year = {2017},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10636 LNCS},
	pages = {505 – 515},
	doi = {10.1007/978-3-319-70090-8_52},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035207086&doi=10.1007%2f978-3-319-70090-8_52&partnerID=40&md5=58e979e4da8509740476fc0d440032b7},
	affiliations = {College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China},
	abstract = {Leaf recognition remains a hot research topic receiving intensive attention in computer vision. In this paper, we propose deep convolutional networks with deep learning framework on the large scale of leaf databases. Different from the existing leaf recognition algorithms that mainly depend on traditional feature extractions and pattern matching operations, our method can achieve automatic leaf recognition based on deep convolutional networks without any explicit feature extraction or matching. Because it does not require any feature detection and selection, the advantages of our framework are obvious, especially for the large scale leaf databases. Specifically, we design deep convolutional networks structure and adopt fine-tuning strategy for our network initialization. In addition, we also develop a visualization-guided parameter tuning scheme to guarantee the accuracy of our deep learning framework. Our method is evaluated on several different databases with different scales. Comparison experiments are performed and demonstrate that the accuracy of our method outperforms traditional methods. © 2017, Springer International Publishing AG.},
	author_keywords = {Deep convolutional networks; Leaf recognition; Learning feature visualization},
	keywords = {Convolution; Database systems; Deep learning; Pattern matching; Visualization; Convolutional networks; Feature detection; Hot research topics; Leaf database; Leaf recognition; Learning frameworks; Network initialization; Parameter-tuning; Feature extraction},
	correspondence_address = {Z. Wen; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; email: wenzk@szu.edu.cn},
	editor = {Liu D. and Xie S. and El-Alfy E.M. and Zhao D. and Li Y.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331970089-2},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 24th International Conference on Neural Information Processing, ICONIP 2017; Conference date: 14 November 2017 through 18 November 2017; Conference code: 204399}
}

@ARTICLE{Sun2017,
	author = {Sun, Yu and Liu, Yuan and Wang, Guan and Zhang, Haiyan},
	title = {Deep Learning for Plant Identification in Natural Environment},
	year = {2017},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2017},
	doi = {10.1155/2017/7361042},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021748295&doi=10.1155%2f2017%2f7361042&partnerID=40&md5=9d287ef972676888a330c259a44e822a},
	affiliations = {School of Information Science and Technology, Beijing Forestry University, Beijing, 100083, China},
	abstract = {Plant image identification has become an interdisciplinary focus in both botanical taxonomy and computer vision. The first plant image dataset collected by mobile phone in natural scene is presented, which contains 10,000 images of 100 ornamental plant species in Beijing Forestry University campus. A 26-layer deep learning model consisting of 8 residual building blocks is designed for large-scale plant classification in natural environment. The proposed model achieves a recognition rate of 91.78% on the BJFU100 dataset, demonstrating that deep learning is a promising technology for smart forestry. © 2017 Yu Sun et al.},
	keywords = {Cell Phone; China; Classification; Datasets as Topic; Environment; Forestry; Machine Learning; Photography; Plants; Universities; Education; Forestry; Timber; Building blockes; Image identification; Learning models; Natural environments; Ornamental plants; Plant classification; Plant identification; University campus; China; classification; environment; forestry; information processing; machine learning; mobile phone; photography; plant; procedures; university; Deep learning},
	correspondence_address = {H. Zhang; School of Information Science and Technology, Beijing Forestry University, Beijing, 100083, China; email: zhyzml@bjfu.edu.cn},
	publisher = {Hindawi Limited},
	issn = {16875265},
	pmid = {28611840},
	language = {English},
	abbrev_source_title = {Comput. Intell. Neurosci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 171; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Tharwat2016461,
	author = {Tharwat, Alaa and Gaber, Tarek and Awad, Yasser M. and Dey, Nilanjan and Hassanien, Aboul Ella},
	title = {Plants identification using feature fusion technique and bagging classifier},
	year = {2016},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {407},
	pages = {461 – 471},
	doi = {10.1007/978-3-319-26690-9_41},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951921396&doi=10.1007%2f978-3-319-26690-9_41&partnerID=40&md5=f836bd977df77138a3d916fc460fa9d1},
	affiliations = {Suez Canal University, Ismailia, Egypt; Bengal College of Engineering and Technology, Durgapur, India; Cairo University, Giza, Egypt; Beni Suef University, Beni Suef, Egypt; Scientific Research Group in Egypt (SRGE), Cairo, Egypt},
	abstract = {In this paper, a plant identification approach using 2D digital images of leaves is proposed. This approach will be used to develop an expert system to identify plant species by processing colored images of its leaf. The approach made use of feature fusion technique and the Bagging classifier. Feature fusion technique is used to combine color, shape, and texture features. Color moments, invariant moments, and Scale Invariant Feature Transform (SIFT) are used to extract the color, shape, and texture features, respectively. Linear Discriminant Analysis (LDA) is used to reduce the number of features and Bagging ensemble is used to match the unknown image and the training or labeled images. The proposed approach was tested using Flavia dataset which consists of 1907 colored images of leaves. The experimental results showed that the accuracy of feature fusion approach was much better than all other single features. Moreover, a comparison with the most related work showed that our approach achieved better accuracy under the same dataset and same experimental setup. © Springer International Publishing Switzerland 2016.},
	author_keywords = {Bagging classifier; Color features; Feature fusion; Linear discriminant analysis (LDA); Plant identification; Scale invariant feature transform (SIFT); Shape features; Texture features},
	keywords = {Color; Discriminant analysis; Expert systems; Image classification; Image processing; Information science; Intelligent systems; Mathematical transformations; Plants (botany); Color features; Feature fusion; Linear discriminant analysis; Plant identification; Scale invariant feature transforms; Shape features; Texture features; Classification (of information)},
	correspondence_address = {T. Gaber; Suez Canal University, Ismailia, Egypt; email: tmgaber@gmail.com},
	editor = {Hassanien A.E. and El-Bendary N. and Gaber T. and Dey N.},
	publisher = {Springer Verlag},
	issn = {21945357},
	isbn = {978-331926688-6},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28; Conference name: 1st International Conference on Advanced Intelligent System and Informatics, AISI 2015; Conference date: 28 November 2015 through 30 November 2015; Conference code: 156549}
}

@CONFERENCE{Heredia2017259,
	author = {Heredia, Ignacio},
	title = {Large-scale plant classification with deep neural networks},
	year = {2017},
	journal = {ACM International Conference on Computing Frontiers 2017, CF 2017},
	pages = {259 – 262},
	doi = {10.1145/3075564.3075590},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027057744&doi=10.1145%2f3075564.3075590&partnerID=40&md5=ddeb782fe75a7c1a59fc5a23e3396849},
	affiliations = {Instituto de Fisica de Cantabria (CSIC-UC), Advanced Computing Department, Av. de los Castros s/n, Santander, Cantabria, 39005, Spain},
	abstract = {This paper discusses the potential of applying deep learning techniques for plant classification and its usage for citizen science in large-scale biodiversity monitoring. We show that plant classification using near state-of-the-art convolutional network architectures like ResNet50 achieves significant improvements in accuracy compared to the most widespread plant classification application in test sets composed of thousands of different species labels. We find that the predictions can be confidently used as a baseline classification in citizen science communities like iNaturalist (or its Spanish fork, Natusfera) which in turn can share their data with biodiversity portals like GBIF. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
	author_keywords = {Biodiversity monitoring; Citizen science; Deep learning; Plant classification},
	keywords = {Biodiversity; Deep learning; Deep neural networks; Network architecture; Biodiversity monitoring; Citizen science; Convolutional networks; Learning techniques; Plant classification; State of the art; Test sets; Classification (of information)},
	correspondence_address = {I. Heredia; Instituto de Fisica de Cantabria (CSIC-UC), Advanced Computing Department, Santander, Cantabria, Av. de los Castros s/n, 39005, Spain; email: iheredia@ifca.unican.es},
	publisher = {Association for Computing Machinery, Inc},
	isbn = {978-145034487-6},
	language = {English},
	abbrev_source_title = {ACM Int. Conf. Comput. Front., CF},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; Conference name: 14th ACM International Conference on Computing Frontiers, CF 2017; Conference date: 15 May 2017 through 17 May 2017; Conference code: 128277; All Open Access, Green Open Access}
}

@CONFERENCE{Rao20175,
	author = {Rao, Anusha and Kulkarni, S.B.},
	title = {An improved technique of plant leaf classificaion using hybrid feature modeling},
	year = {2017},
	journal = {IEEE International Conference on Innovative Mechanisms for Industry Applications, ICIMIA 2017 - Proceedings},
	pages = {5 – 9},
	doi = {10.1109/ICIMIA.2017.7975579},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027453565&doi=10.1109%2fICIMIA.2017.7975579&partnerID=40&md5=5362f384af8e883a0b74132f0babeaac},
	affiliations = {Dept. of ISE, Dayananda Sagar Academy of Technology and Management, Bengaluru, India; Dept. of CSE, SDM College of Engineering and Technology, Dharwad, India},
	abstract = {Various applications have been developed during recent years which are based on the computer vision system. In this field, plant species recognition is a challenging task for researchers due to environmental and image acquisition condition of image. Leaf classification application can be used for various purpose such as remote sensing imaging, botanical characteristically analysis etc. Now a day, amount of dataset is increasing rapidly in this field. Thus it motivates us to develop an efficient model for leaf classification in terms of computation time and accuracy. To address this issue, here in this work we propose a hybrid approach for feature extraction to improve the classification accuracy. Proposed model follows twofold working process whereas in first stage image pre-processing is applied to enhance the image quality of image by applying image enhancement with the help of auto-regressive model, second stage performs feature computation by combining morphological, shape and SIFT feature and finally Deep Neural Network is applied for classification performance evaluation. Proposed work is a combination of image enhancement, morphological feature, SIFT feature and classification technique. For efficient image enhancement, auto-regressive model is adopted here. A comparative study of classification performance is presented which shows the robustness of proposed model. © 2017 IEEE.},
	author_keywords = {Deep Neural Network; Hybrid feature extraction model; Image enhancement; Leaf recognition},
	keywords = {Deep neural networks; Extraction; Feature extraction; Image acquisition; Image enhancement; Image processing; Quality control; Remote sensing; Auto regressive models; Classification accuracy; Classification performance; Classification technique; Hybrid-feature extraction; Leaf recognition; Morphological features; Remote sensing imaging; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150905960-7},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Innov. Mech. Ind. Appl., ICIMIA - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2017 IEEE International Conference on Innovative Mechanisms for Industry Applications, ICIMIA 2017; Conference date: 21 February 2017 through 23 February 2017; Conference code: 129110}
}

@CONFERENCE{Atito2017,
	author = {Atito, Sara and Yanikoglu, Berrin and Aptoula, Erchan},
	title = {Plant identification with large number of species: SabanciU-gebzeTU system in plantCLEF 2017},
	year = {2017},
	journal = {CEUR Workshop Proceedings},
	volume = {1866},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034756341&partnerID=40&md5=1d54ddd4c9f4534cb315bcbc8770138f},
	affiliations = {Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; Institute of Information Technologies, Gebze Technical University, Kocaeli, Turkey},
	abstract = {We describe the plant identification system that was submitted to the LifeCLEF plant identification campaign in 2017 [1], as a collaboration of Sabanci University and Gebze Technical University in Turkey. Similar to our system that got a very close second place in 2016, we fine-tuned two well-known deep learning architectures (VGGNet and GoogLeNet) that were pre-trained on the object recognition dataset of ILSVRC 2012 and used an ensemble of 4-9 networks using score-level combination for the submitted systems. Our best system was obtained with a classifier fusion of 9 networks trained with some differences in training (network architecture, data, or initialization), achieving an average inverse rank of 0.634 on the official test data, while the first place system achieved an impressive score of 0.92.},
	author_keywords = {Convolutional neural networks; Deep learning; Plant identification},
	keywords = {Deep learning; Network architecture; Neural networks; Object recognition; Classifier fusion; Convolutional neural network; Learning architectures; Number of species; Plant identification; Plant identification systems; Technical universities; Test data; Computer architecture},
	editor = {Mandl T. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Goeuriot L. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 18th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2017; Conference date: 11 September 2017 through 14 September 2017; Conference code: 131731}
}

@ARTICLE{Grinblat2016418,
	author = {Grinblat, Guillermo L. and Uzal, Lucas C. and Larese, Mónica G. and Granitto, Pablo M.},
	title = {Deep learning for plant identification using vein morphological patterns},
	year = {2016},
	journal = {Computers and Electronics in Agriculture},
	volume = {127},
	pages = {418 – 424},
	doi = {10.1016/j.compag.2016.07.003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978038820&doi=10.1016%2fj.compag.2016.07.003&partnerID=40&md5=28e5214c7b879b235615236a95efbe35},
	affiliations = {CIFASIS, French Argentine International Center for Information and Systems Sciences, UNR-CONICET, Argentina},
	abstract = {We propose using a deep convolutional neural network (CNN) for the problem of plant identification from leaf vein patterns. In particular, we consider classifying three different legume species: white bean, red bean and soybean. The introduction of a CNN avoids the use of handcrafted feature extractors as it is standard in state of the art pipeline. Furthermore, this deep learning approach significantly improves the accuracy of the referred pipeline. We also show that the reported accuracy is reached by increasing the model depth. Finally, by analyzing the resulting models with a simple visualization technique, we are able to unveil relevant vein patterns. © 2016 Elsevier B.V.},
	author_keywords = {Automatic plant identification; Deep learning; Leaf vein image; Machine vision},
	keywords = {Glycine max; Neural networks; Pipelines; Convolutional neural network; Deep learning; Feature extractor; Leaf vein image; Morphological patterns; Plant identification; State of the art; Visualization technique; accuracy assessment; artificial neural network; image analysis; learning; legume; machinery; morphology; pipeline; visualization; Computer vision},
	correspondence_address = {L.C. Uzal; CIFASIS, French Argentine International Center for Information and Systems Sciences, UNR-CONICET, Argentina; email: uzal@cifasis-conicet.gov.ar},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 419; All Open Access, Green Open Access}
}

@CONFERENCE{Zhang20152143,
	author = {Zhang, Chaoyun and Zhou, Pan and Li, Chenghua and Liu, Lijun},
	title = {A convolutional neural network for leaves recognition using data augmentation},
	year = {2015},
	journal = {Proceedings - 15th IEEE International Conference on Computer and Information Technology, CIT 2015, 14th IEEE International Conference on Ubiquitous Computing and Communications, IUCC 2015, 13th IEEE International Conference on Dependable, Autonomic and Secure Computing, DASC 2015 and 13th IEEE International Conference on Pervasive Intelligence and Computing, PICom 2015},
	pages = {2143 – 2150},
	doi = {10.1109/CIT/IUCC/DASC/PICOM.2015.318},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964269910&doi=10.1109%2fCIT%2fIUCC%2fDASC%2fPICOM.2015.318&partnerID=40&md5=1896de51a63e402c794d4de851275131},
	affiliations = {School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; DNN (Deep Neural Network) Lab, Beijing JingDong Century Trade. Ltd (JD.COM), Beijing, China; TipDM Intelligent Technology, Wuhan, China},
	abstract = {Recently, convolutional neural networks (ConvNets) have achieved marvellous results in different field of recognition, especially in computer vision. In this paper, a seven-layer ConvNet using data augmentation is proposed for leaves recognition. First, we implement multiform transformations (e.g., rotation and translation etc.) to enlarge the dataset without changing their labels. This novel technique recently makes tremendous contribution to the performance of ConvNets as it is able to reduce the over-fitting degree and enhance the generalization ability of the ConvNet. Moreover, in order to get the shapes of leaves, we sharpen all the images with a random parameter. This method is similar to the edge detection, which has been proved useful in the image classification. Then we train a deep convolutional neural network to classify the augmented leaves data with three groups of test set and finally find that the method is quite feasible and effective. The accuracy achieved by our algorithm outperforms other methods for supervised learning on the popular leaf dataset Flavia. © 2015 IEEE.},
	keywords = {Classification (of information); Computer vision; Convolution; Edge detection; Neural networks; Ubiquitous computing; Convnet; Convolutional neural network; Data augmentation; Generalization ability; Novel techniques; Overfitting; Random parameters; Test sets; Image classification},
	editor = {Atzori L. and Jin X. and Jarvis S. and Liu L. and Calvo R.A. and Hu J. and Min G. and Georgalas N. and Wu Y.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150900154-5},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. Comput. Inf. Technol., CIT, IEEE Int. Conf. Ubiquitous Comput. Commun., IUCC, IEEE Int. Conf. Dependable, Auton. Secur. Comput., DASC, IEEE Int. Conf. Pervasive Intell. Comput., PICom},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 77; Conference name: 15th IEEE International Conference on Computer and Information Technology, CIT 2015, 14th IEEE International Conference on Ubiquitous Computing and Communications, IUCC 2015, 13th IEEE International Conference on Dependable, Autonomic and Secure Computing, DASC 2015 and 13th IEEE International Conference on Pervasive Intelligence and Computing, PICom 2015; Conference date: 26 October 2015 through 28 October 2015; Conference code: 118896}
}

@ARTICLE{Boudra2017101,
	author = {Boudra, Safia and Yahiaoui, Itheri and Behloul, Ali},
	title = {Statistical radial binary patterns (SRBP) for bark texture identification},
	year = {2017},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10617 LNCS},
	pages = {101 – 113},
	doi = {10.1007/978-3-319-70353-4_9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036639584&doi=10.1007%2f978-3-319-70353-4_9&partnerID=40&md5=2fcaf6166f88b5a2596211a833a5e08d},
	affiliations = {LaSTIC, University of Batna 2, Batna, Algeria; CReSTIC, Université de Reims Champagne-Ardenne, Reims, France},
	abstract = {This paper presents a plant identification method based on the texture characterization of bark images. We propose a novel statistical radial binary pattern (SRBP) descriptor to encode the between-scale texture information within large neighbourhood areas using the statistical description of the grey scale intensity distribution. The proposed descriptor can efficiently encode the macro local structure. In addition, the proposed SRBP is computationally simple, rotation invariant and low-dimensional descriptor. We conduct comprehensive experiments on three different bark datasets to assess the performances of our approach. The experimental results show that our method achieves high identification rates outperforming different multi-scale LBP variants. © Springer International Publishing AG 2017.},
	author_keywords = {Bark identification; Multi-scale LBP; Statistical binary pattern; Texture descriptor},
	keywords = {Encoding (symbols); Binary patterns; Identification rates; Intensity distribution; Multi-scale LBP; Plant identification; Statistical descriptions; Texture characterizations; Texture descriptor; Computer vision},
	correspondence_address = {S. Boudra; LaSTIC, University of Batna 2, Batna, Algeria; email: safia.boudra@gmail.com},
	editor = {Blanc-Talon J. and Popescu D. and Scheunders P. and Philips W. and Penne R.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331970352-7},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 18th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2017; Conference date: 18 September 2017 through 21 September 2017; Conference code: 206999}
}

@ARTICLE{Pushpa20165142,
	author = {Pushpa, B.R. and Anand, C. and Mithun Nambiar, P.},
	title = {169Ayurvedic plant species recognition using statistical parameters on leaf images},
	year = {2016},
	journal = {International Journal of Applied Engineering Research},
	volume = {11},
	number = {7},
	pages = {5142 – 5147},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966290822&partnerID=40&md5=965d28e21463e82763e217c624cd3841},
	affiliations = {Department of Computer Science, Amrita Vishwa Vidyapeetham, Amrita University, Mysuru Campus, Karnataka, India},
	abstract = {Automatic recognition of plant species recognition is a challenging problem in the area of pattern recognition and computer vision. An efficient plant recognition system will be beneficial to many sectors of society which includes medical field, botanic researches and plant taxonomy study. Manual identification process requires prior knowledge and also it is a lengthy process. This paper proposes a simple and efficient methodology for Ayurvedic plant classification using digital image processing and machine vision technology. The three major phases in proposed methodology are pre-processing, feature extraction and classification. Pre-processing is done in order to highlight the relevant features to be used in the proposed methodology as well as to reduce unwanted noise from the input image, which reduces the chance of getting optimal feature values. In feature extraction phase, different morphologic features such as mean, standard deviation, convex hull ratio, isoperimetric quotient, eccentricity and entropy are extracted from the pre-processed leaf image. In the third phase, a new approach to classify ayurvedic plant species is adopted to recognize plant species by calculating the leaf factor of the input leaf using the extracted feature values and it is compared with the trained values that are stored in the database. An accuracy of 93.75% is obtained for the proposed methodology. © Research India Publications.},
	author_keywords = {Ayurvedic leaf classification; Feature extraction; Leaf factor},
	publisher = {Research India Publications},
	issn = {09734562},
	language = {English},
	abbrev_source_title = {Int. J. Appl. Eng. Res.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 34}
}

@ARTICLE{Cao201651,
	author = {Cao, Jie and Wang, Bin and Brown, Douglas},
	title = {Similarity based leaf image retrieval using multiscale R-angle description},
	year = {2016},
	journal = {Information Sciences},
	volume = {374},
	pages = {51 – 64},
	doi = {10.1016/j.ins.2016.09.023},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987879994&doi=10.1016%2fj.ins.2016.09.023&partnerID=40&md5=6ef5709b3e325fbb3bae712ea0af1222},
	affiliations = {School of Information Engineering, Nanjing University of Finance and Economics, Nanjing 210023, China; School of Engineering, Griffith University, Nathan QLD 4111, Australia},
	abstract = {Leaf image identification is a significant and challenging application of computer vision and image processing. A central issue associated with this task is how to effectively and efficiently describe the leaf images and measure their similarities. In this paper, a novel shape descriptor termed R-angle is proposed. R-angle describes the curvature of the contour by measuring the angle between the intersections of the shape contour with a circle of radius R centered at points sampled around the contour. It is intrinsically invariant to group transforms including scaling, rotation and translation. Varying the parameter R of the proposed R-angle naturally introduces the notation of scale, which we leverage to provide a coarse-to-fine description of the local curvature. A local scale arrangement is proposed by taking the distance between each contour point and the center of the shape to be the maximum scale for a given contour point. Two matching schemes, including L1-norm matching and dynamic programming based matching, are applied to measure the similarities of the leaf shapes. The retrieval experiments conducted on two challenging leaf image datasets indicate that the proposed method significantly outperforms the state-of-the-art methods for leaf identification. An additional experiment on an animal dataset also indicates its potential for general shape recognition. © 2016 Elsevier Inc.},
	author_keywords = {Invariant features; Leaf image retrieval; Plant identification; Shape description; Shape matching},
	keywords = {Computer vision; Dynamic programming; Image retrieval; Object recognition; Additional experiments; Invariant features; Leaf identification; Plant identification; Shape description; Shape descriptors; Shape matching; State-of-the-art methods; Image processing},
	correspondence_address = {B. Wang; School of Information Engineering, Nanjing University of Finance and Economics, Nanjing 210023, China; email: wangbin@njue.edu.cn},
	publisher = {Elsevier Inc.},
	issn = {00200255},
	coden = {ISIJB},
	language = {English},
	abbrev_source_title = {Inf Sci},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 36}
}

@ARTICLE{Holmes2017149,
	author = {Holmes, Matthew},
	title = {Changing techniques in crop plant classification: molecularization at the National Institute of Agricultural Botany during the 1980s},
	year = {2017},
	journal = {Annals of Science},
	volume = {74},
	number = {2},
	pages = {149 – 164},
	doi = {10.1080/00033790.2017.1287308},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013072368&doi=10.1080%2f00033790.2017.1287308&partnerID=40&md5=3dc1c6f9a44f12d33aa5f11e7cbaf607},
	affiliations = {Centre for History and Philosophy of Science, University of Leeds, Leeds, United Kingdom},
	abstract = {Modern methods of analysing biological materials, including protein and DNA sequencing, are increasingly the objects of historical study. Yet twentieth-century taxonomic techniques have been overlooked in one of their most important contexts: agricultural botany. This paper addresses this omission by harnessing unexamined archival material from the National Institute of Agricultural Botany (NIAB), a British plant science organization. During the 1980s the NIAB carried out three overlapping research programmes in crop identification and analysis: electrophoresis, near infrared spectroscopy (NIRS) and machine vision systems. For each of these three programmes, contemporary economic, statutory and scientific factors behind their uptake by the NIAB are discussed. This approach reveals significant links between taxonomic practice at the NIAB and historical questions around agricultural research, intellectual property and scientific values. Such links are of further importance given that the techniques developed by researchers at the NIAB during the 1980s remain part of crop classification guidelines issued by international bodies today. © 2017 Informa UK Limited, trading as Taylor & Francis Group.},
	keywords = {Classification; Crops, Agricultural; Electrophoresis; History, 20th Century; Machine Learning; Spectroscopy, Near-Infrared; United Kingdom; Agricultural machinery; Agriculture; Biological materials; Crops; DNA sequences; Electrophoresis; Gene encoding; Infrared devices; Near infrared spectroscopy; Agricultural research; Archival materials; Crop classification; Crop identification; Machine vision systems; Research programmes; Scientific values; Twentieth century; classification; crop; electrophoresis; genetics; history; machine learning; near infrared spectroscopy; procedures; United Kingdom; Plants (botany)},
	correspondence_address = {M. Holmes; Centre for History and Philosophy of Science, University of Leeds, Leeds, United Kingdom; email: prmrh@leeds.ac.uk},
	publisher = {Taylor and Francis Ltd.},
	issn = {00033790},
	coden = {ANNSA},
	pmid = {28532336},
	language = {English},
	abbrev_source_title = {Ann Sci},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Green Open Access}
}

@ARTICLE{Putzu2016570,
	author = {Putzu, Lorenzo and Di Ruberto, Cecilia and Fenu, Gianni},
	title = {A mobile application for leaf detection in complex background using saliency maps},
	year = {2016},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10016 LNCS},
	pages = {570 – 581},
	doi = {10.1007/978-3-319-48680-2_50},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994464077&doi=10.1007%2f978-3-319-48680-2_50&partnerID=40&md5=50c6f6193f30ff776b0fbf3caa26b097},
	affiliations = {Department of Mathematics and Computer Science, University of Cagliari, via Ospedale 72, Cagliari, 09124, Italy},
	abstract = {Plants are fundamental for human beings, so it’s very important to catalogue and preserve all the plants species. Identifying an unknown plant species is not a simple task. The leaf analysis is one of the approach used for the plant species identification. This task can be completed also automatically by image processing techniques, able to analyse the leaf images and provide a classification based on prior information. Many methods have been proposed in literature in order to complete the whole cataloguing task, providing excellent classification results. Nevertheless, many of the proposed methods work only on images acquired in controlled lighting conditions and with uniform background. In this work we propose a mobile application for leaf analysis for the automatic identification of plant species. The application is mainly devoted to the identification and segmentation steps, resolving the main issues created by uncontrolled lighting conditions with very accurate results. © Springer International Publishing AG 2016.},
	author_keywords = {Image analysis; Leaf recognition; Saliency maps; Segmentation},
	keywords = {Automation; Classification (of information); Computer vision; Image analysis; Image classification; Image processing; Lighting; Mobile computing; Mobile telecommunication systems; Automatic identification; Classification results; Image processing technique; Leaf recognition; Lighting conditions; Mobile applications; Plant species identification; Saliency map; Image segmentation},
	correspondence_address = {L. Putzu; Department of Mathematics and Computer Science, University of Cagliari, Cagliari, via Ospedale 72, 09124, Italy; email: lorenzo.putzu@gmail.com},
	editor = {Distante C. and Popescu D. and Scheunders P. and Philips W. and Blanc-Talon J.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331948679-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 17th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2016; Conference date: 24 October 2016 through 27 October 2016; Conference code: 185869}
}

@ARTICLE{Mehdipour Ghazi2017228,
	author = {Mehdipour Ghazi, Mostafa and Yanikoglu, Berrin and Aptoula, Erchan},
	title = {Plant identification using deep neural networks via optimization of transfer learning parameters},
	year = {2017},
	journal = {Neurocomputing},
	volume = {235},
	pages = {228 – 235},
	doi = {10.1016/j.neucom.2017.01.018},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009807662&doi=10.1016%2fj.neucom.2017.01.018&partnerID=40&md5=60f6c4a7dffb3f36ac937bb7e8222d99},
	affiliations = {Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; Institute of Information Technologies, Gebze Technical University, Kocaeli, Turkey},
	abstract = {We use deep convolutional neural networks to identify the plant species captured in a photograph and evaluate different factors affecting the performance of these networks. Three powerful and popular deep learning architectures, namely GoogLeNet, AlexNet, and VGGNet, are used for this purpose. Transfer learning is used to fine-tune the pre-trained models, using the plant task datasets of LifeCLEF 2015. To decrease the chance of overfitting, data augmentation techniques are applied based on image transforms such as rotation, translation, reflection, and scaling. Furthermore, the networks' parameters are adjusted and different classifiers are fused to improve overall performance. Our best combined system has achieved an overall accuracy of 80% on the validation set and an overall inverse rank score of 0.752 on the official test set. A comparison of our results against the results of the LifeCLEF 2015 plant identification campaign shows that we have improved the overall validation accuracy of the top system by 15% points and its overall inverse rank score on the test set by 0.1 while outperforming the top three competition participants in all categories. The system recently obtained a very close second place in the PlantCLEF 2016. © 2017 Elsevier B.V.},
	author_keywords = {Convolutional neural networks; Deep learning; Inverse rank score; Plant identification; Transfer learning},
	keywords = {Convolution; Convolutional neural network; Deep learning; Plant identification; Rank scores; Transfer learning; accuracy; AlexNet; Article; artificial neural network; GoogLeNet; image processing; information processing; learning; mathematical model; mathematical parameters; performance; plant identification; validation study; VGGNet; Neural networks},
	correspondence_address = {M. Mehdipour Ghazi; Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; email: mehdipour@sabanciuniv.edu},
	publisher = {Elsevier B.V.},
	issn = {09252312},
	coden = {NRCGE},
	language = {English},
	abbrev_source_title = {Neurocomputing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 382}
}

@CONFERENCE{Hang2017,
	author = {Hang, Siang Thye and Aono, Masaki},
	title = {Residual network with delayed max pooling for very large scale plant identification},
	year = {2017},
	journal = {CEUR Workshop Proceedings},
	volume = {1866},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034752888&partnerID=40&md5=ad924f3199b4c4f29b86fa67cc72e1b8},
	affiliations = {Knowledge Data Engineering and Information Retrieval Laboratory, Department of Computer Science and Engineering, Toyohashi University of Technology, Japan},
	abstract = {In our approach, we applied a few modifications to the 50-layered Residual Network. Our preliminary experiments with the Plant-CLEF 2016 dataset showed that the modifications improved classification performance. We have trained three models based on the modified Residual Network configuration with different combinations of trusted and noisy PlantCLEF 2017 datasets. Using confidence scores extracted from the three models, we have submitted four runs and our methods showed competitive classification performance.},
	author_keywords = {Deep learning; Down-sampling; Plant identification},
	keywords = {Deep learning; Classification performance; Confidence score; Down sampling; Max-pooling; Network configuration; Plant identification; Three models; Classification (of information)},
	editor = {Mandl T. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Goeuriot L. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 18th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2017; Conference date: 11 September 2017 through 14 September 2017; Conference code: 131731}
}

@ARTICLE{Kutha Krisnawijaya201778,
	author = {Kutha Krisnawijaya, N.N. and Herdiyeni, Yeni and Silalahi, Bib Paruhum},
	title = {Parallel technique for medicinal plant identification system using fuzzy local binary pattern},
	year = {2017},
	journal = {Journal of ICT Research and Applications},
	volume = {11},
	number = {1},
	pages = {78 – 91},
	doi = {10.5614/itbj.ict.res.appl.2017.11.1.5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020623116&doi=10.5614%2fitbj.ict.res.appl.2017.11.1.5&partnerID=40&md5=69bc6662371729a2b51d3b3e419e2c25},
	affiliations = {Department of Electrical Engineer, Faculty of Engineer and Informatics, Undiknas University, Jalan Bedugul No. 39, Bali, 80224, Indonesia; Department of Computer Science, Faculty of Mathematics and Natural Sciences, Bogor Agricultural University, Darmaga Campus, Jalan Meranti, Wing 20 Level 5, Bogor, 16680, Indonesia; Department of Mathematics, Faculty of Mathematics and Natural Sciences, Bogor Agricultural University, Darmaga Campus, Jalan Meranti, Wing 20 Level 5, Bogor, 16680, Indonesia},
	abstract = {As biological image databases are growing rapidly, automated species identification based on digital data becomes of great interest for accelerating biodiversity assessment, research and monitoring. This research applied high performance computing (HPC) to a medicinal plant identification system. A parallel technique for medicinal plant image processing using Fuzzy Local Binary Pattern (FLBP) is proposed. The FLBP method extends the Local Binary Pattern (LBP) approach by employing fuzzy logic to represent texture images. The main goal of this research was to measure the efficiency of using the proposed parallel technique for medicinal plant image processing and evaluation in order to find out whether this approach is reasonable for handling large data sets. The parallel processing technique was designed in a message-sending model. 30 species of Indonesian medical plants were analyzed. Each species was represented by 48 leaf images. Performance evaluation was measured using the speed-up, efficiency, and isoefficiency of the parallel computing technique. Preliminary results show that HPC worked well in reducing the execution time of medical plant identification. In this work, parallel processing of training images was 7.64 times faster than with sequential processing, with efficiency values greater than 0.9. Parallel processing of testing images was 6.73 times faster than with sequential processing, with efficiency values over 0.9. The system was able to identify images with an accuracy of 68.89%. © 2017 Published by ITB Journal Publisher.},
	author_keywords = {Fuzzy local binary pattern; High performance computing; Image processing; MPI Library; Parallel processing; Plant identification},
	publisher = {Institute for Research and Community Services, Institut Teknologi Bandung},
	issn = {23375787},
	language = {English},
	abbrev_source_title = {J. ICT Res. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Zhao20151145,
	author = {Zhao, Zhong-Qiu and Hong, Yan and Zheng, Peng and Wu, Xindong},
	title = {Plant identification using triangular representation based on salient points and margin points},
	year = {2015},
	journal = {Proceedings - International Conference on Image Processing, ICIP},
	volume = {2015-December},
	pages = {1145 – 1149},
	doi = {10.1109/ICIP.2015.7350979},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956634056&doi=10.1109%2fICIP.2015.7350979&partnerID=40&md5=c86d4b150caebd5b158ac096b87c3f97},
	affiliations = {College of Computer Science and Information Engineering, Hefei University of Technology, China; Department of Computer Science, University of Vermont, United States},
	abstract = {Leaf classification is an important component of living plant identification. A leaf contains important information for plant species identification in spite of its complexity. This paper introduces a method of recognizing leaf images based on triangular representations. A leaf is represented by local descriptors associated with margin sample points and salient sample points. We introduce three new triangular representations - salient triangle area representation (STAR), salient triangle side lengths representation (STSL), and salient triangle area, side lengths and two angles representation (STASLA), and then we combine two local descriptors - one provides a triangular representation of the leaf margin while the other represents the spatial correlation between salient points of the leaf and leaf margin. Experiments on the Image-CLEF 2011 leaf datasets show the effectiveness and the efficiency of the proposed method. © 2015 IEEE.},
	author_keywords = {Feature extraction; Local descriptors; Plant recognition; Triangular representations},
	publisher = {IEEE Computer Society},
	issn = {15224880},
	isbn = {978-147998339-1},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Image Process. ICIP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: IEEE International Conference on Image Processing, ICIP 2015; Conference date: 27 September 2015 through 30 September 2015; Conference code: 117806}
}

@ARTICLE{Hubble201723,
	author = {Hubble, Kelly and Daughters, Katie and Manstead, Antony S.R. and Rees, Aled and Thapar, Anita and Van Goozen, Stephanie H.M.},
	title = {Oxytocin Reduces Face Processing Time but Leaves Recognition Accuracy and Eye-Gaze Unaffected},
	year = {2017},
	journal = {Journal of the International Neuropsychological Society},
	volume = {23},
	number = {1},
	pages = {23 – 33},
	doi = {10.1017/S1355617716000886},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995761477&doi=10.1017%2fS1355617716000886&partnerID=40&md5=1d75a127bcd6d033fcb0251c7dc3e0ff},
	affiliations = {School of Psychology, Cardiff University, Tower Building, 70 Park Place, Cardiff, CF103AT, United Kingdom; Institute of Molecular and Experimental Medicine, School of Medicine, Cardiff University, Cardiff, United Kingdom; Institute of Psychological Medicine and Clinical Neurosciences, School of Medicine, Cardiff University, Cardiff, United Kingdom},
	abstract = {Objectives: Previous studies have found that oxytocin (OXT) can improve the recognition of emotional facial expressions; it has been proposed that this effect is mediated by an increase in attention to the eye-region of faces. Nevertheless, evidence in support of this claim is inconsistent, and few studies have directly tested the effect of oxytocin on emotion recognition via altered eye-gaze Methods: In a double-blind, within-subjects, randomized control experiment, 40 healthy male participants received 24 IU intranasal OXT and placebo in two identical experimental sessions separated by a 2-week interval. Visual attention to the eye-region was assessed on both occasions while participants completed a static facial emotion recognition task using medium intensity facial expressions. Results: Although OXT had no effect on emotion recognition accuracy, recognition performance was improved because face processing was faster across emotions under the influence of OXT. This effect was marginally significant (p<.06). Consistent with a previous study using dynamic stimuli, OXT had no effect on eye-gaze patterns when viewing static emotional faces and this was not related to recognition accuracy or face processing time. Conclusions: These findings suggest that OXT-induced enhanced facial emotion recognition is not necessarily mediated by an increase in attention to the eye-region of faces, as previously assumed. We discuss several methodological issues which may explain discrepant findings and suggest the effect of OXT on visual attention may differ depending on task requirements. © 2016 The International Neuropsychological Society.},
	author_keywords = {Affect recognition; Emotion; Eye-gaze; Faces; Oxytocin; Placebo},
	keywords = {Adult; Analysis of Variance; Attention; Double-Blind Method; Emotions; Face; Facial Expression; Fixation, Ocular; Humans; Male; Neuropsychological Tests; Oxytocin; Pattern Recognition, Visual; Photic Stimulation; Recognition (Psychology); Saliva; Time Factors; Young Adult; oxytocin; adult; analysis of variance; attention; controlled study; double blind procedure; drug effects; emotion; eye fixation; face; facial expression; human; male; metabolism; neuropsychological test; pattern recognition; photostimulation; physiology; randomized controlled trial; recognition; saliva; time factor; young adult},
	correspondence_address = {S.H.M. Van Goozen; School of Psychology, Cardiff University, Cardiff, Tower Building, 70 Park Place, CF103AT, United Kingdom; email: vangoozens@cardiff.ac.uk},
	publisher = {Cambridge University Press},
	issn = {13556177},
	coden = {JINSF},
	pmid = {27866504},
	language = {English},
	abbrev_source_title = {J. Int. Neuropsychol. Soc.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 31; All Open Access, Green Open Access}
}

@ARTICLE{Barré201750,
	author = {Barré, Pierre and Stöver, Ben C. and Müller, Kai F. and Steinhage, Volker},
	title = {LeafNet: A computer vision system for automatic plant species identification},
	year = {2017},
	journal = {Ecological Informatics},
	volume = {40},
	pages = {50 – 56},
	doi = {10.1016/j.ecoinf.2017.05.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019856916&doi=10.1016%2fj.ecoinf.2017.05.005&partnerID=40&md5=f609bf621613494d55a35e8be483a5b7},
	affiliations = {University of Bonn, Institute of Computer Science IV, Friedrich-Ebert-Allee 144, Bonn, 53113, Germany; Westfälische Wilhelms-Universität Münster, Institute for Evolution and Biodiversity and Botanical Garden, Hüfferstrasse 1, Münster, 48149, Germany},
	abstract = {Aims Taxon identification is an important step in many plant ecological studies. Its efficiency and reproducibility might greatly benefit from partly automating this task. Image-based identification systems exist, but mostly rely on hand-crafted algorithms to extract sets of features chosen a priori to identify species of selected taxa. In consequence, such systems are restricted to these taxa and additionally require involving experts that provide taxonomical knowledge for developing such customized systems. The aim of this study was to develop a deep learning system to learn discriminative features from leaf images along with a classifier for species identification of plants. By comparing our results with customized systems like LeafSnap we can show that learning the features by a convolutional neural network (CNN) can provide better feature representation for leaf images compared to hand-crafted features. Methods We developed LeafNet, a CNN-based plant identification system. For evaluation, we utilized the publicly available LeafSnap, Flavia and Foliage datasets. Results Evaluating the recognition accuracies of LeafNet on the LeafSnap, Flavia and Foliage datasets reveals a better performance of LeafNet compared to hand-crafted customized systems. Conclusions Given the overall species diversity of plants, the goal of a complete automatisation of visual plant species identification is unlikely to be met solely by continually gathering assemblies of customized, specialized and hand-crafted (and therefore expensive) identification systems. Deep Learning CNN approaches offer a self-learning state-of-the-art alternative that allows adaption to different taxa just by presenting new training data instead of developing new software systems. © 2017 Elsevier B.V.},
	author_keywords = {Convolutional layers; Convolutional neural network; Deep learning; Feature maps; Feature representation; Plant classification},
	keywords = {algorithm; artificial neural network; automation; computer vision; data set; identification method; image analysis; image classification; leaf; map; software; taxonomy},
	correspondence_address = {P. Barré; University of Bonn, Institute of Computer Science IV, Bonn, Friedrich-Ebert-Allee 144, 53113, Germany; email: barre@cs.uni-bonn.de},
	publisher = {Elsevier B.V.},
	issn = {15749541},
	language = {English},
	abbrev_source_title = {Ecol. Informatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 176}
}

@CONFERENCE{Codizar2016,
	author = {Codizar, Azeil Louisse and Solano, Geoffrey},
	title = {Plant leaf recognition by venation and shape using artificial neural networks},
	year = {2016},
	journal = {IISA 2016 - 7th International Conference on Information, Intelligence, Systems and Applications},
	doi = {10.1109/IISA.2016.7785361},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013196853&doi=10.1109%2fIISA.2016.7785361&partnerID=40&md5=6aee7c211dbc1a896ea4922c0b899e1a},
	affiliations = {Department of Physical Sciences and Mathematics, College of Arts and Sciences, University of the Philippines, Manila, Philippines},
	abstract = {The number of known and unknown plant species increases as time goes by. Research on plant species can be further advanced if there is a quick and accurate system that can identify plants and hasten the classification process. This system will not only help in accelerating plant classification, but will also allow people who are not morphological experts to conduct their own studies. LeaVes is an application designed to classify different plant species based on the leaf's shape and venation. This system uses different image processing and machine learning techniques including centroid-radii, moment invariance, canny edge detection, morphological operations, image difference and artificial neural networks. © 2016 IEEE.},
	author_keywords = {artificial neural networks; leaf shape; plant leaf recognition; venation},
	keywords = {Edge detection; Image processing; Learning systems; Mathematical morphology; Neural networks; Canny edge detection; Classification process; Leaf shape; Machine learning techniques; Morphological operations; Plant classification; Plant leaf; venation; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150903429-1},
	language = {English},
	abbrev_source_title = {IISA - Int. Conf. Inf., Intell., Syst. Appl.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 7th International Conference on Information, Intelligence, Systems and Applications, IISA 2016; Conference date: 13 July 2016 through 15 July 2016; Conference code: 125393}
}

@CONFERENCE{Hu2016702,
	author = {Hu, Shuaiqi and Li, Ke and Bao, Xudong},
	title = {Wood species recognition based on SIFT keypoint histogram},
	year = {2016},
	journal = {Proceedings - 2015 8th International Congress on Image and Signal Processing, CISP 2015},
	pages = {702 – 706},
	doi = {10.1109/CISP.2015.7407968},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966671760&doi=10.1109%2fCISP.2015.7407968&partnerID=40&md5=d950a968b9acfdc90366214fabd58a38},
	affiliations = {School of Information Science and Engineering, Southeast University, Nanjing, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China},
	abstract = {Traditionally, only experts who are equipped with professional knowledge and rich experience are able to recognize different species of wood. Applying image processing techniques for wood species recognition can not only reduce the expense to train qualified identifiers, but also increase the recognition accuracy. In this paper, a wood species recognition technique base on Scale Invariant Feature Transformation (SIFT) keypoint histogram is proposed. We use first the SIFT algorithm to extract keypoints from wood cross section images, and then k-means and k-means++ algorithms are used for clustering. Using the clustering results, an SIFT keypoints histogram is calculated for each wood image. Furthermore, several classification models, including Artificial Neural Networks (ANN), Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) are used to verify the performance of the method. Finally, through comparing with other prevalent wood recognition methods such as GLCM and LBP, results show that our scheme achieves higher accuracy. © 2015 IEEE.},
	author_keywords = {pattern recognition; SIFT keypoint histogram; wood},
	keywords = {Neural Networks; Species Identification; Wood; Algorithms; Clustering algorithms; Graphic methods; Nearest neighbor search; Neural networks; Pattern recognition; Signal processing; Support vector machines; Wood; Classification models; Cross-section images; Image processing technique; K nearest neighbor (KNN); Keypoint; Professional knowledge; Recognition accuracy; Scale invariant feature transformations; Image processing},
	editor = {Wang L. and Lin S. and Tao Z. and Zeng B. and Hui X. and Shao L. and Liang J.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-146739098-9},
	language = {English},
	abbrev_source_title = {Proc. - Int. Congr. Image Signal Process., CISP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 22; Conference name: 8th International Congress on Image and Signal Processing, CISP 2015; Conference date: 14 October 2015 through 16 October 2015; Conference code: 119464; All Open Access, Green Open Access}
}

@CONFERENCE{Siravenha2015297,
	author = {Siravenha, Ana Carolina Quintao and Carvalho, Schubert R.},
	title = {Exploring the Use of Leaf Shape Frequencies for Plant Classification},
	year = {2015},
	journal = {Brazilian Symposium of Computer Graphic and Image Processing},
	volume = {2015-October},
	pages = {297 – 304},
	doi = {10.1109/SIBGRAPI.2015.36},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959335643&doi=10.1109%2fSIBGRAPI.2015.36&partnerID=40&md5=97acbda5efe7ccdff439b5e6d27b5c39},
	affiliations = {Federal University of Para, Vale Institute of Technology, Belem, Brazil; Vale Institute of Technology, Belem, Brazil},
	abstract = {Plant identification and classification play an important role in ecology, but the manual process is cumbersome even for experimented taxonomists. Technological advances allows the development of strategies to make these tasks easily and faster. In this context, this paper describes a methodology for plant identification and classification based on leaf shapes, that explores the discriminative power of the contour-centroid distance in the Fourier frequency domain in which some invariance (e.g. Rotation and scale) are guaranteed. In addition, it is also investigated the influence of feature selection techniques regarding classification accuracy. Our results show that by combining a set of features vectors - in the principal components space - and a feed forward neural network, an accuracy of 97.45% was achieved. © 2015 IEEE.},
	author_keywords = {Feature selection; Fourier transform; Plants classification; Shape features},
	keywords = {Feature extraction; Fourier transforms; Frequency domain analysis; Vector spaces; Classification accuracy; Discriminative power; Plant classification; Plant identification; Principal Components; Selection techniques; Shape features; Technological advances; Classification (of information)},
	publisher = {IEEE Computer Society},
	issn = {15301834},
	isbn = {978-146737962-5},
	language = {English},
	abbrev_source_title = {Braz. Symp. Comput. Graph. Image Process.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 28th SIBGRAPI Conference on Graphics, Patterns and Images, SIBGRAPI 2015; Conference date: 26 August 2015 through 29 August 2015; Conference code: 118552}
}

@CONFERENCE{Lee2017,
	author = {Lee, Sue Han and Chang, Yang Loong and Chan, Chee Seng},
	title = {LifeClef 2017 plant identification challenge: Classifying plants using generic-organ correlation features},
	year = {2017},
	journal = {CEUR Workshop Proceedings},
	volume = {1866},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034779131&partnerID=40&md5=95fcf4704089ed6b72aeaaa4a2baec38},
	affiliations = {Centre of Image and Signal Processing, Fac. Comp. Sci. and Info. Tech., University of Malaya, Malaysia},
	abstract = {This paper describes our proposal in the multi-organ plant identification task (LifeClef2017 challenge [8]). The objective of the challenge is to evaluate to what extent machine learning and computer vision can learn from noisy data compared to trusted data. To address the challenge, we employ our recent proposed hybrid generic-organ convolutional neural network, abbreviated HGO-CNN [11] to train on different composition of plant datasets. Overall, all the submitted runs obtained comparable results in the LifeClef2017 plant classification task.},
	author_keywords = {Convolutional neural network; Deep learning; Plant classification},
	keywords = {Convolution; Deep learning; Learning systems; Convolutional neural network; Correlation features; Noisy data; Plant classification; Plant identification; Neural networks},
	editor = {Mandl T. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Goeuriot L. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 18th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2017; Conference date: 11 September 2017 through 14 September 2017; Conference code: 131731}
}

@CONFERENCE{Wilhelm2016,
	author = {Wilhelm, Thorsten and Wohler, Christian},
	title = {Flexible Mixture Models for Colour Image Segmentation of Natural Images},
	year = {2016},
	journal = {2016 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2016},
	doi = {10.1109/DICTA.2016.7797044},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011017836&doi=10.1109%2fDICTA.2016.7797044&partnerID=40&md5=cb4787dbe85d7b100cc724821c2ba43d},
	affiliations = {Image Analysis Group, TU Dortmund, Dortmund, Germany},
	abstract = {This work proposes a way to increase the flexibility of mixture models for clustering applications. The focus of this paper resides at the segmentation of natural images, as one application of mixture models in real world applications. Furthermore, a novel method for estimating the number of mixture components utilising a Poisson regression approach is proposed. The derived procedure is applied and evaluated on the Berkeley Segmentation database (BSD500) [1] and on the LeafSnap Field dataset [2], a challenging real world application of segmentation tasks in the domain of plant recognition. © 2016 IEEE.},
	keywords = {Color image processing; Mixtures; Berkeley Segmentation Database; Clustering applications; Colour image segmentation; Mixture components; Mixture model; Natural images; Plant recognition; Poisson regression; Image segmentation},
	editor = {Liew A.W.-C. and Zhou J. and Gao Y. and Wang Z. and Fookes C. and Lovell B. and Blumenstein M.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150902896-2},
	language = {English},
	abbrev_source_title = {Int. Conf. Digit. Image Comput.: Techniques Appl., DICTA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2016 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2016; Conference date: 30 November 2016 through 2 December 2016; Conference code: 125581}
}

@CONFERENCE{De Luna20171,
	author = {De Luna, Robert G. and Baldovino, Renann G. and Cotoco, Ezekiel A. and De Ocampo, Anton Louise P. and Valenzuela, Ira C. and Culaba, Alvin B. and Gokongwei, Elmer P. Dadios},
	title = {Identification of philippine herbal medicine plant leaf using artificial neural network},
	year = {2017},
	journal = {HNICEM 2017 - 9th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment and Management},
	volume = {2018-January},
	pages = {1 – 8},
	doi = {10.1109/HNICEM.2017.8269470},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047757187&doi=10.1109%2fHNICEM.2017.8269470&partnerID=40&md5=a1e1f03817329d3711e08d5b4336fd91},
	affiliations = {College of Engineering, De La Salle University Manila, Philippines},
	abstract = {The study described in this paper consists of a system that involves image processing techniques to extract relevant features related to leaf in conjunction with using artificial neural network in order to detect and identify some Philippine herbal plants. Real samples of twelve different herbal medicine plant leaves are collected where each leaf are isolated in single image. Several features are extracted using techniques in image processing. With the artificial neural network acting as autonomous brain network, the system can identify the species of the herbal medicine plant leaves being tested. The system can also provide information about the diseases the herbal plant can cure. For the training, a features dataset of 600 images coming from 50 images per herbal plant are used. With the aid of Python, a neural network model with optimized parameters are established producing 98.16 % identification for the whole dataset. To evaluate the actual performance of the system, a separate 72 sample images of herbal plants are tested with the neural network model implemented in MATLAB. Experimental results demonstrate a 98.61 % accuracy of herbal plant identification. © 2017 IEEE.},
	author_keywords = {artificial neural network; feature extraction; image processing; leaf identification},
	keywords = {Environmental management; Feature extraction; Medicine; Nanotechnology; Neural networks; Plant extracts; Brain networks; Herbal medicines; Herbal plants; Image processing technique; Leaf identification; Neural network model; Optimized parameter; Relevant features; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153860910-1},
	language = {English},
	abbrev_source_title = {HNICEM - Int. Conf. Humanoid, Nanotechnol., Inf. Technol., Commun. Control, Environ. Manag.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 46; Conference name: 9th IEEE International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment and Management, HNICEM 2017; Conference date: 29 November 2017 through 1 December 2017; Conference code: 134372}
}

@ARTICLE{Zhang2017,
	author = {Zhang, Shanwen and Wu, Xiaowei and You, Zhuhong},
	title = {Jaccard distance based weighted sparse representation for coarse-to-fine plant species recognition},
	year = {2017},
	journal = {PLoS ONE},
	volume = {12},
	number = {6},
	doi = {10.1371/journal.pone.0178317},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020304208&doi=10.1371%2fjournal.pone.0178317&partnerID=40&md5=f553c0a348038130bafaba476d47551a},
	affiliations = {Department of Information Engineering, Xijing University, Xi'an, China; Department of Statistics, Virginia Tech, Blacksburg, VA, United States},
	abstract = {Leaf based plant species recognition plays an important role in ecological protection, however its application to large and modern leaf databases has been a long-standing obstacle due to the computational cost and feasibility. Recognizing such limitations, we propose a Jaccard distance based sparse representation (JDSR) method which adopts a two-stage, coarse to fine strategy for plant species recognition. In the first stage, we use the Jaccard distance between the test sample and each training sample to coarsely determine the candidate classes of the test sample. The second stage includes a Jaccard distance based weighted sparse representation based classification(WSRC), which aims to approximately represent the test sample in the training space, and classify it by the approximation residuals. Since the training model of our JDSR method involves much fewer but more informative representatives, this method is expected to overcome the limitation of high computational and memory costs in traditional sparse representation based classification. Comparative experimental results on a public leaf image database demonstrate that the proposed method outperforms other existing feature extraction and SRC based plant recognition methods in terms of both accuracy and computational speed. © 2017 Zhang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	keywords = {Algorithms; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Plant Leaves; Plants; Species Specificity; Acer mono; Alondra (plant); analytic method; Article; cherry blossom; classification algorithm; Ginkgo biloba; image analysis; image processing; Jaccard distance based sparse representation method; loquat; mathematical analysis; morphology; nonhuman; Pittosporum; plant; plant leaf; process optimization; sensitivity analysis; species identification; weighted sparse representation based classification method; algorithm; anatomy and histology; automated pattern recognition; plant; procedures; species difference},
	publisher = {Public Library of Science},
	issn = {19326203},
	coden = {POLNC},
	pmid = {28591147},
	language = {English},
	abbrev_source_title = {PLoS ONE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Robinson2016,
	author = {Robinson, Beth S. and Inger, Richard and Gaston, Kevin J.},
	title = {A rose by any other name: Plant identification knowledge & socio-demographics},
	year = {2016},
	journal = {PLoS ONE},
	volume = {11},
	number = {5},
	doi = {10.1371/journal.pone.0156572},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971275479&doi=10.1371%2fjournal.pone.0156572&partnerID=40&md5=06a1b189f28a0be99d0a0d2914ae5aea},
	affiliations = {Environment and Sustainability Institute, University of Exeter, Penryn, Cornwall, TR10 9FE, United Kingdom},
	abstract = {Concern has been expressed over societal losses of plant species identification skills. These losses have potential implications for engagement with conservation issues, gaining human wellbeing benefits from biodiversity (such as those resulting from nature-based recreational activities), and early warning of the spread of problematic species. However, understanding of the prevailing level of species identification skills, and of its key drivers, remains poor. Here, we explore socio-demographic factors influencing plant identification knowledge and ability to classify plants as native or non-native, employing a novel method of using real physical plants, rather than photographs or illustrations. We conducted face-to-face surveys at three different sites chosen to capture respondents with a range of socio-demographic circumstances, in Cornwall, UK. We found that survey participants correctly identified c.60% of common plant species, were significantly worse at naming non-native than native plants, and that less than 20% of people recognised Japanese knotweed Fallopia japonica, which is a widespread high profile invasive non-native in the study region. Success at naming plants was higher if participants were female, a member of at least one environmental, conservation or gardening organisation, in an older age group (than the base category of 18-29 years), or a resident (rather than visitor) of the study area. Understanding patterns of variation in plant identification knowledge can inform the development of education and engagement strategies, for example, by targeting sectors of society where knowledge is lowest. Furthermore, greater understanding of general levels of identification of problematic invasive non-native plants can guide awareness and education campaigns to mitigate their impacts. © 2016 Robinson et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	keywords = {Adolescent; Adult; Female; Humans; Introduced Species; Male; Pattern Recognition, Visual; Plants; Socioeconomic Factors; United Kingdom; awareness; case report; education; environmental protection; female; gardening; human; photography; plant identification; Polygonum cuspidatum; resident; species; adolescent; adult; introduced species; male; pattern recognition; plant; socioeconomics; United Kingdom},
	publisher = {Public Library of Science},
	issn = {19326203},
	coden = {POLNC},
	pmid = {27227452},
	language = {English},
	abbrev_source_title = {PLoS ONE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 31; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Mukherjee2017317,
	author = {Mukherjee, Soumen and Bhattacharjee, Arup Kumar and Ghosh, Manas and Ganguly, Trishita and Sinha, Rick Punyadyuti and Mandal, Swarup},
	title = {Plant leaf image recognition and classification using perceptron},
	year = {2017},
	journal = {Computational Science and Engineering - Proceedings of the International Conference on Computational Science and Engineering, ICCSE2016},
	pages = {317 – 320},
	doi = {10.1201/9781315375021-64},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018658903&doi=10.1201%2f9781315375021-64&partnerID=40&md5=9c5b5cdc581348a79a30964f84fef886},
	affiliations = {RCC Institute of Information Technology, Kolkata, India},
	abstract = {It is a challenging task for any lay man to analyze plant leaf images because of the many minute variations that exist in them, hence the need for a large data set for analysis. It is a difficult task to develop an automated recognition system which is able to process large information and provide correct estimations and figures. The result of the recognition process categorizes each individual plant sample to a descending series of related plants in terms of their characteristics. It is very time consuming. The focus of the computerized plant recognition system is on the detection and extraction of stable features of the plants. The information extracted from leaf veins, thus, play an important role in the process of identification. The goal of this work is to develop a system where the user can input a picture of any unknown plant leaf and the system classify the plant species in question and display sample images of the nearby matches. Artificial neural networks have applied to this type of problems in pattern recognition, classification and image analysis. © 2017 Taylor & Francis Group.},
	keywords = {Image analysis; Image recognition; Neural networks; Automated recognition; Large datasets; Plant leaf; Plant leaf images; Plant recognition; Plant samples; Plant species; Recognition process; Image classification},
	editor = {Deyasi A. and Debnath P. and Mukherjee S. and Bhattacharjee A.K.},
	publisher = {CRC Press/Balkema},
	isbn = {978-113802983-5},
	language = {English},
	abbrev_source_title = {Comput. Sci. Eng. Proc. Int. Conf. Comput. Sci. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: International Conference on Computational Science and Engineering, ICCSE 2016; Conference date: 4 October 2016 through 6 October 2016; Conference code: 190339}
}

@CONFERENCE{Alsuwaidi2016395,
	author = {Alsuwaidi, Ali and Veys, Charles and Hussey, Martyn and Grieve, Bruce and Yin, Hujun},
	title = {Hyperspectral selection based algorithm for plant classification},
	year = {2016},
	journal = {IST 2016 - 2016 IEEE International Conference on Imaging Systems and Techniques, Proceedings},
	pages = {395 – 400},
	doi = {10.1109/IST.2016.7738258},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004039892&doi=10.1109%2fIST.2016.7738258&partnerID=40&md5=4e4cbacf2806203266c0d74b2624ef03},
	affiliations = {School of Electrical and Electronic Engineering, University of Manchester, Manchester, M13 9PL, United Kingdom},
	abstract = {The popularity of using hyperspectral imaging systems in studying and monitoring plant properties and conditions has increased lately. This increase has been driven by both financial and environmental advantages of such systems. Using a nondestructive hyperspectral imaging system improves the breeding process, increases profit, and reduces the usage of herbicide, thus reducing side effects to plants and environment. This paper is concerned with the use of hyperspectral image analysis for differentiating different plant species as well as their conditions. The main contribution of the work lies in the use of feature selection for choosing relevant, discriminant spectral information as the input to the classifier (e.g. SVM), as compared to the use of empirical spectral indices. Two independent hyperspectral datasets, captured by different instrumentations, were used in evaluation. Experimental results show significant improvements in classification accuracy with several feature selection algorithms compared to with the spectral vegetation and disease indices. The study shows that systematically selection of wavelength features can shed light on attributes that differentiate plants and their conditions. © 2016 IEEE.},
	author_keywords = {feature selection; hyperspectral imaging; spectral indices; support vector machine},
	keywords = {Feature extraction; Imaging systems; Spectroscopy; Support vector machines; Classification accuracy; Feature selection algorithm; Hyperspectral image analysis; Hyperspectral Imaging; Hyperspectral imaging systems; Plant classification; Spectral indices; Spectral information; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150901817-8},
	language = {English},
	abbrev_source_title = {IST - IEEE Int. Conf. Imaging Syst. Techniques, Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 2016 IEEE International Conference on Imaging Systems and Techniques, IST 2016; Conference date: 4 October 2016 through 6 October 2016; Conference code: 124802; All Open Access, Green Open Access}
}

@CONFERENCE{Ghazi2016518,
	author = {Ghazi, Mostafa Mehdipour and Yanikoglu, Berrin and Aptoula, Erchan},
	title = {Open-set plant identification using an ensemble of deep convolutional neural networks},
	year = {2016},
	journal = {CEUR Workshop Proceedings},
	volume = {1609},
	pages = {518 – 524},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019560038&partnerID=40&md5=f6d07956e69662c881cd70d7efdd28c7},
	affiliations = {Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; Institute of Information Technologies, Gebze Technical University, Kocaeli, Turkey},
	abstract = {Open-set recognition, a challenging problem in computer vision, is concerned with identification or verification tasks where queries may belong to unknown classes. This work describes a fine-grained plant identification system consisting of an ensemble of deep convolutional neural networks within an open-set identification framework. Two wellknown deep learning architectures of VGGNet and GoogLeNet, pretrained on the object recognition dataset of ILSVRC 2012, are finetuned using the plant dataset of LifeCLEF 2015. Moreover, GoogLeNet is fine-Tuned using plant and non-plant images for rejecting samples from non-plant classes. Our systems have been evaluated on the test dataset of PlantCLEF 2016 by the campaign organizers and our best proposed model has achieved an official score of 0.738 in terms of the mean average precision, while the best official score is 0.742.},
	author_keywords = {Convolutional Neural Networks; Deep Learning; Open-Set Recognition; Plant Identification},
	keywords = {Convolution; Deep learning; Neural networks; Object recognition; Statistical tests; Convolutional neural network; Fine grained; Learning architectures; Open-Set Recognition; Plant identification; Plant identification systems; Unknown class; Verification task; Deep neural networks},
	editor = {Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Macdonald C. and Balog K.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 2016 Working Notes of Conference and Labs of the Evaluation Forum, CLEF 2016; Conference date: 5 September 2016 through 8 September 2016; Conference code: 127674}
}

@CONFERENCE{Pawara2017479,
	author = {Pawara, Pornntiwa and Okafor, Emmanuel and Surinta, Olarik and Schomaker, Lambert and Wiering, Marco},
	title = {Comparing local descriptors and bags of visualwords to deep convolutional neural networks for plant recognition},
	year = {2017},
	journal = {ICPRAM 2017 - Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods},
	volume = {2017-January},
	pages = {479 – 486},
	doi = {10.5220/0006196204790486},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046347005&doi=10.5220%2f0006196204790486&partnerID=40&md5=288a3e4634c21b4fed01a938eb7e909b},
	affiliations = {Institute of Artificial Intelligence and Cognitive Engineering (ALICE), University of Groningen, Nijenborgh 9, Groningen, Netherlands; Multi-Agent Intelligent Simulation Laboratory (MISL), Mahasarakham University, Mahasarakham, Thailand},
	abstract = {The use of machine learning and computer vision methods for recognizing different plants from images has attracted lots of attention from the community. This paper aims at comparing local feature descriptors and bags of visual words with different classifiers to deep convolutional neural networks (CNNs) on three plant datasets; AgrilPlant, LeafSnap, and Folio. To achieve this, we study the use of both scratch and fine-tuned versions of the GoogleNet and the AlexNet architectures and compare them to a local feature descriptor with k-nearest neighbors and the bag of visual words with the histogram of oriented gradients combined with either support vector machines and multi-layer perceptrons. The results shows that the deep CNN methods outperform the hand-crafted features. The CNN techniques can also learn well on a relatively small dataset, Folio. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
	author_keywords = {Bags of visual words; Convolutional neural network; Deep learning; Local descriptor; Plant classification},
	keywords = {Classification (of information); Convolution; Deep learning; Nearest neighbor search; Neural networks; Pattern recognition; Support vector machines; Bag-of-visual-words; Convolutional neural network; Histogram of oriented gradients; Local descriptors; Local feature descriptor; Multi-layer perceptrons; Plant classification; Visual word; Deep neural networks},
	editor = {De Marsico M. and di Baja G.S. and Fred A.},
	publisher = {SciTePress},
	isbn = {978-989758222-6},
	language = {English},
	abbrev_source_title = {ICPRAM - Proc. Int. Conf. Pattern Recognit. Appl. Methods},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 82; Conference name: 6th International Conference on Pattern Recognition Applications and Methods, ICPRAM 2017; Conference date: 24 February 2017 through 26 February 2017; Conference code: 134792; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Wagenaar2017448,
	author = {Wagenaar, Martijn and Okafor, Emmanuel and Frencken, Wouter and Wiering, Marco A.},
	title = {Using deep convolutional neural networks to predict goal-scoring opportunities in soccer},
	year = {2017},
	journal = {ICPRAM 2017 - Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods},
	volume = {2017-January},
	pages = {448 – 455},
	doi = {10.5220/0006194804480455},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040286414&doi=10.5220%2f0006194804480455&partnerID=40&md5=eecb623cb71b753c9607e23419e195ef},
	affiliations = {Institute of Artificial Intelligence and Cognitive Engineering (ALICE), University of Groningen, Groningen, Netherlands; Football Club Groningen, Groningen, Netherlands; Center of Human Movement Sciences, University of Groningen, Groningen, Netherlands},
	abstract = {Deep learning approaches have successfully been applied to several image recognition tasks, such as face, object, animal and plant classification. However, almost no research has examined on how to use the field of machine learning to predict goal-scoring opportunities in soccer from position data. In this paper, we propose the use of deep convolutional neural networks (DCNNs) for the above stated problem. This aim is actualized using the following steps: 1) development of novel algorithms for finding goal-scoring opportunities and ball possession which are used to obtain positive and negative examples. The dataset consists of position data from 29 matches played by a German Bundlesliga team. 2) These examples are used to create original and enhanced images (which contain object trails of soccer positions) with a resolution size of 256×256 pixels. 3) Both the original and enhanced images are fed independently as input to two DCNN methods: instances of both GoogLeNet and a 3-layered CNN architecture. A K-nearest neighbor classifier was trained and evaluated on ball positions as a baseline experiment. The results show that the GoogLeNet architecture outperforms all other methods with an accuracy of 67.1%. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
	author_keywords = {Convolutional neural networks; Goal-scoring opportunities in soccer; Image recognition},
	keywords = {Convolution; Image enhancement; Image recognition; Nearest neighbor search; Network architecture; Neural networks; Sports; Convolutional neural network; Goal-scoring opportunities in soccer; K-nearest neighbor classifier; Learning approach; Negative examples; Novel algorithm; Plant classification; Position data; Deep neural networks},
	editor = {De Marsico M. and di Baja G.S. and Fred A.},
	publisher = {SciTePress},
	isbn = {978-989758222-6},
	language = {English},
	abbrev_source_title = {ICPRAM - Proc. Int. Conf. Pattern Recognit. Appl. Methods},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 6th International Conference on Pattern Recognition Applications and Methods, ICPRAM 2017; Conference date: 24 February 2017 through 26 February 2017; Conference code: 134792; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Hang2016459,
	author = {Hang, Siang Thye and Tatsuma, Atsushi and Aono, Masaki},
	title = {Bluefield (KDE TUT) at LifeCLEF 2016 plant identification task},
	year = {2016},
	journal = {CEUR Workshop Proceedings},
	volume = {1609},
	pages = {459 – 468},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019562975&partnerID=40&md5=39110d3171e75843ddfe20d526768e16},
	affiliations = {Knowledge Data Engineering and Information Retrieval Laboratory (KDE Lab), Department of Computer Science and Engineering, Toyohashi University of Technology, Japan},
	abstract = {In this paper, we propose an automatic approach for plant image identification. We enhanced the well-known VGG 16-layers Convolutional Neural Network model [1] by replacing the last pooling layer with a Spatial Pyramid Pooling layer [2]. Rectified Linear Units (ReLU) are also replaced with Parametric ReLUs [3]. The enhanced model is trained without external da-Taset. A post processing method is also proposed to reject irrelevant samples. We further improved identification performance using observation identity (ObservationId) provided in the dataset. Our methods showed outstanding per-formance in official evaluation results of the LifeCLEF 2016 Plant Identifica-Tion Task.},
	author_keywords = {Deep Learning; LifeCLEF; Plant Identification; Sample Rejection.},
	keywords = {Deep learning; Automatic approaches; Convolutional neural network; Evaluation results; Image identification; LifeCLEF; Plant identification; Postprocessing methods; Spatial pyramids; Neural networks},
	editor = {Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Macdonald C. and Balog K.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2016 Working Notes of Conference and Labs of the Evaluation Forum, CLEF 2016; Conference date: 5 September 2016 through 8 September 2016; Conference code: 127674}
}

@CONFERENCE{Vyas201666,
	author = {Vyas, Shraddha and Fataniya, Bhupendra and Zaveri, Tanish and Acharya, Sanjeev},
	title = {Automatic image segmentation algorithm for microscopic images of Liquorice and Rhubarb},
	year = {2016},
	journal = {ACM International Conference Proceeding Series},
	volume = {21-24-September-2016},
	pages = {66 – 70},
	doi = {10.1145/2983402.2983422},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994175658&doi=10.1145%2f2983402.2983422&partnerID=40&md5=5d8f564b137ac09860fb9d298a77bdf9},
	affiliations = {Computer Engineering Department, Narnarayan Shastri Institute of Technology, Jetalpur, Ahmedabad, India; Electronics and Communication Department, Nirma University, Ahmedabad, India; Pharmacy Department, Nirma University, Ahmedabad, India},
	abstract = {This paper proposes an automated algorithm for plant identification using microscopic images of powder of herbal plants. In current scenario, the task of identifying plant from its powder form is done by pharmaceutical companies, which perform this task manually. This process takes lots of effort and time. Microscopic image of powder contains varieties of information, which are important evidence for identification of the plant. With every image, different type of noise are present, which makes the segmentation as a critical job. In this paper, we are proposing an algorithm which performs this task automatically by a computer. Our method consists two steps: "Pre-Processing" and "Image Segmentation". Firstly, microscopic images of "Liquorice" and "Rhubarb" plants were taken. On those images Top-hat and Bot-hat transformation are performed. Wiener Filter is used for image smoothing. An image segmentation is performed using Otsu's thresholding algorithm and find region of interest. The extra blobs were removed using morphological operations. Our proposed algorithm shows the efficiency for successfully detection of Liquorice and Rhubarb plants are 91.37% and 92.94% respectively. © 2016 ACM.},
	author_keywords = {Microscopic images; Morphology; Segmentation},
	keywords = {Computer vision; Image processing; Mathematical morphology; Morphology; Plants (botany); Automated algorithms; Automatic image segmentation; Microscopic image; Morphological operations; Pharmaceutical company; Plant identification; Region of interest; Thresholding algorithms; Image segmentation},
	publisher = {Association for Computing Machinery},
	isbn = {978-145034301-5},
	language = {English},
	abbrev_source_title = {ACM Int. Conf. Proc. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 3rd International Symposium on Computer Vision and the Internet, VisionNet 2016; Conference date: 21 September 2016 through 24 September 2016; Conference code: 124246}
}

@CONFERENCE{Naresh2016524,
	author = {Naresh, Y.G. and Nagendraswamy, H.S.},
	title = {A novel fuzzy LBP based symbolic representation technique for classification of medicinal plants},
	year = {2016},
	journal = {Proceedings - 3rd IAPR Asian Conference on Pattern Recognition, ACPR 2015},
	pages = {524 – 528},
	doi = {10.1109/ACPR.2015.7486558},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978881179&doi=10.1109%2fACPR.2015.7486558&partnerID=40&md5=817b2fc019c55f7eb0da6611c1ce2a72},
	affiliations = {DoS in Computer Science, University of Mysore, Mysore, India},
	abstract = {In this paper, a novel fuzzy LBP model for extracting texture features from medicinal plant leaves is proposed. The proposed method is invariant to image transformations and independent of any threshold. Concept of hierarchical clustering based on inconsistency coefficient is used to produce natural clusters for a particular species capturing intra-class variations due to environmental conditions and acquisition system. Interval valued type symbolic feature vector is used to represent each cluster effectively. Thus the proposed system suggests choosing multiple representatives for each species to make the representation more effective and robust. A chi-square distance measure is used to establish matching between the test and reference feature vectors of plant leaves and a nearest neighbor classification technique is used to classify an unknown test sample of medicinal plant leaf. Extensive experiments are conducted to demonstrate the efficacy of the proposed model on our own data set and other publically available leaf datasets. Results of the proposed work has been compared with the contemporary work and found to be superior. © 2015 IEEE.},
	author_keywords = {FLBP; LBP; Plant Classification; Texture},
	keywords = {Pattern recognition; Textures; Environmental conditions; FLBP; Hier-archical clustering; Image transformations; Multiple representatives; Nearest neighbor classification; Plant classification; Symbolic representation; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147996100-9},
	language = {English},
	abbrev_source_title = {Proc. - IAPR Asian Conf. Pattern Recognit., ACPR},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 3rd IAPR Asian Conference on Pattern Recognition, ACPR 2015; Conference date: 3 November 2016 through 6 November 2016; Conference code: 122205}
}

@ARTICLE{Faria2016101,
	author = {Faria, Fabio A and Almeida, Jurandy and Alberton, Bruna and Morellato, Leonor Patricia C and Rocha, Anderson and da S. Torres, Ricardo},
	title = {Time series-based classifier fusion for fine-grained plant species recognition},
	year = {2016},
	journal = {Pattern Recognition Letters},
	volume = {81},
	pages = {101 – 109},
	doi = {10.1016/j.patrec.2015.10.016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949485572&doi=10.1016%2fj.patrec.2015.10.016&partnerID=40&md5=34d8c7f6dce015f765d9be10d86d9240},
	affiliations = {Institute of Science and Technology, Federal University of São Paulo – UNIFESP, São José dos Campos SP, 12247-014, Brazil; Department of Botany, São Paulo State University – UNESP, Rio Claro SP, 13506-900, Brazil; Institute of Computing, University of Campinas – UNICAMP, Campinas, 13083-852, SP, Brazil},
	abstract = {Global warming and its resulting environmental changes surely are ubiquitous subjects nowadays and undisputedly important research topics. One way of tracking such environmental changes is by means of phenology, which studies natural periodic events and their relationship to climate. Phenology is seen as the simplest and most reliable indicator of the effects of climate change on plants and animals. The search for phenological information and monitoring systems has stimulated many research centers worldwide to pursue the development of effective and innovative solutions in this direction. One fundamental requirement for phenological systems is concerned with achieving fine-grained recognition of plants. In this sense, the present work seeks to understand specific properties of each target plant species and to provide the solutions for gathering specific knowledge of such plants for further levels of recognition and exploration in related tasks. In this work, we address some important questions such as: (i) how species from the same leaf functional group differ from each other; (ii) how different pattern classifiers might be combined to improve the effectiveness results in target species identification; and (iii) whether it is possible to achieve good classification results with fewer classifiers for fine-grained plant species identification. In this sense, we perform different analysis considering RGB color information channels from a digital hemispherical lens camera in different hours of day and plant species. A study about the correlation of classifiers associated with time series extracted from digital images is also performed. We adopt a successful selection and fusion framework to combine the most suitable classifiers and features improving the plant identification decision-making task as it is nearly impossible to develop just a single “silver bullet” image descriptor that would capture all subtle discriminatory features of plants within the same functional group. This adopted framework turns out to be an effective solution in the target task, achieving better results than well-known approaches in the literature. © 2015 Elsevier B.V.},
	author_keywords = {Classifier fusion; Diversity measures; Plant species identification},
	keywords = {Biology; Climate change; Decision making; Global warming; Search engines; Time series; Classification results; Classifier fusion; Diversity measure; Environmental change; Innovative solutions; Plant identification; Plant species identification; Specific properties; Image processing},
	correspondence_address = {F.A. Faria; Institute of Computing, University of Campinas – UNICAMP, Campinas, 13083-852, Brazil; email: ffaria@ic.unicamp.br},
	publisher = {Elsevier B.V.},
	issn = {01678655},
	language = {English},
	abbrev_source_title = {Pattern Recogn. Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access, Green Open Access}
}

@CONFERENCE{Isnanto2017455,
	author = {Isnanto, R Rizal and Zahra, Ajub Ajulian and Julietta, Patricia},
	title = {Pattern recognition on herbs leaves using region-based invariants feature extraction},
	year = {2017},
	journal = {Proceedings - 2016 3rd International Conference on Information Technology, Computer, and Electrical Engineering, ICITACEE 2016},
	pages = {455 – 459},
	doi = {10.1109/ICITACEE.2016.7892491},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018958481&doi=10.1109%2fICITACEE.2016.7892491&partnerID=40&md5=caa6454561adbb1359dab9df702610f2},
	affiliations = {Dept. of Computer Engineering, Diponegoro University, Semarang, Indonesia},
	abstract = {As medicine, herbal plants have been widely used since ancient times, and are still used today. There are various types of herbal plants that can be used as medicine but due to the limited ability of communities to recognize the type of plants and the lack of information, both cause the limited use of plants as medicine. In this research, an herbal plants identification system based on leaves pattern was developed. This identification system is based on the shape of the herbal plants' leaves. Before identification, preprocessing stages should be performed such as conversion to grayscale image, conversion to binary image, and image segmentation using Otsu's method. Feature extraction method used in this system is one kind of region-based invariant feature extraction, which is well-known as Hu's seven moments invariant and the Euclidean or Canberra distance as a recognition method. The research was conducted on 15 types of herbal plants. Based on the research, the percentage of recognition in this identification system using Euclidean Distance reached 86.67% with the lowest recognition rate is 40% for mangkokan leaf. While using Canberra distance for recognizing, the percentage of recognition is 72% and the lowest recognition rate is 20% for keji beling leaf. The best recognition rate of 100% for Euclidean distance similarity measure is reached when 9 (nine) types of leaves were implemented, i.e. banyan (beringin), binahong, dolar, keji-beling, Laos, noni (mengkudu), papaya, red betel (sirih merah), and soursop (sirsak) leaves. When Canberra distance used, 100% recognition rate was reached by 5 (five) leaves types, i.e. binahong, dolar, pecut-kuda, papaya, and red betel (sirih merah) leaves. © 2016 IEEE.},
	author_keywords = {Euclidean distance; herbal plant; Hu's seven moments invariant; Identification System; leaves pattern},
	keywords = {Binary images; Bins; Extraction; Feature extraction; Image segmentation; Pattern recognition; Euclidean distance; Feature extraction methods; Herbal plants; Hu's seven moments invariant; Invariant feature extraction; leaves pattern; Recognition methods; Similarity measure; Plants (botany)},
	editor = {Widianto E.D. and Arfan M. and Riyadi M.A. and Facta M.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150901434-7},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Inf. Technol., Comput., Electr. Eng., ICITACEE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 3rd International Conference on Information Technology, Computer, and Electrical Engineering, ICITACEE 2016; Conference date: 19 October 2016 through 21 October 2016; Conference code: 127215}
}

@ARTICLE{Wu201644,
	author = {Wu, Huisi and Liu, Jingjing and Li, Ping and Wen, Zhenkun},
	title = {Leaf recognition based on binary Gabor pattern and extreme learning machine},
	year = {2016},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {9916 LNCS},
	pages = {44 – 54},
	doi = {10.1007/978-3-319-48890-5_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007170677&doi=10.1007%2f978-3-319-48890-5_5&partnerID=40&md5=00e8e73a3882aa9f70af75ca0921f9a9},
	affiliations = {College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Mathematics and Information Technology, The Hong Kong Institute of Education, Hong Kong},
	abstract = {Automatic plant leaf recognition has been a hot research spot in the recent years, where encouraging improvements have been achieved in both recognition accuracy and speed. However, existing algorithms usually only extracted leaf features (such as shape or texture) or merely adopt traditional neural network algorithm to recognize leaf, which still showed limitation in recognition accuracy and speed especially when facing a large leaf database. In this paper, we present a novel method for leaf recognition by combining feature extraction and machine learning. To break the weakness exposed in the traditional algorithms, we applied binary Gabor pattern (BGP) and extreme learning machine (ELM) to recognize leaves. To accelerate the leaf recognition, we also extract BGP features from leaf images with an offline manner. Different from the traditional neural network like BP and SVM, our method based on the ELM only requires setting one parameter, and without additional fine-tuning during the leaf recognition. Our method is evaluated on several different databases with different scales. Comparisons with state-of-the-art methods were also conducted to evaluate the combination of BGP and ELM. Visual and statistical results have demonstrated its effectiveness. © Springer International Publishing AG 2016.},
	author_keywords = {Binary Gabor Pattern; Extreme Learning Machine; Leaf recognition; Leaf recognition processing batch},
	keywords = {Feature extraction; Knowledge acquisition; Plants (botany); Binary Gabor Pattern; Extreme learning machine; Leaf database; Leaf images; Leaf recognition; Neural network algorithm; Recognition accuracy; State-of-the-art methods; Learning systems},
	correspondence_address = {Z. Wen; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; email: wenzk@szu.edu.cn},
	editor = {Chen E. and Tie Y. and Gong Y.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331948889-9},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 17th Pacific-Rim Conference on Multimedia, PCM 2016; Conference date: 15 September 2016 through 16 September 2016; Conference code: 187409}
}

@ARTICLE{Servajean20171376,
	author = {Servajean, Maximilien and Joly, Alexis and Shasha, Dennis and Champ, Julien and Pacitti, Esther},
	title = {Crowdsourcing Thousands of Specialized Labels: A Bayesian Active Training Approach},
	year = {2017},
	journal = {IEEE Transactions on Multimedia},
	volume = {19},
	number = {6},
	pages = {1376 – 1391},
	doi = {10.1109/TMM.2017.2653763},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028344775&doi=10.1109%2fTMM.2017.2653763&partnerID=40&md5=1aede6d9fa76be3536444812a5ba9a01},
	affiliations = {Zenith Team, INRIA, LIRMM, Montpellier, 34095, France; Department of Computer Science, New York University, New York, 10003, NY, United States},
	abstract = {Large-scale annotated corpora have yielded impressive performance improvements in computer vision and multimedia content analysis. However, such datasets depend on an enormous amount of human labeling effort. When the labels correspond to well-known concepts, it is straightforward to train the annotators by giving a few examples with known answers. It is also straightforward to judge the quality of their labels. Neither is true when there are thousands of complex domain-specific labels. Training on all labels is infeasible and the quality of an annotator's judgements may be vastly different for some subsets of labels than for others. This paper proposes a set of data-driven algorithms to 1) train image annotators on how to disambiguate among automatically generated candidate labels, 2) evaluate the quality of annotators' label suggestions, and 3) weigh predictions. The algorithms adapt to the skills of each annotator both in the questions asked and the weights given to their answers. The underlying judgements are Bayesian, based on adaptive priors. We measure the benefits of these algorithms on a live user experiment related to image-based plant identification involving around 1000 people. The proposed methods are shown to enable huge gains in annotation accuracy. A standard user can correctly label around 2% of our data. This goes up to 80% with machine learning assisted training and assignment and up to almost 90% when doing a weighted combination of several annotators' labels. © 2017 IEEE.},
	author_keywords = {Bayes methods; Crowdsourcing; parameter estimation; Taylor series},
	keywords = {Crowdsourcing; Learning systems; Parameter estimation; Personnel training; Taylor series; Active trainings; Automatically generated; Bayes method; Data-driven algorithm; Label suggestion; Multimedia content analysis; Plant identification; User experiments; Quality control},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {15209210},
	coden = {ITMUF},
	language = {English},
	abbrev_source_title = {IEEE Trans Multimedia},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Green Open Access}
}

@CONFERENCE{Sahay2016914,
	author = {Sahay, Aparajita and Chen, Min},
	title = {Leaf analysis for plant recognition},
	year = {2016},
	journal = {Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
	volume = {0},
	pages = {914 – 917},
	doi = {10.1109/ICSESS.2016.7883214},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017025623&doi=10.1109%2fICSESS.2016.7883214&partnerID=40&md5=d899317a36fd7bfded231fde439a933d},
	affiliations = {Computing and Software Systems, School of STEM, University of Washington Bothell, 98011, WA, United States},
	abstract = {Plants are essential resources for nature and people's lives. Plant recognition provides valuable information for plant research and development, and has great impact on environmental protection and exploration. This paper presents a leaf analysis system for plant identification, which consists of three main components. First, given a leaf image, a preprocessing step is conducted for noise reduction. Second, the feature extraction component identifies representative features and computes scale invariant feature descriptors. Third, the matching plant species are identified and returned using a weighted K-nearest neighbor search algorithm. The system is implemented as a Windows phone app and is tested on the LeafSnapdataset[8], an electronic field guide developed by Columbia University and University of Maryland with different combinations of species at various orientations, scales and levels of brightness. The experimental results demonstrate the effectiveness of our proposed framework in plant recognition. © 2016 IEEE.},
	author_keywords = {KNN search; leaf analysis; plant recognition; SIFT},
	keywords = {Noise abatement; Software engineering; k-NN search; leaf analysis; Plant recognition; Research and development; Scale invariant features; SIFT; University of Maryland; Weighted k-nearest neighbors; Nearest neighbor search},
	editor = {Babu M.S.P. and Wenzheng L.},
	publisher = {IEEE Computer Society},
	issn = {23270586},
	isbn = {978-146739903-6},
	language = {English},
	abbrev_source_title = {Proc.IEEE Int. Conf. Software Eng. Serv. Sci., ICSESS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; Conference name: 7th IEEE International Conference on Software Engineering and Service Science, ICSESS 2016; Conference date: 26 August 2016 through 28 August 2016; Conference code: 126924}
}

@CONFERENCE{Goëau2017,
	author = {Goëau, Hervé and Bonnet, Pierre and Joly, Alexis},
	title = {Plant identification based on noisy web data: The amazing performance of deep learning (LifeCLEF 2017)},
	year = {2017},
	journal = {CEUR Workshop Proceedings},
	volume = {1866},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034749875&partnerID=40&md5=c281a97370cc1809a0beba94b8671e03},
	affiliations = {CIRAD, UMR AMAP, France; Inria ZENITH Team, France; LIRMM, Montpellier, France},
	abstract = {The 2017-th edition of the LifeCLEF plant identification challenge is an important milestone towards automated plant identification systems working at the scale of continental floras with 10.000 plant species living mainly in Europe and North America illustrated by a total of 1.1M images. Nowadays, such ambitious systems are enabled thanks to the conjunction of the dazzling recent progress in image classification with deep learning and several outstanding international initiatives, such as the Encyclopedia of Life (EOL), aggregating the visual knowledge on plant species coming from the main national botany institutes. However, despite all these efforts the majority of the plant species still remain without pictures or are poorly illustrated. Outside the institutional channels, a much larger number of plant pictures are available and spread on the web through botanist blogs, plant lovers web-pages, image hosting websites and on-line plant retailers. The LifeCLEF 2017 plant challenge presented in this paper aimed at evaluating to what extent a large noisy training dataset collected through the web and containing a lot of labelling errors can compete with a smaller but trusted training dataset checked by experts. To fairly compare both training strategies, the test dataset was created from a third data source, i.e. the Pl@ntNet mobile application that collects millions of plant image queries all over the world. This paper presents more precisely the resources and assessments of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of the main outcomes.},
	author_keywords = {Bark; Benchmark; Branch; Citizen-science; Collection; Evaluation; Finegrained classification; Flower; Fruit; Images; Leaf; Leaves; LifeCLEF; Plant; Retrieval; Species; Species identification; Stem},
	keywords = {Benchmarking; Deep learning; Fruits; Image processing; Refuse collection; Statistical tests; STEM (science, technology, engineering and mathematics); Websites; Bark; Branch; Citizen science; Evaluation; Flower; Images; Leaf; Leaves; LifeCLEF; Plant; Retrieval; Species; Species identification; Plants (botany)},
	editor = {Mandl T. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Goeuriot L. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20; Conference name: 18th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2017; Conference date: 11 September 2017 through 14 September 2017; Conference code: 131731}
}

@CONFERENCE{Bertrand2017435,
	author = {Bertrand, Sarah and Cerutti, Guillaume and Tougne, Laure},
	title = {Bark recognition to improve leaf-based classification in didactic tree species identification},
	year = {2017},
	journal = {VISIGRAPP 2017 - Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
	volume = {4},
	pages = {435 – 442},
	doi = {10.5220/0006108504350442},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037841699&doi=10.5220%2f0006108504350442&partnerID=40&md5=0825ae58464f648b103909500fe5d180},
	affiliations = {Univ. Lyon, LIRIS, Lyon, F-69676, France; INRIA, Virtual Plants INRIA Team, Montpellier, France},
	abstract = {In this paper, we propose a botanical approach for tree species classification through automatic bark analysis. The proposed method is based on specific descriptors inspired by the characterization keys used by botanists, from visual bark texture criteria. The descriptors and the recognition system are developed in order to run on a mobile device, without any network access. Our obtained results show a similar rate when compared to the state of the art in tree species identification from bark images with a small feature vector. Furthermore, we also demonstrate that the consideration of the bark identification significantly improves the performance of tree classification based on leaf only. © 2017 by SCITEPRESS - Science and Technology Publications, Lda.},
	author_keywords = {Bark; Leaf; Smart-phone; Tree Recognition},
	keywords = {Computer graphics; Forestry; Smartphones; Textures; Bark; Leaf; Network access; Recognition systems; Small features; State of the art; Tree Recognition; Tree species identifications; Computer vision},
	correspondence_address = {S. Bertrand; Univ. Lyon, LIRIS, Lyon, F-69676, France; email: sarah.bertrand@liris.cnrs.fr},
	editor = {Imai F. and Tremeau A. and Braz J.},
	publisher = {SciTePress},
	isbn = {978-989758225-7},
	language = {English},
	abbrev_source_title = {VISIGRAPP - Proc. Int. Jt. Conf. Comput. Vis., Imaging Compu. Graph. Theory Appl.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP 2017; Conference date: 27 February 2017 through 1 March 2017; Conference code: 134874; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Pradeep Kumar2017531,
	author = {Pradeep Kumar, Thallapally and Veera Prasad Reddy, M. and Bora, Prabin Kumar},
	title = {Leaf identification using shape and texture features},
	year = {2017},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {460 AISC},
	pages = {531 – 541},
	doi = {10.1007/978-981-10-2107-7_48},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009513389&doi=10.1007%2f978-981-10-2107-7_48&partnerID=40&md5=6e0bb37e80f85050d67dd52ef17d721b},
	affiliations = {Department of Electronics and Electrical Engineering, Indian Institute of Technology Guwahati, Guwahati, 781039, India},
	abstract = {Identifying plant species based on a leaf image is a challenging task. This paper presents a leaf recognition system using orthogonal moments as shape descriptors and Histogram of oriented gradients (HOG) and Gabor features as texture descriptors. The shape descriptors captures the global shape of leaf image. The internal vein structure is captured by the texture features. The binarized leaf image is pre-processed to make it scale, rotation and translation-invariant. The Krawtchouk moments are computed from the scale and rotation normalized shape image. The HOG feature is computed on rotation normalized gray image. The combined shape and texture features are classified with a support vector machine classifier (SVM). © Springer Science+Business Media Singapore 2017.},
	author_keywords = {Gabor filter; Histogram of oriented gradients; Krawtchouk moments; Plant identification},
	keywords = {Computer vision; Gabor filters; Graphic methods; Support vector machines; Histogram of oriented gradients; Histogram of oriented gradients (HOG); Krawtchouk moment; Leaf identification; Plant identification; Support vector machine classifiers; Texture descriptors; Translation invariants; Image processing},
	correspondence_address = {T. Pradeep Kumar; Department of Electronics and Electrical Engineering, Indian Institute of Technology Guwahati, Guwahati, 781039, India; email: pradeepiitg29@gmail.com},
	editor = {Kumar S. and Raman B. and Roy [initials]P.P. and Sen D.},
	publisher = {Springer Verlag},
	issn = {21945357},
	isbn = {978-981102106-0},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: International Conference on Computer Vision and Image Processing, CVIP 2016; Conference date: 26 February 2016 through 28 February 2016; Conference code: 188049}
}

@CONFERENCE{Mukherjee201698,
	author = {Mukherjee, Gunjan and Chatterjee, Arpitam and Tudu, Bipan},
	title = {Study on the potential of combined GLCM features towards medicinal plant classification},
	year = {2016},
	journal = {2016 2nd International Conference on Control, Instrumentation, Energy and Communication, CIEC 2016},
	pages = {98 – 102},
	doi = {10.1109/CIEC.2016.7513746},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979986176&doi=10.1109%2fCIEC.2016.7513746&partnerID=40&md5=130de54bbbfffe57bb6dc9e960200407},
	affiliations = {Department of MCA, Regent Education and Research Foundation, Kolkata, India; Department of Printing Engineering, Jadavpur University, Kolkata, India; Department of Instrumentation and Electronics Engineering, Jadavpur University, Kolkata, India},
	abstract = {The gray level co-occurrence matrix (GLCM) is widely used for textural feature extraction. The features obtained from GLCM matrix are subjected to the classifiers for the purpose of identification and classification. In this paper the combinations between different features, obtained from GLCM matrix, are studied. For experiments, the leaves of medicinal plants are considered, as proper identification of medicinal plant is crucial for appropriate utilization of their medicinal values. Two popular Indian medicinal plants, namely, Neem and Tulsi have been considered for classification using back propagation multi layer perceptron (BP-MLP) neural network classifier. Beside combination the further classification improvements may be achieved using different preprocessing techniques, which have also been experimented. The results show that preprocessed combined GLCM features can provide higher classification rate compared to raw single GLCM features. © 2016 IEEE.},
	author_keywords = {BP-MLP; Computer vision; data preprocessing; GLCM; medicinal plants classification; neural network},
	keywords = {Backpropagation; Computer vision; Feature extraction; Neural networks; Plants (botany); BP-MLP; Data preprocessing; GLCM; Gray level co occurrence matrix(GLCM); Medicinal plants; Multi layer perceptron; Neural network classifier; Preprocessing techniques; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150900035-7},
	language = {English},
	abbrev_source_title = {Int. Conf. Control, Instrum., Energy Commun., CIEC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; Conference name: 2nd International Conference on Control, Instrumentation, Energy and Communication, CIEC 2016; Conference date: 28 January 2016 through 30 January 2016; Conference code: 122732}
}

@CONFERENCE{Salima2016275,
	author = {Salima, Adzkia and Herdiyeni, Yeni and Douady, Stephane},
	title = {Leaf vein segmentation of medicinal plant using Hessian matrix},
	year = {2016},
	journal = {ICACSIS 2015 - 2015 International Conference on Advanced Computer Science and Information Systems, Proceedings},
	pages = {275 – 279},
	doi = {10.1109/ICACSIS.2015.7415152},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964555621&doi=10.1109%2fICACSIS.2015.7415152&partnerID=40&md5=267ad9984d413a5ac262e2b52b9cde92},
	affiliations = {Department of Computer Science, Faculty of Mathematics and Natural Sciences, Bogor Agricultural University, West Java, Indonesia; Laboratoire: Matiere et System Complex, Batiment Condorcet 10 Rue Alice Domon EtLeonie Duquet, Université Paris Diderot CC, Paris Cedex, 7056 75205, France},
	abstract = {This paper proposes a leaf vein segmentation using Hessian matrix. Leaf venation pattern is a biometric feature that form the basis of leaf characterization and classification. It is specific in certain species thus it can be used as a key feature. Hessian Matrix is a method of the second derivative ridge detection that can be used to segment the image based on its group structure by analyzing eigenvalues of the pixel. We applied thinning to achive the better result of leaf vein. In addition, we performed morphological image processing to fix broken ridges or unconnected leaf veins. We have evaluated four veins type of 80 digital leaf. The experimental results show that 53.75% of leaf image scored 2 and 42.5% scored 1 which means our proposed method has good performance to extract the primary, secondary veins and tertiary leaf vein. This method is promising to help botanist and taxonomist identifying medicinal plant species automatically. © 2015 IEEE.},
	author_keywords = {eigenvalue of hessian matrix analysis; leaf vein segmentation; Plant classification pre-process; ridge detection},
	keywords = {Eigenvalues and eigenfunctions; Forestry; Image segmentation; Information systems; Plant extracts; Plants (botany); Biometric features; Hessian matrices; Hessian matrix analysis; Medicinal plants; Morphological image processing; Plant classification; Ridge detections; Second derivatives; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150900362-4},
	language = {English},
	abbrev_source_title = {ICACSIS - Int. Conf. Adv. Comput. Sci. Inf. Syst., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; Conference name: International Conference on Advanced Computer Science and Information Systems, ICACSIS 2015; Conference date: 10 October 2015 through 11 October 2015; Conference code: 119505}
}

@ARTICLE{Liu2016460,
	author = {Liu, Nian and Kan, Jiang-ming},
	title = {Improved deep belief networks and multi-feature fusion for leaf identification},
	year = {2016},
	journal = {Neurocomputing},
	volume = {216},
	pages = {460 – 467},
	doi = {10.1016/j.neucom.2016.08.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994469223&doi=10.1016%2fj.neucom.2016.08.005&partnerID=40&md5=3bbc36fce3362990b925b4fda4cb7b96},
	affiliations = {School of Technology, Beijing Forestry University, 35# Qinghua East Road,Haidian District, Beijing, 100083, China},
	abstract = {Plant identification based on digital leaf images is a hot topic in the automatic classification of plants. However, due to the increase in the number of plant species, the leaf recognition rate is low because the traditional classification methods extract the few characteristics or use the classifiers with simple structures. This paper applied a combination of texture features and shape features for identification. Texture features include local binary patterns, Gabor filters and gray level co-occurrence matrices, while the shape feature vector is modeled using Hu moment invariants and Fourier descriptors. Improved deep belief networks (DBNs) with dropout, which use proportion integration differentiation control (PID) to decrease the reconstruction error in the process of pre-training, are used as the classifiers. The proposed algorithm was tested on the ICL dataset, and the average recognition rate is 93.9% for 220 types of leaves. The experimental results show that the proposed method has a higher recognition rate and is more robust than the traditional methods, and the training process is completed in a shorter time. © 2016 Elsevier B.V.},
	author_keywords = {DBNs; Leaf identification; PID; Shape feature; Texture feature},
	keywords = {Content based retrieval; Gabor filters; Image segmentation; Plants (botany); Automatic classification; Classification methods; DBNs; Gray-level co-occurrence matrix; Leaf identification; Proportion integration differentiation control(PID); Shape features; Texture features; Article; artificial neural network; automated pattern recognition; Bayesian learning; controlled study; deep belief network; image processing; leaf morphology; mathematical model; plant leaf; priority journal; support vector machine; Image processing},
	correspondence_address = {J.-M. Kan; School of Technology, Beijing Forestry University, Beijing,  Qinghua East Road,Haidian District, 100083, China; email: kanjm@bjfu.edu.cn},
	publisher = {Elsevier B.V.},
	issn = {09252312},
	coden = {NRCGE},
	language = {English},
	abbrev_source_title = {Neurocomputing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 53}
}

@CONFERENCE{Carranza-Rojas2016,
	author = {Carranza-Rojas, Jose and Mata-Montero, Erick},
	title = {On the significance of leaf sides in automatic leaf-based plant species identification},
	year = {2016},
	journal = {2016 IEEE 36th Central American and Panama Convention, CONCAPAN 2016},
	doi = {10.1109/CONCAPAN.2016.7942341},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021440220&doi=10.1109%2fCONCAPAN.2016.7942341&partnerID=40&md5=60db1736d8d165ba4b588d239bd7e8bd},
	affiliations = {PARMA Group, School of Computing, Costa Rica Institute of Technology, Cartago, Costa Rica},
	abstract = {Because the front side of a leaf and the underside are functionally very different - the former captures sunlight to produce photosynthesis and the latter absorbs carbon dioxide and releases oxygen and vapor - they typically have different visual features. In this paper we study the significance of leaf sides in visual recognition systems for automatic plant species identification. We measure the accuracy of species identifications with a dataset of 63 species of trees from Costa Rica that includes pictures of both, front sides and undersides of tree leaves. The dataset is used as a global dataset and is also partitioned as two datasets: one of front side pictures and one of underside pictures. Training and testing of different algorithms is performed and their accuracies computed for the group of species and for each individual species. For the tested dataset, leaf side is a significant factor for automatic plant species identification. On the average, and for most cases, underside pictures lead to more accurate identifications. © 2016 IEEE.},
	author_keywords = {Biodiversity Informatics; Computer Vision; Image Processing; Plant Identification},
	keywords = {Biodiversity; Carbon dioxide; Computer vision; Forestry; Image processing; Costa Rica; Plant identification; Plant species identification; Species identification; Training and testing; Tree leaves; Visual feature; Visual recognition; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-146739578-6},
	language = {English},
	abbrev_source_title = {IEEE Cent. Am. Panama Conv., CONCAPAN},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 36th IEEE Central American and Panama Convention, CONCAPAN 2016; Conference date: 9 November 2016 through 11 November 2016; Conference code: 128222; All Open Access, Green Open Access}
}

@CONFERENCE{Wable2017645,
	author = {Wable, Pradip B. and Chilveri, Purushottam G.},
	title = {Neural network based leaf recognition},
	year = {2017},
	journal = {International Conference on Automatic Control and Dynamic Optimization Techniques, ICACDOT 2016},
	pages = {645 – 648},
	doi = {10.1109/ICACDOT.2016.7877665},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017315764&doi=10.1109%2fICACDOT.2016.7877665&partnerID=40&md5=2095319edaba62821152bd34642a763d},
	affiliations = {SKNCOE Vadgaon bk, Pune, India},
	abstract = {Plants are related to human. Recognize an unfamiliar plant correctly without any expert understanding is big task. Due to Improvement in image processing, it is likely to know leaf image rapidly from which species it is. Pulse coupled neural network is a helpful tool for feature extraction. Entropy sequence is key feature which is obtained from pulse-coupled neural network. Along with entropy sequence other features such as aspect ratio, Zernike moments, Hu's invariants, Form factor, Rectangularity, circularity, area. Artificial neural network is taken as classifier. Proposed method gives better recognition rate compared to existing methods. © 2016 IEEE.},
	author_keywords = {ANN Classification; Feature Extraction; Image Processing; Neural Network},
	keywords = {Aspect ratio; Automation; Deep neural networks; Entropy; Extraction; Feature extraction; Neural networks; Process control; ANN classification; Entropy sequences; Form factors; Key feature; Leaf images; Leaf recognition; Pulse coupled neural network; Zernike moments; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150902080-5},
	language = {English},
	abbrev_source_title = {Int. Conf. Autom. Control Dyn. Optim. Tech., ICACDOT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 1st International Conference on Automatic Control and Dynamic Optimization Techniques, ICACDOT 2016; Conference date: 9 September 2016 through 10 September 2016; Conference code: 126850}
}

@ARTICLE{Yang2017367,
	author = {Yang, Meng-Meng and Phillips, Preetha and Wang, Shuihua and Zhang, Yudong},
	title = {Leaf recognition for plant classification based on wavelet entropy and back propagation neural network},
	year = {2017},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10464 LNAI},
	pages = {367 – 376},
	doi = {10.1007/978-3-319-65298-6_34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028296576&doi=10.1007%2f978-3-319-65298-6_34&partnerID=40&md5=0d3422264349aa170bc194433bd1aa23},
	affiliations = {School of Computer Science and Technology, Nanjing Normal University, Nanjing, 210023, Jiangsu, China; School of Natural Sciences and Mathematics, Shepherd University, Shepherdstown, 25443, WV, United States; West Virginia School of Osteopathic Medicine, 400 N Lee St., Lewisburg, 24901, WV, United States; Department of Electrical Engineering, The City College of New York, CUNY, New York, 10031, NY, United States; Jiangsu Key Laboratory of Advanced Manufacturing Technology, Huaiyin, 223003, Jiangsu, China},
	abstract = {In this paper, we proposed a method for plant classification, which aims to recognize the type of leaves from a set of image instances captured from same viewpoints. Firstly, for feature extraction, this paper adopted the 2-level wavelet transform and obtained in total 7 features. Secondly, the leaves were automatically recognized and classified by Back-Propagation neural network (BPNN). Meanwhile, we employed K-fold cross-validation to test the correctness of the algorithm. The accuracy of our method achieves 90.0%. Further, by comparing with other methods, our method arrives at the highest accuracy. © Springer International Publishing AG 2017.},
	author_keywords = {Back-Propagation; Classification; Feature extraction; K-fold cross-validation; Pattern recognition},
	keywords = {Classification (of information); Extraction; Feature extraction; Neural networks; Pattern recognition; Plants (botany); Robotics; Torsional stress; Wavelet transforms; Back-propagation neural networks; K fold cross validations; Leaf recognition; Plant classification; Wavelet entropies; Backpropagation},
	correspondence_address = {Y. Zhang; School of Computer Science and Technology, Nanjing Normal University, Nanjing, 210023, China; email: yudongzhang@ieee.org},
	editor = {Liu H. and Huang Y. and Wu H. and Yin Z.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331965297-9},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 10th International Conference on Intelligent Robotics and Applications, ICIRA 2017; Conference date: 16 August 2017 through 18 August 2017; Conference code: 196179}
}

@CONFERENCE{Tóth2016569,
	author = {Tóth, Bálint Pál and Osváth, Márton and Papp, Dávid and Szucs, Gábor},
	title = {Deep learning and SVM classification for plant recognition in content-based large scale image retrieval},
	year = {2016},
	journal = {CEUR Workshop Proceedings},
	volume = {1609},
	pages = {569 – 578},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019634405&partnerID=40&md5=e857167f3f273694424f11248543e2d7},
	affiliations = {Department of Telecommunications and Media Informatics, Budapest University of Technology and Economics, Magyar Tudósok krt. 2, Budapest, H-1117, Hungary},
	abstract = {The PlantCLEF 2016 challenge focused on tree, herb and fern species identification based on different types of images. The aim of the task was to classify the plants in the images to species and to give a confidence score depicting the probability that a prediction is true. We elaborated different classification methods for this challenge. We applied dense SIFT for feature detection and description; and Gaussian Mixture Model based Fisher vector was calculated to represent an image with high-level descriptor. Fisher vectors were classified by a special SVM, the C-support vector classification algorithm with RBF (Radial Basis Function) kernel. Furthermore, we applied deep learning method to train convolutional neural network (CNN) for feature learning and fullyconnected layers with softmax output for classification. We also combined these classifiers using the weighted average of their outputs. The final results show that the CNN achieved better result than the SVM, and the combined method slightly surpasses the CNN.},
	author_keywords = {C-support vector classification; Convolutional Neural Networks; Deep Learning; GMM based Fisher vector},
	keywords = {Convolution; Deep learning; Gaussian distribution; Image classification; Image segmentation; Neural networks; Plants (botany); Radial basis function networks; Vectors; Classification methods; Convolutional neural network; Fisher vectors; Gaussian Mixture Model; High-level descriptor; RBF(radial basis function); Species identification; Support vector classification; Image retrieval},
	editor = {Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Macdonald C. and Balog K.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 2016 Working Notes of Conference and Labs of the Evaluation Forum, CLEF 2016; Conference date: 5 September 2016 through 8 September 2016; Conference code: 127674}
}

@CONFERENCE{Lee2015452,
	author = {Lee, Sue Han and Chan, Chee Seng and Wilkin, Paul and Remagnino, Paolo},
	title = {Deep-plant: Plant identification with convolutional neural networks},
	year = {2015},
	journal = {Proceedings - International Conference on Image Processing, ICIP},
	volume = {2015-December},
	pages = {452 – 456},
	doi = {10.1109/ICIP.2015.7350839},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956626642&doi=10.1109%2fICIP.2015.7350839&partnerID=40&md5=f97567d2a068a5c4f935918b45d9072f},
	affiliations = {Centre of Image and Signal Processing, Fac. Comp. Sci. and Info. Tech., University of Malaya, Malaysia; Dept. Natural Capital and Plant Health, Royal Botanic Gardens, Kew, United Kingdom; Comp. and Info. Sys., Kingston University, United Kingdom},
	abstract = {This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a 'black box' solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely represent each of the plant species. Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features. © 2015 IEEE.},
	author_keywords = {deep learning; feature visualisation; plant classification},
	publisher = {IEEE Computer Society},
	issn = {15224880},
	isbn = {978-147998339-1},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Image Process. ICIP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 350; Conference name: IEEE International Conference on Image Processing, ICIP 2015; Conference date: 27 September 2015 through 30 September 2015; Conference code: 117806; All Open Access, Green Open Access}
}

@ARTICLE{Jeon201726,
	author = {Jeon, Wang-Su and Rhee, Sang-Yong},
	title = {Plant leaf recognition using a convolution neural network},
	year = {2017},
	journal = {International Journal of Fuzzy Logic and Intelligent Systems},
	volume = {17},
	number = {1},
	pages = {26 – 34},
	doi = {10.5391/IJFIS.2017.17.1.26},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030544616&doi=10.5391%2fIJFIS.2017.17.1.26&partnerID=40&md5=4edb6831decbbfebee044850272ee5f1},
	affiliations = {Department of IT Convergence Engineering, Kyungnam University, Changwon, South Korea; Department of Computer Engineering, Kyungnam University, Changwon, South Korea},
	abstract = {There are hundreds of kinds of trees in the natural ecosystem, and it can be very difficult to distinguish between them. Botanists and those who study plants however, are able to identify the type of tree at a glance by using the characteristics of the leaf. Machine learning is used to automatically classify leaf types. Studied extensively in 2012, this is a rapidly growing field based on deep learning. Deep learning is itself a self-learning technique used on large amounts of data, and recent developments in hardware and big data have made this technique more practical. We propose a method to classify leaves using the CNN model, which is often used when applying deep learning to image processing. © The Korean Institute of Intelligent Systems.},
	author_keywords = {Classification; CNN; GoogleNet; Leaf; Visual system},
	correspondence_address = {S.-Y. Rhee; Department of Computer Engineering, Kyungnam University, Changwon, South Korea; email: syrhee@kyungnam.ac.kr},
	publisher = {Korean Institute of Intelligent Systems},
	issn = {15982645},
	language = {English},
	abbrev_source_title = {Int. J. Fuzzy Log. Intell. Sys.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 112; All Open Access, Gold Open Access}
}

@CONFERENCE{Affouard2017,
	author = {Affouard, Antoine and Goeau, Hervé and Bonnet, Pierre and Lombardo, Jean-Christophe and Joly, Alexis},
	title = {Pl@ntnet app in the era of deep learning},
	year = {2017},
	journal = {5th International Conference on Learning Representations, ICLR 2017 - Workshop Track Proceedings},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111402920&partnerID=40&md5=61e2dfff1d7a97b4c64d0eac0bafa4b2},
	affiliations = {Pl@ntNet Project, AMAP Joint Research Unit, France; Pl@ntNet Project, Inria, ZENITH Team, France},
	abstract = {Pl@ntNet is a large-scale participatory platform and information system dedicated to the production of botanical data through image-based plant identification. In June 2015, Pl@ntNet mobile front-ends moved from classical hand-crafted visual features to deep-learning based image representations. This paper gives an overview of today's Pl@ntNet architecture and discusses how the introduction of convolutional neural networks did improve the whole workflow along the years. © 5th International Conference on Learning Representations, ICLR 2017 - Workshop Track Proceedings. All Rights Reserved.},
	keywords = {E-learning; Image representation; Neural networks; Convolutional neural network; Front end; Image representations; Image-based; Large-scales; Plant identification; Visual feature; Work-flows; Deep learning},
	publisher = {International Conference on Learning Representations, ICLR},
	language = {English},
	abbrev_source_title = {Int. Conf. Learn. Represent., ICLR - Workshop Track Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24; Conference name: 5th International Conference on Learning Representations, ICLR 2017; Conference date: 24 April 2017 through 26 April 2017; Conference code: 149805}
}

@ARTICLE{Nazarenko2016174,
	author = {Nazarenko, D.V. and Kharyuk, P.V. and Oseledets, I.V. and Rodin, I.A. and Shpigun, O.A.},
	title = {Machine learning for LC-MS medicinal plants identification},
	year = {2016},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	volume = {156},
	pages = {174 – 180},
	doi = {10.1016/j.chemolab.2016.06.003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974733845&doi=10.1016%2fj.chemolab.2016.06.003&partnerID=40&md5=453ed5bae53e5c2a50f8bac7fe8b305e},
	affiliations = {Lomonosov Moscow State University, Faculty of Chemistry, Department of Analytical Chemistry, Moscow, Russian Federation; Lomonosov Moscow State University, Faculty of Computational Mathematics and Cybernetics, Department of Computational Technologies and Modeling, Moscow, Russian Federation; Skolkovo Institute of Science and Technology, Moscow Region, Russian Federation; Institute of Numerical Mathematics of Russian Academy of Sciences, Moscow, Russian Federation},
	abstract = {Herbal medicines are vigorously marketed, but poorly regulated. Analysis methodology for this field is still forming. One particular analytical task is confirmation of plant species identity for medicinal plants used as ingredients. In this work, machine learning approach has been implemented for LC-MS plant species identification. Samples for 36 plant species have been analyzed. Peak data (m/z, abundance) from respective samples have been used for development of classification algorithms. Namely, logistic regression (LR), support vector machine (SVM) and random forest (RF) techniques were used. For most of used machine learning algorithms, classification accuracy of 95% higher were obtained on cross-validation dataset. Now, massive training datasets are needed for full-scale application of this approach. © 2016 Elsevier B.V.},
	author_keywords = {Liquid chromatography-mass spectrometry; Machine learning; Multiclass classification; Plant species identification},
	keywords = {Araliaceae; Article; classification algorithm; controlled study; liquid chromatography; logistic regression analysis; machine learning; mass spectrometry; mathematical computing; measurement accuracy; medicinal plant; nonhuman; plant identification; priority journal; random forest; support vector machine},
	correspondence_address = {D.V. Nazarenko; Lomonosov Moscow State University, Faculty of Chemistry, Department of Analytical Chemistry, Moscow, Russian Federation; email: dmitro.nazarenko@gmail.com},
	publisher = {Elsevier B.V.},
	issn = {01697439},
	coden = {CILSE},
	language = {English},
	abbrev_source_title = {Chemometr. Intelligent Lab. Syst.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18}
}

@CONFERENCE{Sabu2017145,
	author = {Sabu, Amala and Sreekumar, K.},
	title = {Literature review of image features and classifiers used in leaf based plant recognition through image analysis approach},
	year = {2017},
	journal = {Proceedings of the International Conference on Inventive Communication and Computational Technologies, ICICCT 2017},
	pages = {145 – 149},
	doi = {10.1109/ICICCT.2017.7975176},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027446955&doi=10.1109%2fICICCT.2017.7975176&partnerID=40&md5=45c7b7058cc83109766b1e20c40258cc},
	affiliations = {Department Of Computer Science, College of Engineering Poonjar, India},
	abstract = {Plants play an important role in Earth's ecology by providing sustenance, shelter and maintaining a healthy atmosphere. Some of these plants have important medicinal properties. Automatic recognition of plant leaf is a challenging problem in the area of computer vision.An efficient Ayurvedic plant leaf recognition system will beneficial to many sectors of society which include medicinal field,botanic research etc. With the help of image processing and pattern recognition ,we can easily recognize the leaf images.This paper gives a survey on different leaf recognition methods and classifications. Plant leaf classification is a technique where leaf is classified based on its different features. © 2017 IEEE.},
	author_keywords = {ANN(Artificial Neural Network); KNN(knearest neighbors); PNN(Probablistic Neural Network); SVM(Support Vector Machine)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150905297-4},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Inven. Commun. Comput. Technol., ICICCT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 23; Conference name: 2017 International Conference on Inventive Communication and Computational Technologies, ICICCT 2017; Conference date: 10 March 2017 through 11 March 2017; Conference code: 129164}
}

@CONFERENCE{Eid201676,
	author = {Eid, Heba F. and Hassanien, Aboul Ella and Kim, Tai-Hoon},
	title = {Leaf Plant Identification System Based on Hidden Naïve Bays Classifier},
	year = {2016},
	journal = {Proceedings - 2015 4th International Conference on Advanced Information Technology and Sensor Application, AITS 2015},
	pages = {76 – 79},
	doi = {10.1109/AITS.2015.28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964615066&doi=10.1109%2fAITS.2015.28&partnerID=40&md5=9fba6e91cef2453f1101e2dcbbee1890},
	affiliations = {Faculty of Science, Al-Azhar University, Cairo, Egypt; Faculty of Computers and Information, Cairo University, Egypt; Faculty of Computers and Information, BeniSuef University, Egypt; Department of Convergence Security, Sungshin Women's University, South Korea; Scientific Research Group in Egypt (SRGE), Egypt},
	abstract = {Plant identification is vital for the management of plant species. An automated plant identification system is required for the characterization of plant species without requiring the expertise of botanists. This paper presents an efficient and computational model for plant species identification using digital images of leaves. The proposed identification system combines the leaf biometric features, where shape and venation features are used for leaf image classification. 10 combined biometric leaf features are extracted and passed to Hidden naaive bays classifiers to be categorized. Several experiments are conducted and demonstrated on 1907 sample leaves of 32 different plant species taken form Flavia dataset. Where, the proposed plant identification model shows consistently performances of 97% average identification accuracy. © 2015 IEEE.},
	author_keywords = {plant identification; Plant biometrics; Classification; Hidden näive bays.},
	keywords = {Biometrics; Classification (of information); Image processing; Plants (botany); Biometric features; Computational model; Digital image; Identification accuracy; Plant identification; Plant identification systems; Plant species; Plant species identification; Image classification},
	editor = {Kang B. and Kim H.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-146737572-6},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Adv. Inf. Technol. Sens. Appl.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 4th International Conference on Advanced Information Technology and Sensor Application, AITS 2015; Conference date: 21 August 2015 through 23 August 2015; Conference code: 119383}
}

@ARTICLE{Kazerouni201781,
	author = {Kazerouni, Masoud Fathi and Schlemper, Jens and Kuhnert, Klaus-Dieter},
	title = {Automatic plant recognition system for challenging natural plant species},
	year = {2017},
	journal = {Computer Science Research Notes},
	volume = {2702},
	number = {May},
	pages = {81 – 90},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072783362&partnerID=40&md5=868d261164d37c5c602e56742db15161},
	affiliations = {Institute of Real-Time Learning Systems, Hölderlinstr. 3, Siegen, D-57076, Germany},
	abstract = {Photosynthesis is one of turning points to shape the world. Plants use this process to convert light energy into chemical energy. Some of the early microorganisms evolved a way to use the energy from sunlight to make sugar out of simpler molecules, but unlike green plants today, the first photosynthesizing organisms did not release oxygen as waste product, so there was no oxygen in the air. Plants are very busy factories and leaves are the main place for production. A useful plant recognition system is capable of identification of different species in natural environment. In natural environment, plants and leaves grow in different regions and climates. During day, variation of light intensity can be considered as an important factor. Thus, recognition of species in different conditions is a real need as plants are ubiquitous in human life. A dataset of natural images has been utilized. The dataset contains four different plant species of Siegerland, Germany. Modern combined description algorithms, SURF, FAST-SURF, and HARRIS-SURF, have been carried out to implement a reliable system for plants species recognition and classification in natural environment. One of well known methods in machine learning community, Support Vector Machine, has been applied in the implemented systems. All steps of system's implementation are described in related sections. The highest obtained accuracy belongs to the implemented system by means of SURF algorithm and equals to 93.9575. © 2017 Computer Science Research Notes.},
	author_keywords = {Combination; FAST; FAST-SURF; Feature detection; Feature extraction; HARRIS; HARRIS-SURF; Light intensity; Natural images; Natural plant species recognition; SURF; Weather condition},
	keywords = {Computer graphics; Computer vision; Feature extraction; Light; Oxygen; Support vector machines; Visualization; Combination; FAST; FAST-SURF; Feature detection; HARRIS; HARRIS-SURF; Light intensity; Natural images; Natural plants; SURF; Plants (botany)},
	editor = {Skala V.},
	publisher = {University of West Bohemia},
	issn = {24644617},
	isbn = {978-808694350-3},
	language = {English},
	abbrev_source_title = {Comp. Sci. Res. Notes},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 25th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, WSCG 2017; Conference date: 29 May 2017 through 2 June 2017; Conference code: 151404}
}

@CONFERENCE{Näsi20161143,
	author = {Näsi, R. and Honkavaara, E. and Tuominen, S. and Saari, H. and Pölönen, I. and Hakala, T. and Viljanen, N. and Soukkamäki, J. and Näkki, I. and Ojanen, H. and Reinikainen, J.},
	title = {Uas based tree species identification using the novel fpi based hyperspectral cameras in visible, nir and swir spectral ranges},
	year = {2016},
	journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
	volume = {2016-January},
	pages = {1143 – 1148},
	doi = {10.5194/isprsarchives-XLI-B1-1143-2016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987923404&doi=10.5194%2fisprsarchives-XLI-B1-1143-2016&partnerID=40&md5=93ea8085373b42c7341ae78fb13f781a},
	affiliations = {Department of Remote Sensing and Photogrammetry, Finnish Geospatial Research Institute, Geodeetinrinne 2, Masala, FI-02430, Finland; Natural Resources Institute Finland (Luke), PO Box 18, Vantaa, FI-01301, Finland; VTT Microelectronics, VTT, P.O.Box 1000, FI-02044, Finland; Department of Mathematical Information Tech., University of Jyväskylä, P.O.Box 35, Jyväskylä, FI-40014, Finland; Rikola Ltd, Kaitoväylä 1F2, Oulu, FI-90590, Finland; Arboretum Mustila Foundation, Mustilantie 57, Elimäki, FI-47200, Finland},
	abstract = {Unmanned airborne systems (UAS) based remote sensing offers flexible tool for environmental monitoring. Novel lightweight Fabry-Perot interferometer (FPI) based, frame format, hyperspectral imaging in the spectral range from 400 to 1600 nm was used for identifying different species of trees in a forest area. To the best of the authors' knowledge, this was the first research where stereoscopic, hyperspectral VIS, NIR, SWIR data is collected for tree species identification using UAS. The first results of the analysis based on fusion of two FPI-based hyperspectral imagers and RGB camera showed that the novel FPI hyperspectral technology provided accurate geometric, radiometric and spectral information in a forested scene and is operational for environmental remote sensing applications.},
	author_keywords = {Hyperspectral; Photogrammetry; SWIR; Tree species; UAS},
	keywords = {Cameras; Environmental technology; Fabry-Perot interferometers; Forestry; Photogrammetry; Spectroscopy; Stereo image processing; Unmanned aerial vehicles (UAV); Environmental Monitoring; Environmental remote sensing; Hyper-spectral cameras; HyperSpectral; SWIR; Tree species; Tree species identifications; Unmanned airborne systems (UAS); Remote sensing},
	publisher = {International Society for Photogrammetry and Remote Sensing},
	issn = {16821750},
	language = {English},
	abbrev_source_title = {Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; Conference name: 23rd International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences Congress, ISPRS 2016; Conference date: 12 July 2016 through 19 July 2016; Conference code: null; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Pawara2017615,
	author = {Pawara, Pornntiwa and Okafor, Emmanuel and Schomaker, Lambert and Wiering, Marco},
	title = {Data augmentation for plant classification},
	year = {2017},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10617 LNCS},
	pages = {615 – 626},
	doi = {10.1007/978-3-319-70353-4_52},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036654317&doi=10.1007%2f978-3-319-70353-4_52&partnerID=40&md5=1a794542824c006b6e4ad71471bd280a},
	affiliations = {Institute of Artificial Intelligence and Cognitive Engineering (ALICE), University of Groningen, Nijenborgh 9, Groningen, Netherlands},
	abstract = {Data augmentation plays a crucial role in increasing the number of training images, which often aids to improve classification performances of deep learning techniques for computer vision problems. In this paper, we employ the deep learning framework and determine the effects of several data-augmentation (DA) techniques for plant classification problems. For this, we use two convolutional neural network (CNN) architectures, AlexNet and GoogleNet trained from scratch or using pre-trained weights. These CNN models are then trained and tested on both original and data-augmented image datasets for three plant classification problems: Folio, AgrilPlant, and the Swedish leaf dataset. We evaluate the utility of six individual DA techniques (rotation, blur, contrast, scaling, illumination, and projective transformation) and several combinations of these techniques, resulting in a total of 12 data-augmentation methods. The results show that the CNN methods with particular data-augmented datasets yield the highest accuracies, which also surpass previous results on the three datasets. Furthermore, the CNN models trained from scratch profit a lot from data augmentation, whereas the fine-tuned CNN models do not really profit from data augmentation. Finally, we observed that data-augmentation using combinations of rotation and different illuminations or different contrasts helped most for getting high performances with the scratch CNN models. © Springer International Publishing AG 2017.},
	author_keywords = {Data augmentation; Deep convolutional neural networks; Plant classification},
	keywords = {Classification (of information); Computer vision; Convolution; Deep learning; Deep neural networks; Image enhancement; Neural networks; Profitability; Vision aids; Classification performance; Computer vision problems; Convolutional neural network; Data augmentation; Learning frameworks; Learning techniques; Plant classification; Projective transformation; Metadata},
	correspondence_address = {M. Wiering; Institute of Artificial Intelligence and Cognitive Engineering (ALICE), University of Groningen, Groningen, Nijenborgh 9, Netherlands; email: m.a.wiering@rug.nl},
	editor = {Blanc-Talon J. and Popescu D. and Scheunders P. and Philips W. and Penne R.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331970352-7},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 72; Conference name: 18th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2017; Conference date: 18 September 2017 through 21 September 2017; Conference code: 206999; All Open Access, Green Open Access}
}

@CONFERENCE{Imamoglu2017,
	author = {Imamoglu, Nevrez and Kimura, Motoki and Miyamoto, Hiroki and Fujita, Aito and Nakamura, Ryosuke},
	title = {Solar power plant detection on multi-spectral satellite imagery using weakly-supervised CNN with feedback features and m-PCNN fusion},
	year = {2017},
	journal = {British Machine Vision Conference 2017, BMVC 2017},
	doi = {10.5244/c.31.183},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088066371&doi=10.5244%2fc.31.183&partnerID=40&md5=c7ff28ee137a0ce01585b4bf3226787c},
	affiliations = {Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Tokyo, Japan},
	abstract = {Most of the traditional convolutional neural networks (CNNs) implement bottom-up approach (feed-forward) for image classifications. However, many scientific studies demonstrate that visual perception in primates rely on both bottom-up and top-down connections. Therefore, in this work, we propose a CNN network with feedback structure for solar power plant detection on middle-resolution satellite images. To express the strength of the top-down connections, we introduce feedback CNN network (FB-Net) to a baseline CNN model used for solar power plant classification on multi-spectral satellite data. Moreover, we introduce a method to improve class activation mapping (CAM) to our FB-Net, which takes advantage of multi-channel pulse coupled neural network (m-PCNN) for weakly-supervised localization of the solar power plants from the features of proposed FB-Net. For the proposed FB-Net CAM with m-PCNN, experimental results demonstrated promising results on both solar-power plant image classification and detection task. © 2017. The copyright of this document resides with its authors.},
	keywords = {Cams; Computer vision; Feedforward neural networks; Image classification; Mammals; Satellite imagery; Solar energy; Solar power plants; Activation mapping; Bottom up approach; Bottom-up and top-down; Convolutional neural network; Plant classification; Pulse coupled neural network; Scientific studies; Supervised localization; Solar power satellites},
	publisher = {BMVA Press},
	isbn = {190172560X; 978-190172560-5},
	language = {English},
	abbrev_source_title = {Br. Mach. Vis. Conf., BMVC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 28th British Machine Vision Conference, BMVC 2017; Conference date: 4 September 2017 through 7 September 2017; Conference code: 151123; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Wang2017637,
	author = {Wang, Zhaobin and Li, Huale and Zhu, Ying and Xu, TianFang},
	title = {Review of Plant Identification Based on Image Processing},
	year = {2017},
	journal = {Archives of Computational Methods in Engineering},
	volume = {24},
	number = {3},
	pages = {637 – 654},
	doi = {10.1007/s11831-016-9181-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979231126&doi=10.1007%2fs11831-016-9181-4&partnerID=40&md5=4245861ef1e5ccb4b15a915bab276c99},
	affiliations = {School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Institute of Biology, Gansu Academy of Sciences, Lanzhou, 730000, China},
	abstract = {Plant recognition is closely related to people’s life. The operation of the traditional plant identification method is complicated, and is unfavorable for popularization. The rapid development of computer image processing and pattern recognition technology makes it possible for computer’s automatic recognition of plant species based on image processing. There are more and more researchers drawing their attention on the computer’s automatic identification technology based on plant images in recent years. Based on this, we have carried on a wide range of research and analysis on the plant identification method based on image processing in recent years. First of all, the research significance and history of plant recognition technologies are introduced in this paper; secondly, the main technologies and steps of plant recognition are reviewed; thirdly, more than 30 leaf features (including 16 shape features, 11 texture features, four color features), and then SVM was used to evaluate these features and their fusion features, and 8 commonly used classifiers are introduced in detail. Finally, the paper is ended with a conclusion of the insufficient of plant identification technologies and a prediction of future development. © 2016, CIMNE, Barcelona, Spain.},
	keywords = {Automation; Pattern recognition; Automatic identification; Automatic recognition; Computer image processing; Pattern recognition technologies; Plant identification; Plant recognition; Research and analysis; Research significances; Image processing},
	correspondence_address = {Z. Wang; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; email: zhaobin_wang@hotmail.com},
	publisher = {Springer Netherlands},
	issn = {11343060},
	language = {English},
	abbrev_source_title = {Arch. Comput. Methods Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 62}
}

@ARTICLE{Dyrmann201672,
	author = {Dyrmann, Mads and Karstoft, Henrik and Midtiby, Henrik Skov},
	title = {Plant species classification using deep convolutional neural network},
	year = {2016},
	journal = {Biosystems Engineering},
	volume = {151},
	pages = {72 – 80},
	doi = {10.1016/j.biosystemseng.2016.08.024},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989955034&doi=10.1016%2fj.biosystemseng.2016.08.024&partnerID=40&md5=d48e32deb74cee557fa5fe528eda2ae6},
	affiliations = {The Maersk Mc-Kinney Moller Institute, University of Southern Denmark, Denmark; Department of Engineering, Aarhus University, Denmark},
	abstract = {Information on which weed species are present within agricultural fields is important for site specific weed management. This paper presents a method that is capable of recognising plant species in colour images by using a convolutional neural network. The network is built from scratch trained and tested on a total of 10,413 images containing 22 weed and crop species at early growth stages. These images originate from six different data sets, which have variations with respect to lighting, resolution, and soil type. This includes images taken under controlled conditions with regard to camera stabilisation and illumination, and images shot with hand-held mobile phones in fields with changing lighting conditions and different soil types. For these 22 species, the network is able to achieve a classification accuracy of 86.2%. © 2016 IAgrE},
	author_keywords = {Convolutional Neural; Deep learning; Networks; Plant classification; Weed control},
	keywords = {Convolution; E-learning; Lighting; Networks (circuits); Neural networks; Agricultural fields; Classification accuracy; Controlled conditions; Convolutional Neural; Convolutional neural network; Deep learning; Lighting conditions; Plant classification; Weed control},
	correspondence_address = {M. Dyrmann; The Maersk Mc-Kinney Moller Institute, University of Southern Denmark, Denmark; email: mady@mmmi.sdu.dk},
	publisher = {Academic Press},
	issn = {15375110},
	coden = {BEINB},
	language = {English},
	abbrev_source_title = {Biosyst. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 405}
}

@ARTICLE{Plata20171333,
	author = {Plata, Diego Rueda and Ramos-Pollán, Raúl and González, Fabio A.},
	title = {Effective training of convolutional neural networks with small, specialized datasets},
	year = {2017},
	journal = {Journal of Intelligent and Fuzzy Systems},
	volume = {32},
	number = {2},
	pages = {1333 – 1342},
	doi = {10.3233/JIFS-169131},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011416874&doi=10.3233%2fJIFS-169131&partnerID=40&md5=ebd9b6ee24868c0fa6d0c0730e299db4},
	affiliations = {Universidad Industrial de Santander, Cra 27 Calle 9, Bucaramanga, Colombia; MindLab Research Group, Universidad Nacional de Colombia, Bogotá, Colombia},
	abstract = {This work proposes a supervised layer-wise strategy to train deep convolutional neural networks (DCNs) particularly suited for small, specialized image datasets. DCNs are increasingly being used with considerable success in image classification tasks and trained over large datasets (with more than 1M images and 10 K classes). Pre-trained successful DCNs can then be used for new smaller datasets (10 K to 100 K images) through a transfer learning process which cannot guarantee competitive a-priori performance if the new data is of different or specialized nature (medical imaging, plant recognition, etc.). We therefore seek out to find competitive techniques to train DCNs for such small datasets, and hereby describe a supervised greedy layer-wise method analogous to that used in unsupervised deep networks. Our method consistently outperforms the traditional methods that train a full DCN architecture in a single stage, yielding an average of over 20 increase in classification performance across all DCN architectures and datasets used in this work. Furthermore, we obtain more interpretable and cleaner visual features. Our method is better suited for small, specialized datasets since we require a training cycle for each DCN layer and this increases its computing time almost linearly with the number of layers. Nevertheless, it still remains as a fraction of the computing time required to generate pre-trained models with large generic datasets, and poses no additional requirements on hardware. This constitutes a solid alternative for training DCNs when transfer learning is not possible and, furthermore, suggests that state of the art DCN performance with large datasets might yet be improved at the expense of a higher computing time. © 2017 - IOS Press and the authors. All rights reserved.},
	author_keywords = {Convolutional networks; Deep learning; Greedy layer-wise training},
	keywords = {Convolution; Deep learning; Deep neural networks; Medical imaging; Network architecture; Neural networks; Classification performance; Convolutional networks; Convolutional neural network; Layer-wise; Number of layers; Plant recognition; State of the art; Transfer learning; Classification (of information)},
	correspondence_address = {R. Ramos-Pollán; Universidad Industrial de Santander, Bucaramanga, Cra 27 Calle 9, Colombia; email: rramosp@uis.edu.co},
	publisher = {IOS Press},
	issn = {10641246},
	language = {English},
	abbrev_source_title = {J. Intelligent Fuzzy Syst.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Lasseck2017,
	author = {Lasseck, Mario},
	title = {Image-based plant species identification with deep Convolutional Neural Networks},
	year = {2017},
	journal = {CEUR Workshop Proceedings},
	volume = {1866},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034747899&partnerID=40&md5=c857ed71a33cfc61af0f7a11a500cd89},
	affiliations = {Museum für Naturkunde, Berlin, Germany},
	abstract = {This paper presents deep learning techniques for image-based plant identification at very large scale. State-of-the-art Deep Convolutional Neural Networks (DCNNs) are fine-tuned to classify 10,000 species. To improve identification performance several models trained on different datasets with multiple image dimensions and aspect ratios are ensembled. Various data augmentation techniques have been applied to prevent overfitting and to further improve model accuracy and generalization. The proposed approach is evaluated in the LifeCLEF 2017 campaign. It provides the best system among all participating teams by achieving a mean reciprocal rank (MRR) of 92 % and a top-5 accuracy of 96 % on the official PlantCLEF test set.},
	author_keywords = {Biodiversity; Convolutional Neural Networks; Data augmentation; Deep learning; Fine-grained image classification; Plant species identification},
	keywords = {Aspect ratio; Biodiversity; Convolution; Deep learning; Deep neural networks; Image enhancement; Neural networks; Convolutional neural network; Data augmentation; Fine grained; Learning techniques; Mean reciprocal ranks; Participating teams; Plant identification; Plant species identification; Image processing},
	correspondence_address = {M. Lasseck; Museum für Naturkunde, Berlin, Germany; email: Mario.Lasseck@mfn-Berlin.de},
	editor = {Mandl T. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Goeuriot L. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 18th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2017; Conference date: 11 September 2017 through 14 September 2017; Conference code: 131731}
}

@ARTICLE{Moghaddam2016133,
	author = {Moghaddam, Parviz Ahmadi and Arasteh, Amir Sheykhi and Komarizadeh, Mohammad Hasan and Babazadeh, Saeedeh},
	title = {Developing a selective thinning algorithm in sugar beet fields using machine vision system},
	year = {2016},
	journal = {Computers and Electronics in Agriculture},
	volume = {122},
	pages = {133 – 138},
	doi = {10.1016/j.compag.2016.01.025},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957591776&doi=10.1016%2fj.compag.2016.01.025&partnerID=40&md5=f7d0567dbe97ab29e51b0b4026c5321f},
	affiliations = {Department of Biosystems Engineering, Faculty of Agriculture, Urmia University, Urmia, Iran},
	abstract = {Row crops thinning is an effective operation in the production of most agricultural crops. However, it is costly, exhausting and harmful for the health of workers. The main purpose of this research is developing a high accuracy algorithm to recognize sugar beet plants and eliminate excessive plants. Images were captured with a CCD digital camera when all plants had 4-6 leaves. In this study, two methods were used to recognize sugar beet plants, the mass center algorithm (MC) and the average width algorithm (AW). Images revealed that overlapping between plants is the main problem in plants recognition. The average width method is appeared to be more accurate than the other method, especially in high overlapping conditions. Moreover, the mean of accuracies in removal of plants, which should be removed, are significantly different (α = 0.05) by T-test. Device testing in vitro conditions indicated that accuracy of average width algorithm in detection of excessive plants is reached to 88%. The results showed that three sequential images should be checked simultaneously in order to reduce errors in recognition of excessive plants. © 2016 Elsevier B.V.},
	author_keywords = {Image processing; Machine vision; Overlapping; Sugar beet; T-test; Thinning},
	keywords = {Beta vulgaris subsp. vulgaris; Agricultural machinery; Algorithms; Computer vision; Crops; Image processing; Plants (botany); Agricultural crops; Machine vision systems; Overlapping; Selective thinning; Sequential images; Sugar beet fields; T-tests; Thinning; agricultural land; algorithm; health worker; image processing; machinery; research work; statistical analysis; sugar beet; thinning; Sugar beets},
	correspondence_address = {P.A. Moghaddam; Department of Biosystems Engineering, Faculty of Agriculture, Urmia University, Urmia, Iran; email: p.ahmadi@urmia.ac.ir},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Chakkaravarthy2016581,
	author = {Chakkaravarthy, S. Sibi and Sajeevan, G. and Kamalanaban, E. and Kumar, K. A. Varun},
	title = {Automatic leaf vein feature extraction for first degree veins},
	year = {2016},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {425},
	pages = {581 – 592},
	doi = {10.1007/978-3-319-28658-7_49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954494486&doi=10.1007%2f978-3-319-28658-7_49&partnerID=40&md5=79020715f74755afc5fb140a6a262922},
	affiliations = {Department of Computer Science and Engineering, VelTech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology, Chennai, India; Department of Computer Science and Engineering, SS & DM Group, Centre for Development of Advanced Computing (C-DAC), Pune, India},
	abstract = {Leaf vein is one of the most important and complex feature of the leaf used in automatic plant identification system for automatic classification and identification of plant species. Leaves of different species have different characteristic features which help in classification of specific plant species. These features help the botanists in identifying the key species of the plants from its leaf images more accurately. Vein feature is one of the most important complex features of leaf in plant species. In this paper we proposed a new feature extraction model, to extract the vein features from the leaf images. The proposed system using Hough lines stems the extraction of vein feature from the leaf images by plotting the lines over the first degree veins. Angle of lines from the primary vein to the secondary vein is considered as the input parameter for processing the extracted vein features. The centroid vein angle is considered to be the primary feature. The vein feature was given as the input to the neural network for efficient classification and the results were tested with 15 species of plants taken from “leafilia” data sets. © Springer International Publishing Switzerland 2016.},
	author_keywords = {Leaf classification; Leaf identification system; Leaf recognition system; Leaf vein; Leafilia; Venation},
	keywords = {Classification (of information); Complex networks; Extraction; Feature extraction; Plants (botany); Signal processing; Leaf classification; Leaf identification; Leaf recognition; Leaf vein; Leafilia; Venation; Image processing},
	correspondence_address = {S.S. Chakkaravarthy; Department of Computer Science and Engineering, VelTech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology, Chennai, India; email: sb.sibi@gmail.com},
	editor = {Li K.-C. and Mosin S. and Krishnan S. and Ma M. and Bandyopadhyay S. and Thampi S.M.},
	publisher = {Springer Verlag},
	issn = {21945357},
	isbn = {978-331928656-3},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 2nd International Symposium on Signal Processing and Intelligent Recognition Systems, SIRS 2015; Conference date: 16 December 2015 through 19 December 2015; Conference code: 159669}
}

@ARTICLE{Ustyuzhanin2017310,
	author = {Ustyuzhanin, Anton and Dammer, Karl-Heinz and Giebel, Antje and Weltzien, Cornelia and Schirrmann, Michael},
	title = {Discrimination of common ragweed (Ambrosia artemisiifolia) and Mugwort (Artemisia vulgaris) based on bag of visual words model},
	year = {2017},
	journal = {Weed Technology},
	volume = {31},
	number = {2},
	pages = {310 – 319},
	doi = {10.1614/WT-D-16-00068.1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019249183&doi=10.1614%2fWT-D-16-00068.1&partnerID=40&md5=0f43e54b5ae58604776662ce4fa42478},
	affiliations = {Department of Engineering for Crop Production, Leibniz Institute for Agricultural Engineering and Bioeconomy, Max-Eyth-Allee 100, Potsdam, 14469, Germany},
	abstract = {Common ragweed is a plant species causing allergic and asthmatic symptoms in humans. To control its propagation, an early identification system is needed. However, due to its similar appearance with mugwort, proper differentiation between these two weed species is important. Therefore, we propose a method to discriminate common ragweed and mugwort leaves based on digital images using bag of visual words (BoVW). BoVW is an object-based image classification that has gained acceptance in many areas of science. We compared speeded-up robust features (SURF) and grid sampling for keypoint selection. The image vocabulary was built using K-means clustering. The image classifier was trained using support vector machines. To check the robustness of the classifier, specific model runs were conducted with and without damaged leaves in the trainings dataset. The results showed that the BoVW model allows the discrimination between common ragweed and mugwort leaves with high accuracy. Based on SURF keypoints with 50% of 788 images in total as training data, we achieved a 100% correct recognition of the two plant species. The grid sampling resulted in slightly less recognition accuracy (98 to 99%). In addition, the classification based on SURF was up to 31 times faster. Nomenclature: Common ragweed, Ambrosia artemisiifolia L.; mugwort, Artemisia vulgaris L. © Weed Science Society of America, 2017.},
	author_keywords = {digital images; machine learning; plant recognition; speeded-up robust features; Weed species},
	keywords = {Ambrosia artemisiifolia; Artemisia vulgaris; accuracy assessment; data set; digital image; herb; image classification; machine learning; numerical model; recognition; sampling; support vector machine; weed},
	correspondence_address = {A. Ustyuzhanin; Department of Engineering for Crop Production, Leibniz Institute for Agricultural Engineering and Bioeconomy, Potsdam, Max-Eyth-Allee 100, 14469, Germany; email: austyuzhanin@atb-potsdam.de},
	publisher = {Cambridge University Press},
	issn = {0890037X},
	coden = {WETEE},
	language = {English},
	abbrev_source_title = {Weed Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Isnanto2017219,
	author = {Isnanto, R. Rizal and Hidayatno, Achmad and Zahra, Ajub Ajulian and Eskanesiari and Bagaskara, Aditya Indra and Septiana, Risma},
	title = {Herb leaves recognition using combinations of Hu's moment variants-Backpropagation neural network and 2-D Gabor filter-Learning vector quantization (LVQ)},
	year = {2017},
	journal = {Proceedings - 2017 4th International Conference on Information Technology, Computer, and Electrical Engineering, ICITACEE 2017},
	volume = {2018-January},
	pages = {219 – 224},
	doi = {10.1109/ICITACEE.2017.8257706},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050401231&doi=10.1109%2fICITACEE.2017.8257706&partnerID=40&md5=91cdc07faf3c3d882b6cab9150c49d20},
	affiliations = {Department of Computer Engineering, Indonesia; Department of Electrical Engineering, Faculty of Engineering, Diponegoro University, Semarang, Indonesia},
	abstract = {Indonesian people use herbs as an alternative choice to heal many kinds of diseases. The lack of information and knowledge about the merit of herbs will make the recognition turns difficult. The herbs can be recognized by the shape of the leaf. Capturing the leaf as an image allows constructing an automatic system of herbs recognition using image processing. Two steps of image processing for recognition are feature extraction and classification. 2D Gabor filter and Hu's moment invariants are used to feature extraction process. 2D Gabor Filters are influenced by some vectors that have different values for each feature of leaves. The hu's moment invariants are influenced by the geometric operation. The next step is classification grouping the result of the feature extraction into the right cluster. In this research, two classification methods will be combined with the two feature extraction methods. The first is hu's moment invariants and backpropagation neural network and the second is 2D Gabor filter and Learning Vector Quantization. Each of combination gives the different results. The first combination has recognition rate is about 81.4% and the second combination is about 80%. © 2017 IEEE.},
	author_keywords = {classification; feature extraction; herbs; image processing; leaf recognition},
	keywords = {Backpropagation; Classification (of information); Extraction; Feature extraction; Image processing; Neural networks; Vector quantization; Back propagation neural networks; Classification methods; Feature extraction and classification; Feature extraction methods; Geometric operations; herbs; Leaf recognition; Learning Vector Quantization; Gabor filters},
	editor = {Facta M. and Riyadi M.A. and Prasetijo A.B. and Widianto E.D. and Eridani D.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153863946-7},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Inf. Technol., Comput., Electr. Eng., ICITACEE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 4th International Conference on Information Technology, Computer, and Electrical Engineering, ICITACEE 2017; Conference date: 18 October 2017 through 19 October 2017; Conference code: 134244}
}

@CONFERENCE{Šulc2017,
	author = {Šulc, Milan and Matas, Jiří},
	title = {Learning with noisy and trusted labels for fine-grained plant recognition},
	year = {2017},
	journal = {CEUR Workshop Proceedings},
	volume = {1866},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034733773&partnerID=40&md5=00846e3f4dbb94fac82ea42ea9a46267},
	affiliations = {Center for Machine Perception, Dept. of Cybernetics, Faculty of Electrical Eng., Czech Technical University, Prague, Czech Republic},
	abstract = {The paper describes the deep learning approach to automatic visual recognition of 10 000 plant species submitted to the PlantCLEF 2017 challenge. We evaluate modifications and extensions of the state-ofthe-art Inception-ResNet-v2 CNN architecture, including maxout, bootstrapping for training with noisy labels, and filtering the data with noisy labels using a classifier pre-trained on the trusted dataset. The final pipeline consists of a set of CNNs trained with different modifications on different subsets of the provided training data. With the proposed approach, we were ranked as the third best team in the LifeCLEF 2017 challenge.},
	keywords = {Fine grained; Learning approach; Noisy labels; Plant recognition; Plant species; Training data; Visual recognition; Classification (of information)},
	editor = {Mandl T. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Goeuriot L. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 18th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2017; Conference date: 11 September 2017 through 14 September 2017; Conference code: 131731}
}

@CONFERENCE{Hang2017,
	author = {Hang, Siang Thye and Aono, Masaki},
	title = {Open world plant image identification based on convolutional neural network},
	year = {2017},
	journal = {2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016},
	doi = {10.1109/APSIPA.2016.7820676},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013829228&doi=10.1109%2fAPSIPA.2016.7820676&partnerID=40&md5=d845c19372a384d9e4c02e829a00ea08},
	affiliations = {Toyohashi University of Technology, Japan},
	abstract = {In this paper, we propose several enhancements to the well-known VGG 16-layers Convolutional Neural Network (CNN) model towards open world image classification, by taking plant identification as an example. We first propose to replace the last pooling layer of the VGG 16-layers model with a Spatial Pyramid Pooling layer, enabling the model to accept arbitrary sized input images. Second, for the activation function, we replace Rectified Linear Unit (ReLU) with Parametric ReLU in order to increase the adaptability of parameter learning. In addition, we introduce the Unseen Category Query Identification algorithm to identify and omit images of unseen category, thus preventing false classification into predefined categories. Such algorithm is essential in real life, since there is no guarantee that a given image has to fall into a predefined category. We use the dataset from the LifeCLEF 2016 plant identification task. We compare our results with other participants and demonstrate that our enhanced model with proposed algorithm exhibits outstanding performance. © 2016 Asia Pacific Signal and Information Processing Association.},
	keywords = {Convolution; Neural networks; Activation functions; Convolutional neural network; Identification algorithms; Image identification; Linear units; Parameter learning; Plant identification; Spatial pyramids; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-988147682-1},
	language = {English},
	abbrev_source_title = {Asia-Pac. Signal Inf. Process. Assoc. Annu. Summit Conf., APSIPA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; Conference name: 2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016; Conference date: 13 December 2016 through 16 December 2016; Conference code: 126005}
}

@ARTICLE{Mata-Montero201626,
	author = {Mata-Montero, Erick and Carranza-Rojas, Jose},
	title = {Automated plant species identification: Challenges and opportunities},
	year = {2016},
	journal = {IFIP Advances in Information and Communication Technology},
	volume = {481},
	pages = {26 – 36},
	doi = {10.1007/978-3-319-44447-5_3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986325889&doi=10.1007%2f978-3-319-44447-5_3&partnerID=40&md5=ddad93a5f22215cdbf8543194ddb0787},
	affiliations = {Costa Rica Institute of Technology, Cartago, Costa Rica},
	abstract = {The number of species of macro organisms on the planet is estimated at about 10 million. This staggering diversity and the need to better understand it led inevitably to the development of classification schemes called biological taxonomies. Unfortunately, in addition to this enormous diversity, the traditional identification and classification workflows are both slow and error-prone; classification expertise is in the hands of a small number of expert taxonomists; and to make things worse, the number of taxonomists has steadily declined in recent years. Automated identification of organisms has therefore become not just a long time desire but a need to better understand, use, and save biodiversity. This paper presents a survey of recent efforts to use computer vision and machine learning techniques to identify organisms. It focuses on the use of leaf images to identify plant species. In addition, it presents the main technical and scientific challenges as well as the opportunities for herbaria and cybertaxonomists to take a quantum leap towards identifying biodiversity efficiently and empowering the general public by putting in their hands automated identification tools. © IFIP International Federation for Information Processing 2016.},
	author_keywords = {Biodiversity informatics; Citizen-Science; Computer vision; Cybertaxonomy; Image processing; Leaf recognition; Machine learning; Plant identification; Species identification},
	keywords = {Artificial intelligence; Automation; Biodiversity; Image processing; Learning systems; Citizen science; Cybertaxonomy; Informatics; Leaf recognition; Plant identification; Species identification; Computer vision},
	correspondence_address = {E. Mata-Montero; Costa Rica Institute of Technology, Cartago, Costa Rica; email: emata@itcr.ac.cr},
	editor = {Pont A. and Mata F.J.},
	publisher = {Springer New York LLC},
	issn = {18684238},
	isbn = {978-331944446-8},
	language = {English},
	abbrev_source_title = {IFIP Advances in Information and Communication Technology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 25; Conference name: 6th IFIP World Information Technology Forum, WITFOR 2016; Conference date: 12 September 2016 through 14 September 2016; Conference code: 180769; All Open Access, Green Open Access}
}

@CONFERENCE{Lee2016502,
	author = {Lee, Sue Han and Chang, Yang Loong and Chan, Chee Seng and Remagnino, Paolo},
	title = {Plant identification system based on a convolutional neural network for the lifeclef 2016 plant classification task},
	year = {2016},
	journal = {CEUR Workshop Proceedings},
	volume = {1609},
	pages = {502 – 510},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019636805&partnerID=40&md5=52a073bdf272431ebb0a334e838f7711},
	affiliations = {Centre of Image and Signal Processing, Fac. Comp. Sci. and Info. Tech, University of Malaya, Malaysia; Computer Science Dept, Kingston University, United Kingdom},
	abstract = {In this paper, we describe the architecture of our plant classification system for the LifeClef 2016 challenge [14]. The objective of the task is to identify 1000 species of images of plants corresponding to 7 different plant organs, as well as automatically detecting invasive species from unknown classes. To address the challenge [10], we proposed a plant classification system that uses a convolutional neural network (CNN).},
	author_keywords = {Convolutional Neural Network; Deep Learning; Plant Classification},
	keywords = {Deep learning; Neural networks; Convolutional neural network; Invasive species; Plant classification; Plant identification systems; Plant organs; Unknown class; Convolution},
	editor = {Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Macdonald C. and Balog K.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; Conference name: 2016 Working Notes of Conference and Labs of the Evaluation Forum, CLEF 2016; Conference date: 5 September 2016 through 8 September 2016; Conference code: 127674}
}

@CONFERENCE{Toma2017,
	author = {Toma, Alexandru and Ştefan, Liviu Daniel and Ionescu, Bogdan},
	title = {UPB HES SO @ PlantCLEF 2017: Automatic plant image identification using transfer learning via Convolutional neural networks},
	year = {2017},
	journal = {CEUR Workshop Proceedings},
	volume = {1866},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034758427&partnerID=40&md5=fcffa7d6fa8d3d5b60f0f0d11974210e},
	affiliations = {Multimedia Lab - CAMPUS, University Politehnica of Bucharest, Romania},
	abstract = {Recent advances in computer vision have made possible the use of neural networks in large scale image retrieval tasks. An example application is the automated plant classification. However, training a network from scratch takes a lot of computational effort and turns out to be very time consuming. In this paper, we investigate a transfer learning approach in the context of the 2017 PlantCLEF task, for automatic plant image classification. The proposed approach is based on the well-known AlexNet Convolutional Neural Network (CNN) model. The network was fine-tuned using the 2017 PlantCLEF Encyclopedia of Life (EOL) training data, which consists of approximately 260,000 plant images belonging to 10,000 species. The learning process was sped up in the upper layers leaving original features almost untouched. Our best proposed official run scored 0,361 in terms of the Mean Reciprocal Rank (MRR) when evaluated on the test dataset.},
	author_keywords = {Convolutional neural networks; Deep learning; LifeCLEF; Plant identification; Transfer learning},
	keywords = {Convolution; Deep learning; Neural networks; Statistical tests; Computational effort; Convolutional neural network; Encyclopedia of lives; Image identification; LifeCLEF; Mean reciprocal ranks; Plant identification; Transfer learning; Image retrieval},
	editor = {Mandl T. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Goeuriot L. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 18th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2017; Conference date: 11 September 2017 through 14 September 2017; Conference code: 131731}
}

@CONFERENCE{Herdiyeni201654,
	author = {Herdiyeni, Yeni and Ginanjar, Asep Rahmat and Anggoro, M. Rake Linggar and Douady, Stephane and Zuhud, Ervizal A.M},
	title = {MedLeaf: Mobile biodiversity informatics tool for mapping and identifying Indonesian medicinal Plants},
	year = {2016},
	journal = {Proceedings of the 2015 7th International Conference of Soft Computing and Pattern Recognition, SoCPaR 2015},
	pages = {54 – 59},
	doi = {10.1109/SOCPAR.2015.7492783},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979248347&doi=10.1109%2fSOCPAR.2015.7492783&partnerID=40&md5=6a85102bc00d12a3d69e419a40d52fec},
	affiliations = {Department of Computer Science, Faculty of Mathematics and Natural Sciences, Bogor Agricultural University, West Java, Indonesia; Laboratoire: Matiere et System Complex, University of Paris Diderot, Paris, France; Department of Forest Resources Conservation and Ecotourism, Faculty Forestry, Bogor Agriculture University, West Java, Indonesia},
	abstract = {We presents a mobile biodiversity informatics tools for identifying and mapping Indonesian medicinal plants. The system - called MedLeaf - has been developed as a prototype data resource for documenting, integrating, disseminating, and identifying of Indonesian medicinal plants. Identification of medicinal plant is done automatically based on digital image processing. Fuzzy Local Binary Pattern (LBP) and geometrical features are used to extract leaves features. Probabilistic Neural Network is used as classifier for discrimination. Data set consist of 85 species of Indonesian medicinal plants with 3,502 leaves digital images. Our results indicate that combination of leaves features outperform than using single features with accuracy 88.5%. The distribution of medicinal plants can be shown on mobile phone using GIS application. The application is essential to help people identify the medicinal plants and disseminate information of medicinal plants distribution in Indonesia. © 2015 IEEE.},
	author_keywords = {biodiversity informatics; FLBP; leaves shape; leaves texture; medicinal plants},
	keywords = {Biodiversity; Geographic information systems; Image processing; Information dissemination; Information science; Mapping; Neural networks; Pattern recognition; Plant extracts; Soft computing; FLBP; Geometrical features; GIS application; Informatics; leaves shape; Local binary patterns; Medicinal plants; Probabilistic neural networks; Plants (botany)},
	editor = {Koppen M. and Muda A.K. and Ma K. and Xue B. and Takagi H. and Abraham A.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-146739360-7},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Soft Comput. Pattern Recognit., SoCPaR},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 7th International Conference of Soft Computing and Pattern Recognition, SoCPaR 2015; Conference date: 13 November 2015 through 15 November 2015; Conference code: 122346}
}

@CONFERENCE{Lukic2017485,
	author = {Lukic, Marko and Tuba, Eva and Tuba, Milan},
	title = {Leaf recognition algorithm using support vector machine with Hu moments and local binary patterns},
	year = {2017},
	journal = {SAMI 2017 - IEEE 15th International Symposium on Applied Machine Intelligence and Informatics, Proceedings},
	pages = {485 – 490},
	doi = {10.1109/SAMI.2017.7880358},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017617427&doi=10.1109%2fSAMI.2017.7880358&partnerID=40&md5=4ba0c886d13edc6bdabecfa83dbc51fb},
	affiliations = {Graduate School of Computer Science, John Naisbitt University, Bulevar umetnosti 29, Belgrade, Serbia; Faculty of Mathematics, University of Belgrade, Studentski trg 16, Belgrade, Serbia},
	abstract = {Leaf recognition is convenient for plant classification and it is an important subfield of pattern recognition. Different leaf features such as color, shape and texture are used as well as different classifiers including artificial neural networks, k-nearest neighbor and support vector machines. In this paper we propose an algorithm based on tuned support vector machine as a classifier and Hu moments and uniform local binary pattern histogram parameters as features. Our proposed algorithm was tested on leaf images from standard benchmark database and compared with other approaches from literature where it proved to be more successful (higher recognition percentage). © 2017 IEEE.},
	author_keywords = {Hu moments; Leaf recognition; local binary patterns; plant classification; support vector machines},
	keywords = {Artificial intelligence; Content based retrieval; Deep neural networks; Image segmentation; Nearest neighbor search; Neural networks; Support vector machines; Vectors; Benchmark database; Hu moments; K-nearest neighbors; Leaf recognition; Local binary patterns; Plant classification; Shape and textures; Uniform local binary patterns; Pattern recognition},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-150905654-5},
	language = {English},
	abbrev_source_title = {SAMI - IEEE Int. Symp. Appl. Mach. Intell. Inform., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 40; Conference name: 15th IEEE International Symposium on Applied Machine Intelligence and Informatics, SAMI 2017; Conference date: 26 January 2017 through 28 January 2017; Conference code: 126905}
}

@ARTICLE{Jye201798,
	author = {Jye, Kho Soon and Manickam, Sugumaran and Malek, Sorayya and Mosleh, Mogeeb and Dhillon, Sarinder Kaur},
	title = {Automated plant identification using artificial neural network and support vector machine},
	year = {2017},
	journal = {Frontiers in Life Science},
	volume = {10},
	number = {1},
	pages = {98 – 107},
	doi = {10.1080/21553769.2017.1412361},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054785366&doi=10.1080%2f21553769.2017.1412361&partnerID=40&md5=b4ef9a81367440099d8033d7bd7266ae},
	affiliations = {Faculty of Science, Data Science & Bioinformatics Laboratory, Institute of Biological Sciences, University of Malaya, Kuala Lumpur, Malaysia; Faculty of Science, Rimba Ilmu Botanic Garden, Institute of Biological Sciences, University of Malaya, Kuala Lumpur, Malaysia; Faculty of Engineering & Information Technology, Software Engineering Department, Taiz University, Taiz, Yemen},
	abstract = {Ficus is one of the largest genera in plant kingdom reaching to about 1000 species worldwide. While taxonomic keys are available for identifying most species of Ficus, it is very difficult and time consuming for interpretation by a nonprofessional thus requires highly trained taxonomists. The purpose of the current study is to develop an efficient baseline automated system, using image processing with pattern recognition approach, to identify three species of Ficus, which have similar leaf morphology. Leaf images from three different Ficus species namely F. benjamina, F. pellucidopunctata and F. sumatrana were selected. A total of 54 leaf image samples were used in this study. Three main steps that are image pre-processing, feature extraction and recognition were carried out to develop the proposed system. Artificial neural network (ANN) and support vector machine (SVM) were the implemented recognition models. Evaluation results showed the ability of the proposed system to recognize leaf images with an accuracy of 83.3%. However, the ANN model performed slightly better using the AUC evaluation criteria. The system developed in the current study is able to classify the selected Ficus species with acceptable accuracy. © 2018 The Author(s).},
	author_keywords = {Artificial neural network; Automated species identification; Ficus; Life data technology; Support vector machine},
	correspondence_address = {S.K. Dhillon; Faculty of Science, Data Science & Bioinformatics Laboratory, Institute of Biological Sciences, University of Malaya, Kuala Lumpur, 50603, Malaysia; email: sarinder@um.edu.my},
	publisher = {Taylor and Francis Ltd.},
	issn = {21553769},
	language = {English},
	abbrev_source_title = {Frontiers Life Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 41; All Open Access, Bronze Open Access}
}

@ARTICLE{Bonnet20161647,
	author = {Bonnet, Pierre and Joly, Alexis and Goëau, Hervé and Champ, Julien and Vignau, Christel and Molino, Jean-François and Barthélémy, Daniel and Boujemaa, Nozha},
	title = {Plant identification: man vs. machine: LifeCLEF 2014 plant identification challenge},
	year = {2016},
	journal = {Multimedia Tools and Applications},
	volume = {75},
	number = {3},
	pages = {1647 – 1665},
	doi = {10.1007/s11042-015-2607-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960328399&doi=10.1007%2fs11042-015-2607-4&partnerID=40&md5=419d984969f5449fc1dd93b63a20cde7},
	affiliations = {CIRAD, UMR AMAP, Montpellier, France; LIRMM, Montpellier, France; Inria ZENITH team, Montpellier, France; INRA UMR AGAP, Montpellier, France; Tela Botanica, Montpellier, France; IRD, Montpellier, France; CIRAD, BIOS Direction and INRA, UMR AMAP, Montpellier, F-34398, France; Direction of Saclay Center, INRIA, Paris, France},
	abstract = {This paper reports a large-scale experiment aimed at evaluating how state-of-art computer vision systems perform in identifying plants compared to human expertise. A subset of the evaluation dataset used within LifeCLEF 2014 plant identification challenge was therefore shared with volunteers of diverse expertise, ranging from the leading experts of the targeted flora to inexperienced test subjects. In total, 16 human runs were collected and evaluated comparatively to the 27 machine-based runs of LifeCLEF challenge. One of the main outcomes of the experiment is that machines are still far from outperforming the best expert botanists at the image-based plant identification competition. On the other side, the best machine runs are competing with experienced botanists and clearly outperform beginners and inexperienced test subjects. This shows that the performances of automated plant identification systems are very promising and may open the door to a new generation of ecological surveillance systems. © 2015, Springer Science+Business Media New York.},
	author_keywords = {Digital data; Human evaluation; Image analysis; Visual plant identification},
	keywords = {Computer vision; Image analysis; Statistical tests; Computer vision system; Digital datas; Human evaluation; Human expertise; Large scale experiments; Plant identification; Plant identification systems; Surveillance systems; Image processing},
	correspondence_address = {H. Goëau; Inria ZENITH team, Montpellier, France; email: herve.goeau@inria.fr},
	publisher = {Springer New York LLC},
	issn = {13807501},
	coden = {MTAPF},
	language = {English},
	abbrev_source_title = {Multimedia Tools Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; All Open Access, Green Open Access}
}

@ARTICLE{Kan2017581,
	author = {Kan, H.X. and Jin, L. and Zhou, F.L.},
	title = {Classification of medicinal plant leaf image based on multi-feature extraction},
	year = {2017},
	journal = {Pattern Recognition and Image Analysis},
	volume = {27},
	number = {3},
	pages = {581 – 587},
	doi = {10.1134/S105466181703018X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029476740&doi=10.1134%2fS105466181703018X&partnerID=40&md5=dc420e0bc51c9f4e32320dfb5541f49f},
	affiliations = {College of Medical Information Engineering, Anhui University of Chinese Medicine, Hefei, Anhui, 230012, China},
	abstract = {Medicinal plants are the main source of traditional Chinese medicine (TCM), which provides the basic protection of human health. The research and application of medicinal plant classification methodology has important implications in the TCM resource preservation, TCM authentication, and the teaching method of TCM identification. This paper proposes an automatic classification method based on leaf images of medicinal plants to address the limitation of manual classification method in identifying medicinal plants. Our approach will first preprocess the leaf images of medicinal plants; then it will compute the ten shape feature (SF) and five texture characteristics (TF); finally, it will classify the leaves of medicinal plants using support vector machine (SVM) classifier. The classifier has been applied to 12 different medicinal plant leaf images and achieved an average successful recognition rate of 93.3%. The result indicates that it is feasible to automatically classify medicinal plants by using multi-feature extraction of leaf images in combination with SVM. The paper provides a valuable theoretical framework in the research and development of medicinal plant classification system. © 2017, Pleiades Publishing, Ltd.},
	author_keywords = {gray level co-occurrence matrix; hu moment invariants; leaf image; SVM classifier},
	correspondence_address = {H.X. Kan; College of Medical Information Engineering, Anhui University of Chinese Medicine, Hefei, Anhui, 230012, China; email: ffdkhx@ahtcm.edu.cn},
	publisher = {Maik Nauka-Interperiodica Publishing},
	issn = {10546618},
	language = {English},
	abbrev_source_title = {Pattern Recogn. Image Anal.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 50}
}

@ARTICLE{Wirdiani20161847,
	author = {Wirdiani, Ni Kadek Ayu and Oka Sudana, A.A.K.},
	title = {Medicinal plant recognition of leaf shape using Localized Arc Pattern Method},
	year = {2016},
	journal = {International Journal of Engineering and Technology},
	volume = {8},
	number = {4},
	pages = {1847 – 1854},
	doi = {10.21817/ijet/2016/v8i4/160804167},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988568913&doi=10.21817%2fijet%2f2016%2fv8i4%2f160804167&partnerID=40&md5=f1d338ae749680d5d37d5449854807b0},
	affiliations = {Information Technology Department, Faculty of Engineering, Udayana University Jimbaran, Bali, Indonesia},
	abstract = {Medicinal plants are plants that have benefit in order to supply the needs of families traditionally medicine. Medicinal plants have diverse types that causing modern society have difficulty in recognizing these crops. Medicinal plants generally can be identified by the leaves, stems and fruit. One of the leaves characteristics can be distinguished based on vein structure and shape of its. Based on these problem, plant recognition based on vein and shape are made by using Localized Arc Pattern Method. There are two important processes in Plant Recognition Applications. First process is Enrollment and the second is Recognition process. In the Enrolment process, the leaves image filed as many as 6 images for each leaves type. This image then calculated based on the 42 special model pattern obtained and the feature is stored as a reference image. Leaves images that used as test image are 200 images. On the Recognition process, the test image will be process which as same as at Enrollment process, however feature from the test image will be comparing with reference image in database, then it calculate the difference value. This process uses a threshold value to determine whether the test images leaves are recognized or not. When dissimilarity value is smaller than the threshold is known as the same leaves, when instead then it known as a different leaves or not known at all. Experiment result shows in this application can recognize 77% of total leaves and False Accepted Ratio (FAR) equal to 4.5% and False Rejection Ratio (FRR) equal to 18.5%. This result was influenced by the shiny surface of leaf and shape of the leaves are small.},
	author_keywords = {Dissimilarity value; FAR; Feature extraction; FRR; Leaf recognition; Localized Arc Pattern; Matching process; Medicinal plant recognition},
	publisher = {Engg Journals Publications},
	issn = {23198613},
	language = {English},
	abbrev_source_title = {Intern. J. Eng. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Bronze Open Access}
}

@CONFERENCE{Ballado20171,
	author = {Ballado, Alejandro H. and Garcia, Ramon G. and Chichoco, Joanne Gem Z. and Domingo, Bianca Marie B. and Santuyo, Kimberly Joy M. and Sulmaca, Van Jay S. and Bentir, Sarah Alma P. and Sarte, Shydel M.},
	title = {Forest mapping and classification of forest Type using LiDAR data and tree specie identification through image processing based on leaf extraction algorithms},
	year = {2017},
	journal = {HNICEM 2017 - 9th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment and Management},
	volume = {2018-January},
	pages = {1 – 6},
	doi = {10.1109/HNICEM.2017.8269434},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047781351&doi=10.1109%2fHNICEM.2017.8269434&partnerID=40&md5=bcebe5fc26e6bd3bf03d393e7dab0bcf},
	affiliations = {School of Electrical Electronics and Computer Engineering, Mapúa University, Intramuros, Manila, Philippines},
	abstract = {With the use of Light Detection and Ranging (LiDAR) Data, this study focuses on the processing of the LiDAR derived data through different software tools to generate a map that can classify forest types. A 20 × 20 meter plot in the selected forest area was identified in this study for the field validation of the classified leaf type. Leaf recognition is performed using Neural Network in Matlab. The leaf statistics were measured through the prototype developed using leaf extraction algorithms T-test is used for the comparative measurement between the perimeter of the extracted data and the actual perimeter of a sample leaf. The result shows that for the specie, the actual perimeter is statistically the same with the perimeter measured by the developed prototype. The accuracy of classification was calculated as 91.25%. The overall minimum and maximum precision of the prototype is computed to be 90.40% and 99.14%, respectively. © 2017 IEEE.},
	author_keywords = {circumference at breast height; diameter at breast; leaf extraction; LiDAR; tree height},
	keywords = {Classification (of information); Data mining; Environmental management; Extraction; Forestry; Image classification; MATLAB; Nanotechnology; Optical radar; Accuracy of classifications; Breast height; Comparative measurements; diameter at breast; Extraction algorithms; Field validation; Light detection and ranging; Tree height; Mapping},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153860910-1},
	language = {English},
	abbrev_source_title = {HNICEM - Int. Conf. Humanoid, Nanotechnol., Inf. Technol., Commun. Control, Environ. Manag.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 9th IEEE International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment and Management, HNICEM 2017; Conference date: 29 November 2017 through 1 December 2017; Conference code: 134372}
}

@ARTICLE{Chuanlei201774,
	author = {Chuanlei, Zhang and Shanwen, Zhang and Jucheng, Yang and Yancui, Shi and Jia, Chen},
	title = {Apple leaf disease identification using genetic algorithm and correlation based feature selection method},
	year = {2017},
	journal = {International Journal of Agricultural and Biological Engineering},
	volume = {10},
	number = {2},
	pages = {74 – 83},
	doi = {10.3965/j.ijabe.20171002.2166},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017027848&doi=10.3965%2fj.ijabe.20171002.2166&partnerID=40&md5=b5483f887e0072399a928fec393aa387},
	affiliations = {College of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, 300222, China; XiJing University, Xi’an, 710123, China},
	abstract = {Apple leaf disease is one of the main factors to constrain the apple production and quality. It takes long time to detect the diseases by using the traditional diagnostic approach, so farmers often miss the best time to prevent and treat the diseases. Apple leaf disease recognition based on leaf image is an essential research topic in the field of computer vision, where the key task is to find an effective way to represent the diseased leaf images. In this research, based on image processing techniques and pattern recognition methods, an apple leaf disease recognition method was proposed. A color transformation structure for the input RGB (Red, Green and Blue) image was designed firstly and then RGB model was converted to HSI (Hue, Saturation and Intensity), YUV and gray models. The background was removed based on a specific threshold value, and then the disease spot image was segmented with region growing algorithm (RGA). Thirty-eight classifying features of color, texture and shape were extracted from each spot image. To reduce the dimensionality of the feature space and improve the accuracy of the apple leaf disease identification, the most valuable features were selected by combining genetic algorithm (GA) and correlation based feature selection (CFS). Finally, the diseases were recognized by SVM classifier. In the proposed method, the selected feature subset was globally optimum. The experimental results of more than 90% correct identification rate on the apple diseased leaf image database which contains 90 disease images for there kinds of apple leaf diseases, powdery mildew, mosaic and rust, demonstrate that the proposed method is feasible and effective. © 2017, Chinese Society of Agricultural Engineering. All rights reserved.},
	author_keywords = {Apple leaf disease; Diseased leaf recognition; Genetic algorithm and correlation based feature selection (GA-CFS); Region growing algorithm (RGA)},
	correspondence_address = {Y. Jucheng; College of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, 1038 Dagu Nanlu, Hexi District, 300222, China; email: jcyang@tust.edu.cn},
	publisher = {Chinese Society of Agricultural Engineering},
	issn = {19346344},
	language = {English},
	abbrev_source_title = {Int. J. Agric. Biol. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 116}
}

@CONFERENCE{Bagal20164079,
	author = {Bagal, V.C. and Manza, R.R.},
	title = {Feature extraction of plant species from leaf architecture},
	year = {2016},
	journal = {International Conference on Electrical, Electronics, and Optimization Techniques, ICEEOT 2016},
	pages = {4079 – 4081},
	doi = {10.1109/ICEEOT.2016.7755481},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006742079&doi=10.1109%2fICEEOT.2016.7755481&partnerID=40&md5=c4a4d32d3ad04f5c513a52d377484575},
	affiliations = {Department of MCA, K. K. Wagh Institute of Engineering Education and Research, Nashik, Maharashtra, India; Department of CS and IT, Dr. Babasaheb Ambedkar Marathwada University, Aurangabad, Maharashtra, India},
	abstract = {Plants are very important for human being as well as for other living species on the earth. The food that people eat daily, comes directly or indirectly from plants. In medical field, doctors use X-ray image to correctly identify disease. Here we used the same principle. Geometrical features and digital morphological features are extracted from 2-dimensional image of leaf. The aim of this study is to introduce suitable features of leaf image which can be useful in further research on plant identification. © 2016 IEEE.},
	author_keywords = {feature extraction; image preprocssing; leaf; morphological features},
	keywords = {Extraction; Feature extraction; Medical imaging; Geometrical features; image preprocssing; leaf; Leaf architecture; Medical fields; Morphological features; Plant identification; Plant species; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-146739939-5},
	language = {English},
	abbrev_source_title = {Int. Conf. Electr., Electron., Optim. Techniques, ICEEOT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2016 International Conference on Electrical, Electronics, and Optimization Techniques, ICEEOT 2016; Conference date: 3 March 2016 through 5 March 2016; Conference code: 124975}
}

@ARTICLE{Naresh20161789,
	author = {Naresh, Y.G. and Nagendraswamy, H.S.},
	title = {Classification of medicinal plants: An approach using modified LBP with symbolic representation},
	year = {2016},
	journal = {Neurocomputing},
	volume = {173},
	pages = {1789 – 1797},
	doi = {10.1016/j.neucom.2015.08.090},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955175209&doi=10.1016%2fj.neucom.2015.08.090&partnerID=40&md5=bb961a4e5577d2ac553704e0b4dc3092},
	affiliations = {Department of studies in Computer Science, Manasagangothri, University of Mysore, Mysore, India},
	abstract = {In this work, a symbolic approach for classification of plant leaves based on texture features is proposed. Modified Local binary patterns (MLBP) is proposed to extract texture features from plant leaves. Texture of plant leaves belonging to same plant species may vary due to maturity levels, acquisition and environmental conditions. Hence, the concept of clustering is used to choose multiple class representatives and the intra-cluster variations are captured using interval valued type symbolic features. The classification is facilitated using a simple nearest neighbor classifier. Extensive experiments have been carried out on newly created UoM Medicinal Plant Dataset as well as publically available Flavia, Foliage and Swedish plant leaf datasets. Results obtained by proposed methodology are compared with the contemporary methodologies. The Outex dataset is also considered for experiments and the results are promising even on this synthetic dataset. © 2015 Elsevier B.V.},
	author_keywords = {Local Binary Patterns; Plant recognition; Symbolic representation; Texture classification},
	keywords = {Bins; Pattern recognition; Plant extracts; Environmental conditions; Local binary patterns; Nearest Neighbor classifier; Plant recognition; Symbolic features; Symbolic representation; Texture classification; Texture features; Article; classification; classifier; cluster analysis; data base; experimental study; foliage; intermethod comparison; mathematical computing; medicinal plant; methodology; modified local binary pattern; priority journal; separation technique; symbolism; Plants (botany)},
	publisher = {Elsevier B.V.},
	issn = {09252312},
	coden = {NRCGE},
	language = {English},
	abbrev_source_title = {Neurocomputing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 105}
}

@ARTICLE{Winberg2017238,
	author = {Winberg, Simon and Naidoo, Khagendra and Ramone, Moeko},
	title = {Accelerating computer-based recognition of fynbos leaves using a graphics processing unit},
	year = {2017},
	journal = {South African Computer Journal},
	volume = {29},
	number = {3},
	pages = {238 – 262},
	doi = {10.18489/sacj.v29i3.432},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037566206&doi=10.18489%2fsacj.v29i3.432&partnerID=40&md5=6eabf0d79ae3b536520251d03b8cb697},
	affiliations = {Department of Electrical Engineering, University of Cape Town, South Africa},
	abstract = {The Cape Floristic Kingdom (CFK) is the most diverse floristic kingdom in the world and has been declared an international heritage site. However, it is under threat from wild fires and invasive species. Much of the work of managing this natural resource, such as removing alien vegetation or fighting wild fires, is done by volunteers and casual workers. The Fynbos Leaf Optical Recognition Application (FLORA) was developed to assist in the recognition of plants of the CFK. The first version of FLORA was developed as a rapid prototype in MATLAB, but suffered from slow performance and did not run as a lightweight standalone executable. FLORA was thus re-developed as a standalone C++ application and subsequently enhanced using a graphics processing unit (GPU). This paper presents all three versions, viz., the MATLAB prototype, the C++ non-accelerated version, and the C++ GPU-accelerated version. The accuracy of predictions remained consistent. The C++ version was noticeable faster than the original prototype, achieving an average speed-up of 42 for high-resolution images. The GPU-accelerated version was even faster achieving an average speed-up of 54. Such time saving would be perceptible for batch processing, such as rebuilding feature descriptors in the leaf database. © the author(s).},
	author_keywords = {Automated plant identification; Computer vision; Image processing; Parallel algorithms},
	correspondence_address = {S. Winberg; Department of Electrical Engineering, University of Cape Town, South Africa; email: simon.winberg@uct.ac.za},
	publisher = {South African Institute of Computer Scientists and Information Technologists},
	issn = {10157999},
	language = {English},
	abbrev_source_title = {S. Afr. Comput. J.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{AlSuwaidi2017251,
	author = {AlSuwaidi, Ali and Grieve, Bruce and Yin, Hujun},
	title = {Towards spectral-texture approach to hyperspectral image analysis for plant classification},
	year = {2017},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10585 LNCS},
	pages = {251 – 260},
	doi = {10.1007/978-3-319-68935-7_28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034269082&doi=10.1007%2f978-3-319-68935-7_28&partnerID=40&md5=06607a70ce37e2cade76c10147ce259c},
	affiliations = {School of Electrical and Electronic Engineering, The University of Manchester, Manchester, M13 9PL, United Kingdom},
	abstract = {The use of hyperspectral imaging systems in studying plant properties, types, and conditions has significantly increased due to numerous economical and financial benefits. It can also enable automatic identification of plant phenotypes. Such systems can underpin a new generation of precision agriculture techniques, for instance, the selective application of plant nutrients to crops, preventing costly losses to soils, and the associated environmental impact to their ingress into watercourses. This paper is concerned with the analysis of hyperspectral images and data for monitoring and classifying plant conditions. A spectral-texture approach based on feature selection and the Markov random field model is proposed to enhance classification and prediction performance, as compared to conventional approaches. Two independent hyperspectral datasets, captured by two proximal hyperspectral instrumentations with different acquisition dates and exposure times, were used in the evaluation. Experimental results show promising improvements in the discrimination performance of the proposed approach. The study shows that such an approach can shed a light on the attributes that can better differentiate plants, their properties, and conditions. © Springer International Publishing AG 2017.},
	author_keywords = {Feature selection; Hyperspectral imaging; Markov random field; Spectral analysis; Texture analysis},
	keywords = {Automation; Environmental impact; Feature extraction; Image analysis; Image classification; Image segmentation; Independent component analysis; Markov processes; Soils; Spectroscopy; Spectrum analysis; Automatic identification; Conventional approach; Markov Random Field model; Markov Random Fields; Plant classification; Precision Agriculture; Prediction performance; Texture analysis; Hyperspectral imaging},
	correspondence_address = {A. AlSuwaidi; School of Electrical and Electronic Engineering, The University of Manchester, Manchester, M13 9PL, United Kingdom; email: ali.bghalsuwaidi@postgrad.manchester.ac.uk},
	editor = {Yin H. and Zhang M. and Wen Y. and Cai G. and Gu T. and Tallon-Ballesteros A.J. and Du J. and Gao Y. and Chen S.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331968934-0},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 18th International Conference on Intelligent Data Engineering and Automated Learning, IDEAL 2017; Conference date: 30 October 2017 through 1 November 2017; Conference code: 202239; All Open Access, Green Open Access}
}

@ARTICLE{Al-Kharaz2016219,
	author = {Al-Kharaz, Ali A. and Tao, Xiaohui and Zhang, Ji and Lafta, Raid},
	title = {Adopting hybrid descriptors to recognise leaf images for automatic plant specie identification},
	year = {2016},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10086 LNAI},
	pages = {219 – 233},
	doi = {10.1007/978-3-319-49586-6_15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000643982&doi=10.1007%2f978-3-319-49586-6_15&partnerID=40&md5=8348191ca50a1e7f3ba9be8151f04b68},
	affiliations = {School of Agricultural, Computational and Environmental Sciences, University of Southern Queensland, Toowoomba, Australia; Department of IT, Technical College of Management, Baghdad Foundation of Technical Education, Baghdad, Iraq},
	abstract = {In recent years, leaf image recognition and classification has become one of the most important subjects in computer vision. Many approaches have been proposed to recognise and classify leaf images relying on features extraction and selection algorithms. In this paper, a concept of distinctive hybrid descriptor is proposed consisting of both global and local features. HSV Colour histogram (HSV-CH) is extracted from leaf images as the global features, whereas Local Binary Pattern after two level wavelet decomposition (WavLBP) is extracted to represent the local characteristics of leaf images. A hybrid method, namely “Hybrid Descriptor” (HD), is then proposed considering both the global and local features. The proposed method has been empirically evaluated using four data sets of leaf images with 256×256 pixels. Experimental results indicate that the performance of proposed method is promising – the HD outperformed typical leaf image recognising approaches as baseline models in experiments. The presented work makes clear, significant contribution to knowledge advancement in leaf recognition and image classification. © Springer International Publishing AG 2016.},
	author_keywords = {Colour histogram; Global feature; LBP; Leaf image; Local feature; Texture; Wavelet},
	keywords = {Binary images; Computer vision; Data mining; Graphic methods; Image processing; Image recognition; Textures; Wavelet decomposition; Colour histograms; Global feature; Leaf images; Local feature; Wavelet; Image classification},
	correspondence_address = {A.A. Al-Kharaz; School of Agricultural, Computational and Environmental Sciences, University of Southern Queensland, Toowoomba, Australia; email: Ali.Al-kharaz@usq.edu.au},
	editor = {Li J. and Li X. and Wang S. and Li J. and Sheng Q.Z.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331949585-9},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 12th International Conference on Advanced Data Mining and Applications, ADMA 2016; Conference date: 12 December 2016 through 15 December 2016; Conference code: 186889; All Open Access, Green Open Access}
}

@CONFERENCE{Kumar2016975,
	author = {Kumar, V. Sathiesh and Gogul, I. and Raj, M. Deepan and Pragadesh, S.K. and Sebastin, J. Sarathkumar},
	title = {Smart Autonomous Gardening Rover with Plant Recognition Using Neural Networks},
	year = {2016},
	journal = {Procedia Computer Science},
	volume = {93},
	pages = {975 – 981},
	doi = {10.1016/j.procs.2016.07.289},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985991243&doi=10.1016%2fj.procs.2016.07.289&partnerID=40&md5=db9253eb92279cadeaa51b0a5a2b0328},
	affiliations = {Department of Electronics Engineering, Madras Institute of Technology, Anna University, Chennai, 600044, India; Department of Aerospace Engineering, Madras Institute of Technology, Anna Univeristy, Chennai, 600044, India},
	abstract = {Modernization of our environment (pruning trees for constructing tall buildings) results in climatic changes and ecological imbalance. To mitigate the effect, gardening (to plant trees and shrubs) becomes more and more important than just a hobby. Besides, maintenance of a garden is a tedious process and also time-consuming. Often the gardener lacks in knowledge about the requirements of plant (nutrient and the amount of water to be sprayed) to enhance its growth. In this regard, it is necessary to build an autonomous gardening robotic vehicle which automatically identifies and classifies the plant species using feature extraction algorithms (Scale Invariant Feature Transform (SIFT), Speeded-Up Robust Features (SURF), Oriented FAST and Rotated BRIEF (ORB)) and neural networks, respectively. It also measures the key parameters for gardening such as temperature, humidity, heat level, wind speed, wind direction and soil moisture. The data acquired from the on-board sensors of the gardening rover are sent to the cloud storage platform on a regular basis. Based on the acquired data and history, future predictions are made to maintain the garden more effectively and efficiently. A website and an android application are developed for monitoring and controlling the rover from a remote area. This system is a combination of new technologies involving an interdisciplinary approach to carry out precision gardening using Internet of Things (IoT). © 2016 Published by Elsevier B.V.},
	author_keywords = {arboriculture; floriculture; horticulture; Intelligent agriculture robotics; Internet of things; neural networks; plant recognition methods},
	keywords = {Agriculture; Algorithms; Feature extraction; Forestry; Internet; Internet of things; Modernization; Neural networks; Robotics; Soil moisture; Tall buildings; Wind; arboriculture; Feature extraction algorithms; floriculture; horticulture; Monitoring and controlling; Oriented fast and rotated brief (ORB); Plant recognition; Scale invariant feature transforms; Digital storage},
	correspondence_address = {V.S. Kumar; Department of Electronics Engineering, Madras Institute of Technology, Anna University, Chennai, 600044, India; email: sathiieesh@gmail.com},
	editor = {Krishna D.D. and Mathew J. and Jose J.},
	publisher = {Elsevier B.V.},
	issn = {18770509},
	language = {English},
	abbrev_source_title = {Procedia Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24; Conference name: 6th International Conference On Advances In Computing and Communications, ICACC 2016; Conference date: 6 September 2016 through 8 September 2016; Conference code: 131418; All Open Access, Gold Open Access}
}

@CONFERENCE{Šulc2016579,
	author = {Šulc, Milan and Mishkin, Dmytro and Matas, Jiří},
	title = {Very deep residual networks with MaxOut for plant identification in the wild},
	year = {2016},
	journal = {CEUR Workshop Proceedings},
	volume = {1609},
	pages = {579 – 586},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018486097&partnerID=40&md5=e625ca79acc7dd2f24e230d49a620d64},
	affiliations = {Center for Machine Perception, FEE, CTU in Prague, Technicka 2, Prague 6, 166 27, Czech Republic},
	abstract = {The paper presents our deep learning approach to automatic recognition of plant species from photos. We utilized a very deep 152-layer residual network [15] model pre-Trained on ImageNet, replaced the original fully connected layer with two randomly initialized fully connected layers connected with maxout [13], and fine-Tuned the network on the PlantCLEF 2016 training data. Bagging of 3 networks was used to further improve accuracy. With the proposed approach we scored among the top 3 teams in the PlantCLEF 2016 plant identification challenge.},
	keywords = {Automatic recognition; Learning approach; Plant identification; Plant species; Training data},
	editor = {Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Macdonald C. and Balog K.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2016 Working Notes of Conference and Labs of the Evaluation Forum, CLEF 2016; Conference date: 5 September 2016 through 8 September 2016; Conference code: 127674}
}

@CONFERENCE{Kiani2017357,
	author = {Kiani, Ehsan and Shahadat, Mohamad Al and Sadikoglu, Fahreddin},
	title = {Child perception-based plant species identification},
	year = {2017},
	journal = {Procedia Computer Science},
	volume = {120},
	pages = {357 – 364},
	doi = {10.1016/j.procs.2017.11.250},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040223782&doi=10.1016%2fj.procs.2017.11.250&partnerID=40&md5=9afdff2e1d55f4c688af9e8879b98acf},
	affiliations = {Department of Mechatronics Engineering, Near East University, POBOX:99138, Mersin 10, Nicosia, Cyprus},
	abstract = {Automation of mechanically pulling the weeds out of the crop row not only copes well with the new European high environmental standards but also removes the high cost of mere conventional alternative as high hand-weeding. The objective of this research work is to propose a similar distinguishing methodology of a weeding labour when discriminating the weeds species to choose and remove the undesired ones. The method is governed by a systematic analysis carried out on recognition method of an immaturely trained human brain. In other words, a number of children, who have never seen a maize agricultural field, were examined while recognizing a maize pattern using at most five sample images. The proposed method works mainly based on morphological operators for extraction of fundamental plant features in the image. The advantage of the proposed method is producing similar results to human labour which is approximate identification. This final decision was made by a fuzzy classifier which generate the degree of membership to either of weed or crop plant groups. Unlike the very popular research trend for object classification in the literature, our proposed methodology neither requires huge sample of images nor a high capacity processor. The accuracy of maize plant discrimination from typical Mediterranean weeds under extreme July sun illumination was observed as 95% for a scene of a single plant and 85% for a scene containing multiple objects. © 2018 The Authors. Published by Elsevier B.V.},
	author_keywords = {approximate plant identification (API); fuzzy decision maker; in-row weeding; machine vision; Weed/crop plant classification},
	keywords = {Agricultural machinery; Computation theory; Computer vision; Crops; Decision making; Fuzzy sets; Soft computing; Degree of membership; Environmental standards; Fuzzy decision-makers; Morphological operator; Object classification; Plant classification; Plant identification; Plant species identification; Image processing},
	correspondence_address = {E. Kiani; Department of Mechatronics Engineering, Near East University, Nicosia, POBOX:99138, Mersin 10, Cyprus; email: ehskiani@gmail.com},
	editor = {Jamshidi M. and Pedrycz W. and Aliev R.A. and Kacprzyk J.},
	publisher = {Elsevier B.V.},
	issn = {18770509},
	language = {English},
	abbrev_source_title = {Procedia Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 9th International Conference on Theory and Application of Soft Computing, Computing with Words and Perception, ICSCCW 2017; Conference date: 22 August 2017 through 23 August 2017; Conference code: 133174; All Open Access, Gold Open Access}
}

@CONFERENCE{AlSuwaidi20171,
	author = {AlSuwaidi, Ali and Grieve, Bruce and Yin, Hujun},
	title = {Spectral-Texture approach to hyperspectral image analysis for plant classification with SVMs},
	year = {2017},
	journal = {IST 2017 - IEEE International Conference on Imaging Systems and Techniques, Proceedings},
	volume = {2018-January},
	pages = {1 – 6},
	doi = {10.1109/IST.2017.8261496},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049394786&doi=10.1109%2fIST.2017.8261496&partnerID=40&md5=1f0578ee17630430ae00afa2578e17fc},
	affiliations = {School of Electrical and Electronic Engineering, University of Manchester, Manchester, M13 9PL, United Kingdom},
	abstract = {Numerous environmental and financial benefits of using hyperspectral imaging have driven much increased applications on plant monitoring and diagnosis. This paper is concerned with analysis of hyperspectral images for plant discrimination by means of their spectral and texture properties. The main contribution of the work lies in the use feature selection and Markov random field model (MRF) to facilitate such spectral-Texture analysis to enhance prediction performance, as compared to conventional analysis methods. A hyperspectral dataset on control and stressed Arabidopsis plant leaves captured by a proximal hyperspectral imaging system was used in the experiment. Texture parameters with different orders were estimated from the MRF model and two support vector machine settings were used in the evaluation. Experimental results showed significant improvements in classification performance of the proposed spectral-Texture approach over the conventional analysis methods. © 2017 IEEE.},
	author_keywords = {feature selection; hyperspectral imaging; markov random field; spectral analysis; texture analysis},
	keywords = {Feature extraction; Image analysis; Image classification; Image segmentation; Image texture; Imaging systems; Magnetorheological fluids; Markov processes; Plants (botany); Spectroscopy; Spectrum analysis; Support vector machines; Textures; Classification performance; Conventional analysis method; Markov random field models; Markov Random Fields; Plant classification; Prediction performance; Texture analysis; Texture properties; Hyperspectral imaging},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-153861620-8},
	language = {English},
	abbrev_source_title = {IST - IEEE Int. Conf. Imaging Syst. Techniques, Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 2017 IEEE International Conference on Imaging Systems and Techniques, IST 2017; Conference date: 18 October 2017 through 20 October 2017; Conference code: 134295; All Open Access, Green Open Access}
}

@CONFERENCE{Rojas-Hernandez2016,
	author = {Rojas-Hernandez, Rafael and Lopez-Chau, Asdrubal and Trujillo-Mora, Valentin and Rojas-Hernandez, Carlos A.},
	title = {Plant identification using new geometric features with standard data mining methods},
	year = {2016},
	journal = {ICNSC 2016 - 13th IEEE International Conference on Networking, Sensing and Control},
	doi = {10.1109/ICNSC.2016.7479024},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978081747&doi=10.1109%2fICNSC.2016.7479024&partnerID=40&md5=f89e2229ab53d643782712f1c32fd354},
	affiliations = {Universidad Autónoma Del Estado de México, CU UAEM Zumpango, Estado de México, 55600, Mexico},
	abstract = {Plant identification belongs to a specific application domain of data mining. Images of plant leaves are usually used as the main element to distinguish a plant from another. For proper identification, feature extraction is necessary. In the literature, most plant recognition systems use the features along with a classification method, which has been adapted or modified to face this type of application. In this paper, we propose three new geometric features that describe the vertical and horizontal symmetry of leaves. These features are simple to extract from images. According to the results of experiments, when these features are used in conjunction with other well-known geometric characteristics, the performance of classical classification methods is remarkably improved. To show the effectiveness of the proposal, we test seven classifiers with images of leaves publicly available on the Internet. © 2016 IEEE.},
	author_keywords = {feature extraction; geometric feature; leaf identification; Plant recognition},
	keywords = {Data mining; Extraction; Feature extraction; Geometry; Classification methods; Data mining methods; Geometric characteristics; Geometric feature; Horizontal symmetry; Leaf identification; Plant identification; Plant recognition; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-146739975-3},
	language = {English},
	abbrev_source_title = {ICNSC - IEEE Int. Conf. Netw., Sens. Control},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 13th IEEE International Conference on Networking, Sensing and Control, ICNSC 2016; Conference date: 28 April 2016 through 30 April 2016; Conference code: 121881}
}

@ARTICLE{Rahmani201617,
	author = {Rahmani, Mohamed Elhadi and Amine, Abdelmalek and Hamou, Reda Mohamed},
	title = {Supervised machine learning for plants identification based on images of their leaves},
	year = {2016},
	journal = {International Journal of Agricultural and Environmental Information Systems},
	volume = {7},
	number = {4},
	pages = {17 – 31},
	doi = {10.4018/IJAEIS.2016100102},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991704717&doi=10.4018%2fIJAEIS.2016100102&partnerID=40&md5=3f1c743e204ce6b7bee47865d7fff78f},
	affiliations = {GeCoDe Laboratory, Department of Computer Science, Dr. Moulay Tahar University of Saida, Saida, Algeria},
	abstract = {Botanists study in general the characteristics of leaves to give to each plant a scientific name; such as shape, margin...etc. This paper proposes a comparison of supervised plant identification using different approaches. The identification is done according to three different features extracted from images of leaves: a fine-scale margin feature histogram, a Centroid Contour Distance Curve shape signature and an interior texture feature histogram. First represent each leaf by one feature at a time in, then represent leaves by two features, and each leaf was represented by the three features. After that, the authors classified the obtained vectors using different supervised machine learning techniques; the used techniques are Decision tree, Naïve Bayes, K-nearest neighbour, and neural network. Finally, they evaluated the classification using cross validation. The main goal of this work is studying the influence of representation of leaves' images on the identification of plants, and also studying the use of supervised machine learning algorithm for plant leaves classification. Copyright © 2016, IGI Global.},
	author_keywords = {Data Mining in Agriculture; Data Representation; Decision Tree; K-Nearest Neighbour; Naive Bayes; Neural Network; Plants Leaves Classification; Supervised Classification},
	keywords = {Artificial intelligence; Data mining; Decision trees; Graphic methods; Image processing; Learning algorithms; Learning systems; Nearest neighbor search; Neural networks; Plants (botany); Trees (mathematics); Data representations; K-nearest neighbours; Naive bayes; Plant identification; Plant leaves classifications; Supervised classification; Supervised machine learning; Texture features; agriculture; algorithm; artificial neural network; data mining; image classification; leaf; nearest neighbor analysis; plant; supervised learning; Supervised learning},
	publisher = {IGI Global},
	issn = {19473192},
	language = {English},
	abbrev_source_title = {Int. J. Agric. Environ. Inf. Syst.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Bressane2016,
	author = {Bressane, Adriano and Roveda, José Arnaldo Frutuoso and Martins, Antonio Cesar Germano},
	title = {Pattern recognition in trunk images based on co-occurrence descriptors: A proposal applied to tree species identification},
	year = {2016},
	journal = {2015 Latin-America Congress on Computational Intelligence, LA-CCI 2015},
	doi = {10.1109/LA-CCI.2015.7435983},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969625941&doi=10.1109%2fLA-CCI.2015.7435983&partnerID=40&md5=a9c9a7b8b45f4a1d3578a4c67a9b8035},
	affiliations = {Environmental Sciences Graduate Program, UNESP - Univ. Estadual Paulista, Sorocaba city, São Paulo State, Brazil},
	abstract = {Tree species identification is required for many applications. However, current techniques are dependent on the presence of morphological structures such as leaves, which restricts its use in certain situations and seasons. In this context, the use of trunk images can be an alternative. Therefore, the present study developed a pattern recognition based on co-occurrence descriptors, aiming evaluate its performance in the identification of 8 tree species from the Brazilian deciduous native forest, achieving promising results, with precision better than 0.8 for most of them, accuracy equivalent to 0.77 and average area under curve by Receiver Operating Characteristic of 0.88, during the tests with cross-validation sets. © 2015 IEEE.},
	author_keywords = {Brazilian forest; Co-occurrence descriptors; Decision Tree; Image processing; Trunk images},
	keywords = {Artificial intelligence; Decision trees; Forestry; Pattern recognition; Brazilian forest; Cross validation; Descriptors; Morphological structures; Native forests; Receiver operating characteristics; Tree species identifications; Trunk images; Image processing},
	editor = {Vellasco M.M.B.R. and Tupac Valdivia Y.J. and Lopes H.S.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-146738418-6},
	language = {English},
	abbrev_source_title = {Latin-Am. Congr. Comput. Intell., LA-CCI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2nd Latin-America Congress on Computational Intelligence, LA-CCI 2015; Conference date: 13 October 2015 through 16 October 2015; Conference code: 120052}
}@CONFERENCE{Yanikoglu2014771,
	author = {Yanikoglu, Berin and Yildiran, S. Tolga and Tirkaz, Caglar and Aptoula, Erchan},
	title = {Sabanci-Okan system at LifeCLEF 2014 Plant Identification Competition},
	year = {2014},
	journal = {CEUR Workshop Proceedings},
	volume = {1180},
	pages = {771 – 777},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981234393&partnerID=40&md5=fc9215bf7b6006a174dec2d195affe5b},
	affiliations = {Sabanci University, Istanbul, Turkey; Okan University, Istanbul, Turkey},
	abstract = {We describe our system in 2014 LifeCLEF [1] Plant Identification Competition. The sub-system for isolated leaf category (LeafS-cans) was basically the same as last year [2], while plant photographs in all the remaining categories were classified using either local descriptors or deep learning techniques. However, due to large amount of data, large number of classes and shortage of time, our system was not very successful in the plant photograph sub-categories; but we obtained better results in isolated leaf images. As announced by the organizers, we obtained an inverse rank score of 0.127 overall and 0.449 for isolated leaves.},
	keywords = {Photography; Deep learning; Large amounts; Leaf images; Local descriptors; Number of class; Plant identification; Rank scores; Sub-systems; Plants (botany)},
	editor = {Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Halvey M. and Kraaij W.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2014 Cross Language Evaluation Forum Conference, CLEF 2014; Conference date: 15 September 2014 through 18 September 2014; Conference code: 110355}
}

@CONFERENCE{Gai2015890,
	author = {Gai, Jingyao and Tang, Lie and Steward, Brian},
	title = {Plant recognition through the fusion of 2D and 3D images for robotic weeding},
	year = {2015},
	journal = {American Society of Agricultural and Biological Engineers Annual International Meeting 2015},
	volume = {2},
	pages = {890 – 898},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951760358&partnerID=40&md5=2616f78667947ae522d2f32d05c70962},
	affiliations = {Lowa State University, United States},
	abstract = {In crop production systems, weed management is vitally important. But both manual weeding and herbicide-based weed controlling are problematic due to concerns in cost, operator health, emergence of herbicide-resistant weed species, and environment impact. Automated robotic weeding offers a possibility of controlling weeds in a precise fashion, particularly for weeds growing near crops or within crop rows. However, identification and localization of plants have not yet been fully automated. The goal of this reported project is to develop a high-throughput plant recognition and localization algorithm by fusing 2D color and textural data with 3D point cloud data. Plant morphological models were developed and applied for plant recognition against different weed species at different growth stages. Copyright © (2015) by the American Society of Agricultural and Biological Engineers All rights reserved.},
	author_keywords = {Computer vision; Robotic weeding; Sensor fusion},
	keywords = {Agriculture; Crops; Cultivation; Herbicides; Robotics; Weed control; Crop production systems; Different growth stages; Environment impact; Herbicide resistant weeds; Localization algorithm; Plant morphological; Robotic weeding; Sensor fusion; Computer vision},
	publisher = {American Society of Agricultural and Biological Engineers},
	isbn = {978-151081050-1},
	language = {English},
	abbrev_source_title = {Am. Soc. Agric. Biol. Eng. Annu. Int. Meet.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20; Conference name: American Society of Agricultural and Biological Engineers Annual International Meeting 2015; Conference date: 26 July 2015 through 29 July 2015; Conference code: 116712}
}

@ARTICLE{Safa201514939,
	author = {Safa, Soodabeh and Khalid, Fatimah and Abdullah, Lili Nurliana and Azman, Azreen},
	title = {Multi view K-SVD dictionary learning for plant identification},
	year = {2015},
	journal = {International Journal of Applied Engineering Research},
	volume = {10},
	number = {6},
	pages = {14939 – 14951},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928910589&partnerID=40&md5=0421995b3baab5299a01674b51d3d14b},
	affiliations = {Department of Multimedia, University Putra Malaysia, Serdang, 43300, Malaysia},
	abstract = {These days, various forms of data can be seen in real word. Images as a popular type of data are widely used in many resources such as web pages, video, etc. Meanwhile, each images as a source of data is comprised of many features. Different methods of feature extraction present occasionally different descriptions of the data, each can be considered as one view of data. In this study, K-Singular Value Decomposition (KSVD) method which is a popular method for sparse dictionary learning is developed for multi view data. Sparse dictionary learning has been applied in multi view data identification problem to find out which classes belongs to basis vector. Different feature extraction methods such as Scale-Invariant Feature Transform (SIFT) and GIST of a scene are used for plant identification to describe images of leaves in many studies, but in most of them, these different views of data are merged as high dimensional features. In this study K-SVD, as a dimension reduction algorithm is developed for sparse dictionary learning for multi view data create low dimensional visualization of images in plant identification scope. The experimental results show improvement in accuracy compared to regular K-SVD and early fusion of features. © Research India Publications.},
	author_keywords = {Basis vector (atom); K-SVD algorithm; Multi view data; Plant identification; Regularizer; Sparse dictionary learning},
	publisher = {Research India Publications},
	issn = {09734562},
	language = {English},
	abbrev_source_title = {Int. J. Appl. Eng. Res.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hsiao2014389,
	author = {Hsiao, Jou-Ken and Kang, Li-Wei and Chang, Ching-Long and Lin, Chih-Yang},
	title = {Comparative study of leaf image recognition with a novel learning-based approach},
	year = {2014},
	journal = {Proceedings of 2014 Science and Information Conference, SAI 2014},
	pages = {389 – 393},
	doi = {10.1109/SAI.2014.6918216},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909594480&doi=10.1109%2fSAI.2014.6918216&partnerID=40&md5=d15e33a0b013ee8887f231385d0dc87c},
	affiliations = {Department of Computer Science and Information Engineering, National Yunlin University of Science and Technology, Yunlin, Taiwan; Graduate School of Engineering Science and Technology-Doctoral Program, National Yunlin University of Science and Technology, Yunlin, Taiwan; Department of Computer Science and Information Engineering, Asia University, Taichung, Taiwan},
	abstract = {Automatic plant identification via computer vision techniques has been greatly important for a number of professionals, such as environmental protectors, land managers, and foresters. In this paper, we conduct a comparative study on leaf image recognition and propose a novel learning-based leaf image recognition technique via sparse representation (or sparse coding) for automatic plant identification. In our learning-based method, in order to model leaf images, we learn an overcomplete dictionary for sparsely representing the training images of each leaf species. Each dictionary is learned using a set of descriptors extracted from the training images in such a way that each descriptor is represented by linear combination of a small number of dictionary atoms. Moreover, we also implement a general bag-of-words (BoW) model-based recognition system for leaf images, used for comparison. We experimentally compare the two approaches and show unique characteristics of our sparse coding-based framework. As a result, efficient leaf recognition can be achieved on public leaf image dataset based on the two evaluated methods, where the proposed sparse coding-based framework can perform better. © 2014 The Science and Information (SAI) Organization.},
	author_keywords = {bag-of-words; classification; dictionary learning; leaf recognition; plant identification},
	keywords = {Classification (of information); Codes (symbols); Image coding; Image recognition; Information retrieval; Personnel training; Bag of words; Computer vision techniques; Dictionary learning; Environmental protector; Leaf recognition; Model-based recognition; Over-complete dictionaries; Plant identification; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-098931931-7},
	language = {English},
	abbrev_source_title = {Proc. Sci. Inf. Conf., SAI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 30; Conference name: 2014 Science and Information Conference, SAI 2014; Conference date: 27 August 2014 through 29 August 2014; Conference code: 114737}
}

@CONFERENCE{Wang20151430,
	author = {Wang, Zhaobin and Zheng, Xu and Sun, Xiaoguang and Wang, Hao and Zhu, Ying and Liu, Jianpeng and Ma, Yide},
	title = {A new petiole detection algorithm based on leaf image},
	year = {2015},
	journal = {Canadian Conference on Electrical and Computer Engineering},
	volume = {2015-June},
	number = {June},
	pages = {1430 – 1434},
	doi = {10.1109/CCECE.2015.7129490},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938337989&doi=10.1109%2fCCECE.2015.7129490&partnerID=40&md5=ba2d1afa4ba311644437272bd5cf77ae},
	affiliations = {School of Information Science and Engineering, Lanzhou University, Lanzhou City, Gansu Province, 730000, China; Institute of Biology, Gansu Academy of Sciences, Lanzhou City, Gansu Province, 730000, China; Xi'An Institute of Applied Optics, Xi'an City, Shaanxi Province, 710065, China},
	abstract = {Leaf is one of the most important organs of plant and often used as one of the basic characters in plant classification. The developmental condition of leaf can provide us with lots of critical information, such as the plant's health condition, the prospection of crop yield and so on. Leaf image processing by computer has been widely used for the extraction and dissection of leaf images in relevant researches. Image processing of leaf also offers an effective platform for plant classification and growth observation. A basic problem of leaf image processing is detecting and dislodging the petiole from the whole leaf image. Here this paper presents an algorithm which combines the dual-channel pulse coupled neural network (PCNN) model and HSI color space for leaf petiole detection. Totally 169 sorts of leaf images are tested by the proposed algorithm. The experimental results show that our method has potential availability in reducing mis-evaluation and increasing application scale as a tool in relative study. © 2015 IEEE.},
	keywords = {Signal detection; Crop yield; Detection algorithm; Dual channel; Health condition; HSI color space; Leaf images; Plant classification; Pulse coupled neural network models; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {08407789},
	isbn = {978-147995827-6},
	coden = {CCCEF},
	language = {English},
	abbrev_source_title = {Can Conf Electr Comput Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 2015 28th IEEE Canadian Conference on Electrical and Computer Engineering, CCECE 2015; Conference date: 3 May 2015 through 6 May 2015; Conference code: 113115}
}

@CONFERENCE{Winberg2013,
	author = {Winberg, Simon and Katz, Shaun and Mishra, Amit Kumar},
	title = {Fynbos leaf online plant recognition application},
	year = {2013},
	journal = {2013 4th National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics, NCVPRIPG 2013},
	doi = {10.1109/NCVPRIPG.2013.6776220},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898777259&doi=10.1109%2fNCVPRIPG.2013.6776220&partnerID=40&md5=ea196cf7df996b7e748ad06088459b54},
	affiliations = {University of Cape Town, Cape Town, South Africa},
	abstract = {Computer-aided plant identification combines computer vision and pattern recognition. The Cape Floristic Kingdom is the most varied of plant kingdoms, comprising thousands of species of fynbos plants. While it is easier to classify fynbos when they are flowering, mostly flower for only a few weeks in a year. This paper concerns an image processing application for automatic identification of certain fynbos using leaf photographs. The architecture of this application is overviewed prior to focusing on the leaf recognition operations, and how these were experimentally tested using a series of experiments, culminating in a comprehensive test to measure identification accuracy, effectiveness of the online user interface, and the processing speed. Our conclusions reflect on the overall effectiveness of the application and our plans to take it further. © 2013 IEEE.},
	author_keywords = {computer vision; digital morphological features; fynbos; image procesing; leaf database; plant recognition},
	keywords = {Automation; Computer vision; Image processing; User interfaces; fynbos; image procesing; Leaf database; Morphological features; Plant recognition; Pattern recognition},
	publisher = {IEEE Computer Society},
	language = {English},
	abbrev_source_title = {Natl. Conf. Comput. Vis., Pattern Recogn., Image Process. Graph., NCVPRIPG},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 2013 4th National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics, NCVPRIPG 2013; Conference date: 18 December 2013 through 21 December 2013; Conference code: 104512}
}

@CONFERENCE{Di Ruberto2014601,
	author = {Di Ruberto, Cecilia and Putzu, Lorenzo},
	title = {A fast leaf recognition algorithm based on SVM classifier and high dimensional feature vector},
	year = {2014},
	journal = {VISAPP 2014 - Proceedings of the 9th International Conference on Computer Vision Theory and Applications},
	volume = {1},
	pages = {601 – 609},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906899600&partnerID=40&md5=9dd931511a516d2accd61bcccfced798},
	affiliations = {Department of Mathematics and Computer Science, University of Cagliari, 09124 Cagliari, via Ospedale 72, Italy},
	abstract = {Plants are fundamental for human beings, so it's very important to catalog and preserve all the plants species. Identifying an unknown plant species is not a simple task. Automatic image processing techniques based on leaves recognition can help to find the best features useful for plant representation and classification. Many methods present in literature use only a small and complex set of features, often extracted from the binary images or the boundary of the leaf. In this work we propose a leaf recognition method which uses a new features set that incorporates shape, color and texture features. A total of 138 features are extracted and used for training a SVM model. The method has been tested on Flavia dataset (Wu et al., 2007), showing excellent performance both in terms of accuracy that often reaches 100%, and in terms of speed, less than a second to process and extract features from an image. Copyright © 2014 SCITEPRESS - Science and Technology Publications. All rights reserved.},
	author_keywords = {Feature extraction; Image analysis; Leaf recognition; Plant classification; Support vector machine},
	keywords = {Algorithms; Feature extraction; Image analysis; Image processing; Image retrieval; Support vector machines; Automatic image processing; Color and texture features; Features sets; High dimensional feature; Leaf recognition; Plant classification; Plant species; SVM classifiers; Plants (botany)},
	publisher = {SciTePress},
	isbn = {978-989758003-1},
	language = {English},
	abbrev_source_title = {VISAPP - Proc. Int. Conf. Comput. Vis. Theory Appl.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; Conference name: 9th International Conference on Computer Vision Theory and Applications, VISAPP 2014; Conference date: 5 January 2014 through 8 January 2014; Conference code: 107286}
}

@ARTICLE{Backes201551,
	author = {Backes, André R. and de Mesquita Sá Junior, Jarbas Joaci and Kolb, Rosana Marta},
	title = {Color fractal descriptors for adaxial epidermis texture classification},
	year = {2015},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {9423},
	pages = {51 – 58},
	doi = {10.1007/978-3-319-25751-8_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983656147&doi=10.1007%2f978-3-319-25751-8_7&partnerID=40&md5=5f8721959c6d491e79f2bb7e2c089876},
	affiliations = {Faculdade de Computação, Universidade Federal de Uberlândia, Av. João Naves de Ávila, 2121, Uberlândia, 38408-100, MG, Brazil; Departamento de Engenharia de Computação, Campus de Sobral - Universidade Federal Do Ceará, Rua Estanislau Frota, S/N, Centro, Sobral, 62010-560, Ceará, Brazil; Departamento de Ciências Biológicas, Faculdade de Ciências E Letras, Universidade Estadual Paulista, UNESP, Av. Dom Antônio, 2100, Assis, 19806-900, SP, Brazil},
	abstract = {The leaves are an important plant organ and source of information for the traditional plant taxonomy. This study proposes a plant classification approach using the adaxial epidermis tissue, a specific cell layer that covers the leaf. To accomplish this task, we apply a high discriminative color texture analysis method based on the Bouligand- Minkowski fractal dimension. In an experimental comparison, the success rate obtained by our proposed approach (96.66%) was the highest among all the methods used, demonstrating that the Bouligand- Minkowski method is very suitable to extract discriminant features from the adaxial epidermis. Thus, this research can significantly contribute with other studies on plant classification by using computer vision. © Springer International Publishing Switzerland 2015.},
	author_keywords = {Adaxial epidermis tissue; Bouligand-Minkowski method; Color; Fractal dimension; Texture analysis},
	keywords = {Color; Computer vision; Fractals; Pattern recognition; Plants (botany); Tissue; Color texture analysis; Experimental comparison; Minkowski; Minkowski fractals; Plant classification; Plant taxonomy; Texture analysis; Texture classification; Fractal dimension},
	correspondence_address = {A.R. Backes; Faculdade de Computação, Universidade Federal de Uberlândia, Uberlândia, Av. João Naves de Ávila, 2121, 38408-100, Brazil; email: arbackes@yahoo.com.br},
	editor = {Pardo A. and Kittler J.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331925750-1},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 20th Iberoamerican Congress on on Pattern Recognition, CIARP 2015; Conference date: 9 November 2015 through 12 November 2015; Conference code: 127398; All Open Access, Bronze Open Access}
}

@CONFERENCE{Ghazi2015,
	author = {Ghazi, Mostafa Mehdipour and Yanikoglu, Berrin and Aptoula, Erchan and Muslu, Ozlem and Ozdemir, Murat Can},
	title = {Sabanci-Okan system in LifeCLEF 2015 plant identification competition},
	year = {2015},
	journal = {CEUR Workshop Proceedings},
	volume = {1391},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982811036&partnerID=40&md5=ec408fdddf1f5cdbb072bd7b030dd672},
	affiliations = {Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; Computer Engineering Department, Okan University, Istanbul, Turkey},
	abstract = {We present our deep learning based plant identification system in the LifeCLEF 2015. The approach is based on a simple deep convolutional network called PCANet and does not require large amounts of data due to using principal component analysis to learn the weights. After learning multistage filter banks, a simple binary hashing is applied to the filtered data, and features are pooled from block histograms. A multiclass linear support vector machine is then trained and the system is evaluated using the plant task datasets of LifeCLEF 2014 and 2015. As announced by the organizers, our submission achieved an overall inverse rank score of 0.153 in the image-based and an inverse rank score of 0.162 in the observation-based task of LifeCLEF 2015, as well as an inverse rank score of 0.51 for the LeafScan dataset of LifeCLEF 2014.},
	author_keywords = {Deep learning; Inverse rank score; PCANet; Plant identification; Support vector machine},
	keywords = {Support vector machines; Convolutional networks; Deep learning; Large amounts of data; Linear Support Vector Machines; PCANet; Plant identification; Plant identification systems; Rank scores; Principal component analysis},
	editor = {Jones G.J.F. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and San Juan E.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 16th Conference and Labs of the Evaluation Forum, CLEF 2015; Conference date: 8 September 2015 through 11 September 2015; Conference code: 122644}
}

@CONFERENCE{Rodrigo201341,
	author = {Rodrigo, Ranga and Samarawickrame, Kalani and Mindya, Sheron},
	title = {An intelligent flower analyzing system for medicinal plants},
	year = {2013},
	journal = {21st International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, WSCG 2013 - Poster Proceedings},
	pages = {41 – 44},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926647816&partnerID=40&md5=0eb7957a1b45abde61c58e71f96bd61d},
	affiliations = {School of Electrical and Computer Engineering, University of Moratuwa, Sri Lanka; Informatics Institute of Technology, Sri Lanka},
	abstract = {The natural sciences have been transformed with the incorporation of advanced technologies, and the current technological wave of change is also revolutionizing biological science. With the wide use of herbal medicines in traditional medical systems, the demand for medicinal plants has increased enormously. Though the current trend has made the use of medicinal plants more popular, we still require better methods to distinguish between medicinal plants and plants which do not possess medicinal value. Manual methods of plant recognition rely on a plants flower information. However, these manual processes of flower recognition are not straightforward for lay persons unless expert guidance is provided. Motivated by this reasoning, an intelligent flower analyzing system has been developed to recognize medicinal plants. Various tests have been performed on 160 images taken from four types of flowers, to recognize flowers based on their colour and shape features. The experiment was carried out using Support Vector Machine (SVM) and Principal Component Analysis (PCA) methodologies respectively for colour and shape extraction. Acquiring an average of 65% recognition rate implies that the applicability PCA and SVM in the specified domain is a valuable step forward.},
	keywords = {Computer graphics; Computer vision; Plants (botany); Support vector machines; Advanced technology; Analyzing system; Biological science; Flower recognition; Herbal medicines; Medicinal plants; Medicinal values; Plant recognition; Principal component analysis},
	editor = {Skala V. and University of West Bohemia, Univerzitni 8, Plzen},
	publisher = {Vaclav Skala - Union Agency},
	isbn = {978-808694376-3},
	language = {English},
	abbrev_source_title = {Int. Conf. Central Eur. Comput. Graph., Vis. Comput. Vis., WSCG - Poster Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 21st International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, WSCG 2013; Conference date: 24 June 2013 through 27 June 2013; Conference code: 111636}
}

@CONFERENCE{Chen2014693,
	author = {Chen, Qiang and Abedini, Mani and Garnavi, Rahil and Liang, Xi},
	title = {IBM Research Australia at LifeCLEF2014: Plant identification task},
	year = {2014},
	journal = {CEUR Workshop Proceedings},
	volume = {1180},
	pages = {693 – 704},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981216978&partnerID=40&md5=395d2c42faf59844468abc967751c3dd},
	affiliations = {IBM Research Australia, Australia},
	abstract = {In this paper, we present the system and learning strategies that were applied by the IBM Research team to the plant identification task of LifeCLEF 2014. Plant identification is one of the most popular fine-grained categorization tasks. To ensure high classification accuracy, we have utilised strong visual features together with fusion of robust machine learning techniques. Our proposed system involves automatic delineation of the region of interest (e.g. plant's leaf, flower, etc.) in the given image, followed by extracting multiple complementary low level features. The features have been then encoded into the sophisticated Fisher Vector representation which enables accurate classification with linear classifiers. We have also applied the recent development of deep learning. More importantly our system combines multiple source of information, i.e. integrates organ annotation with image data, and adopts fusion of classifiers which has led to great results. The extensive experiments demonstrate the effectiveness of the proposed system, where three (out of four) of our submissions outperforms all submissions by other teams, therefore the team achieves the first place in LifeCLEF 2014 Plant task.},
	author_keywords = {Deep learning; Feature coding; Fine-grained object recognition; Plant recognition},
	keywords = {Artificial intelligence; Image segmentation; Learning systems; Object recognition; Classification accuracy; Deep learning; Feature coding; Fine grained; Fusion of classifiers; Machine learning techniques; Plant identification; Plant recognition; Classification (of information)},
	editor = {Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Halvey M. and Kraaij W.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 2014 Cross Language Evaluation Forum Conference, CLEF 2014; Conference date: 15 September 2014 through 18 September 2014; Conference code: 110355}
}

@CONFERENCE{Prasad2013405,
	author = {Prasad, Shitala and Peddoju, Sateesh K. and Ghosh, Debashis},
	title = {Mobile plant species classification: A low computational aproach},
	year = {2013},
	journal = {2013 IEEE 2nd International Conference on Image Information Processing, IEEE ICIIP 2013},
	pages = {405 – 409},
	doi = {10.1109/ICIIP.2013.6707624},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893735720&doi=10.1109%2fICIIP.2013.6707624&partnerID=40&md5=d68cc4b4655847177a90d8ea49bf6d3e},
	affiliations = {Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, Haridwar, India; Department of Electronics and Communication Engineering, Indian Institute of Technology, Roorkee, Haridwar, India},
	abstract = {In this paper a reduced shape and color feature extraction method is proposed for a mobile device based plant classification system. For scientists, botanists, farmers, and others plant identification is a useful and important task. The original image captured is reduced to similar aspect ratio which does not affect the shape information but reduces the computation cost nearly up to half of the total cost. The algorithm first calculates the geometric feature and then polar Fourier transform and trained using k-NN classifier. Then two nearest classes were selected on the basics of smallest distance which is further rectified by the color features using a decision tree. The algorithm proves to be better in performance compared to other already existing algorithms. © 2013 IEEE.},
	author_keywords = {Color Features; Geometric Shape Features; k-NN; Leaf Recognition; Low Computation; Plant Identification; Polar Fourier Transform},
	keywords = {Aspect ratio; Content based retrieval; Data processing; Decision trees; Mobile devices; Color features; Geometric shape; k-NN; Leaf recognition; Plant identification; Algorithms},
	isbn = {978-146736101-9},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Image Inf. Process., IEEE ICIIP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; Conference name: 2013 IEEE 2nd International Conference on Image Information Processing, IEEE ICIIP 2013; Conference date: 9 December 2013 through 11 December 2013; Conference code: 102507}
}

@CONFERENCE{Mabrouk2014201,
	author = {Mabrouk, Amira Ben and Najjar, Asma and Zagrouba, Ezzeddine},
	title = {Image flower recognition based on a new method for color feature extraction},
	year = {2014},
	journal = {VISAPP 2014 - Proceedings of the 9th International Conference on Computer Vision Theory and Applications},
	volume = {2},
	pages = {201 – 206},
	doi = {10.5220/0004636302010206},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906910958&doi=10.5220%2f0004636302010206&partnerID=40&md5=024080ad9268597e6987212af078033f},
	affiliations = {Team of Research SIIVA- Lab. RIADI, Institut Supérieur D'Informatique, Université Tunis Elmanar, Tunis, Tunisia},
	abstract = {In this paper, we present, first, a new method for color feature extraction based on SURF detectors. Then, we proved its efficiency for flower image classification. Therefore, we described visual content of the flower images using compact and accurate descriptors. These features are combined and the learning process is performed using a multiple kernel framework with a SVM classifier. The proposed method has been tested on the dataset provided by the university of oxford and achieved better results than our implementation of the method proposed by Nilsback and Zisserman (Nilsback and Zisserman, 2008) in terms of classification rate and execution time. Copyright © 2014 SCITEPRESS - Science and Technology Publications. All rights reserved.},
	author_keywords = {Lab Color Space; MKL; SURF; SVM; Visual Vocabulary},
	keywords = {Classification (of information); Color; Color image processing; Computer vision; Extraction; Support vector machines; Classification rates; Color feature extraction; Flower recognition; Its efficiencies; Lab color space; SURF; University of Oxford; Visual vocabularies; Feature extraction},
	publisher = {SciTePress},
	isbn = {978-989758004-8},
	language = {English},
	abbrev_source_title = {VISAPP - Proc. Int. Conf. Comput. Vis. Theory Appl.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; Conference name: 9th International Conference on Computer Vision Theory and Applications, VISAPP 2014; Conference date: 5 January 2014 through 8 January 2014; Conference code: 107286}
}

@ARTICLE{Rueda-Plata2015275,
	author = {Rueda-Plata, Diego and Ramos-Pollán, Raúl and González, Fabio A.},
	title = {Supervised greedy layer-wise training for deep convolutional networks with small datasets},
	year = {2015},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {9329},
	pages = {275 – 284},
	doi = {10.1007/978-3-319-24069-5_26},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984623442&doi=10.1007%2f978-3-319-24069-5_26&partnerID=40&md5=f472a40c7f0d29a5dce6a4111e6e6b66},
	affiliations = {Universidad Industrial de Santander, Bucaramanga, Colombia; Universidad Nacional de Colombia, Bogotá, Colombia},
	abstract = {Deep convolutional neural networks (DCNs) are increasingly being used with considerable success in image classification tasks trained over large datasets. However, such large datasets are not always available or affordable in many applications areas where we would like to apply DCNs, having only datasets of the order of a few thousands labelled images, acquired and annotated through lenghty and costly processes (such as in plant recognition, medical imaging, etc.). In such cases DCNs do not generally show competitive performance and one must resort to fine-tune networks that were costly pretrained with large generic datasets where there is no a-priori guarantee that they would work well in specialized domains. In this work we propose to train DCNs with a greedy layerwise method, analogous to that used in unsupervised deep networks. We show how, for small datasets, this method outperforms DCNs which do not use pretrained models and results reported in the literature with other methods. Additionally, our method learns more interpretable and cleaner visual features. Our results are also competitive as compared with convolutional methods based on pretrained models when applied to general purpose datasets, and we obtain them with much smaller datasets (1.2 million vs. 10K images) at a fraction of the computational cost. We therefore consider this work a first milestone in our quest to successfully use DCNs for small specialized datasets. © Springer International Publishing Switzerland 2015.},
	author_keywords = {Convolutional networks; Deep learning; Greedy layer-wise training},
	keywords = {Convolution; Image classification; Medical imaging; Neural networks; Competitive performance; Computational costs; Convolutional networks; Convolutional neural network; Deep learning; Layer-wise; Layerwise method; Plant recognition; Classification (of information)},
	correspondence_address = {R. Ramos-Pollán; Universidad Industrial de Santander, Bucaramanga, Colombia; email: rramosp@uis.edu.co},
	publisher = {Springer Verlag},
	issn = {03029743},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; Conference name: 7th International Conference on Computational Collective Intelligence, ICCCI 2015; Conference date: 21 September 2015 through 23 September 2015; Conference code: null}
}

@ARTICLE{Ab Jabal20131295,
	author = {Ab Jabal, Mohamad Faizal and Hamid, Suhardi and Shuib, Salehuddin and Ahmad, Illiasaak},
	title = {Leaf features extraction and recognition approaches to classify plant},
	year = {2013},
	journal = {Journal of Computer Science},
	volume = {9},
	number = {10},
	pages = {1295 – 1304},
	doi = {10.3844/jcssp.2013.1295.1304},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890369931&doi=10.3844%2fjcssp.2013.1295.1304&partnerID=40&md5=1cde956ee3de59277f6bab5142816f16},
	affiliations = {Department of Computer Science, Faculty of Computer Science and Mathematics, Universiti Teknologi MARA, Malaysia},
	abstract = {Plant classification based on leaf identification is becoming a popular trend. Each leaf carries substantial information that can be used to identify and classify the origin or the type of plant. In medical perspective, images have been used by doctors to diagnose diseases and this method has been proven reliable for years. Using the same method as doctors, researchers try to simulate the same principle to recognise a plant using high quality leaf images and complex mathematical formulae for computers to decide the origin and type of plants. The experiments have yielded many success stories in the lab, but some approaches have failed miserably when tested in the real world. This happens because researchers may have ignored the facts that the real world sampling may not have the luxury and complacency as what they may have in the lab. What this study intends to deliver is the ideal case approach in plant classification and recognition that not only applicable in the real world, but also acceptable in the lab. The consequence from this study is to introducing more external factors for consideration when experimenting real world sampling for leaf recognition and classification does this. © 2013 Science Publications.},
	author_keywords = {Extraction; Leaf features; Plant classification; Recognition},
	issn = {15493636},
	language = {English},
	abbrev_source_title = {J. Comput. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 46; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Yusof201368,
	author = {Yusof, Rubiyah and Khalid, Marzuki and M. Khairuddin, Anis Salwa},
	title = {Application of kernel-genetic algorithm as nonlinear feature selection in tropical wood species recognition system},
	year = {2013},
	journal = {Computers and Electronics in Agriculture},
	volume = {93},
	pages = {68 – 77},
	doi = {10.1016/j.compag.2013.01.007},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874773345&doi=10.1016%2fj.compag.2013.01.007&partnerID=40&md5=f8c23623bc9739e8499c71d982cb26c7},
	affiliations = {Center for Artificial Intelligence and Robotics, Faculty of Electrical Engineering, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, Malaysia},
	abstract = {Classifying tropical wood species pose a considerable economic challenge and failure to classify the wood species accurately can have significant effects on timber industries. Previous works on tropical wood species recognition systems considered methods for classification of linear features of the wood species. However, tropical wood species are known to exhibit nonlinear features due to several factors such as age of the tree, samples taken from different parts of the tree, etc. to address the nonlinear features of the tropical wood species, a Kernel-Genetic Algorithm (K-GA) technique for feature selection is proposed. This method combines the Kernel Discriminant Analysis (KDA) technique with Genetic Algorithm (GA) to generate nonlinear wood features and at the same time reduce dimension of the wood database. The proposed system achieved a classification accuracy of 98.69%, showing marked improvement to the work done previously. © 2013 Elsevier B.V.},
	author_keywords = {Genetic algorithm; Nonlinear feature selection; Texture identification; Wood recognition system},
	keywords = {Algorithms; Forestry; Pattern Recognition; Texture; Tropics; Wood; Feature extraction; Forestry; Genetic algorithms; Tropics; Classification accuracy; Economic challenges; Kernel discriminant analysis; Linear feature; Nonlinear features; Timber industry; Tropical wood species; Wood features; Wood recognition; accuracy assessment; database; discriminant analysis; genetic algorithm; nonlinearity; sampling; timber industry; tropical environment; wood; Wood},
	correspondence_address = {R. Yusof; Center for Artificial Intelligence and Robotics, Faculty of Electrical Engineering, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; email: rubiyah@ic.utm.my},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 53}
}

@ARTICLE{García-Mateos2015158,
	author = {García-Mateos, G. and Hernández-Hernández, J.L. and Escarabajal-Henarejos, D. and Jaén-Terrones, S. and Molina-Martínez, J.M.},
	title = {Study and comparison of color models for automatic image analysis in irrigation management applications},
	year = {2015},
	journal = {Agricultural Water Management},
	volume = {151},
	pages = {158 – 166},
	doi = {10.1016/j.agwat.2014.08.010},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027944671&doi=10.1016%2fj.agwat.2014.08.010&partnerID=40&md5=f31194bb8ecd97a14153f6f6fbefe548},
	affiliations = {Computer Science and Systems Department, University of Murcia, Murcia, 30100, Spain; Food Engineering and Agricultural Equipment Department, Technical University of Cartagena, Cartagena, 30203, Spain; Academic Unit of Engineering, Autonomous University of Guerrero, Chilpancingo, Guerrero, Mexico},
	abstract = {Images can provide valuable information on the percentage of ground cover, which is essential in determining crop irrigation needs. Techniques based on color analysis allow classifying accurately and efficiently soil/plant regions in the images. Many color spaces have been proposed, among them: RGB, rgb, XYZ, L*. a*. b*, L*. u*. v*, HSV, HLS, YCrCb, YUV, I1I2I3 and TSL. Different possibilities to model the probability distribution of a given color class appear for each space; one of the most widespread non-parametric methods is modeling using histograms. This presents various alternatives in order to represent a color class: the number of channels, which channels to use, and the size of histograms. Using a wide and varied set of images of lettuce crops (. Lactuca sativa)-previously classified manually in soil and plant pixels-a comprehensive analysis and comparison of the proposed color models has been conducted for the soil/plant classification problem. The experimental results demonstrate the superiority of models that separate luminance from chrominance. In particular, L*. a*. b* provides the best results with a* channel, producing a 99.2% of correct classification. Further processing stages improve this performance up to 99.5% accuracy, taking less than 1/3 of a second per image in a normal laptop. These results can be applied to reduce water consumption by optimizing the accuracy and efficiency of automatic image analysis of crops. © 2014 Elsevier B.V.},
	author_keywords = {Automatic irrigation computation; Color classification; Color spaces; Image processing in agriculture},
	keywords = {Lactuca; Lactuca sativa; Simplexvirus; Color codes; Crops; Graphic methods; Image analysis; Image processing; Irrigation; Probability distributions; Soils; Automatic image analysis; Automatic irrigation; Color classification; Color space; Comprehensive analysis; Irrigation management; Nonparametric methods; Water consumption; accuracy assessment; agricultural management; agricultural technology; color; histogram; image analysis; image processing; irrigation system; leafy vegetable; pixel; Color},
	correspondence_address = {G. García-Mateos; Computer Science and Systems Department, University of Murcia, Murcia, 30100, Spain; email: ginesgm@um.es},
	publisher = {Elsevier B.V.},
	issn = {03783774},
	coden = {AWMAD},
	language = {English},
	abbrev_source_title = {Agric. Water Manage.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 74}
}

@ARTICLE{Gwo2013124,
	author = {Gwo, Chih-Ying and Wei, Chia-Hung and Li, Yue},
	title = {Rotary matching of edge features for leaf recognition},
	year = {2013},
	journal = {Computers and Electronics in Agriculture},
	volume = {91},
	pages = {124 – 134},
	doi = {10.1016/j.compag.2012.12.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872651489&doi=10.1016%2fj.compag.2012.12.005&partnerID=40&md5=5cc913b13953dee5076345841b4cf485},
	affiliations = {Department of Information Management, Chien Hsin University of Science and Technology, Taoyuan 320, 229 Chien-Hsin Road, Taiwan; College of Software, Nankai University, Nankai District, 300071, 94 Weijin Road, China},
	abstract = {With advances in cloud computing technology, handheld computers and smartphones can now perform plant recognition by taking a photograph of a plant. This study proposes novel features to describe leaf edge variation. The Bayes theorem is used to calculate the maximal matching score for rotary matching. The Viterbi training algorithm is then applied to find the model parameters of rotary matching. The experimental results show that the top one of 13-tuple reaches 94.4% and the first two can also achieve 100% in the test set. The results have verified that the proposed features are invariant to translation, rotation and size. © 2012 Elsevier B.V.},
	author_keywords = {Bayes theorem; Leaf recognition; Pattern recognition; Viterbi training algorithm},
	keywords = {Hand held computers; Pattern recognition; Bayes theorem; Computing technology; Edge features; Leaf edge; Leaf recognition; Maximal matchings; Model parameters; Plant recognition; Test sets; Training algorithms; Viterbi; agricultural technology; algorithm; Bayesian analysis; computer; experimental study; pattern recognition; photograph; Photography},
	correspondence_address = {C.-H. Wei; Department of Information Management, Chien Hsin University of Science and Technology, Taoyuan 320, 229 Chien-Hsin Road, Taiwan; email: rogerwei@uch.edu.tw},
	publisher = {Elsevier B.V.},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16}
}

@CONFERENCE{Che Hussin2013226,
	author = {Che Hussin, Nuril Aslina and Jamil, Nursuriati and Nordin, Sharifalillah and Awang, Khalil},
	title = {Plant species identification by using scale invariant feature transform (SIFT) and grid based colour moment (GBCM)},
	year = {2013},
	journal = {2013 IEEE Conference on Open Systems, ICOS 2013},
	pages = {226 – 230},
	doi = {10.1109/ICOS.2013.6735079},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897696295&doi=10.1109%2fICOS.2013.6735079&partnerID=40&md5=311cd49b631663dfb57552daa327048c},
	affiliations = {Faculty of Computer and Mathematical Sciences, Department of Computer Science, Universiti Teknologi MARA, 40450 Shah Alam, Selangor, Malaysia},
	abstract = {Plant identification using plant leaves is a very challenging task. The most important and crucial phase in plant identification is the phase of feature extraction. This paper presents a method of shape feature extraction that is Scale Invariant Feature Transform (SIFT) and colour feature extraction Grid Based Colour Moment (GBCM) to identify plant. Forty plant species images were collected from their natural habitats and captured under various time of the day. These plant images are then used as ground truth images. These images are further rotated and scaled to produce another forty test images. The extracted features of the test images are then identified by calculating their Euclidean Distance (ED) against the ground truth and achieved identification accuracy rate of 87.5 percent. The proposed feature extraction methods showed potential in identifying plant images captured under natural illumination. However, further work need to be done to improve accuracy of plant identification. © 2013 IEEE.},
	author_keywords = {Colour moment; Euclidean distance; Plant leaf identification; Scale invariant feature transform},
	keywords = {Color; Feature extraction; Plants (botany); Colour moments; Euclidean distance; Feature extraction methods; Identification accuracy; Natural illumination; Plant leaf; Plant species identification; Scale invariant feature transforms; Image processing},
	publisher = {IEEE Computer Society},
	isbn = {978-147990285-9},
	language = {English},
	abbrev_source_title = {IEEE Conf. Open Syst., ICOS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 23; Conference name: 2013 IEEE Conference on Open Systems, ICOS 2013; Conference date: 2 December 2013 through 4 December 2013; Conference code: 103067}
}

@ARTICLE{Horaisová2014177,
	author = {Horaisová, Kateřina and Kukal, Jaromír},
	title = {Could k-nn classifier be useful in tree leaves recognition},
	year = {2014},
	journal = {Archives of Control Sciences},
	volume = {24},
	number = {2},
	pages = {177 – 192},
	doi = {10.2478/acsc-2014-0011},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902486399&doi=10.2478%2facsc-2014-0011&partnerID=40&md5=aaa42b43408d6f3322180300469e45b8},
	affiliations = {Czech Technical University in Prague, Faculty of Nuclear Sciences and Physical Engineering, Prague, Trojanova 13, Czech Republic},
	abstract = {This paper presents a method for affine invariant recognition of two-dimensional binary objects based on 2D Fourier power spectrum. Such function is translation invariant and their moments of second order enable construction of affine invariant spectrum except of the rotation effect. Harmonic analysis of samples on circular paths generates Fourier coefficients whose absolute values are affine invariant descriptors. Affine invariancy is approximately saved also for large digital binary images as demonstrated in the experimental part. The proposed method is tested on artificial data set first and consequently on a large set of 2D binary digital images of tree leaves. High dimensionality of feature vectors is reduced via the kernel PCA technique with Gaussian kernel and the k-NN classifier is used for image classification. The results are summarized as k-NN classifier sensitivity after dimensionality reduction. The resulting descriptors after dimensionality reduction are able to distinguish real contours of tree leaves with acceptable classification error. The general methodology is directly applicable to any set of large binary images. All calculations were performed in the MATLAB environment.},
	author_keywords = {Affine invariance; Binary image; Fourier transform; Harmonic analysis; K-NN classifier; Pattern recognition},
	keywords = {Forestry; Image Analysis; Affine transforms; Binary trees; Forestry; Fourier analysis; Fourier transforms; Harmonic analysis; Image classification; Nearest neighbor search; Object recognition; Pattern recognition; Trees (mathematics); Binary images; Bins; Affine invariance; Affine invariant descriptors; Classification errors; Dimensionality reduction; Fourier power spectrum; General methodologies; k-NN classifier; Translation invariants; Binary images; Image classification},
	publisher = {Institute of Automatic Control - Silesian University of Technology},
	issn = {12302384},
	language = {English},
	abbrev_source_title = {Arch. Control Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Mosgaard Giselsson20135585,
	author = {Mosgaard Giselsson, Thomas and Skov Midtiby, Henrik and Nyholm Jørgensen, Rasmus},
	title = {Seedling discrimination with shape features derived from a distance transform},
	year = {2013},
	journal = {Sensors (Switzerland)},
	volume = {13},
	number = {5},
	pages = {5585 – 5602},
	doi = {10.3390/s130505585},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877652752&doi=10.3390%2fs130505585&partnerID=40&md5=e2ac0251efdcb810d4af4cea11b1ce4b},
	affiliations = {Institute of Chemical Engineering, Biotechnology and Environmental Technology, University of Southern Denmark, Campusvej 55, DK-5230 Odense, Denmark},
	abstract = {The aim of this research is an improvement of plant seedling recognition by two new approaches of shape feature generation based on plant silhouettes. Experiments show that the proposed feature sets possess value in plant recognition when compared with other feature sets. Both methods approximate a distance distribution of an object, either by resampling or by approximation of the distribution with a high degree Legendre polynomial. In the latter case, the polynomial coefficients constitute a feature set. The methods have been tested through a discrimination process where two similar plant species are to be distinguished into their respective classes. The used performance assessment is based on the classification accuracy of 4 different classifiers (a k-Nearest Neighbor, Naive-Bayes, Linear Support Vector Machine, Nonlinear Support Vector Machine). Another set of 21 well-known shape features described in the literature is used for comparison. The used data consisted of 139 samples of cornflower (Centaura cyanus L.) and 63 samples of nightshade (Solanum nigrum L.). The highest discrimination accuracy was achieved with the Legendre Polynomial feature set and amounted to 97.5%. This feature set consisted of 10 numerical values. Another feature set consisting of 21 common features achieved an accuracy of 92.5%. The results suggest that the Legendre Polynomial feature set can compete with or outperform the commonly used feature sets. © 2013 by the authors; licensee MDPI, Basel, Switzerland.},
	author_keywords = {Machine vision; Object recognition; Shape characterization},
	keywords = {Computer vision; Object recognition; Support vector machines; Classification accuracy; Discrimination accuracy; Distance distributions; Linear Support Vector Machines; Nonlinear Support Vector Machines; Performance assessment; Polynomial coefficients; Shape characterization; Polynomial approximation},
	correspondence_address = {T. Mosgaard Giselsson; Institute of Chemical Engineering, Biotechnology and Environmental Technology, University of Southern Denmark, Campusvej 55, DK-5230 Odense, Denmark; email: tgi@kbm.sdu.dk},
	issn = {14248220},
	language = {English},
	abbrev_source_title = {Sensors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Wang2014975,
	author = {Wang, Zhaobin and Sun, Xiaoguang and Ma, Yide and Zhang, Hongjuan and Ma, Yurun and Xie, Weiying and Zhang, Yaonan},
	title = {Plant recognition based on intersecting cortical model},
	year = {2014},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	pages = {975 – 980},
	doi = {10.1109/IJCNN.2014.6889656},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908468874&doi=10.1109%2fIJCNN.2014.6889656&partnerID=40&md5=1a5098612eed00cad8c5938cdb5542b4},
	affiliations = {School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Cold and Arid Regions Environmental and Engineering, Research Institute, Chinese Academy of Sciences, Lanzhou, China},
	abstract = {Plant recognition recently becomes more and more attractive in computer vision and pattern recognition. Although some researchers have proposed several methods, their accuracy is not satisfactory. Therefore, a novel method of plant recognition based on leaf image is proposed in the paper. Both shape and texture features are employed in the proposed method Texture feature is extracted by intersecting cortical model, and shape feature is obtained by the representation of center distance sequence. Support vector machine is employed for the classifier. The leaf image is preprocessed to get better quality for extracting features, and then entropy sequence and center distance sequence are obtained by intersecting cortical model and center distance transform, respectively. Redundant data of entropy sequence vector and center distance are reduced by principal component analysis. Finally, feature vector is imported into the classifier for classification. In order to evaluate the performance, several existing methods are used to compare with the proposed method and three leaf image datasets are taken as test samples. The experimental result shows the proposed method gets the better accuracy of recognition than other methods. © 2014 IEEE.},
	author_keywords = {classification; feature extraction; ICM; leaf image; plant recognition},
	keywords = {Entropy; Feature extraction; Image processing; Pattern recognition; Support vector machines; Textures; Center distance; Entropy sequences; Extracting features; Intersecting cortical models; Leaf images; Plant recognition; Shape and textures; Texture features; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147991484-5},
	coden = {85OFA},
	language = {English},
	abbrev_source_title = {Proc Int Jt Conf Neural Networks},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28; Conference name: 2014 International Joint Conference on Neural Networks, IJCNN 2014; Conference date: 6 July 2014 through 11 July 2014; Conference code: 108721}
}

@CONFERENCE{Tan2014,
	author = {Tan, Wooi-Nee and Sem, Racheal and Tan, Yi-Fei},
	title = {Blooming flower recognition by using eigenvalues of shape features},
	year = {2014},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {9159},
	doi = {10.1117/12.2064504},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902286061&doi=10.1117%2f12.2064504&partnerID=40&md5=3647fb08947f75b5188ec1e684162fd6},
	affiliations = {Faculty of Engineering, Multimedia University, 63100 Cyberjaya, Selangor, Jalan Multimedia, Malaysia},
	abstract = {This paper introduces the concept of eigenvalues in describing the shape features of blooming flowers, and implements the idea in recognizing the blooming flowers automatically. The input images of blooming flowers are taken from natural scene in the form of RGB images. The proposed method first segments and crops the targeted flower object, then calculates four shape features to form a 2 × 2 matrix. Eigenvalues of the matrix computed from the testing set are then used to compare with those eigenvalues of the reference set. The advantage of utilizing the idea of eigenvalues is that the dimension of parameters used in comparison can be reduced. Based on the experimental result on database which consists of 5 types of flowers with a total of 46 images, a recognition rate of 80.43% is achieved. © 2014 Copyright SPIE.},
	author_keywords = {eigenvalues; Flower recognition; shape features},
	keywords = {Image processing; Eigenvalues; Flower recognition; Input image; Natural scenes; Reference set; RGB images; Shape features; Eigenvalues and eigenfunctions},
	publisher = {SPIE},
	issn = {0277786X},
	isbn = {978-162841186-7},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 6th International Conference on Digital Image Processing, ICDIP 2014; Conference date: 5 April 2014 through 6 April 2014; Conference code: 105517}
}

@ARTICLE{Xu2015155,
	author = {Xu, Gong-Sheng and Yuan, Jing-Hua and Zhang, Xiao-Ping and Shang, Li and Huang, Zhi-Kai and Zhu, Hao-Dong and Gan, Yong},
	title = {Implementation of plant leaf recognition system on ARM tablet based on local ternary pattern},
	year = {2015},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {9226},
	pages = {155 – 164},
	doi = {10.1007/978-3-319-22186-1_15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944745974&doi=10.1007%2f978-3-319-22186-1_15&partnerID=40&md5=8f9dc828e9515d7553cd09c9431b1e0d},
	affiliations = {College of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Communication Technology, College of Electronic Information Engineering, Suzhou Vocational University, Suzhou, Jiangsu, 215104, China; College of Mechanical and Electrical Engineering, Nanchang Institute of Technology, Nanchang, Jiangxi, 330099, China; College of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zhengzhou, China},
	abstract = {The Local Binary Pattern (LBP) and its variants is powerful in capturing image features and computational simplicity, However LBP’s sensitivity to noise, particularly in near-uniform image regions has stimulated many transformations of LBP to improve the ability of feature description. The Local Ternary Pattern (LTP) extends the conventional LBP to ternary codes and makes a significant improvement. LTP is more resistant to noise, but no longer strictly invariant to gray-level transformations. In this paper, by adopting the Average Local Gray Level (ALG) to take place of the traditional gray value of the center pixel and taking an auto-adaptive strategy on the selection of the threshold, we propose the Enhanced Local Ternary Pattern (ELTP) to improve the performance of LTP and implement an android application to recognize plant-leaf image and identify the species of the plant. © Springer International Publishing Switzerland 2015.},
	author_keywords = {Android application; ELTP; LBP; LTP; Texture classification},
	keywords = {Android (operating system); Binary images; Computation theory; Computer control; Intelligent computing; Android applications; ELTP; LBP; LTP; Texture classification; Pattern recognition systems},
	editor = {Hussain A. and Huang D.-S. and Jo K.-H.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331922185-4},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 11th International Conference on Intelligent Computing, ICIC 2015; Conference date: 20 August 2015 through 23 August 2015; Conference code: 139689}
}

@CONFERENCE{Choi2015,
	author = {Choi, Sungbin},
	title = {Plant identification with deep convolutional neural network: SNUMedinfo at LifeCLEF plant identification task 2015},
	year = {2015},
	journal = {CEUR Workshop Proceedings},
	volume = {1391},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982844984&partnerID=40&md5=d6d29d2c9273e09d0bc75b8b862be7be},
	affiliations = {Department of Biomedical Engineering, Seoul National University, South Korea},
	abstract = {This paper describes our participation at the LifeCLEF Plant identification task 2015. Given various images of plant parts such as leaf, flower or stem, this task is about identification of plant species given multi-image observation query. We utilized GoogLeNet for individual image classification, and combined image classification results for plant identification per observation. Our approach achieved best performance in this task.},
	author_keywords = {Borda-fuse; Deep convolutional neural network; Goog-LeNet; Image classification},
	keywords = {Convolution; Image processing; Neural networks; Convolutional neural network; Goog-LeNet; Multi-images; Plant identification; Plant species; Image classification},
	correspondence_address = {S. Choi; Department of Biomedical Engineering, Seoul National University, South Korea; email: wakeup06@empas.com},
	editor = {Jones G.J.F. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and San Juan E.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 16th Conference and Labs of the Evaluation Forum, CLEF 2015; Conference date: 8 September 2015 through 11 September 2015; Conference code: 122644}
}

@ARTICLE{Wang2013192,
	author = {Wang, Hang-jun and Qi, Heng-nian and Wang, Xiao-Feng},
	title = {A new Gabor based approach for wood recognition},
	year = {2013},
	journal = {Neurocomputing},
	volume = {116},
	pages = {192 – 200},
	doi = {10.1016/j.neucom.2012.02.045},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878518801&doi=10.1016%2fj.neucom.2012.02.045&partnerID=40&md5=50d32b13cb09f4f8b657e11fa8e68d68},
	affiliations = {Hefei Institute of Intelligent Machines, Chinese Academy of Science, Hefei 230031, China; Department of Automation, University of Science and Technology of China, Hefei 230027, China; School of Information Engineering, Zhejiang A and F University, Lin'an 311300, China; Key Lab of Network and Intelligent Information Processing, Department of Computer Science and Technology, Hefei University, Hefei 230601, China},
	abstract = {Correct wood recognition has an important meaning in the rational use of wood resources. To complete this task automatically, based on wood stereogram images, we propose a new Gabor based wood recognition approach in this paper, which has been successfully applied in many pattern recognition fields for its robustness against local distortions. However, only a few approaches can make full use of the information in Gabor patterns. To obtain more information of Gabor feature for wood recognition, we first use a set of 40 Gabor patterns to represent a wood image, which consist of important information at different orientation and scales. Then, we apply the block-based feature extraction with more statistical features besides mean and standard deviation on these Gabor patterns to enhance the discriminative ability of our approach. Finally, we reduce the dimensionality of the proposed feature descriptor by using feature selection. Only a few features are selected to achieve both high recognition performance and computational efficiency. We evaluate our approach on the wood database in Zhejiang A & F University (ZAFU), which contains 24 wood species. Experimental results show that by adopting proper sub-block numbers and blocking schemes, our approach outperforms the most of state-of-the-art approaches. © 2012 Elsevier B.V.},
	author_keywords = {Feature selection; Gabor patterns; Texture analysis; Wood recognition; Wood stereogram images},
	keywords = {Image Analysis; Pattern Recognition; Texture; Wood; Feature extraction; Discriminative ability; Gabor patterns; Mean and standard deviations; Recognition performance; State-of-the-art approach; Texture analysis; Wood recognition; Wood stereograms; article; automated pattern recognition; classification algorithm; contrast; controlled study; Gabor pattern; image analysis; image processing; image quality; machine learning; mathematical computing; priority journal; stereoradiography; wood; Wood},
	correspondence_address = {H.-N. Qi; School of Information Engineering, Zhejiang A and F University, Lin'an 311300, China; email: qihengnian@yahoo.com.cn},
	issn = {18728286},
	coden = {NRCGE},
	language = {English},
	abbrev_source_title = {Neurocomputing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18}
}

@ARTICLE{Mattila2013621,
	author = {Mattila, Heta and Valli, Pertti and Pahikkala, Tapio and Teuhola, Jukka and Nevalainen, Olli S. and Tyystjärvi, Esa},
	title = {Comparison of chlorophyll fluorescence curves and texture analysis for automatic plant identification},
	year = {2013},
	journal = {Precision Agriculture},
	volume = {14},
	number = {6},
	pages = {621 – 636},
	doi = {10.1007/s11119-013-9320-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886721095&doi=10.1007%2fs11119-013-9320-y&partnerID=40&md5=49817cc38199d23315e724dc023ad671},
	affiliations = {Molecular Plant Biology, Department of Biochemistry and Food Chemistry, University of Turku, 20014 Turku, Finland; Department of Information Technology, University of Turku, 20014 Turku, Finland},
	abstract = {With automatic plant identification methods, the amount of herbicides used in agriculture can be reduced when herbicides are sprayed only on weeds. In the present study, leaves of oat (Avena sativa) and dandelion (Taraxacum officinale, TAROF) were arranged so that there was overlap between the species, imaged with a pulse amplitude modulation fluorescence camera and photographed with a digital color camera. The fluorescence induction curves from each pixel were parameterized to obtain a set of features and from color photographs, texture features were calculated. A support vector algorithm that also performed feature selection was used for pattern recognition of both data sets. Fluorescence-based identification worked well with oat leaves, producing 92.2 % of correctly identified pixels, whereas the texture-based method often mis-identified the central vein of a TAROF leaf as oat, identifying correctly only 66.5 % of oat pixels. With TAROF that shows a clear dicot-type texture, the texture method was slightly better (96.4 % correctly identified pixels) than the fluorescence method (94.6 %). In fluorescence-based identification, the accuracy varied between entire TAROF leaves, probably reflecting the genetic variability of TAROF. The results suggest that the accuracy of identification could be improved by combining two identification methods. © 2013 Springer Science+Business Media New York.},
	author_keywords = {Automatic plant identification; Chlorophyll a fluorescence; Fluorescence fingerprinting; Leaf texture; Support vector machine},
	keywords = {Avena sativa; Dicotyledoneae; Taraxacum officinale; chlorophyll; data set; fluorescence; herbicide; leaf morphology; pattern recognition; pixel; plant; soil texture},
	correspondence_address = {E. Tyystjärvi; Molecular Plant Biology, Department of Biochemistry and Food Chemistry, University of Turku, 20014 Turku, Finland; email: esatyy@utu.fi},
	issn = {13852256},
	coden = {PREAF},
	language = {English},
	abbrev_source_title = {Precis. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@CONFERENCE{Chathura Priyankara2015148,
	author = {Chathura Priyankara, H.A. and Withanage, D.K.},
	title = {Computer assisted plant identification system for Android},
	year = {2015},
	journal = {MERCon 2015 - Moratuwa Engineering Research Conference},
	pages = {148 – 153},
	doi = {10.1109/MERCon.2015.7112336},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936762933&doi=10.1109%2fMERCon.2015.7112336&partnerID=40&md5=bd14666bcfb62b24bfa9f45beea38c58},
	affiliations = {Faculty of Information Technology, University of Moratuwa, Sri Lanka},
	abstract = {Plant leaves provide sufficient features to distinguish them among other species. Identification of plants using leaf images is a classic problem in digital image processing. Usually those image processing systems use shape based digital morphological features for leaf identification task. Even there are number of studies on leaf based plant identification, very few of them are for mobiles. In this paper we describe a leaf image based plant identification system using SIFT features combining with Bag Of Word (BOW) model and Support Vector Machine (SVM) classifier. The system is trained to classify 20 species and obtained 96.48 % accuracy level. Based on the results, we developed an Android application communicates with the server and gives users the ability to identify plant species using photographs taken of plant leaves using the smart phone. © 2015 IEEE.},
	author_keywords = {image processing; leaf features; plant identification},
	keywords = {Android (operating system); Engineering research; Plants (botany); Smartphones; Support vector machines; Android applications; Computer assisted; Image processing system; leaf features; Leaf identification; Morphological features; Plant identification; Plant identification systems; Image processing},
	correspondence_address = {H.A. Chathura Priyankara; Faculty of Information Technology, University of Moratuwa, Sri Lanka; email: priyankarahac@gmail.com},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147991740-2},
	language = {English},
	abbrev_source_title = {MERCon - Moratuwa Eng. Res. Conf.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20; Conference name: 1st Moratuwa Engineering Research Conference, MERCon 2015; Conference date: 7 April 2015 through 8 April 2015; Conference code: 112531}
}

@ARTICLE{Qiao20147,
	author = {Qiao, Junfei and Zhang, Zhaozhao and Bo, Yingchun},
	title = {An online self-adaptive modular neural network for time-varying systems},
	year = {2014},
	journal = {Neurocomputing},
	volume = {125},
	pages = {7 – 16},
	doi = {10.1016/j.neucom.2012.09.038},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888041633&doi=10.1016%2fj.neucom.2012.09.038&partnerID=40&md5=8bc929b45fe294b1551a571a68401302},
	affiliations = {College of Electronic and Control Engineering, Beijing University of University of Technology, Beijing, 100124, China; Institute of Electronic and Information Engineering, LiaoNing Technical University, Liaoning, 125105, China; College of information and Control Engineering, China University of Petroleum, Shandong, 266555, China},
	abstract = {We propose an online self-adaptive modular neural network (OSAMNN) for time-varying systems. Starting with zero subnetworks, OSAMNN uses a single-pass subtractive cluster algorithm to update the centers of radial-basis function (RBF) neurons for learning. Then the input space can be partitioned. The OSAMNN structure is capable of growing or merging subnetworks to maintain suitable model complexity, and the centers of RBF neurons can also be dynamically adjusted according to changes in the data environment. A fuzzy strategy is applied to select suitable subnetworks to learn the current sample. This method yields improved learning efficiency and accuracy. OSAMNN can adapt its architecture to realize online modeling of time-varying nonlinear input-output maps. Results for experiments on benchmark and real-world time-varying systems support the proposed techniques. © 2013.},
	author_keywords = {Fuzzy strategy; Modular neural network; Online; Self-adaptive; Time-varying system},
	keywords = {Radial basis function networks; Time varying systems; Cluster algorithms; Data environment; Fuzzy strategies; Learning efficiency; Modular neural networks; Online; Radial basis functions; Self-adaptive; accuracy; algorithm; article; artificial neural network; cluster analysis; fuzzy system; information service; machine learning; mathematical analysis; mathematical computing; mathematical model; online self adaptive modular neural network; online system; plant identification; prediction; priority journal; radial based function; waste water management; Neural networks},
	correspondence_address = {J. Qiao; College of Electronic and Control Engineering, Beijing University of University of Technology, Beijing, 100124, China; email: isibox@sina.com},
	issn = {18728286},
	coden = {NRCGE},
	language = {English},
	abbrev_source_title = {Neurocomputing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 43}
}

@ARTICLE{Yusof20131589,
	author = {Yusof, Rubiyah and Khalid, Marzuki and Mohd Khairuddin, Anis Salwa},
	title = {Fuzzy logic-based pre-classifier for tropical wood species recognition system},
	year = {2013},
	journal = {Machine Vision and Applications},
	volume = {24},
	number = {8},
	pages = {1589 – 1604},
	doi = {10.1007/s00138-013-0526-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886790583&doi=10.1007%2fs00138-013-0526-9&partnerID=40&md5=e10193dce08f4e89f1702cef97df6a06},
	affiliations = {Center for Artificial Intelligence and Robotics, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, Malaysia},
	abstract = {Classifying tropical wood species poses a considerable economic challenge and failure to classify the wood species accurately can have significant effects on timber industries. The problem of wood recognition is compounded with the nonlinearities of the features among the similar wood species. Besides that, large wood databases presented a problem of large processing time especially for online wood recognition system. In view of these problems, we propose the use of fuzzy logic-based pre-classifier as a means of treating uncertainty to improve the classification accuracy of tropical wood recognition system. The pre-classifier serve as a clustering mechanism for the large database simplifying the classification process making it more efficient. The use of the fuzzy logic-based pre-classifier has managed to increase the accuracy of the wood recognition system by 4 % and reduce the processing time for training and testing by more than 75 % and 26 % respectively. © 2013 Springer-Verlag Berlin Heidelberg.},
	author_keywords = {Fuzzy logic; Pattern recognition; Texture; Wood pores; Wood species recognition system},
	keywords = {Classification; Fuzzy Logic; Pattern Recognition; Species Identification; Texture; Tropics; Wood; Classification (of information); Fuzzy logic; Pattern recognition; Textures; Tropics; Classification accuracy; Classification process; Clustering mechanism; Economic challenges; Species recognition systems; Training and testing; Tropical wood species; Wood pores; Wood},
	correspondence_address = {R. Yusof; Center for Artificial Intelligence and Robotics, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; email: rubiyah@ic.utm.my},
	issn = {14321769},
	coden = {MVAPE},
	language = {English},
	abbrev_source_title = {Mach Vision Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24}
}

@ARTICLE{Lv2014927,
	author = {Lv, Feng-hua and Wang, Hang-jun},
	title = {Graph cuts-based feature extraction of plant leaf},
	year = {2014},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {279},
	pages = {927 – 935},
	doi = {10.1007/978-3-642-54927-4_89},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921840360&doi=10.1007%2f978-3-642-54927-4_89&partnerID=40&md5=3f5070a74318b0ae4080ac928b672b9d},
	affiliations = {Jinhua Polytechnic, Jinhua, 321007, China; Tianmu College, Zhejiang A&F University, Lin’an, 311300, China},
	abstract = {As leaf is one of the most important organs in a plant, contour features of plant leaves are important for the identification of plant species. So researchers have proposed many methods to improve the progress of the plant identification. In this paper, we present a graph cuts-based method using Min-Cut/Max Flow algorithm to obtain the leaf blade section. Then, five basic features are computed to further obtain six digital morphological features. These experimental results show that the graph cuts algorithm and the presented leaf features are important for leaf recognition. © Springer-Verlag Berlin Heidelberg 2014.},
	author_keywords = {Feature extraction; Graph cuts; Plant leaf; Statistical features},
	keywords = {Extraction; Feature extraction; Flow graphs; Image segmentation; Intelligent systems; Knowledge engineering; Plants (botany); Contour features; Graph cut; Leaf recognition; Min cut/max flows; Morphological features; Plant identification; Plant leaf; Statistical features; Graphic methods},
	correspondence_address = {F.-H. Lv; Jinhua Polytechnic, Jinhua, 321007, China; email: 213837053@qq.com},
	editor = {Wen Z. and Li T.},
	publisher = {Springer Verlag},
	issn = {21945357},
	isbn = {978-364254926-7},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 8th International Conference on Intelligent Systems and Knowledge Engineering, ISKE 2013; Conference date: 20 November 2013 through 23 November 2013; Conference code: 112849}
}

@ARTICLE{Ma2013106,
	author = {Ma, Lin-Hai and Zhao, Zhong-Qiu and Wang, Jing},
	title = {ApLeafis: An Android-based plant leaf identification system},
	year = {2013},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {7995 LNCS},
	pages = {106 – 111},
	doi = {10.1007/978-3-642-39479-9_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882773791&doi=10.1007%2f978-3-642-39479-9_13&partnerID=40&md5=41ea2575c4e706b62beda25feaac020b},
	affiliations = {College of Computer Science and Information Engineering, Hefei University of Technology, Hefei 230009, China; Department of Computer Science, Hong Kong Baptist University, Hong Kong, Hong Kong},
	abstract = {To automatically identify plant species is very useful for ecologists, amateur botanists, educators, and so on. In this paper, an Android-based mobile application designed to automatically identify plant species by the photographs of tree leaves is described. In this application, one leaf image can be either a digital image from the existing leaf image database or a picture collected by a camera. The picture should be a single leaf placed on a light and untextured background without other clutter. The identification process consists of totally three steps: leaf image segmentation, feature extraction, and species identification. The demo system is evaluated on the ImageCLEF2012 Plant Identification database which contains 126 tree species from the French Mediterranean area. The output of the system to users is the top several species which match the query leaf image the best, as well as the textual descriptions and additional images about leaves, flowers, etc., of theirs. Our system works well with state-of-the-art identification performance. © 2013 Springer-Verlag.},
	keywords = {Forestry; Mathematics; Plants; Robots; Trees; Computer applications; Feature extraction; Forestry; Intelligent computing; Plants (botany); Query processing; Robots; Trees (mathematics); Digital image; Identification process; Mediterranean areas; Mobile applications; Plant identification; Plant species; Species identification; Textual description; Search engines},
	issn = {16113349},
	isbn = {978-364239478-2},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; Conference name: 9th International Conference on Intelligent Computing, ICIC 2013; Conference date: 28 July 2013 through 31 July 2013; Conference code: 98860}
}

@ARTICLE{Brilhador2015219,
	author = {Brilhador, Anderson and Serrarens, Daniel A. and Lopes, Fabrício M.},
	title = {A computer vision approach for automatic measurement of the inter-plant spacing},
	year = {2015},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {9423},
	pages = {219 – 227},
	doi = {10.1007/978-3-319-25751-8_27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983666685&doi=10.1007%2f978-3-319-25751-8_27&partnerID=40&md5=54b35ce99195c088c0c9dd64c53d68f0},
	affiliations = {Federal University of Technology - Paraná, Cornélio Procópio, PR, Brazil; Belagrícola Comércio e Representaçlão de Produtos Agríolas Ltda - Paraná, Tamarama, Brazil},
	abstract = {Global food demand is increasing every year and it is needed to respond to this demand. In addition, some crops such as corn, which is the most produced grain in the world, is used as food, feed, bio-energy and other industrial purposes. Thus, it is needed the development of new technologies that make possible to produce more from less land. In particular, the corn crop is sensitive to its spatial arrangement and any variation in plant distribution pattern can lead to reduction in corn production. Nowadays, the uniformity of the plant spacing is checked manually by agronomists in order to predict possible production losses. In this context, this work proposes an automatic approach for measuring the spacing between corn plants in the early stages of growth. The proposed approach is based on computer vision techniques in order to evaluate the automatic inter-plant spacing measurement from images in a simple and efficient way, allowing its use on devices with low computational power such as smart phones and tablets. An image dataset was built as an additional contribution of this work containing 2186 corn plants in two conditions: tillage after the application of herbicide (TH) with 1387 corn plants and conventional tillage (CT) with 799 corn plants. The dataset is available at url: http://github.com/Brilhador/cornspacing. The experimental results achieve 90% of precision and 92% of sensitivity in corn plant identification. Regarding the automatic measurement of the interplant spacing, the results showed no significant differences from the same measurements taken manually, indicating the effectiveness of the proposed approach in two distinct types of planting. © Springer International Publishing Switzerland 2015.},
	author_keywords = {Computer vision; Image processing; Inter-plant spacing; Pattern recognition; Precision agriculture},
	keywords = {Agricultural machinery; Computer vision; Computerized tomography; Crops; Image processing; Pattern recognition; Plants (botany); Smartphones; Automatic approaches; Automatic measurements; Computer vision techniques; Conventional tillage; Plant distribution patterns; Plant spacing; Precision Agriculture; Spatial arrangements; Agriculture},
	correspondence_address = {F.M. Lopes; Federal University of Technology - Paraná, Cornélio Procópio, Brazil; email: fabricio@utfpr.edu.br},
	editor = {Pardo A. and Kittler J.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331925750-1},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 20th Iberoamerican Congress on on Pattern Recognition, CIARP 2015; Conference date: 9 November 2015 through 12 November 2015; Conference code: 127398; All Open Access, Bronze Open Access}
}

@ARTICLE{Caputo2013250,
	author = {Caputo, Barbara and Muller, Henning and Thomee, Bart and Villegas, Mauricio and Paredes, Roberto and Zellhofer, David and Goeau, Herve and Joly, Alexis and Bonnet, Pierre and Martinez Gomez, Jesus and Varea, Ismael Garcia and Cazorla, Miguel},
	title = {ImageCLEF 2013: The vision, the data and the open challenges},
	year = {2013},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {8138 LNCS},
	pages = {250 – 268},
	doi = {10.1007/978-3-642-40802-1_26},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886441880&doi=10.1007%2f978-3-642-40802-1_26&partnerID=40&md5=0d77929690113c30332f33840dc42332},
	affiliations = {Idiap Research Institue, Martigny, Switzerland; University of Applied Sciences Western Switzerland in Sierre, Switzerland; Yahoo Research, Barcelona, Spain; ITI/DSIC, Universitat Politècnica de València, Spain; Brandenburg University of Technology, Germany; INRIA-IMEDIA, Paris, France; INRIA-ZENITH, Montpellier, France; CIRAD, UMR AMAP, Montpellier, France; University of Castilla-La Mancha, Albacete, Spain; University of Alicante, Alicante, Spain},
	abstract = {This paper presents an overview of the ImageCLEF 2013 lab. Since its first edition in 2003, ImageCLEF has become one of the key initiatives promoting the benchmark evaluation of algorithms for the cross-language annotation and retrieval of images in various domains, such as public and personal images, to data acquired by mobile robot platforms and botanic collections. Over the years, by providing new data collections and challenging tasks to the community of interest, the ImageCLEF lab has achieved an unique position in the multi lingual image annotation and retrieval research landscape. The 2013 edition consisted of three tasks: the photo annotation and retrieval task, the plant identification task and the robot vision task. Furthermore, the medical annotation task, that traditionally has been under the ImageCLEF umbrella and that this year celebrates its tenth anniversary, has been organized in conjunction with AMIA for the first time. The paper describes the tasks and the 2013 competition, giving an unifying perspective of the present activities of the lab while discussion the future challenges and opportunities. © 2013 Springer-Verlag.},
	keywords = {Image retrieval; Plants (botany); Benchmark evaluation; Community of interest; Future challenges; Image annotation; Medical annotation; Mobile robot platforms; Photo annotations; Plant identification; Computer vision},
	issn = {16113349},
	isbn = {978-364240801-4},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 34; Conference name: 4th International Conference of the CLEF Initiative, CLEF 2013; Conference date: 23 September 2013 through 26 September 2013; Conference code: 100368; All Open Access, Green Open Access}
}

@CONFERENCE{Yusof2013529,
	author = {Yusof, Rubiyah and Khalid, Marzuki and Khairuddin, Anis Salwa Mohd},
	title = {Fuzzy data management on pores arrangement for tropical wood species recognition system},
	year = {2013},
	journal = {Proceedings of 2013 Science and Information Conference, SAI 2013},
	pages = {529 – 535},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892542530&partnerID=40&md5=7d89e302b943da01c2326a96f988637f},
	affiliations = {Center for Artificial Intelligence and Robotics, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Malaysia Japan International Institute of Technology, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, Malaysia},
	abstract = {Ahuman-decision basedclassification wood recognition system is designed to classify 52 tropical wood species. The system is designed based on visual inspection of the wood anatomy textures, which can actually be presented as image data processing using statistical parameters representing the texture and the grey values. There are thousands of wood images being processed in the wood database. In order to overcome the large processing time needed to process the large wood database and the nonlinearity problems of the wood texture, an efficient fuzzy data management technique is proposed. The fuzzy data management technique is implemented based on the size and quantity of pores on each wood image which mimics the human interpretation on wood features. Finally, a multilayer feedforward neural network is used to classify the wood species. This paper involves comparison of the system's performance with and without the implementation of fuzzy data management. The results show that the inclusion of the fuzzy data management has improved the classification accuracy by approximately 4.0% and reduce the processing time for training and testing. © 2013 The Science and Information Organization.},
	author_keywords = {fuzzy logic; nonlinear features; pattern recognition; texture analysis; wood pores},
	keywords = {Classification; Data Bases; Fuzzy Logic; Problem Solving; Tropical Atmospheres; Wood; Data processing; Feedforward neural networks; Fuzzy logic; Industrial management; Information management; Pattern recognition; Textures; Classification accuracy; Image data processing; Multilayer feedforward neural networks; Nonlinear features; Statistical parameters; Texture analysis; Tropical wood species; Wood pores; Wood},
	isbn = {978-098931930-0},
	language = {English},
	abbrev_source_title = {Proc. Sci. Inf. Conf., SAI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2013 Science and Information Conference, SAI 2013; Conference date: 7 October 2013 through 9 October 2013; Conference code: 101986}
}

@CONFERENCE{Wang20134417,
	author = {Wang, Bin and Brown, Douglas and Gao, Yongsheng and La Salle, John},
	title = {Mobile plant leaf identification using smart-phones},
	year = {2013},
	journal = {2013 IEEE International Conference on Image Processing, ICIP 2013 - Proceedings},
	pages = {4417 – 4421},
	doi = {10.1109/ICIP.2013.6738910},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897748572&doi=10.1109%2fICIP.2013.6738910&partnerID=40&md5=fba115001763258e1d0874af6fc1206f},
	affiliations = {Key Laboratory of Electronic Business, Nanjing University of Finance and Economics, Nanjing 210096, China; School of Engineering, Griffith University, QLD 4111, Australia; Atlas of Living Australia, CSIRO Ecosystem Sciences, Canberra, ACT 2601, Australia},
	abstract = {A novel shape description method is proposed for mobile retrieval of leaf images to aid in plant recognition. In this method, traveling the shape contour, the convexity and concavity properties of the arches of various levels are measured, respectively, to generate a multiscale shape descriptor. Its performance has been tested on two leaf datasets and the experimental results indicated higher recognition accuracies than the state-of-the-art approaches with a speed improvement of more than 170 times. The proposed method has been successfully applied to develop a prototype system of online plant leaf identification working on a consumer mobile platform. © 2013 IEEE.},
	author_keywords = {leaf image retrieval; mobile leaf identification; plant identification; shape description},
	keywords = {Image retrieval; Smartphones; Leaf identification; Plant identification; Plant recognition; Recognition accuracy; Shape description; Shape descriptors; Speed improvement; State-of-the-art approach; Image processing},
	publisher = {IEEE Computer Society},
	isbn = {978-147992341-0},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Image Process., ICIP - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; Conference name: 2013 20th IEEE International Conference on Image Processing, ICIP 2013; Conference date: 15 September 2013 through 18 September 2013; Conference code: 115163}
}

@CONFERENCE{Lavania2015,
	author = {Lavania, Shubham and Matey, Palash Sushil},
	title = {Leaf recognition using contour based edge detection and SIFT algorithm},
	year = {2015},
	journal = {2014 IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2014},
	doi = {10.1109/ICCIC.2014.7238345},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944393516&doi=10.1109%2fICCIC.2014.7238345&partnerID=40&md5=4fff7ce7657c7d6782a2f8c22de7fe3b},
	affiliations = {School of Electronic Engineering, VIT University, Vellore, India},
	abstract = {The paper presents two advanced methods for comparative study in the field of computer vision. The first method involves the implementation of the Scalar Invariant Fourier Transform (SIFT) algorithm for the leaf recognition based on the key descriptors value. The second method involves the contour-based corner detection and classification which is done with the help of Mean Projection algorithm. The advantage of this system over the other Curvature Scale Space (CSS) systems is that there are fewer false-positive (FP) and false-negative (FN) points compared with recent standard corner detection techniques. The performance analysis of both the algorithm was done on the flavia database. © 2014 IEEE.},
	author_keywords = {contour-based corner detector; corner detection; leaf recognition; mean projection transform; SIFT},
	keywords = {Algorithms; Artificial intelligence; Computer vision; Corner detection; Corner detector; Leaf recognition; Projection transform; SIFT; Edge detection},
	editor = {Krishnan N. and Karthikeyan M.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147993972-5},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Comput. Intell. Comput. Res., IEEE ICCIC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 23; Conference name: 5th IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2014; Conference date: 18 December 2014 through 20 December 2014; Conference code: 114451}
}

@ARTICLE{Schur2015696,
	author = {Schur, Amir and Tappert, Charles C.},
	title = {Employing mobile applications in human-machine interaction in visual pattern recognition research},
	year = {2015},
	journal = {Communications in Computer and Information Science},
	volume = {528},
	pages = {696 – 699},
	doi = {10.1007/978-3-319-21380-4_117},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951844949&doi=10.1007%2f978-3-319-21380-4_117&partnerID=40&md5=40174fae536bcf1e3818600ecf279654},
	affiliations = {Seidenberg School of CSIS, Pace University, 1 Martine Ave., White Plains, 10606, NY, United States},
	abstract = {This study is part of the first author’s continued dissertation research in human-machine interaction in visual pattern recognition. Previous research focused on evaluating human-machine interaction using a flower recognition tool. Initial research showed that human interaction in color recognition improved accuracy significantly. We then looked more deeply into various automated color recognition algorithms and ways of combining them with human feedback. Described here is the process of upgrading the initial system into a new mobile application using Appinventor. After data collection, models were built for various color spaces. Sharing this experience may help other researchers incorporating a human-computer interaction component into their work. © Springer International Publishing Switzerland 2015.},
	author_keywords = {Appinventor; Color space; Feature extraction; Human-computer interaction; Pattern classification; Visual object recognition},
	keywords = {Color; Feature extraction; Man machine systems; Mobile computing; Mobile telecommunication systems; Object recognition; Pattern recognition; Appinventor; Color space; Flower recognition; Human interactions; Human machine interaction; Mobile applications; Visual object recognition; Visual pattern recognition; Human computer interaction},
	correspondence_address = {A. Schur; Seidenberg School of CSIS, Pace University, White Plains, 1 Martine Ave., 10606, United States; email: amirschur@aol.com},
	editor = {Stephanidis C.},
	publisher = {Springer Verlag},
	issn = {18650929},
	isbn = {978-331921379-8},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 17th International Conference on Human Computer Interaction, HCI 2015; Conference date: 2 August 2015 through 7 August 2015; Conference code: 141169; All Open Access, Bronze Open Access}
}

@ARTICLE{Ozyavuz2015264,
	author = {Ozyavuz, M. and Bilgili, B.C. and Salici, A.},
	title = {Determination of vegetation changes with NDVI method},
	year = {2015},
	journal = {Journal of Environmental Protection and Ecology},
	volume = {16},
	number = {1},
	pages = {264 – 273},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946738219&partnerID=40&md5=51bfb722e239bd2f39a7bf4e23cd039c},
	affiliations = {Department of Landscape Architecture, Faculty of Fine Arts, Design and Architecture, Namik Kemal University, Tekirdag, 59 030, Turkey; Department of Landscape Architecture, Faculty of Forestry, Cankiri Karatekin Universtiy, Cankiri, 18 200, Turkey; Department of Landscape Architecture, Faculty of Architecture, Mustafa Kemal University, Hatay, 31 040, Turkey},
	abstract = {Remote sensing and geographical information system methods are used frequently in field use, planning and determination of land changes. The data types used in this study vary according to the objective of the study. In this study, the Landsat TM satellite patterns of 1987, 2002 and 2012 years were classified according to NDVI and the changes in the vegetation intensity of the study area were determined. First, rectification of satellite images was carried out after which radiometric correction was made to decrease the negative atmospheric effects. Afterwards, NDVI classification was made which is the most commonly used method for plant classification and separate data was acquired for each image. Accuracy analysis was carried out in order to evaluate the reliability of the acquired data. Within the scope of the study, NDVI classification of Landsat satellite images from the years 1987, 2002 and 2012 was carried out. The highest NDVI values were determined in the year 2002 at the end of the classification. As a result of the accuracy analysis, 78% and kappa index of 0.6903 were determined for the year 1987, 76% and kappa index of 0.6731 were determined for the year 2002 and 80% and kappa index of 0.7085 were determined for the year 2012.},
	author_keywords = {Geographic information system; NDVI; Remote sensing; Vegetation},
	keywords = {accuracy assessment; GIS; image classification; Landsat thematic mapper; NDVI; pattern recognition; remote sensing; satellite imagery; vegetation},
	correspondence_address = {M. Ozyavuz; Department of Landscape Architecture, Faculty of Fine Arts, Design and Architecture, Namik Kemal University, Tekirdag, 59 030, Turkey; email: mozyavuz@gmail.com},
	publisher = {Scibulcom Ltd.},
	issn = {13115065},
	language = {English},
	abbrev_source_title = {J. Environ. Prot. Ecol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 35}
}

@ARTICLE{Cheng2014416,
	author = {Cheng, Keyang and Tan, Xiaoyang},
	title = {Sparse representations based attribute learning for flower classification},
	year = {2014},
	journal = {Neurocomputing},
	volume = {145},
	pages = {416 – 426},
	doi = {10.1016/j.neucom.2014.05.011},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906939062&doi=10.1016%2fj.neucom.2014.05.011&partnerID=40&md5=0fb0512a630f89424ed6bb69c2878811},
	affiliations = {School of Information Science and oTechnology, Nanjing University of aeronautics and astronautics, Nanjing, Jiangsu, 210016, China; School of Computer Science and oTelecommunications Engineering, Jiangsu University, Zhenjiang, Jiangsu 212013, China},
	abstract = {Classification for flowers is a very difficult task. Traditional methods need to built a classifier for each flower category, and obtain large number of flower samples to train these classifiers. In practice, many different types of flowers make the job become very difficult and boring. In this work, we present an attribute based approach for flowers recognition. Particularly, instead of training for a specific category of flowers directly based on manually designed features such as SIFT and HoG, we extract a series of visual attributes from a given set of flower images and generalize these to new images with possibly unknown flowers. A recently proposed sparse representations classification scheme is employed to predict the attributes of a given flower image from any category. In addition, we use a genetic algorithm to find the most discriminative attributes among others for better performance during the stage of flower classification. The effectiveness of the proposed method is validated on a publicly available flower classification database with promising results. © 2014 Elsevier B.V.},
	author_keywords = {Attribute learning; Attribute reduce; Flower classification; Sparse representation},
	keywords = {Genetic algorithms; Attribute learning; Attribute reduce; Attribute-based; Better performance; Classification scheme; Sparse representation; Visual attributes; analytic method; article; attribute learning; automated pattern recognition; automation; classification; data base; discrimination learning; flower; flower morphology; genetic algorithm; image processing; nonhuman; priority journal; sparse representation method; validation study; Classifiers},
	correspondence_address = {K. Cheng; School of Information Science and oTechnology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu 210016, China; email: kycheng@ujs.edu.cn},
	publisher = {Elsevier},
	issn = {09252312},
	coden = {NRCGE},
	language = {English},
	abbrev_source_title = {Neurocomputing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21}
}

@CONFERENCE{Munisami2015740,
	author = {Munisami, Trishen and Ramsurn, Mahess and Kishnah, Somveer and Pudaruth, Sameerchand},
	title = {Plant Leaf Recognition Using Shape Features and Colour Histogram with K-nearest Neighbour Classifiers},
	year = {2015},
	journal = {Procedia Computer Science},
	volume = {58},
	pages = {740 – 747},
	doi = {10.1016/j.procs.2015.08.095},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971012113&doi=10.1016%2fj.procs.2015.08.095&partnerID=40&md5=b6425381809c033c4afe45a19c7f34b6},
	affiliations = {Department of Computer Science and Engineering, Faculty of Engineering, University of Mauritius, Mauritius; Department of Ocean Engineering and ICT, Faculty of Ocean Studies, University of Mauritius, Mauritius},
	abstract = {Automated systems for plant recognition can be used to classify plants into appropriate taxonomies. Such information can be useful for botanists, industrialists, food engineers and physicians. In this work, a recognition system capable of identifying plants by using the images of their leaves has been developed. A mobile application was also developed to allow a user to take pictures of leaves and upload them to a server. The server runs pre-processing and feature extraction techniques on the image before a pattern matcher compares the information from this image with the ones in the database in order to get potential matches. The different features that are extracted are the length and width of the leaf, the area of the leaf, the perimeter of the leaf, the hull area, the hull perimeter, a distance map along the vertical and horizontal axes, a colour histogram and a centroid-based radial distance map. A k-Nearest Neighbour classifier was implemented and tested on 640 leaves belonging to 32 different species of plants. An accuracy of 83.5% was obtained. The system was further enhanced by using information obtained from a colour histogram which increased the recognition accuracy to 87.3%. Furthermore, our system is simple to use, fast and highly scalable. © 2015 The Authors.},
	author_keywords = {colour histogram; k-nearest neighbour; Leaf; pattern recognition; shape features},
	keywords = {Automation; Color; Computer vision; Feature extraction; Graphic methods; Internet; Nearest neighbor search; Pattern recognition; Colour histograms; Feature extraction techniques; K-nearest neighbours; Leaf; Mobile applications; Recognition accuracy; Recognition systems; Shape features; Plants (botany)},
	correspondence_address = {S. Pudaruth; Department of Ocean Engineering and ICT, Faculty of Ocean Studies, University of Mauritius, Mauritius; email: s.pudaruth@uom.ac.mu},
	editor = {Thampi S.M. and James A.P. and Al-Jumeily D.},
	publisher = {Elsevier},
	issn = {18770509},
	language = {English},
	abbrev_source_title = {Procedia Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 155; Conference name: 2nd International Symposium on Computer Vision and the Internet, VisionNet 2015; Conference date: 10 August 2015 through 13 August 2015; Conference code: 121594; All Open Access, Bronze Open Access}
}

@ARTICLE{Pant201327,
	author = {Pant, Paras and Heikkinen, Ville and Hovi, Aarne and Korpela, Ilkka and Hauta-Kasari, Markku and Tokola, Timo},
	title = {Evaluation of simulated bands in airborne optical sensors for tree species identification},
	year = {2013},
	journal = {Remote Sensing of Environment},
	volume = {138},
	pages = {27 – 37},
	doi = {10.1016/j.rse.2013.07.016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882605980&doi=10.1016%2fj.rse.2013.07.016&partnerID=40&md5=46e1fe606a0993a2e6b4fe33c084f116},
	affiliations = {School of Computing, University of Eastern Finland, POB 111, 80101, Finland; Department of Forest Sciences, University of Helsinki, POB 27, 00014, Finland; School of Forest Sciences, University of Eastern Finland, POB 111, 80101, Finland},
	abstract = {Airborne multispectral remote sensing devices have been used in automatic identification of tree species, and the spatial and spectral properties of the sensors affect the remote sensing measurement results. Previous work based on a simulation model with ground-level measured reflectance data of pine (Pinus sylvestris L.), spruce (Picea abies (L.) H. Karst.), and birch (Betula pubescens Ehrh. and Betula pendula Roth) tree species and idealized Leica ADS80 sensitivities suggested that the addition of a fifth sensitivity band in the red edge wavelength region to the existing Leica ADS80 system significantly improves the classification performance. In this paper, we extend this analysis using a simulated model with accurate spectral sensitivity information and airborne AisaEAGLE II hyperspectral data for these three tree species. We simulated multispectral responses using spectral sensitivity characteristics of the Leica ADS40, the Vexcel UltraCam-D, the Intergraph-Z/I Digital mapping camera and the Leica ADS40 system with an added band in the 691-785. nm region. We evaluated the tree species classification performance of these simulated responses using Discriminant Analysis and Support Vector Machine classifiers. The classification experiment result showed that the simulated responses of the 5-band multispectral system yielded the most robust classification performance with approximately 98% accuracy. This result was similar to the accuracy obtained with the hyperspectral data. Although differences were observed in the sensitivity functions of the 4-band systems, there were no large differences observed in the classification performances between them. With the simulated 5-band system, there was an increase of 5-13% points in classification accuracy when compared to the accuracies of the 4-band systems. The results obtained via proposed 5-band system support results from previous studies suggesting that there is a need for a sensitivity band in the red edge wavelength region for applications in tree species classification. © 2013 Elsevier Inc.},
	author_keywords = {Airborne multispectral sensors; Feature extraction; Pattern classification; Sensor sensitivity; Tree classification},
	keywords = {Forestry; Remote Sensing; Sensors; Spectroscopy; Automation; Computer simulation; Discriminant analysis; Feature extraction; Pattern recognition; Remote sensing; Spectroscopy; Support vector machines; Automatic identification; Classification performance; Digital mapping cameras; Multispectral remote sensing; Multispectral sensors; Sensor sensitivity; Support vector machine classifiers; Tree species identifications; accuracy assessment; airborne sensor; automation; classification; optical instrument; simulation; spectral analysis; tree; wavelength; Forestry},
	correspondence_address = {P. Pant; School of Computing, University of Eastern Finland, POB 111, 80101, Joensuu, Joensuu Campus, Finland; email: paras.pant@uef.fi},
	issn = {00344257},
	coden = {RSEEA},
	language = {English},
	abbrev_source_title = {Remote Sens. Environ.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21}
}

@CONFERENCE{Sünderhauf2014756,
	author = {Sünderhauf, Niko and McCool, Chris and Upcroft, Ben and Perez, Tristan},
	title = {Fine-grained plant classification using convolutional neural networks for feature extraction},
	year = {2014},
	journal = {CEUR Workshop Proceedings},
	volume = {1180},
	pages = {756 – 762},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961292721&partnerID=40&md5=8e64d5b0fbe1da6a098b38fb200a5449},
	affiliations = {Agricultural Robotics Program, Queensland University of Technology, 2 George Street, Brisbane, 4001, QLD, Australia},
	abstract = {We present an overview of the QUT plant classification system submitted to LifeCLEF 2014. This system uses generic features extracted from a convolutional neural network previously used to perform general object classification. We examine the effectiveness of these features to perform plant classification when used in combination with an extremely randomised forest. Using this system, with minimal tuning, we obtained relatively good results with a score of 0.249 on the test set of LifeCLEF 2014.},
	author_keywords = {Convolutional neural network; Extremely random forest; Plant classification},
	keywords = {Convolution; Decision trees; Feature extraction; Neural networks; Convolutional neural network; Fine grained; Generic features; Object classification; Plant classification; Random forests; System use; Test sets; Classification (of information)},
	editor = {Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Halvey M. and Kraaij W.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27; Conference name: 2014 Cross Language Evaluation Forum Conference, CLEF 2014; Conference date: 15 September 2014 through 18 September 2014; Conference code: 110355}
}

@ARTICLE{Zhao2015348,
	author = {Zhao, Zhong-Qiu and Xie, Bao-Jian and Cheung, Yiu-Ming and Wu, Xindong},
	title = {Plant leaf identification via a growing convolution neural network with progressive sample learning},
	year = {2015},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {9004},
	pages = {348 – 361},
	doi = {10.1007/978-3-319-16808-1_24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945959664&doi=10.1007%2f978-3-319-16808-1_24&partnerID=40&md5=b12778f16b099e7577058796570ee560},
	affiliations = {College of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; Department of Computer Science, Hong Kong Baptist University, Hong Kong; Department of Computer Science, University of Vermont, Burlington, United States; United International College, Beijing Normal University–Hong Kong Baptist University, Zhuhai, China},
	abstract = {Plant identification is an important problem for ecologists, amateur botanists, educators, and so on. Leaf, which can be easily obtained, is usually one of the important factors of plants. In this paper, we propose a growing convolution neural network (GCNN) for plant leaf identification and report the promising results on the ImageCLEF2012 Plant Identification database. TheGCNN owns a growing structure which starts training from a simple structure of a single convolution kernel and is gradually added new convolution neurons to. Simultaneously, the growing connection weights are modified until the squared-error achieves the desired result. Moreover, we propose a progressive learning method to determine the number of learning samples, which can further improve the recognition rate. Experiments and analyses show that our proposed GCNN outperforms other state-of-the-art algorithms such as the traditional CNN and the hand-crafted features with SVM classifiers. © Springer International Publishing Switzerland 2015.},
	keywords = {Convolution; Connection weights; Convolution kernel; Convolution neural network; Learning samples; Plant identification; Progressive learning; Simple structures; State-of-the-art algorithms; Computer vision},
	editor = {Yang M.-H. and Saito H. and Cremers D. and Reid I.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331916807-4},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; Conference name: 12th Asian Conference on Computer Vision, ACCV 2014; Conference date: 1 November 2014 through 5 November 2014; Conference code: 118689}
}

@ARTICLE{del Pozo-Baños2015377,
	author = {del Pozo-Baños, Marcos and Ticay-Rivas, Jaime R. and Alonso, Jesús B. and Travieso, Carlos M.},
	title = {Features extraction techniques for pollen grain classification},
	year = {2015},
	journal = {Neurocomputing},
	volume = {150},
	number = {PB},
	pages = {377 – 391},
	doi = {10.1016/j.neucom.2014.05.085},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922755210&doi=10.1016%2fj.neucom.2014.05.085&partnerID=40&md5=329ba59aef3ff5d90deb284b1740a006},
	affiliations = {Signals and Communications Department, Institute for Technological Development and Innovation in Communications, University of Las Palmas de Gran Canaria, Campus Universitario de Tafira, Las Palmas de Gran Canaria, Las Palmas, 35017, Spain},
	abstract = {An extensive study on pollen grain identification is presented in this work. A combination of geometrical and texture characteristics is proposed as pollen grain discriminative features as well as the usage of the most popular feature extraction techniques. Multi-Layer Neural Network and Least Square Support Vector Machines (LS-SVM) with Radial Basis Function were used as classifier systems. K-fold and hold-out cross-validation techniques were applied in order to achieve reliable results. When testing with a 17-species database, the combination of the proposed set of features processed by Linear Discriminant Analysis and the LS-SVM has provided the best performance, reaching a 94.92%±0.61 of success rate. Subsequently, the combination of both classifier methods provided better results, achieving 95.27%±0.49 of accuracy. © 2014 Elsevier B.V.},
	author_keywords = {Palynology; Pattern recognition; Plant biometric; Pollen grain identification},
	keywords = {Biometrics; Discriminant analysis; Extraction; Feature extraction; Network layers; Pattern recognition; Plants (botany); Radial basis function networks; Cross-validation technique; Discriminative features; Feature extraction techniques; Least square support vector machines; Linear discriminant analysis; Palynology; Pollen grains; Texture characteristics; Article; artificial neural network; biometry; controlled study; geometry; grain; independent component analysis; nonhuman; plant identification; plant structures; pollen; principal component analysis; regression analysis; support vector machine; Support vector machines},
	publisher = {Elsevier},
	issn = {09252312},
	coden = {NRCGE},
	language = {English},
	abbrev_source_title = {Neurocomputing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14}
}

@CONFERENCE{Zhang2014,
	author = {Zhang, William Y. and Hua, Xian-Sheng},
	title = {Plant identification with noisy web data},
	year = {2014},
	journal = {Proceedings - IEEE International Conference on Multimedia and Expo},
	volume = {2014-September},
	number = {Septmber},
	doi = {10.1109/ICME.2014.6890180},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937501151&doi=10.1109%2fICME.2014.6890180&partnerID=40&md5=6d41f380236cc9ee29470c7a35d81090},
	affiliations = {Monta Vista High School, United States; Microsoft Research, United States},
	abstract = {One of the main problems in image based plant identification has been the lack of quality training image data. A few attempts for solving this problem through generating high quality plant images from crowd sourced Web image collections like Flickr are proposed in this paper. These methods try to automatically identify correct and informative training images from those Web images, which typically have very noisy metadata (for example, user tags in Flickr), to enhance existing manually labeled training set. Firstly, for each plant, a set of images is collected from searching Flickr by using the plant name as the query. Then, images are clustered into visually consistent clusters, and in each cluster hopefully a majority of the images are all relevant or irrelevant to the particular plant. From these clusters, a managed plant image data set from ImageCLEF is used as reference to automatically select the highest quality cluster for each plant. The image quality of the selected clusters is further improved by two algorithms: an iterative method and image similarity based ranking. We show that the larger training data set automatically selected by this method significantly increases the accuracy of image based plant identification. In addition, this approach is a generic solution to almost all image recognition problems as long as additional (noisy) training data can be obtained from the Internet automatically. © 2014 IEEE.},
	author_keywords = {crowd sourced big data; Image classification; machine learning; plant identification},
	keywords = {Artificial intelligence; Big data; Image classification; Image recognition; Iterative methods; Learning systems; Problem solving; Social networking (online); Generic solutions; Image similarity; Plant identification; Quality training; Training data; Training data sets; Training image; Training sets; Image processing},
	publisher = {IEEE Computer Society},
	issn = {19457871},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Multimedia Expo},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2014 IEEE International Conference on Multimedia and Expo, ICME 2014; Conference date: 14 July 2014 through 18 July 2014; Conference code: 107344}
}

@ARTICLE{Syahputra2014697,
	author = {Syahputra, Hermawan and Harjoko, Agus and Wardoyo, Retantyo and Pulungan, Reza},
	title = {Plant recognition using stereo leaf image using gray-level co-occurrence matrix},
	year = {2014},
	journal = {Journal of Computer Science},
	volume = {10},
	number = {4},
	pages = {697 – 704},
	doi = {10.3844/jcssp.2014.697.704},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986321462&doi=10.3844%2fjcssp.2014.697.704&partnerID=40&md5=97c79785752cf3a5787b3fcf6cfb8181},
	affiliations = {Department of Mathematics, Faculty of Mathematics and Natural Sciences, State University of Medan, Medan, Indonesia; Department of Computer Science and Electronics, Faculty of Mathematics and Natural Sciences, Universitas Gadjah Mada, Yogyakarta, Indonesia},
	abstract = {Adequate knowledge, such as information about the unique characteristics of each plant, is necessary to identify plant. Researchers have made plant recognition based on leaf characteristics. The leaf image-based plant recognition in view of different angles is a new challenge. In this study, the research on the plant recognition was conducted based on leaf images resulted from 3D stereo camera. The 3D images are very influential in the development of computer vision theory, which can provide more detailed information of an object. One of the information that can be obtained is about the position of the object in its image with the background as well as of the camera. One of the ways used to obtain such information is to calculate the disparity. However, this method will only tell the position of the object compared to other objects without that of range. Sum Absolute Different (SAD) is a method that can be used to find the disparity value. The SAD method does not require heavy computations and long process. Before calculating the disparity, all the images should be previously segmented. The objective of this segmentation is to separate all the objects from the background. Furthermore, filtering and polynomial transformation at the results of disparity is necessary to improve the quality of resultant images. Furthermore, 22 features were extracted using GLCM features (second order statistics) of images resulted from disparity improvement. The highest accuracy of match in the recognition of plant varieties was obtained at 50 cm distance and in the recognition of three plant varieties was 83.3%. © 2014 Science Publications.},
	author_keywords = {Disparity; GLCM; Plant recognition; Stereo vision},
	publisher = {Science Publications},
	issn = {15493636},
	language = {English},
	abbrev_source_title = {J. Comput. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Lee2013109,
	author = {Lee, Kue-Bum and Chung, Kwang-Woo and Hong, Kwang-Seok},
	title = {An implementation of leaf recognition system based on leaf contour and centroid for plant classification},
	year = {2013},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {214 LNEE},
	pages = {109 – 116},
	doi = {10.1007/978-94-007-5857-5_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870799349&doi=10.1007%2f978-94-007-5857-5_12&partnerID=40&md5=ff227b6c074727edc6424267ced8bad1},
	affiliations = {School of Information and Communication Engineering, Sungkyunkwan University, Suwon, Kyungki-do 440-746, 300, Chunchun-dong, Jangan-gu, South Korea; Department of Railway Operation System Engineering, Korea National University of Transportation, Uiwang-si, Kyungki-do 437-763, 157, Cheoldobangmulkwan-ro, South Korea},
	abstract = {In this paper, we propose a leaf recognition system based on the leaf contour and centroid that can be used for plant classification. The proposed approach uses frequency domain data by performing a Fast Fourier transform (FFT) for the leaf recognition system. Twenty leaf features were extracted for leaf recognition. First, the distance between the centroid and all points on the leaf contours were calculated. Second, an FFT was performed using the calculated distances. Ten features were extracted using the calculated distances, FFT magnitude, and its phase. Ten features were also extracted based on the digital morphological features using four basic geometric features and five vein features. To verify the validity of the approach, images of 1907 leaves were used to classify 32 kinds of plants. In the experimental results, the proposed leaf recognition system showed an average recognition rate of 95.44 %, and we can confirm that the recognition rate of the proposed advanced leaf recognition method was better than that of the existed leaf recognition method. © 2013 Springer Science+Business Media.},
	author_keywords = {Fast fourier transform (FFT); Leaf feature extraction; Leaf recognition; Plant classification},
	keywords = {Fast Fourier transforms; Feature extraction; Information technology; Frequency-domain data; Geometric feature; Leaf recognition; Morphological features; Plant classification; Recognition rates; Plants (botany)},
	correspondence_address = {K.-B. Lee; School of Information and Communication Engineering, Sungkyunkwan University, Suwon, Kyungki-do 440-746, 300, Chunchun-dong, Jangan-gu, South Korea; email: leo0608@skku.edu},
	issn = {18761119},
	isbn = {978-940075856-8},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 7th International Conference on Ubiquitous Information Technologies and Applications, CUTE 2012; Conference date: 20 December 2012 through 22 December 2012; Conference code: 94342}
}

@CONFERENCE{Mzoughi20133967,
	author = {Mzoughi, Olfa and Yahiaoui, Itheri and Boujemaa, Nozha and Zagrouba, Ezzeddine},
	title = {Advanced tree species identification using multiple leaf parts image queries},
	year = {2013},
	journal = {2013 IEEE International Conference on Image Processing, ICIP 2013 - Proceedings},
	pages = {3967 – 3971},
	doi = {10.1109/ICIP.2013.6738817},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897805482&doi=10.1109%2fICIP.2013.6738817&partnerID=40&md5=1d6efc621eddce4b19684878a315467b},
	affiliations = {INRIA FRANCE, Intitut Supérieur d'Informatique, Université de Tunis El Manar, Tunisia; SIIVA/RIADI, Intitut Supérieur D'Informatique, Université de Tunis El Manar, Tunisia; CReSTIC Université de Reims, France},
	abstract = {There has recently been increasing interest in using advanced computer vision techniques for automatic plant identification. Most of the approaches proposed are based on an analysis of leaf characteristics. Nevertheless, two aspects have still not been well exploited: (1) domain-specific or botanical knowledge (2) the extraction of meaningful and relevant leaf parts. In this paper, we describe a new automated technique for leaf image retrieval that attempts to take these particularities into account. The proposed method is based on local representation of leaf parts. The part-based decomposition is defined and usually used by botanists. The global image query is a combination of part sub-images queries. Experiments carried out on real world leaf images, the Pl@ntLeaves scan images (3070 images totalling 70 species), show an increase in performance compared to global leaf representation. © 2013 IEEE.},
	author_keywords = {botanical knowledge; leaf parts; local representation; partial similarities; Plant identification},
	keywords = {Image retrieval; botanical knowledge; leaf parts; local representation; Partial similarities; Plant identification; Image processing},
	publisher = {IEEE Computer Society},
	isbn = {978-147992341-0},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Image Process., ICIP - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 23; Conference name: 2013 20th IEEE International Conference on Image Processing, ICIP 2013; Conference date: 15 September 2013 through 18 September 2013; Conference code: 115163}
}

@CONFERENCE{Amlekar2015,
	author = {Amlekar, M.M. and Gaikwad, A.T. and Manza, R.R. and Yannawar, P.L.},
	title = {Leaf shape extraction for plant classification},
	year = {2015},
	journal = {2015 International Conference on Pervasive Computing: Advance Communication Technology and Application for Society, ICPC 2015},
	doi = {10.1109/PERVASIVE.2015.7087088},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929248926&doi=10.1109%2fPERVASIVE.2015.7087088&partnerID=40&md5=7b36a1320bf39b995da32d0fc1800fae},
	affiliations = {Institute of Management Studies and Information Technology, Aurangabad, Maharashtra, India; Dept. of CS and IT, Dr. B.A.M. University, Aurangabad, Maharashtra, India},
	abstract = {This research paper presents the leaf shape extraction for plant classification. Leaves are very important component of the plant which actually identifies and classify the plants. Classification of the plant by their leaf biometric features is commonly performed task of trained botanist and taxonomist. To perform this task they need to perform various set of operations. Because of this the task of classification of plants manually is time consuming. There are many biometric features of leaves of the plants for classification. Here the shape of leaves of the plant species are extracted for plant classification. In this paper, various operators are studied for the leaf extraction from images by using the image processing techniques. © 2015 IEEE.},
	author_keywords = {Canny; leaf shape; plant classification; Prewitt; Roberts; Sobel},
	keywords = {Biometrics; Extraction; Image processing; Ubiquitous computing; Canny; Leaf shape; Plant classification; Prewitt; Roberts; Sobel; Plants (botany)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147996272-3},
	language = {English},
	abbrev_source_title = {Int. Conf. Pervasive Comput.: Adv. Commun. Technol. Appl. Soc., ICPC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; Conference name: 2015 International Conference on Pervasive Computing, ICPC 2015; Conference date: 8 January 2015 through 10 January 2015; Conference code: 111933}
}

@ARTICLE{Backes201589,
	author = {Backes, André Richard and Sá, Jarbas Joaci de Mesquita and Kolb, Rosana Marta},
	title = {A gravitational model for plant classification using adaxial epidermis texture},
	year = {2015},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {9280},
	pages = {89 – 96},
	doi = {10.1007/978-3-319-23234-8_9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944790129&doi=10.1007%2f978-3-319-23234-8_9&partnerID=40&md5=f4b7a809f3e8e7df84515a5a9acfd33d},
	affiliations = {Universidade Federal de Uberlândia, Av. João Naves de Ávila 2121, Uberlândia, CEP: 38408-100, MG, Brazil; Departamento de Engenharia de Computação, Universidade Federal do Ceará, Campus de Sobral, Rua Estanislau Frota, S/N, Centro, Sobral, CEP: 62010-560, Ceará, Brazil; Departamento de Ciências Biológicas, Universidade Estadual Paulista UNESP, Av. Dom Antônio, 2100, Assis, CEP: 19806-900, SP, Brazil},
	abstract = {The leaves are very informative plant organs. They are extensively used in plant anatomical studies focusing taxonomy. Their both inner and outer structures provide very discriminant features from vegetal species. In this study, we propose using images from adaxial epidermis for plant classification. The adaxial epidermis is a very variable region in a plant leaf cross-section. It differs in color, number of layers and presence/absence of hypodermis. To accomplish this task, we propose combining complexity analysis methods with a gravitational collapsing system to extract texture features from adaxial epidermis samples. Experimental results show that this combination of techniques surpasses traditional and state-of-the-art methods in both grayscale and color images of adaxial epidermis. © Springer International Publishing Switzerland 2015.},
	author_keywords = {Adaxial epidermis; Color; Gravitational system; Texture analysis},
	keywords = {Color; Image processing; Plants (botany); Adaxial epidermis; Complexity analysis; Gravitational model; Gravitational systems; Inner and outer structures; Plant classification; State-of-the-art methods; Texture analysis; Image analysis},
	editor = {Murino V. and Murino V. and Puppo E.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331923233-1},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 18th International Conference on Image Analysis and Processing, ICIAP 2015; Conference date: 7 September 2015 through 11 September 2015; Conference code: 141079}
}

@ARTICLE{Joly201422,
	author = {Joly, Alexis and Goëau, Hervé and Bonnet, Pierre and Bakić, Vera and Barbe, Julien and Selmi, Souheil and Yahiaoui, Itheri and Carré, Jennifer and Mouysset, Elise and Molino, Jean-François and Boujemaa, Nozha and Barthélémy, Daniel},
	title = {Interactive plant identification based on social image data},
	year = {2014},
	journal = {Ecological Informatics},
	volume = {23},
	pages = {22 – 34},
	doi = {10.1016/j.ecoinf.2013.07.006},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905119264&doi=10.1016%2fj.ecoinf.2013.07.006&partnerID=40&md5=dae09e3e11599b3cc9d7920b8f33b91f},
	affiliations = {INRIA, ZENITH team, LIRMM, F-34090 Montpellier, France; CIRAD, UMR AMAP, F-34398 Montpellier, France; INRA, UMR AMAP, F-34398 Montpellier, France; CReSTIC, University of Reims, France; Tela Botanica, F-34000 Montpellier, France; IRD, UMR AMAP, F-34398 Montpellier, France; INRIA Saclay, F-92120 Palaiseau, France; CIRAD, BIOS, Direction and INRA, UMR AMAP, F-34398 Montpellier, France},
	abstract = {Speeding up the collection and integration of raw botanical observation data is a crucial step towards a sustainable development of agriculture and the conservation of biodiversity. Initiated in the context of a citizen sciences project, the main contribution of this paper is an innovative collaborative workflow focused on image-based plant identification as a mean to enlist new contributors and facilitate access to botanical data. Since 2010, hundreds of thousands of geo-tagged and dated plant photographs were collected and revised by hundreds of novice, amateur and expert botanists of a specialized social network. An image-based identification tool - available as both a web and a mobile application - is synchronized with that growing data and allows any user to query or enrich the system with new observations. An important originality is that it works with up to five different organs contrarily to previous approaches that mainly relied on the leaf. This allows querying the system at any period of the year and with complementary images composing a plant observation. Extensive experiments of the visual search engine as well as system-oriented and user-oriented evaluations of the application show that it is already very helpful to determine a plant among hundreds or thousands of species. At the time of writing, the whole framework covers about half of the plant species living in France (2200 species), which already makes it the widest existing automated identification tool (with its imperfections). © 2013 Elsevier B.V.},
	author_keywords = {Bark; Botanist; Citizen science; Collaborative; Computer vision; Crowdsourcing; Ecology; Flower; Fruit; Identification; Images; Leaf; Monitoring; Multi-organ; Multimedia; Plant; Retrieval; Social network; Surveillance; Visual},
	keywords = {automation; identification method; image analysis; plant community; social network; species diversity; sustainable development},
	correspondence_address = {A. Joly; INRIA, ZENITH team, LIRMM, F-34090 Montpellier, France; email: alexis.joly@inria.fr},
	publisher = {Elsevier},
	issn = {15749541},
	language = {English},
	abbrev_source_title = {Ecol. Informatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 100}
}

@ARTICLE{Qi20142199,
	author = {Qi, Xianbiao and Xiao, Rong and Li, Chun-Guang and Qiao, Yu and Guo, Jun and Tang, Xiaoou},
	title = {Pairwise rotation invariant co-occurrence local binary pattern},
	year = {2014},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume = {36},
	number = {11},
	pages = {2199 – 2213},
	doi = {10.1109/TPAMI.2014.2316826},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907853291&doi=10.1109%2fTPAMI.2014.2316826&partnerID=40&md5=40cab4b134a92ce4f6d5bab19821fd57},
	affiliations = {School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Microsoft Corporation, 1 Microsoft Way, Redmond, 98052, WA, United States; Shenzhen Key Laboratory for Computer Vision and Pattern Recognition, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China},
	abstract = {Designing effective features is a fundamental problem in computer vision. However, it is usually difficult to achieve a great tradeoff between discriminative power and robustness. Previous works shown that spatial co-occurrence can boost the discriminative power of features. However the current existing co-occurrence features are taking few considerations to the robustness and hence suffering from sensitivity to geometric and photometric variations. In this work, we study the Transform Invariance (TI) of co-occurrence features. Concretely we formally introduce a Pairwise Transform Invariance (PTI) principle, and then propose a novel Pairwise Rotation Invariant Co-occurrence Local Binary Pattern (PRICoLBP) feature, and further extend it to incorporate multi-scale, multi-orientation, and multi-channel information. Different from other LBP variants, PRICoLBP can not only capture the spatial context co-occurrence information effectively, but also possess rotation invariance. We evaluate PRICoLBP comprehensively on nine benchmark data sets from five different perspectives, e.g., encoding strategy, rotation invariance, the number of templates, speed, and discriminative power compared to other LBP variants. Furthermore we apply PRICoLBP to six different but related applications - texture, material, flower, leaf, food, and scene classification, and demonstrate that PRICoLBP is efficient, effective, and of a well-balanced tradeoff between the discriminative power and robustness. © 1979-2012 IEEE.},
	author_keywords = {Co-occurrence LBPs; flower recognition; food recognition; leaf recognition; material recognition; rotation invariance; scene recognition; texture classification},
	keywords = {Algorithms; Databases, Factual; Food; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Plants; Co-occurrence; Flower recognition; Leaf recognition; Material recognition; Rotation invariance; Scene recognition; Texture classification; algorithm; automated pattern recognition; classification; factual database; food; image processing; plant; procedures},
	publisher = {IEEE Computer Society},
	issn = {01628828},
	coden = {ITPID},
	pmid = {26353061},
	language = {English},
	abbrev_source_title = {IEEE Trans Pattern Anal Mach Intell},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 240}
}

@CONFERENCE{Karamti2014747,
	author = {Karamti, Hanen and Fakhfakh, Sana and Tmar, Mohamed and Mahdi, Walid and Gargouri, Faiez},
	title = {MIRACL at LifeCLEF 2014: Multi-organ observation for plant identification},
	year = {2014},
	journal = {CEUR Workshop Proceedings},
	volume = {1180},
	pages = {747 – 755},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981234343&partnerID=40&md5=aa5bcb382da8afed0fd36a0d964ec767},
	affiliations = {MIRACL Laboratory, City ons Sfax, University of Sfax, B.P.3023, Sfax, Tunisia},
	abstract = {ImageCLEF 2014 has a challenge based on analysis for identifying plants. This article describes our first participation to the multiimage plant observation queries task of PlantCLEF 2014. The task will be evaluated as a plant species retrieval task based on multi-image plant observations queries. The goal is to retrieve the correct plant species among the top results of a ranked list of species returned by the evaluated system. In this paper, we present two method. Our first method is purely visual and entirely automatic, using only the image information. One should mention that the total time spent with preparing this submission was only about three week. The results were accordingly fairly poor. The challenge of our second method is to identify plant species based on combination of textual and structural context of image. Indeed, we have used the meta-data in our system for exploring the image characteristics. Our approach is based on a modern technique for exploitation of structure of XML document. Also, the results were accordingly fairly poor. Although our results are not quite promising as compared to other participant groups, they can still guide our work in this field for some conclusions reached.},
	author_keywords = {Feature extraction; Image retrieval; ImageCLEF; Plant observation; XML},
	keywords = {Feature extraction; XML; Image characteristics; Image information; ImageCLEF; Modern techniques; Multi-images; Plant identification; Plant observation; Plant species; Image retrieval},
	editor = {Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Halvey M. and Kraaij W.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2014 Cross Language Evaluation Forum Conference, CLEF 2014; Conference date: 15 September 2014 through 18 September 2014; Conference code: 110355}
}

@ARTICLE{Silva2013197,
	author = {Silva, Pedro F. B. and Marcal, André R. S. and Da Silva, Rubim M. Almeida},
	title = {Evaluation of features for leaf discrimination},
	year = {2013},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {7950 LNCS},
	pages = {197 – 204},
	doi = {10.1007/978-3-642-39094-4_23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884488199&doi=10.1007%2f978-3-642-39094-4_23&partnerID=40&md5=703448ac70709e772848157e7b8687f9},
	affiliations = {Departamento de Matemática, Faculdade de Ciências, Universidade Do Porto, Portugal; Departamento de Biologia, Faculdade de Ciências, Universidade Do Porto, Portugal},
	abstract = {A number of shape features for automatic plant recognition based on digital image processing have been proposed by Pauwels et al. in 2009. A database with 15 classes and 171 leaf samples was considered for the evaluation of these measures using linear discriminant analysis and hierarchical clustering. The results obtained match the human visual shape perception with an overall accuracy of 87%. © 2013 Springer-Verlag.},
	keywords = {Artificial intelligence; Computer science; Hier-archical clustering; Human visual; Linear discriminant analysis; Overall accuracies; Plant recognition; Shape features; Shape perception; Image analysis},
	issn = {16113349},
	isbn = {978-364239093-7},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 84; Conference name: 10th International Conference on Image Analysis and Recognition, ICIAR 2013; Conference date: 26 June 2013 through 28 June 2013; Conference code: 99441}
}

@ARTICLE{Hsiao201577,
	author = {Hsiao, Jou-Ken and Kang, Li-Wei and Chang, Ching-Long and Lin, Chih-Yang},
	title = {Learning-based leaf image recognition frameworks},
	year = {2015},
	journal = {Studies in Computational Intelligence},
	volume = {591},
	pages = {77 – 91},
	doi = {10.1007/978-3-319-14654-6_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922879706&doi=10.1007%2f978-3-319-14654-6_5&partnerID=40&md5=9d7487bccb108a11ce1fc8cd3814f86f},
	affiliations = {Department of Computer Science and Information Engineering, National Yunlin University of Science and Technology, Yunlin, Taiwan; Graduate School of Engineering Science and Technology-Doctoral Program, National Yunlin University of Science and Technology, Yunlin, Taiwan; Department of Computer Science and Information Engineering, Asia University, Taichung, Taiwan},
	abstract = {Automatic plant identification via computer vision techniques has been greatly important for a number of professionals, such as environmental protectors, land managers, and foresters. In this chapter, we propose two learning-based leaf image recognition frameworks for automatic plant identification and conduct a comparative study between them with existing approaches. First, we propose to learn sparse representation for leaf image recognition. In order to model leaf images, we learn an over-complete dictionary for sparsely representing the training images of each leaf species. Each dictionary is learned using a set of descriptors extracted from the training images in such a way that each descriptor is represented by linear combination of a small number of dictionary atoms. Second, we also propose a general bag-of-words (BoW) model-based recognition system for leaf images, mainly used for comparison. We experimentally compare the two learning-based approaches and show unique characteristics of our sparse representation- based framework. As a result, efficient leaf recognition can be achieved on public leaf image dataset based on the two proposed methods. We also show that the proposed sparse representation-based framework can outperform our BoWbased one and state-of-the-art approaches, conducted on the same dataset. © Springer International Publishing Switzerland 2015.},
	author_keywords = {Bag-of-words (BoW); Dictionary learning; Leaf image recognition; Plant identification; Sparse representation},
	publisher = {Springer Verlag},
	issn = {1860949X},
	language = {English},
	abbrev_source_title = {Stud. Comput. Intell.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Yusof2013737,
	author = {Yusof, Rubiyah and Rosli, Nenny Ruthfalydia},
	title = {Tropical wood species recognition system based on gabor filter as image multiplier},
	year = {2013},
	journal = {Proceedings - 2013 International Conference on Signal-Image Technology and Internet-Based Systems, SITIS 2013},
	pages = {737 – 743},
	doi = {10.1109/SITIS.2013.120},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894150245&doi=10.1109%2fSITIS.2013.120&partnerID=40&md5=8530fc29ff806f30e4797f7f5800ded5},
	affiliations = {Centre for Artificial Intelligence and Robotics, Universiti Teknologi Malaysia, International Campus, Jalan Semarak, 54100 Kuala Lumpur, Malaysia},
	abstract = {The main problem in wood species recognition system is the lack of discriminative features of the texture images. Some of the wood species have similar patterns with others and some have different patterns even though they are of the same species. Moreover, the growth rings for tropical wood changes slightly due seasonal changes in climate. One of the ways to improve the system is by providing more features representation of each species. In this work, Gabor filter is proposed to generate multiple processed images from a single image so that more features can be extracted and trained by the neural network. After the raw image has been sharpened and contrast enhancement has been applied at the preprocessing stage, the image will be convolved with Gabor filters. The output of the convolution generates Gabor images which are images extracted based on frequency and spatial information of the original images. These Gabor images will be used by grey level co-occurrence matrix (GLCM) for feature extraction. A multi-layer neural network based on popular back-propagation (MLBP) algorithm is used for classification. The result shows that increasing the number of features by means of Gabor filters as well as the right combination of Gabor filters increases the accuracy rate of the system. © 2013 IEEE.},
	author_keywords = {Gabor filter; Grey level co-occurrence matrix (GLCM); Image multiplier; Neural network; Texture pattern recognition; Wood recognition},
	isbn = {978-147993211-5},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Signal-Image Technol. Internet-Based Syst., SITIS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; Conference name: 2013 9th International Conference on Signal-Image Technology and Internet-Based Systems, SITIS 2013; Conference date: 2 December 2013 through 5 December 2013; Conference code: 102740}
}

@ARTICLE{SandeepKumar201599,
	author = {SandeepKumar, E. and Talasila, Viswanath},
	title = {Recognition of medicinal plants based on its leaf features},
	year = {2015},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {327},
	pages = {99 – 113},
	doi = {10.1007/978-81-322-2141-8_9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922162066&doi=10.1007%2f978-81-322-2141-8_9&partnerID=40&md5=8b2fc249dc6ee4b68b33a7fbe557b9f8},
	affiliations = {Department of TCE, M.S. Ramaiah Institute of Technology, Bangalore, Karnataka, India},
	abstract = {Ayurveda is one of the oldest forms of medicine, and the use of medicinal plants is a crucial aspect in Ayurvedic treatment. In this paper, we develop an automated system to identify the vast number of medicinal plants relevant for Ayurveda. We focus on the use of image processing and pattern recognition algorithms for plant identification. A unique feature identifier is computed, and this feature algorithm is tested on ten different medicinal plants for accurate identification. The main result in this paper is a demonstration that the features attributed to the leaf of each plant are Gaussian distributed. © Springer India 2015.},
	author_keywords = {Ayurvedic medicinal system; Gaussian distribution; Image processing},
	keywords = {Automation; Gaussian distribution; Image processing; Pattern recognition; Supervisory personnel; Automated systems; Ayurvedic medicinal system; Gaussian distributed; Medicinal plants; Pattern recognition algorithms; Plant identification; Unique features; Plants (botany)},
	editor = {Vijay V.V. and Adhikari B. and Seshadri H. and Fulwani D.K. and Yadav S.K.},
	publisher = {Springer Verlag},
	issn = {18761100},
	isbn = {978-813222140-1},
	language = {English},
	abbrev_source_title = {Lect. Notes Electr. Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 37th National Systems Conference, 2013; Conference date: 5 December 2013 through 7 December 2013; Conference code: 112889}
}

@CONFERENCE{Fakhfakh2014715,
	author = {Fakhfakh, Sana and Akrout, Belhassen and Tmar, Mohamed and Mahdi, Walid},
	title = {A visual search of multimedia documents in ImageCLEF 2014},
	year = {2014},
	journal = {CEUR Workshop Proceedings},
	volume = {1180},
	pages = {715 – 723},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981271706&partnerID=40&md5=0af6ea780adab28d4f41088be61cfa61},
	affiliations = {Laboratoire Miracl ISIMS Pole Technologique de Sfax, BP 242-3021, Sakiet Ezzit Sfax, Tunisia},
	abstract = {The Image search by content is an area that is based on a set of low-level features such as histograms, textures, the distribution of colors, shapes an brightness. The structure element is a shortcut of the image may vary depending on the designer of the XML document and which may change according to application needs. The visual appearance of the image is a permanent factor that undergoes no change. Therefore, we present in this paper a new method of search by content based on Harris detector, Haar wavelet and Color histogram for the step of feature extraction to extract a binary signature for all multimedia documents.Finally We estimate the similarities between codes and search relevant images with Hamming distance.},
	author_keywords = {Future extraction; Hamming distance; Plant identification; Signature extraction},
	keywords = {Extraction; Feature extraction; Graphic methods; Image retrieval; Optical communication; Statistical methods; Color histogram; Harris detector; Low-level features; Multimedia documents; Plant identification; Signature extraction; Structure elements; Visual appearance; Hamming distance},
	editor = {Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Halvey M. and Kraaij W.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2014 Cross Language Evaluation Forum Conference, CLEF 2014; Conference date: 15 September 2014 through 18 September 2014; Conference code: 110355}
}

@ARTICLE{Fern2014209,
	author = {Fern, Bong Mei and Sulong, Ghazali Bin and Rahim, Mohd Shafry Mohd},
	title = {Leaf recognition based on leaf tip and leaf base using centroid contour gradient},
	year = {2014},
	journal = {Advanced Science Letters},
	volume = {20},
	number = {1},
	pages = {209 – 212},
	doi = {10.1166/asl.2014.5300},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887906745&doi=10.1166%2fasl.2014.5300&partnerID=40&md5=9c784c022a3ea0561f4e64562edbd0ed},
	affiliations = {Universiti Teknologi Malaysia, 81310 Johor Bahru, Malaysia},
	abstract = {In this paper, we suggest to normalize the leaf tip and leaf base as both of them may incline to one direction which able to influence the data extraction process. The features extraction method we used is Centroid Contour Gradient (CCG) which calculate the gradient between pairs of boundary point corresponding to interval angle, θ. CCG had outperformed its competitors which is Centroid Contours Distance (CCD) as it is successfully captures the curvature of leaf tip and leaf base. The accuracy to classify the leaf tip using CCG is 99.47%, and CCD is only 80.30%. For the accuracy of leaf base classification, CCG (98%) also outperforms CCD (88%). The average accuracy to recognize the 5 classes of plant is 96.6% for CCG and 74.4% for CCD. In this research, we utilized the Feed-forwad Back-propagation as our classifier. © 2014 American Scientific Publishers All rights reserved.},
	author_keywords = {Centroid contour distance; Centroid contour gradient; Leaf base; Leaf recognition; Leaf tip},
	issn = {19367317},
	language = {English},
	abbrev_source_title = {Adv. Sci. Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Dorj201249,
	author = {Dorj, Ulzii-Orshikh and Lee, Malrey and Diyan-Ul-Imaan},
	title = {A new method for Tangerine tree flower recognition},
	year = {2012},
	journal = {Communications in Computer and Information Science},
	volume = {353 CCIS},
	pages = {49 – 56},
	doi = {10.1007/978-3-642-35521-9_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870770533&doi=10.1007%2f978-3-642-35521-9_7&partnerID=40&md5=6d994d0b0ceebed4b38e38411cdadc99},
	affiliations = {Center for Advanced Image and Information Technology, School of Electronics and Information Engineering, Chon Buk National University, Jeonju, Chon Buk, 561-756, 664-14, 1Ga, Deokjin-Dong, South Korea},
	abstract = {Different machine vision strategies are adapted for performing automated real time agricultural tasks in order to increase more productivity with less cost. Based on this notion, a new method is developed and implemented for detecting white color flowers in Tangerine tree and counting Tangerine fruit flowers to yield better outputs with regard to the existing schemes. Gaussian filter is employed to reduce unwanted noise in Tangerine tree flower recognition for Tangerine yield mapping system. It is observed that the newly developed method gives better valid output for tangerine tree flower detection in natural outdoor lighting, with different lighting condition without any alternative lighting source to control the luminance. The simulation result reveals that the new method is reliable, feasible and efficient compared to other existing methods. © 2012 Springer-Verlag.},
	author_keywords = {color detection; counting algorithm; Gaussian smoothing; machine vision; Tangerine tree flower; yield estimation},
	keywords = {Agriculture; Algorithms; Flowers; Forestry; Illumination; Neural Networks; Pattern Recognition; Agricultural machinery; Computer applications; Computer vision; Forestry; Information technology; Lighting; Color detection; Flower recognition; Gaussian filters; Gaussian smoothing; Lighting conditions; Real time; Tangerine tree flower; White color; Yield estimation; Yield mapping; Trees (mathematics)},
	correspondence_address = {M. Lee; Center for Advanced Image and Information Technology, School of Electronics and Information Engineering, Chon Buk National University, Jeonju, Chon Buk, 561-756, 664-14, 1Ga, Deokjin-Dong, South Korea; email: mrlee@chonbuk.ac.kr},
	issn = {18650929},
	isbn = {978-364235520-2},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 2012 Int. Conf. on MulGraB 2012, the 2012 Int. Conf. on BSBT 2012, and the 1st Int. Conf. on Intelligent Urban Computing, IUrC 2012, Held as Part of the Future Generation Information Technology Conference, FGIT 2012; Conference date: 16 December 2012 through 19 December 2012; Conference code: 94349}
}

@CONFERENCE{Goëau2013423,
	author = {Goëau, Hervé and Bonnet, Pierre and Joly, Alexis and Bakić, Vera and Barbe, Julien and Yahiaoui, Itheri and Selmi, Souheil and Carré, Jennifer and Barthélémy, Daniel and Boujemaa, Nozha and Molino, Jean-François and Duché, Grégoire and Péronnet, Aurélien},
	title = {Pl@ntnet mobile app},
	year = {2013},
	journal = {MM 2013 - Proceedings of the 2013 ACM Multimedia Conference},
	pages = {423 – 424},
	doi = {10.1145/2502081.2502251},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887454234&doi=10.1145%2f2502081.2502251&partnerID=40&md5=29bca1906e89c9d7022853002f4f08c4},
	affiliations = {INRIA, ZENITH, France; CIRAD, UMR, AMAP, France; IRD, UMR, AMAP, France; CReSTIC Univ., Reims, France; Tela Botanica, France; CIRAD, BIOS, France; INRIA, Saclay, France},
	abstract = {Pl@ntNet is an image sharing and retrieval application for the identification of plants, available on iPhone and iPad de- vices. Contrary to previous content-based identification ap- plications it can work with several parts of the plant includ- ing owers, leaves, fruits and bark. It also allows integrating user's observations in the database thanks to a collaborative workow involving the members of a social network special- ized on plants. Data collected so far makes it one of the largest mobile plant identification tool. Copyright © 2013 ACM.},
	author_keywords = {Bark; Botanist; Citizen science; Collaborative; Computer vision; Crowdsourcing; Ecology; Fruit; Identification; Images; Leaf; Moni- Toring; Multi- organ; Multimedia; Ower; Plant; Retrieval; Social net- work; Surveillance; Visual},
	keywords = {Computer vision; Ecology; Fruits; Identification (control systems); Image processing; Space surveillance; Bark; Botanist; Citizen science; Collaborative; Crowdsourcing; Images; Leaf; Multi- organ; Multimedia; Ower; Plant; Retrieval; Social net- work; Visual; Plants (botany)},
	isbn = {978-145032404-5},
	language = {English},
	abbrev_source_title = {MM - Proc. ACM Multimedia Conf.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 50; Conference name: 21st ACM International Conference on Multimedia, MM 2013; Conference date: 21 October 2013 through 25 October 2013; Conference code: 100792; All Open Access, Green Open Access}
}

@CONFERENCE{Rankothge2013,
	author = {Rankothge, W.H. and Dissanayake, D.M.S.B. and Gunathilaka, U.V.K.T. and Gunarathna, S.A.C.M. and Mudalige, C.M. and Thilakumara, R.P.},
	title = {Plant recognition system based on Neural Networks},
	year = {2013},
	journal = {2013 International Conference on Advances in Technology and Engineering, ICATE 2013},
	doi = {10.1109/ICAdTE.2013.6524735},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879668130&doi=10.1109%2fICAdTE.2013.6524735&partnerID=40&md5=9c198ab2fc9e305f61aec2bc6ebd1dca},
	affiliations = {Sri Lanka Institute of Information Technology, Malabe, Sri Lanka},
	abstract = {With the evolution of technologies, people have adopted their day today lives to utilize the benefits of highly advanced technologies. Artificial Intelligence and Neural Networks are playing major roles in this process and they have been involved in fields of medicine, automobiles, aeronautical science, military and many more. Unfortunately very little concern is devoted to the botanical science field, especially in taxonomic researches of plants. Even today, identification and classification of unknown plant species are performed manually by expert personnel who are very few in number. It takes a long time and the results are not very accurate. Advanced Plant Identification System (APIS) is an intelligent system which has the ability to identify tree species from photographs of their leaves and it provides more accurate results within less time. © 2013 IEEE.},
	author_keywords = {Artificial Intelligence; Neural Networks; Pattern Recognition; Plant Identification},
	keywords = {Artificial intelligence; Intelligent systems; Neural networks; Pattern recognition; Technology; Advanced technology; Artificial intelligence and neural networks; Evolution of technology; Plant identification; Plant identification systems; Plant recognition; Plant species; Tree species; Plants (botany)},
	isbn = {978-146735618-3},
	language = {English},
	abbrev_source_title = {Int. Conf. Adv. Technol. Eng., ICATE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 2013 International Conference on Advances in Technology and Engineering, ICATE 2013; Conference date: 23 January 2013 through 25 January 2013; Conference code: 97488}
}

@ARTICLE{Cerutti2014177,
	author = {Cerutti, Guillaume and Tougne, Laure and Coquin, Didier and Vacavant, Antoine},
	title = {Leaf margins as sequences: A structural approach to leaf identification},
	year = {2014},
	journal = {Pattern Recognition Letters},
	volume = {49},
	pages = {177 – 184},
	doi = {10.1016/j.patrec.2014.07.016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907303298&doi=10.1016%2fj.patrec.2014.07.016&partnerID=40&md5=3e8e37bc96a15a6aef38aad639534436},
	affiliations = {Université de Lyon, CNRS, France; Université Lyon 2, LIRIS, UMR5205, F-69676, France; LISTIC, Domaine Universitaire, F-74944 Annecy le Vieux, France; Clermont Université, Université d'Auvergne, ISIT, F-63001 Clermont-Ferrand, France},
	abstract = {In the context of an automated leaf identification process, the use of thorough leaf margin descriptors is essential given the importance of this criterion in the determination of the species. The spatial properties of teeth along the leaf contour are something to keep track of, which is made possible through the use of structured representations. This paper introduces a sequence representation of leaf margins where teeth are viewed as symbols of a multivariate real valued alphabet. It presents the methods developed to make use of this description for classification and implementation in a mobile tree identifying application. The results of various classification methods are compared and discussed, both in terms of species recognition and of consistency with botanical concepts. © 2014 Elsevier B.V. All rights reserved.},
	author_keywords = {Classification; Edit distance; Leaf recognition; Multivariate sequences},
	keywords = {Classification (of information); Pattern recognition; Classification methods; Edit distance; Leaf identification; Leaf recognition; Multivariate sequences; Spatial properties; Species recognition; Structural approach; Software engineering},
	correspondence_address = {G. Cerutti; LIRIS, Université Lyon 2, 69676 Bron, France; email: guillaume.cerutti@liris.cnrs.fr},
	publisher = {Elsevier},
	issn = {01678655},
	language = {English},
	abbrev_source_title = {Pattern Recogn. Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27}
}

@ARTICLE{Hussin20141914,
	author = {Hussin, Nuril Aslina Che and Jamil, Nursuriati and Nordin, Sharifalillah and Awang, Khalil},
	title = {Geometrical-invariant grid-based colour moment (GBCM) for plant identification},
	year = {2014},
	journal = {Advanced Science Letters},
	volume = {20},
	number = {10-12},
	pages = {1914 – 1917},
	doi = {10.1166/asl.2014.5643},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919331662&doi=10.1166%2fasl.2014.5643&partnerID=40&md5=ccecb85b48553bac787883771579e78b},
	affiliations = {Department of Computer Science, Universiti Teknologi MARA, Shah Alam, 40450, Selangor, Malaysia},
	abstract = {In any object identification problem, feature extraction method is an important phase as it determines the identification’s accuracy. This paper presents a colour feature extraction techniques known as Grid Based Color Moment (GBCM) to identify herbal plants of Malaysia. Thirty plant species images are collected from their natural habitats and captured under various time of the day. These plant images are then used as ground truth images. They are further rotated and scaled to produce another thirty test images. The extracted features of the test images are then identified by calculating their Euclidean Distance (ED) against the ground truth and achieved an accuracy rate of 96.7 percent. © 2014 American Scientific Publishers All rights reserved.},
	author_keywords = {Color moment; Euclidean distance; Plant leaf identification; Scale invariant feature transform},
	publisher = {American Scientific Publishers},
	issn = {19366612},
	language = {English},
	abbrev_source_title = {Adv. Sci. Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Cerutti20131482,
	author = {Cerutti, Guillaume and Tougne, Laure and Mille, Julien and Vacavant, Antoine and Coquin, Didier},
	title = {Understanding leaves in natural images - A model-based approach for tree species identification},
	year = {2013},
	journal = {Computer Vision and Image Understanding},
	volume = {117},
	number = {10},
	pages = {1482 – 1501},
	doi = {10.1016/j.cviu.2013.07.003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885374723&doi=10.1016%2fj.cviu.2013.07.003&partnerID=40&md5=10a797ed82ca283634ea69897a9ea096},
	affiliations = {Université de Lyon, CNRS, France; Université Lyon 2, LIRIS, UMR5205, F-69676 Bron cedex, 5 Avenue Pierre Mendés France, France; Université Lyon 1, LIRIS, UMR5205, F-69622 Villeurbanne Cedex, France; Clermont Université, Université d'Auvergne, ISIT, F-63001 Clermont-Ferrand, France; LISTIC, Domaine Universitaire, F-74944 Annecy le Vieux, France},
	abstract = {With the aim of elaborating a mobile application, accessible to anyone and with educational purposes, we present a method for tree species identification that relies on dedicated algorithms and explicit botanyinspired descriptors. Focusing on the analysis of leaves, we developed a working process to help recognize species, starting from a picture of a leaf in a complex natural background. A two-step active contour segmentation algorithm based on a polygonal leaf model processes the image to retrieve the contour of the leaf. Features we use afterwards are high-level geometrical descriptors that make a semantic interpretation possible, and prove to achieve better performance than more generic and statistical shape descriptors alone. We present the results, both in terms of segmentation and classification, considering a database of 50 European broad-leaved tree species, and an implementation of the system is available in the iPhone application Folia. © 2013 Elsevier Inc. All rights reserved.},
	author_keywords = {Active contours; Classification; Geometric features; Image segmentation; Natural background; Plant recognition; Tree leaf},
	keywords = {Classification (of information); Forestry; Image processing; Plants (botany); Semantics; Trees (mathematics); Active contours; Geometric feature; Natural backgrounds; Plant recognition; Tree leaf; Image segmentation},
	correspondence_address = {G. Cerutti; Université Lyon 2, LIRIS, UMR5205, F-69676 Bron cedex, 5 Avenue Pierre Mendés France, France; email: guillaume.cerutti@liris.cnrs.fr},
	publisher = {Academic Press Inc.},
	issn = {10773142},
	coden = {CVIUF},
	language = {English},
	abbrev_source_title = {Comput Vision Image Understanding},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 91; All Open Access, Green Open Access}
}

@ARTICLE{Wang2014101,
	author = {Wang, Xuan and Liang, Junhua and Guo, Fangxia},
	title = {Feature extraction algorithm based on dual-scale decomposition and local binary descriptors for plant leaf recognition},
	year = {2014},
	journal = {Digital Signal Processing: A Review Journal},
	volume = {34},
	pages = {101 – 107},
	doi = {10.1016/j.dsp.2014.08.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027950323&doi=10.1016%2fj.dsp.2014.08.005&partnerID=40&md5=8cc897d563f39f0fb9133a13f8c7ce8f},
	affiliations = {School of Physics and Information Technology, Shaanxi Normal University, Xi'an, Shaanxi, 710062, China; School of Information Science and Engineering, Hebei North University, Zhang Jiakou, Hebei, 075000, China},
	abstract = {Plant leaf recognition is very important and necessary to agricultural information and ecological protection. Unfortunately, the robustness and discriminability of the existing methods are insufficient. This paper describes a novel plant leaf recognition method. In order to extract distinctive features from plant leaf images and reduce the probability of disruption by occlusion, clutter, or noise, a novel feature extraction algorithm based on dual-scale decomposition and local binary descriptors is proposed. The dual-scale decomposition consists of two phases. In the first phase, a plant leaf image is decomposed into several subbands with an adaptive lifting wavelet scheme. In the second phase, each subband is filtered using a group of variable-scale Gaussian filters. Local binary descriptors are extracted from the filtered subbands to capture both shape and texture characteristics, and then the histograms of the local binary descriptors at different scales and different subbands are determined and regarded as features. In order to improve the robustness and discriminability of plant leaf recognition further, a fuzzy k-nearest neighbors' classifier is introduced for matching. Experimental results show that the proposed approach yields a better performance in terms of the classification accuracies compared with the state of the art methods. It is also shown that this method is relatively robust to noise, occlusion and smoothing. © 2014 Elsevier Inc.},
	author_keywords = {Adaptive lifting wavelet scheme; Dual-scale decomposition; Fuzzy k-nearest neighbors' classifier; Local binary pattern; Plant recognition},
	keywords = {Extraction; Feature extraction; Motion compensation; Nearest neighbor search; Dual scale; Fuzzy k nearest neighbor (FKNN); Lifting wavelet; Local binary patterns; Plant recognition; Wavelet decomposition},
	publisher = {Elsevier Inc.},
	issn = {10512004},
	coden = {DSPRE},
	language = {English},
	abbrev_source_title = {Digital Signal Process Rev J},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 29}
}

@ARTICLE{Yu2013,
	author = {Yu, Peng and Wan, Cha-Yan and Wu, Chang-Shun and Shou, Jia-Ni and Cheng, Cun-Gui},
	title = {Classification of green bristle grass, yellow foxtail and Chinese pennisetum seeds via HATR-FT-IR combined with chemometrics},
	year = {2013},
	journal = {Journal of Spectroscopy},
	volume = {1},
	number = {1},
	doi = {10.1155/2013/531747},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876890945&doi=10.1155%2f2013%2f531747&partnerID=40&md5=339e110617ab6784f8cba70d2e17cdea},
	affiliations = {College of Chemistry and Life Sciences, Zhejiang Normal University, Jinhua 321004, China; Chuyang Honors College, Zhejiang Normal University, Jinhua 321004, China},
	abstract = {Fourier transform infrared (FT-IR) and horizontal attenuated total reflectance (HATR) technique are used to obtain the FT-IR spectra of the seed of green bristle grass (the seed from Setaria viridis (L.) Beauv), yellow foxtail seed (the seed from Setaria glauca (L.) Beauv), and the Chinese pennisetum seed (the seed from Setaria faberii Herrum). In order to extrude the difference among them, cluster analysis is considered to identify the three kinds of plant seeds. Because they belong to the sibling plant seeds, and have similar chemical components and close FT-IR spectra. The result of Cluster analysis is not satisfactory. The discrete wavelet transformation (DWT) and a support vector machine (SVM) were used for further study. The compression detail 3 and 4 in DWT are used to extract the feature vectors, which are used to train SVM. The trained SVM is used to classify seed of green bristle grass, yellow foxtail seed and Chinese pennisetum seed. The seed samples are collected from different places around the country. With 40 testing samples we could effectively identify the sibling plants, seed of green bristle grass, yellow foxtail seed and Chinese pennisetum seed by FT-IR with discrete wavelet feature extraction and SVM classification. © 2013 Peng Yu et al.},
	keywords = {Cluster analysis; Discrete wavelet transforms; Feature extraction; Support vector machines; Attenuated total reflectance; Chemical component; Discrete wavelet transformation; Discrete wavelets; Feature vectors; Fourier transform infrared; SVM classification; Testing samples; article; chemometrics; Chinese pennisetum seed; cluster analysis; discrete wavelet transformation; grass; green bristle grass; horizontal attenuated total reflectance fourier transform infrared spectroscopy; infrared spectroscopy; nonhuman; plant identification; plant seed; support vector machine; wavelet analysis; yellow foxtail seed; Seed},
	correspondence_address = {C.-G. Cheng; College of Chemistry and Life Sciences, Zhejiang Normal University, Jinhua 321004, China; email: chengcg123@sina.com},
	issn = {23144939},
	language = {English},
	abbrev_source_title = {J. Spectroscopy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@CONFERENCE{Tiay201499,
	author = {Tiay, Tanakorn and Benyaphaichit, Pipimphorn and Riyamongkol, Panomkhawn},
	title = {Flower recognition system based on image processing},
	year = {2014},
	journal = {Proceedings of the 2014 3rd ICT International Senior Project Conference, ICT-ISPC 2014},
	pages = {99 – 102},
	doi = {10.1109/ICT-ISPC.2014.6923227},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911430168&doi=10.1109%2fICT-ISPC.2014.6923227&partnerID=40&md5=b32f31425562674b890f4d56cd3281c9},
	affiliations = {Department of Electrical and Computer Engineering, Faculty of Engineering, Naresuan University, Muang, Phitsanulok, 65000, Thailand},
	abstract = {The flower recognition system based on image processing has been developed. This system uses edge and color characteristics of flower images to classify flowers. Hu's seven-moment algorithm is applied to acquire edge characteristics. Red, green, blue, hue, and saturation characteristics are derived from histograms. K-nearest neighbor is used to classify flowers. The accuracy of this system is more than 80%. © 2014 IEEE.},
	author_keywords = {flower detection; flower recognition; graph cut; Hu's seven moments; image processing; K-nearest neighbor},
	keywords = {Image processing; Motion compensation; Nearest neighbor search; Flower detections; Flower recognition; Graph cut; Hu's seven moments; K-nearest neighbors; Graphic methods},
	editor = {Mitrpanont J.L.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147995572-5},
	language = {English},
	abbrev_source_title = {Proc. ICT Int. Sr. Proj. Conf., ICT-ISPC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27; Conference name: 2014 3rd ICT International Senior Project Conference, ICT-ISPC 2014; Conference date: 26 March 2014 through 27 March 2014; Conference code: 108697}
}

@ARTICLE{Nasir2014121,
	author = {Nasir, A. Fakhri A. and Rahman, M. Nordin A. and Mat, Nashriyah and Mamat, A. Rasid},
	title = {Automatic identification of ficus deltoidea Jack (Moraceae) varieties based on leaf},
	year = {2014},
	journal = {Modern Applied Science},
	volume = {8},
	number = {5},
	pages = {121 – 131},
	doi = {10.5539/mas.v8n5p121},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907184321&doi=10.5539%2fmas.v8n5p121&partnerID=40&md5=3ab4cbc441df3ce7b4c3d4984aee3bea},
	affiliations = {Faculty of Informatics and Computing, Universiti Sultan Zainal Abidin, Tembila Campus, Besut, Terengganu, 22200, Malaysia; Faculty of Agriculture, Biotechnology and Food Science, Universiti Sultan Zainal Abidin, Tembila Campus, Besut, Terengganu, 22200, Malaysia},
	abstract = {Currently, the traditional method used to identify Ficus deltoidea Jack (Moraceae) varieties require the plant taxonomists to observe and examine the leaf morphology of herbarium or live specimens. An automated variety identification system would ease the herbs collector to carry out valuable plant identification work. In this paper, a model for F. deltoidea varieties identification based on their leaf shape, color and texture was developed. Five different varieties of F. deltoidea were used in the proposed work with sixty nine sample data collected for each of varieties. First, the F. deltoidea leaves were plucked and the picture of leaves is then taken by a digital scanner in the format of JPEG. For leaf shape, a total of fourteen shape features were extracted based on basic geometric features. The mean of different color channels was calculated in leaf color feature extraction. Furthermore, four texture features based on gray-level co-occurrence matrix was implemented to extract leaf texture properties. By using the leaf structure, a set of three different leaf properties which are leaf shape, color and texture features was extracted. The features weight is then calculated using eigenvalues coefficient in principal component analysis. The best principal components are retained for identification experiments. Lastly, Nearest Neighbor with Euclidean distance was used in variety identification based on three different leaf properties mentioned above. The effectiveness of different leaf features are demonstrated in the identification experiment. © 2014 by the author(s).},
	author_keywords = {Content-based image retrieval; Ficus deltoidea Jack; Image processing; Pattern recognition; Plant leaf identification},
	publisher = {Canadian Center of Science and Education},
	issn = {19131844},
	language = {English},
	abbrev_source_title = {Mod. Appl. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Zhao20151112,
	author = {Zhao, Zhong-Qiu and Ma, Lin-Hai and Cheung, Yiu-ming and Wu, Xindong and Tang, Yuanyan and Chen, Chun Lung Philip},
	title = {ApLeaf: An efficient android-based plant leaf identification system},
	year = {2015},
	journal = {Neurocomputing},
	volume = {151},
	number = {P3},
	pages = {1112 – 1119},
	doi = {10.1016/j.neucom.2014.02.077},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918522479&doi=10.1016%2fj.neucom.2014.02.077&partnerID=40&md5=44b8d0da1af52af8a67f383ed2f8aeb7},
	affiliations = {College of Computer Science and Information Engineering, Hefei University of Technology, Hefei, 230009, China; Department of Computer Science, Hong Kong Baptist University, Hong Kong, China; United International College, Beijing Normal University, Zhuhai, China; Hong Kong Baptist University, Zhuhai, China; Department of Computer Science, University of Vermont, United States; Faculty of Science and Technology, University of Macau, Macau, Macao},
	abstract = {To automatically identify plant species is very useful for ecologists, amateur botanists, educators, and so on. The Leafsnap is the first successful mobile application system which tackles this problem. However, the Leafsnap is based on the IOS platform. And to the best of our knowledge, as the mobile operation system, the Android is more popular than the IOS. In this paper, an Android-based mobile application designed to automatically identify plant species according to the photographs of tree leaves is described. In this application, one leaf image can be either a digital image from one existing leaf image database or a picture collected by a camera. The picture should be a single leaf placed on a light and untextured background without other clutter. The identification process consists of three steps: leaf image segmentation, feature extraction, and species identification. The demo system is evaluated on the ImageCLEF2012 Plant Identification database which contains 126 tree species from the French Mediterranean area. The outputs of the system to users are the top several species which match the query leaf image the best, as well as the textual descriptions and additional images about plant leaves, flowers, etc. Our system works well with state-of-the-art identification performance. © 2014 Elsevier B.V.},
	author_keywords = {Android application; Feature fusion; Image retrieval; Plant identification},
	keywords = {Image Analysis; Leaves; Plants; Species Identification; Android (operating system); Forestry; Image retrieval; Image segmentation; Mobile computing; Plants (botany); Query processing; Trees (mathematics); Android applications; Feature fusion; Identification process; Mediterranean areas; Mobile applications; Plant identification; Species identification; Textual description; ApLeaf; Article; camera; comparative study; controlled study; data base; digital imaging; feature extraction; flower; image analysis; image processing; image quality; image retrieval; leaf image segmentation; mathematical analysis; mobile application; photography; plant identification; plant leaf; species identification; wavelet analysis; Search engines},
	publisher = {Elsevier B.V.},
	issn = {09252312},
	coden = {NRCGE},
	language = {English},
	abbrev_source_title = {Neurocomputing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 62}
}

@CONFERENCE{Casanova2013,
	author = {Casanova, Dalcimar and Backes, André Ricardo and Bruno, Odemir Martinez},
	title = {Pattern recognition tool based on complex network-based approach},
	year = {2013},
	journal = {Journal of Physics: Conference Series},
	volume = {410},
	number = {1},
	doi = {10.1088/1742-6596/410/1/012048},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875712121&doi=10.1088%2f1742-6596%2f410%2f1%2f012048&partnerID=40&md5=407bca5fc702a1f5fb5ad9f6dc8b0328},
	affiliations = {Instituto de Física de São Carlos (IFSC), Universidade de São Paulo, 13560-970 São Carlos, SP, Av. Trabalhador São Carlense, 400, Brazil; Faculdade de Computação, Universidade Federal de Uberlândia, 38408-100, MG Uberlândia, Av. João Naves de Ávila, 2121, Brazil},
	abstract = {This work proposed a generalization of the method proposed by the authors: 'A complex network-based approach for boundary shape analysis'. Instead of modelling a contour into a graph and use complex networks rules to characterize it, here, we generalize the technique. This way, the work proposes a mathematical tool for characterization signals, curves and set of points. To evaluate the pattern description power of the proposal, an experiment of plat identification based on leaf veins image are conducted. Leaf vein is a taxon characteristic used to plant identification proposes, and one of its characteristics is that these structures are complex, and difficult to be represented as a signal or curves and this way to be analyzed in a classical pattern recognition approach. Here, we model the veins as a set of points and model as graphs. As features, we use the degree and joint degree measurements in a dynamic evolution. The results demonstrates that the technique has a good power of discrimination and can be used for plant identification, as well as other complex pattern recognition tasks. © Published under licence by IOP Publishing Ltd.},
	keywords = {Integrated circuits; Pattern recognition; AS graph; Boundary shapes; Degree measurements; Dynamic evolution; Mathematical tools; Network-based approach; Pattern description; Plant identification; Complex networks},
	correspondence_address = {D. Casanova; Instituto de Física de São Carlos (IFSC), Universidade de São Paulo, 13560-970 São Carlos, SP, Av. Trabalhador São Carlense, 400, Brazil; email: dalcimar@gmail.com},
	publisher = {Institute of Physics Publishing},
	issn = {17426588},
	language = {English},
	abbrev_source_title = {J. Phys. Conf. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 1st International Conference on Mathematical Modelling in Physical Sciences, IC-MSQUARE 2012; Conference date: 3 September 2012 through 7 September 2012; Conference code: 96292; All Open Access, Bronze Open Access}
}

@ARTICLE{Lee20133106,
	author = {Lee, Hyo-Haeng and Li, Xianghua and Chung, Kwag-Woo and Hong, Kwang-Seok},
	title = {Flower image recognition using multi-class SVM},
	year = {2013},
	journal = {Applied Mechanics and Materials},
	volume = {284-287},
	pages = {3106 – 3110},
	doi = {10.4028/www.scientific.net/AMM.284-287.3106},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873927064&doi=10.4028%2fwww.scientific.net%2fAMM.284-287.3106&partnerID=40&md5=73fae32f4ad9b49af3d1ba3e0ac788e2},
	affiliations = {School of Information and Communication Engineering, Sungkyunkwan University, Suwon County, South Korea; Department of Railway Operation System Engineering, Korea National University of Transportation, Suwon County, South Korea},
	abstract = {This In this paper, a specific system is developed to recognize images of flower types. The proposed automatic flower boundary extraction method consists of two major procedures: the detection of four edge points and boundary tracing. Flower recognition includes two stages: feature extraction and matching. For the flower boundary extraction portion, we present a new technique for automatically identifying a flower's boundary in an image. For boundary tracing, an intelligent scissors algorithm is applied. The color gradient magnitude cost term is implemented so that it can act directly on the three components of the color image. Suggested extraction of the characteristics has used division of the image in three levels (level 1, level 2, and level 3), the RGB and YCbCr of each level, the minimum Euclidean distance value of eight colors, and the number of petals. Using multi-class SVM, this dissertation derived 97.07% recognition of thirteen different types of flower images. © (2013) Trans Tech Publications, Switzerland.},
	author_keywords = {Automatic boundary extraction; Feature extraction; Flower recognition; Multi-class SVM},
	keywords = {Extraction; Feature extraction; Image matching; Image recognition; Innovation; Boundary extraction; Boundary tracing; Color gradients; Color images; Edge point; Feature extraction and matching; Flower recognition; Intelligent scissors; Level 2; Level-1; Minimum euclidean distances; Multiclass SVM; Three component; Image processing},
	correspondence_address = {H.-H. Lee; School of Information and Communication Engineering, Sungkyunkwan University, Suwon County, South Korea; email: hyohaeng@skku.edu},
	issn = {16627482},
	isbn = {978-303785612-3},
	language = {English},
	abbrev_source_title = {Appl. Mech. Mater.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2nd International Conference on Engineering and Technology Innovation 2012, ICETI 2012; Conference date: 2 November 2012 through 6 November 2012; Conference code: 95485}
}

@ARTICLE{Arunpriya20156257,
	author = {Arunpriya, C. and Thanamani, Antony Selvadoss},
	title = {A new framework for tea plant recognition using extreme learning machine with very few features},
	year = {2015},
	journal = {International Journal of Applied Engineering Research},
	volume = {10},
	number = {3},
	pages = {6257 – 6270},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926443592&partnerID=40&md5=1d25907ce3a6de1068c08f5a93217fd0},
	affiliations = {P.S.G.R Krishnammal College for Women, Coimbatore, India; Nallamuthu Gounder Mahalingam College, Pollachi, India},
	abstract = {Due to more and more tea varieties in the current tea market, rapid and accurate identification of tea varieties is crucial for tea quality control. Tea quality mainly depends on the variety of leaf, growing environment, manufacturing conditions, size of ground tea leaves and infusion preparation. In the past few years, tea cultivar has been assessed by morphological assessment coupled with pattern recognition. This paper uses an efficient machine learning approach called Extreme Learning Machine (ELM) for the classification purpose. The proposed approach consists of four phases which are as preprocessing, feature extraction, feature clustering and classification. Additionally, this work proposes an iterative algorithm for feature clustering and applies it to leaf recognition. Feature clustering is a powerful tool to reduce the dimensionality of the selected feature. For improving the accuracy and performance of tea leaf recognition, ELM is implemented. The classifier is tested with 20 leaves from each variety and compared with k-NN and RBF approach. The proposed ELM classification produces effective results. © Research India Publications.},
	author_keywords = {Extreme learning Machine (ELM); Feature clustering and Classification; Feature extraction; Leaf recognition; Preprocessing},
	publisher = {Research India Publications},
	issn = {09734562},
	language = {English},
	abbrev_source_title = {Int. J. Appl. Eng. Res.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Şerban2013,
	author = {Şerban, Cristina and Siriţeanu, Alexandra and Gheorghiu, Claudia and Iftene, Adrian and Alboaie, Lenuţa and Breabən, Mihaela},
	title = {Combining image retrieval, metadata processing and naive Bayes classification at plant identification 2013},
	year = {2013},
	journal = {CEUR Workshop Proceedings},
	volume = {1179},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922051383&partnerID=40&md5=2f657ce3d0e59d32d8cd4b44ea553ecd},
	affiliations = {UAIC, Faculty of Computer Science, Alexandru Ioan Cuza University, Romania},
	abstract = {This paper aims to combine intuition and practical experience in the context of ImageCLEF 2013 Plant Identification task. We propose a flexible, modular system which allows us to analyse and combine the results after apply-ing methods such as image retrieval using LIRe, metadata clustering and naive Bayes classification. Although the training collection is quite extensive, cover-ing a large number of species, in order to obtain accurate results with our photo annotation algorithm we enriched our system with new images from a reliable source. As a result, we performed four runs with different configurations, and the best run was ranked 5th out of a total of 12 group participants.},
	author_keywords = {Image retrieval; ImageCLEF; Metadata pro-cessing; Naive Bayes; Plant identification},
	keywords = {Classifiers; Image processing; Image retrieval; Metadata; ImageCLEF; Modular system; Naive bayes; Naive Bayes classification; Number of species; Photo annotations; Plant identification; Practical experience; Search engines},
	editor = {Forner P. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Navigli R. and Tufis D.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2013 Cross Language Evaluation Forum Conference, CLEF 2013; Conference date: 23 September 2013 through 26 September 2013; Conference code: 110354}
}

@CONFERENCE{Haug20141142,
	author = {Haug, Sebastian and Michaels, Andreas and Biber, Peter and Ostermann, Jorn},
	title = {Plant classification system for crop /weed discrimination without segmentation},
	year = {2014},
	journal = {2014 IEEE Winter Conference on Applications of Computer Vision, WACV 2014},
	pages = {1142 – 1149},
	doi = {10.1109/WACV.2014.6835733},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904705432&doi=10.1109%2fWACV.2014.6835733&partnerID=40&md5=c29ee702081da98e3fb447bfad62c879},
	affiliations = {Robert Bosch GmbH, Corporate Research, Germany; Leibniz Universität Hannover, Germany},
	abstract = {This paper proposes a machine vision approach for plant classification without segmentation and its application in agriculture. Our system can discriminate crop and weed plants growing in commercial fields where crop and weed grow close together and handles overlap between plants. Automated crop / weed discrimination enables weed control strategies with specific treatment of weeds to save cost and mitigate environmental impact. © 2014 IEEE.},
	keywords = {Computer vision; Environmental impact; Weed control; Control strategies; ITS applications; Plant classification; Weed discrimination; Weed plants; Crops},
	publisher = {IEEE Computer Society},
	isbn = {978-147994985-4},
	language = {English},
	abbrev_source_title = {IEEE Winter Conf. Appl. Comput. Vis., WACV},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 113; Conference name: 2014 IEEE Winter Conference on Applications of Computer Vision, WACV 2014; Conference date: 24 March 2014 through 26 March 2014; Conference code: 106417}
}

@ARTICLE{Gaber2015375,
	author = {Gaber, Tarek and Tharwat, Alaa and Snasel, Vaclav and Hassanien, Aboul Ella},
	title = {Plant identification: Two dimensional-based vs. One dimensional-based feature extraction methods},
	year = {2015},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {368},
	pages = {375 – 385},
	doi = {10.1007/978-3-319-19719-7_33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946434160&doi=10.1007%2f978-3-319-19719-7_33&partnerID=40&md5=1b1859975f92810532dd3d8e2448eadd},
	affiliations = {Suez Canal University, Ismailia, Egypt; VSB-TU of Ostrava, It4innovations, Ostrava, Czech Republic; Cairo University, Giza, Egypt},
	abstract = {In this paper, a plant identification approach using 2D digital leaves images is proposed. The approach made use of two methods of features extraction (one-dimensional (1D) and two-dimensional (2D) techniques) and the Bagging classifier. For the 1D-based method, PCA and LDA techniques were applied, while 2D-PCA and 2D-LDA algorithms were used for the 2D-based method. To classify the extracted features in both methods, the Bagging classifier, with the decision tree as a weak learner, was used. The proposed approach, with its four feature extraction techniques, was tested using Flavia dataset which consists of 1907 colored leaves images. The experimental results showed that the accuracy and the performance of our approach, with the 2D-PCA and 2D-LDA, was much better than using the PCA and LDA. Furthermore, it was proven that the 2D-LDA-based method gave the best plant identification accuracy and increasing the weak learners of the Bagging classifier leaded to a better accuracy. Also, a comparison with the most related work showed that our approach achieved better accuracy under the same dataset and same experimental setup. © Springer International Publishing Switzerland 2015.},
	author_keywords = {2D-LDA; 2D-PCA; Bagging classifier; Decision tree; Leaf image; Leaves images; Linear discriminant analysis; Plant identification; Principal component analysis; Weak learners},
	keywords = {Decision trees; Discriminant analysis; Extraction; Face recognition; Feature extraction; Image processing; Learning systems; Plants (botany); Soft computing; 2D-LDA; 2D-PCA; Leaf images; Leaves images; Linear discriminant analysis; Plant identification; Weak learner; Principal component analysis},
	editor = {Department of Civil Engineering, Escuela Politecnica Superior, University of Burgos, Avenida de Cantabria S/N, Burgos, 09006 and Herrero A. and Baruque B. and Sedano J. and Technological Institute of Castilla y Leon, C/Lopez Bravo 70, Pol. Ind. Villalonquejar, Burgos, 09001 and University of Salamanca, Department of Computer Science and Automation, Plaza de La Merced, S/N, Salamanca, 37008 and Quintian H. and Corchado E.},
	publisher = {Springer Verlag},
	issn = {21945357},
	isbn = {978-331919718-0},
	language = {English},
	abbrev_source_title = {Adv. Intell. Sys. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; Conference name: 10th International Conference on Soft Computing Models in Industrial and Environmental Applications, SOCO 2015; Conference date: 15 June 2015 through 17 June 2015; Conference code: 154129}
}

@ARTICLE{Tsolakidis2014406,
	author = {Tsolakidis, Dimitris G. and Kosmopoulos, Dimitrios I. and Papadourakis, George},
	title = {Plant leaf recognition using Zernike moments and histogram of oriented gradients},
	year = {2014},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {8445 LNCS},
	pages = {406 – 417},
	doi = {10.1007/978-3-319-07064-3_33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900561116&doi=10.1007%2f978-3-319-07064-3_33&partnerID=40&md5=bf350a588f351c9c54627f468970573b},
	affiliations = {Department of Informatics Engineering, Technological Educational Institute of Crete, GR-71004, Heraklion, Greece},
	abstract = {A method using Zernike Moments and Histogram of Oriented Gradients for classification of plant leaf images is proposed in this paper. After preprocessing, we compute the shape features of a leaf using Zernike Moments and texture features using Histogram of Oriented Gradients and then the Support Vector Machine classifier is used for plant leaf image classification and recognition. Experimental results show that using both Zernike Moments and Histogram of Oriented Gradients to classify and recognize plant leaf image yields accuracy that is comparable or better than the state of the art. The method has been validated on the Flavia and the Swedish Leaves datasets as well as on a combined dataset. © 2014 Springer International Publishing.},
	author_keywords = {Histogram of oriented gradients; Leaf recognition; support vector machine; Zernike moments},
	keywords = {Artificial intelligence; Graphic methods; Image classification; Plants (botany); Support vector machines; Classification and recognition; Histogram of oriented gradients; Leaf recognition; Plant leaf images; State of the art; Support vector machine classifiers; Texture features; Zernike moments; Feature extraction},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331907063-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 48; Conference name: 8th Hellenic Conference on Artificial Intelligence: Methods and Applications, SETN 2014; Conference date: 15 May 2014 through 17 May 2014; Conference code: 105114}
}

@ARTICLE{Oncevay-Marcos2015326,
	author = {Oncevay-Marcos, Arturo and Juarez-Chambi, Ronald and Khlebnikov-Núñez, Sofía and Beltráón-Castañn, César},
	title = {Leaf-based plant identification through morphological characterization in digital images},
	year = {2015},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {9257},
	pages = {326 – 335},
	doi = {10.1007/978-3-319-23117-4_28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945968478&doi=10.1007%2f978-3-319-23117-4_28&partnerID=40&md5=156ce13f3e1f67385c32a1aedc2301bc},
	affiliations = {Department of Engineering, Research Group on Pattern Recognition and Applied Artificial Intelligence, Pontificia Universidad Católica Del Per, Lima, Peru},
	abstract = {The plant species identification is a manual process performed mainly by botanical scientists based on their experience. In order to improve this task, several plant classification processes has been proposed applying pattern recognition. In this work, we propose a method combining three visual attributes of leaves: boundary shape, texture and color. Complex networks and multi-scale fractal dimension techniques were used to characterize the leaf boundary shape, the Haralick’s descriptors for texture were extracted, and color moments were calculated. Experiments were performed on the ImageCLEF 2012 train dataset, scan pictures only. We reached up to 90.41% of accuracy regarding the leafbased plant identification problem for 115 species. © Springer International Publishing Switzerland 2015.},
	author_keywords = {Color moments; Complex networks; Haralick’s descriptors; Leaf-based plant identification; Multiscale fractal dimension},
	keywords = {Color; Complex networks; Fractal dimension; Fractals; Pattern recognition; Plants (botany); Color moments; Descriptors; Morphological characterization; Multiscale fractals; Plant classification; Plant identification; Plant species identification; Visual attributes; Image analysis},
	editor = {Azzopardi G. and Petkov N.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331923116-7},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 16th International Conference on Computer Analysis of  Images and Patterns, CAIP 2015; Conference date: 2 September 2015 through 4 September 2015; Conference code: 140469}
}

@ARTICLE{Kim2015,
	author = {Kim, Changwan and Son, Hyojoo and Kim, Changmin},
	title = {Fully automated as-built 3D pipeline extraction method from laser-scanned data based on curvature computation},
	year = {2015},
	journal = {Journal of Computing in Civil Engineering},
	volume = {29},
	number = {4},
	doi = {10.1061/(ASCE)CP.1943-5487.0000401},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932178153&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000401&partnerID=40&md5=f5b86c12548f09428dc082489c145bea},
	affiliations = {Dept. of Architectural Engineering, Chung-Ang Univ., Seoul, 156-756, South Korea},
	abstract = {There has been a growing demand for the three-dimensional (3D) reconstruction of as-built pipelines. The as-built 3D pipeline reconstruction process consists of the measurement of an industrial plant, identification of pipelines, and generation of 3D models of the pipelines. Although measurement is now efficiently performed using laser-scanning technology, and in spite of significant progress in 3D pipeline model generation, the identification of pipelines from large and complex sets of laser-scanned data continues to pose a challenge. The aim of this study is to propose a method to automatically extract 3D points corresponding to as-built pipelines that occupy large areas of industrial plants from laser-scanned data. The proposed extraction method consists of the following steps: preprocessing, segmentation of the 3D point cloud, feature extraction based on curvature computation, and pipeline classification. An experiment was performed at an operating industrial plant to validate the proposed method. The experimental result revealed that the proposed method can indeed contribute to the automation of as-built 3D pipeline reconstruction. © 2014 American Society of Civil Engineers.},
	author_keywords = {As-built pipeline; As-built reconstruction; Curvature computation; Industrial plant; Pipeline extraction},
	keywords = {3D modeling; Data mining; Extraction; Image reconstruction; Industrial plants; Three dimensional computer graphics; 3d pipeline modeling; 3D point cloud; As-built reconstruction; Extraction method; Fully automated; Laser scanning technology; Reconstruction process; Three-dimensional (3-D) reconstruction; Pipelines},
	correspondence_address = {C. Kim; Dept. of Architectural Engineering, Chung-Ang Univ., Seoul, 156-756, South Korea; email: changwan@cau.ac.kr},
	publisher = {American Society of Civil Engineers (ASCE)},
	issn = {08873801},
	coden = {JCCEE},
	language = {English},
	abbrev_source_title = {J. Comput. Civ. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 53}
}

@CONFERENCE{Paczolay2014685,
	author = {Paczolay, Dénes and Bánhalmi, András and Nyúl, László G. and Bilicki, Vilmos and Sárosi, Árpád},
	title = {Wlab of University of Szeged at LifeCLEF 2014 plant identification task},
	year = {2014},
	journal = {CEUR Workshop Proceedings},
	volume = {1180},
	pages = {685 – 692},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981244690&partnerID=40&md5=2455ccfbc9d63548d1b14ad14f81dd74},
	affiliations = {University of Szeged, Szeged, Hungary},
	abstract = {Our aim is to implement a plant identification application that can run on smartphones, and this shared task includes it. After the plant identification task of 2013 we concluded that the most frequent trees (e. g. in Hungary) can be identified well by a leaf, when there is a white paper background behind it at the time of photographing. This is why we want to classify more accurately this kind of pictures. Our other important goal is to develop a system which can be trained online while smartphone users take more and more photos. In the previous year there was a separate shared task for the 'Sheet as Background' photos, however this year the task is very complex, and supposes solutions for hard image processing problems, not just feature extraction and classification solutions.},
	author_keywords = {Contour and metric feature combination; Contour histograms; Gradient histograms in color channels; Leaf classification; Plant identification; Sheet as Background},
	keywords = {Feature extraction; Graphic methods; Image processing; Color channels; Contour histograms; Feature combination; Leaf classification; Plant identification; Sheet as Background; Smartphones},
	editor = {Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Halvey M. and Kraaij W.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2014 Cross Language Evaluation Forum Conference, CLEF 2014; Conference date: 15 September 2014 through 18 September 2014; Conference code: 110355}
}

@CONFERENCE{Patil2013519,
	author = {Patil, Bhagya and Pattanshetty, Anupama and Nandyal, Suvarna},
	title = {Plant classification using SVM classifier},
	year = {2013},
	journal = {IET Conference Publications},
	volume = {2013},
	number = {CP646},
	pages = {519 – 523},
	doi = {10.1049/cp.2013.2639},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907533440&doi=10.1049%2fcp.2013.2639&partnerID=40&md5=babe27538b1114029975a6aebef9db81},
	affiliations = {Dept. of CSE, APPAIET, Gulbarga, India; Dept. of ISE, APPAIET, Glb, India; Dept. of CSE, PDACEG, Gulbarga, Karnataka, India},
	abstract = {The plants play an important role in nature, this paper deals with the plant identification and classification. So, in this paper we are classifying the plants based on colour histogram, edge detection and direction features using support vector machine (SVM) classifier. The feature extraction helps in extracting the features which help in analysis of an image classification. Colour histogram feature is easy to compute and is effective in characterizing the distribution of the colour in an image. The Edge Histogram (EH) is the feature uses the sobel operator to capture the spatial distribution of edges. The edge histogram has eight bins corresponding to the sobel filters to count the number of edge pixels in eight directions. These features will be stored in database which contains 100 plant images. The feature vector is trained and tested with 100 plant images. The support vector machines (SVM) are superior of all machine learning algorithms and the overall percentage of the classification accuracy is 78%.},
	author_keywords = {Color histogram; Edge histogram; Feature extraction; Image acquisition; Support vector machine},
	keywords = {Artificial intelligence; Color; Edge detection; Extraction; Feature extraction; Graphic methods; Image acquisition; Image classification; Image processing; Image retrieval; Learning algorithms; Support vector machines; Vectors; Classification accuracy; Color histogram; Colour histograms; Direction features; Edge histogram; Plant classification; Plant identification; SVM classifiers; Classification (of information)},
	publisher = {Institution of Engineering and Technology},
	language = {English},
	abbrev_source_title = {IET Conf Publ},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 3rd International Conference on Computational Intelligence and Information Technology, CIIT 2013; Conference date: 18 October 2013 through 19 October 2013; Conference code: 114637}
}

@ARTICLE{Chaki201561,
	author = {Chaki, Jyotismita and Parekh, Ranjan and Bhattacharya, Samar},
	title = {Plant leaf recognition using texture and shape features with neural classifiers},
	year = {2015},
	journal = {Pattern Recognition Letters},
	volume = {58},
	pages = {61 – 68},
	doi = {10.1016/j.patrec.2015.02.010},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925386662&doi=10.1016%2fj.patrec.2015.02.010&partnerID=40&md5=e14376e23185d1f11eb229149fcd2d61},
	affiliations = {School of Education Technology, Jadavpur University, Kolkata, India},
	abstract = {Abstract This paper proposes a novel methodology of characterizing and recognizing plant leaves using a combination of texture and shape features. Texture of the leaf is modeled using Gabor filter and gray level co-occurrence matrix (GLCM) while shape of the leaf is captured using a set of curvelet transform coefficients together with invariant moments. Since these features are in general sensitive to the orientation and scaling of the leaf image, a pre-processing stage prior to feature extraction is applied to make corrections for varying translation, rotation and scaling factors. Efficacy of the proposed methods is studied by using two neural classifiers: a neuro-fuzzy controller (NFC) and a feed-forward back-propagation multi-layered perceptron (MLP) to discriminate between 31 classes of leaves. The features have been applied individually as well as in combination to investigate how recognition accuracies can be improved. Experimental results demonstrate that the proposed approach is effective in recognizing leaves with varying texture, shape, size and orientations to an acceptable degree. © 2015 Elsevier B.V. All rights reserved.},
	author_keywords = {Curvelet transform; Feed-forward back-propagation multi-layered perceptron; Gabor filter; Gray level co-occurrence matrix; Invariant moments; Neuro-fuzzy controller},
	keywords = {Backpropagation; Feature extraction; Fuzzy filters; Gabor filters; Plants (botany); Textures; Curvelet transforms; Gray level co-occurrence matrix; Invariant moment; Multi-layered Perceptron; Neuro-fuzzy controller; Fuzzy inference},
	publisher = {Elsevier},
	issn = {01678655},
	language = {English},
	abbrev_source_title = {Pattern Recogn. Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 189}
}

@CONFERENCE{Issolah2013,
	author = {Issolah, Mohamed and Lingrand, Diane and Precioso, Frederic},
	title = {SIFT, BoW architecture and one-against-all Support vector machine},
	year = {2013},
	journal = {CEUR Workshop Proceedings},
	volume = {1179},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922032581&partnerID=40&md5=ae07a9822780ca6b6e9ea743a3341d93},
	affiliations = {I3S Lab - UMR 7271 UNS-CNRS, 2000, route des Lucioles - Les Algorithmes - bt. Euclide B, Sophia Antipolis, 06900, France},
	abstract = {For this first participation to ImageClef Plant Identification, we build on the reference Bag-of-Word framework (BoW). We extract Points-of-Interest (PoI) using the SIFT detector in every image and de scribe each local feature with the SIFT descriptor. The visual dictionary is built with a K-means algorithm of 100 clusters on the local features. Each image is then represented by its histogram onto the dictionary using hard-assignment strategy. We classify the images with as many binary one-against-all Support Vector Machines as the number of plant classes per organ types. Our aim is to evaluate for the plant identification task a classic baseline of multi-class image categorization. Our first results illustrate how difficult this task is and that a framework which has be come a standard baseline for classifying general image datasets is not immediately relevant on Plant Identification data.},
	keywords = {Classification (of information); Image classification; Support vector machines; Assignment strategies; Image Categorization; Image datasets; k-Means algorithm; Plant identification; Points of Interest(POI); Sift descriptor; Visual dictionaries; Image processing},
	editor = {Forner P. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Navigli R. and Tufis D.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2013 Cross Language Evaluation Forum Conference, CLEF 2013; Conference date: 23 September 2013 through 26 September 2013; Conference code: 110354}
}

@ARTICLE{Zhao2015661,
	author = {Zhao, Sen and Zhang, Xiao-Ping and Shang, Li and Huang, Zhi-Kai and Zhu, Hao-Dong and Gan, Yong},
	title = {Implementation of leaf image recognition system based on LBP and B/S framework},
	year = {2015},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {9225},
	pages = {661 – 670},
	doi = {10.1007/978-3-319-22180-9_66},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943615379&doi=10.1007%2f978-3-319-22180-9_66&partnerID=40&md5=471b3fdec63ab255929858add696e188},
	affiliations = {College of Electronics and Information Engineering, Tongji University, 4800 Cao’an Highway, Jiading, Shanghai, China; Department of Communication Technology, College of Electronic Information Engineering, Suzhou Vocational University, Suzhou, Jiangsu, 215104, China; College of Mechanical and Electrical Engineering, Nanchang Institute of Technology, Nanchang, Jiangxi, 330099, China; College of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zhengzhou, China},
	abstract = {Plant identification system is on the basis of the previous, through continuous optimizing all aspects of the algorithm to improve efficiency and accuracy of the algorithm. For feature extraction, since the local binary pattern was proposed in the past decades, it has been widely used in computer vision to describe the feature for image classification such as image recognition, motion detection and medical image analysis. According to accuracy of the descriptor always fluctuates with different samples, some improved pattern of LBP has been presented in papers. Complete Local Binary Pattern (CLBP) is an optimized version which set an additional magnitude value to local differences. This paper shows extensive experiments of implement the LBP derivatives for plants texture identification. Finally realize an online system to identify what kind of the plant image user uploaded based on LBP descriptor. © Springer International Publishing Switzerland 2015.},
	author_keywords = {B/S; LBP; NNS; Online system; Texture classification},
	keywords = {Algorithms; Binary images; Bins; Computation theory; Computer vision; Feature extraction; Image recognition; Intelligent computing; Medical imaging; Online systems; Optimization; Descriptors; LBP; Leaf images; Local binary patterns; Motion detection; NNS; Plant identification systems; Texture classification; Image classification},
	editor = {Bevilacqua V. and Huang D.-S. and Premaratne P.},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {978-331922179-3},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 11th International Conference on Intelligent Computing, ICIC 2015; Conference date: 20 August 2015 through 23 August 2015; Conference code: 139679}
}

@CONFERENCE{Goëau2014598,
	author = {Goëau, Hervé and Joly, Alexis and Bonnet, Pierre and Selmi, Souheil and Molino, Jean-Franois and Barthélémy, Daniel and Boujemaa, Nozha},
	title = {LifeCLEF plant identification task 2014},
	year = {2014},
	journal = {CEUR Workshop Proceedings},
	volume = {1180},
	pages = {598 – 615},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981252354&partnerID=40&md5=7776d530d48f5934425ad69cb8584061},
	affiliations = {Inria ZENITH Team, France; LIRMM, Montpellier, France; CIRAD, UMR, AMAP, France; IRD, UMR, AMAP, France; CIRAD, BIOS Direction, INRA, UMR AMAP, F-34398, France; INRIA, Direction of Saclay Center, France},
	abstract = {The LifeCLEFs plant identification task provides a testbed for a system-oriented evaluation of plant identification about 500 species trees and herbaceous plants. Seven types of image content are considered: scan and scan-like pictures of leaf, and 6 kinds of detailed views with unconstrained conditions, directly photographed on the plant: flower, fruit, stem & bark, branch, leaf and entire view. The main originality of this data is that it was specifically built through a citizen sciences initiative conducted by Tela Botanica, a French social network of amateur and expert botanists. This makes the task closer to the conditions of a realworld application. This overview presents more precisely the resources and assessments of task, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results. With a total of ten groups from six countries and with a total of twenty seven submitted runs, involving distinct and original methods, this fourth year task confirms Image & Multimedia Retrieval community interest for biodiversity and botany, and highlights further challenging studies in plant identification.},
	author_keywords = {Bark; Benchmark; Branch; Collection; Evaluation; Fine-grained classification; Flower; Fruit; Identification; Images; Leaf; Leaves; LifeCLEF; Plant; Retrieval; Species; Stem},
	keywords = {Benchmarking; Biodiversity; Fruits; Identification (control systems); Image processing; Refuse collection; Bark; Branch; Evaluation; Fine grained; Flower; Images; Leaf; Leaves; LifeCLEF; Plant; Retrieval; Species; Stem; Plants (botany)},
	editor = {Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Halvey M. and Kraaij W.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 31; Conference name: 2014 Cross Language Evaluation Forum Conference, CLEF 2014; Conference date: 15 September 2014 through 18 September 2014; Conference code: 110355}
}

@CONFERENCE{Panwar2014,
	author = {Panwar, Rachana and Goyal, Kusha and Pandey, Nilay and Khanna, Nitin},
	title = {Imaging system for classification of local flora of Uttarakhand region},
	year = {2014},
	journal = {2014 International Conference on Power, Control and Embedded Systems, ICPCES 2014},
	doi = {10.1109/ICPCES.2014.7062815},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946686205&doi=10.1109%2fICPCES.2014.7062815&partnerID=40&md5=fe1d5cc40865f14fe700075f314d42ce},
	affiliations = {Department of Electronics and Communication Engineering, Graphic Era University, Dehradun, Uttarakhand, India},
	abstract = {Many plants are facing the risk of extinction due to unplanned urbanization and over growth of population. Digital databases of plants should be maintained for proper tracking of local flora and making data-driven policies/decisions for their preservation. Plant identification is important for medical as well as educational purposes but maintaining an exhaustive digital database is a challenging task due to the presence of large number of plant species. This paper proposes a system for building a digital database of local flora and recognizing different plants using their leaf images. The system proposed in this paper involves four steps: 1) image acquisition, 2) image preprocessing, 3) feature extraction, and 4) classification. Images are acquired using commonly available general purpose desktop scanner with white paper as a background. In the image-preprocessing module, the system applies several image-processing techniques to prepare a leaf image for the feature extraction process. Then twelve leaf-shape based features are estimated and IBI classifier is used to classify the plant species. The proposed system was used to build a dataset of local flora of Uttarakhand region, consisting of 1684 images of thirty-two different plant species. The database contains around fifty leaves of each plant species. The proposed system gives promising results with an average classification accuracy of 79% for these thirty species of plants. © 2014 IEEE.},
	author_keywords = {Feature Extraction; IBI; Image Pre-processing; Plant Identification; Shape-based features},
	keywords = {Classification (of information); Database systems; Embedded systems; Extraction; Feature extraction; Image acquisition; Plants (botany); Population statistics; Classification accuracy; Desktop scanners; Digital database; IBI; Image preprocessing; Image processing technique; Plant identification; Shape based features; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147995912-9},
	language = {English},
	abbrev_source_title = {Int. Conf. Power, Control Embed. Syst., ICPCES},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 2014 International Conference on Power, Control and Embedded Systems, ICPCES 2014; Conference date: 26 December 2014 through 28 December 2014; Conference code: 111616}
}

@CONFERENCE{Goëau2013,
	author = {Goëau, Hervé and Bonnet, Pierre and Joly, Alexis and Bakic, Vera and Barthelemy, Daniel and Boujemaa, Nozha and Molino, Jean-François},
	title = {The ImageCLEF 2013 plant identification task},
	year = {2013},
	journal = {CEUR Workshop Proceedings},
	volume = {1179},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922032510&partnerID=40&md5=679409dec1aaf08b483506202be2006b},
	affiliations = {INRIA, IMEDIA and ZENITH Teams, France; CIRAD, UMR AMAP, France; CIRAD, BIOS Direction and INRA, UMR AMAP, F-34398, France; IRD, UMR AMAP, France; INRIA, Department of Saclay Center, France},
	abstract = {The ImageCLEFs plant identification task provides a testbed for a system-oriented evaluation of plant identification about 250 species trees and herbaceous plants based on detailed views of leaves, owers, fruits, stems and bark or some entire views of the plants. Two types of image content are considered: SheetAsBackgroud which contains only leaves in a front of a generally white uniform background, and NaturalBack ground which contains the 5 kinds of detailed views with unconstrained conditions, directly photographed on the plant. The main originality of this data is that it was specifically built through a citizen sciences initiative conducted by Tela Botanica, a French social network of amateur and expert botanists. This makes the task closer to the conditions of a realworld application. This overview presents more precisely the resources and assessments of task, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results. With a total of twelve groups from nine countries and with a total of thirty three runs submitted, involving distinct and original methods, this third year task confirms Image Retrieval community interest for biodiversity and botany, and highlights further challenging studies in plant identification.},
	author_keywords = {Bark; Benchmark; Collection; Evaluation; Fine-grained classification; Flowers; Fruits; Identification; ImageCLEF; Images; Leaf; Leaves; Plant; Retrieval; Species; Stem},
	keywords = {Benchmarking; Biodiversity; Fruits; Identification (control systems); Image processing; Image retrieval; Refuse collection; Bark; Evaluation; Fine grained; Flowers; ImageCLEF; Images; Leaf; Leaves; Plant; Retrieval; Species; Stem; Plants (botany)},
	editor = {Forner P. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Navigli R. and Tufis D.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 2013 Cross Language Evaluation Forum Conference, CLEF 2013; Conference date: 23 September 2013 through 26 September 2013; Conference code: 110354}
}

@CONFERENCE{Rasche2013,
	author = {Rasche, Christoph and Florea, Laura and Vertan, Constantin},
	title = {Has an image classification approach any chance at all (in plant classification)?},
	year = {2013},
	journal = {CEUR Workshop Proceedings},
	volume = {1179},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922032633&partnerID=40&md5=db6b0949332606a3fe8de5b42168b3a0},
	affiliations = {Universitatea Politehnica Bucuresti, Bucuresti, 061071, Romania},
	abstract = {Somewhat. We extracted, partitioned and described con tours, histogrammed their geometric parameters and concatenated the histograms to form a single image vector with which we classified the plant images using a Linear Discriminant Analysis (LDA); that is, no segmentation or saliency selection was performed. Despite the obvious simplicity of the LDA classification we reached the middle of the ranking for sheet-as-background images. While contour-based feature extraction is presently still a lone-some strategy in comparison to the prevailing gradient-based matching techniques (e.g. SIFT), it may soon be a viable alternative - once we developed appropriate classification methodology to deal with the descriptors.},
	author_keywords = {Contour extraction; Curve partitioning; Grouping; Image vector},
	keywords = {Discriminant analysis; Extraction; Face recognition; Feature extraction; Graphic methods; Image processing; Image segmentation; Classification approach; Classification methodologies; Contour Extraction; Curve partitioning; Grouping; Image vectors; Linear discriminant analysis; Plant classification; Image classification},
	editor = {Forner P. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Navigli R. and Tufis D.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2013 Cross Language Evaluation Forum Conference, CLEF 2013; Conference date: 23 September 2013 through 26 September 2013; Conference code: 110354}
}

@CONFERENCE{Jamil2015436,
	author = {Jamil, Nursuriati and Hussin, Nuril Aslina Che and Nordin, Sharifalillah and Awang, Khalil},
	title = {Automatic Plant Identification: Is Shape the Key Feature?},
	year = {2015},
	journal = {Procedia Computer Science},
	volume = {76},
	pages = {436 – 442},
	doi = {10.1016/j.procs.2015.12.287},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962815778&doi=10.1016%2fj.procs.2015.12.287&partnerID=40&md5=6d50652abef13d13b8210b2e0f50e480},
	affiliations = {Digital Image, Audio and Speech Technology Group (DIAST), Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, 40450, Malaysia},
	abstract = {Shape is the most popular feature used in plant leaf identification, be it manual or automatic plant identification. In this paper, a study is conducted to investigate the most contributing features among three low-level features for plant leaf identification. Intra- and inter-class identification are conducted using 455 herbal medicinal plant leaves, with 70% allocated for training and 30% for testing dataset. Shape feature is extracted using Scale Invariant Feature Transform (SIFT); colour is represented using colour moments; and Segmentation-Based Fractal Texture Analysis (SFTA) is utilized to describe texture feature. Intra-class analysis showed that fusion of texture and shape surpassed fusion of texture, shape and colour. Single texture feature identification also achieved highest identification rate compared to identification using colour or shape. Inter-class analysis further support texture to be the discriminative feature among the low-level features. Results demonstrate that single texture feature outperformed colour or shape feature achieving 92% identification rate. Furthermore, fusion of all three features accomplished 94% identification rate. © 2015 The Authors.},
	author_keywords = {colour moment; feature extraction; fractal texture analysis; plant identification; SIFT},
	keywords = {Color; Feature extraction; Fractals; Intelligent control; Robotics; Smart sensors; Statistical tests; Colour moments; Discriminative features; Identification rates; Low-level features; Plant identification; Scale invariant feature transforms; SIFT; Texture analysis; Plants (botany)},
	correspondence_address = {N. Jamil; Digital Image, Audio and Speech Technology Group (DIAST), Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, 40450, Malaysia; email: lizajamil@computer.org},
	editor = {Yussof H. and Miskon M.F.},
	publisher = {Elsevier B.V.},
	issn = {18770509},
	language = {English},
	abbrev_source_title = {Procedia Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 39; Conference name: IEEE International Symposium on Robotics and Intelligent Sensors, IEEE IRIS 2015; Conference date: 18 October 2015 through 20 October 2015; Conference code: 123218; All Open Access, Bronze Open Access}
}

@CONFERENCE{Reyes2015,
	author = {Reyes, Angie K. and Caicedo, Juan C. and Camargo, Jorge E.},
	title = {Fine-tuning deep convolutional networks for plant recognition},
	year = {2015},
	journal = {CEUR Workshop Proceedings},
	volume = {1391},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982808821&partnerID=40&md5=ae4468523e49ee66798593d00d12c359},
	affiliations = {Laboratory for Advanced Computational Science and Engineering Research, Universidad Antonio Nariño, Colombia; Fundación Universitaria Konrad Lorenz, Colombia},
	abstract = {This paper describes the participation of the ECOUAN team in the LifeCLEF 2015 challenge. We used a deep learning approach in which the complete system was learned without hand-engineered components. We pre-trained a convolutional neural network using 1.8 million images and used a fine-tuning strategy to transfer learned recognition capabilities from general domains to the specific challenge of Plant Identification task. The classification accuracy obtained by our method outperformed the best result obtained in 2014. Our group obtained the 4th position among all teams and the 10th position among 18 runs.},
	author_keywords = {Convolutional neural networks; Deep learning; Image retrieval; ImageCLEF; Plant recognition},
	keywords = {Image retrieval; Neural networks; Classification accuracy; Convolutional networks; Convolutional neural network; Deep learning; Engineered components; ImageCLEF; Plant identification; Plant recognition; Convolution},
	editor = {Jones G.J.F. and Cappellato L. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and San Juan E.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 74; Conference name: 16th Conference and Labs of the Evaluation Forum, CLEF 2015; Conference date: 8 September 2015 through 11 September 2015; Conference code: 122644}
}

@CONFERENCE{Dhawale2014,
	author = {Dhawale, V.R. and Tidke, J.A. and Dudul, S.V.},
	title = {Efficient classification of pollen grains using computational intelligence approach},
	year = {2014},
	journal = {2014 International Conference for Convergence of Technology, I2CT 2014},
	doi = {10.1109/I2CT.2014.7092120},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949929301&doi=10.1109%2fI2CT.2014.7092120&partnerID=40&md5=0f5892bec4b24bf13b23e67fc79df75c},
	affiliations = {Laboratory of Artificial Intelligence, Dept. of Applied Electronics, Sant Gadge Baba Amravati University, Amravati, Maharashtra, India; Laboratory of Reproductive Biology of Angiosperm, Dept. of Botany, Sant Gadge Baba Amravati University, Amravati, Maharashtra, India},
	abstract = {Pollen grains play an important role in classification system of plants. Pollen morphological characters are very useful in plant classification and identification of plants. Pollen studies are widely used in allergy and epidemiology research, fossil fuel exploration, forensic science, food, pharmaceutical, and cosmetic industry, biotechnology, and so many other fields. Hence classification of pollen grains is a challenge that can be effectively done by computational intelligence approach. The traditional method of pollen classification analyses the pollen morphological characters using microscopy. This procedure is tedious and requires experts from the field of palynology. With a view to extract features from pollen images, a new classification strategy is developed which proposes Image Histogram coefficients in addition to image statistics and shape descriptor. The suitability of classifiers based on Multilayer Perceptron (MLP) Neural Network and Support Vector Machine (SVM) is explored with the optimization of their respective parameters in view of reduction in time as well as space complexity. Performance of two classifiers has been compared with respect to MSE, NMSE, and Classification accuracy. The Average Classification Accuracy of MLP comprising of two hidden layers is found to be superior (95 % on Cross Validation dataset) to SVM based classifier. Finally, optimal Classification strategy has been developed, which could be easily modified to classify more than 10 species. The proposed strategy will provide an effective alternative to traditional method of pollen image analysis for plant taxonomy and species identification, which is obvious from the cross-validation performance on the dataset containing ten different plant species. © 2014 IEEE.},
	author_keywords = {Classifier; Computational Intelligence; Multi-layer Perceptron Neural Network; palynology; Pollen SEM images; Support Vector Machine},
	keywords = {Artificial intelligence; Classifiers; Complex networks; Image processing; Image retrieval; Morphology; Network layers; Plants (botany); Support vector machines; Vector spaces; Classification accuracy; Fossil fuel exploration; Morphological characters; Multi-layer perceptron neural networks; Optimal classification; palynology; SEM image; Species identification; Classification (of information)},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147993759-2},
	language = {English},
	abbrev_source_title = {Int. Conf. Converg. Technol., ICT},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2014 International Conference for Convergence of Technology, I2CT 2014; Conference date: 6 April 2014 through 8 April 2014; Conference code: 112024}
}

@ARTICLE{Arunpriya2015179,
	author = {Arunpriya, C. and Thanamani, Antony Selvadoss},
	title = {Fuzzy inference system algorithm of plant classification for tea leaf recognition},
	year = {2015},
	journal = {Indian Journal of Science and Technology},
	volume = {8},
	pages = {179 – 184},
	doi = {10.17485/ijst/2015/v8iS7/64304},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928790894&doi=10.17485%2fijst%2f2015%2fv8iS7%2f64304&partnerID=40&md5=ad06129780c4bc3223d40c9c3541d8b6},
	affiliations = {Nallamuthu Gounder Mahalingam College, Pollachi, India},
	abstract = {Background/Objectives: Biologists found that the morphological, physiological, bio-chemical and molecular methods of plant identification are found to be laborious and require great amount of technical knowledge. This research paper concentrates on the identification of varieties of tea using leaf images. It aims to identify the species in an easy and an accurate manner. Methods/Statistical analysis: The phases involved in this work are image pre processing, feature extraction and classification. Three classification algorithms such as Fuzzy Inference system, Radial basis function network and K-nearest neighbour were used and optimized to achieve a better accuracy and execution time. Results/Findings: The classification algorithm K-nearest neighbor, Radial basis function neural network and Fuzzy Inference System are trained with 40 samples and tested using 20 samples. Conclusions: Fuzzy Inference System has a better accuracy and a lesser execution time, when compared to the other classification algorithm.},
	author_keywords = {Classification algorithm; Fuzzy inference system (FIS); Leaf recognition; Pre-processing},
	publisher = {Indian Society for Education and Environment},
	issn = {09746846},
	language = {English},
	abbrev_source_title = {Indian J. Sci. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Maheswari2014415,
	author = {Maheswari, A. and Bharathi, N. and Neelamegam, P. and Gayathridevi, T.},
	title = {Classification and recognition of herbal leaf using SVM algorithm},
	year = {2014},
	journal = {Research Journal of Pharmaceutical, Biological and Chemical Sciences},
	volume = {5},
	number = {5},
	pages = {415 – 423},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907156693&partnerID=40&md5=e98a6fdbd4c8eac6b166a086e465e119},
	affiliations = {School of Computing, SASTRA University, Thanjavur, Tamil Nadu, India; School of Electrical and Electronics Engineering, SASTRA University, Thanjavur, Tamil Nadu, India; Dept of ECE, SRC, SASTRA UNIVERSITY, Kumbakonam, Tamil Nadu, India},
	abstract = {Ayurveda is an ancient holistic healing system, which is nearly 5000 years old that uses natural herbal leaves to heal various diseases. Herbal leaves have been widely utilized by the industries like food, medical and cosmetic. But most of the herbal plants are endangered. So, there is a need to identify and regrow them for the use of future generations. So it is very indispensable to develop an automatic system to identify species correctly. This paper proposes an algorithm for recognition of herb leaf through image processing and classification technique using Support Vector Machine (SVM) classifier. The scale invariant feature transform (SIFT) method is used for the feature extraction in leaves. The work is carried out using 5 species of leaves in MATLAB environment. SVM classifier is trained with 273 samples and tested with 129 samples of herbal and non-herbal leaves are presented in this paper. The output showed that the system reached the recognition rate with 92.25% accuracy. The system is convenient to use and cost effective for the botanist, Scientist and Researcher to identify herbal leaves.},
	author_keywords = {Classification; Herbal leaves recognition; Image processing; SIFT features; SVM classifier},
	keywords = {accuracy; Article; classification algorithm; controlled study; image analysis; image processing; image quality; leaf morphology; mathematical computing; mathematical model; nonhuman; plant leaf; process development; species comparison; support vector machine},
	publisher = {Research Journal of Pharmaceutical, Biological and Chemical Sciences},
	issn = {09758585},
	language = {English},
	abbrev_source_title = {Res. J. Pharm., Biol. Chem. Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@CONFERENCE{Jassmann2015,
	author = {Jassmann, Timothy J. and Tashakkori, Rahman and Parry, R. Mitchell},
	title = {Leaf classification utilizing a convolutional neural network},
	year = {2015},
	journal = {Conference Proceedings - IEEE SOUTHEASTCON},
	volume = {2015-June},
	number = {June},
	doi = {10.1109/SECON.2015.7132978},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938099705&doi=10.1109%2fSECON.2015.7132978&partnerID=40&md5=64a0b765554e43eade1c3ee5191c4a2c},
	affiliations = {Department of Computer Science, Appalachian State University, Boone, 28608, NC, United States},
	abstract = {Tree identification is an important task in biological research. Since not all biologists are qualified for this task and experts are not accessible all the time, an application to assist in plant identification would be extremely useful. This paper describes an application classifies the type of a tree based on a picture of one of its leaves. The system developed for this research utilizes a convolutional neural network in an android mobile application to classify natural images of leaves. © 2015 IEEE.},
	author_keywords = {convolutional neural network; image processing; leaf classification},
	keywords = {Convolution; Image classification; Image processing; Plants (botany); Biological research; Leaf classification; Mobile applications; Natural images; Plant identification; Tree identification; Tree-based; Convolutional neural networks},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {07347502},
	isbn = {978-146737300-5},
	coden = {CPISD},
	language = {English},
	abbrev_source_title = {Conf Proc IEEE SOUTHEASTCON},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; Conference name: IEEE SoutheastCon 2015; Conference date: 9 April 2015 through 12 April 2015; Conference code: 113116}
}

@CONFERENCE{Hsiao2014209,
	author = {Hsiao, Jou-Ken and Kang, Li-Wei and Chang, Ching-Long and Hsu, Chao-Yung and Chen, Chia-Yen},
	title = {Learning sparse representation for leaf image recognition},
	year = {2014},
	journal = {Digest of Technical Papers - IEEE International Conference on Consumer Electronics},
	pages = {209 – 210},
	doi = {10.1109/ICCE-TW.2014.6904061},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907734965&doi=10.1109%2fICCE-TW.2014.6904061&partnerID=40&md5=9d5cd1241bbf1b9b15c1f52f28db65a1},
	affiliations = {Department of Computer Science and Information Engineering, National Yunlin University of Science and Technology, Yunlin, Taiwan; Graduate School of Engineering Science and Technology-Doctoral Program, National Yunlin University of Science and Technology, Yunlin, Taiwan; Institute of Information Science, Academia Sinica, Taipei, Taiwan; Department of Computer Science and Information Engineering, National University of Kaohsiung, Taiwan},
	abstract = {Automatic plant identification via computer vision techniques has been greatly important for a number of professionals, such as environmental protectors, land managers, and foresters. In this paper, a novel leaf image recognition technique via sparse representation is proposed for automatic plant identification. In order to model leaf images, we learn an overcomplete dictionary for sparsely representing the training images of each leaf species. Each dictionary is learned using a set of descriptors extracted from the training images in such a way that each descriptor is represented by linear combination of a small number of dictionary atoms. For each test leaf image, we calculate the correlation between the image and each learned dictionary of leaf species to achieve the identification of the leaf image. As a result, efficient leaf recognition can be achieved on public leaf dataset based on the proposed framework leading to a more compact and richer representation of leaf images compared to traditional clustering approaches. Moreover, our method is also adapted to newly added leaf species without retraining classifiers and suitable to be highly parallelized as well as integrated with any leaf image descriptors/features. © 2014 IEEE.},
	keywords = {Image recognition; Personnel training; Computer vision techniques; Environmental protector; Learned dictionaries; Linear combinations; Over-complete dictionaries; Plant identification; Sparse representation; Traditional clustering; Image processing},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {0747668X},
	isbn = {978-147993830-8},
	coden = {DTPEE},
	language = {English},
	abbrev_source_title = {Dig Tech Pap IEEE Int Conf Consum Electron},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; Conference name: 1st IEEE International Conference on Consumer Electronics - Taiwan, ICCE-TW 2014; Conference date: 26 May 2014 through 28 May 2014; Conference code: 107374}
}

@ARTICLE{Rahman2015135,
	author = {Rahman, M. Nordin A. and Nasir, Ahmad Fakhri Ab. and Mat, Nashriyah and Rasid Mamat, A.},
	title = {Image segmentation using open MP and its application in plant species classification},
	year = {2015},
	journal = {International Journal of Software Engineering and its Applications},
	volume = {9},
	number = {5},
	pages = {135 – 144},
	doi = {10.14257/ijseia.2015.9.5.13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930883055&doi=10.14257%2fijseia.2015.9.5.13&partnerID=40&md5=df891978258ff88cd158b3606f396ea8},
	affiliations = {Faculty of Informatics and Computing, Universiti Sultan Zainal Abidin, Tembila Campus, Besut, Terengganu, 22200, Malaysia; Faculty of Agriculture, Biotechnology and Food Science, Universiti Sultan Zainal Abidin, Tembila Campus, Besut, Terengganu, 22200, Malaysia},
	abstract = {Segmentation is very important in early stage of image processing pipelines. Final results of image processing are strongly depending on the initial image segmentation quality. A good quality result often comes at the price of high computational cost including computation speed. Image segmentation requires long computation task caused by sequential processing of huge sizes of image and complex tasks. Nowadays, multi-core architectures are emerging as an attractive platform for parallel processing because it has two or more independent cores in a single physical package and their comparatively low cost. In this paper, two parallelization strategies (fine-grain and coarse-grain approach) are proposed for computing leaf image segmentation. The Canny Edge Detector and Otsu thresholding methods are used due to their wide range of usage for leaf segmentation in plant classification. The implementation is developed under multi-core architecture with shared memory multiprocessors. The OpenMP (Open Multi-Processing), an API (Application Programming Interface) is utilized for writing multi-threaded applications in shared memory architecture. The comparative study with two parallelization strategies is discussed further in this paper. © 2015 SERSC.},
	author_keywords = {Image processing; Image segmentation; Leaf shape based classification; OpenMP; Parallel processing},
	publisher = {Science and Engineering Research Support Society},
	issn = {17389984},
	language = {English},
	abbrev_source_title = {Int. J. Softw. Eng. Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Wang2013,
	author = {Wang, Hang-jun and Zhang, Guang-qun and Qi, Heng-nian},
	title = {Wood Recognition Using Image Texture Features},
	year = {2013},
	journal = {PLoS ONE},
	volume = {8},
	number = {10},
	doi = {10.1371/journal.pone.0076101},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885395589&doi=10.1371%2fjournal.pone.0076101&partnerID=40&md5=99ea9b7281386f160c6ff5bdac9c9f79},
	affiliations = {College of Tianmu, Zhejiang AandF University, Lin'an, China; Hefei Institute of Intelligent Machines, Chinese Academy of Science, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; School of Information and Technology, Zhejiang AandF University, Lin'an, China},
	abstract = {Inspired by theories of higher local order autocorrelation (HLAC), this paper presents a simple, novel, yet very powerful approach for wood recognition. The method is suitable for wood database applications, which are of great importance in wood related industries and administrations. At the feature extraction stage, a set of features is extracted from Mask Matching Image (MMI). The MMI features preserve the mask matching information gathered from the HLAC methods. The texture information in the image can then be accurately extracted from the statistical and geometrical features. In particular, richer information and enhanced discriminative power is achieved through the length histogram, a new histogram that embodies the width and height histograms. The performance of the proposed approach is compared to the state-of-the-art HLAC approaches using the wood stereogram dataset ZAFU WS 24. By conducting extensive experiments on ZAFU WS 24, we show that our approach significantly improves the classification accuracy. © 2013 Wang et al.},
	keywords = {Algorithms; Artificial Intelligence; Databases, Factual; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Wood; analytic method; article; extraction; geometry; height; higher local order autocorrelation; histogram; image analysis; recognition; stereoradiography; wood},
	correspondence_address = {H.-J. Wang; College of Tianmu, Zhejiang AandF University, Lin'an, China; email: hangjunw@mail.ustc.edu.cn},
	issn = {19326203},
	coden = {POLNC},
	pmid = {24146821},
	language = {English},
	abbrev_source_title = {PLoS ONE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 29; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Elhariri2014271,
	author = {Elhariri, Esraa and El-Bendary, Nashwa and Hassanien, Aboul Ella},
	title = {Plant classification system based on leaf features},
	year = {2014},
	journal = {Proceedings of 2014 9th IEEE International Conference on Computer Engineering and Systems, ICCES 2014},
	pages = {271 – 276},
	doi = {10.1109/ICCES.2014.7030971},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946688740&doi=10.1109%2fICCES.2014.7030971&partnerID=40&md5=8c4f7dca64966478358e3df988c0051d},
	affiliations = {Faculty of Computer Sciences and Information, Fayoum University, 7030971, Egypt; Arab Academy for Science, Technology, and Maritime Transport, Cairo, Egypt; Faculty of Computers and Information, Cairo University, Cairo, Egypt},
	abstract = {This paper presents a classification approach based on Random Forests (RF) and Linear Discriminant Analysis (LDA) algorithms for classifying the different types of plants. The proposed approach consists of three phases that are pre-processing, feature extraction, and classification phases. Since most types of plants have unique leaves, so the classification approach presented in this research depends on plants leave. Leaves are different from each other by characteristics such as the shape, color, texture and the margin. The used dataset for this experiments is a database of different plant species with total of only 340 leaf images, was downloaded from UCI- Machine Learning Repository. It was used for both training and testing datasets with 10-fold cross-validation. Experimental results showed that LDA achieved classification accuracy of (92.65%) against the RF that achieved accuracy of (88.82%) with combination of shape, first order texture, Gray Level Co-occurrence Matrix (GLCM), HSV color moments, and vein features. © 2014 IEEE.},
	author_keywords = {features extraction; gray level co-occurrence matrix (GLCM); image classification; leaves; linear discriminant analysis (LDA); plants; random forests (RF)},
	keywords = {Artificial intelligence; Decision trees; Discriminant analysis; Extraction; Feature extraction; Image classification; Learning systems; Matrix algebra; Plants (botany); Textures; Features extraction; Gray level co occurrence matrix(GLCM); leaves; Linear discriminant analysis; plants; Random forests; Classification (of information)},
	editor = {Wahba A.M. and El-Kharashi M.W. and Taher M. and Bahaa El-Din A.M. and Zaki A.M.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147996594-6},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Comput. Eng. Syst., ICCES},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 42; Conference name: 2014 9th IEEE International Conference on Computer Engineering and Systems, ICCES 2014; Conference date: 22 December 2014 through 23 December 2014; Conference code: 110743}
}

@CONFERENCE{Aptoula20131496,
	author = {Aptoula, E. and Yanikoglu, B.},
	title = {Morphological features for leaf based plant recognition},
	year = {2013},
	journal = {2013 IEEE International Conference on Image Processing, ICIP 2013 - Proceedings},
	pages = {1496 – 1499},
	doi = {10.1109/ICIP.2013.6738307},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897798384&doi=10.1109%2fICIP.2013.6738307&partnerID=40&md5=09058a77a35ccb9420bbe3dd021af61f},
	affiliations = {Okan University, Computer Engineering Department, 34959, Istanbul, Turkey; Sabanci University, Computer Engineering Department, 34956, Istanbul, Turkey},
	abstract = {Although plant recognition has become an increasingly popular research topic, it remains nonetheless a scientific and technical challenge. Besides all the difficulties of classic object recognition, such as illumination, viewpoint and scale variations, plants can additionally exhibit visual changes depending on their age and condition, thus demanding a specialized approach. In this paper, we present two descriptors based on mathematical morphology; the first consists of the computation of morphological covariance on the leaf contour profile and the second is an extension of the recently introduced circular covariance histogram, capturing leaf venation characteristics. The effectiveness of both descriptors has been validated with the ImageClef'12 plant identification dataset. © 2013 IEEE.},
	author_keywords = {circular covariance histogram; feature extraction; mathematical morphology; morphological covariance; Plant recognition},
	keywords = {Feature extraction; Graphic methods; Morphology; Object recognition; circular covariance histogram; Leaf venation; Morphological covariance; Morphological features; Plant identification; Plant recognition; Research topics; Technical challenges; Mathematical morphology},
	publisher = {IEEE Computer Society},
	isbn = {978-147992341-0},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Image Process., ICIP - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 36; Conference name: 2013 20th IEEE International Conference on Image Processing, ICIP 2013; Conference date: 15 September 2013 through 18 September 2013; Conference code: 115163}
}@ARTICLE{Cheng2010148,
	author = {Cheng, Cungui and Liu, Jia and Zhang, Changjiang and Cai, Miaozhen and Wang, Hong and Xiong, Wei},
	title = {An overview of infrared spectroscopy based on continuous wavelet transform combined with machine learning algorithms: Application to chinese medicines, plant classification, and cancer diagnosis},
	year = {2010},
	journal = {Applied Spectroscopy Reviews},
	volume = {45},
	number = {2},
	pages = {148 – 164},
	doi = {10.1080/05704920903435912},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949548139&doi=10.1080%2f05704920903435912&partnerID=40&md5=e15d2205e8f3e84bac7249008abb35b5},
	affiliations = {Department of Chemistry, Zhejiang Normal University, Jinhua, Zhejiang 321004, China; Department of Electronics and Information Engineering, Zhejiang Normal University, Jinhua, Zhejiang, China; Department of Biology, Zhejiang Normal University, Jinhua, Zhejiang, China},
	abstract = {Infrared spectroscopy has been a workhorse technique for materials analysis and can result in positively identifying many different types of material. In recent years there have been reports using wavelet analysis and machine learning algorithms to extract features of Fourier transform infrared spectrometry (FTIR). The machine learning algorithms contain back-propagation neural network (BPNN), radial basis function neural network (RBFNN), and support vector machine (SVM). This article reviews the important advances in FTIR analysis employing a continuous wavelet transform (CWT) and machine learning algorithms, especially in the applications of the method for Chinese medicine identification, plant classification, and cancer diagnosis. Copyright © Taylor & Francis Group, LLC.},
	author_keywords = {Cancer diagnosis; Chinese medicine identification; CWT; FTIR; Machine learning; Plant classification},
	keywords = {Feature extraction; Fourier transform infrared spectroscopy; Medicine; Neural networks; Radial basis function networks; Support vector machines; Wavelet analysis; Wavelet transforms; Cancer diagnosis; Chinese medicine identification; FTIR; Machine-learning; Plant classification; Learning algorithms},
	correspondence_address = {C. Cheng; Department of Chemistry, Zhejiang Normal University, Jinhua, Zhejiang 321004, China; email: ccg@zjnu.cn},
	issn = {1520569X},
	coden = {APSRB},
	language = {English},
	abbrev_source_title = {Appl Spectrosc Rev},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32}
}

@CONFERENCE{Yusof2010289,
	author = {Yusof, Rubiyah and Rosli, Nenny Ruthfalydia and Khalid, Marzuki},
	title = {Using Gabor filters as image multiplier for tropical wood species recognition system},
	year = {2010},
	journal = {UKSim2010 - UKSim 12th International Conference on Computer Modelling and Simulation},
	pages = {289 – 294},
	doi = {10.1109/UKSIM.2010.61},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954505351&doi=10.1109%2fUKSIM.2010.61&partnerID=40&md5=fe02e5505eec1da94bda0cf82a8a74a7},
	affiliations = {Centre for Artificial Intelligence and Robotics, Universiti Teknologi Malaysia, International Campus, 54100 Kuala Lumpur, Jalan Semarak, Malaysia},
	abstract = {One of the main problems in wood species recognition systems is the lack of discriminative features of the texture images. In order to overcome this, we use Gabor filter in the pre-processing stage of the wood texture image to multiply the number of features for a single image, thus providing more information for feature extractor to capture. The textural wood features are extracted using two feature extraction methods which are co-occurrence matrix approach, known as grey level co-occurrence matrix (GLCM) and also Gabor filters to generate more variation of features and to improve the accuracy rate. The combined features extracted from GLCM and Gabor filters are sent to the classifier module. A multi-layer neural network based on the popular back propagation (MLBP) algorithm is used for classification. The results show that increasing the number of features by using Gabor filters as image multiplier and the combination of features from Gabor filters and GLCM feature extractors improved the accuracy rate of the wood species recognition system. © 2010 IEEE.},
	author_keywords = {Gabor filter; Grey level co-occurrence matrix (GLCM); Image multiplier; Neural network; Texture pattern recognition; Wood recognition},
	keywords = {Computers; Filters; Images; Matrices; Neural Networks; Patterns; Simulation; Texture; Wood; Computer simulation; Frequency multiplying circuits; Matrix algebra; Neural networks; Textures; Wood; Co-occurrence-matrix; Gabor filter; Grey levels; Image multiplier; Texture pattern recognition; Wood recognition; Feature extraction},
	correspondence_address = {R. Yusof; Centre for Artificial Intelligence and Robotics, Universiti Teknologi Malaysia, International Campus, 54100 Kuala Lumpur, Jalan Semarak, Malaysia; email: rubiyah@ic.utm.my},
	isbn = {978-076954016-0},
	language = {English},
	abbrev_source_title = {UKSim - UKSim Int. Conf. Comput. Model. Simul.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32; Conference name: 12th UKSim International Conference on Modelling and Simulation, UKSim 2010; Conference date: 24 March 2010 through 26 March 2010; Conference code: 80938}
}

@CONFERENCE{Arora2012,
	author = {Arora, Akhil and Gupta, Ankit and Bagmar, Nitesh and Mishra, Shashwat and Bhattacharya, Arnab},
	title = {A plant identification system using shape and morphological features on segmented lea ets: Team IITK, CLEF 2012},
	year = {2012},
	journal = {CEUR Workshop Proceedings},
	volume = {1178},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922041554&partnerID=40&md5=ad2f7065db7ca64f9cd7184cffe6fa40},
	affiliations = {Department of Computer Science and Engineering, Indian Institute of Technology, Kanpur, India; Department of Computer Science and Engineering, University of Florida, Gainesville, FL, United States},
	abstract = {Automatic plant identification tasks have witnessed increased interest from the machine learning community in recent years. This paper describes our (team IITK's) participation in the Plant Identification Task, CLEF 2012, organized by the Combined Lab Evaluation Forum (CLEF) where the challenge was to identify plant species based on leaf images. We first categorize the different types of images and then use a variety of novel preprocessing methods such as shadow and background correction, petiole removal and automatic leaet segmentation for identifying the leaf blobs. We next use complex network framework along with novel tooth detection method and morphological operations to compute several useful features. Finally, we use a random forest for classification. Based on the proposed approach, we achieved 2nd rank on the overall score in the competition.},
	author_keywords = {Complex network features; Leaflet segmentation; Peti-ole removal; Plant identification; Shadow correction; Tooth features},
	keywords = {Artificial intelligence; Complex networks; Decision trees; Image segmentation; Learning systems; Mathematical morphology; Machine learning communities; Morphological features; Morphological operations; Network features; Plant identification; Plant identification systems; Shadow correction; Tooth features; Image processing},
	editor = {Karlgren J. and Womser-Hacker C. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Forner P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; Conference name: 2012 Cross Language Evaluation Forum Conference, CLEF 2012; Conference date: 17 September 2012 through 20 September 2012; Conference code: 110353}
}

@CONFERENCE{Khairuddin2011305,
	author = {Khairuddin, Anis Salwa Mohd and Khalid, Marzuki and Yusof, Rubiyah},
	title = {Using two stage classification for improved tropical wood species recognition system},
	year = {2011},
	journal = {Smart Innovation, Systems and Technologies},
	volume = {11 SIST},
	pages = {305 – 314},
	doi = {10.1007/978-3-642-22158-3_30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866338113&doi=10.1007%2f978-3-642-22158-3_30&partnerID=40&md5=1e50be67efe8280a4a658131d40733d1},
	affiliations = {Center for Artificial Intelligence and Robotics, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Department of Electrical Engineering, University of Malaya, Kuala Lumpur, Malaysia},
	abstract = {An automated wood recognition system is designed based on five stages: data acquisition, pre-processing images, feature extraction, pre classification and classification. The proposed system is able to identify 52 types of wood species based on wood features extracted using Basic Grey Level Aura Matrix (BGLAM) technique and statistical properties of pores distribution (SPPD) technique. The features obtained from both feature extractors are fused together and will determine the classification between the various wood species. In order to enhance the class separability, a pre-classification stage is developed which includes clustering and dimension reduction. K-means clustering is introduced to cluster the 52 wood species. As for dimension reduction, we proposed linear discriminant analysis (LDA) to solve linear data and kernel discriminant analysis/ generalized singular value decomposition (KDA/GSVD) to solve nonlinearly structured data. For final classification, K-Nearest Neighbour (KNN) classifier is implemented to classify the wood species. © 2011 Springer-Verlag Berlin Heidelberg.},
	author_keywords = {K-means cluster; KDA/GSVD; KNN classifier; LDA; Wood recognition},
	keywords = {Classification; Computers; Data; Species Identification; Wood; Feature extraction; Interactive computer systems; Multimedia systems; K-means; k-NN classifier; KDA/GSVD; LDA; Wood recognition; Wood},
	correspondence_address = {M. Khalid; Center for Artificial Intelligence and Robotics, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; email: marzuki.khalid@gmail.com},
	issn = {21903026},
	isbn = {978-364222157-6},
	language = {English},
	abbrev_source_title = {Smart Innov. Syst. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 4th International Conference on Intelligent Interactive Multimedia Systems and Services, IIMSS 2011; Conference date: 20 July 2011 through 22 July 2011; Conference code: 92612}
}

@CONFERENCE{Bauer2010,
	author = {Bauer, Alexander and Jürgens, Verena and Angele, Susanne},
	title = {Dealing with uncertain feature assessments in interactive object recognition},
	year = {2010},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {7835},
	doi = {10.1117/12.865043},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649735943&doi=10.1117%2f12.865043&partnerID=40&md5=1bfe2efec04a4aab951127ce101aa7e4},
	affiliations = {Fraunhofer-Institute of Optronics, Systems Technologies and Image Exploitation (IOSB), 76131 Karlsruhe, Fraunhoferstr. 1, Germany},
	abstract = {Object recognition is a typical task of aerial reconnaissance and especially in military applications, to determine the class of an unknown object on the battlefield can give valuable information on its capabilities and its threat. RecceMan® (Reconnaissance Manual) is a decision support system for object recognition developed by the Fraunhofer IOSB. It supports object recognition by automating the tedious task of matching the object features with the set of possible object classes, while leaving the assessment of features to the trained human interpreter. The quality of the features assessed by the user is influenced by several factors such as the quality of the image of the object. These factors are potential sources of error, which can lead to an incorrect classification and therefore have to be considered by the system. To address this issue, two methods for consideration of uncertainty in human feature assessment - a probabilistic and a heuristic approach - are presented and compared based on an experiment in the exemplary domain of flower recognition. © 2010 Copyright SPIE - The International Society for Optical Engineering.},
	author_keywords = {decision support systems; Image exploitation; object recognition},
	keywords = {Artificial intelligence; Decision making; Decision support systems; Decision theory; Feature extraction; Heuristic methods; Military applications; Remote sensing; Aerial reconnaissance; Flower recognition; Fraunhofer; Heuristic approach; Image exploitation; Interactive object recognition; Object class; Potential sources; Uncertain features; Unknown objects; Object recognition},
	correspondence_address = {A. Bauer; Fraunhofer-Institute of Optronics, Systems Technologies and Image Exploitation (IOSB), 76131 Karlsruhe, Fraunhoferstr. 1, Germany; email: alexander.bauer@iosb.fraunhofer.de},
	issn = {0277786X},
	isbn = {978-081948353-9},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: Electro-Optical Remote Sensing, Photonic Technologies, and Applications IV; Conference date: 20 September 2010 through 23 September 2010; Conference code: 82657; All Open Access, Green Open Access}
}

@CONFERENCE{Zheng2012,
	author = {Zheng, Peng and Zhao, Zhong-Qiu and Glotin, Herve},
	title = {Zhao HFUT at image CLEF 2012 plant identification task},
	year = {2012},
	journal = {CEUR Workshop Proceedings},
	volume = {1178},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922051529&partnerID=40&md5=50bf3c12849a8046fdd060b5083e70e5},
	affiliations = {College of Computer Science and Information Engineering, Hefei University of Technology, China; Systems and Information Sciences lab, LSIS CNRS and Univ. of Sud-Toulon Var - La, Garde, 83957, France; Institut Iniversitaire de France, Paris, France},
	abstract = {This paper presents the contribution of ZhaoHFUT group to the ImageCLEF 2012 Plant identification task. The task involves identifying various species of trees based on images of their leaves. In this task, we adopted the main structure of the ScSPM model and another extension of Scale Invariant Feature Transform (SIFT) descriptors, namely flip SIFT descriptors, to investigate the performance of them considering their good performance in object classification. Although our results are not quite promising as compared to other participant groups, they can still guide our work in this field for some conclusions reached.},
	author_keywords = {Feature extraction; Flip SIFT descriptors; Plant identification; ScSPM},
	keywords = {Feature extraction; Object recognition; Descriptors; ImageCLEF; Main structure; Object classification; Plant identification; Scale invariant feature transforms; ScSPM; SIFT descriptors; Plants (botany)},
	editor = {Karlgren J. and Womser-Hacker C. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Forner P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2012 Cross Language Evaluation Forum Conference, CLEF 2012; Conference date: 17 September 2012 through 20 September 2012; Conference code: 110353}
}

@CONFERENCE{Yang2010241,
	author = {Yang, Yan and Nie, Peng-Cheng and He, Yong},
	title = {A method of honey plant classification based on IR spectrum: Extract feature wavelength using genetic algorithm and classify using linear discriminate analysis},
	year = {2010},
	journal = {3rd International Symposium on Intelligent Information Technology and Security Informatics, IITSI 2010},
	pages = {241 – 245},
	doi = {10.1109/IITSI.2010.105},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952687783&doi=10.1109%2fIITSI.2010.105&partnerID=40&md5=494c8497989082bac0c466d4a0e585b4},
	affiliations = {College of Bio-systems Engineering and Food Science, Zhejiang University, Hangzhou, 310029, China; School of Mathematical Sciences, Guangxi Teachers Education University, Nanning, China; School of Electronic and Information Engineering, Nanchang Hang Kong University, Nanchang, China},
	abstract = {Bayesian linear classifier is the basic scheme to solve model classification basing on statistics. Face with the classification of three different nectar plant, the near infrared spectrum data was acquired. The character of the near infrared spectrums is known as litter sample and higher dimension. In this paper, the method has developed to acquire the feature wavelength based on genetic algorithm. It can solve the problem of the effective information extraction from the high-dimensional data matrix. The fitness function of genetic algorithm is been set to minimize the error rate of classification. The K-S algorithm was used to construct the calibration set and validation set. There are 132 samples in the calibration set and 42 samples in the validation set. The feature wavelengths were acquired respectively basing on different preprocessing. The result indicates using the 10 feature wavelengths based on raw data can obtain best resolution compare with the principal component analysis - linear discriminate analysis model. The result indicated that the GA-LDA classifier can made the model to be simplified and the correction rate can be increased evidently after using the feature wavelength. © 2010 IEEE.},
	author_keywords = {Bayesian decision; Feature wavelength; Genetic arithmetic; Linear classifier},
	keywords = {Bayesian networks; Calibration; Classifiers; Function evaluation; Genetic algorithms; Information science; Information technology; Infrared devices; Infrared spectroscopy; Near infrared spectroscopy; Principal component analysis; Wavelength; Bayesian; Bayesian decision; Discriminate analysis; Error rate; Fitness functions; Genetic arithmetic; High dimensional data; Higher dimensions; Information Extraction; IR spectrum; LDA classifiers; Linear classifiers; Model classification; Near infrared spectra; Near-infrared spectrum; Plant classification; S-algorithms; Feature extraction},
	correspondence_address = {Y. Yang; College of Bio-systems Engineering and Food Science, Zhejiang University, Hangzhou, 310029, China; email: y_yae@163.com},
	isbn = {978-076954020-7},
	language = {English},
	abbrev_source_title = {Int. Symp. Intelligent Inf. Technol. Secur. Informatics, IITSI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2010 International Symposium on Intelligent Information Technology and Security Informatics, IITSI 2010; Conference date: 2 April 2010 through 4 April 2010; Conference code: 80396}
}

@ARTICLE{Valliammal2012847,
	author = {Valliammal, N. and Geethalakshmi, S.N.},
	title = {An Amalgam approach for feature extraction and classification of leaves using support vector machine},
	year = {2012},
	journal = {Advances in Intelligent and Soft Computing},
	volume = {166 AISC},
	number = {VOL. 1},
	pages = {847 – 855},
	doi = {10.1007/978-3-642-30157-5_84},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865162520&doi=10.1007%2f978-3-642-30157-5_84&partnerID=40&md5=42aeebedfa5d0b05c1bb0aedf8b58a6c},
	affiliations = {Department Computer Science, Avinashilingam Institute of Home Science and Higher Education for Women, Coimbatore, Tamil Nadu 47, India},
	abstract = {This paper describes the need for the development of automatic plant recognition system for classification of plant leaves. In this paper, an automatic Computer Aided Plant Leaf Recognition (CAP-LR) is presented. To implement the above system initially the input image is pre-processed in order to remove the background noise and to enhance the leaf image. As a second stage the system efficiently extracts the different feature vectors of the leaves and gives it as input to the Support Vector Machine (SVM) for classification into plant leaves or tree leaves. Geometric, texture and color features are extracted for classification. The method is validated by K-Map which calculates the accuracy, sensitivity and efficiency. The experimental result shows that the system has faster processing speed and higher recognition rate. © 2012 Springer-Verlag GmbH.},
	author_keywords = {Classification; Color; Feature Extraction; Geometric; Plant recognition; SVM; Texture features},
	keywords = {Classification (of information); Color; Computer science; Feature extraction; Image segmentation; Textures; Background noise; Color features; Computer aided; Feature extraction and classification; Feature vectors; Geometric; Input image; Leaf images; Plant leaf; Plant leaves; Plant recognition; Processing speed; Recognition rates; SVM; Texture features; Tree leaves; Support vector machines},
	correspondence_address = {N. Valliammal; Department Computer Science, Avinashilingam Institute of Home Science and Higher Education for Women, Coimbatore, Tamil Nadu 47, India; email: Valli.p.2008@gmail.com},
	issn = {18675662},
	isbn = {978-364230156-8},
	language = {English},
	abbrev_source_title = {Adv. Intell. Soft Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2nd International Conference on Computer Science, Engineering and Applications, ICCSEA 2012; Conference date: 25 May 2012 through 27 May 2012; Conference code: 92023}
}

@ARTICLE{Ioannou2010955,
	author = {Ioannou, Konstantinos},
	title = {A web-based expert system for identification of European Pinus sp},
	year = {2010},
	journal = {Journal of Food, Agriculture and Environment},
	volume = {8},
	number = {3-4 PART 2},
	pages = {955 – 958},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751667537&partnerID=40&md5=d4bee60466f109e1dfff73b57684b343},
	affiliations = {Laboratory of Forest Informatics, School of Forestry and Natural Environment, Aristotle University of Thessaloniki, Thessaloniki, PO Box 247, 54124, Greece},
	abstract = {One of the most difficult problems faced by botanists is the quick and accurate identification of species or genera. Pinus sp. consists of 18 species in Europe alone. Each of these species have major or minor differences from the rest, starting from the number of needle groups and ending to the presence or not of wings on the seeds. The purpose of this paper is to present an expert system which will be able to identify the correct Pinus species based on the encasement of experts knowledge. An expert system is a computer application, which based on an Inference engine and a Knowledge base, is able to solve a variety of problems based solely to the knowledge acquired by the programmer. The system is created based on client-server technology and it is easily accessed by everyone through the World Wide Web, allowing multiple users to simultaneously access its Knowledge base.},
	author_keywords = {Artificial intelligence; Expert system; Forest development; Pine identification},
	keywords = {article; artificial intelligence; case report; Europe; expert system; human; Internet; knowledge base; machine learning; nonhuman; pine; plant seed; species identification; accuracy; computer program; expert system; plant identification},
	correspondence_address = {K. Ioannou; Laboratory of Forest Informatics, School of Forestry and Natural Environment, Aristotle University of Thessaloniki, Thessaloniki, PO Box 247, 54124, Greece; email: ioannou.konstantinos@gmail.com},
	publisher = {WFL Publisher Ltd.},
	issn = {14590255},
	language = {English},
	abbrev_source_title = {J. Food Agric. Environ.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Feng20091,
	author = {Feng, Youqian and Zhang, Shanwen},
	title = {Supervised locally linear embedding for plant leaf image feature extraction},
	year = {2009},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {5754 LNCS},
	pages = {1 – 7},
	doi = {10.1007/978-3-642-04070-2_1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350397329&doi=10.1007%2f978-3-642-04070-2_1&partnerID=40&md5=172cdd8e31b30f89bf1db908b17dc2ea},
	affiliations = {Science Institute, Air-Force Engineering University, Xi'an 710051, China; Intelligent Computation Lab, Hefei Institute of Intelligent Machines, Chinese Academy of Science, Hefei, Anhui 230031, P.O. Box 1130, China},
	abstract = {The objects of traditional plant identification were too broad and the classification features of it were usually not synthetic and the recognition rate was always slightly low. This paper gives one recognition approach based on supervised locally linear embedding (LLE) and K-nearest neighbors. The recognition results for thirty kinds of broad-leaved trees were realized and the average correct recognition rate reached 98.3%. Comparison with other recognition method demonstrated the proposed method is effective in advancing the recognition rate. © 2009 Springer Berlin Heidelberg.},
	author_keywords = {Manifold learning; Plant classification; Plant leaf image; Supervised locally linear embedding},
	keywords = {Computer science; Feature extraction; Intelligent computing; Broadleaved trees; Classification features; K-nearest neighbors; Manifold learning; Plant classification; Plant identification; Plant leaf; Plant leaf image; Recognition methods; Recognition rates; Supervised locally linear embedding; Object recognition},
	correspondence_address = {Y. Feng; Science Institute, Air-Force Engineering University, Xi'an 710051, China; email: fengyouqian123@163.com},
	issn = {16113349},
	isbn = {3642040691; 978-364204069-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 5th International Conference on Intelligent Computing, ICIC 2009; Conference date: 16 September 2009 through 19 September 2009; Conference code: 77838}
}

@ARTICLE{Bhattacharyya2011113,
	author = {Bhattacharyya, Debnath and Kim, Tai-Hoon and Lee, Gang-Soo},
	title = {Leaf image analysis towards plant identification},
	year = {2011},
	journal = {Communications in Computer and Information Science},
	volume = {260 CCIS},
	pages = {113 – 125},
	doi = {10.1007/978-3-642-27183-0_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855374474&doi=10.1007%2f978-3-642-27183-0_13&partnerID=40&md5=1725a613ae3705f06ce2a2e99bae86d8},
	affiliations = {Information Technology Department, MCKV Institute of Engineering, Liluah, Howrah - 711204, India; GVSA, School of Information Science, Unviersity of Tasmania, Australia; Department of Computer Engineering, Hannam University, Daejeon, South Korea},
	abstract = {Plants can be classified and identified by naturally or artificially as per the botanists. Plants can be identified by their leaves also. There are different varieties of plants grown throughout the world. Their identifications are studied using various laboratory methods. The morphological and genetically characteristics are employed to classify different leafs as well as plants. However, the presence of wide morphological varieties through evolution among the various leaf cultivars made it more complex and difficult to classify them. Leaf structures play a very crucial role in determining the characteristics of a plant. The broad and narrow shaped leaves, leaf arrangement, leaf margin characteristics features which differentiate various leaf of a plant. In this paper, we propose the methods to identify the leaf using an image analysis based approach. © 2011 Springer-Verlag.},
	author_keywords = {classification; image processing; image segmentation; pattern recognition},
	keywords = {Classification (of information); Image analysis; Image processing; Image segmentation; Internet protocols; Pattern recognition; Signal processing; A plants; Laboratory methods; Leaf structure; Plant identification; Information technology},
	correspondence_address = {T.-H. Kim; GVSA, School of Information Science, Unviersity of Tasmania, Australia; email: taihoonk@utas.edu.au},
	issn = {18650929},
	isbn = {978-364227182-3},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2011 International Conference on Signal Processing, Image Processing and Pattern Recognition, SIP 2011, Held as Part of the 3rd International Mega-Conference on Future-Generation Information Technology, FGIT 2011, in Conjunction with GDC 2011; Conference date: 8 December 2011 through 10 December 2011; Conference code: 87964}
}

@ARTICLE{Schudoma2012127,
	author = {Schudoma, Christian and Steinfath, Matthias and Sprenger, Heike and Van Dongen, Joost T. and Hincha, Dirk and Zuther, Ellen and Geigenberger, Peter and Kopka, Joachim and Köhl, Karin and Walther, Dirk},
	title = {Conducting molecular biomarker discovery studies in plants},
	year = {2012},
	journal = {Methods in Molecular Biology},
	volume = {918},
	pages = {127 – 150},
	doi = {10.1007/978-1-61779-995-2_10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934438230&doi=10.1007%2f978-1-61779-995-2_10&partnerID=40&md5=4eebd3d3cf7dfae78ac935f5acb28c9f},
	affiliations = {Max Planck Institute for Molecular Plant Physiology, Potsdam-Golm, Germany; Department Biologie i, Ludwig-Maximilians-Universität München, Planegg-Martinsried, Germany},
	abstract = {Molecular biomarkers are molecules whose concentrations in a biological system inform about the current phenotypical state and, more importantly, may also be predictive of future phenotypic trait endpoints. The identification of biomarkers has gained much attention in targeted plant breeding since technologies have become available that measure many molecules across different levels of molecular organization and at decreasing costs. In this chapter, we outline the general strategy and workflow of conducting biomarker discovery studies. Critical aspects of study design as well as the statistical data analysis and model building will be highlighted. © 2012 Springer Science+Business Media, LLC.},
	author_keywords = {Biomarker; Breeding; Classification; Feature selection; Machine learning; OMICS technologies; Phenotype; Plants; Study design},
	keywords = {molecular marker; article; diversifying selection; harvest period; knowledge discovery; machine learning; phenotype; plant; plant breeding; plant identification; power analysis; predictive value; priority journal; statistical model; validation study; workflow},
	publisher = {Humana Press Inc.},
	issn = {10643745},
	isbn = {978-161779994-5},
	pmid = {22893290},
	language = {English},
	abbrev_source_title = {Methods Mol. Biol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@CONFERENCE{Goëau2011,
	author = {Goëau, Hervé and Joly, Alexis and Yahiaoui, Itheri and Bonnet, Pierre and Mouysset, Elise},
	title = {Participation of INRIA & Pl@ntNet to image CLEF 2011 plant images classification task},
	year = {2011},
	journal = {CEUR Workshop Proceedings},
	volume = {1177},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922042080&partnerID=40&md5=d0a5e92d0d2364fc46a72dba1035fb62},
	affiliations = {INRIA, IMEDIA team, France; CIRAD, UMR AMAP, France; Tela Botanica, France},
	abstract = {This paper presents the participation of INRIA IMEDIA group and the Pl@ntNet project to ImageCLEF 2011 plant identification task. Image CLEF's plant identification task provides a testbed for the system-oriented evaluation of tree species identification based on leaf images. The aim is to investigate image retrieval approaches in the con- text of crowdsourced images of leaves collected in a collaborative manner. IMEDIA submitted two runs to this task and obtained the best evaluation score for two of the three image categories addressed within the benchmark. The paper presents the two approaches employed, and pro- vides an analysis of the obtained evaluation results.},
	author_keywords = {Benchmark; Classification; Collection; Evaluation; Identification; Imageclef; Images; Imedia; Inria; Leaves; Pl@ntnet; Plant},
	keywords = {Benchmarking; Classification (of information); Identification (control systems); Image retrieval; Plants (botany); Refuse collection; Evaluation; ImageCLEF; Images; Imedia; Inria; Leaves; Pl@ntnet; Plant; Image processing},
	editor = {Petras V. and Forner P. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Clough P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 2011 Cross Language Evaluation Forum Conference, CLEF 2011; Conference date: 19 September 2011 through 22 September 2011; Conference code: 110352}
}

@ARTICLE{Lee2012309,
	author = {Lee, Kue-Bum and Li, Xianghua and Hong, Kwang-Seok},
	title = {Performance improvement of leaf contour extraction in complex background for leaf recognition},
	year = {2012},
	journal = {International Journal of Multimedia and Ubiquitous Engineering},
	volume = {7},
	number = {2},
	pages = {309 – 314},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865625405&partnerID=40&md5=5a457430261eb0d8f7c2f1bbbaa57ce6},
	affiliations = {School of Information and Communication Enginering, Sungkyunkwan University, Jangan-gu, Suwon, KyungKi-do, 440-746, 300, Chunchun-dong, South Korea},
	abstract = {In this system we improved the performance of an automatic system for extracting leaf contour. The proposed leaf contour extraction method consists of three major procedures: the detection of four edge points, and contour tracing. Leaf detection includes two stages: feature extraction and matching. For the leaf contour extraction part, we present a new technique for automatically identifying the contour of a leaf in an image. For contour tracing, an Intelligent Scissor (IS) algorithm is applied. The color gradient magnitude and Canny edge detection are analyzed and included as the cost terms of the Intelligent Scissor algorithm. The color gradient magnitude cost term is implemented so that it can act directly on the three components of the color image. For the third procedure, we implement of the performance improvement. The main idea about proposed method is when the program detected the incorrect four edge points, we using the mouse and clicked the right position instead of the incorrect one.},
	author_keywords = {Color gradient magnitude; Edge point; Intelligent scissor (IS) algorithm; Leaf contour extraction},
	keywords = {Algorithms; Color image processing; Edge detection; Feature extraction; Tools; Automatic systems; Canny edge detection; Color gradients; Color images; Complex background; Contour Extraction; Contour tracing; Edge point; Intelligent scissors; Leaf recognition; Performance improvements; Three component; Image matching},
	correspondence_address = {K.-B. Lee; School of Information and Communication Enginering, Sungkyunkwan University, Jangan-gu, Suwon, KyungKi-do, 440-746, 300, Chunchun-dong, South Korea; email: Leo0608@skku.edu},
	issn = {19750080},
	language = {English},
	abbrev_source_title = {Int. J. Multimedia Ubiquitous Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Solé-Casals2009243,
	author = {Solé-Casals, Jordi and Travieso, Carlos M. and Alonso, Jesús B. and Ferrer, Miguel A.},
	title = {Improving a leaves automatic recognition process using PCA},
	year = {2009},
	journal = {Advances in Soft Computing},
	volume = {49},
	pages = {243 – 251},
	doi = {10.1007/978-3-540-85861-4_29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149131686&doi=10.1007%2f978-3-540-85861-4_29&partnerID=40&md5=34d5f5d2a0e9328e0d80013b6522718b},
	affiliations = {Signal Processing Group, University of Vic, E-08500 Vic, Barcelona, c/ de la Laura 13, Spain; Department of Signals and Communications, CeTIC, University of Las Palmas de Gran Canaria, Las Palmas de Gran Canaria E-35017, Campus de Universitário Tafira s/n, Spain},
	abstract = {In this work we present a simulation of a recognition process with perimeter characterization of a simple plant leaves as a unique discriminating parameter. Data coding allowing for independence of leaves size and orientation may penalize performance recognition for some varieties. Border description sequences are then used, and Principal Component Analysis (PCA) is applied in order to study which is the best number of components for the classification task, implemented by means of a Support Vector Machine (SVM) System. Obtained results are satisfactory, and compared with [4] our system improves the recognition success, diminishing the variance at the same time. © 2009 Springer-Verlag Berlin Heidelberg.},
	author_keywords = {Characteristics selection; Leaves recognition; Parameterization; Pattern recognition; Principal component analysis},
	correspondence_address = {J. Solé-Casals; Signal Processing Group, University of Vic, E-08500 Vic, Barcelona, c/ de la Laura 13, Spain; email: jordi.sole@uvic.cat},
	editor = {Corchado J.M. and De Paz J.F. and Universidad de Salamanca, Departamento de Informatica y Automa, Facultad de Ciencias, 37008, Salamanca, Plaza de la Merced S/N and Rocha M.P. and Universidade do Minho, Informatica/CCTC, Campus de Gualtar, 4710-057 Braga and Riverola F.F. and Escuela Superior de Ingenieria Inf., Edificio Politecnico. Despacho 408.},
	issn = {18600794},
	isbn = {978-354085860-7},
	language = {English},
	abbrev_source_title = {Adv. Soft Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Green Open Access}
}

@CONFERENCE{Shaneyfelt20112928,
	author = {Shaneyfelt, Ted and Agaian, Sos and Jamshidi, Mo},
	title = {Quaternion based segmentation for vanilla recognition},
	year = {2011},
	journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
	pages = {2928 – 2933},
	doi = {10.1109/ICSMC.2011.6084110},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-83755163768&doi=10.1109%2fICSMC.2011.6084110&partnerID=40&md5=c72158c72de0d248f96ac504f135f56d},
	affiliations = {Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, TX, United States},
	abstract = {Vanilla is the second most expensive spice worldwide. The high cost of vanilla has led to the problem of dangerous adulterated substitutes. Its high cost is attributed largely to the labor intensive hand pollination required where the melipona bee is not present. This article proposes a method of segmenting vanilla images intended for robotic control of a future automated pollination system. We present the specialization of a hypercomplex numbers based segmentation technique for vanilla flower recognition. The specialization overcomes much of the difficulty of differentiating green flowers from their similarly colored surroundings. Comparison is given to previous hypercomplex numbers based segmentation without the specialization. © 2011 IEEE.},
	author_keywords = {Agriculture; Farming; Image recognition; Machine vision; Robotics; Signal processing},
	keywords = {Agriculture; Computer vision; Cybernetics; Image recognition; Signal processing; Farming; Flower recognition; Hand-pollination; High costs; Hypercomplex number; Labor intensive; Robotic controls; Segmentation techniques; Robotics},
	correspondence_address = {T. Shaneyfelt; Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, TX, United States; email: ted.shaneyfelt@utsa.edu},
	issn = {1062922X},
	isbn = {978-145770652-3},
	coden = {PICYE},
	language = {English},
	abbrev_source_title = {Conf. Proc. IEEE Int. Conf. Syst. Man Cybern.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2011 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2011; Conference date: 9 October 2011 through 12 October 2011; Conference code: 87707}
}

@CONFERENCE{Ting20124343,
	author = {Ting, Yuan and Kondo, Naoshi and Wei, Li},
	title = {Sunlight fluctuation compensation for tomato flower detection using web camera},
	year = {2012},
	journal = {Procedia Engineering},
	volume = {29},
	pages = {4343 – 4347},
	doi = {10.1016/j.proeng.2012.01.668},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858384944&doi=10.1016%2fj.proeng.2012.01.668&partnerID=40&md5=9f94727dc3ba92125e9e099e172e9307},
	affiliations = {China Agricultural University, Haidianqu, 100083 Beijing, Qinghuadonglu 17, China; Kyoto University, Sakyo-ku, Tokyo 606-8502, Japan},
	abstract = {As basic research to develop a machine vision system to detect tomato flowers, a robust color and brightness compensation approach against the sunlight fluctuation was proposed. This approach used the scanning strategy to obtain an uniform condition for precise color and brightness measurement even under changing sunlight illumination. According to the relations between color differences and camera properties, adaptive camera parameter adjustment was calibrated to generate the best possible color correction. Experimental results showed that this method could not only measure tomato flower colors, but also meet the requirements of automatic flower recognition under varying illumination conditions. © 2011 Published by Elsevier Ltd.},
	author_keywords = {Automatic color compensation; Flower detection; Greenhouse; Sunlight condition},
	keywords = {Cameras; Computer vision; Electronics engineering; Fruits; Greenhouses; Luminance; Basic research; Brightness compensation; Brightness measurements; Camera parameter; Color compensations; Color correction; Color difference; Flower recognition; Illumination conditions; Machine vision systems; Scanning strategies; Sunlight condition; Web camera; Color},
	correspondence_address = {Y. Ting; China Agricultural University, Haidianqu, 100083 Beijing, Qinghuadonglu 17, China; email: yuanting122@hotmail.com},
	issn = {18777058},
	language = {English},
	abbrev_source_title = {Procedia Eng.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 2012 International Workshop on Information and Electronics Engineering, IWIEE 2012; Conference date: 10 March 2012 through 11 March 2012; Conference code: 89020; All Open Access, Gold Open Access}
}

@CONFERENCE{Wang2010V1269,
	author = {Wang, Bi-Hui and Wang, Hang-Jun and Qi, Heng-Nian},
	title = {Wood recognition based on grey-level co-occurrence matrix},
	year = {2010},
	journal = {ICCASM 2010 - 2010 International Conference on Computer Application and System Modeling, Proceedings},
	volume = {1},
	pages = {V1269–V1272},
	doi = {10.1109/ICCASM.2010.5619388},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649628857&doi=10.1109%2fICCASM.2010.5619388&partnerID=40&md5=28b8d0d3a0c28256f6b7f202764210c3},
	affiliations = {School of Information Science and Technology, ZheJiang Agriculture and Forestry University, Linan, 311300, China},
	abstract = {By reason of the convenient obtaining of wood stereogram images, it's suitable for us to apply them to the application of wood recognition. In order to extract features from the wood stereogram images, gray level co-occurrence matrix (CLCM) was used to statistic texture features. Under the image resolution of 100*100, four directions, i.e. 0°,45°,90°, and 135°, were severed as the generated pixel directions of CLCM. Besides, providing the pixel interval with 4 and gray level with 128; Also six features, Energy, Entropy, Contrast, Dissimilarity, Inverse Difference Moment, and Variance, were used as classification features in the experiment. According to the experiment of the wood recognition, about 91.7% recognition rates were acquired through feature extractions of 24 wood species, and 480 samples, and the use of the SVM classifier. The experiment results showed that it was feasible to apply the six proposed features of CLCM to the wood recognition, and they can finish the task effectively. © 2010 IEEE.},
	author_keywords = {GLCM; SVM; Texture feature; Wood stereogram},
	keywords = {Contrast; Energy; Entropy; Experimentation; Image Analysis; Texture; Variance; Wood; Computer applications; Experiments; Image resolution; Pixels; Security of data; Textures; Wood; Classification features; GLCM; Gray level co-occurrence matrix; Gray levels; Grey-level co-occurrence matrixes; Inverse differences; Recognition rates; SVM; SVM classifiers; Texture feature; Texture features; Wood recognition; Feature extraction},
	correspondence_address = {H.-J. Wang; School of Information Science and Technology, ZheJiang Agriculture and Forestry University, Linan, 311300, China; email: 837012008@qq.com},
	isbn = {978-142447236-9},
	language = {English},
	abbrev_source_title = {ICCASM - Int. Conf. Comput. Appl. Syst. Model., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28; Conference name: 2010 International Conference on Computer Application and System Modeling, ICCASM 2010; Conference date: 22 October 2010 through 24 October 2010; Conference code: 82681}
}

@CONFERENCE{Deepika201244,
	author = {Deepika, K. and Ruth, I. and Keerthana, S. and Sathya Bama, B. and Avvailakshmi, S. and Vidhya, A.},
	title = {Robust plant recognition using Graph cut based flower segmentation and PHOG based feature extraction},
	year = {2012},
	journal = {2012 International Conference on Machine Vision and Image Processing, MVIP 2012},
	pages = {44 – 47},
	doi = {10.1109/MVIP.2012.6428757},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874236814&doi=10.1109%2fMVIP.2012.6428757&partnerID=40&md5=936222ca1d1a152688cdd2e199dee706},
	affiliations = {Thiagarajar College of Engineering, Madurai, India},
	abstract = {This paper proposes an efficient computer-aided plant recognition method based on plant flower images using shape and texture features intended mainly for medical industry, botanical gardening and cosmetic industry. The target flower is segmented from the complex background using Graph cut segmentation. Shape and texture features are extracted for the segmented image. In the shape domain, a feature descriptor is developed using Pyramidal Histogram of Oriented Gradients (PHOG) that represents the image shape. It captures the distribution of intensity gradients or edge directions. Then in the texture domain, the feature descriptor is developed using Pyramidal Local Binary Pattern (PLBP). The relevant images are retrieved from the database by matching the concatenated histogram of the PHOG and PLBP feature descriptors for the given input image. Results on a database of 200 sample images belonging to different types of plants show an increased efficiency of 96%. © 2012 IEEE.},
	author_keywords = {Graph cut; Histogram matching; Local Binary Pattern; Pyramidal Histogram of Oriented Gradients},
	keywords = {Computer vision; Feature extraction; Graphic methods; Image segmentation; Medical imaging; Textures; Complex background; Computer-aided; Cosmetic industry; Edge direction; Feature descriptors; Graph cut; Graph-cut segmentations; Histogram matching; Histogram of oriented gradients; Input image; Intensity gradients; Local binary patterns; Medical industries; Plant recognition; Segmented images; Shape and textures; Image texture},
	isbn = {978-146732320-8},
	language = {English},
	abbrev_source_title = {Int. Conf. Mach. Vis. Image Process., MVIP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2012 International Conference on Machine Vision and Image Processing, MVIP 2012; Conference date: 14 December 2012 through 15 December 2012; Conference code: 95713}
}

@CONFERENCE{Valliammal2011,
	author = {Valliammal, N. and Geethalakshmi, S.N.},
	title = {Hybrid image segmentation algorithm for leaf recognition and characterization},
	year = {2011},
	journal = {Proceedings of 2011 International Conference on Process Automation, Control and Computing, PACC 2011},
	doi = {10.1109/PACC.2011.5978883},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052221613&doi=10.1109%2fPACC.2011.5978883&partnerID=40&md5=ca3bd677d83c49947c8cdac06b8f463b},
	affiliations = {Department of Computer Science, Avinashilingam Deemed University for Women, Coimbatore-641043, India},
	abstract = {Plants play an important role in both human life and other lives that exist on the earth. Due to environmental deterioration and lack of awareness, many rare plant species are at the margins of extinction. Despite the great advances made in botany, there are many plants yet to be discovered, classified, and utilized; unknown plants are treasures waiting to be found. Leaf classification and recognition for plant identification plays a vital role in all these endeavors. There has been little work reported on leaves, flower and fruit image processing and recognition. In recent years, several researchers have dedicated their work to leaf characterisation. As an inherent trait, leaf vein definitely contains the important information for plant species recognition despite its complex modality. A new approach that combines a thresholding method and H-maxima transformation based method is proposed to extract the leaf veins. A preliminary segmentation based on the intensity histogram of leaf images is first carried out to coarsely determine vein regions using thresholding. This is followed by a fine segmentation using H maxima transformation based method for object pixel as its inputs. Compared with other methods, experimental results show that this combined approach is capable of extracting more accurate venation modality of the leaf for the subsequent vein pattern classification. The approach can also reduce the computing time compared with other approach. © 2011 IEEE.},
	author_keywords = {H-maxima transformation; Hybrid method and computation time; Image segmentation; Leaf characterization; Plant identification; Recognition; Thresholding},
	keywords = {Identification (control systems); Plants (botany); Process control; H-maxima transformation; Hybrid method; Leaf characterization; Plant identification; Recognition; Thresholding; Image segmentation},
	correspondence_address = {N. Valliammal; Department of Computer Science, Avinashilingam Deemed University for Women, Coimbatore-641043, India; email: valli.p.2008@gmail.com},
	isbn = {978-161284764-1},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Process Autom., Control Comput., PACC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26; Conference name: 2011 International Conference on Process Automation, Control and Computing, PACC 2011; Conference date: 20 July 2011 through 22 July 2011; Conference code: 86336}
}

@CONFERENCE{Jaafar2010,
	author = {Jaafar, M. Yazid M. and Osman, Mohd. Azam and Zainon, Wan Mohd Nazmee Wan and Talib, Abdullah Zawawi},
	title = {A framework for grid-based mobile flower recognition},
	year = {2010},
	journal = {2010 International Conference on Distributed Frameworks for Multimedia Applications, DFmA 2010},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052012680&partnerID=40&md5=10d18fd58dffb8148e0e1293066f4e5e},
	affiliations = {School of Computer Sciences, Universiti Sains Malaysia, 11800 USM Pulau Pinang, Malaysia},
	abstract = {Identifying an organic object like a flower against the background picture is known to be challenging. Several methods have been proposed so that unique characteristics of flowers can be extracted with increased accuracy. Establishing this process as a mobile flower identification service for the public requires major adaptation of the system due to numerous picture requests being received simultaneously. This paper presents a framework for grid-based mobile flower recognition that employs 1) grid computing technology for further optimization of multiple concurrent image processing and 2) mobile wireless communication as the primary data transfer for accessing this mobile service. Globus Toolkit is applied as the middleware in the distributed computing environment whereby Short Message Services (SMS) and Multimedia Messaging Services (MMS) technology are used for picture and result delivery. © 2010 University Sains Malaysia.},
	author_keywords = {grid computing; image processingfeature extraction; mobile communication},
	keywords = {Data transfer; Image processing; Message passing; Middleware; Mobile telecommunication systems; Multimedia services; Wireless telecommunication systems; Distributed computing environment; Flower recognition; Globus Toolkit; Grid computing technology; Grid-based; image processingfeature extraction; Mobile communications; Mobile service; Mobile wireless communications; Multimedia messaging service; Organic objects; Short message services; Grid computing},
	correspondence_address = {M.Y.M. Jaafar; School of Computer Sciences, Universiti Sains Malaysia, 11800 USM Pulau Pinang, Malaysia; email: mymj95956@student.usm.my},
	isbn = {978-602974791-1},
	language = {English},
	abbrev_source_title = {Int. Conf. Distrib. Frameworks Multimedia Appl., DFmA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2010 International Conference on Distributed Frameworks for Multimedia Applications, DFmA 2010; Conference date: 2 August 2010 through 3 August 2010; Conference code: 86073}
}

@CONFERENCE{Gao2012101,
	author = {Gao, Liwen and Lin, Xiaohua},
	title = {A study on the automatic recognition system of medicinal plants},
	year = {2012},
	journal = {2012 2nd International Conference on Consumer Electronics, Communications and Networks, CECNet 2012 - Proceedings},
	pages = {101 – 103},
	doi = {10.1109/CECNet.2012.6201425},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861871122&doi=10.1109%2fCECNet.2012.6201425&partnerID=40&md5=8f9eb7fb54415fdc6bb8e43bcb798a72},
	affiliations = {College of Information Technology, Guangzhou University of Chinese Medicine, Guangzhou, China; College of Chinese Materia Medica, Guangzhou University of Chinese Medicine, Guangzhou, China},
	abstract = {China covers a vast territory, with a wide range of plants spreading throughout the country. It is never easy to accurately identify the species of one certain plant or its medicinal features. This paper studied and has achieved an automatic recognition system of medicinal plants. With the leaf image recognition of medicinal plants as its core, the system involves the up-to-date technologies of image processing and neural network. The system has boosted the transformation of medicinal plant recognition from manual basis to semi-automation. It is good for overcoming the subjectivity of human beings and has accelerated the recognition, and it will be popular among a wide range of users. © 2012 IEEE.},
	author_keywords = {automatic recognition; image processing; medicinal plant; neural network},
	keywords = {Consumer electronics; Image processing; Image recognition; Neural networks; Automatic recognition; Automatic recognition system; Human being; Medicinal plants; Semi-automation; Plants (botany)},
	isbn = {978-145771415-3},
	language = {English},
	abbrev_source_title = {Int. Conf. Consum. Electron., Commun. Networks, CECNet - Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2012 2nd International Conference on Consumer Electronics, Communications and Networks, CECNet 2012; Conference date: 21 April 2012 through 23 April 2012; Conference code: 90025}
}

@ARTICLE{Valliammal2011316,
	author = {Valliammal, N. and Geethalakshmi, S.N.},
	title = {Leaf and flower recognition using preferential image segmentation algorithm},
	year = {2011},
	journal = {Communications in Computer and Information Science},
	volume = {204 CCIS},
	pages = {316 – 325},
	doi = {10.1007/978-3-642-24043-0_32},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054857606&doi=10.1007%2f978-3-642-24043-0_32&partnerID=40&md5=fdab520b3f76cf06fd9e1b8b9c96b839},
	affiliations = {Avinashilingam Deemed University for Women, Coimbatore-641043, India},
	abstract = {Automatic plant classification systems are essential for a wide range of applications including environment protection, plant resource survey, as well as for education. With the aid of advanced information technology, image processing and machine learning techniques, automatic plant identification and classification will enhance such systems with more functionality, such as automatic labeling and flexible searching. Image segmentation and object recognition are two aspects of digital image processing which are being increasingly used in many applications including leaf recognition. In this paper, the Preferential Image Segmentation (PIS) method is used to segment an object of interest from the original image. A probabilistic curve evolution method with particle filters is used to measure the similarity between shapes during matching process. The experimental results prove that the preferential image segmentation can be successfully applied in leaf recognition and segmentation from a plant image. © 2011 Springer-Verlag Berlin Heidelberg.},
	author_keywords = {Filters; Preferential image segmentation; Segmentation; Tree of shapes},
	keywords = {Computer science; Education computing; Image matching; Imaging systems; Information technology; Object recognition; Algorithms; Artificial intelligence; Filters (for fluids); Learning systems; A plants; Automatic labeling; Curve evolution methods; Environment protection; Flower recognition; Image segmentation algorithm; Leaf recognition; Machine learning techniques; Matching process; Object of interests; Original images; Particle filter; Plant classification; Plant identification; Preferential image segmentation; Tree of shapes; Curve evolution methods; Environment protection; Flower recognition; Image segmentation algorithm; Machine learning techniques; Plant classification; Plant identification; Tree of shapes; Image segmentation},
	correspondence_address = {N. Valliammal; Avinashilingam Deemed University for Women, Coimbatore-641043, India; email: Valli.p.2008@gmail.com},
	issn = {18650929},
	isbn = {978-364224042-3},
	language = {English},
	abbrev_source_title = {Commun. Comput. Info. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 1st International Conference on Computer Science, Engineering and Information Technology, CCSEIT 2011; Conference date: 23 September 2011 through 25 September 2011; Conference code: 87036}
}

@CONFERENCE{Priya2012428,
	author = {Priya, C. Arun and Balasaravanan, T. and Thanamani, Antony Selvadoss},
	title = {An efficient leaf recognition algorithm for plant classification using support vector machine},
	year = {2012},
	journal = {International Conference on Pattern Recognition, Informatics and Medical Engineering, PRIME 2012},
	pages = {428 – 432},
	doi = {10.1109/ICPRIME.2012.6208384},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863906563&doi=10.1109%2fICPRIME.2012.6208384&partnerID=40&md5=1c447875aa3792c9ca5cee7acd855394},
	affiliations = {P.S.G.R Krishnammal College for Women, Coimbatore, India; Nehru Arts and Science College, Coimbatore, India; Nallamuthu Gounder Mahalingam College, Pollachi, India},
	abstract = {Recognition of plants has become an active area of research as most of the plant species are at the risk of extinction. This paper uses an efficient machine learning approach for the classification purpose. This proposed approach consists of three phases such as preprocessing, feature extraction and classification. The preprocessing phase involves a typical image processing steps such as transforming to gray scale and boundary enhancement. The feature extraction phase derives the common DMF from five fundamental features. The main contribution of this approach is the Support Vector Machine (SVM) classification for efficient leaf recognition. 12 leaf features which are extracted and orthogonalized into 5 principal variables are given as input vector to the SVM. Classifier tested with flavia dataset and a real dataset and compared with k-NN approach, the proposed approach produces very high accuracy and takes very less execution time. © 2012 IEEE.},
	author_keywords = {Digital Morphological Features (DMFs); Leaf Recognition; Support Vector Machine},
	keywords = {Biomedical engineering; Feature extraction; Image processing; Information science; Active area; Boundary enhancement; Data sets; Execution time; Feature extraction and classification; Fundamental features; Gray scale; Input vector; Leaf recognition; Learning approach; Morphological features; Plant classification; Plant species; Preprocessing phase; Principal Variables; Three phasis; Support vector machines},
	correspondence_address = {C.A. Priya; P.S.G.R Krishnammal College for Women, Coimbatore, India; email: arunpriya.bs@gmail.com},
	isbn = {978-146731037-6},
	language = {English},
	abbrev_source_title = {Int. Conf. Pattern Recogn., Informatics Med. Eng., PRIME},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 111; Conference name: 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering, PRIME 2012; Conference date: 21 March 2012 through 23 March 2012; Conference code: 91139}
}

@CONFERENCE{Zheng2009,
	author = {Zheng, Xiaodong and Wang, Xiaojie},
	title = {Fast leaf vein extraction using hue and intensity information},
	year = {2009},
	journal = {Proceedings - 2009 International Conference on Information Engineering and Computer Science, ICIECS 2009},
	doi = {10.1109/ICIECS.2009.5364313},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949607272&doi=10.1109%2fICIECS.2009.5364313&partnerID=40&md5=38a2c6265e62e9144567ef8639782068},
	affiliations = {Dep. of Computer Science and Application, Zhengzhou Institute of Aeronautical Industry Management, Zhengzhou 450015, China; School of Mechanical Engineering, Zhengzhou University, Zhengzhou 450002, China},
	abstract = {Leaf vein contains significant physiological information of plant. Leaf vein extraction is one of the key steps in plant modeling and plant recognition. Several methods have been presented on leaf vein extraction but all of them are limited in narrow application fields. In this paper a universal method is proposed using hue and intensity information. According to the color difference between leaf vein and mesophyll, leaves are classified as the contrasting color leaf and the concolorous leaf. Leaf vein of the contrasting color leaf can be extracted using only hue information. Leaf vein extraction of the concolorous leaf needs both hue and intensity information. The method has been programmed under Visual C++6.0 and many leaves from different plants have been tested. The results show a good applicability. Compared with other method, new method is simple, fast and universal. It can be embedded in the leaf feature extraction system and work in human-Computer interaction way. ©2009 IEEE.},
	author_keywords = {Color image segmentation; Hue; Intensity; Leaf vein extraction},
	keywords = {Color; Computational methods; Digital image storage; Feature extraction; Human computer interaction; Image segmentation; Knowledge management; Application fields; Color difference; Color image segmentation; In-plants; Intensity information; Plant recognition; Universal method; Visual C++6.0; Information use},
	correspondence_address = {X. Zheng; Dep. of Computer Science and Application, Zhengzhou Institute of Aeronautical Industry Management, Zhengzhou 450015, China; email: dong20013@126.com},
	isbn = {978-142444994-1},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Inf. Eng. Comput. Sci., ICIECS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 2009 International Conference on Information Engineering and Computer Science, ICIECS 2009; Conference date: 19 December 2009 through 20 December 2009; Conference code: 79561}
}

@CONFERENCE{Bremananth2009615,
	author = {Bremananth, R. and Nithya, B. and Saipriya, R.},
	title = {Wood species recognition using GLCM and correlation},
	year = {2009},
	journal = {ARTCom 2009 - International Conference on Advances in Recent Technologies in Communication and Computing},
	pages = {615 – 619},
	doi = {10.1109/ARTCom.2009.10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-73849094268&doi=10.1109%2fARTCom.2009.10&partnerID=40&md5=4b360541510f42a4a3fb76e37b786d8e},
	affiliations = {Department of Computer Applications, Sri Ramakrishna Engineering College, Coimbatore, India},
	abstract = {The proposed system identifies the species of the wood using the textural features present in its barks. Each species of a wood has its own unique patterns in its bark, which enabled the proposed system to identify it accurately. Automatic wood recognition system has not yet been well established mainly due to lack of research in this area and the difficulty in obtaining the wood database. In our work, a wood recognition system has been designed based on pre-processing techniques, feature extraction and by correlating the features of those wood species for their classification. Texture classification is a problem that has been studied and tested using different methods due to its valuable usage in various pattern recognition problems, such as wood recognition, rock classification. The most popular technique used for the textural classification is Gray-level Co-occurrence Matrices (GLCM). The features from the enhanced images are thus extracted using the GLCM is correlated, which determines the classification between the various wood species. The result thus obtained shows a high rate of recognition accuracy proving that the techniques used in suitable to be implemented for commercial purposes. © 2009 IEEE.},
	author_keywords = {Correlation; Grey level co-occurrence matrix; Probability density function; Wood recognition},
	keywords = {Correlation; Image Analysis; Pattern Recognition; Probability; Wood; Feature extraction; Image processing; Matrix algebra; Wood; Co-occurrence-matrix; Gray-level co-occurrence matrix; Grey level co-occurrence matrix; Grey levels; High rate; Pattern recognition problems; Pre-processing; Recognition accuracy; Rock classification; Species recognition; Techniques used; Textural classification; Textural feature; Texture classification; Wood recognition; Probability density function},
	correspondence_address = {R. Bremananth; Department of Computer Applications, Sri Ramakrishna Engineering College, Coimbatore, India; email: bremresearch@gmail.com},
	isbn = {978-076953845-7},
	language = {English},
	abbrev_source_title = {ARTCom - Int. Conf. Adv. Recent Technol. Commun. Comput.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26; Conference name: ARTCom 2009 - International Conference on Advances in Recent Technologies in Communication and Computing; Conference date: 27 October 2009 through 28 October 2009; Conference code: 78825}
}

@ARTICLE{Polat20121523,
	author = {Polat, Ediz and Yildiz, Cihat},
	title = {Stationary aircraft detection from satellite images},
	year = {2012},
	journal = {Istanbul University - Journal of Electrical and Electronics Engineering},
	volume = {12},
	number = {2},
	pages = {1523 – 1528},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884229438&partnerID=40&md5=b7511985e3becd1efffa6c9f65cf0227},
	affiliations = {Electrical-Electronics Engineering Department, Kirikkale University, Kirikkale, Turkey},
	abstract = {Satellite image analysis is an important research area in the field of image processing. Detection and recognition of regions and objects from satellite images find many useful civil applications such as detection of buildings, roads, bridges and other man-made objects as well as land plant classification. On the other hand, the detection of stationary aircrafts in airports can be strategically important in military applications. In this study, a learning-based system that detects stationary aircrafts in satellite images obtained from Google Earth is developed. The features that emphasize the geometric structure of an aircraft are determined using 2D Gabor filter. The aircraft detection is performed using Support Vector Machines (SVM) classification method. The SVM is a supervised learning method that analyzes data and recognizes patterns for classification The SVM takes a set of input data (a vector consists of Gabor filter output of images) and predicts the one of two classes (aircraft or non-aircraft). The performance of the system is demonstrated using satellite images collected from airports in Europe and United States.},
	author_keywords = {Aircraft detection; Gabor filter; Satellite image analysis; Support vector machines},
	keywords = {Aircraft; Aircraft detection; Gabor filters; Image analysis; Military applications; Satellites; 2-D Gabor filter; Civil applications; Classification methods; Geometric structure; Satellite image analysis; Satellite images; Stationary aircraft; Supervised learning methods; Support vector machines},
	publisher = {Istanbul University},
	issn = {13030914},
	language = {English},
	abbrev_source_title = {Istanb. Univ. J. Electr. Electron. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Hu20124667,
	author = {Hu, Rongxiang and Jia, Wei and Ling, Haibin and Huang, Deshuang},
	title = {Multiscale distance matrix for fast plant leaf recognition},
	year = {2012},
	journal = {IEEE Transactions on Image Processing},
	volume = {21},
	number = {11},
	pages = {4667 – 4672},
	doi = {10.1109/TIP.2012.2207391},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867853365&doi=10.1109%2fTIP.2012.2207391&partnerID=40&md5=a5a5fa55fc41e7315e4db0ee35b9df98},
	affiliations = {Hefei Institutes of Physical Science, Chinese Academy of Science, Hefei 230031, China; Department of Automation, University of Science and Technology of China, Hefei 230027, China; Department of Computer and Information Science, Temple University, Philadelphia, PA 19122, United States},
	abstract = {In this brief, we propose a novel contour-based shape descriptor, called the multiscale distance matrix, to capture the shape geometry while being invariant to translation, rotation, scaling, and bilateral symmetry. The descriptor is further combined with a dimensionality reduction to improve its discriminative power. The proposed method avoids the time-consuming pointwise matching encountered in most of the previously used shape recognition algorithms. It is therefore fast and suitable for real-time applications. We applied the proposed method to the task of plan leaf recognition with experiments on two data sets, the Swedish Leaf data set and the ICL Leaf data set. The experimental results clearly demonstrate the effectiveness and efficiency of the proposed descriptor. © 2012 IEEE.},
	author_keywords = {Cost matrix; inner distance; multiscale distance matrix (MDM); plant leaf; shape recognition},
	keywords = {Algorithms; Artificial Intelligence; Databases, Factual; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Plant Leaves; Reproducibility of Results; Image processing; Mathematical models; Cost matrices; Distance matrices; inner distance; Plant leaf; Shape recognition; algorithm; artificial intelligence; automated pattern recognition; classification; factual database; histology; image processing; letter; methodology; plant leaf; reproducibility; Geometry},
	correspondence_address = {R. Hu; Hefei Institutes of Physical Science, Chinese Academy of Science, Hefei 230031, China; email: hurongxiang2008@gmail.com},
	issn = {10577149},
	coden = {IIPRE},
	pmid = {22875247},
	language = {English},
	abbrev_source_title = {IEEE Trans Image Process},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 183}
}

@ARTICLE{Zhang2012275,
	author = {Zhang, H. and Hinze, L.L. and Lan, Y. and Westbrook, J.K. and Hoffmann, W. Clint},
	title = {Discriminating among cotton cultivars with varying leaf characteristics using hyperspectral radiometry},
	year = {2012},
	journal = {Transactions of the ASABE},
	volume = {55},
	number = {1},
	pages = {275 – 280},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859746022&partnerID=40&md5=451984f8cd97c1f02602ce04a39123da},
	affiliations = {Department of Biological and Agricultural Engineering, Texas A and M University, College Station, TX, United States; USDA-ARS Water Management Research, Parlier, CA, United States; USDA-ARS Crop Germplasm Research Unit, College Station, TX, United States; USDA-ARS Areawide Pest Management Research Unit, College Station, TX 77845, 2771 F and B Road, United States},
	abstract = {There is a rapidly growing interest in methods for automatic plant identification in agricultural research. Cotton (Gossypium spp.) is a crop well-suited to precision agriculture and its inherent goals of increasing yields while minimizing environmental impacts. Ten cotton (G. hirsutum and G. barbadense) cultivars with differing leaf characteristics were evaluated in a greenhouse environment. Hyperspectral data collected with a handheld spectroradiometer were used to distinguish among the cultivars. The features extracted by principal component analysis and stepwise selection approaches were used for discriminant analysis. The best discrimination accuracy by selected wavelengths was 90.4% for G. hirsutum cultivars, 100% for G. barbadense cultivars, and 91.6% for pooled cultivars of the two species. Spectral wavelengths at 550 and 760 nm were most relevant to the discrimination between these two cotton species. Two vegetation indices, NDVI and PRI, were also investigated for any significant differences across cotton cultivars. The results demonstrated that hyperspectral radiometry has good potential for discrimination of G. hirsutum and G. barbadense cotton cultivars in early stages of growth. © 2012 American Society of Agricultural and Biological Engineers.},
	author_keywords = {Cotton cultivar discrimination; Glands; Hyperspectral; Leaf pigment; Pubescence},
	keywords = {Gossypium; Gossypium barbadense; Gossypium hirsutum; Cotton; Discriminant analysis; Feature extraction; Principal component analysis; Radiometers; Radiometry; Agricultural research; Cotton cultivars; Discrimination accuracy; Glands; Gossypium spp; Greenhouse environment; Handhelds; HyperSpectral; Hyperspectral Data; Plant identification; Precision Agriculture; Pubescence; Spectro-radiometers; Vegetation index; cotton; crop yield; cultivar; data acquisition; discriminant analysis; environmental impact; greenhouse ecosystem; leaf; pigment; precision agriculture; principal component analysis; radiometer; wavelength; Cultivation},
	correspondence_address = {Y. Lan; USDA-ARS Areawide Pest Management Research Unit, College Station, TX 77845, 2771 F and B Road, United States; email: yubin.lan@ars.usda.gov},
	issn = {21510032},
	language = {English},
	abbrev_source_title = {Trans. ASABE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@CONFERENCE{Liao2010334,
	author = {Liao, Yu-Ping and Zhou, Hao-Gong and Fan, Gang-Ren},
	title = {Accelerating recognition system of leaves on Nios II embedded platform},
	year = {2010},
	journal = {3CA 2010 - 2010 International Symposium on Computer, Communication, Control and Automation},
	volume = {1},
	pages = {334 – 337},
	doi = {10.1109/3CA.2010.5533816},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956116481&doi=10.1109%2f3CA.2010.5533816&partnerID=40&md5=07181af0cfbedeae6f9e75652bbd71bb},
	affiliations = {Ching Yun University, Taoyuan, China},
	abstract = {In this paper, we present an efficient HW/SW codesign architecture of a Recognition System of Leaves. The architecture includes pre-processing modules for the input video signal from the camera and interfaces for the external video memory and the LCD. The recognition of leaf is implemented by hardware in FPGA. By using the five mega-pixel camera included in the Altera DE2-70 kit for input, image processing can be done by FPGA Cyclone II EP2C70F89C6N with ∼70,000 LEs on Altera DE2-70 kit. The measurement results can verify HW/SW codesign architecture of leaves recognition easily achieves faster processing performance than the pure SW technique. Using hardware acceleration presents a speed up factor of 7 times over a software implementation. © 2010 IEEE.},
	author_keywords = {DE2-70; FPGA; Leaf; Nios II; Recognition System},
	keywords = {Cameras; Computer hardware; Image processing; DE2-70; FPGA; Leaf; NIOS II; Recognition systems; Field programmable gate arrays (FPGA)},
	correspondence_address = {Y.-P. Liao; Ching Yun University, Taoyuan, China; email: lyp@cyu.edu.tw},
	isbn = {978-142445566-9},
	language = {English},
	abbrev_source_title = {3CA - Int. Symp. Comput., Commun., Control Autom.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 2010 International Symposium on Computer, Communication, Control and Automation, 3CA 2010; Conference date: 5 May 2010 through 7 May 2010; Conference code: 81512}
}

@CONFERENCE{Valliammal2012708,
	author = {Valliammal, N. and Geethalakshmi, S.N.},
	title = {Multiple noise reduction using hybrid method for leaf recognition},
	year = {2012},
	journal = {2012 International Conference on Devices, Circuits and Systems, ICDCS 2012},
	pages = {708 – 712},
	doi = {10.1109/ICDCSyst.2012.6188689},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861218261&doi=10.1109%2fICDCSyst.2012.6188689&partnerID=40&md5=c705c6b8a4bc491d617aa22a29ef2954},
	affiliations = {Department of Computer Science, Avinashilingam Institute for Home Science and Higher Education for Women, Coimbatore-641 043, India},
	abstract = {Plant recognition is very demanding in biology and agriculture as new plant discovery and the computerization of the management of plant species become more popular. Developing an automatic plant recognition system becomes a very challenging task. Image pre-processing techniques, such as filtering for noise reduction and enhancement of the image is highly essential. The computerized plant recognition mainly suppresses noise in the input image leading to stable feature extraction of plants. By applying Gaussian noise, the leaf image appears blurred, due to which the edges of the leaf vein is not clearly visible. Speckle noise occurrence is often undesirable, and also very dangerously causes damage to the leaf shape and structure. So, the ultimate goal is to develop a system where both multiple Gaussian and speckle noise can be removed and restored so that the image becomes noise free and produces clear vein and shape of the leaf which is highly essential for further process. Hybrid filter method is developed to remove the noise, enhance the quality of image and thereby produces better results compared to other traditional filters. Different parametric metrics are used to evaluate the performance of the hybrid filter and proves by giving suitable results when compared to other traditional filters. © 2012 IEEE.},
	author_keywords = {denoising; Gaussian and speckle noise; Multiple noise; wavelet thresholding; wiener filter},
	keywords = {Acoustic noise measurement; Feature extraction; Gaussian noise (electronic); Image segmentation; Speckle; De-noising; Multiple noise; Speckle noise; Wavelet thresholding; WIENER filters; Plants (botany)},
	correspondence_address = {N. Valliammal; Department of Computer Science, Avinashilingam Institute for Home Science and Higher Education for Women, Coimbatore-641 043, India; email: valli.p.2008@gmail.com},
	isbn = {978-145771545-7},
	language = {English},
	abbrev_source_title = {Int. Conf. Devices, Circuits Syst., ICDCS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 2012 International Conference on Devices, Circuits and Systems, ICDCS 2012; Conference date: 15 March 2012 through 16 March 2012; Conference code: 89815}
}

@CONFERENCE{Enficiaud2012520,
	author = {Enficiaud, Raffi and Mouine, Sofìene},
	title = {Landmark extraction from leaves with palmate venation: Application to grape},
	year = {2012},
	journal = {ICPRAM 2012 - Proceedings of the 1st International Conference on Pattern Recognition Applications and Methods},
	volume = {2},
	pages = {520 – 524},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862203819&partnerID=40&md5=a5e566383394502181948dacc62b34ff},
	affiliations = {INRIA Paris-Rocquencourt - IMEDIA, Domaine de Voluceau - Rocquencourt, 78153 Le Chesnay, B.P. 105, France},
	abstract = {The growing interest of Content Base Image Retrieval techniques in the context of plant identification requires the development of appropriate features. A considerable amount of information about the taxonomic identity of a plant is contained in its leaves, and most of the botanical expertise uses jointly the contour and the venation network. The current work focuses principally on the extraction of the venation network, the base and secondary landmarks of leaves with uncluttered background, assuming only their structure as palmate. Morphological operators are used to extract a first approximation of the venation network, which is then filtered by a voting scheme and reconstructed using a connected component like algorithm. The base point and the primary veins are then extracted with an accuracy of 100%, which allows identification of the lobes and the measurement their relative length.},
	author_keywords = {Computational botany; Mathematical morphology; Plant identification; Venation extraction},
	keywords = {Approximation algorithms; Mathematical morphology; Pattern recognition; A plants; Amount of information; Base points; Computational botany; Connected component; Image retrieval techniques; Morphological operator; Plant identification; Relative length; Venation networks; Voting schemes; Plants (botany)},
	correspondence_address = {R. Enficiaud; INRIA Paris-Rocquencourt - IMEDIA, Domaine de Voluceau - Rocquencourt, 78153 Le Chesnay, B.P. 105, France; email: raffi.enficiaud@mines-paris.org},
	isbn = {978-989842598-0},
	language = {English},
	abbrev_source_title = {ICPRAM - Proc. Int. Conf. Pattern Recogn. Appl. Methods},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 1st International Conference on Pattern Recognition Applications and Methods, ICPRAM 2012; Conference date: 6 February 2012 through 8 February 2012; Conference code: 90182}
}

@CONFERENCE{Zulkifli2011430,
	author = {Zulkifli, Zalikha and Saad, Puteh and Mohtar, Itaza Afiani},
	title = {Plant leaf identification using moment invariants & general regression neural network},
	year = {2011},
	journal = {Proceedings of the 2011 11th International Conference on Hybrid Intelligent Systems, HIS 2011},
	pages = {430 – 435},
	doi = {10.1109/HIS.2011.6122144},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856740566&doi=10.1109%2fHIS.2011.6122144&partnerID=40&md5=479ef793f5ddcec737394d7b246d0a5b},
	affiliations = {Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA (Perak), Seri Iskandar, Malaysia; Faculty of Computer Science and Information Systems, Universiti Teknologi Malaysia, Skudai, Malaysia},
	abstract = {Living plant identification based on images of leaf is a very challenging task in the field of pattern recognition and computer vision. However, leaf classification is an important component of computerized living plant recognition. The leaf contains important information for plant species identification despite its complexity. The objective of this study is to compare the effectiveness of Zernike Moment Invariant (ZMI), Legendre Moment Invariant (LMI) and Tchebichef Moment Invariant (TMI) features in extracting features from leaf images. Then, the features extracted from the most effective moment invariant technique are classified using the General Regression Neural Network (GRNN). There are two main stages involved in plant leaf identification. The first stage is known as feature extraction process where moment invariant methods are applied. The output of this process is a set of a global vector feature that represents the shape of the leaf images. It is shown that TMI can extract vector feature with Percentage of Absolute Error (PAE) less than 10.38 percent. Therefore, TMI vector feature will be the input to the second stage. The second stage involves classification of leaf images based on the derived feature gained in the previous stage. It is found that the feature vectors enabled the GRNN classifier to achieve 100 percent classification rate. Thus, the finding from this study can provide useful information for developing automated plant classification tools. © 2011 IEEE.},
	author_keywords = {moment invariants; plant leaf identification; regression neural network},
	keywords = {Image processing; Intelligent systems; Neural networks; Plants (botany); Vectors; Absolute error; Classification rates; Extracting features; Feature vectors; General regression neural network; Leaf classification; Leaf images; Legendre moments; Moment invariant; Plant classification; Plant identification; Plant leaf; Plant recognition; Plant species identification; Regression neural networks; Tchebichef moment invariants; Zernike moment invariants; Feature extraction},
	correspondence_address = {Z. Zulkifli; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA (Perak), Seri Iskandar, Malaysia; email: zalik764@perak.uitm.edu.my},
	isbn = {978-145772150-2},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Hybrid Intelligent Syst., HIS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 39; Conference name: 2011 11th International Conference on Hybrid Intelligent Systems, HIS 2011; Conference date: 5 December 2011 through 8 December 2011; Conference code: 88378}
}

@CONFERENCE{Nakarmi20103835,
	author = {Nakarmi, Akash D. and Tang, Lie},
	title = {Inter-plant spacing sensing at early growth stages using a time-of-flight of light based 3D vision sensor},
	year = {2010},
	journal = {American Society of Agricultural and Biological Engineers Annual International Meeting 2010, ASABE 2010},
	volume = {5},
	pages = {3835 – 3846},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649714907&partnerID=40&md5=4684658dad15a3369a8abe430c57c698},
	affiliations = {Dept. of Agricultural and Biosystems Engineering, Iowa State University, United States},
	abstract = {Uniform plant spacing is always desired for equal distribution of water and nutrients among plants. Researchers in the past have shown that variations in plant spacing result in significant variation in final crop yields. Planter manufacturers and researchers have been working closely to develop computer vision-based automatic interplant spacing sensing systems. Current systems mostly utilize top-view images using a stereo rig, or a video camera. These systems are highly sensitive to color variations introduced by shadow formations and glares and have difficulties when plant canopies start occluding. We developed an interplant spacing sensing system using a time-of-flight (TOF) based 3D vision sensor. The camera was capable of capturing depth and intensity data with one single shot. The depth images captured from the side were stitched together using distance information from a wheel encoder in conjunction with a feature-based image sequencing process. Multiple layers of image data were used for stem location identification. The use of depth images made the plant identification less sensitive to color variations. A covered vehicle was designed to prevent the sunlight from directly shedding on the plants and to reduce the interference from wind, which in turn made the system usable throughout the day. The vertical camera position was easily adjustable to work with different growth stages of the crops. The use of side-view images made the system capable to detecting inclined plants and therefore, boosted the performance of the system in precisely locating the stem centers, and thereby minimized the measurement errors. Based on the initial trials on corn plants of growth stages V3-V6, the system has achieved 100% plant identification accuracy with a RSME of 0.15 cm for inter-plant spacing measurements.},
	author_keywords = {3D vision; Early growth stages; Image processing; Spacing sensing; Time-of-flight},
	keywords = {Cameras; Computer vision; Crops; Engineers; Imaging systems; Location; Measurement errors; Sensors; Video cameras; 3-D vision; 3D vision sensor; Camera positions; Color variations; Corn plant; Crop yield; Current system; Depth image; Distance information; Distribution of water; Early growth; Early growth stages; Feature-based; Growth stages; Highly sensitive; Image data; Image sequencing; Intensity data; Location identification; Multiple layers; Plant canopies; Plant identification; Plant spacing; Sensing systems; Single shots; Spacing sensing; Stereo rigs; Time of flight; Wheel encoders; Three dimensional},
	publisher = {American Society of Agricultural and Biological Engineers},
	isbn = {978-161738835-4},
	language = {English},
	abbrev_source_title = {Am. Soc. Agric. Biol. Eng. Annu. Int. Meet.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Arefi2011379,
	author = {Arefi, Arman and Motlagh, Asad Modarres and Khoshroo, Alireza},
	title = {Recognition of weed seed species by image processing},
	year = {2011},
	journal = {Journal of Food, Agriculture and Environment},
	volume = {9},
	number = {1},
	pages = {379 – 383},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755143220&partnerID=40&md5=5790427c67c13ae62587a2f9975b36b8},
	affiliations = {Department of Agricultural Machinery, Faculty of Agriculture, Urmia University, Urmia, P. O. Box 57155-1849, Iran; Department of Agricultural Engineering, Faculty of Agriculture, Yasouj University, Yasouj, Iran},
	abstract = {Recently, in order to increase the speed detection of seeds, the methods based on computer vision is expanded. Since, some characteristics such as colour, morphology and texture show the difference of objects. Therefore, it is necessary that these parameters should be used by machine vision for reorganization of different objects from each other. In this study, identification of four major weed seeds (common vetch, cleavers, cornflower and great bur-parsley), that are widely found in farms of West and North West of Iran, was done by digital image analysis. Recognizing and removing the weed seeds from the main product is very important. By recognizing the weed seed species from each other, it is possible to determine the percentage of farm pollution to each of weed seeds, and then weed control operations will be applied. The value of products is determined through ratio of weed seeds weight to total weight. Therefore, recognition of weed seed species as the first step in determination of products value is very important. For this purpose, by using a chamber of imaging, some uniform images of samples were acquired. Then, a program was coded in Matlab software for segmentation of the samples. Recognition of weed seeds was based on morphology and colour. For recognizing common vetch, great bur-parsley and cornflower colour features defined in RGB and HSI colour models were used. These colour features were mean (red), mean and variance of saturation component. Recognition of cleavers was done by two morphology features, Shape factor 1 and Shape factor 2. According to the results, total classification accuracy was 98.40%. This shows that the system has great potential to serve as an intelligent recognition system in real applications.},
	author_keywords = {Image processing; Machine vision; Variety identification; Weed seed},
	keywords = {Centaurea cyanus; Galium aparine; Petroselinum crispum; Vicia sativa subsp. nigra; agricultural parameters; article; common vetch; computer program; cornflower; image analysis; image processing; Iran; nonhuman; plant; plant identification; plant seed; weed; weed seed},
	correspondence_address = {A. Arefi; Department of Agricultural Machinery, Faculty of Agriculture, Urmia University, Urmia, P. O. Box 57155-1849, Iran; email: arefi.arman@yahoo.com},
	publisher = {WFL Publisher Ltd.},
	issn = {14590255},
	language = {English},
	abbrev_source_title = {J. Food Agric. Environ.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Goh20091302,
	author = {Goh, Hanlin and Lim, Joo-Hwee and Quek, Chai},
	title = {Fuzzy associative conjuncted maps network},
	year = {2009},
	journal = {IEEE Transactions on Neural Networks},
	volume = {20},
	number = {8},
	pages = {1302 – 1319},
	doi = {10.1109/TNN.2009.2023213},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-68949209640&doi=10.1109%2fTNN.2009.2023213&partnerID=40&md5=d4b042eaed09dbf831507eee3c7b8008},
	affiliations = {Centre for Computational Intelligence, School of Computer Engineering, Nanyang Technological University, Singapore 639798, Singapore; Computer Vision and Image Understanding Department, Institute for Infocomm Research, Singapore 119613, Singapore},
	abstract = {The fuzzy associative conjuncted maps (FASCOM) is a fuzzy neural network that associates data of nonlinearly related inputs and outputs. In the network, each input or output dimension is represented by a feature map that is partitioned into fuzzy or crisp sets. These fuzzy sets are then conjuncted to form antecedents and consequences, which are subsequently associated to form if-then rules. The associative memory is encoded through an offline batch mode learning process consisting of three consecutive phases. The initial unsupervised membership function initialization phase takes inspiration from the organization of sensory maps in our brains by allocating membership functions based on uniform information density. Next, supervised Hebbian learning encodes synaptic weights between input and output nodes. Finally, a supervised error reduction phase fine-tunes the network, which allows for the discovery of the varying levels of influence of each input dimension across an output feature space in the encoded memory. In the series of experiments, we show that each phase in the learning process contributes significantly to the final accuracy of prediction. Further experiments using both toy problems and real-world data demonstrate significant superiority in terms of accuracy of nonlinear estimation when benchmarked against other prominent architectures and exhibit the network's suitability to perform analysis and prediction on real-world applications, such as traffic density prediction as shown in this paper. © 2009 IEEE.},
	author_keywords = {Fuzzy associative conjuncted maps (FASCOM); Fuzzy associative memory; Fuzzy neural networks; Hebbian learning; Iris plant classification; Multivariate data analysis; Nakanishi's nonlinear estimation tasks; Neurofuzzy systems; Supervised learning; Traffic density prediction; Two-spiral classification; Unsupervised learning},
	keywords = {Algorithms; Artificial Intelligence; Automobiles; Databases, Factual; Fuzzy Logic; Humans; Information Theory; Iris Plant; Learning; Memory; Mental Recall; Multivariate Analysis; Neural Networks (Computer); Neuronal Plasticity; Neurons; Nonlinear Dynamics; Pattern Recognition, Automated; Synaptic Transmission; Associative processing; Classification (of information); Education; Estimation; Fuzzy sets; Membership functions; Microcontrollers; Multivariant analysis; Nonlinear network analysis; Supervised learning; Unsupervised learning; Fuzzy associative conjuncted maps (FASCOM); Fuzzy associative memory; Hebbian learning; Iris plant classification; Multivariate data analysis; Nakanishi's nonlinear estimation tasks; Neurofuzzy systems; Traffic density prediction; Two-spiral classification; algorithm; article; artificial intelligence; artificial neural network; automated pattern recognition; car; classification; factual database; fuzzy logic; human; information science; Iridaceae; learning; memory; methodology; multivariate analysis; nerve cell; nerve cell plasticity; nonlinear system; physiology; recall; synaptic transmission; Fuzzy neural networks},
	correspondence_address = {H. Goh; Centre for Computational Intelligence, School of Computer Engineering, Nanyang Technological University, Singapore 639798, Singapore; email: hlgoh@i2r.a-star.edu.sg},
	issn = {10459227},
	coden = {ITNNE},
	pmid = {19635694},
	language = {English},
	abbrev_source_title = {IEEE Trans Neural Networks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Green Open Access}
}

@CONFERENCE{Huang2010225,
	author = {Huang, Rong-Guo and Jin, Sang-Hyeon and Han, Ying-Li and Hong, Kwang-Seok},
	title = {Flower image recognition based on image rotation and DIE},
	year = {2010},
	journal = {Proceeding - 6th International Conference on Digital Content, Multimedia Technology and Its Applications, IDC2010},
	pages = {225 – 228},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958037854&partnerID=40&md5=c2581e2d4e3edd08af24f773eb81bc32},
	affiliations = {School of Information and Communication Engineering, Sungkyunkwan University, Suwon, Kyung Ki-do, 440-746, 300 Chunchun-dong, Jangan-gu, South Korea},
	abstract = {In this paper, we suggest and implement a flower image recognition system using Difference Image Entropy (hereinafter, DIE), image rotation, contour and color information of the object. Conventional studies on flower or leaf recognition have restrictions and limitations that include a sharp drop of recognition rate due to the varying positions and number of objects in the original object image. Hence, this paper focuses on 1) contour feature extraction technology by drawing and designating flower region of the user's interest, 2) image rotation and color feature extraction technology by drawing and designating flower region of the user's interest, and 3) a distributed processing-based flower image recognition technology using DIE, for robust flower image recognition from the given original flower image with multi-flower objects. The suggested system was evaluated using fourteen species of flowers with each ten samples. Experimental results achieved an average recognition rate of 93.6%.},
	keywords = {Dies; Image recognition; Rotation; Technology; Color feature extraction; Color information; Contour features; Difference image entropies; Distributed processing; Extraction technology; Flower image recognition; Image rotation; Leaf recognition; Recognition rates; User's interest; Feature extraction},
	correspondence_address = {R.-G. Huang; School of Information and Communication Engineering, Sungkyunkwan University, Suwon, Kyung Ki-do, 440-746, 300 Chunchun-dong, Jangan-gu, South Korea; email: hrg316@skku.edu},
	isbn = {978-898867827-5},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Digit. Content, Multimedia Technol. Its Appl., IDC},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 6th International Conference on Digital Content, Multimedia Technology and Its Applications, IDC2010; Conference date: 16 August 2010 through 18 August 2010; Conference code: 81944}
}

@ARTICLE{Guo201175,
	author = {Guo, L.P. and Huang, L.Q. and Zhang, X.P. and Bittner, L. and Pezzei, C. and Pallua, J. and Schönbichler, S. and Huck-Pezzei, V.A. and Bonn, G.K. and Huck, C.W.},
	title = {Application of near-infrared spectroscopy (NIRS) as a tool for quality control in traditional Chinese medicine (TCM)},
	year = {2011},
	journal = {Current Bioactive Compounds},
	volume = {7},
	number = {2},
	pages = {75 – 84},
	doi = {10.2174/157340711796011188},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959359512&doi=10.2174%2f157340711796011188&partnerID=40&md5=23b604d86dd78cc8f521ca0a05896d01},
	affiliations = {Institute of Chinese Materia Medica, China Academy of Chinese Medical Sciences, Beijing, China; Institute of Analytical Chemistry and Radiochemistry, Leopold-Franzens University, 6020 Innsbruck, Innrain 52a, Austria},
	abstract = {Traditional Chinese Medicine (TCM) is becoming more and more popular all over the world. Novel analytical tools for quality control are highly demanded enabling analysis starting at breeding and ending at biological fluids including urine or serum. Compared to analytical separation methods (chromatography, electrophoresis) near-infrared spectroscopy (NIRS) allows analyzing matter of interest non-invasively, fast and physical/chemical parameters simultaneously. It can be used for the quantitative control of certain (active) ingredients. In many cases identification can only be achieved by pattern recognition. Therefore, NIRS combined with cluster analysis offers huge potential to identify e.g. species, geographic origin, special medicinal formula etc. In the present contribution the fundamentals, possibilities of NIR applied in quality control of TCM are pointed out and its ad-and disadvantages are discussed in detail by several practical examples. © 2011 Bentham Science Publishers Ltd.},
	author_keywords = {Geoherbs; Near-infrared spectroscopy (NIRS); Processing; Quality; Traditional chinese medicine (TCM)},
	keywords = {Chinese drug; analytic method; article; Chinese medicine; cluster analysis; controlled study; human; infrared spectroscopy; plant identification; priority journal; quality control; traditional medicine},
	correspondence_address = {C. W. Huang; Institute of Analytical Chemistry and Radiochemistry, Leopold-Franzens University, 6020 Innsbruck, Innrain 52a, Austria; email: huangluqi@263.net},
	issn = {18756646},
	language = {English},
	abbrev_source_title = {Curr. Bioact. Compd.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12}
}

@CONFERENCE{Weiss2010339,
	author = {Weiss, Ulrich and Biber, Peter and Laible, Stefan and Bohlmann, Karsten and Zell, Andreas},
	title = {Plant species classification using a 3D LIDAR sensor and machine learning},
	year = {2010},
	journal = {Proceedings - 9th International Conference on Machine Learning and Applications, ICMLA 2010},
	pages = {339 – 345},
	doi = {10.1109/ICMLA.2010.57},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952423722&doi=10.1109%2fICMLA.2010.57&partnerID=40&md5=9c3e7d699c1f1f93f99009f4fb6a71a0},
	affiliations = {Corporate Sector Research and Advance Engineering, Robert Bosch GmbH, Schwieberdingen, Germany; Wilhelm-Schickard-Institute for Computer Science (WSI), University of Tuebingen, Tuebingen, Germany},
	abstract = {In the domain of agricultural robotics, one major application is crop scouting, e.g., for the task of weed control. For this task a key enabler is a robust detection and classification of the plant and species. Automatically distinguishing between plant species is a challenging task, because some species look very similar. It is also difficult to translate the symbolic high level description of the appearances and the differences between the plants used by humans, into a formal, computer understandable form. Also it is not possible to reliably detect structures, like leaves and branches in 3D data provided by our sensor. One approach to solve this problem is to learn how to classify the species by using a set of example plants and machine learning methods. In this paper we are introducing a method for distinguishing plant species using a 3D LIDAR sensor and supervised learning. For that we have developed a set of size and rotation invariant features and evaluated experimentally which are the most descriptive ones. Besides these features we have also compared different learning methods using the toolbox Weka. It turned out that the best methods for our application are simple logistic regression functions, support vector machines and neural networks. In our experiments we used six different plant species, typically available at common nurseries, and about 20 examples of each species. In the laboratory we were able to identify over 98% of these plants correctly. © 2010 IEEE.},
	author_keywords = {3D laser sensor; Agricultural robotics; Plant classification; Supervised learning},
	keywords = {Neural networks; Optical radar; Robotics; Robots; Sensors; Supervised learning; Three dimensional; Weed control; 3D data; Agricultural robotics; High level description; Laser sensor; Learning methods; LIDAR sensors; Logistic regressions; Machine learning methods; Machine-learning; Plant classification; Plant species; Robust detection; Rotation invariant features; Agricultural machinery},
	correspondence_address = {U. Weiss; Corporate Sector Research and Advance Engineering, Robert Bosch GmbH, Schwieberdingen, Germany; email: ulrich.weiss@de.bosch.com},
	isbn = {978-076954300-0},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Mach. Learn. Appl., ICMLA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 39; Conference name: 9th International Conference on Machine Learning and Applications, ICMLA 2010; Conference date: 12 December 2010 through 14 December 2010; Conference code: 84064; All Open Access, Green Open Access}
}

@CONFERENCE{Yanikoglu2011,
	author = {Yanikoglu, Berrin and Aptoula, Erchan and Tirkaz, Caglar},
	title = {Sabanci-Okan System at Image Clef 2011: Plant identification task},
	year = {2011},
	journal = {CEUR Workshop Proceedings},
	volume = {1177},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922032436&partnerID=40&md5=a1252a9239ee0f66a4d38c9bb677b4fe},
	affiliations = {Sabanci University, Istanbul, 34956, Turkey; Okan University, Istanbul, 34959, Turkey},
	abstract = {We describe our participation in the plant identification task of Image Clef 2011. Our approach employs a variety of texture, shape as well as color descriptors. Due to the morphometric properties of plants, mathematical morphology has been advocated as the main methodology for texture characterization, supported by a multitude of contour-based shape and color features. We submitted a single run, where the focus has been almost exclusively on scan and scan-like images, due primarily to lack of time. Moreover, special care has been taken to obtain a fully automatic system, operating only on image data. While our photo results are low, we consider our submission successful, since besides being our first attempt, our accuracy is the highest when considering the average of the scan and scan-like results, upon which we had concentrated our efforts.},
	author_keywords = {Fourier descriptors; Mathematical morphology; Morphological covariance; Plant identification; Support vector machines},
	keywords = {Mathematical morphology; Support vector machines; Textures; Automatic systems; Color descriptors; Color features; Fourier descriptors; Image data; Morphological covariance; Plant identification; Texture characterizations; Image processing},
	editor = {Petras V. and Forner P. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Clough P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 2011 Cross Language Evaluation Forum Conference, CLEF 2011; Conference date: 19 September 2011 through 22 September 2011; Conference code: 110352}
}

@CONFERENCE{Pornpanomchai2010V1123,
	author = {Pornpanomchai, Chomtip and Sakunreraratsame, Ponrath and Wongsasirinart, Rosita and Youngtavichavhart, Nuttakan},
	title = {Herb flower recognition system (HFRS)},
	year = {2010},
	journal = {ICEIE 2010 - 2010 International Conference on Electronics and Information Engineering, Proceedings},
	volume = {1},
	pages = {V1123–V1127},
	doi = {10.1109/ICEIE.2010.5559906},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049337322&doi=10.1109%2fICEIE.2010.5559906&partnerID=40&md5=d01ffaed8d84d45af2deba2cf5a51e6d},
	affiliations = {Faculty of Information and Communication Technology, Mahidol University, Rajchatawee, Bangkok 10400, Rama 6 Road, Thailand},
	abstract = {The objective of this research is to build an automatic method for recognizing a Thai herb flower based on the Minimum Distance Method. The herb flower images, acquired from a digital camera, are taken in the real environment. We use the characteristics of herb flowers to design our classification algorithms, which consist of the average red, green and blue (RGB) colors, the herb flower size and the edge of petals feature. The experiments are conducted on more than 380 pictures from 16 species of herb flowers. The training data set is around 220 pictures. We test the system by using 110 pictures for a training data set and 50 pictures for an un-training data set. The precision rates of the recognition system are 98.18 percent and 94 percent, respectively. The average access time is 0.87 seconds per image. © 2010 IEEE.},
	author_keywords = {Flower features extraction; Herb flower recognition; Minimum distance method},
	keywords = {Cameras; Statistical tests; Access time; Automatic method; Classification algorithm; Features extraction; Flower recognition; Flower size; Minimum distance; Precision rates; Real environments; Recognition systems; Red , green and blues; Training data sets; Feature extraction},
	correspondence_address = {C. Pornpanomchai; Faculty of Information and Communication Technology, Mahidol University, Rajchatawee, Bangkok 10400, Rama 6 Road, Thailand; email: itcpp@mahidol.ac.th},
	isbn = {978-142447680-0},
	language = {English},
	abbrev_source_title = {ICEIE - Int. Conf. Electron. Inf. Eng., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 2010 International Conference on Electronics and Information Engineering, ICEIE 2010; Conference date: 1 August 2010 through 3 August 2010; Conference code: 81916}
}

@CONFERENCE{Siemens20123221,
	author = {Siemens, Mark C. and Herbon, Ryan and Gayler, Ronald R. and Nolte, Kurt D. and Brooks, Davie},
	title = {Automated machine for thinning lettuce - Development and evaluation},
	year = {2012},
	journal = {American Society of Agricultural and Biological Engineers Annual International Meeting 2012, ASABE 2012},
	volume = {4},
	pages = {3221 – 3234},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871741047&partnerID=40&md5=20ee72808a21939c0b00ce722478a4cf},
	affiliations = {University of Arizona, Yuma, AZ 85364, 6425 W. 8th St., United States; Mule Deer Automation, Silver City, NM 88061, 5212 Little Walnut Rd., United States; University of Arizona Cooperative Extension, Yuma, AZ 85364, 2200 W. 28th St., United States; Pasquinelli Produce Co., Yuma, AZ 85364, 2144 W. 24th St., United States},
	abstract = {Despite the tremendous advances made in agricultural mechanization, the precise task of thinning lettuce seedlings is still performed by crews of workers using hand hoes in commercial field production. Finding laborers to perform this physically demanding work is becoming increasingly difficult. The objectives of this project were to 1) develop an automated machine for thinning lettuce to improve labor resource use efficiency and 2) evaluate the performance of the machine as compared to conventional methods. The prototype machine was developed to thin lettuce seedlings nominally planted 2 inches apart to the desired final plant spacing of 10-11 inches. The device is principally comprised of a machine vision system for detecting lettuce seedlings and their location and a system for intermittently delivering an herbicidal spray to kill plants. When tested at a travel speed of 1.5 mph, automated machine thinning performance in terms of plant spacing, plant spacing uniformity, number of live plants per acre and time required for a hand laborer to remove plants missed during thinning was not significantly different from hand thinning when plants were thinned using sulfuric acid (10% v/v), paraquat (1.3 pt a.i./acre) or vinegar (20% v/v) (P = 0.05). Automated machine thinning performance after hand weeding was also not significantly from hand thinning in terms of plant spacing, COV in plant spacing and plant stand when plants were sprayed with any of the five chemicals tested. Based on these results, it was concluded that the automated machine was able to reliably control spray delivery such that plants were selectively thinned to the desired final plant spacing. Yields were not significantly affected when plants were thinned using the automated machine with any of the herbicidal spray solutions tested indicating that the machine was able to deliver herbicidal spray with sufficient accuracy that seedlings are not injured. Further machine development is needed so that a unit more suitable for commercial production can be tested on a large scale to confirm these results and to determine whether such a system is viable for use in commercial production.},
	author_keywords = {Automated; Lettuce; Machine; Machine vision; Performance testing; Plant recognition; Precision; Seedlings; Thinning},
	keywords = {Acetic acid; Agricultural machinery; Agriculture; Automation; Computer vision; Sulfuric acid; Automated; Lettuce; Machine; Performance testing; Plant recognition; Precision; Seedlings; Thinning; Seed},
	correspondence_address = {M.C. Siemens; University of Arizona, Yuma, AZ 85364, 6425 W. 8th St., United States; email: siemens@cals.arizona.edu},
	publisher = {American Society of Agricultural and Biological Engineers},
	isbn = {978-162276208-8},
	language = {English},
	abbrev_source_title = {Am. Soc. Agric. Biol. Eng. Annu. Int. Meet.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: American Society of Agricultural and Biological Engineers Annual International Meeting 2012; Conference date: 29 July 2012 through 1 August 2012; Conference code: 94562}
}

@CONFERENCE{Paris2012,
	author = {Paris, Sébastien and Halkias, Xanadu and Glotin, Hervé},
	title = {Participation of LSIS/DYNI to ImageCLEF 2012 plant images classiFIcation task},
	year = {2012},
	journal = {CEUR Workshop Proceedings},
	volume = {1178},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922022479&partnerID=40&md5=fe093e7ba8ee7bffad7f76c2873df239},
	affiliations = {LSIS/DYNI, Aix-Marseille University, France; LSIS/DYNI, University of South Toulon-Var, France; Institut National de France, France},
	abstract = {This paper presents the participation of the LSIS/DYNI team for the ImageCLEF 2012 plant identification challenge. Image- CLEF's plant identification task provides a testbed for the system-oriented evaluation of tree species identification based on leaf images. The goal is to investigate image retrieval approaches in the context of crowd sourced images of leaves collected in a collaborative manner. The LSIS/DYNI team submitted three runs to this task and obtained the best evaluation scores (S = 0:32) for the "photograph" image category with an automatic method. Our approach is based on a modern computer vision framework involving local, highly discriminative visual descriptors, sophisticated visual-patches encoder and large-scale supervised classifi- cation. The paper presents the three procedures employed, and provides an analysis of the obtained evaluation results.},
	author_keywords = {Benchmark; Classification; Collection; DYNI; Evaluation; Identification; ImageCLEF; Images; Leaves; LSIS; Plant},
	keywords = {Benchmarking; Classification (of information); Computer vision; Identification (control systems); Image retrieval; Plants (botany); Refuse collection; DYNI; Evaluation; ImageCLEF; Images; Leaves; LSIS; Plant; Image processing},
	editor = {Karlgren J. and Womser-Hacker C. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Forner P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2012 Cross Language Evaluation Forum Conference, CLEF 2012; Conference date: 17 September 2012 through 20 September 2012; Conference code: 110353}
}

@ARTICLE{Ji-Yong201119687,
	author = {Ji-Yong, Shi and Xiao-Bo, Zou and Jie-Wen, Zhao and Han-Ping, Mao and Kai-Liang, Wang and Zheng-Wei, Chen and Xiao-Wei, Huang},
	title = {Diagnostics of nitrogen deficiency in mini-cucumber plant by near infrared reflectance spectroscopy},
	year = {2011},
	journal = {African Journal of Biotechnology},
	volume = {10},
	number = {85},
	pages = {19687 – 19692},
	doi = {10.5897/AJB11.557},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855814461&doi=10.5897%2fAJB11.557&partnerID=40&md5=3c2ef2832f4bf1420402a7610bc104c0},
	affiliations = {School of Food and Biological Engineering, Jiangsu University, 212013 Zhenjiang, Jiangsu, 301 Xuefu Road, China; Key Laboratory of Modern Agricultural Equipment and Technology, 212013 Zhenjiang, Jiangsu, 301 Xuefu Road, China},
	abstract = {In protected agriculture, deficiency of an essential element may drastically affect plant growth, appearance and most importantly yield. Information about nutrient deficiencies in crops grown in controlled environment is essential to optimize food productivity. In this study, near infrared reflectance spectroscopy (NIRS) analysis was used to identify nitrogen (N) deficiency coupled with pattern recognition methods in mini-cucumber plants grown under non-soil conditions. Leaves at the first three nodes of nitrogen deficient plants and control plant were used for NIRS data acquisition. K-nearest neighbors (KNN) and artificial neural network (ANN) were applied to build diagnostics models, respectively. Some parameters of the model were optimized by cross-validation. The performance of the KNN model and the ANN model based on NIRS data was compared. Experiment results showed that the ANN model was better than the KNN model. The optimal ANN model was achieved when principle component factors were equal to 5 and identification rate of the ANN model were 100% in both the training set and the prediction set. This study demonstrated that the NIRS coupled with ANN pattern recognition method can be successfully applied to the diagnostics of nitrogen deficiency in minicucumber plant grown under non-soil conditions. © 2011 Academic Journals.},
	author_keywords = {Artificial neural network; Deficiency; Near infrared reflectance spectroscopy (NIRS); Nitrogen},
	keywords = {Cucumis sativus; nitrogen; analytic method; article; artificial neural network; controlled study; cucumber; intermethod comparison; k nearest neighbor; near infrared reflectance spectroscopy; nitrogen deficiency; nonhuman; plant growth; plant identification; plant leaf; plant nutrition; process model; process optimization},
	correspondence_address = {Z. Xiao-Bo; School of Food and Biological Engineering, Jiangsu University, 212013 Zhenjiang, Jiangsu, 301 Xuefu Road, China; email: zou_xiaobo@ujs.edu.cn},
	issn = {16845315},
	language = {English},
	abbrev_source_title = {Afr. J. Biotechnol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@CONFERENCE{Mzoughi20121033,
	author = {Mzoughi, Olfa and Yahiaoui, Itheri and Boujemaa, Nozha},
	title = {Petiole shape detection for advanced leaf identification},
	year = {2012},
	journal = {Proceedings - International Conference on Image Processing, ICIP},
	pages = {1033 – 1036},
	doi = {10.1109/ICIP.2012.6467039},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875866856&doi=10.1109%2fICIP.2012.6467039&partnerID=40&md5=f21f702e3f91fb8e99197b0ed08798cf},
	affiliations = {INRIA Rocquencourt, France; CReSTIC Universit́e de Reims, France},
	abstract = {Automatic plant identification is a relatively new research area in computer vision that has increasingly attracted high interest as a promising solution for the development of many botanical industries and for the success of biodiversity conservation. Most of the approaches proposed are based on the analysis of morphological properties of leaves. They have applied several well-known generic shape descriptors. Nevertheless, faced with the large amount of leaf species, botanical knowledge, especially about leaf parts (petiole, blade and their insertion point) is important to enhance their precision, hence, a crucial need to extract them from image. In this paper, we propose a fully automatic approach for petiole detection, based on the concept of local translational symmetry, which is applied to a some regions of the leaf. These regions are chosen w.r.t their size (small) taking into account the large diversity of leaf morphology (compound, oblong, orbicular). This method has been tested on two datasets and has provided more than 90% of correct detections. © 2012 IEEE.},
	author_keywords = {contour; Leaf identification; local translational symmetry; petiole},
	keywords = {Conservation; Plants (botany); Automatic approaches; Biodiversity conservation; contour; Generic shapes; Large amounts; Leaf identification; Leaf morphology; Morphological properties; petiole; Plant identification; Shape detection; Translational symmetry; Image processing},
	issn = {15224880},
	isbn = {978-146732533-2},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Image Process. ICIP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; Conference name: 2012 19th IEEE International Conference on Image Processing, ICIP 2012; Conference date: 30 September 2012 through 3 October 2012; Conference code: 95956}
}

@CONFERENCE{Nasirzadeh2010308,
	author = {Nasirzadeh, M. and Khazael, A. Arab and Bin Khalid, Marzuki},
	title = {Woods recognition system based on local binary pattern},
	year = {2010},
	journal = {Proceedings - 2nd International Conference on Computational Intelligence, Communication Systems and Networks, CICSyN 2010},
	pages = {308 – 313},
	doi = {10.1109/CICSyN.2010.27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649847450&doi=10.1109%2fCICSyN.2010.27&partnerID=40&md5=076db97c95ed84cfbea48da44b0da294},
	affiliations = {Department of Electrical Engineering, University Technology Malaysia, Skudai, Malaysia},
	abstract = {Malaysia is the largest exporter of tropical woods in the world, accounting for 70 percent of the world's supply of raw-logs. Sabah and Sarawak, the two Malaysian states on the island of Borneo, occupies some of the oldest and the most diverse rain forest in the world. Malaysia has a rich variety of tree species, and the wood produced from each of these has unique structure, physical and mechanical properties. The differences in woods structure and properties allow for the manufacture of woods based products with many different appearances and uses. In order to use this precious material efficiently, proper species must be used in the appropriate places. Intelligent Woods species recognition is a new application studied in the Computer Vision field to help prevent misclassifying of woods species in woods industries. Woods recognition is an implementation on identifying the different species of woods provided with the images captured for the woods samples or the characteristics observed. In this study, the features from the enhanced woods images are extracted using the LBP histogram, which determines the classification between the various woods species. The recognition is performed using a nearest neighbor classifier in the computed feature space with Chi square as a dissimilarity measure. The intelligent woods recognition system is designed to explore the possibility of developing a system which is able to perform automated woods recognition based on woods anatomy. The result thus obtained shows a high rate of recognition accuracy proving that the techniques due to its rotation invariance and robustness to gray-scale variations are very promising for practical applications. © 2010 IEEE.},
	author_keywords = {Histogram Fourier transforms; Image processing; Local binary pattern; Nearest neighborhood; Woods recognition systems},
	keywords = {Artificial Intelligence; Binary Systems; Communication; Fourier Analysis; Graphic Methods; Image Analysis; Malaysia; Mechanical Properties; Pattern Recognition; Patterns; Species Identification; Wood; Artificial intelligence; Communication systems; Computer vision; Feature extraction; Fourier transforms; Graphic methods; Imaging systems; Mechanical properties; Wood; Dissimilarity measures; Feature space; Gray scale; High rate; Histogram Fourier transforms; Local binary patterns; Malaysia; Malaysians; Nearest Neighbor classifier; Nearest neighborhood; New applications; Physical and mechanical properties; Rain forests; Recognition accuracy; Recognition systems; Rotation invariance; Sarawak; Species recognition; Structure and properties; Tree species; Tropical wood; Pattern recognition systems},
	correspondence_address = {M. Nasirzadeh; Department of Electrical Engineering, University Technology Malaysia, Skudai, Malaysia; email: maryamnasirzade@yahoo.com},
	isbn = {978-076954158-7},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Comput. Intell., Commun. Syst. Networks, CICSyN},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 36; Conference name: 2nd International Conference on Computational Intelligence, Communication Systems and Networks, CICSyN 2010; Conference date: 28 July 2010 through 30 July 2010; Conference code: 82816}
}

@ARTICLE{Rossatto2011103,
	author = {Rossatto, Davi Rodrigo and Casanova, Dalcimar and Kolb, Rosana Marta and Bruno, Odemir Martinez},
	title = {Fractal analysis of leaf-texture properties as a tool for taxonomic and identification purposes: A case study with species from Neotropical Melastomataceae (Miconieae tribe)},
	year = {2011},
	journal = {Plant Systematics and Evolution},
	volume = {291},
	number = {1},
	pages = {103 – 116},
	doi = {10.1007/s00606-010-0366-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650928978&doi=10.1007%2fs00606-010-0366-2&partnerID=40&md5=0c1c2f10662a9e30cd0de668b59ad594},
	affiliations = {Laboratório de Fisiologia Vegetal, Departamento de Botânica, Universidade de Brasília-UnB, 70904-970 Brasília, DF, Caixa postal 04457, Brazil; Instituto de Física de São Carlos, Universidade de São Paulo-USP, 13560-970 São Carlos, SP, Av. Trabalhador São Carlense, 400, Brazil; Faculdade de Ciências e Letras, Departamento de Ciências Biológicas, Universidade Estadual Paulista-UNESP, 19806-900 Assis, SP, Av. Dom Antônio, 2100, Brazil},
	abstract = {Melastomataceae is a common and dominant family in Neotropical vegetation, with high species diversity which leads to a large variation in some morphological structures. Despite this, some species of Melastomataceae are very similar in their external leaf morphology, leading to difficulties in their identification without the presence of reproductive organs. Here we have proposed and tested a computer-aided texture-based approach used to correctly identify and distinguish leaves of some species of Melastomataceae that occur in a region of Neotropical savanna in Southeastern Brazil, also comparing it with other previously proposed approaches. The results demonstrated that our approach may clearly separate the studied species, analyzing the patterns of leaf texture (both adaxial and abaxial surfaces), and achieving better accuracy (100%) than other methods. Our work has suggested that leaf texture properties can be used as a new characteristic for identification, and as an additional source of information in taxonomic and systematic studies. As the method may be supervised by experts, it is also suitable for discrimination of species with high morphological plasticity, improving the automated discrimination task. This approach can be very useful for identification of species in the absence of reproductive material, and is a rapid and powerful tool for plant identification. © 2010 Springer-Verlag.},
	author_keywords = {Computer vision; Fractal analysis; Leaf texture; Melastomataceae; Plant identification; Volumetric fractal},
	keywords = {Melastomataceae; Miconieae},
	correspondence_address = {O. M. Bruno; Instituto de Física de São Carlos, Universidade de São Paulo-USP, 13560-970 São Carlos, SP, Av. Trabalhador São Carlense, 400, Brazil; email: bruno@ifsc.usp.br},
	issn = {16156110},
	coden = {ESPFB},
	language = {English},
	abbrev_source_title = {Plant Syst. Evol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 36}
}

@ARTICLE{Yusof20127363,
	author = {Yusof, Rubiyah and Khairuddin, Uswah and Khalid, Marzuki},
	title = {A new mutation operation for faster convergence in genetic algorithm feature selection},
	year = {2012},
	journal = {International Journal of Innovative Computing, Information and Control},
	volume = {8},
	number = {10 B},
	pages = {7363 – 7378},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866049073&partnerID=40&md5=27534e0ba4cbeb332298ca21032ef663},
	affiliations = {Centre for Artificial Intelligence and Robotics, Universiti Teknologi Malaysia, Jalan Semarak, 54100, Kuala Lumpur, Malaysia},
	abstract = {Feature selection is an important step in data classification because it has a high impact on classification accuracy. Feature selection using Genetic Algorithm (GA) is usually done in a wrapper method. The process is time consuming especially for large dimensional database. We propose a new mutation operation for faster feature selection by GA based on elitism of the allele. Normal elitism in GA preserves the most fit chromosomes which are evaluated using the fitness function. In the same way, the highest fit allele will be preserved and the fitness of the allele is evaluated based on the frequency of occurrences. The chromosome undergoing this mutation process will have a high if not the highest fitness because it is created based on a high fit allele. It will be the catalyst to increase the rate of convergence towards achieving an optimal features combination. Experiments for feature selection using this method are conducted using a database of tropical wood species which has a large variation of features. Results of the experiments show that a high accuracy is obtained for the recognition of the tropical wood species using the feature selection method. In addition, it has also been shown that the chromosomes created by the new mutation operation have high fitness and the rate of optimal convergence is improved substantially. The new mutation operation is not only useful for large database, but also can be used for small or medium sized database. ICIC International © 2012.},
	author_keywords = {Classification; Feature selection; GA; Wood recognition},
	keywords = {Approximation theory; Classification (of information); Experiments; Feature extraction; Gallium; Genes; Genetic algorithms; Health; Classification accuracy; Data classification; Dimensional database; Faster convergence; Feature selection methods; Fitness functions; High impact; Large database; Medium-sized database; Mutation operations; Mutation process; Optimal convergence; Rate of convergence; Tropical wood; Wood recognition; Wrapper methods; Database systems},
	correspondence_address = {R. Yusof; Centre for Artificial Intelligence and Robotics, Universiti Teknologi Malaysia, Jalan Semarak, 54100, Kuala Lumpur, Malaysia; email: rubiyah@ic.utm.my},
	issn = {13494198},
	language = {English},
	abbrev_source_title = {Int. J. Innov. Comput. Inf. Control},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20}
}

@ARTICLE{Khairuddin2011441,
	author = {Khairuddin, Uswah and Yusof, Rubiyah and Khalid, Marzuki and Cordova, Florian},
	title = {Optimized feature selection for improved tropical wood species recognition system},
	year = {2011},
	journal = {ICIC Express Letters, Part B: Applications},
	volume = {2},
	number = {2},
	pages = {441 – 446},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952427379&partnerID=40&md5=ffbb6f67bbea6d82d24456c50947e295},
	affiliations = {Centre for Artificial Intelligence and Robotics (CAIRO), Universiti Teknologi Malaysia, Jalan Semarak, 54100, Kuala Lumpur, Malaysia},
	abstract = {An automated wood recognition system can classify wood species in just a matter of seconds and can replace manual inspection by human. The system captures image of wood surface and features are extracted from these images before being input into the classifier. However, due to variations in tropical wood species, a single feature extraction method cannot accommodate multispecies classification. Therefore, we propose the use of two feature extraction method which are Basic Grey Level Aura Matrix (BGLAM) and statistical properties of pores distribution (SPPD). Features from both feature extractors are fused together. However, the use of the fusion method resulted in the increase in the number of features for classification. In order to ensure only features that are discriminating enough as well as reduce the number of features, we developed an optimize feature selection algorithm to the fused feature set by selecting only discriminant features to be used for classification. A wrapper Genetic Algorithm (GA) is introduced into the system to make feature selection and classification. Linear Discriminant Analysis (LDA) is wrapped inside GA to select only discriminating and non-noisy features to be subset database for final classification. k-Nearest Neighbour (k-NN) classifier is also used for comparison. The result of experiments shows that the combined features of BGLAM and SPPD give good classification results. Wrapper based GA can select good features and increases system's final classification accuracy. © 2011 ISSN 2185-2766.},
	author_keywords = {Feature selection; GA; kNN; LDA; Wood recognition},
	keywords = {Algorithms; Classification; Discriminant Analysis; Optimization; Selection; Wood; Discriminant analysis; Genetic algorithms; Optimization; Wood; Feature selection; GA; kNN; LDA; Wood recognition; Feature extraction},
	correspondence_address = {R. Yusof; Centre for Artificial Intelligence and Robotics (CAIRO), Universiti Teknologi Malaysia, Jalan Semarak, 54100, Kuala Lumpur, Malaysia; email: rubiyah@ic.utm.my},
	issn = {21852766},
	language = {English},
	abbrev_source_title = {ICIC Express Lett Part B Appl.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21}
}

@ARTICLE{Kadir201282,
	author = {Kadir, Abdul and Nugroho, Lukito Edi and Susanto, Adhi and Insap Santosa, P.},
	title = {Experiments of zernike moments for leaf identification},
	year = {2012},
	journal = {Journal of Theoretical and Applied Information Technology},
	volume = {41},
	number = {1},
	pages = {82 – 93},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864201341&partnerID=40&md5=2ab67d459486454cdf083b5226a334b0},
	affiliations = {Department of Electrical Engineering, Gadjah Mada University, Indonesia},
	abstract = {So far, plant identification has challenges for several researchers. Various methods and features have been proposed. However, there are still many approaches could be investigated to develop robust plant identification systems. This paper reports several experiments in using Zernike moments to build foliage plant identification systems. In this case, Zernike moments were combined with other features: geometric features, color moments and gray-level co-occurrence matrix (GLCM). To implement the identifications systems, two approaches has been investigated. First approach used a distance measure and the second used Probabilistic Neural Networks (PNN). The results show that Zernike Moments have a prospect as features in leaf identification systems when they are combined with other features. © 2005 - 2012 JATIT & LLS.},
	author_keywords = {City block; GLCM; Leaf identification system; PNN; Zernike moments},
	keywords = {Experiments; Neural networks; City block; GLCM; Leaf identification; PNN; Zernike moments; Feature extraction},
	correspondence_address = {A. Kadir; Department of Electrical Engineering, Gadjah Mada University, Indonesia; email: akadir@mti.ugm.ac.id},
	publisher = {Asian Research Publishing Network (ARPN)},
	issn = {19928645},
	language = {English},
	abbrev_source_title = {J. Theor. Appl. Inf. Technol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 48}
}

@CONFERENCE{Kluge2009321,
	author = {Kluge, A. and Nordmeyer, H.},
	title = {Automated weed detection in winter wheat by using artificial neural networks},
	year = {2009},
	journal = {Precision Agriculture 2009 - Papers Presented at the 7th European Conference on Precision Agriculture, ECPA 2009},
	pages = {321 – 327},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874741374&partnerID=40&md5=e9f9ab4a1d4dff05313312669de5adac},
	affiliations = {Julius Kühn-Institut, Federal Research Centre for Cultivated Plants, Institute for Plant Protection in Field Crops and Grassland, 38100 Braunschweig, Messeweg 11-12, Germany},
	abstract = {A weed detection system was developed that consisted of artificial neural networks. A CCD camera acquired digital colour images of weeds. To recognize weeds more effectively than by discriminant analysis, an artificial neural network was used. In this paper new geometrical features were applied, these were the angle between a point on the leaf axis and a point at the edge of the leaf. This feature is nearly independent from the dimension of leaves. Several angles measured in this way were evaluated by an artificial neural network. A classification between two selected weeds (Galium aparine and Veronica hederifolia) was made. The recognition rate was 90.0% for Veronica hederifolia and 93.4% for Galium aparine. The results show that the neural network model can distinguish weed species at the cotyledon growth stage. The weed detection system with artificial neural networks could improve weed detection for site specific weed control.},
	author_keywords = {Herbicide reduction; Leaf recognition; Machine vision; Precision farming; Weeds},
	keywords = {Agriculture; Computer vision; Discriminant analysis; Neural networks; Cotyledon growth; Geometrical features; Herbicide reductions; Leaf recognition; Neural network model; Precision farming; Weed detection; Weeds; Weed control},
	isbn = {978-908686113-2},
	language = {English},
	abbrev_source_title = {Precis. Agric. - Pap. Presented Eur. Conf. Precis. Agric., ECPA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 7th European Conference on Precision Agriculture, ECPA 2009; Conference date: 6 July 2009 through 8 July 2009; Conference code: 102120}
}

@ARTICLE{Jin2009591,
	author = {Jin, Jian and Tang, Lie},
	title = {Corn plant sensing using real-time stereo vision},
	year = {2009},
	journal = {Journal of Field Robotics},
	volume = {26},
	number = {6-7},
	pages = {591 – 608},
	doi = {10.1002/rob.20293},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-68049093816&doi=10.1002%2frob.20293&partnerID=40&md5=7945be1271ab6e24377ae2347c3c07b8},
	affiliations = {Department of Agricultural and Biosystems Engineering, Iowa State University, Ames, IA 50011, United States},
	abstract = {Though some two-dimensional (2D) machine vision-based systems for early-growth-stage corn plant sensing exist, some of their shortcomings are difficult to overcome. The greatest challenge comes from separating individual corn plants with overlapped plant canopies. With 2D machine vision, variation in outdoor lighting conditions and weeds in the background also pose difficulties in corn plant identification. Adding the depth dimension has the potential to improve the performance of such a sensing system. A new corn plant sensing system using a real-time stereo vision system was investigated in this research. Top-view depth images of corn plant canopy were acquired. By processing the depth images, the algorithm effectively updated the plant skeleton structures and finally recognized individual corn plants and detected their center positions. The stereo vision system was tested over corn plants of V2-V3 growth stages in both laboratory and field conditions. Experimental results showed that the stereo vision system was capable of detecting both separated and overlapped corn plants. During the field test, 96.7% of the corn plants were correctly detected, and plant center positions were estimated with maximum distance errors of 5 and 1 cm for 74.6% and 62.3% of detections, respectively. © 2009 Wiley Periodicals, Inc.},
	keywords = {Computer vision; Sensors; Signal filtering and prediction; Corn plant; Depth image; Field conditions; Field test; Growth stages; Lighting conditions; Machine vision; Maximum distance; Plant canopies; Real time stereo; Sensing systems; Skeleton structure; Stereo vision system; Stereo vision},
	correspondence_address = {J. Jin; Department of Agricultural and Biosystems Engineering, Iowa State University, Ames, IA 50011, United States; email: jinjian@iastate.edu},
	issn = {15564967},
	language = {English},
	abbrev_source_title = {J. Field. Rob.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 69}
}

@ARTICLE{Guerrero201211149,
	author = {Guerrero, J.M. and Pajares, G. and Montalvo, M. and Romeo, J. and Guijarro, M.},
	title = {Support Vector Machines for crop/weeds identification in maize fields},
	year = {2012},
	journal = {Expert Systems with Applications},
	volume = {39},
	number = {12},
	pages = {11149 – 11155},
	doi = {10.1016/j.eswa.2012.03.040},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861191044&doi=10.1016%2fj.eswa.2012.03.040&partnerID=40&md5=e70bba9433ecf8d6144004e3c2820d8c},
	affiliations = {Dpto. Ingeniería Del Software e Inteligencia Artificial, Facultad Informática, Universidad Complutense, Madrid 28040, Spain; Dpt. Arquitectura Computadores y Automática, Facultad Informática, Universidad Complutense, Madrid 28040, Spain},
	abstract = {In Precision Agriculture (PA) automatic image segmentation for plant identification is an important issue to be addressed. Emerging technologies in optical imaging sensors play an important role in PA. In maize fields, site-specific treatments, with chemical products or mechanical manipulations, are applied for weeds elimination. Maize is an irrigated crop, also unprotected from rainfall. After a strong rain, soil materials (particularly clays) mixed with water impregnate the vegetative cover. The green spectral component associated to the plants is masked by the dominant red spectral component coming from soil materials. This makes methods based on the greenness identification fail under such situations. We propose a new method based on Support Vector Machines for identifying plants with green spectral components masked and unmasked. The method is also valid for post-treatment evaluation, where loss of greenness in weeds is identified with the effectiveness of the treatment and in crops with damage or masking. The performance of the method allows to verify its viability for automatic tasks in agriculture based on image processing. © 2012 Elsevier Ltd. All rights reserved.},
	author_keywords = {Image segmentation; Precision Agriculture; Support Vector Machines; Weeds/crop discrimination},
	keywords = {Crops; Image segmentation; Rain; Automatic image segmentation; Chemical products; Emerging technologies; Irrigated crops; Mechanical manipulation; Optical imaging sensors; Plant identification; Post treatment; Precision Agriculture; Site-specific; Soil materials; Spectral components; Vegetative cover; Weeds/crop discrimination; Support vector machines},
	correspondence_address = {J.M. Guerrero; Dpto. Ingeniería Del Software e Inteligencia Artificial, Facultad Informática, Universidad Complutense, Madrid 28040, Spain; email: jmguerre@fdi.ucm.es},
	issn = {09574174},
	coden = {ESAPE},
	language = {English},
	abbrev_source_title = {Expert Sys Appl},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 191}
}

@CONFERENCE{Ahamed20111981,
	author = {Ahamed, Tofael and Kulmutiwat, Supachai and Thanpattranon, Pawin and Tuntiwut, Sasiwimon and Ryozo, Noguchi and Takigawa, Tomohiro},
	title = {Monitoring of plant growth using laser range finder},
	year = {2011},
	journal = {American Society of Agricultural and Biological Engineers Annual International Meeting 2011, ASABE 2011},
	volume = {3},
	pages = {1981 – 1992},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-81255149945&partnerID=40&md5=6f950d1e48e246357836a027a4a66df4},
	affiliations = {Graduate School of Life and Environmental Sciences, University of Tsukuba, Tsukuba, 305-8572, Ibaraki, 1-1-1 Tennodai, Japan},
	abstract = {Monitoring of plant growth rate over the growing season is important for site-specific management. Researchers have addressed plant identification and growth using machine vision though the dynamic range and illumination is a challenging to validate image processing and potential uses of CCD sensor. To overcome the illumination and dynamic range problems of CCD sensor, we made an attempt to implement Laser Range Finder (LRF) for tracking crop growth and artificial landmark to locate the autonomous tractor while navigation inside the field. Therefore, the objectives of this research are to monitor plant growth based on the reflection height from plants and develop crop growth map using scanned data received from LRF. A URG-04LX LRF has been selected for experiments and installed at the back of the autonomous tractor. The calibration of laser sensor has conducted for successful landmarks detection. A full size autonomous tractor has been used in this experiment with Programmable Logic Control (PLC) unit and hydraulic actuators for gear shifting and steering mechanism. The navigation system has been developed based on relative positioning using artificial landmarks. The 2D laser cloud scan points of reflection height from plants are considered as a reference to develop surface map of plant growth. A LMS 211 laser range finder was installed to discriminate artificial landmark and navigated the autonomous tractor inside the field. Further experiments and correlation will be established to assess the yield and remote monitoring of plant growth using wireless networking over the growing season.},
	author_keywords = {Artificial landmark; Autonomous tractor; Navigation; Plant growth monitoring; Reflection height},
	keywords = {Agricultural machinery; Computer vision; Crops; Engineers; Experiments; Hydraulic machinery; Navigation; Navigation systems; Radar equipment; Sensors; Tractors (agricultural); Two dimensional; Artificial landmark; Autonomous tractors; CCD sensors; Crop growth; Dynamic range; Dynamic range problem; Gear shifting; Growing season; Hydraulic actuator; Landmarks detection; Laser range finders; Laser sensor; Plant growth; Plant identification; Programmable logic control; Relative positioning; Remote monitoring; Site specific management; Steering mechanisms; Surface map; Wireless networking; Tractors (truck)},
	publisher = {American Society of Agricultural and Biological Engineers},
	isbn = {978-161839156-8},
	language = {English},
	abbrev_source_title = {Am. Soc. Agric. Biol. Eng. Annu. Int. Meet.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: American Society of Agricultural and Biological Engineers Annual International Meeting 2011; Conference date: 7 August 2011 through 10 August 2011; Conference code: 87301}
}

@CONFERENCE{Solé-Casals2009462,
	author = {Solé-Casals, Jordi and Travieso, Carlos M. and Ferrer, Miguel A. and Alonso, Jesús B. and Briceflo, Juan Carlos},
	title = {Automatic recognition of leaves by shape detection pre-processing with ica},
	year = {2009},
	journal = {BIOSIGNALS 2009 - Proceedings of the 2nd International Conference on Bio-Inspired Systems and Signal Processing},
	pages = {462 – 467},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650516886&partnerID=40&md5=a838c89bc2df84dccaec0fa090ad4ea2},
	affiliations = {Signal Processign Group, University of Vic, E-08500, Vic, Sagrada Familia 7, Spain; Department of Signals and Communications, Technological Centre for Innovation on Communication (CeTIC), University of Las Palmas de Gran Canaria, TaRra s/n, E-35017, Las Palmas de Gran Canaria, Campus de Universitario, Spain; Computer Science Department, University of Costa Rica, Sede Rodrigo Facio Brenes, San Jose, Montes de Oca, Post-Code 2060, Costa Rica},
	abstract = {In this work we present a simulation of a recognition process with perimeter characterization of a simple plant leaves as a unique discriminating parameter. Data coding allowing for independence of leaves size and orientation may penalize performance recognition for some varieties. Border description sequences are then used to characterize the leaves. Independent Component Analysis (ICA) is then applied in order to study which is the best number of components to be considered for the classification task, implemented by means of an Artificial Neural Network (ANN). Obtained results with ICA as a pre-processing tool are satisfactory, and compared with some references our system improves the recognition success up to 80.8% depending on the number of considered independent components.},
	author_keywords = {Artificial Neural Networks; Independent Component Analysis; Leaves Recognition; Parameterization; Pattern Recognition},
	keywords = {Backpropagation; Hemodynamics; Multivariant analysis; Neural networks; Pattern recognition; Signal processing; Artificial Neural Network; Artificial Neural Networks; Automatic recognition; Classification tasks; Data coding; Independent components; Leaves Recognition; Number of components; Plant leaves; Pre-processing; Preprocessing tools; Recognition process; Shape detection; Independent component analysis},
	correspondence_address = {J. Solé-Casals; Signal Processign Group, University of Vic, E-08500, Vic, Sagrada Familia 7, Spain; email: jordi.sole@uvic.cat},
	isbn = {978-989811165-4},
	language = {English},
	abbrev_source_title = {BIOSIGNALS - Proc. Int. Conf. Bio-Inspired Syst. Signal Process.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2nd International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2009; Conference date: 14 January 2009 through 17 January 2009; Conference code: 76662}
}

@ARTICLE{Suzuki200967,
	author = {Suzuki, Y. and Okamoto, H. and Kataoka, T.},
	title = {Development of discriminant model for weed detection using hyperspectral imaging},
	year = {2009},
	journal = {Acta Horticulturae},
	volume = {824},
	pages = {67 – 74},
	doi = {10.17660/ActaHortic.2009.824.7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350091249&doi=10.17660%2fActaHortic.2009.824.7&partnerID=40&md5=f8ed7ffb5294a50c997047fc7810c82e},
	affiliations = {Graduate School of Agriculture, Hokkaido University, Sapporo, Japan; Research Faculty of Agriculture, Hokkaido University, Sapporo, Japan},
	abstract = {Physical weed control is environmentally safer than chemical methods. However, most of the physical methods are manual. Therefore, development of a robotic weeding system is necessary to practice efficient weed control. The objective of this study was to develop a pixel discriminant model for image segmentation between crop and weed using hyperspectral imaging. A hyperspectral image was processed pixel by pixel. First, every pixel spectrum was extracted from the image. Next, each pixel spectrum was classified into soil and plant. If the pixel spectrum was identified as plant, it was classified into crop and weed. In the pixel discriminant model for soil and plant, simple NDVI thresholding was employed. The pixel discriminant model for crop and weed consisted of a normalizer, an explanatory variable generator and the discriminator. Development of the explanatory variable generator was tried with two different methods (RAW or PCA). Development of discriminator was also tried with two different methods (LDA or NN). In this study, four types of models (RAW-LDA, RAW-NN, PCA-LDA and PCA-NN) were developed for discrimination between crop and weed and validated. Finally, segmented images for crop, weed, and soil were generated from the images by applying these models. Pixel discrimination between soil and plant was performed with high accuracy. In the pixel discrimination between crop and weed, success rates of all discriminant models were more than 85%. As for the accuracy of the models, NN models were superior to LDA, and the PCA method was superior to RAW. However, with respect to processing speed, the RAW method was superior to PCA. As a result of image segmentation, most of pixel spectra in the images were identified correctly. In conclusion, this study showed the possibility for weed detection by using hyperspectral imaging.},
	author_keywords = {Image processing; Machine vision; Plant classification; Remote sensing; Soybean},
	keywords = {Glycine max},
	publisher = {International Society for Horticultural Science},
	issn = {05677572},
	language = {English},
	abbrev_source_title = {Acta Hortic.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{de Sa Jr.2011467,
	author = {de Sa Jr., Jarbas Joaci M. and Backes, André R. and Rossatto, Davi Rodrigo and Kolb, Rosana M. and Bruno, Odemir M.},
	title = {Measuring and analyzing color and texture information in anatomical leaf cross sections: An approach using computer vision to aid plant species identification},
	year = {2011},
	journal = {Botany},
	volume = {89},
	number = {7},
	pages = {467 – 479},
	doi = {10.1139/b11-038},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051556556&doi=10.1139%2fb11-038&partnerID=40&md5=dfc086922f8310ca9ad36caf360fad2d},
	affiliations = {Departamento de Ciências Matemáticas e de Computação, 13560-970, São Carlos, SP, Avenida Trabalhador São Carlense, n 400, Brazil; Faculdade de Computação, Universidade Federal de Uberlândia, 38408-100, Uberlândia, MG, Campus Santa Mônica, Av. Joao Naves de Avila, n 2121, Brazil; Departamento de Ciências Biológicas, Faculdade de Ciências e Letras, Universidade Estadual Paulista, UNESP, 19806-900, Assis, SP, Av. Dom Antônio, 2100, Brazil; Instituto de Física de São Carlos, 13560-970, São Carlos, SP, Avenida Trabalhador São Carlense, n 400, Brazil},
	abstract = {Currently, studies on leaf anatomy have provided an important source of characters helping taxonomic, systematic, and phylogenetic studies. These studies strongly rely on measurements of characters (such as tissue thickness) and qualitative information (structures description, presence-absence of structures). In this work, we provide a new computational approach that semiautomates the collection of some quantitative data (cuticle, adaxial epidermis, and total leaf thickness) and accesses a new source of information in leaf cross-section images: the texture and the color of leaf tissues. Our aim was to evaluate this information for plant identification purposes. We successfully tested our system identifying eight species from different phylogenetic positions in the angiosperm phylogeny from the neotropical savanna of central Brazil. The proposed system checks the potential of identifying the species for each extracted measure using the Jeffrey-Matusita distance and composes a feature vector with the most important metrics. A linear discriminant analysis with leave-one-out to classify the samples was used. The experiments achieved a 100% success rate in terms of identifying the studied species accessing the above-described parameters, demonstrating that our computational approach can be a helpful tool for anatomical studies, especially ones devoted to plant identification and systematic studies.},
	author_keywords = {Feature extraction; Jeffrey-matusita distance; Linear discriminant analysis; Plant identification; Taxonomy},
	keywords = {Brazil; Magnoliophyta; anatomy; color; computer simulation; computer vision; cuticle; discriminant analysis; identification method; leaf; Neotropic Ecozone; phylogenetics; phylogeny; quantitative analysis; savanna; taxonomy; texture},
	correspondence_address = {D. R. Rossatto; Departamento de Ciências Matemáticas e de Computação, 13560-970, São Carlos, SP, Avenida Trabalhador São Carlense, n 400, Brazil; email: drrossatto@gmail.com},
	issn = {19162804},
	language = {English},
	abbrev_source_title = {Bot.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16}
}

@ARTICLE{Liu2010214,
	author = {Liu, Xien and Li, Mengjun and Sun, Yuanlun and Deng, Xiaoyan},
	title = {Support vector data description for weed/corn image recognition},
	year = {2010},
	journal = {Journal of Food, Agriculture and Environment},
	volume = {8},
	number = {1},
	pages = {214 – 219},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649109586&partnerID=40&md5=f4e8ba34a25052790b979f3b9dd622d8},
	affiliations = {College of Basic Sciences, Huazhong Agricultural University, Wuhan, 430070, China},
	abstract = {This paper focused on support vector data description (SVDD) technique for the weed/corn recognition based on imbalanced weed/corn image samples. The original images were taken from corn fields under natural lighting and various field backgrounds, and converted into gray level images by a vegetation index: Excess Green minus Excess Red (ExG-ExR). The 2-level wavelet transform was employed to decompose grey image into the approximate component and the detail components. The features of weed/corn images, including the morphological features and the wavelet-based energy features, were extracted. The morphological features were obtained from binary weed/corn images. The various combinations of these features were used as input vector to construct support vector data description models. The experiment results showed that the SVDD model could provide better recognition effect than support vector machine (SVM) and Fisher linear discrimination algorithm (FLDA) based upon the same imbalanced training data set. The two "best" SVDD models were obtained, which achieved a weed/corn recognition rate of 95.59%.},
	author_keywords = {Feature extraction; Support vector data description; Wavelet transform; Weed/crop recognition},
	keywords = {Zea mays; article; corn; image quality; plant identification; plant morphology; support vector machine; weed},
	publisher = {WFL Publisher Ltd.},
	issn = {14590255},
	language = {English},
	abbrev_source_title = {J. Food Agric. Environ.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Zhi2012503,
	author = {Zhi, Zhi-De and Hu, Rong-Xiang and Wang, Xiao-Feng},
	title = {A new weighted ARC-SC approach for leaf image recognition},
	year = {2012},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {7390 LNAI},
	pages = {503 – 509},
	doi = {10.1007/978-3-642-31576-3_64},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865010552&doi=10.1007%2f978-3-642-31576-3_64&partnerID=40&md5=c53bc4e14956323d6c0fcc7021f524bf},
	affiliations = {Department of Automation, University of Science and Technology of China, Hefei, Anhui 230027, China; Intelligent Computing Lab., Hefei Institute of Intelligent Machines, Chinese Academy of Sciences, Anhui 230031, China; Key Lab. of Network and Intelligent Information Processing, Hefei University, Hefei, 230601, China},
	abstract = {In this paper, we present a novel feature extraction approach for plant leaf image recognition, which applies the arc length information to replace the Euclidean distance in traditional Shape Context (SC) method. Meanwhile, the shape is divided by the arc length into two parts, i.e. local and global feature. It can obtain the weighed cost of shape matching by combining the local with global feature. We compare this algorithm with the classic Inner-Distance Shape Context (IDSC) method on both Swedish and ICL leaf image dataset. Experimental results show that the proposed method achieves better performance compared with SC and IDSC methods. © 2012 Springer-Verlag.},
	author_keywords = {arc length; dynamic programming; Leaf recognition; shape context},
	keywords = {Dynamic programming; Feature extraction; Intelligent computing; Arc length; Euclidean distance; Global feature; Leaf images; Leaf recognition; Shape contexts; Shape matching; Image recognition},
	correspondence_address = {X.-F. Wang; Intelligent Computing Lab., Hefei Institute of Intelligent Machines, Chinese Academy of Sciences, Anhui 230031, China; email: xfwang@iim.ac.cn},
	issn = {16113349},
	isbn = {978-364231575-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 8th International Conference on Intelligent Computing Theories and Applications, ICIC 2012; Conference date: 25 July 2012 through 29 July 2012; Conference code: 91948}
}

@ARTICLE{Abadi2010173,
	author = {Abadi, Mohamed and Capelle-Laizé, Anne-Sophie and Khoudeir, Majdi and Combes, Didier and Carré, Serge},
	title = {Grassland species characterization for plant family discrimination by image processing},
	year = {2010},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {6134 LNCS},
	pages = {173 – 181},
	doi = {10.1007/978-3-642-13681-8_21},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956271527&doi=10.1007%2f978-3-642-13681-8_21&partnerID=40&md5=ed45550a31dc183c727e734eef1d29db},
	affiliations = {XLIM-SIC, Université de Poitiers, 86962 Futuroscope-Chasseneuil, BP 30179, France; INRA Poitou-Charentes, UR4 P3F, 86600 Lusignan, BP 6, France},
	abstract = {Pasture species belonging to poaceae and fabaceae families constitute of essential elements to maintain natural and cultivated regions. Their balance and productivity are key factors for good functioning of the grassland ecosystems. The study is based on a process of image processing. First of all an individual signature is defined while considering geometric characteristics of each family. Then, this signature is used to discriminate between these families. Our approach focuses on the use of shape features in different situations. Specifically, the approach is based on cutting the representative leaves of each plant family. After cutting, we obtain leaves sections of different sizes and random geometry. Then, the shape features are calculated. Principal component analysis is used to select the most discriminatory features. The results will be used to optimize the acquisition conditions. We have a discrimination rate of more than 90% for the experiments carried out in a controlled environment. Experiments are being carried out to extend this study in natural environments. © Springer-Verlag Berlin Heidelberg 2010.},
	author_keywords = {Image processing; Leaf recognition; Pasture; Plant classification; Poaceae and fabaceae family; Shape features},
	keywords = {Agriculture; Ecosystems; Experiments; Image processing; Imaging systems; Principal component analysis; Leaf recognition; Pasture; Plant classification; Poaceae; Shape features; Feature extraction},
	correspondence_address = {M. Abadi; XLIM-SIC, Université de Poitiers, 86962 Futuroscope-Chasseneuil, BP 30179, France; email: abadi@sic.sp2mi.univ-poitiers.fr},
	issn = {16113349},
	isbn = {364213680X; 978-364213680-1},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 4th International Conference on Image and Signal Processing, ICISP 2010; Conference date: 30 June 2010 through 2 July 2010; Conference code: 84903; All Open Access, Bronze Open Access}
}

@ARTICLE{Du2009627,
	author = {Du, Minggang and Zhang, Shanwen and Wang, Hong},
	title = {Supervised isomap for plant leaf image classification},
	year = {2009},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {5755 LNAI},
	pages = {627 – 634},
	doi = {10.1007/978-3-642-04020-7_67},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350423346&doi=10.1007%2f978-3-642-04020-7_67&partnerID=40&md5=8690ce98b3209ee25492b04c415b899a},
	affiliations = {Shanxi Normal University, Linfen 041004, China; Faculty of Science, Zhongyuan University of Technology, Zhengzhou 450007, China},
	abstract = {Plant classification is very important and necessary with respect to agricultural informization, ecological protection and plant automatic classification system. Compared with other methods, such as cell and molecule biology methods, classification based on leaf image is the first choice for plant classification. Plant recognition and classification is a complex and difficult problem, and is very important in Computer-Aided Plant Species Identification technology. The feature extraction is a key step to plant classification. This paper presents a method to extract discriminant features for plant leaf images by using supervised Isomap. Experiments on the leaf image dataset have been performed. Experimental results show that the supervised Isomap is very effective and feasible. © 2009 Springer Berlin Heidelberg.},
	author_keywords = {Isomap; K-nearest neighbor; Plant classification; Plant leaf image; Supervised Isomap},
	keywords = {Automatic indexing; Biology; Cell membranes; Computer science; Feature extraction; Image analysis; Image classification; Isomap; K-nearest neighbor; Plant classification; Plant leaf image; Supervised Isomap; Intelligent computing},
	issn = {16113349},
	isbn = {3642040195; 978-364204019-1},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: 5th International Conference on Intelligent Computing, ICIC 2009; Conference date: 16 September 2009 through 19 September 2009; Conference code: 77839}
}

@CONFERENCE{Casanova2011,
	author = {Casanova, Dalcimar and Florindo, João Batista and Bruno, Odemir Martinez},
	title = {IFSC/USP at Image CLEF 2011: Plant identification task},
	year = {2011},
	journal = {CEUR Workshop Proceedings},
	volume = {1177},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922041474&partnerID=40&md5=656f538f88e78de64fb02d03e343e1fe},
	affiliations = {IFSC-Instituto de FÍsica de São Carlos, USP-Universidade de São Paulo, São Carlos, Brazil},
	abstract = {The leaves are one of the most important main sources used for plant identification. Because of this the Image CLEF 2011 proposed a challenge based on leaf analysis for plant identification. This paper reports the experiment results of the IFSC/USP team in participating of this task. The main goal is investigate the performance of Complex Network method for feature extraction and classification of plant species. The achieved results are promising and can help the botanists in the future.},
	author_keywords = {Complex network; FDA; Leaves; Plant identification; Taxonomy},
	keywords = {Complex networks; Feature extraction; Plants (botany); Taxonomies; FDA; Feature extraction and classification; Leaves; Plant identification; Plant species; Image processing},
	editor = {Petras V. and Forner P. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Clough P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2011 Cross Language Evaluation Forum Conference, CLEF 2011; Conference date: 19 September 2011 through 22 September 2011; Conference code: 110352}
}

@CONFERENCE{Zhang20104234,
	author = {Zhang, Y. and Slaughter, D.C.},
	title = {Development of a robust weed species mapping system using hyperspectral imaging for precision weed control in processing tomato},
	year = {2010},
	journal = {American Society of Agricultural and Biological Engineers Annual International Meeting 2010, ASABE 2010},
	volume = {5},
	pages = {4234 – 4249},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649712984&partnerID=40&md5=8a3cde78946d020490a4b358be68c255},
	affiliations = {Biological and Agricultural Engineering, University of California, Davis, CA, United States},
	abstract = {Californian processing tomatoes are planted over a 4-month period, during which there is a substantial change in the growing environment. Most of the prior research using visible and near infrared spectroscopy for plant identification have been limited in scope to plants grown under a single environmental condition. This work studied the impacts of variations in diurnal growing temperature range on hyperspectral features in the visible and near infrared region with a specific focus on species identification and high-resolution mapping. Six major Californian processing tomato cultivars, black nightshade (Solanum nigrum L.) and red-root pigweed (Amaranthus retroflexus L.) were grown under a variety of diurnal temperature ranges simulating the range common in the Californian springtime planting period and one treatment simulating greenhouse growing conditions. The principal change in canopy reflectance with changing temperature occurred in the 480-670 nm and 695-795 nm regions. A species-temperature interaction effect between tomato and black nightshade was observed in the performance of a species classifier based upon the full 400-795 nm range. A bilaterally symmetric adverse effect on the classification rates of these two species occurred when the classification models were applied to plants grown under a diurnal temperature range different from the conditions under which the training plants were grown. The overall species classification rate was higher for models (91%) trained under mild conditions compared to models (83%) trained under higher diurnal temperature range conditions and were nearly as high as the classification rate (92%) achieved by models calibrated at global temperature conditions.},
	author_keywords = {Diurnal temperature; Hyperspectral imaging; Machine learning; Plant identification; Weed mapping},
	keywords = {Agricultural machinery; Cultivation; Engineers; Fruits; Infrared devices; Learning systems; Mapping; Near infrared spectroscopy; Plants (botany); Diurnal temperatures; Hyperspectral Imaging; Machine-learning; Plant identification; Weed mapping; Weed control},
	correspondence_address = {Y. Zhang; Biological and Agricultural Engineering, University of California, Davis, CA, United States; email: yunzhang@ucdavis.edu},
	publisher = {American Society of Agricultural and Biological Engineers},
	isbn = {978-161738835-4},
	language = {English},
	abbrev_source_title = {Am. Soc. Agric. Biol. Eng. Annu. Int. Meet.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hong2012141,
	author = {Hong, Soon-Won and Choi, Lynn},
	title = {Automatic recognition of flowers through color and edge based contour detection},
	year = {2012},
	journal = {2012 3rd International Conference on Image Processing Theory, Tools and Applications, IPTA 2012},
	pages = {141 – 146},
	doi = {10.1109/IPTA.2012.6469535},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875870433&doi=10.1109%2fIPTA.2012.6469535&partnerID=40&md5=f0d6d37f284ad2daa440d8a23abbe710},
	affiliations = {School of Electrical Engineering, Korea University, South Korea},
	abstract = {Unlike simple images processed by the existing image-based search engines, flowers have wider and more irregular range of shapes and patterns. In this paper we present an automatic recognition system of flowers for smartphone users. After a user transmits a flower image to the server, the image processing and searching is performed only by the server, eliminating the user interaction from the recognition process. The server detects the contour of a flower image by using both color-based and edge-based contour detection. Then, we classify its color groups and contour shapes by using k-means clustering and history matching. After comparing the input image with the reference images stored on the server, the server sends the most similar image to the user. We also address the image recognition failure issue caused by the light and the camera angle by partial recognition and image recovery. We have obtained the success rate of 94.8 % for 500 images from 100 species. © 2012 IEEE.},
	author_keywords = {contour detection; flower recognition; histogram matching; search engines; smartphones},
	keywords = {Image processing; Image recognition; Search engines; Smartphones; Automatic recognition; Automatic recognition system; Contour detection; Flower recognition; Histogram matching; History matching; K-means clustering; Recognition process; Color},
	correspondence_address = {S.-W. Hong; School of Electrical Engineering, Korea University, South Korea; email: aldig@korea.ac.kr},
	isbn = {978-146732583-7},
	language = {English},
	abbrev_source_title = {Int. Conf. Image Process. Theory, Tools Appl., IPTA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; Conference name: 2012 3rd International Conference on Image Processing Theory, Tools and Applications, IPTA 2012; Conference date: 15 October 2012 through 18 October 2012; Conference code: 96361}
}

@CONFERENCE{Premaratne201020,
	author = {Premaratne, Upeka},
	title = {Application of rectangular features for the localization of fertile material in plant images},
	year = {2010},
	journal = {Proceedings of the 2010 5th International Conference on Information and Automation for Sustainability, ICIAfS 2010},
	pages = {20 – 25},
	doi = {10.1109/ICIAFS.2010.5715628},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952753012&doi=10.1109%2fICIAFS.2010.5715628&partnerID=40&md5=8398030c49c398d74d9a3009965f57d6},
	affiliations = {Department of Electronic and Telecommunication Engineering, University of Moratuwa, Katubedda Moratuwa, Sri Lanka},
	abstract = {Analysis of fertile material such as flowers and fruit is a key factor in the proper identification of plant species. Despite object recognition being a mature research area, the use of it in automated plant identification is still relatively new. This paper describes a novel method of detecting fertile material in plant images using rectangular features. Rectangular features are obtained for the grayscale image, the J value image, the magnitude and angle of the gradient of the image. From these, the features with the best performance are selected based on their ability to detect fertile material (flowers) and non-fertile material (leaves). Based on performance, the rectangular features of the grayscale image and J value image are used. Multiple Support Vector Machine (SVM) classifiers with different kernels are tested and the final result is obtained using classifier voting based on the confidence of each classifier. After applying the classifier to the entire image, regions of interest of fertile material are isolated and post processed. © 2010 IEEE.},
	author_keywords = {Flowers; J value; Localization; Object recognition; Plant fertile material; Rectangle features},
	keywords = {Automation; Materials; Object recognition; Sustainable development; Flowers; J value; Localization; Plant fertile material; Rectangle features; Feature extraction},
	correspondence_address = {U. Premaratne; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Katubedda Moratuwa, Sri Lanka; email: upeka@ent.mrt.ac.lk},
	isbn = {978-142448551-2},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Inf. Autom. Sustainability, ICIAfS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2010 5th International Conference on Information and Automation for Sustainability, ICIAfS 2010; Conference date: 17 December 2010 through 19 December 2010; Conference code: 84198}
}

@ARTICLE{Zhang201195,
	author = {Zhang, Y. and Slaughter, D.C.},
	title = {Hyperspectral species mapping for automatic weed control in tomato under thermal environmental stress},
	year = {2011},
	journal = {Computers and Electronics in Agriculture},
	volume = {77},
	number = {1},
	pages = {95 – 104},
	doi = {10.1016/j.compag.2011.04.001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957676935&doi=10.1016%2fj.compag.2011.04.001&partnerID=40&md5=f39c5a0fe80c1be917ff3a83f9942ffc},
	affiliations = {Department of Biological and Agricultural Engineering, University of California Davis, Davis, CA 95616, One Shields Avenue, United States},
	abstract = {This work studied the impacts of variations in environmental temperature on hyperspectral imaging features in the visible and near infrared regions for robust species identification for weed mapping in tomato production. Six major Californian processing tomato cultivars, black nightshade (Solanum nigrum L.) and redroot pigweed (Amaranthus retroflexus L.) were grown under a variety of diurnal temperature ranges simulating conditions common in the Californian springtime planting period and one additional treatment simulating greenhouse growing conditions. The principal change in canopy reflectance with varying temperature occurred in the 480-670 and 720-810. nm regions. The overall classification rate ranged from 62.5% to 91.6% when classifiers trained under single temperatures were applied to plants grown at different temperatures. Eliminating the 480-670. nm region from the classifier's feature set mitigated the temperature effect by stabilizing the total crop vs. weed classification rate at 86.4% over the temperature ranges. A site-specific recalibration method was also successful in alleviating the bias created by calibrating the models on the extreme temperatures and increased the classification accuracy to 90.3%. A global calibration method, incorporating all four temperature conditions in the classifier feature space, provided the best average total classification accuracy of 92.2% out of the methods studied, and was fairly robust to the varying diurnal temperature conditions. © 2011 Elsevier B.V.},
	author_keywords = {Hyperspectral imaging; Machine learning; Plant identification; Temperature; Tomato; Weed control},
	keywords = {Amaranthus; Amaranthus retroflexus; Lycopersicon esculentum; Solanum nigrum; Cultivation; Fruits; Identification (control systems); Mammals; Plants (botany); Additional treatment; Amaranthus; Canopy reflectance; Classification accuracy; Classification rates; Diurnal temperature ranges; Diurnal temperatures; Environmental stress; Environmental temperature; Extreme temperatures; Feature sets; Feature space; Global calibration; Growing conditions; HyperSpectral; Hyperspectral Imaging; Machine-learning; Plant identification; Planting period; Recalibrations; Redroot pigweed; Site-specific; Solanum nigrum; Species identification; Species mapping; Temperature conditions; Temperature effects; Temperature range; Tomato; Tomato production; Varying temperature; Visible and near infrared; calibration; cultivar; dicotyledon; diurnal variation; environmental stress; greenhouse ecosystem; growing season; imaging method; near infrared; temperature effect; weed control; Weed control},
	correspondence_address = {Y. Zhang; Department of Biological and Agricultural Engineering, University of California Davis, Davis, CA 95616, One Shields Avenue, United States; email: yunzhang@ucdavis.edu},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 25}
}

@ARTICLE{Zhang201265,
	author = {Zhang, Yun and Slaughter, David C. and Staab, Erik S.},
	title = {Robust hyperspectral vision-based classification for multi-season weed mapping},
	year = {2012},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	volume = {69},
	pages = {65 – 73},
	doi = {10.1016/j.isprsjprs.2012.02.006},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862782421&doi=10.1016%2fj.isprsjprs.2012.02.006&partnerID=40&md5=24a53cfbf14b8cae7373506f6fdf29bd},
	affiliations = {Department of Biological and Agricultural Engineering, University of California, Davis, CA 95616, One Shields Avenue, United States},
	abstract = {This study investigated the robustness of hyperspectral image-based plant recognition to seasonal variability in a natural farming environment in the context of automated in-row weed control. A machine vision system was developed and equipped with a CCD camera integrated with a line-imaging spectrograph for close-range weed sensing and mapping. Three canonical Bayesian classifiers were developed using canopy reflectance (400-795. nm) collected over three seasons for tomato and weeds. The performance of the three season-specific classifiers was tested by changing environmental conditions, resulting in an increase in total error rate of up to 36%. Global calibration across the complete span of the three seasons produced overall classification accuracies of 85.0%, 90.0% and 92.7%, respectively, for 2005, 2006 and 2008. To improve the stability of global classifier over multiple seasons, a multiclassifier system was constructed with three canonical Bayesian classifiers optimized for the three seasons individually. This system was tested on a data set simulating an upcoming season with field conditions similar to that in 2005. The system increased the total discrimination accuracy to 95.8% for the tested season under simulation. This method provided an innovative direction for achieving robust plant recognition over multiple seasons by integrating expert knowledge from historical data that most closely matched the new field environment. © 2012 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
	author_keywords = {Computer vision; Machine learning; Multiclassifier system; Plant recognition; Seasonal variability; Weed control},
	keywords = {Lycopersicon esculentum; Learning systems; Weed control; Bayesian classifier; Canopy reflectance; Classification accuracy; Data sets; Discrimination accuracy; Environmental conditions; Error rate; Expert knowledge; Field conditions; Global calibration; Historical data; HyperSpectral; Image-based; Machine vision systems; Multiclassifier system; Plant recognition; Seasonal variability; Vision based; accuracy assessment; Bayesian analysis; calibration; canopy reflectance; data set; fruit; optimization; pattern recognition; remote sensing; seasonal variation; vegetation mapping; weed control; Computer vision},
	correspondence_address = {Y. Zhang; Department of Biological and Agricultural Engineering, University of California, Davis, CA 95616, One Shields Avenue, United States; email: yunzhang@ucdavis.edu},
	issn = {09242716},
	coden = {IRSEE},
	language = {English},
	abbrev_source_title = {ISPRS J. Photogramm. Remote Sens.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 40}
}

@ARTICLE{Husin201218,
	author = {Husin, Z. and Shakaff, A.Y.M. and Aziz, A.H.A. and Farook, R.S.M. and Jaafar, M.N. and Hashim, U. and Harun, A.},
	title = {Embedded portable device for herb leaves recognition using image processing techniques and neural network algorithm},
	year = {2012},
	journal = {Computers and Electronics in Agriculture},
	volume = {89},
	pages = {18 – 29},
	doi = {10.1016/j.compag.2012.07.009},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865614146&doi=10.1016%2fj.compag.2012.07.009&partnerID=40&md5=91c5d6ce9e47e6c8d8b0c58a1c3c6598},
	affiliations = {School of Computer and Communication Engineering, Universiti Malaysia Perlis, Malaysia; School of Mechatronic Engineering, Universiti Malaysia Perlis, Malaysia; Agrotechnology Unit, Universiti Malaysia Perlis, Malaysia; Institute of Nano Electronic Engineering, Universiti Malaysia Perlis, Malaysia},
	abstract = {Herbs have been widely used in food preparation, medicine and cosmetic industry. Knowing which herbs to be used would be very critical in these applications. Nevertheless, the current way of identification and determination of the types of herbs is still being done manually and prone to human error. Designing a convenient and automatic recognition system of herbs species is essential since this will improve herb species classification efficiency. This research focus on recognition approach to the shape and texture features of the herbs leaves. It aims to realize the computerized method to classify the herbs plants in a very convenient way. Portable herb leaves recognition system through image and data processing techniques is implemented as automated herb plant classification system. It is very easy to use and inexpensive system designed especially for helping scientist in agricultural field. The proposed system employs neural networks algorithm and image processing techniques to perform recognition on twenty species of herbs. One hundred samples for each species went through the system and the recognition accuracy was at 98.9%. Most importantly the system is capable of identifying the herbs leaves species even though they are dried, wet, torn or deformed. The efficiency and effectiveness of the proposed method in recognizing and classifying the different herbs species is demonstrated by experiments. © 2012 Elsevier B.V.},
	author_keywords = {Embedded portable device; Herbs leaves database; Herbs leaves recognition; Neural network algorithm; Singular Value Decomposition (SVD)},
	keywords = {Algorithms; Data processing; Image processing; Neural networks; Portable equipment; Singular value decomposition; Agricultural fields; Automatic recognition system; Computerized methods; Cosmetic industry; Data processing techniques; Food preparation; Herb species; Herbs leaves recognition; Human errors; Image processing technique; Neural network algorithm; Neural networks algorithms; Plant classification; Portable device; Recognition accuracy; Recognition systems; Texture features; accuracy assessment; agricultural technology; algorithm; artificial neural network; automation; biotechnology; data processing; database; experimental study; herb; image processing; leaf; vegetation classification; Plants (botany)},
	correspondence_address = {Z. Husin; School of Computer and Communication Engineering, Universiti Malaysia Perlis, Malaysia; email: zulhusin@unimap.edu.my},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 41}
}

@CONFERENCE{Kim2009580,
	author = {Kim, Jung-Hyun and Huang, Rong-Guo and Jin, Sang-Hyeon and Hong, Kwang-Seok},
	title = {Mobile-based flower recognition system},
	year = {2009},
	journal = {3rd International Symposium on Intelligent Information Technology Application, IITA 2009},
	volume = {3},
	pages = {580 – 583},
	doi = {10.1109/IITA.2009.407},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77649277961&doi=10.1109%2fIITA.2009.407&partnerID=40&md5=ee94aa242abd7875e9e86de76bc94e79},
	affiliations = {School of Information and Communication Engineering, Sungkyunkwan University, Suwon, 440-746, South Korea},
	abstract = {Conventional flower or leaf recognition studies have some restrictions and limitations. These include a sharp drop in recognition rate due to the varying positions and number of relevant objects in the original object image. Hence, this paper suggests and implements a mobile-based flower recognition system using Difference Image Entropy (DIE) and contour features of the flower from the original image with multi-flower objects. In system framework includes 1) WiBro Net.-based transmission and designation module of the relevant flower object by drawing the flower region of the user's interest, 2) contour feature extraction module, and 3) DIE-based flower recognition module. The system was evaluated using ten species of flowers with each ten samples. Experimental results achieved an optimum average recognition rate of 95% and average response run-time of 9,033ms, for a set of ten images per species. © 2009 IEEE.},
	author_keywords = {Contour feature; Difference image entropy; Flower recognition},
	keywords = {Dies; Entropy; Information technology; Contour features; Difference images; Flower recognition; Leaf recognition; Original images; Recognition rates; Runtimes; System framework; User's interest; Feature extraction},
	correspondence_address = {J.-H. Kim; School of Information and Communication Engineering, Sungkyunkwan University, Suwon, 440-746, South Korea; email: kjh0328@skku.edu},
	isbn = {978-076953859-4},
	language = {English},
	abbrev_source_title = {Int. Symp. Intelligent Inf. Technol. Appl., IITA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; Conference name: 3rd International Symposium on Intelligent Information Technology Application, IITA 2009; Conference date: 21 November 2009 through 22 November 2009; Conference code: 79449}
}

@CONFERENCE{Goëau2012,
	author = {Goëau, Hervé and Bonnet, Pierre and Joly, Alexis and Yahiaoui, Itheri and Barthelemy, Daniel and Boujemaa, Nozha and Molino, Jean-François},
	title = {The ImageCLEF 2012 plant identification task},
	year = {2012},
	journal = {CEUR Workshop Proceedings},
	volume = {1178},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922041926&partnerID=40&md5=2195c9e9095678909648172c9f108ffd},
	affiliations = {INRIA, IMEDIA and ZENITH teams, France; CIRAD, UMR AMAP, France; CIRAD, BIOS Direction and INRA, UMR AMAP, F-34398, France; IRD, UMR AMAP, France; Laboratoire CReSTIC, Université de Reims, France},
	abstract = {The ImageCLEF's plant identification task provides a testbed for the system-oriented evaluation of plant identification, more precisely on the 126 tree species identification based on leaf images. Three types of image content are considered: Scan, Scan-like (leaf photographs with a white uniform background), and Photograph (unconstrained leaf with natural background). The main originality of this data is that it was specifically built through a citizen sciences initiative conducted by Tela Botanica, a French social network of amateur and expert botanists. This makes the task closer to the conditions of a real-world application. This overview presents more precisely the resources and assessments of task, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results. With a total of eleven groups from eight countries and with a total of 30 runs submitted, involving distinct and original methods, this second year pilot task confirms Image Retrieval community interest for biodiversity and botany, and highlights further challenging studies in plant identification.},
	author_keywords = {Benchmark; Classification; Collection; Evaluation; Identification; ImageCLEF; Images; Leaves; Plant},
	keywords = {Benchmarking; Biodiversity; Classification (of information); Identification (control systems); Image retrieval; Photography; Plants (botany); Refuse collection; Evaluation; ImageCLEF; Images; Leaves; Plant; Image processing},
	editor = {Karlgren J. and Womser-Hacker C. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Forner P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 22; Conference name: 2012 Cross Language Evaluation Forum Conference, CLEF 2012; Conference date: 17 September 2012 through 20 September 2012; Conference code: 110353}
}

@ARTICLE{Gan20112286,
	author = {Gan, Yang Ying and Hou, Chun Sheng and Zhou, Ting and Xu, Shu Fa},
	title = {Plant identification based on artificial intelligence},
	year = {2011},
	journal = {Advanced Materials Research},
	volume = {255-260},
	pages = {2286 – 2290},
	doi = {10.4028/www.scientific.net/AMR.255-260.2286},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959786420&doi=10.4028%2fwww.scientific.net%2fAMR.255-260.2286&partnerID=40&md5=8ba3563b02816225b44265bfdb76599b},
	affiliations = {Institute of Sci-Tech Information, Guangdong Academy of Agricultural Science, China; Bee Research Institute, Chinese Academy of Agricultural Sciences, China},
	abstract = {Species identification plays an important role in botanical research, but traditional identification tool, which mainly depends on reference books or identification keys, is often recognized as a difficult and frustrating task, especially for novices. In recent decades, many efforts have been made by taxonomists and programmers to ease the difficulty of species identification by developing a range of tools that increasingly involved the use of computers. In this paper, new advances of plant identification based on three main artificial intelligent technologies: expert system, artificial neural network, and machine vision are briefly introduced. Several trends of plant identification tools for non-expert users are also proposed in the last part. © (2011) Trans Tech Publications, Switzerland.},
	author_keywords = {Artificial neural network; Expert system; Machine vision; Plant identification},
	keywords = {Building materials; Civil engineering; Computer vision; Construction equipment; Expert systems; Artificial intelligent; Artificial Neural Network; Identification keys; Identification tools; Plant identification; Species identification; Neural networks},
	correspondence_address = {Y.Y. Gan; Institute of Sci-Tech Information, Guangdong Academy of Agricultural Science, China; email: sharelot@163.com},
	issn = {10226680},
	isbn = {978-303785139-5},
	language = {English},
	abbrev_source_title = {Adv. Mater. Res.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2011 International Conference on Civil Engineering and Building Materials, CEBM 2011; Conference date: 29 July 2011 through 31 July 2011; Conference code: 85320}
}

@CONFERENCE{Hamid2011,
	author = {Hamid, Rahayu A. and Thom, James A.},
	title = {RMIT at Image CLEF 2011 plant identification},
	year = {2011},
	journal = {CEUR Workshop Proceedings},
	volume = {1177},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922032538&partnerID=40&md5=2e15dbab133600c431da99578a2a2914},
	affiliations = {School of Computer Science and Information Technology, RMIT University, Melbourne, Australia},
	abstract = {This paper presents the contribution of the ISAR group at RMIT University to the ImageCLEF 2011 Plant identification task. The task involves identifying various different species of trees based on images of their leaves. Our main objective is to investigate the performance of two classification algorithms in associating the correct tree species to each test image. We extracted visual features from the data set using the feature extraction module in GIFT. From all the features extracted, we selected 166 features of the colour histogram. The classification algorithms used are instance based learning and decision trees. Both algorithms were implemented using the Weka 3 data mining toolkit. Classifiers for both algorithms were evaluated by a 10 folds cross-validation. Based on the official results, our runs did not perform well due to three main reasons namely, feature selection, training data and classifier parameters.},
	author_keywords = {Classification; Image feature extraction; Plant identification},
	keywords = {Algorithms; Data; Images; Algorithms; Data mining; Decision trees; Extraction; Feature extraction; Forestry; Image processing; Plants (botany); Classification algorithm; Colour histograms; Cross validation; Image feature extractions; Instance based learning; Plant identification; Training data; Visual feature; Classification (of information)},
	editor = {Petras V. and Forner P. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Clough P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2011 Cross Language Evaluation Forum Conference, CLEF 2011; Conference date: 19 September 2011 through 22 September 2011; Conference code: 110352}
}

@ARTICLE{Kebapci20111475,
	author = {Kebapci, Hanife and Yanikoglu, Berrin and Unal, Gozde},
	title = {Plant image retrieval using color, shape and texture features},
	year = {2011},
	journal = {Computer Journal},
	volume = {54},
	number = {9},
	pages = {1475 – 1490},
	doi = {10.1093/comjnl/bxq037},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052575312&doi=10.1093%2fcomjnl%2fbxq037&partnerID=40&md5=e3e9a665daa4965bc1d047e2d0ad1e9c},
	affiliations = {Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul 34956, Turkey},
	abstract = {We present a content-based image retrieval system for plant image retrieval, intended especially for the house plant identification problem. A plant image consists of a collection of overlapping leaves and possibly flowers, which makes the problem challenging. We studied the suitability of various well-known color, shape and texture features for this problem, as well as introducing some new texture matching techniques and shape features. Feature extraction is applied after segmenting the plant region from the background using the max-flow min-cut technique. Results on a database of 380 plant images belonging to 78 different types of plants show promise of the proposed new techniques and the overall system: in 55% of the queries, the correct plant image is retrieved among the top-15 results. Furthermore, the accuracy goes up to 73% when a 132-image subset of well-segmented plant images are considered. © The Author 2010. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved.},
	author_keywords = {Gabor wavelets; image retrieval; plants; SIFT},
	keywords = {Color matching; Feature extraction; Image retrieval; Query languages; Textures; A plants; Content-based image retrieval system; Gabor wavelets; Matching techniques; Max-flow min-cut; Plant identification; plants; Shape features; SIFT; Texture features; Search engines},
	correspondence_address = {B. Yanikoglu; Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul 34956, Turkey; email: berrin@sabanciuniv.edu},
	issn = {14602067},
	coden = {CMPJA},
	language = {English},
	abbrev_source_title = {Comput J},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 67}
}

@CONFERENCE{Grozea2012,
	author = {Grozea, Cristian},
	title = {Brainsignals submission to plant identification task at ImageCLEF 2012},
	year = {2012},
	journal = {CEUR Workshop Proceedings},
	volume = {1178},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922021368&partnerID=40&md5=b00e5cffde80d30dc0d0c58ddf973317},
	affiliations = {Fraunhofer FOKUS, Berlin, Germany},
	abstract = {This article describes our first participation to the plant identification task of ImageCLEF. We have approached it as intended with the least possible computer-vision related techniques (especially without costly image preprocessing and feature extraction) and as much as possible machine learning techniques (random forests). Our method is purely visual and entirely automatic, using only the image information. One should mention that the total time spent with preparing this submission was only about one week. The results were accordingly fairly poor, but probably usable for mobile applications, as the whole process of preprocessing and classification is very fast. We propose a slight modification to the Otsu algorithm, which makes it better suitable for the scan-like images.},
	keywords = {Artificial intelligence; Computer vision; Decision trees; Feature extraction; Image information; Image preprocessing; Machine learning techniques; Mobile applications; Otsu algorithm; Plant identification; Random forests; Whole process; Learning systems},
	editor = {Karlgren J. and Womser-Hacker C. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Forner P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2012 Cross Language Evaluation Forum Conference, CLEF 2012; Conference date: 17 September 2012 through 20 September 2012; Conference code: 110353}
}

@CONFERENCE{Yusof2009,
	author = {Yusof, Rubiyah and Rosli, Nenny Ruthfalydia and Khalid, Marzuki},
	title = {Tropical wood species recognition based on Gabor filter},
	year = {2009},
	journal = {Proceedings of the 2009 2nd International Congress on Image and Signal Processing, CISP'09},
	doi = {10.1109/CISP.2009.5302660},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-73849105373&doi=10.1109%2fCISP.2009.5302660&partnerID=40&md5=eb83997a4b2c18d7562add8aa2851b0b},
	affiliations = {Center for Artificial Intelligence and Robotics, Universiti Teknologi Malaysia, International Campus, 54100 Kuala Lumpur, Jalan Semarak, Malaysia},
	abstract = {Tropical timber woods have more than 1,000 species. Some of the species have similar patterns with others and some have different patterns even though they are of the same species. One of the main problems in wood species recognition system is the lack of discriminative features of the texture images. Gabor filter has been extensively used as feature extractor for various applications such as face detection, face recognition, image retrieval and font type extraction. In our work, we propose the use of Gabor filter to generate multiple processed images from a single image so that more features can be extracted and will be trained by neural network. The use of Gabor filters will optimally localized the properties of the images in both spatial and frequency domain. The features of the filtered images are extracted using co-occurrence matrix approach, known as grey level co-occurrence matrix (GLCM). A multi-layer neural network based on the popular BP (back propagation) algorithm is used for classification. The results show that increasing the number of features by means of Gabor filters as well as the right combination of Gabor filters increases the accuracy rate of the system. ©2009 IEEE.},
	author_keywords = {Gabor filter; Grey level co-occurrence matrix (GLCM); Image processing; Neural network; Texture pattern recognition; Wood recognition},
	keywords = {Image Analysis; Neural Networks; Pattern Recognition; Signals; Texture; Wood; Backpropagation; Feature extraction; Image enhancement; Image retrieval; Imaging systems; Neural networks; Signal processing; Textures; Wood; Co-occurrence-matrix; Gabor filter; Grey levels; Texture pattern recognition; Wood recognition; Face recognition},
	isbn = {978-142444131-0},
	language = {English},
	abbrev_source_title = {Proc. Int. Congr. Image Signal Process., CISP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 2009 2nd International Congress on Image and Signal Processing, CISP'09; Conference date: 17 October 2009 through 19 October 2009; Conference code: 78839}
}

@CONFERENCE{Tan2012,
	author = {Tan, Wooi-Nee and Tan, Yi-Fei and Koo, Ah-Choo and Lim, Yan-Peng},
	title = {Petals' shape descriptor for blooming flowers recognition},
	year = {2012},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {8334},
	doi = {10.1117/12.966367},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874712216&doi=10.1117%2f12.966367&partnerID=40&md5=68d4dde86206111366a8bdfbceaf00dd},
	affiliations = {Faculty of Engineering, Multimedia University, Jalan Multimedia, 63100 Cyberjaya, Selangor, Malaysia; Faculty of Creative Multimedia, Multimedia University, Jalan Multimedia, 63100 Cyberjaya, Selangor, Malaysia},
	abstract = {This paper proposes a new descriptor to identify the petals' shape of a blooming flower based on the digital images captured in natural scene. The proposed descriptor can be used as one of features in computer aided flower recognition system beside the commonly used features such as number of petals and color. Experiments were conducted on the Malaysia flowers with same number of petals and with similar color across different species of flowers. 35 images from 7 species were used as the training set to set up the reference values of petals' shape descriptor and 7 new images were used as the testing set. The descriptor calculated from the testing set is then compared to the reference values from the training set to achieve the flowers recognition purpose. With the given set of data, complete success in full identification rate was obtained. © 2012 SPIE.},
	author_keywords = {flower recognition; petal; shape descriptor},
	keywords = {Engineering; Molecular physics; As numbers; Computer aided; Descriptors; Digital image; Flower recognition; Identification rates; Malaysia; Natural scenes; petal; Reference values; Shape descriptors; Training sets; Image processing},
	correspondence_address = {W.-N. Tan; Faculty of Engineering, Multimedia University, Jalan Multimedia, 63100 Cyberjaya, Selangor, Malaysia; email: wntan@mmu.edu.my},
	issn = {0277786X},
	isbn = {978-081948991-3},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 4th International Conference on Digital Image Processing, ICDIP 2012; Conference date: 7 April 2012 through 8 April 2012; Conference code: 95942}
}

@CONFERENCE{Suta2012123,
	author = {Suta, Loreta Adriana and Scuturici, Mihaela and Miguet, Serge and Tougne, Laure and Vaida, Mircea-Florin},
	title = {Local blur assessment in natural images},
	year = {2012},
	journal = {VISAPP 2012 - Proceedings of the International Conference on Computer Vision Theory and Applications},
	volume = {1},
	pages = {123 – 128},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862147973&partnerID=40&md5=6405a524d852d2a2c286dff6a62e96c2},
	affiliations = {Technical University of Cluj, Napoca, 400114, Cluj Napoca, Romania; Université de Lyon, CNRS, Bron, France; Université Lyon 2, LIRIS, UMR5205, F-69676, Lyon, France},
	abstract = {This paper presents a local no-reference blur assessment method in natural macro-like images. The purpose is to decide the blurriness of the object of interest. In our case, it represents the first step for a plant recognition system. Blur detection works on small non-overlapping blocks using wavelet decomposition and edge classification. At the block level the number of edges is less than on global images. A new set of rules is obtained by a supervised decision tree algorithm trained on a manually labelled base of 1500 blurred/unblurred images. Our purpose is to achieve a qualitative decision of the blurriness/sharpness of the object of interest making it the first step towards a segmentation process. Experimental results show this method outperforms two other methods found in literature, even if applied on a block basis. Together with a presegmentation step, the method allows to decide if the object of interest (leaf, flower) is sharp in order to extract precise botanical key identification features (e. g. leaf border).},
	author_keywords = {Local blur detection; No-reference blur metric; Wavelet analysis},
	keywords = {Decision trees; Image quality; Wavelet analysis; Wavelet decomposition; A plants; Assessment methods; Blur detection; Decision-tree algorithm; Edge classification; Natural images; No references; Object of interests; Pre-segmentation; Segmentation process; Set of rules; Computer vision},
	correspondence_address = {L.A. Suta; Technical University of Cluj, Napoca, 400114, Cluj Napoca, Romania; email: Loreta.Suta@com.utcluj.ro},
	isbn = {978-989856503-7},
	language = {English},
	abbrev_source_title = {VISAPP - Proc. Int. Conf. Comput. Vis. Theory Appl.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: International Conference on Computer Vision Theory and Applications, VISAPP 2012; Conference date: 24 February 2012 through 26 February 2012; Conference code: 90194}
}

@ARTICLE{Nakarmi201223,
	author = {Nakarmi, A.D. and Tang, L.},
	title = {Automatic inter-plant spacing sensing at early growth stages using a 3D vision sensor},
	year = {2012},
	journal = {Computers and Electronics in Agriculture},
	volume = {82},
	pages = {23 – 31},
	doi = {10.1016/j.compag.2011.12.011},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855865864&doi=10.1016%2fj.compag.2011.12.011&partnerID=40&md5=4f9d6fa879f90d8a9bb7359180a7981e},
	affiliations = {Agricultural and Biosystems Engineering, Iowa State University, Ames, IA 50011, 153 Davidson Hall, United States; Agricultural and Biosystems Engineering, Iowa State University, Ames, IA 50011, 203 Davidson Hall, United States},
	abstract = {An inter-plant spacing sensing system using a TOF (time of flight) of light based 3D sensor was developed. The 3D sensor was capable of capturing distance information, intensity and amplitude data in a single shot. The side view depth images captured were stitched together using distance information from a wheel encoder in conjunction with a feature-based image sequencing process for the stem location identification. One obvious advantage of the system over current color-based 2D systems was the use of depth images for plant identification, which was less sensitive to color variations. A covered cart was designed to prevent the sunlight from directly shedding on the plants and to reduce the interference from wind, which in turn made the system usable throughout the day. The vertical camera position was easily adjustable making the system suitable to work with plants at different growth stages. The use of side-view images made the system capable of detecting inclined plants and therefore, boosted the performance of the system in precisely locating the stem centers, which in turn minimized the measurement errors. The measurement accuracy demonstrated the system superiority over the current systems which make use of top-view images for inter-plant spacing sensing. The system achieved an overall mean root mean squared error (RMSE) of 0.017m with a mean plant misidentification ratio of 2.2%. The coefficient of determination (R 2) was 0.95 between the in-field manual distance measurements and the system distance estimates. © 2011 Elsevier B.V.},
	author_keywords = {3D; Corn plant spacing sensing; Early growth stages; Image processing; Machine vision},
	keywords = {Zea mays; Computer vision; Image processing; Sensors; Three dimensional; 2-D systems; 3D; 3D sensor; 3D vision sensor; Camera positions; Coefficient of determination; Color variations; Corn plant spacing sensing; Current system; Depth image; Distance information; Early growth; Feature-based; Growth stages; Image sequencing; In-field; Location identification; Measurement accuracy; Over current; Plant identification; Root mean squared errors; Sensing systems; Side view; Single shots; Time of flight; Wheel encoders; accuracy assessment; agricultural application; agricultural research; automation; color; computer vision; developmental stage; error analysis; image processing; imaging method; maize; sensor; spacing; stem; stereo image; three-dimensional modeling; Agriculture},
	correspondence_address = {L. Tang; Agricultural and Biosystems Engineering, Iowa State University, Ames, IA 50011, 203 Davidson Hall, United States; email: lietang@iastate.edu},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 60}
}

@CONFERENCE{Kundu2012608,
	author = {Kundu, S. and Hazra, A. and Deb, K. and Hazra, P.},
	title = {Dimensionality reduction of morphological features of tomato leaves and fruiting habits},
	year = {2012},
	journal = {Proceedings of the 2012 International Conference on Communications, Devices and Intelligent Systems, CODIS 2012},
	pages = {608 – 611},
	doi = {10.1109/CODIS.2012.6422276},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874403931&doi=10.1109%2fCODIS.2012.6422276&partnerID=40&md5=dbf8960b81a41e252987cad1750f21be},
	affiliations = {Department of Computer Science and Engineering, Jadavpur University, Kolkata, India; Dept. of Vegetable and Crops, Faculty of Horticulture, Bidhan Chandra Krishi Viswavidyalaya, Kalyani, India},
	abstract = {Tomato (Solanum lycopersicum L) belongs to the family Solanaceae, which is extensively grown around the world. In Agriculture and Horticulture, the genetic purity of cultivars is critical to farmers, plant breeders, seed producers and as well as regulatory agencies. The genetic and morphological shape based features are used to classify different tomato cultivars and species. However, the large variations present in the shapes of tomato leaves and fruits make it complex enough to classify. In this paper we have applied image processing techniques on tomato leaves and fruits to obtain enhanced binarized images which precede the most crucial part of shape based feature extraction. Different morphological features are obtained and analyzed for tomato leaf recognition prototyping model. Finally, features are transformed for dimension reduction to get a better visibility of the features through Principal Component Analysis. © 2012 IEEE.},
	author_keywords = {Dimensionality Reduction; Feature Transformation; Leaf Classification; Morphology; Principal Component Analysis; Tomato Fruit; Tomato leaf},
	keywords = {Agriculture; Feature extraction; Image processing; Intelligent systems; Morphology; Plants (botany); Principal component analysis; Seed; Dimensionality reduction; Feature transformations; Leaf classification; Tomato fruits; Tomato leaf; Fruits},
	isbn = {978-146734698-6},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Commun., Devices Intelligent Syst., CODIS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2012 International Conference on Communications, Devices and Intelligent Systems, CODIS 2012; Conference date: 28 December 2012 through 29 December 2012; Conference code: 95803}
}

@ARTICLE{Vaughn2011347,
	author = {Vaughn, Nicholas R. and Moskal, L. Monika and Turnblom, Eric C.},
	title = {Fourier transformation of waveform Lidar for species recognition},
	year = {2011},
	journal = {Remote Sensing Letters},
	volume = {2},
	number = {4},
	pages = {347 – 356},
	doi = {10.1080/01431161.2010.523021},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054718909&doi=10.1080%2f01431161.2010.523021&partnerID=40&md5=20e2d25108f19f219817894fccb0cf48},
	affiliations = {School of Forest Resources, University of Washington, Seattle, WA 98195, United States},
	abstract = {In precision forestry, tree species identification is one of the critical variables of forest inventory. Lidar, specifically full-waveform Lidar, holds high promise in the ability to identify dominant hardwood tree species in forests. Raw waveform Lidar data contain more information than can be represented by a limited series of fitted peaks. Here we attempt to use this information with a simple transformation of the raw waveform data into the frequency domain using a fast Fourier transform. Some relationships are found among the influences of component frequencies across a given species. These relationships are exploited using a classification tree approach to separate three hardwood tree species native to the Pacific Northwest of the United States. We are able to correctly classify 75% of the trees (̂κ 0:615) in our training data set. Each tree's species was predicted using a classification tree built from all the other training trees. Two of the species grow in proximity and grow to a similar form, making differentiation difficult. Across all the classification trees built during the analysis, a small group of frequencies is predominantly used as predictors to separate the species. © 2011 Taylor & Francis.},
	keywords = {Forestry; Fourier Analysis; Hardwoods; Inventories; Pattern Recognition; Radar; Pacific Northwest; Agriculture; Fast Fourier transforms; Fourier analysis; Hardwoods; Metadata; Optical radar; Classification trees; Critical variables; Forest inventory; Frequency domains; LIDAR data; Pacific Northwest; Small groups; Species recognition; Training data sets; Tree species; Wave forms; Waveform data; forest inventory; forestry practice; Fourier transform; growth form; growth rate; identification method; image analysis; lidar; native species; vegetation classification; waveform analysis; wood; Plant extracts},
	correspondence_address = {L.M. Moskal; School of Forest Resources, University of Washington, Seattle, WA 98195, United States; email: lmmoskal@u.washington.edu},
	issn = {21507058},
	language = {English},
	abbrev_source_title = {Remote Sens. Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@CONFERENCE{Yahiaoui2012254,
	author = {Yahiaoui, Itheri and Mzoughi, Olfa and Boujemaa, Nozha},
	title = {Leaf shape descriptor for tree species identification},
	year = {2012},
	journal = {Proceedings - IEEE International Conference on Multimedia and Expo},
	pages = {254 – 259},
	doi = {10.1109/ICME.2012.130},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868154199&doi=10.1109%2fICME.2012.130&partnerID=40&md5=038666181edd44d9e67bcc5629c91145},
	affiliations = {INRIA Rocquencourt, France; CReSTIC Université de Reims, France},
	abstract = {The problem of automatic leaf identification is particularly challenging because, in addition to constraints derived from image processing such as geometric deformations (rotation, scale, translation) and illumination variations, it involves difficulties arising from foliar properties. These include two main aspects: the first is the enormous number and diversity of leaf species and the second, which is relevant to some special species, is the high inter-species and the low intra-species similarity. In this paper, we present a novel boundary-based approach that attempts to overcome the most of these constraints. This method has been compared to results obtained in the image CLEF 2011 plant identification task. The main advantage of this first benchmark edition is that different image retrieval techniques were tested and a crowd-sourced leaf dataset was used. Our method provides the best classification rate for scan and scan-like pictures. Besides its high accuracy, our method satisfies real-time requirements with a low computational cost. © 2012 IEEE.},
	author_keywords = {feature extraction; leaf database; leaf form; Plant recognition; shape descriptor},
	keywords = {Feature extraction; Image processing; Classification rates; Computational costs; Data sets; Geometric deformations; Illumination variation; Image retrieval techniques; leaf form; Leaf identification; Leaf shape; Plant identification; Plant recognition; Real time requirement; Shape descriptors; Tree species; Exhibitions},
	issn = {1945788X},
	language = {English},
	abbrev_source_title = {Proc. IEEE Int. Conf. Multimedia Expo},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 39; Conference name: 2012 13th IEEE International Conference on Multimedia and Expo, ICME 2012; Conference date: 9 July 2012 through 13 July 2012; Conference code: 93415}
}

@ARTICLE{Qi2012158,
	author = {Qi, Xianbiao and Xiao, Rong and Guo, Jun and Zhang, Lei},
	title = {Pairwise rotation invariant co-occurrence local binary pattern},
	year = {2012},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {7577 LNCS},
	number = {PART 6},
	pages = {158 – 171},
	doi = {10.1007/978-3-642-33783-3_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867840628&doi=10.1007%2f978-3-642-33783-3_12&partnerID=40&md5=496e853a86ef76c482401ce6033ae49a},
	affiliations = {Beijing University of Posts and Telecommunications, Beijing 100876, China; Microsoft Research Asia, Beijing 100080, China},
	abstract = {In this work, we introduce a novel pairwise rotation invariant co-occurrence local binary pattern (PRI-CoLBP) feature which incorporates two types of context - spatial co-occurrence and orientation co-occurrence. Different from traditional rotation invariant local features, pairwise rotation invariant co-occurrence features preserve relative angle between the orientations of individual features. The relative angle depicts the local curvature information, which is discriminative and rotation invariant. Experimental results on the CUReT, Brodatz, KTH-TIPS texture dataset, Flickr Material dataset, and Oxford 102 Flower dataset further demonstrate the superior performance of the proposed feature on texture classification, material recognition and flower recognition tasks. © 2012 Springer-Verlag.},
	keywords = {Classification (of information); Computer vision; Textures; Brodatz; Co-occurrence; Co-occurrence features; Data sets; Flower recognition; Local binary patterns; Local curvature; Material recognition; Rotation invariant; Texture classification; Rotation},
	issn = {16113349},
	isbn = {978-364233782-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 37; Conference name: 12th European Conference on Computer Vision, ECCV 2012; Conference date: 7 October 2012 through 13 October 2012; Conference code: 93332}
}

@ARTICLE{Wang2011435,
	author = {Wang, Hang-Jun and Qi, Heng-Nian and Wang, Xiao-Feng},
	title = {A new wood recognition method based on Gabor entropy},
	year = {2011},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {6839 LNAI},
	pages = {435 – 440},
	doi = {10.1007/978-3-642-25944-9_56},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862963245&doi=10.1007%2f978-3-642-25944-9_56&partnerID=40&md5=f52c78e049a097d6b2de096cb7540c35},
	affiliations = {Department of Automation, University of Science and Technology of China, 230027, Hefei, China; Hefei Institute of Intelligent Machines, Chinese Academy of Science, 230031, Hefei, China; School of Information Engineering, Zhejiang A and F University, 311300, Lin'an, China; Key Lab. of Network and Intelligent Information Processing, Department of Computer Science and Technology, Hefei University, Hefei, 230601, China},
	abstract = {Correct wood recognition has an important meaning in rational use of wood resources. Automatic wood recognition based on wood stereogram are studied in this paper. According to the wood stereogram characteristics, a method of image normalization is presented firstly. Then wood texture features are extracted using Gabor wavelet with analyzing the best scale and orientation parameters. In addition to the mean and standard deviation on the Gabor filter bank, entropy, contrast and other statistical features are used for classification. Experimental results show that the entropy can better extract texture features on Gabor wavelet, which greatly improve the wood recognition rate. © 2012 Springer-Verlag.},
	author_keywords = {Gabor filter; texture analysis; Wood recognition; wood stereogram},
	keywords = {Computation; Forest Products; Image Analysis; Pattern Recognition; Statistical Analysis; Wood; Computation theory; Entropy; Feature extraction; Filter banks; Intelligent computing; Textures; Gabor filter; Gabor filter banks; Gabor wavelets; Method of images; Orientation parameter; Standard deviation; Statistical features; texture analysis; Texture features; Wood recognition; Wood resources; Wood},
	correspondence_address = {H.-N. Qi; School of Information Engineering, Zhejiang A and F University, 311300, Lin'an, China; email: qihengnian@yahoo.com.cn},
	issn = {16113349},
	isbn = {978-364225943-2},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; Conference name: 7th International Conference on Intelligent Computing, ICIC 2011; Conference date: 11 August 2011 through 14 August 2011; Conference code: 88035}
}

@CONFERENCE{Prasad2011343,
	author = {Prasad, Shitala and Kudiri, Krishna Mohan and Tripathi, R.C.},
	title = {Relative sub-image based features for leaf recognition using support vector machine},
	year = {2011},
	journal = {ACM International Conference Proceeding Series},
	pages = {343 – 346},
	doi = {10.1145/1947940.1948012},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952932899&doi=10.1145%2f1947940.1948012&partnerID=40&md5=eedac773330f132c831c6c019517ef28},
	affiliations = {Department of Information Technology, Indian Institute of Information Technology, Allahabad, India},
	abstract = {In this paper, we extract our proposed RSC features from leaf images and use SVM classifier to implement an automated leaf recognition system for plant leaf identification and classification. Automatic plant species identification and classification is helpful in biology, forest and agriculture to study and discover new species in plant in botanical gardens and is also used to recognize the medicinal plants to prepare herbal medicines. Here, 300 leaf features are extracted from a single leaf of 624 leaf dataset to classify 23 different kinds of plant species with an average accuracy of 95%. Compared with other approaches, our proposed algorithm has less time complexity and is easy to implementation with higher accuracy. Copyright © 2011 ACM.},
	author_keywords = {Feature extraction; Leaf recognition; Plant classification; Relative sub-images coefficients; Support vector machine},
	keywords = {Algorithms; Image processing; Optical character recognition; Plant extracts; Support vector machines; Botanical gardens; Data sets; Herbal medicines; Image-based features; Leaf images; Leaf recognition; Medicinal plants; New species; Plant classification; Plant leaf; Plant species; Plant species identification; Subimages; SVM classifiers; Time complexity; Feature extraction},
	correspondence_address = {S. Prasad; Department of Information Technology, Indian Institute of Information Technology, Allahabad, India; email: ihc2009011@iiita.ac.in},
	isbn = {978-145030464-1},
	language = {English},
	abbrev_source_title = {ACM Int. Conf. Proc. Ser.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 37; Conference name: International Conference on Communication, Computing and Security, ICCCS 2011; Conference date: 12 February 2011 through 14 February 2011; Conference code: 84349}
}

@CONFERENCE{Henries2012,
	author = {Henries, Dale G. and Tashakkori, Rahman},
	title = {Extraction of leaves from herbarium images},
	year = {2012},
	journal = {IEEE International Conference on Electro Information Technology},
	doi = {10.1109/EIT.2012.6220752},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864189484&doi=10.1109%2fEIT.2012.6220752&partnerID=40&md5=3c1e221b510f2e2bacc7d306619d422d},
	affiliations = {Department of Computer Science, Appalachian State University, Boone, NC, United States},
	abstract = {Detection of a leaf from the image that contains the leaf, branches, and other background material is challenging. The existing approaches for automated leaf extraction do not provide satisfactory results when the end-users provide a plant image. For such an application to be feasible, the automated leaf extraction algorithm should handle leaves of various colors, shapes, sizes, and locations within the image. This paper presents an algorithm for automating the process of extracting the possible target leaves from herbarium plant images. Our results indicate high level of accuracy for the proposed algorithm. © 2012 IEEE.},
	author_keywords = {Automatic leaf detection; herbarium image leaf recognition; leaf extraction},
	keywords = {Algorithms; End-users; Extraction algorithms; Leaf recognition; Image processing},
	correspondence_address = {D.G. Henries; Department of Computer Science, Appalachian State University, Boone, NC, United States; email: dgh@cs.appstate.edu},
	issn = {21540373},
	isbn = {978-146730819-9},
	language = {English},
	abbrev_source_title = {IEEE Int. Conf. Electro Inform. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 2012 IEEE International Conference on Electro/Information Technology, EIT 2012; Conference date: 6 May 2012 through 8 May 2012; Conference code: 91401}
}

@CONFERENCE{Singh2011,
	author = {Singh, Krishna and Gupta, Indra and Gupta, Sangeeta},
	title = {Classification of bamboo plant based on digital image processing by central moment},
	year = {2011},
	journal = {ICIIP 2011 - Proceedings: 2011 International Conference on Image Information Processing},
	doi = {10.1109/ICIIP.2011.6108840},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862925861&doi=10.1109%2fICIIP.2011.6108840&partnerID=40&md5=6530a4c01e69ef43664eb170f50a5c83},
	affiliations = {Department of Electrical Engineering, Indian Institute of Technology Roorkee, India; Forest Research Institute, Dehradun, India},
	abstract = {There is advancement in every day for image classification starting from object classification to remote sensing image. Plant classification from their part is one of the most current research works going in the area of image processing. The proposed work is a new approach for bamboo species classification from their Culm sheath by using Central moment. Automated recognition of bamboo has not yet been well established mainly due to lack of research in this area, non-availability and difficulty in obtaining the database. Therefore need of recognition of bamboo species is required by the user. The proposed work is an automated classification of bamboo species system based on shape features of bamboo Culm sheath by using the central moment classifier. Four different bamboo species are taking for experiment in the proposed work. The results obtained shows considerable recognition accuracy proving that the techniques used is suitable to be implemented for commercial purposes. © 2011 IEEE.},
	author_keywords = {Culm Sheath; image moment; plant classification},
	keywords = {Data processing; Image reconstruction; Remote sensing; Automated classification; Automated recognition; Bamboo species; Central moments; Culm sheath; Image moments; Object classification; Plant classification; Recognition accuracy; Remote sensing images; Shape features; System-based; Techniques used; Bamboo},
	correspondence_address = {K. Singh; Department of Electrical Engineering, Indian Institute of Technology Roorkee, India; email: singhkrishna5@gmail.com},
	isbn = {978-161284861-7},
	language = {English},
	abbrev_source_title = {ICIIP - Proc.: Int. Conf. Image Inf. Process.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2011 International Conference on Image Information Processing, ICIIP 2011; Conference date: 3 November 2011 through 5 November 2011; Conference code: 88114}
}

@CONFERENCE{Yanikoglu2012,
	author = {Yanikoglu, Berrin and Aptoula, Erchan and Tirkaz, Caglar},
	title = {Sabanci-okan system at image Clef 2012: Combining features and classifiers for plant identification},
	year = {2012},
	journal = {CEUR Workshop Proceedings},
	volume = {1178},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922032598&partnerID=40&md5=5e1a64f8359cae73c113d85ae932bd31},
	affiliations = {Sabanci University, Istanbul, 34956, Turkey; Okan University, Istanbul, 34959, Turkey},
	abstract = {We describe our participation in the plant identification task of ImageClef 2012. We submitted two runs, one fully automatic and another one where human assistance was provided for the images in the photo category. We have not used the meta-data in either one of the systems, for exploring the extent of image analysis for the plant identification problem. Our approach in both runs employs a variety of shape, texture and color descriptors (117 in total). We have found shape to be very discriminative for isolated leaves (scan and pseudoscan categories), followed by texture. While we have experimented with color, we could not make use of the color information. We have employed the watershed algorithm for segmentation, in slightly different forms for automatic and human assisted systems. Our systems have obtained the best overall results in both automatic and manual categories, with 43% and 45% identification accuracies respectively. We have also obtained the best results on the scanned image category with 58% accuracy.},
	author_keywords = {Classifier combination; Mathematical morphology; Plant identification; Support vector machines},
	keywords = {Classifiers; Color; Mathematical morphology; Plants (botany); Support vector machines; Textures; Classifier combination; Color descriptors; Color information; Human assistance; Identification accuracy; Plant identification; Scanned images; Water-shed algorithm; Image processing},
	editor = {Karlgren J. and Womser-Hacker C. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Forner P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; Conference name: 2012 Cross Language Evaluation Forum Conference, CLEF 2012; Conference date: 17 September 2012 through 20 September 2012; Conference code: 110353}
}

@CONFERENCE{Tilneac2011353,
	author = {Tilneac, Mihaela and Dolga, Valer},
	title = {Extrinsic calibration of a multi-camera network used for individual plant phenotyping},
	year = {2011},
	journal = {Proceedings - 2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing, ICCP 2011},
	pages = {353 – 359},
	doi = {10.1109/ICCP.2011.6047896},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755142969&doi=10.1109%2fICCP.2011.6047896&partnerID=40&md5=1a357917483c083b060d81a3d9fec0f3},
	affiliations = {Department of Mechatronics, Faculty of Mechanical Engineering, Timisoara, Romania},
	abstract = {Individual plant recognition is a big challenge in the agricultural robotics. Implementation of sensor technologies and algorithms for automatic plant phenotyping are of increasing importance [3]. For an individual plant phenotyping, a multi-camera network is required. The problem of camera calibration is the focus of this research. In this paper, we present a novel extrinsic camera calibration method, developed to be applied in agricultural field. © 2011 IEEE.},
	author_keywords = {agricultural robot; camera calibration; computer vision; multi-sensor fusion; weed control},
	keywords = {Calibration; Computer vision; Intelligent robots; Sensors; Weed control; Agricultural fields; Agricultural robot; camera calibration; Extrinsic calibration; Extrinsic camera; Multi-camera networks; Multi-sensor fusion; Phenotyping; Plant recognition; Sensor technologies; Cameras},
	correspondence_address = {M. Tilneac; Department of Mechatronics, Faculty of Mechanical Engineering, Timisoara, Romania; email: mihaela.tilneac@mec.upt.ro},
	isbn = {978-145771478-8},
	language = {English},
	abbrev_source_title = {Proc. - IEEE Int. Conf. Intelligent Comput. Commun. Process., ICCP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing, ICCP 2011; Conference date: 25 August 2011 through 27 August 2011; Conference code: 87254}
}

@ARTICLE{Beghin2010345,
	author = {Beghin, Thibaut and Cope, James S and Remagnino, Paolo and Barman, Sarah},
	title = {Shape and texture based plant leaf classification},
	year = {2010},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {6475 LNCS},
	number = {PART 2},
	pages = {345 – 353},
	doi = {10.1007/978-3-642-17691-3_32},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650877013&doi=10.1007%2f978-3-642-17691-3_32&partnerID=40&md5=df2be9c2afe7120dea6143564f9cf99e},
	affiliations = {Digital Imaging Research Centre, Kingston University, London, United Kingdom},
	abstract = {This article presents a novel method for classification of plants using their leaves. Most plant species have unique leaves which differ from each other by characteristics such as the shape, colour, texture and the margin. The method introduced in this study proposes to use two of these features: the shape and the texture. The shape-based method will extract the contour signature from every leaf and then calculate the dissimilarities between them using the Jeffrey-divergence measure. The orientations of edge gradients will be used to analyse the macro-texture of the leaf. The results of these methods will then be combined using an incremental classification algorithm. © 2010 Springer-Verlag.},
	author_keywords = {incremental classification; Plant identification; Shape-based analysis; Sobel operator; texture-based analysis},
	keywords = {Computer vision; Mathematical operators; Computer vision; Divergence measures; Edge gradient; Incremental classifications; Macrotextures; Novel methods; Plant identification; Plant leaf; Plant species; Shape based; Shape-based analysis; Sobel operator; Classification algorithm; Divergence measures; Plant identification; Plant leaf classifications; Plant species; Shape and textures; Shape-based analysis; Sobel operator; Textures; Plants (botany)},
	correspondence_address = {T. Beghin; Digital Imaging Research Centre, Kingston University, London, United Kingdom; email: t.beghin@kingston.ac.uk},
	issn = {16113349},
	isbn = {3642176909; 978-364217690-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 97; Conference name: 12th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2010; Conference date: 13 December 2010 through 16 December 2010; Conference code: 83367}
}

@ARTICLE{Jha2011,
	author = {Jha, Ashwani and Shankar, Ravi},
	title = {Employing machine learning for reliable miRNA target identification in plants},
	year = {2011},
	journal = {BMC Genomics},
	volume = {12},
	doi = {10.1186/1471-2164-12-636},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855166072&doi=10.1186%2f1471-2164-12-636&partnerID=40&md5=6b0c7041aa9fd59d70b9151487f3a9a5},
	affiliations = {Studio of Computational Biology and Bioinformatics, Biotechnology Division, Institute of Himalayan Bioresource Technology, Council of Scientific and Industrial Research (CSIR), Palampur 176061 (HP), India},
	abstract = {Background: miRNAs are ~21 nucleotide long small noncoding RNA molecules, formed endogenously in most of the eukaryotes, which mainly control their target genes post transcriptionally by interacting and silencing them. While a lot of tools has been developed for animal miRNA target system, plant miRNA target identification system has witnessed limited development. Most of them have been centered around exact complementarity match. Very few of them considered other factors like multiple target sites and role of flanking regions.Result: In the present work, a Support Vector Regression (SVR) approach has been implemented for plant miRNA target identification, utilizing position specific dinucleotide density variation information around the target sites, to yield highly reliable result. It has been named as p-TAREF (plant-Target Refiner). Performance comparison for p-TAREF was done with other prediction tools for plants with utmost rigor and where p-TAREF was found better performing in several aspects. Further, p-TAREF was run over the experimentally validated miRNA targets from species like Arabidopsis, Medicago, Rice and Tomato, and detected them accurately, suggesting gross usability of p-TAREF for plant species. Using p-TAREF, target identification was done for the complete Rice transcriptome, supported by expression and degradome based data. miR156 was found as an important component of the Rice regulatory system, where control of genes associated with growth and transcription looked predominant. The entire methodology has been implemented in a multi-threaded parallel architecture in Java, to enable fast processing for web-server version as well as standalone version. This also makes it to run even on a simple desktop computer in concurrent mode. It also provides a facility to gather experimental support for predictions made, through on the spot expression data analysis, in its web-server version.Conclusion: A machine learning multivariate feature tool has been implemented in parallel and locally installable form, for plant miRNA target identification. The performance was assessed and compared through comprehensive testing and benchmarking, suggesting a reliable performance and gross usability for transcriptome wide plant miRNA target identification. © 2011 Jha and Shankar; licensee BioMed Central Ltd.},
	keywords = {Artificial Intelligence; MicroRNAs; Plants; Animalia; Arabidopsis; Eukaryota; Lycopersicon esculentum; Medicago; microRNA; microRNA 156; RNA; transcriptome; unclassified drug; microRNA; Arabidopsis; article; computer prediction; data analysis software; DNA flanking region; intermethod comparison; machine learning; mathematical computing; nonhuman; plant identification; RNA analysis; RNA sequence; support vector machine; artificial intelligence; genetics; plant},
	correspondence_address = {R. Shankar; Studio of Computational Biology and Bioinformatics, Biotechnology Division, Institute of Himalayan Bioresource Technology, Council of Scientific and Industrial Research (CSIR), Palampur 176061 (HP), India; email: ravish@ihbt.res.in},
	issn = {14712164},
	coden = {BGMEE},
	pmid = {22206472},
	language = {English},
	abbrev_source_title = {BMC Genomics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 30; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Goéau2011,
	author = {Goéau, Hervé and Bonnet, Pierre and Joly, Alexis and Boujemaa, Nozha and Barthelemy, Daniel and Molino, Jean-François and Birnbaum, Philippe and Mouysset, Elise and Picard, Marie},
	title = {The CLEF 2011 plant images classification task},
	year = {2011},
	journal = {CEUR Workshop Proceedings},
	volume = {1177},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922041505&partnerID=40&md5=622e0259ec6fa889a038005a1b03957d},
	affiliations = {INRIA, IMEDIA team, France; INRA, UMR, AMAP, France; CIRAD, BIOS, AMAP, F-34398, France; IRD, UMR, AMAP, France; CIRAD, UMR, AMAP, France; Tela Botanica, France},
	abstract = {Image CLEF' plant identification task provides a testbed for the system-oriented evaluation of tree species identification based on leaf images. The aim is to investigate image retrieval approaches in the context of crowdsourced images of leaves collected in a collaborative manner. This paper presents an overview of the resources and assessments of the plant identification task at ImageCLEF 2011, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results.},
	author_keywords = {Benchmark; Classification; Collection; Evaluation; Identification; Image clef; Images; Leaves; Plant},
	keywords = {Benchmarking; Classification (of information); Identification (control systems); Image retrieval; Plants (botany); Refuse collection; Evaluation; Image clef; Images; Leaves; Plant; Image processing},
	editor = {Petras V. and Forner P. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Clough P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; Conference name: 2011 Cross Language Evaluation Forum Conference, CLEF 2011; Conference date: 19 September 2011 through 22 September 2011; Conference code: 110352}
}

@CONFERENCE{Villena-Román2011,
	author = {Villena-Román, Julio and Lana-Serrano, Sara and González-Cristóbal, José Carlos},
	title = {DAEDALUS at image CLEF 2011 plant identification task: Using SIFT keypoints for object detection},
	year = {2011},
	journal = {CEUR Workshop Proceedings},
	volume = {1177},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922041469&partnerID=40&md5=78abc9e26adef1948e3454e3a69b706c},
	affiliations = {Universidad Carlos III de Madrid, Spain; Universidad Politécnica de Madrid, Spain; DAEDALUS - Data, Decisions and Language, S.A., Spain},
	abstract = {This paper describes the participation of DAEDALUS at ImageCLEF 2011 Plant Identification task. The task is evaluated as a supervised classification problem over 71 tree species from the French Mediterranean area used as class labels, based on visual content from scan, scan-like and natural photo images. Our approach to this task is to build a classifier based on the detection of keypoints from the images extracted using Lowe's Scale Invariant Feature Transform (SIFT) algorithm. Although our overall classification score is very low as compared to other participant groups, the main conclusion that can be drawn is that SIFT keypoints seem to work significantly better for photos than for the other image types, so our approach may be a feasible strategy for the classification of this kind of visual content.},
	author_keywords = {Classifier; Image retrieval; Keypoints; Pl@ntleaves; Plant identification task; Scale-invariant feature transform; Sift; Test; Training},
	keywords = {Classifiers; Image retrieval; Object detection; Personnel training; Testing; Keypoints; Pl@ntleaves; Plant identification; Scale invariant feature transforms; Sift; Image processing},
	editor = {Petras V. and Forner P. and Ferro N. and University of Padua, Department of Information Engineering (DEI), Via Gradenigo 6/B, Padova and Clough P.},
	publisher = {CEUR-WS},
	issn = {16130073},
	language = {English},
	abbrev_source_title = {CEUR Workshop Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: 2011 Cross Language Evaluation Forum Conference, CLEF 2011; Conference date: 19 September 2011 through 22 September 2011; Conference code: 110352}
}

@ARTICLE{Nanni20122254,
	author = {Nanni, Loris and Brahnam, Sheryl and Lumini, Alessandra},
	title = {Local phase quantization descriptor for improving shape retrieval/classification},
	year = {2012},
	journal = {Pattern Recognition Letters},
	volume = {33},
	number = {16},
	pages = {2254 – 2260},
	doi = {10.1016/j.patrec.2012.07.007},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866777530&doi=10.1016%2fj.patrec.2012.07.007&partnerID=40&md5=7b6af275e7273477b087acbcc29bf83c},
	affiliations = {Department of Information Engineering, 35131 Padova, Via Gradenigo 6/A, Italy; Computer Information Systems, Missouri State University, Springfield, MO 65804, 901 S. National, United States; DEIS, Università di Bologna, 40136 Bologna, Viale Risorgimento 2, Italy},
	abstract = {Shape classification is a field of study with applications ranging from object classification to leaf recognition. In this paper we present an approach based on a matrix descriptor, the local phase quantization, for improving the performance of such widely used shape descriptors as the inner distance shape context (ID), shaper context (SC), and height functions (HF). The basic idea of our novel approach is to transform the shape descriptor obtained by ID/SC/HF into a matrix so that matrix descriptors can be extracted. These matrix descriptors are then compared with the Jeffrey distance and combined with standard ID/SC/HF shape similarity. Since it has recently been shown that ID/SC/HF shape similarities can be coupled with learning context-sensitive shape similarity using graph transduction (LGT) for improving the results, we have also coupled our approach with LGT. Our proposed approach is tested on a wide variety of shape databases including MPEG7 CE-Shape-1, Kimia silhouettes, Tari dataset, a leaf dataset, a tools dataset, a myths figures dataset, and our new human dancer dataset. The experimental results demonstrate that the proposed approach yields significant improvements over baseline shape matching algorithms. All Matlab code used in the proposed paper is available at bias.csr.unibo.it/nanni/Shape.rar. © 2012 Elsevier B.V. All rights reserved.},
	author_keywords = {Height functions; Inner distance shape context; Local phase quantization; Shape classification; Shape context; Texture descriptors},
	keywords = {Pattern recognition; Software engineering; Height functions; Local phase; Shape classification; Shape contexts; Texture descriptors; Image segmentation},
	correspondence_address = {L. Nanni; Department of Information Engineering, 35131 Padova, Via Gradenigo 6/A, Italy; email: loris.nanni@unipd.it},
	issn = {01678655},
	language = {English},
	abbrev_source_title = {Pattern Recogn. Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26}
}

@CONFERENCE{Uluturk2012,
	author = {Uluturk, Caner and Ugur, Aybars},
	title = {Recognition of leaves based on morphological features derived from two half-regions},
	year = {2012},
	journal = {INISTA 2012 - International Symposium on INnovations in Intelligent SysTems and Applications},
	doi = {10.1109/INISTA.2012.6247030},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866601856&doi=10.1109%2fINISTA.2012.6247030&partnerID=40&md5=fa9db6ef3625d5faf06050d99a0f871a},
	affiliations = {Department of Computer Engineering, Ege University, Izmir, Turkey},
	abstract = {Leaf recognition systems can be used for automatic plant taxonomy and provide understanding and managing of plants in botany, medicine, industry and food sector. Trees and flowery plants can be classified by using leaf recognition. This paper proposes a simple method based on bisection of leaves for recognition. After preprocessing techniques are applied for leaves, 7 low-cost morphological features are extracted which are used in the literature. We produced 3 additional features using half leaf images. Most of leaf species have morphological structure that resembles each other a lot. For these leaves, while structural features of one half resemble, features of other half differ. Taking advantage of this knowledge, leaf is oriented according to its major axis and two parts are acquired by slicing leaf on its centroid vertically. Area, extent and eccentricity features are extracted for each part and their proportions to each other are taken as new features in this study. These all 10 features are used as an input to probabilistic neural network (PNN). PNN is trained with 1120 leaf images from 32 different plant species which are taken from FLAVIA dataset. 160 leaf images from the plant species are used for testing. Our experiments and comparisons show that method based on half leaf features has reached one of the best results in the literature for PNN with 92.5% recognition accuracy. © 2012 IEEE.},
	author_keywords = {classification; feature extraction; image processing; leaf recognition; neural networks},
	keywords = {Classification (of information); Feature extraction; Image processing; Intelligent systems; Neural networks; Data sets; Food sector; Leaf images; Leaf recognition; Major axis; Morphological features; Morphological structures; Plant species; Plant taxonomy; Preprocessing techniques; Probabilistic neural networks; Recognition accuracy; SIMPLE method; Structural feature; Plants (botany)},
	correspondence_address = {C. Uluturk; Department of Computer Engineering, Ege University, Izmir, Turkey; email: uluturkcaner@gmail.com},
	isbn = {978-146731446-6},
	language = {English},
	abbrev_source_title = {INISTA - Int. Symp. INnovations Intelligent SysT. Appl.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 34; Conference name: International Symposium on INnovations in Intelligent SysTems and Applications, INISTA 2012; Conference date: 2 July 2012 through 4 July 2012; Conference code: 92831}
}

@ARTICLE{Zhang2009948,
	author = {Zhang, Shanwen and Chau, Kwok-Wing},
	title = {Dimension reduction using semi-supervised locally linear embedding for plant leaf classification},
	year = {2009},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {5754 LNCS},
	pages = {948 – 955},
	doi = {10.1007/978-3-642-04070-2_100},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350776230&doi=10.1007%2f978-3-642-04070-2_100&partnerID=40&md5=927ad408bda3555dcbc0a08e213bbe66},
	affiliations = {Institute of Intelligent Machine, Chinese Academy of Science, Hefei, Anhui 230031, P.O. Box 1130, China; Department of Civil and Structural Engineering, Hong Kong Polytechnic University, Hong Kong},
	abstract = {Plant has plenty use in foodstuff, medicine and industry, and is also vitally important for environmental protection. So, it is important and urgent to recognize and classify plant species. Plant classification based on leaf images is a basic research of botanical area and agricultural production. Due to the high nature complexity and high dimensionality of leaf image data, dimensional reduction algorithms are useful and necessary for such type of data analysis, since it can facilitate fast classifying plants, and understanding and managing plant leaf features. Supervised locally linear embedding (SLLE) is a powerful feature extraction method, which can yield very promising recognition results when coupled with some simple classifiers. In this paper, a semi-SLLE is proposed and is applied to plant classification based on leaf images. The experiment results show that the proposed algorithm performs very well on leaf image data which exhibits a manifold structure. © 2009 Springer Berlin Heidelberg.},
	author_keywords = {Locally linear embedding; Plant classification; Plant leaf image; Semi-SLLE; Supervised locally linear embedding (SLLE)},
	keywords = {Agriculture; Algorithms; Computer science; Feature extraction; Sugar (sucrose); Locally linear embedding; Plant classification; Plant leaf image; Semi-SLLE; Supervised locally linear embedding (SLLE); Intelligent computing},
	issn = {16113349},
	isbn = {3642040691; 978-364204069-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 92; Conference name: 5th International Conference on Intelligent Computing, ICIC 2009; Conference date: 16 September 2009 through 19 September 2009; Conference code: 77838}
}

@ARTICLE{Ni200943,
	author = {Ni, Li-Jun and Zhang, Li-Guo and Xie, Juan and Luo, Jian-Qun},
	title = {Pattern recognition of Chinese flue-cured tobaccos by an improved and simplified K-nearest neighbors classification algorithm on near infrared spectra},
	year = {2009},
	journal = {Analytica Chimica Acta},
	volume = {633},
	number = {1},
	pages = {43 – 50},
	doi = {10.1016/j.aca.2008.11.044},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749183648&doi=10.1016%2fj.aca.2008.11.044&partnerID=40&md5=c0fc2c69ec051a6d022da1b4279fbf03},
	affiliations = {School of Chemistry and Molecular Engineering, East China University of Science and Technology, Shanghai, 200237, China},
	abstract = {In tobacco industry of China, tobacco leaves are classified and managed in terms of their cultivation areas and plant parts of tobacco-stalks. However, sometimes intentionally or involuntary mislabeling cultivation areas, blending tobacco plant parts would occur into tobacco market. The error will affect the style and quality of cigarettes. In the present work, more than 1000 Chinese flue-cured tobacco leaf samples, which have 12 genotypes and cultivated from 5 to 10 regions of China in 2003 and 2004, have been discriminated by means of an improved and simplified KNN classification algorithm (IS-KNN) based on near infrared (NIR) spectra. An original method of optimizing number of significant principal components (PCs) based on analysis of error and cross-validation was advanced. Compared with conventional pattern recognition methods KNN, NN, LDA and PLS-DA, IS-KNN exhibits good adaptability in discrimination of complicated Chinese flue-cured tobaccos. The practice in this work shows that optimized number of PCs and performance of classification models are closely relative to complicated extent of samples but not to number of categories or samples. The results demonstrated the usefulness of NIR spectra combined with chemometrics as an objective and rapid method for the authentication and identification of tobacco leaves or other kinds of powder samples. © 2008 Elsevier B.V. All rights reserved.},
	author_keywords = {Chemometrics; Improved and simplified K-nearest neighbors; Near infrared; Optimization of principal component number; Pattern recognition of multi-categories; Tobacco},
	keywords = {Algorithms; China; Pattern Recognition, Automated; Plant Leaves; Reproducibility of Results; Spectroscopy, Near-Infrared; Tobacco; Nicotiana obtusifolia; Nicotiana tabacum; Agricultural products; Blending; Curing; Flues; Infrared devices; Infrared spectroscopy; Optimization; Pattern recognition; Principal component analysis; article; chemometrics; China; classification algorithm; genotype; k nearest neighbors classification algorithm; near infrared reflectance spectroscopy; nonhuman; plant identification; principal component analysis; priority journal; tobacco; validation process; Chemometrics; Classification models; Cross validations; Improved and simplified K-nearest neighbors; KNN classifications; Near infrared; Near infrared spectrums; Pattern recognition methods; Powder samples; Rapid methods; Tobacco industries; Tobacco leaves; Tobacco markets; Tobacco plants; Tobacco},
	correspondence_address = {L.-J. Ni; School of Chemistry and Molecular Engineering, East China University of Science and Technology, Shanghai, 200237, China; email: nljfyt@sina.com},
	issn = {18734324},
	coden = {ACACA},
	pmid = {19110114},
	language = {English},
	abbrev_source_title = {Anal. Chim. Acta},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 59}
}

@CONFERENCE{Singh2010412,
	author = {Singh, Krishna and Gupta, Indra and Gupta, Sangeeta},
	title = {Retrieval and classification of leaf shape by Support Vector Machine using binary Decision Tree, probabilistic neural network and generic Fourier Moment technique: A comparative study},
	year = {2010},
	journal = {Proc. of the IADIS Int. Conf. - Computer Graphics, Visualization, Computer Vision and Image Processing, CGVCVIP 2010, Visual Commun., VC 2010, Web3DW 2010, Part of the MCCSIS 2010},
	pages = {412 – 417},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955148614&partnerID=40&md5=ff9da352893faa2cc5d06455e124fdcc},
	affiliations = {Department of Electrical Engineering, IIT Roorkee, India; FRI Dehradun, India},
	abstract = {The Leaf Recognition becomes important for Plant Classification. This paper presents classification of plants based on their leaf shape employing three different techniques, the Support Vector Machine with binary Decision Tree (SVMBDT), Probabilistic Neural Network using Principle component analysis (PNN-PCNN) and Fourier moment for solving multiclass problems. In the proposed work these three techniques are used for comparing the performance of the classification of Leaves based on their classification accuracy. The proposed SVM based Binary Decision Tree architecture takes advantage of both the efficient computation of the decision tree architecture and the high classification accuracy of SVMs. This can lead to a dramatic improvement in recognition speed when addressing problems with large number of classes. Classification accuracy from all the three techniques are compared and it is observed that SVM-BDT performs better than PNN-PCNN and Fourier Moment technique. © 2010 IADIS.},
	author_keywords = {Fourier moment; PNN-PCNN; SVM-BDT},
	keywords = {Decision trees; Fourier transforms; Imaging systems; Network architecture; Neural networks; Principal component analysis; Support vector machines; Three dimensional; Virtual reality; Visualization; Binary decision trees; Classification accuracy; Comparative studies; Efficient computation; Fourier moments; Leaf recognition; Leaf shape; Multi-class problems; Plant classification; PNN-PCNN; Principle component analysis; Probabilistic neural networks; Recognition speed; SVM-BDT; Three different techniques; Computer vision},
	isbn = {978-972893922-9},
	language = {English},
	abbrev_source_title = {Proc. IADIS Int. Conf. Comput. Graph., Vis., Comput. Vis. Image Process., CGVCVIP, Vis. Commun., VC, Web3DW, Part MCCSIS},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: IADIS Int. Conf. - Computer Graphics, Visualization, Computer Vision and Image Processing, CGVCVIP 2010, Visual Commun., VC 2010, Web Virtual Reality and Three-Dimensional Worlds, Web3DW 2010, Part of the MCCSIS 2010; Conference date: 27 July 2010 through 29 July 2010; Conference code: 84575}
}

@CONFERENCE{Huang2009618,
	author = {Huang, Rong-Guo and Sang-HyeonJin and Kim, Jung-Hyun and Hong, Kwang-Seok},
	title = {Flower image recognition using difference image entropy},
	year = {2009},
	journal = {MoMM2009 - The 7th International Conference on Advances in Mobile Computing and Multimedia},
	pages = {618 – 621},
	doi = {10.1145/1821748.1821868},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955114694&doi=10.1145%2f1821748.1821868&partnerID=40&md5=088b0ec41a6011e7c0d8b9accfdc0d81},
	affiliations = {Sungkyunkwan University, Jangan-gu, Suwon, Kyungki-do, 440-746, 300 Cheoncheon-dong, South Korea},
	abstract = {In this paper, we suggest and implement a flower image recognition system using Difference Image Entropy (hereinafter, DIE) and contour information of the object. Conventional studies on flower or leaf recognition have restrictions and limitations that include a sharp drop of recognition rate due to the varying positions and number of objects in the original object image. Hence, this paper focuses on 1) contour feature extraction technology by drawing and designating flower region of the user's interest, and 2) a distributed processing-based flower image recognition technology using DIE, for robust flower image recognition from the given original flower image with multi-flower objects. The suggested system was evaluated using ten species of flowers with each ten samples. Experimental results achieved an average recognition rate of 95%. Copyright 2009 ACM.},
	author_keywords = {Contour features extraction; Difference Image Entropy; Flower image recognition},
	keywords = {Dies; Entropy; Image recognition; Mobile computing; Contour features; Contour information; Difference Image Entropy; Difference images; Distributed processing; Extraction technology; Image recognition system; Image recognition technology; Leaf recognition; Recognition rates; User's interest; Feature extraction},
	correspondence_address = {R.-G. Huang; Sungkyunkwan University, Jangan-gu, Suwon, Kyungki-do, 440-746, 300 Cheoncheon-dong, South Korea; email: hrg316@skku.edu},
	isbn = {978-160558659-5},
	language = {English},
	abbrev_source_title = {MoMM - Int. Conf. Adv. Mob. Comput. Multimedia},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: 7th International Conference on Advances in Mobile Computing and Multimedia, MoMM2009; Conference date: 14 December 2009 through 16 December 2009; Conference code: 81267}
}

@ARTICLE{Liu2009253,
	author = {Liu, Jiandu and Zhang, Shanwen and Deng, Shengli},
	title = {A method of plant classification based on wavelet transforms and support vector machines},
	year = {2009},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {5754 LNCS},
	pages = {253 – 260},
	doi = {10.1007/978-3-642-04070-2_29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350397332&doi=10.1007%2f978-3-642-04070-2_29&partnerID=40&md5=7b0d3bd3182759e83af771be2c5c4a9e},
	affiliations = {Missile Institute, Air-Force Engineering University, Sanyuan 713800, China; Hefei Institute of Intelligent Machines, Chinese Academy of Science, Hefei, Anhui 230031, China; Air-Force 93861 Army, Sanyuan 713800, China},
	abstract = {As one of the most important morphological taxonomy features, plant leaf with many strong points has significant influence on research. In this paper, we propose a novel method of plant classification from leaf image set based on wavelet transforms and support vector machines (SVMS). Firstly, the leaf images are converted into the time-frequency domain image by wavelet transforms without any further preprocessing such as image enhancement and texture thinning, and then feature extraction vector is conducted. Then the effectiveness of the proposed method is evaluated by the classification accuracy of SVM classifier. The experimental results about the data set with 300 leaf images show that the method has higher recognition rate and faster processing speed. © 2009 Springer Berlin Heidelberg.},
	author_keywords = {Plant leaf image feature extraction; Support vector machines; Wavelet transforms},
	keywords = {Computer science; Feature extraction; Image enhancement; Image retrieval; Intelligent computing; Support vector machines; Taxonomies; Vectors; Classification accuracy; Data sets; Leaf images; Novel methods; Plant classification; Plant leaf; Plant leaf image feature extraction; Processing speed; Recognition rates; SVM classifiers; Time frequency domain; Wavelet transforms},
	correspondence_address = {J. Liu; Missile Institute, Air-Force Engineering University, Sanyuan 713800, China; email: ljdzxj@sohu.com},
	issn = {16113349},
	isbn = {3642040691; 978-364204069-6},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28; Conference name: 5th International Conference on Intelligent Computing, ICIC 2009; Conference date: 16 September 2009 through 19 September 2009; Conference code: 77838}
}

@CONFERENCE{Zhang2010v15521,
	author = {Zhang, Shanwen and Feng, YouQian},
	title = {Plant leaf classification using plant leaves based on rough set},
	year = {2010},
	journal = {ICCASM 2010 - 2010 International Conference on Computer Application and System Modeling, Proceedings},
	volume = {15},
	pages = {v15521–v15525},
	doi = {10.1109/ICCASM.2010.5622528},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649574840&doi=10.1109%2fICCASM.2010.5622528&partnerID=40&md5=2d60aa9eab773ed6d39d901b0a816e6c},
	affiliations = {Department of Engineering and Technology, Xijing University, Xi'an 710123, China; Science Institute, Air-Force Engineering University, Xi'an, 710051, China},
	abstract = {Plant classification using plant leaves has been an important and difficult task. The key question in plant classification is extraction features and reduction features. In this paper, a method of plant leaf classification is proposed based on the neighborhood rough set. We mainly show that the is applicable to plant leaf classification. Experimental results on plant leaf image database demonstrate that the proposed method is effective and feasible for leaf classification. © 2010 IEEE.},
	author_keywords = {Attribute reduction; Forward attribute reduction based on neighborhood model (FARNeM); Leaf classification; Neighborhood rough set},
	keywords = {Feature extraction; Rough set theory; Attribute reduction; Leaf classification; Neighborhood rough set; Plant classification; Plant leaf; Plant leaves; Rough set; Computer applications},
	correspondence_address = {S. Zhang; Department of Engineering and Technology, Xijing University, Xi'an 710123, China; email: zhangshanwen1965@163.com},
	isbn = {978-142447236-9},
	language = {English},
	abbrev_source_title = {ICCASM - Int. Conf. Comput. Appl. Syst. Model., Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 2010 International Conference on Computer Application and System Modeling, ICCASM 2010; Conference date: 22 October 2010 through 24 October 2010; Conference code: 82681}
}@CONFERENCE{Kirchoff2008825,
	author = {Kirchoff, Bruce and Remington, David and Fu, Lixin and Sadri, Fereidoon},
	title = {A new type of image-based key},
	year = {2008},
	journal = {BioMedical Engineering and Informatics: New Development and the Future - Proceedings of the 1st International Conference on BioMedical Engineering and Informatics, BMEI 2008},
	volume = {1},
	pages = {825 – 829},
	doi = {10.1109/BMEI.2008.301},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-51549108508&doi=10.1109%2fBMEI.2008.301&partnerID=40&md5=39b04dc649ae160eea3a03653fabcaa7},
	affiliations = {Department of Biology, UNC Greensboro, Greensboro, NC 27402-6170, United States; Department of Computer Science, UNC Greensboro, Greensboro, NC 27402-6170, United States},
	abstract = {Keys are character based tools for plant identification. They are based on the decomposition of the plant into very small, atomistic parts. These parts are described with the technical and often arcane terminology of plant taxonomy. Even the best electronic keys (Delta, Lucid) make use of this terminology. Keys are not based on pattern recognition, the forte of visual experts. Instead they demand that the user look at the plant as if it consisted of a series of isolated parts that are classified by name. Keys would be more effective if they were visually based. They would be easier to use for visual experts because accurate perception is their providence. They would also be easier to use for novices because they would not depend on knowledge of arcane terminology. This paper proposed an innovative image-based key system for species recognition.},
	keywords = {Biophysics; Computer networks; Feature extraction; Pattern recognition; Terminology; Image-based; Informatics; Keys (for locks)},
	correspondence_address = {B. Kirchoff; Department of Biology, UNC Greensboro, Greensboro, NC 27402-6170, United States; email: kirchoff@uncg.edu},
	isbn = {978-076953118-2},
	language = {English},
	abbrev_source_title = {Biomed. Eng. Informatics: New Dev. Future - Proc. Int. Conf. Biomed. Eng. Informatics, BMEI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: BioMedical Engineering and Informatics: New Development and the Future - 1st International Conference on BioMedical Engineering and Informatics, BMEI 2008; Conference date: 27 May 2008 through 30 May 2008; Conference code: 73399}
}

@CONFERENCE{Yang2000709,
	author = {Yang, Chun-Chieh and Prasher, Shiv O. and Landry, Jacques-André},
	title = {Applications of artificial neural networks to plant recognition in the field},
	year = {2000},
	journal = {2000 ASAE Annual Intenational Meeting, Technical Papers: Engineering Solutions for a New Century},
	volume = {1},
	pages = {709 – 723},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-2342616171&partnerID=40&md5=24b79021b02bcc97ef1dc3df370ea69e},
	affiliations = {Dept. of Agricultural Engineering, McGill University, Ste-Anne-de-Bellevue, Que., Canada},
	abstract = {The main objective of this project was to develop a weed recognition system to assist in the precision application of herbicides in corn fields. Digital images were collected in May, 1998 using a commercially-available digital camera. The intensities of the three primary colors, red, green and blue, were compared for each pixel of the images. The color of a pixel was left unchanged if the green intensity was greater than each of the other two; otherwise the three intensities were set to zero. Background objects were thus removed from the images. The resulting pixel intensities of the modified images were used as the inputs for artificial neural networks (ANNs). Learning Vector Quantization (LVQ) ANNs were used to distinguish corn from weeds, as well as to differentiate between weed species. The success rate for a single ANN in distinguishing a given weed species from corn was as high as 90%, and as high as 80% in distinguishing any of several weed species from corn. Better success rate might be obtainable with more elaborate schemes for data input, and/or structural improvements such as cascading. The image-processing time for the ANNs was as short as 0.48 s per image, thus making it useful for real-time data processing and application of herbicides. The development of such ANNs for weed recognition could be useful in precision farming, to guide site-specific herbicide application. Their use could reduce the total quantity of herbicide applied, thereby lowering costs and the risk of pollution.},
	author_keywords = {Artificial neural networks; Learning Vector Quantization; Precision farming; Site-specific herbicide application; Weed recognition},
	keywords = {Algorithms; Computational methods; Farms; Herbicides; Image processing; Learning systems; Problem solving; Project management; Soils; Vectors; Weed control; Learning vector quantization; Precision farming; Site-specific herbicide application; Weed recognition; Neural networks},
	language = {English},
	abbrev_source_title = {ASAEAnnu. Int. Meeting Tech. Pap. Eng. Solut. for a New Cent.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2000 ASAE Annual International Meeting, Technical Papers: Engineering Solutions for a New Century; Conference date: 9 July 2000 through 12 July 2000; Conference code: 62827}
}

@CONFERENCE{Zheru20031035,
	author = {Zheru, Chi and Li, Houqiang and Wang, Chao},
	title = {Plant species recognition based on bark patterns using novel Gabor filter banks},
	year = {2003},
	journal = {Proceedings of 2003 International Conference on Neural Networks and Signal Processing, ICNNSP'03},
	volume = {2},
	pages = {1035 – 1038},
	doi = {10.1109/ICNNSP.2003.1281045},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149287405&doi=10.1109%2fICNNSP.2003.1281045&partnerID=40&md5=70604a15db7ef96b0078a875ccb149f7},
	affiliations = {Centex for Multimedia Signal Processing, Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Dept. of EEIS, Univ. of Sci. and Tech. of China, Hefei, Anhui 230027, China},
	abstract = {This paper presents a novel style of Gabor filter banks designed for plant species recognition using their bark texture features. In this paper, texture is modeled as multiple narrowband signals that are characterized by their central frequencies and normalized ratios of amplitudes. The normalized ratio of amplitudes is employed as an energy weight for combining narrowband signals. Based on this texture model, a set of texture features can be extracted from each kind of plant bark that is used to characterize the plant and to design the corresponding Gabor filter bank. A classifier is constructed by these Gabor filter banks. Plant recognition experiments on a small database of bark images have been conducted and the effectiveness of our approach is confirmed by the experimental results. © 2003 IEEE.},
	keywords = {Feature extraction; Neural networks; Signal processing; Textures; Bark image; Central frequency; Gabor filter banks; Narrowband signal; Plant recognition; Plant species; Texture features; Texture models; Filter banks},
	correspondence_address = {C. Zheru; Centex for Multimedia Signal Processing, Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hong Kong, Hong Kong; email: enzheru@polyu.edu.hk},
	isbn = {0780377028; 978-078037702-8},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Neural Networks Signal Process., ICNNSP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27; Conference name: 2003 International Conference on Neural Networks and Signal Processing, ICNNSP'03; Conference date: 14 December 2003 through 17 December 2003; Conference code: 82534}
}

@CONFERENCE{Shan19962607,
	author = {Shan, Zhuofeng and Kim, Hung-man and Wang, Fei-Yue},
	title = {Plant identification and performance optimization for neuro-fuzzy networks},
	year = {1996},
	journal = {Proceedings of the IEEE International Conference on Systems, Man and Cybernetics},
	volume = {4},
	pages = {2607 – 2612},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030420347&partnerID=40&md5=0e3c3f6f1599a43409897b9b2b913a6a},
	affiliations = {Univ of Arizona, Tucson, United States},
	abstract = {This paper discusses the structures and learning algorithms for identification and optimization with neuro-fuzzy networks (NFN). NFN are knowledge-based multilayer neural networks constructed by integrating three types of modular subnets for pattern recognition, fuzzy reasoning, and control synthesis, respectively. In this way, a NFN combines the reasoning procedure of fuzzy logic and learning capability of neural networks uniquely, thus is able to incorporate linguistic knowledge in the form of fuzzy rules in its network structure and then refine this knowledge through training and self learning. Simulation results are presented here to illustrate these ideas.},
	keywords = {Artificial intelligence; Computational linguistics; Computer simulation; Control system synthesis; Fuzzy sets; Identification (control systems); Knowledge based systems; Learning algorithms; Learning systems; Optimization; Pattern recognition; Performance; Fuzzy reasoning; Fuzzy rules; Linguistic knowledge; Neuro-fuzzy networks; Neural networks},
	publisher = {IEEE},
	issn = {08843627},
	coden = {PICYE},
	language = {English},
	abbrev_source_title = {Proc IEEE Int Conf Syst Man Cybern},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: Proceedings of the 1996 IEEE International Conference on Systems, Man and Cybernetics. Part 4 (of 4); Conference date: 14 October 1996 through 17 October 1996; Conference code: 46016}
}

@CONFERENCE{Saitoh2000507,
	author = {Saitoh, Takeshi and Kaneko, Toyohisa},
	title = {Automatic recognition of wild flowers},
	year = {2000},
	journal = {Proceedings - International Conference on Pattern Recognition},
	volume = {15},
	number = {2},
	pages = {507 – 510},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0043098025&partnerID=40&md5=341df2186a008bd997895a4a95ae3389},
	affiliations = {Toyohashi University of Technology, Toyohashi 441-8580, 1-1 Hibarigaoka, Tempaku-cho, Japan},
	abstract = {This paper describes an automatic method for recognizing wild flowers. Recognition requires two pictures, a frontal flower image and a leaf image taken by a digital camera. Seventeen(17) features, eight from the flower and also nine from the leaf, are fed to a neural network. We collected 20 pairs of pictures from 16 wild flowers in the fields around our campus. We obtained a recognition rate of 95% with all the 17 features. Then we investigated which features are more effective for recognition and found that four features of flowers and two features of leaves can yield the best accuracy of 96%. © 2000 IEEE.},
	keywords = {Software engineering; Automatic method; Automatic recognition; Leaf images; Recognition rates; Pattern recognition},
	correspondence_address = {T. Saitoh; Toyohashi University of Technology, Toyohashi 441-8580, 1-1 Hibarigaoka, Tempaku-cho, Japan; email: saitoh@mmip.tutics.tut.ac.jp},
	issn = {10514651},
	coden = {PICRE},
	language = {English},
	abbrev_source_title = {Proc Int Conf Pattern Recognit},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 62}
}

@CONFERENCE{Samal1994183,
	author = {Samal, A. and Peterson, B. and Holliday, D.J.},
	title = {Recognizing plants using stochastic L-systems},
	year = {1994},
	journal = {Proceedings - International Conference on Image Processing, ICIP},
	volume = {1},
	pages = {183 – 187},
	doi = {10.1109/ICIP.1994.413300},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997216953&doi=10.1109%2fICIP.1994.413300&partnerID=40&md5=3922e56d94adef55eba4e79f9606e791},
	affiliations = {University of Nebraska-Lincoln, Department of Computer Science and Engineering, Lincoln, 68588-0115, NE, United States},
	abstract = {Recognizing naturally occurring objects has been a difficult task in computer vision. One of the keys to recognizing objects is the development of a suitable model. One type of model, the fractal, has been used successfully to model complex natural objects. A class of fractals, the L-system, has not only been used to model natural plants, but has also aided in their recognition. This research extends the work in plant recognition using L-systems in two ways. Stochastic L-systems are used to model and generate more realistic plants. Furthermore, to handle the complexity of recognition, a learning system is used that automatically generates a decision tree for classification. Results indicate that the approach used here has great potential as a method for recognition of natural objects. © 1994 IEEE.},
	keywords = {Computer vision; Decision trees; Fractals; Image processing; Image understanding; Stochastic systems; L-systems; Model complexes; Natural objects; Natural plants; Naturally occurring; Plant recognition; Stochastic L-system; Two ways; Stochastic models},
	publisher = {IEEE Computer Society},
	issn = {15224880},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Image Process. ICIP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: Proceedings of the 1994 1st IEEE International Conference on Image Processing. Part 3 (of 3); Conference date: 13 November 1994 through 16 November 1994; Conference code: 42570; All Open Access, Green Open Access}
}

@CONFERENCE{Qi2004990,
	author = {Qi, Hengnian and Yang, Jiangang and Lin, Jie},
	title = {Sawtooth identification and counting of leaf edge based on SVM},
	year = {2004},
	journal = {Proceedings of the International Conference on Artificial Intelligence, IC-AI'04},
	volume = {2},
	pages = {990 – 995},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744260609&partnerID=40&md5=4a22cac56a7cbfa6b847b237afd2038a},
	affiliations = {Institute of Artificial Intelligence, Zhejiang University, Hangzhou, China; School of Information Engineering, Zhejiang Forestry College, Lin'an, Zhejiang, China},
	abstract = {The focus of Computer-Aided Plant-Identification (CAPI) is the stable features extraction of plant. Sawtooth number of leaf edge is such an important feature for some species of plants. It is difficult to depict sawteeth shapes in rigid mathematic methods. However, a trained support vector machine (SVM) with good adaptability can be applied to classify sawtooth and nonsawtooth samples. According to the principle of SVM and the characteristics of the samples, a linear kernel is chosen here for the SVM. The samples can be obtained by a circular sample window sliding along the edge of the leaf, and then be rotated to a standard pose for decreasing the complexity of identification. By avoiding repeated sampling and counting of the same sawtooth, the algorithm presented in the paper accomplishes automatic counting of the sawtooth number. The results of the experiment show that the SVM-based method works well. For the equivalent of SVM and the three-layer feedforward neural network, three-layer BP net can also be used for the task of sawtooth identification. In addition, the elementary experiment shows that skeletonisation method is another good scheme for counting the number of sawtooth.},
	keywords = {Computational methods; Feature extraction; Feedforward neural networks; Plants (botany); Speech recognition; Vectors; Computer-aided plant-identification (CAPI); Leaf edge; Sawtooth identification machines (SIM); Support vector machines (SVM); Computer applications},
	correspondence_address = {H. Qi; Institute of Artificial Intelligence, Zhejiang University, Hangzhou, China; email: qihengnian@zju.edu.cn},
	isbn = {1932415335; 978-193241533-9},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Artif. Intell. IC AI},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: Proceedings of the International Conference on Artificial Intelligence, IC-AI'04; Conference date: 21 June 2004 through 24 June 2004; Conference code: 64231}
}

@CONFERENCE{Saitoh200427,
	author = {Saitoh, Takeshi and Aoki, Kimiya and Kaneko, Toyohisa},
	title = {Automatic recognition of blooming flowers},
	year = {2004},
	journal = {Proceedings - International Conference on Pattern Recognition},
	volume = {1},
	pages = {27 – 30},
	doi = {10.1109/ICPR.2004.1333997},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044299384&doi=10.1109%2fICPR.2004.1333997&partnerID=40&md5=04ac762ab35993b52b7a1f3250eb30ee},
	affiliations = {Toyohashi University of Technology, Toyohashi, 441-8580, 1-1 Hibarigaoka, Tempaku-cho, Japan; Department of Electrical Engineering, Tottori University, Tottori, 680-8552, 4-101 Koyama, Japan; SCCS, Chukyo University, Toyota-shi, Aichi, 470-0393, Japan},
	abstract = {This paper describes an automatic method for recognizing a blooming flower based on a photograph taken with a digital camera in natural scene. The problem of identifying an object against the background is known to be difficult. In this paper, we employ a photograph where the object (a blooming flower) is focused but the background is defocused. For extracting a flower region, we propose a new method that extracts a boundary by selecting a route with minimizing a sum of the local cost divided by the route length. Experiments were conducted for 600 pictures (20 pictures each for 30 species). A successful boundary extraction rate of 97% and a flower recognition rate of 90% were obtained.},
	keywords = {Automation; Boundary conditions; Color; Extraction; Pattern recognition; Photography; Extraction rates; Feature yielding; Interactive extraction; Recognition rate; Feature extraction},
	correspondence_address = {T. Saitoh; Department of Electrical Engineering, Tottori University, Tottori, 680-8552, 4-101 Koyama, Japan; email: saitoh@vcl.ics.tut.ac.jp},
	issn = {10514651},
	isbn = {0769521282},
	coden = {PICRE},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Pattern Recognit.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 79; Conference name: Proceedings of the 17th International Conference on Pattern Recognition, ICPR 2004; Conference date: 23 August 2004 through 26 August 2004; Conference code: 64011}
}

@ARTICLE{Takizawa2005630,
	author = {Takizawa, Hotaka and Ezaki, Nobuo and Mizuno, Shinji and Yamamoto, Shinji},
	title = {Plant Recognition by Integrating Color and Range Data Obtained Through Stereo Vision},
	year = {2005},
	journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
	volume = {9},
	number = {6},
	pages = {630 – 636},
	doi = {10.20965/jaciii.2005.p0630},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250699043&doi=10.20965%2fjaciii.2005.p0630&partnerID=40&md5=baa1c74d9f7f62113ec1e02d210d44bf},
	affiliations = {University of Tsukuba, Ibaraki, 305-8573, Japan; Toba National College of Maritime Technology, Mie, 517-8501, Japan; Toyohashi University of Technology, Aichi, 441-8580, Japan; Chukyo University, Aichi, 470-0393, Japan},
	abstract = {We present a new method for recognizing plants automatically and nondestructively by measuring plants three-dimensionally, including height, width, and leaf area, which are important clues for determining a plant condition. We use only two cameras and small plant preparation. A pair of color images for a plant is obtained by a binocular stereo camera, then leaf and stalk areas are extracted from color images, range data is calculated from stereo images, three-dimensional reconstruction for the plant is generated by using the Delaunay triangulation and the plant is measured in the generated reconstruction. Two experimental results are shown for actual plants. © Fuji Technology Press Ltd.},
	author_keywords = {color; integration; plant recognition; range data; stereo vision},
	keywords = {Cameras; Data integration; Stereo image processing; Stereo vision; Binocular stereo; Colour image; Condition; Delaunay triangulation; Leaf area; Plant recognition; Range data; Stereo cameras; Stereoimages; Three-dimensional reconstruction; Color},
	correspondence_address = {H. Takizawa; University of Tsukuba, Ibaraki, 305-8573, Japan; email: takizawa@parl.jp},
	publisher = {Fuji Technology Press},
	issn = {13430130},
	language = {English},
	abbrev_source_title = {J. Adv. Comput. Intell. Intelligent Informatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Shropshire1993220,
	author = {Shropshire, Geoffrey J. and Glas, Cees},
	title = {Spectral band selection for color machine vision used for plant identification},
	year = {1993},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {1836},
	pages = {220 – 230},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027284644&partnerID=40&md5=b6e92f2a32bef8e5333aeaa7a569c0a7},
	affiliations = {Univ. of Idaho, Moscow, ID, USA},
	abstract = {A monochrome video camera and a set of narrow bandpass optical filters were used to capture images in 13 spectral bands from 400 nm to 1000 nm. Images of 4 plant species were captured. Pixel data was extracted from the images in user defined areas on the plant leaves. Attempts were made to classify the plants based on the spectral information in the extracted data.},
	keywords = {Agricultural products; Color image processing; Optical filters; Plants; Spectral bands; Computer vision},
	publisher = {Publ by Int Soc for Optical Engineering},
	issn = {0277786X},
	isbn = {0819410373},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: Optics in Agriculture and Forestry; Conference date: 16 November 1992 through 17 November 1992; Conference code: 18611}
}

@CONFERENCE{Guyer1984,
	author = {Guyer, D.E. and Miles, G.E. and Schreiber, M.M.},
	title = {COMPUTER VISION AND IMAGE PROCESSING FOR PLANT IDENTIFICATION.},
	year = {1984},
	journal = {Paper - American Society of Agricultural Engineers},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021570619&partnerID=40&md5=5e327394ce900620e25de50f21fe9877},
	affiliations = {Purdue Univ, Dep of Agricultural, Engineering, West Lafayette, IN, USA, Purdue Univ, Dep of Agricultural Engineering, West Lafayette, IN, USA},
	abstract = {A set of crop and weed species were experimented with to determine the potential of computer vision and image processing in agricultural applications. Plants were segmented from a soil background and spatial features extracted from the plant images. The spatial feature values provided information for successfully discerning between plant species.},
	keywords = {IMAGE PROCESSING - Image Analysis; VISION - Computer Applications; COMPUTER VISION; CROP SPECIES; EXPERIMENTAL STUDY; PLANT IDENTIFICATION; WEED SPECIES; BIOMASS},
	publisher = {ASAE},
	issn = {01450166},
	coden = {AAEPC},
	language = {English},
	abbrev_source_title = {Paper - American Society of Agricultural Engineers},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; Conference name: 1984 Winter Meeting - American Society of Agricultural Engineers: Engineering the Future - Capitalizing on the New Technologies.; Conference date: ; Conference code: 7199}
}

@ARTICLE{Holliday199533,
	author = {Holliday, David J. and Samal, Ashok},
	title = {Object recognition using L-system fractals},
	year = {1995},
	journal = {Pattern Recognition Letters},
	volume = {16},
	number = {1},
	pages = {33 – 42},
	doi = {10.1016/0167-8655(94)00076-F},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029207589&doi=10.1016%2f0167-8655%2894%2900076-F&partnerID=40&md5=dfb0e9b5d0013ef4850348cf1e497436},
	affiliations = {Department of Computer Science and Engineering, University of Nebraska -Lincoln, Lincoln, NE 68588-0115, United States},
	abstract = {Fractals have been successfully used for modeling many classes of natural objects. We use a class of fractals, the L-system, to model and to recognize plants and trees. Results indicate that this approach has great potential for recognizing natural objects. © 1995.},
	author_keywords = {Fractals; L-system; Pattern recognition; Plants and trees},
	keywords = {Fractals; Mathematical models; Pattern recognition; L system fractals; Natural object recognition; Plant recognition; Tree recognition; Object recognition},
	correspondence_address = {A. Samal; Department of Computer Science and Engineering, University of Nebraska -Lincoln, Lincoln, NE 68588-0115, United States; email: samal@cse.unl.edu},
	issn = {01678655},
	language = {English},
	abbrev_source_title = {Pattern Recogn. Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@CONFERENCE{Ishak2007,
	author = {Ishak, Asnor Juraiza and Mokri, Siti Salasiah and Mustafa, Mohd Marzuki and Hussain, Aini},
	title = {Weed detection utilizing quadratic polynomial and ROI techniques},
	year = {2007},
	journal = {2007 5th Student Conference on Research and Development, SCORED},
	doi = {10.1109/SCORED.2007.4451360},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-50449093453&doi=10.1109%2fSCORED.2007.4451360&partnerID=40&md5=f2dc0c12eb4fc534b896d43bc7dfb97a},
	affiliations = {IEEE; Department of Electrical and Electronic Eng., Faculty of Engineering, Universiti Putra Malaysia, 43300 Serdang, Selangor, Malaysia; Department of Electrical, Electronic and System Eng., Faculty of Engineering, Universiti Kembangsaan Malaysia, 43600 Bangi, Selangor, Malaysia},
	abstract = {Machine vision for selective weeding or selective herbicide spraying relies substantially on the ability of the system to analyze weed images and process the extracted knowledge for decision making prior to implementing the identified control action. To control weed, different weed type would require different herbicide formulation. Consequently the weed must be identified and classified accordingly. In this work, weed images were classified as either broad or narrow weed type. A fundamental problem in weed image recognition using planar curve analysis is to detect curve. It is difficult to successfully extract curve from the image of weed edges since the appropriate scale to use for extraction is not known a priori As such, this paper considers a curve detection method based on the quadratic polynomial technique which include the use of the region-of-interests (ROI) technique. The ROI technique creates image subsets by selecting regions of the displayed image. The ROIs are typically used to extract statistics for image operations such as classification. As such, the objective of this paper is to present a novel application of curve detection feature extraction technique in weed classification. ©2007 IEEE.},
	author_keywords = {Feature extraction; Neural network; Pattern recognition; Plant identification; Region of interest},
	keywords = {Classification (of information); Computer networks; Computer vision; Curve fitting; Decision making; Feature extraction; Herbicides; Image enhancement; Image recognition; Polynomial approximation; Polynomials; Problem solving; Quadratic programming; Research and development management; Neural network; Pattern recognition; Plant identification; Region of interest; Research and development; Pest control},
	correspondence_address = {A. J. Ishak; Department of Electrical and Electronic Eng., Faculty of Engineering, Universiti Putra Malaysia, 43300 Serdang, Selangor, Malaysia; email: asnorji@eng.upm.edu.my},
	isbn = {1424414709; 978-142441470-3},
	language = {English},
	abbrev_source_title = {Stud. Conf. Res. Dev., SCORED},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; Conference name: 2007 5th Student Conference on Research and Development, SCORED; Conference date: 11 December 2007 through 12 December 2007; Conference code: 73256; All Open Access, Green Open Access}
}

@ARTICLE{Meyer199713,
	author = {Meyer, George E. and Franti, Thomas G. and Mortensen, David A.},
	title = {Seek and destroy},
	year = {1997},
	journal = {Resource: Engineering and Technology for Sustainable World},
	volume = {4},
	number = {12},
	pages = {13 – 14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031331712&partnerID=40&md5=63122a8cdc3a5c8b1aa98e111da1d0e3},
	affiliations = {Univ of Nebraska, Lincoln, United States},
	abstract = {An intelligent machine vision using new technologies could improve weed or crop plant identification and provide a solution for chemical application to control environmental safety during site specific crop management (SSCM). It is a valuable scientific research tool to evaluate simpler optical sensor system performance, map and discriminate weeds and investigate their ecology. It aims to imitate human perception for complex plant analyses.},
	keywords = {Accident prevention; Artificial intelligence; Computational methods; Crops; Environmental protection; Geographic information systems; Herbicides; Image analysis; Image understanding; Spraying; Water quality; Weed control; Site specific crop management (SSCM); Spot spraying; Texture feature analysis; Computer vision},
	publisher = {ASAE},
	issn = {10763333},
	coden = {RSOUE},
	language = {English},
	abbrev_source_title = {Resour Eng Technol Sustainable World},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Zou20072335,
	author = {Zou, Jie and Nagy, George},
	title = {Visible models for interactive pattern recognition},
	year = {2007},
	journal = {Pattern Recognition Letters},
	volume = {28},
	number = {16},
	pages = {2335 – 2342},
	doi = {10.1016/j.patrec.2007.08.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048855526&doi=10.1016%2fj.patrec.2007.08.005&partnerID=40&md5=043788e1a34a2da8f0f04170fad5051a},
	affiliations = {National Library of Medicine, Bethesda, MD 20894, 8600 Rockville Pike, United States; Department of Electrical, Computer and Systems Engineering, DocLab, Rensselaer Polytechnic Institute, Troy, NY 12180, JEC 6020, 110 8th street, United States},
	abstract = {The exchange of information between human and machine has been a bottleneck in interactive visual classification. The visible model of an object to be recognized is an abstraction of the object superimposed on its picture. It is constructed by the machine but it can be modified by the operator. The model guides the extraction of features from the picture. The classes are rank ordered according to the similarities (in the hidden high-dimensional feature space) between the unknown picture and a set of labeled reference pictures. The operator can either accept one of the top three candidates by clicking on a displayed reference picture, or modify the model. Model adjustment results in the extraction of new features, and a new rank ordering. The model and feature extraction parameters are re-estimated after each classified object, with its model and label, is added to the reference database. Pilot experiments show that interactive recognition of flowers and faces is more accurate than automated classification, faster than unaided human classification, and that both machine and human performance improve with use. © 2007 Elsevier B.V. All rights reserved.},
	author_keywords = {Face recognition; Flower recognition; Human-computer interaction; Pattern recognition; Visible model},
	keywords = {Face recognition; Human computer interaction; Image classification; Information retrieval; Mathematical models; Flower recognition; Visual classification; Feature extraction},
	correspondence_address = {G. Nagy; Department of Electrical, Computer and Systems Engineering, DocLab, Rensselaer Polytechnic Institute, Troy, NY 12180, JEC 6020, 110 8th street, United States; email: nagy@ecse.rpi.edu},
	issn = {01678655},
	language = {English},
	abbrev_source_title = {Pattern Recogn. Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12}
}

@ARTICLE{Yovel2008,
	author = {Yovel, Yossi and Franz, Matthias Otto and Stilz, Peter and Schnitzler, Hans-Ulrich},
	title = {Plant classification from bat-like echolocation signals},
	year = {2008},
	journal = {PLoS Computational Biology},
	volume = {4},
	number = {3},
	doi = {10.1371/journal.pcbi.1000032},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-41849148607&doi=10.1371%2fjournal.pcbi.1000032&partnerID=40&md5=78b39fd707d00f286a38a2eb04134d0b},
	affiliations = {Animal Physiology, Zoological Institute, University of Tuebingen, Tuebingen, Germany; Max-Planck-Institute for Biological Cybernetics, Tuebingen, Germany; University of Applied Sciences, Konstanz, Germany},
	abstract = {Classification of plants according to their echoes is an elementary component of bat behavior that plays an important role in spatial orientation and food acquisition. Vegetation echoes are, however, highly complex stochastic signals: from an acoustical point of view, a plant can be thought of as a three-dimensional array of leaves reflecting the emitted bat call. The received echo is therefore a superposition of many reflections. In this work we suggest that the classification of these echoes might not be such a troublesome routine for bats as formerly thought. We present a rather simple approach to classifying signals from a large database of plant echoes that were created by ensonifying plants with a frequency-modulated bat-like ultrasonic pulse. Our algorithm uses the spectrogram of a single echo from which it only uses features that are undoubtedly accessible to bats. We used a standard machine learning algorithm (SVM) to automatically extract suitable linear combinations of time and frequency cues from the spectrograms such that classification with high accuracy is enabled. This demonstrates that ultrasonic echoes are highly informative about the species membership of an ensonified plant, and that this information can be extracted with rather simple, biologically plausible analysis. Thus, our findings provide a new explanatory basis for the poorly understood observed abilities of bats in classifying vegetation and other complex objects. © 2008 Yovel et al.},
	keywords = {Algorithms; Animals; Artificial Intelligence; Chiroptera; Echolocation; Pattern Recognition, Automated; Plant Physiology; Plants; Sound Spectrography; Learning algorithms; Learning systems; Spectrographs; Stochastic systems; Vegetation; Frequency modulated; Large database; Plant classification; Simple approach; Spatial orientations; Spectrograms; Standard machines; Stochastic signals; Three dimensional arrays; Ultrasonic pulse; acoustics; algorithm; animal behavior; article; bat; echolocation; machine learning; mathematical model; nonhuman; plant; sound detection; ultrasound; vegetation; animal; artificial intelligence; automated pattern recognition; classification; methodology; physiology; plant physiology; sound detection; Classification (of information)},
	correspondence_address = {Y. Yovel; Animal Physiology, Zoological Institute, University of Tuebingen, Tuebingen, Germany; email: yossiyovel@hotmail.com},
	publisher = {Public Library of Science},
	issn = {1553734X},
	pmid = {18369425},
	language = {English},
	abbrev_source_title = {PLoS Comput. Biol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 67; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Udantha1997258,
	author = {Udantha, Madhuka G.P.D. and Jayamaha, Ayola D.N.},
	title = {Leaf recognition and classification algorithm to be used by indigenous medicine},
	year = {1997},
	journal = {2014 14th International Conference on Advances in ICT for Emerging Regions, ICTer 2014 - Conference Proceedings},
	pages = {258},
	doi = {10.1109/ICTER.2014.7083916},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949924594&doi=10.1109%2fICTER.2014.7083916&partnerID=40&md5=346574e80e72f5c1e0d495250292aadd},
	affiliations = {Department of Computer Science and Engineering, Faculty of Engineering, University of Moratuwa, Sri Lanka},
	abstract = {In Sri Lanka there is a lot of emphasis on herbal and indigenous medicine. Majority prefer Ayurvedic medicine to Western medicine due to various factors such as less or rather no side effects. The fact that Ayurvedic medical system stood the test of time sums up to its worth as an alternative course of treatment for various ailments. However, a major drawback in the practice of indigenous medicine is the difficulty in finding ingredients. The literature cites efforts that have been made to come up with techniques to help this situation. Eg: Prof. H.M.D.R. Herath Social Science Department Head, University of Peradeniya had created an alphabetically ordered list of all trees and plants of Sri Lanka. © 2014 IEEE.},
	author_keywords = {image processing; indigenous medicine; neural networking},
	keywords = {Image processing; Ayurvedic medicine; Classification algorithm; Leaf recognition; Medical systems; neural networking; Side effect; Sri Lanka; Western medicines; Algorithms},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	isbn = {978-147997732-1},
	language = {English},
	abbrev_source_title = {Int. Conf. Adv. ICT Emerg. Reg., ICTer - Conf. Proc.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2014 14th International Conference on Advances in ICT for Emerging Regions, ICTer 2014; Conference date: 10 December 2014 through 13 December 2014; Conference code: 111925}
}

@ARTICLE{Fu2006881,
	author = {Fu, H. and Chi, Z.},
	title = {Combined thresholding and neural network approach for vein pattern extraction from leaf images},
	year = {2006},
	journal = {IEE Proceedings: Vision, Image and Signal Processing},
	volume = {153},
	number = {6},
	pages = {881 – 892},
	doi = {10.1049/ip-vis:20060061},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751550214&doi=10.1049%2fip-vis%3a20060061&partnerID=40&md5=13e4ebc961ee8581ddc1634ad747d47b},
	affiliations = {Hong Kong Polytechnic University, Center for Multimedia Signal Processing, Department of Electronic and Information Engineering, Hung Hom, Hong Kong},
	abstract = {Living plant recognition based on images of leaf, flower and fruit is a very challenging task in the field of pattern recognition and computer vision. There has been little work reported on flower and fruit image processing and recognition. In recent years, several researchers have dedicated their work to leaf characterisation. As an inherent trait, leaf vein definitely contains the important information for plant species recognition despite its complex modality. A new approach that combines a thresholding method and an artificial neural network (ANN) classifier is proposed to extract leaf veins. A preliminary segmentation based on the intensity histogram of leaf images is first carried out to coarsely determine vein regions. This is followed by a fine segmentation using a trained ANN classifier with ten features extracted from a window centred on the object pixel as its inputs. Compared with other methods, experimental results show that this combined approach is capable of extracting more accurate venation modality of the leaf for the subsequent vein pattern classification. The approach can also reduce the computing time compared with a direct neural network approach. © 2006 The Institution of Engineering and Technology.},
	keywords = {Computer vision; Image processing; Image segmentation; Neural networks; Pattern recognition systems; Plants (botany); Complex modality; Inherent traits; Intensity histogram; Plant recognition; Feature extraction},
	correspondence_address = {H. Fu; Hong Kong Polytechnic University, Center for Multimedia Signal Processing, Department of Electronic and Information Engineering, Hung Hom, Hong Kong; email: enhongfu@eie.polyu.edu.hk},
	publisher = {Institution of Engineering and Technology},
	issn = {1350245X},
	coden = {IVIPE},
	language = {English},
	abbrev_source_title = {IEE Proc Vision Image Signal Proc},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 70}
}

@CONFERENCE{Søgaard2005613,
	author = {Søgaard, H.T. and Lund, I.},
	title = {Investigation of the accuracy of a machine vision based robotic micro-spray system},
	year = {2005},
	journal = {Precision Agriculture 2005, ECPA 2005},
	pages = {613 – 619},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957772452&partnerID=40&md5=5ab6590f19fbc8d2d771cdb07490a3fe},
	affiliations = {Danish Institute of Agricultural Sciences, Dept. of Agricultural Engineering, Research Centre Bygholm, DK-8700 Horsens, Schüttesvej 17, Denmark},
	abstract = {Experiments with a new concept for precise application of herbicides in the seed line have been conducted. The concept combines plant recognition, micro-spraying and autonomous robotics. A machine vision system recognizes objects to be sprayed, and a micro-spray system targets very small doses of liquid at the detected objects, while the RTK-GPS controlled autonomous vehicle takes care of the navigation in the field. The experiments were carried out under controlled indoor conditions, and the results show that the spray liquid was applied at subcentimetre accuracy. However, the investigations also showed that potentials for even higher accuracy exist.},
	author_keywords = {Centimetre accuracy; Herbicide application; Micro dosing; Object recognition},
	keywords = {Agriculture; Experiments; Herbicides; Liquids; Object recognition; Robotics; Robots; Weed control; Autonomous robotics; Autonomous Vehicles; Centimetre accuracy; Herbicide application; Indoor conditions; Machine vision systems; Microdosing; Plant recognition; Computer vision},
	isbn = {978-907699869-5},
	language = {English},
	abbrev_source_title = {Precis. Agric., ECPA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: 5th European Conference on Precision Agriculture, ECPA 2005; Conference date: 9 June 2005 through 12 June 2005; Conference code: 102118}
}

@CONFERENCE{Zou200576,
	author = {Zou, Jie and Gattani, Abhishek},
	title = {Computer-assisted visual inter active recognition and its prospects of implementation over the internet},
	year = {2005},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {5670},
	pages = {76 – 87},
	doi = {10.1117/12.585112},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-21944434433&doi=10.1117%2f12.585112&partnerID=40&md5=5c7ccbff8798fe03450349ec9306537d},
	affiliations = {Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY, 110 8th Street, United States},
	abstract = {The methodology of interactive visual recognition for human-centered low-throughput applications and the prospects of implementing computer assisted visual interactive recognition (CAVIAR) was discussed. A mobile CAVIAR system was implemented, where a pocket PC as a client, connects to a server through wireless communication. A parameterized geometric model to mediate communication between human and computer for interactive visual pattern recognition was proposed. It was shown that large training sets improve accuracy irrespective of the classification and CAVIAR implemented over the internet fits perfectly within the open mind framework as it provides a means to collect data from non-expert user over the internet.},
	keywords = {Bandwidth; Computer networks; Computer programming; Human computer interaction; Image retrieval; Image segmentation; Internet; Learning systems; Mathematical models; Flower recognition; Image based interaction; Interactive segmentation; Mobile; Open mind; Shape model; Visual pattern recognition; Pattern recognition},
	correspondence_address = {J. Zou; Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY, 110 8th Street, United States; email: zouj@rpi.edu},
	issn = {0277786X},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: Proceedings of SPIE-IS and T Electronic Imaging - Internet Imaging VI; Conference date: 18 January 2005 through 20 January 2005; Conference code: 65157}
}

@ARTICLE{Hemming2001233,
	author = {Hemming, J. and Rath, T.},
	title = {Computer-vision-based weed identification under field conditions using controlled lighting},
	year = {2001},
	journal = {Journal of Agricultural and Engineering Research},
	volume = {78},
	number = {3},
	pages = {233 – 243},
	doi = {10.1006/jaer.2000.0639},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002811038&doi=10.1006%2fjaer.2000.0639&partnerID=40&md5=11eb6f8f661ae8a2b94ddcf758f67b99},
	affiliations = {Inst. of Agric. and Environ. Eng., Wageningen, Netherlands; Inst. for Hort. and Agric. Eng., University of Hanover, Germany},
	abstract = {The methods of digital image analysis were used to develop an identification system for weeds in crops. Two vegetable crops (cabbage and carrots) and a number of naturally occurring weed species were used to develop the classification algorithms. Considering the rougher environment, special attention was given to the open-field experiments. The images were obtained with a device that provided controlled lighting conditions. The analysis was carried out off-line. Eight different morphological features and three colour features were calculated for each single object to build a joint feature space. On the basis of sample data sets of each class, statistics were carried out to determine the features, which are suitable for discrimination. A membership function based on a fuzzy logic approach was formed and used for the classification. The experiments showed that colour features can help to increase the classification accuracy. Moreover, colour was used successfully for the segmentation procedure of plants and soil. Depending on growth stage, weed density and method of calculation between 51 and 95% of the plants were classified correctly. Problems still exists by separating and allocating single plants in plant stands where the plants have grown together. Compared to other studies the plant identification system presented is an improvement, especially considering that the experiments were carried out under field conditions. © 2001 Silsoe Research Institute.},
	correspondence_address = {J. Hemming; Inst. of Agric. and Environ. Eng., Wageningen, Netherlands; email: j.hemming@imag.wag-ur.nl},
	publisher = {Academic Press},
	issn = {00218634},
	coden = {JAERA},
	language = {English},
	abbrev_source_title = {J. Agric. Eng. Res.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 197}
}

@ARTICLE{Hemming20021,
	author = {Hemming, Jochen and Rath, Thomas},
	title = {Image processing for plant determination using the Hough transform and clustering methods},
	year = {2002},
	journal = {Gartenbauwissenschaft},
	volume = {67},
	number = {1},
	pages = {1 – 10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036216533&partnerID=40&md5=c6742e4335f30d04e0ae44dad74194a4},
	affiliations = {IMAG-Inst. of Agric./Envtl. Engg., NL-6700 AA Wageningen, P.O. Box 43, Netherlands},
	abstract = {One important goal of future crop production is to reduce the usage of herbicides. In many circumstances automatic detection and determination of weeds and crops is necessary for feasible weed control. Previous approaches in the area of computer vision based plant classification in a natural environment have only limited success, especially under variable field conditions with overlapping plants. The objective of this study was to apply a technique to cluster objects classified by a computer vision system in order to recognise not only single leaves but also whole plants. Images of the crop were obtained using a device that blocked out the natural light to provide controlled artificial lighting conditions. The crop rows were calculated from the determined plant positions. Plants which were not located in the row were labelled as weeds. It has been investigated, whether information on the row position can be used to reduce the classification errors. With this approach plant classification under field conditions can be improved and a plant classification accuracy of over 90% for cabbage and 70% for carrots has been obtained. The results clearly depended on the growing stage of the crop, on specific parameters of the row identification process and on the question whether the type 1 error (not identified crops) or the type 2 error (weed classified as crop) should be minimised.},
	keywords = {Brassica oleracea var. capitata; Daucus carota},
	correspondence_address = {J. Hemming; IMAG-Inst. of Agric./Envtl. Engg., NL-6700 AA Wageningen, P.O. Box 43, Netherlands; email: j.hemming@imag.wag-ur.nl},
	issn = {0016478X},
	coden = {GTBWA},
	language = {English},
	abbrev_source_title = {Gartenbauwissenschaft},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@CONFERENCE{Zou2004311,
	author = {Zou, Jie and Nagy, George},
	title = {Evaluation of model-based interactive flower recognition},
	year = {2004},
	journal = {Proceedings - International Conference on Pattern Recognition},
	volume = {2},
	pages = {311 – 314},
	doi = {10.1109/ICPR.2004.1334185},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044256378&doi=10.1109%2fICPR.2004.1334185&partnerID=40&md5=e990640484bc93acf0714eb02978db80},
	affiliations = {Rensselaer Polytechnic Institute, DocLab., ECSE Department, Troy, NY 12180, 110 8th ST, United States},
	abstract = {We introduce the concept of Computer Assisted Visual InterActive Recognition (CAVIAR). In CAVIAR, a parameterized geometrical model serves as the human-computer communication channel. We implemented a flower recognition system and evaluated it on 30 inexperienced subjects. Major conclusions include: 1) the accuracy of the CAVIAR system is much higher than that of the machine alone; 2) its recognition time is much lower than that of the human alone; 3) it can be initialized with as few as one training sample per class and still achieve high accuracy; 4) it demonstrates a self-learning ability, which suggests that instead of initializing the CAVIAR system with many training samples, we can trust the system's self-learning ability.},
	keywords = {Cameras; Database systems; Digital devices; Human computer interaction; Image analysis; Image retrieval; Image segmentation; Mathematical models; Automatic recognition; Computer Assisted Visual InterActive Recognition (CAVIAR); Interactive flower recognition; Pattern recognition},
	correspondence_address = {J. Zou; Rensselaer Polytechnic Institute, DocLab., ECSE Department, Troy, NY 12180, 110 8th ST, United States; email: zouj@rpi.edu},
	issn = {10514651},
	isbn = {0769521282},
	coden = {PICRE},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Pattern Recognit.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 44; Conference name: Proceedings of the 17th International Conference on Pattern Recognition, ICPR 2004; Conference date: 23 August 2004 through 26 August 2004; Conference code: 64011}
}

@CONFERENCE{Komi20072039,
	author = {Komi, Pauli J. and Jackson, Mike R. and Parkin, Rob M.},
	title = {Plant classification combining colour and spectral cameras for weed control purposes},
	year = {2007},
	journal = {IEEE International Symposium on Industrial Electronics},
	pages = {2039 – 2042},
	doi = {10.1109/ISIE.2007.4374921},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-50049118333&doi=10.1109%2fISIE.2007.4374921&partnerID=40&md5=c6b102843ab638ad3aebefa823691eff},
	affiliations = {Mechatronics Research Centre, Loughborough University, Loughborough, United Kingdom},
	abstract = {Weed plant detection and classification is a difficult task for any computer vision system. Previous studies show promising results with either colour camera or spectral imaging solutions. However, typical colour camera solutions have found it hard to deal with overlapping leaves, and spectral solutions often lack in the spatial resolution required for accurate leaf level detection. In this paper a novel system for weed detection and classification is presented using both low-cost RGB (Red, Green, Blue) colour and spectral (400 - 1000 nm) cameras combining the strengths of these individual technologies. The system presented performs accurate leaf level classification and is capable of identification at 97.6% with non-overlapping full leaves in laboratory under controlled lighting conditions. Plant leaf samples from 6 different plant types were used. With dedicated hardware and optimized software the system should be capable of at least 5 km/h real-time operation in field conditions. ©2007 IEEE.},
	keywords = {Agricultural engineering; Artificial intelligence; Cameras; Classification (of information); Color; Computer vision; Electronics industry; Image processing; Industrial electronics; Optical properties; Pest control; Technical presentations; Visual communication; Computer-vision systems; Control purposes; Dedicated hardware; In-field; International symposium; Leaf level; Lighting conditions; Plant classification; Plant types; Real-time operations; Spatial resolution SR); Spectral cameras; Spectral imaging; Spectral solutions; Weed detection; Weed control},
	correspondence_address = {P. J. Komi; Mechatronics Research Centre, Loughborough University, Loughborough, United Kingdom; email: p.j.komi@lboro.ac.uk},
	isbn = {1424407559; 978-142440755-2},
	coden = {85PTA},
	language = {English},
	abbrev_source_title = {IEEE Int Symp Ind Electron},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; Conference name: 2007 IEEE International Symposium on Industrial Electronics, ISIE 2007; Conference date: 4 June 2007 through 7 June 2007; Conference code: 72681; All Open Access, Green Open Access}
}

@ARTICLE{Okamoto200731,
	author = {Okamoto, Hiroshi and Murata, Tetsuro and Kataoka, Takashi and Hata, Shun-Ichi},
	title = {Plant classification for weed detection using hyperspectral imaging with wavelet analysis: Research paper},
	year = {2007},
	journal = {Weed Biology and Management},
	volume = {7},
	number = {1},
	pages = {31 – 37},
	doi = {10.1111/j.1445-6664.2006.00234.x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847141943&doi=10.1111%2fj.1445-6664.2006.00234.x&partnerID=40&md5=2cf8547a1cd78c12c31f7f04e4a81208},
	affiliations = {Research Faculty of Agriculture, Hokkaido University, Sapporo, Japan; Research Faculty of Agriculture, Hokkaido University, Kita-ku, Sapporo 060-8589, Kita-9, Nishi-9, Japan},
	abstract = {The goal of this study is to develop a new weed detection method that can be applied for automatic mechanical weed control. For successful weed detection, plants must be classified into crops and weeds according to their species. In this study, we employed a portable hyperspectral imaging system. The hyperspectral camera can capture landscape images that include crops, weeds, and the soil surface, and can provide more extensive information than conventional red, green, and blue (RGB) images. Although RGB images consist of red, green, and blue wavebands, the obtained hyperspectral images consist of 240 wavebands of spectral information. Hyperspectral imaging is expected to provide powerful technology for agricultural sensing. In the initial step of this study, the image pixels of the plants (crop or weeds) were segmented from the background soil surface using Euclidean distance as the discriminant function. In the next step, the image pixels of the crop (sugarbeet) and weeds (four species) were classified using the difference in the spectral characteristics of the plant species. In this process, classification variables were generated using wavelet transformation for data compression, noise reduction, and feature extraction, and then stepwise linear discriminant analysis was applied. The validation results indicate that the developed classification method has potential for practical use. © 2007 The Authors; Journal compilation © 2007 Weed Science Society of Japan.},
	author_keywords = {Discriminant analysis; Image processing; Machine vision; Weed control},
	correspondence_address = {H. Okamoto; Research Faculty of Agriculture, Hokkaido University, Kita-ku, Sapporo 060-8589, Kita-9, Nishi-9, Japan; email: hiro@bpe.agr.hokudai.ac.jp},
	issn = {14456664},
	coden = {WBMEA},
	language = {English},
	abbrev_source_title = {Weed Biol. Manage.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 80}
}

@CONFERENCE{Wu200711,
	author = {Wu, Stephen Gang and Bao, Forrest Sheng and Xu, Eric You and Wang, Yu-Xuan and Chang, Yi-Fan and Xiang, Qiao-Liang},
	title = {A leaf recognition algorithm for plant classification using probabilistic neural network},
	year = {2007},
	journal = {ISSPIT 2007 - 2007 IEEE International Symposium on Signal Processing and Information Technology},
	pages = {11 – 16},
	doi = {10.1109/ISSPIT.2007.4458016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-71549143224&doi=10.1109%2fISSPIT.2007.4458016&partnerID=40&md5=1c4cb31436e58d3b972461ab3722a1f9},
	affiliations = {Institute of Applied Chemistry, Chinese Academy of Science, China; Dept. of Computer Science, Texas Tech University, United States; Dept. of Computer Science and Engineering, Washington University, St. Louis, WA, United States; School of Information and Telecommunications Eng., Nanjing Univ. of P and T, China; Dept. of Electronic Engineering, National Taiwan Univ. of Science and Technology, Taiwan},
	abstract = {In this paper, we employ Probabilistic Neural Network (PNN) with image and data processing techniques to implement a general purpose automated leaf recognition for plant classification. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the PNN. The PNN is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. Compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation. ©2007 IEEE.},
	author_keywords = {Feature extraction; Leaf recognition; Plant classification; Probabilstic neural network},
	keywords = {Data processing; Feature extraction; Information technology; Signal processing; Data processing techniques; General purpose; Input vector; Leaf recognition; Plant classification; Principal Variables; Probabilistic neural networks; Probabilstic neural network; Neural networks},
	correspondence_address = {F. S. Bao; Dept. of Computer Science, Texas Tech University, United States; email: shengbao@ieee.org},
	isbn = {978-142441835-0},
	language = {English},
	abbrev_source_title = {ISSPIT - IEEE Int. Symp. Signal Process. Inf. Technol.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 661; Conference name: ISSPIT 2007 - 2007 IEEE International Symposium on Signal Processing and Information Technology; Conference date: 15 December 2007 through 18 December 2007; Conference code: 78853; All Open Access, Green Open Access}
}

@ARTICLE{Grzywna2006113,
	author = {Grzywna, Zbigniew J. and Borys, Przemysław and Dudek, Gabriela},
	title = {On the mathematical reconstruction of two dimensional plants},
	year = {2006},
	journal = {Bioscience Reports},
	volume = {26},
	number = {2},
	pages = {113 – 129},
	doi = {10.1007/s10540-006-9011-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745448020&doi=10.1007%2fs10540-006-9011-2&partnerID=40&md5=136c6897e540f8ffaa8aef67879c12ae},
	affiliations = {Section of Physics and Applied Mathematics, Faculty of Chemistry, Silesian University of Technology, 44-100 Gliwice, Ks. M. Strzody 9, Poland},
	abstract = {A set of 10, chosen medicinal plants (some of them with a reputation as remedies for tuberculosis) has been investigated through Partitioned Iterated Function Systems-Semi Fractals with Angle (PIFS-SFA) coding, Lempel, Ziv, Welch with quantization and noise (LZW-QN) compression, and surface density statistics (f(α)-SDS) discrimination techniques. The final outcomes of this quantitative analysis were, firstly: the linear ordering of the plants in question accompanied by the hope that it reflects their medical significance, secondly: the mathematical representation of each of the plants, and thirdly: the impressive compression achieved, leading to remarkable computer memory saving, and still permitting successful pattern recognition i.e., proper identification of the plant from the compressed image. © Springer Science+Business Media, Inc. 2006.},
	author_keywords = {Medicinal plants; Self similarity; Tuberculosis},
	keywords = {Algorithms; Computer Storage Devices; Data Compression; Fractals; Image Processing, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated; Plants, Medicinal; acid; allantoin; ascorbic acid; aukubine; carboxylic acid; choline; essential oil; fat; flavonoid; glechomina; glycoside; menthol; menthone; mineral salt; natural product; pectin; saponin; silicic acid; silicon dioxide; tannin; unclassified drug; article; Centraria islandica; computer memory; fractal analysis; glechoma hederacea; imaging; mathematical model; medicinal plant; Mentha; mentha arvensis; Mentha piperita; Meum mutellina; noise reduction; nonhuman; pattern recognition; plant identification; Plantago; plantago lanceolata; plantago major; Polypodium; polypodium vulgare; Pulmonaria; pulmonaria officinalis; quantitative analysis; statistical analysis; surface density statistics; tuberculosis; verbascum thapsus},
	correspondence_address = {Z.J. Grzywna; Section of Physics and Applied Mathematics, Faculty of Chemistry, Silesian University of Technology, 44-100 Gliwice, Ks. M. Strzody 9, Poland; email: zbigniew.grzywna@polsl.pl},
	issn = {01448463},
	coden = {BRPTD},
	pmid = {16763763},
	language = {English},
	abbrev_source_title = {Biosci. Rep.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Burks2000441,
	author = {Burks, T.F. and Shearer, S.A. and Payne, F.A.},
	title = {Classification of weed species using color texture features and discriminant analysis},
	year = {2000},
	journal = {Transactions of the American Society of Agricultural Engineers},
	volume = {43},
	number = {2},
	pages = {441 – 448},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034163651&partnerID=40&md5=a4792f438956cc708d63e3aa96fbd33a},
	affiliations = {ASAE; Department of Biosystems and Agricultural Engineering, University of Kentucky, Lexington, KY, United States; University of Kentucky, 128 Agricultural Engineering Bldg., Lexington, KY 40546-0276, United States},
	abstract = {The environmental impact of herbicide utilization has stimulated research into new methods of weed control, such as selective herbicide application on highly infested crop areas. This research utilized the Color Co-occurrence Method (CCM) to determine whether traditional statistical discriminant analysis can be used to discriminate between six different classes of groundcover. The weed species evaluated were giant foxtail, crabgrass, common lambsquarter, velvetleaf, and ivyleaf morningglory, along with a soil image data set. The between species discriminant analysis showed that the CCM texture statistics procedure was able to classify between five weed species and soil with an accuracy of 93% using hue and saturation statistics, only. A significant accomplishment of this work was the elimination of the intensity texture features from the model, which reduces computational requirements by one-third.},
	author_keywords = {Herbicide application; Image processing; Leaf and plant identification; Machine vision; Precision farming},
	keywords = {Computer vision; Crops; Environmental impact; Herbicides; Image processing; Statistical methods; Color cooccurrence method; Discriminant analysis; Precision farming; classification; herbicide; weed; weed control; Weed control},
	correspondence_address = {T.F. Burks; University of Kentucky, 128 Agricultural Engineering Bldg., Lexington, KY 40546-0276, United States; email: tburks@bae.uky.edu},
	issn = {00012351},
	coden = {TAAEA},
	language = {English},
	abbrev_source_title = {Trans. Am. Soc. Agric. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 98}
}

@ARTICLE{Suzuki2008163,
	author = {Suzuki, Yumiko and Okamoto, Hiroshi and Kataoka, Takashi},
	title = {Image segmentation between crop and weed using hyperspectral imaging for weed detection in soybean field},
	year = {2008},
	journal = {Environmental Control in Biology},
	volume = {46},
	number = {3},
	pages = {163 – 173},
	doi = {10.2525/ecb.46.163},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-63149169116&doi=10.2525%2fecb.46.163&partnerID=40&md5=bb9b18e76cee3475e1b09fc951e3b20f},
	affiliations = {Graduate School of Agriculture, Hokkaido University, Kita-ku, Sapporo 060-8589, Kita-9, Nishi-9, Japan; Research Faculty of Agriculture, Hokkaido University, Kita-ku, Sapporo 060-8589, Kita-9, Nishi-9, Japan},
	abstract = {The goal of this study was image segmentation between crop and weed using hyperspectral imaging for weed detection, This technique is useful for selective weeding using spot spraying or a robotic system. A hyperspectral image consists of a large number of pixel spectra. Therefore, image segmentation was executed pixel by pixel. After each pixel spectrum was extracted from the image, it was identified as soil or plant. If the pixel spectrum was identified as plant, it was categorized as crop or weed. For the pixel discriminant model between soil and plant, simple NDVI thresholding was employed. The pixel discriminant model between crop and weed consisted of normalization, generation of explanatory variables and discrimination, and four types of models were developed and validated. Finally, segmented images between crop and weed were generated from the hyperspectral images by applying these models. As the validation results, pixel discrimination between soil and plant was performed with 99.9%. Also, most pixel discriminant models between crop and weed had a success rate of more than 90%. As a result of image segmentation between crop and weed, most hyperspectral images were segmented correctly. This study demonstrated the possibility of weed detection using hyperspectral imaging.},
	author_keywords = {Image processing; Linear discriminant analysis; Machine vision; Multiple-layered neural network; Plant classification; Principal component analysis},
	correspondence_address = {Y. Suzuki; Graduate School of Agriculture, Hokkaido University, Kita-ku, Sapporo 060-8589, Kita-9, Nishi-9, Japan; email: yumisuzuki@bpe.agr.hokudai.ac.jp},
	publisher = {Biotron Institute},
	issn = {1880554X},
	language = {English},
	abbrev_source_title = {Enviro. Cont. Biol.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 25; All Open Access, Bronze Open Access}
}

@ARTICLE{Søgaard2007315,
	author = {Søgaard, H.T. and Lund, I.},
	title = {Application Accuracy of a Machine Vision-controlled Robotic Micro-dosing System},
	year = {2007},
	journal = {Biosystems Engineering},
	volume = {96},
	number = {3},
	pages = {315 – 322},
	doi = {10.1016/j.biosystemseng.2006.11.009},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846960157&doi=10.1016%2fj.biosystemseng.2006.11.009&partnerID=40&md5=9de66bdbe880ca1bc0d2e02a36915981},
	affiliations = {Danish Institute of Agricultural Sciences, Department of Agricultural Engineering, Research Centre Bygholm, DK-8700 Horsens, Schüttesvej 17, Denmark},
	abstract = {Experiments with a new concept for the precise application of herbicides in a seed line have been conducted. The concept combines plant recognition, micro-dosing and autonomous robotics. A machine vision system recognises objects to be sprayed, and a micro-dosing system targets very small doses of liquid at the detected objects, while the autonomous vehicle takes care of the navigation. The experiments were carried out under controlled indoor conditions. The results show that the spray liquid can be applied at subcentimetre accuracy and that the application rate can be reduced by two orders of magnitude compared to recommendations used for conventional broadcast spraying. © 2006 IAgrE.},
	keywords = {Dosimetry; Herbicides; Navigation systems; Plants (botany); Robotics; Autonomous robotics; Autonomous vehicle; Micro-dosing; Computer vision},
	issn = {15375110},
	coden = {BEINB},
	language = {English},
	abbrev_source_title = {Biosyst. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 63}
}

@CONFERENCE{Fu2004681,
	author = {Fu, Hong and Chi, Zheru and Feng, Dagan and Song, Jiatao},
	title = {Machine learning techniques for ontology-based leaf classification},
	year = {2004},
	journal = {2004 8th International Conference on Control, Automation, Robotics and Vision (ICARCV)},
	volume = {1},
	pages = {681 – 686},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-21244435571&partnerID=40&md5=dceb8761938c71ba0da6ab91e766a875},
	affiliations = {Center for Multimedia Signal Processing, Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; School of Information Technologies, University of Sydney, NSW 2006, Australia; Department of Information and Communication Engineering, Zhejiang University, Hangzhou 310027, China},
	abstract = {Leaf classification, indexing as well as retrieval is an important part of a computerized plant identification system. In this paper, an integrated approach for an ontology-based leaf classification system is proposed, wherein machine learning techniques play a crucial role for the automatization of the system. For the leaf contour classification, a scaled CCD code system is proposed to categorize the basic shape and margin type of a leaf by using the similar taxonomy principle adopted by the botanists. Then a trained neural network is employed to recognize the detailed tooth patterns. The measurement on an unlobed leaf is also conducted automatically according to the method used in botany. For the leaf vein recognition, the vein texture is extracted by employing an efficient combined thresholding and neural network approach so as to obtain more vein details of a leaf. Compared with the past studies, the proposed method integrates low-level features of an image and the specific knowledge in the domain (ontology) of botany, and therefore provides a more practical system for users to comprehend and handle. Primary experiments have shown promising results and proven the feasibility of the proposed system. © 2004 IEEE.},
	keywords = {Automation; Codes (symbols); Feature extraction; Image segmentation; Indexing (of information); Mathematical models; Neural networks; Plants (botany); Textures; Leaf classification; Machine learning techniques; Ontology; Plant identification systems; Learning systems},
	correspondence_address = {Z. Chi; Center for Multimedia Signal Processing, Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; email: enzheru@polyu.edu.hk},
	isbn = {0780386531},
	language = {English},
	abbrev_source_title = {Int. Conf. Control. Autom. Rob. Vis. ICARCV},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26; Conference name: 8th International Conference on Control, Automation, Robotics and Vision (ICARCV); Conference date: 6 December 2004 through 9 December 2004; Conference code: 65079}
}

@ARTICLE{Billert200255,
	author = {Billert, Oleg and Singher, Liviu},
	title = {Adaptive multiple filtering},
	year = {2002},
	journal = {Optical Engineering},
	volume = {41},
	number = {1},
	pages = {55 – 68},
	doi = {10.1117/1.1425790},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036118737&doi=10.1117%2f1.1425790&partnerID=40&md5=4f92adb74ec69bb010c74cc8021ee08f},
	affiliations = {Technion Israel Institute of Technology, Quality Assurance and Reliability, Haifa IL-32000, Israel; Technion Israel Institute of Technology, Agricultural Engineering, Haifa IL-32000, Israel},
	abstract = {The method described represents an attractive compromise between the use of a single filter for a number of image distortions and one filter for each image or distortion of an image. The goal of this work is the generation of a number of filters, each of them being able to recognize a number of distortions. One of the main problems of the filter design is its high sensitivity to internal noise of the system, optical aberrations, etc. In this work, the most common case of image distortion invariance has been considered together with system noise invariance. The simulation results indicate the absence of a false alarm and good identification. The filter generation is based on a learning process in an electro-optical pattern recognition system. The genetic algorithm serves as an optimization method. A binary filter has been selected as the spatial filter. The comparison between a traditional matched spatial complex filter and this adaptive binary filter performance indicates the significant benefits of the latter. Statistical tools were used to estimate and compare the significance of the difference between the output average of a rejection and recognition image sets. © 2002 Society of Photo-Optical Instrumentation Engineers.},
	author_keywords = {Genetic algorithm; Multiple filters; Optical pattern recognition; Plant classification},
	keywords = {Aberrations; Computer simulation; Genetic algorithms; Pattern recognition; Signal distortion; Multiple filters; Adaptive filtering},
	issn = {00913286},
	language = {English},
	abbrev_source_title = {Opt Eng},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10}
}

@CONFERENCE{Gu2005253,
	author = {Gu, Xiao and Du, Ji-Xiang and Wang, Xiao-Feng},
	title = {Leaf recognition based on the combination of wavelet transform and Gaussian interpolation},
	year = {2005},
	journal = {Lecture Notes in Computer Science},
	volume = {3644},
	number = {PART I},
	pages = {253 – 262},
	doi = {10.1007/11538059_27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-27144450942&doi=10.1007%2f11538059_27&partnerID=40&md5=2717331808717759490bdda809e895fe},
	affiliations = {Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, Anhui 230031, P.O.Box 1130, China; Department of Automation, University of Science and Technology of China, Hefei 230027, China},
	abstract = {In this paper, a new approach for leaf recognition using the result of segmentation of leaf's skeleton based on the combination of wavelet transform (WT) and Gaussian interpolation is proposed. And then the classifiers, a nearest neighbor classifier (1-NN), a k -nearest neighbor classifier (k-NN) and a radial basis probabilistic neural network (RBPNN) are used, based on run-length features (RF) extracted from the skeleton to recognize the leaves. Finally, the effectiveness and efficiency of the proposed method is demonstrated by several experiments. The results show that the skeleton can be successfully and obviously extracted from the whole leaf, and the recognition rates of leaves based on their skeleton can be greatly improved. © Springer-Verlag Berlin Heidelberg 2005.},
	keywords = {Classification (of information); Feature extraction; Interpolation; Mathematical models; Pattern recognition; Plants (botany); Wavelet transforms; Acoustics; Image segmentation; Intelligent computing; Musculoskeletal system; Nearest neighbor search; Radar; Gaussian interpolation; Leaf recognition; Nearest neighbor classifiers; Run-length features; Bark; Effectiveness and efficiencies; K-nearest neighbor classifier; Nearest Neighbor classifier; Radial basis probabilistic neural networks; Image segmentation; Wavelet transforms},
	correspondence_address = {X. Gu; Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, Anhui 230031, P.O.Box 1130, China; email: xgu@iim.ac.cn},
	publisher = {Springer Verlag},
	issn = {03029743},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 67; Conference name: International Conference on Intelligent Computing, ICIC 2005; Conference date: 23 August 2005 through 26 August 2005; Conference code: 65860}
}

@ARTICLE{Åstrand200221,
	author = {Åstrand, Björn and Baerveldt, Albert-Jan},
	title = {An agricultural mobile robot with vision-based perception for mechanical weed control},
	year = {2002},
	journal = {Autonomous Robots},
	volume = {13},
	number = {1},
	pages = {21 – 35},
	doi = {10.1023/A:1015674004201},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036645195&doi=10.1023%2fA%3a1015674004201&partnerID=40&md5=0a45b64c312761c256b10709fa9daf31},
	affiliations = {Halmstad University, Halmstad, Sweden},
	abstract = {This paper presents an autonomous agricultural mobile robot for mechanical weed control in outdoor environments. The robot employs two vision systems: one gray-level vision system that is able to recognize the row structure formed by the crops and to guide the robot along the rows and a second, color-based vision system that is able to identify a single crop among weed plants. This vision system controls a weeding-tool that removes the weed within the row of crops. The row-recognition system is based on a novel algorithm and has been tested extensively in outdoor field tests and proven to be able to guide the robot with an accuracy of ±2 cm. It has been shown that color vision is feasible for single plant identification, i.e., discriminating between crops and weeds. The system as a whole has been verified, showing that the subsystems are able to work together effectively. A first trial in a greenhouse showed that the robot is able to manage weed control within a row of crops.},
	author_keywords = {Machine vision; Mobile robot; Plant recognition; Weed control},
	keywords = {Agriculture; Computer vision; Crops; Greenhouses; Mobile robots; Object recognition; Weed control; Plant recognition; Row recognition system; Robotics},
	correspondence_address = {B. Åstrand; Halmstad University, Halmstad, Sweden; email: Bjorn.Astrand@ide.hh.se},
	issn = {09295593},
	coden = {AUROF},
	language = {English},
	abbrev_source_title = {Auton Robots},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 291}
}

@CONFERENCE{Du200571,
	author = {Du, Peng and Zhao, Huijie and Zhang, Bing and Zheng, Lanfen},
	title = {Independent component analysis for hyperspectral imagery plant classification},
	year = {2005},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {5673},
	pages = {71 – 81},
	doi = {10.1117/12.584356},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-21944449122&doi=10.1117%2f12.584356&partnerID=40&md5=991583872829952af00c4352d652ca90},
	affiliations = {Beihang University, Beijing, China; Institute of Remote Sensing Applications, Chinese Academy of Sciences, Beijing, China},
	abstract = {In land investigation, it is often required to extract plant and vegetation information from land covers, especially when plants are sparsely dispersed. To avoid the expensive ground survey, hyperspectral remote sensing image is adopted due to its narrow spectral bandwidth and high spectral resolution. However conventional unsupervised classification techniques often suffer from requiring priori as input parameter and sensitiveness to interference. This paper proposes an Independent Component Analysis (ICA) based unsupervised classification algorithm. ICA is a technique that stems out from the Blind Source Separation. In hyperspectral data processing, ICA projects data vectors to the space where the items of the vectors are mutually statistically independent, and therefore is capable of extracting various kinds of plant information. So as to strengthen the contrast of the resulted independent components, histogram adjustment and mathematical morphology post-processing procedure are appended after ICA decomposition. Through real hyperspectral data experiments, our algorithm has been verified to have better performance for classification than Kmeans and ISODATA. Besides, computation efficiency and noise robustness have also been improved by a noise filtering preprocessing procedure. © 2005 SPIE and IS&T.},
	author_keywords = {Feature extraction; Hyperspectral remote sensing; Independent component analysis; Plant classification},
	keywords = {Acoustic noise; Algorithms; Bandwidth; Feature extraction; Imaging techniques; Independent component analysis; Plants (botany); Statistical methods; Hyperspectral imagery; Hyperspectral remote sensing; Plant classification; Spectral bandwidth; Remote sensing},
	issn = {0277786X},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; Conference name: Proceedings of SPIE-IS and T Electronic Imaging - Applications of Neural Networks and Machine Learning in Image Processing IX; Conference date: 19 January 2005 through 20 January 2005; Conference code: 65162}
}

@ARTICLE{Franz1991673,
	author = {Franz, E. and Gebhardt, M.R. and Unklesbay, K.B.},
	title = {Shape description of completely visible and partially occluded leaves for identifying plants in digital images},
	year = {1991},
	journal = {Transactions of the American Society of Agricultural Engineers},
	volume = {34},
	number = {2},
	pages = {673 – 681},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026124532&partnerID=40&md5=7a7cdbdb77eb41192091f186d331289a},
	affiliations = {USDA-ARS, College Station, United States},
	abstract = {A common source of information used to identify plants is leaf shape. Methods were developed and implemented to identify in situ leaves of velvetleaf (Abutilon theophrasti), ivyleaf morning glory (Ipomoae hederacea), giant foxtail (Setaria faberi), and soybean (Glycine max) seedlings at the growth stages of (1) fully expanded cotyledons, and (2) fully developed, first true leaves. Curvature was used to describe boundaries of both completely visible and partially occluded leaves. Completely visible leaves were identified by aligning the curvature function of each leaf with that of each of the models. Functions were aligned to minimize local differences in curvature. A Fourier-Mellin correlation was used to calculate scale factors for resampling curvature functions of partially occluded leaves. Partially occluded leaves were identified by aligning the resampled curvatures with each of the models. Leaves which were oriented at oblique angles with respect to the image plane were identified by aligning the curvature function with that generated from rotated model leaves. The general shape-matching procedure was not significantly affected by differences in scale or orientation between models and unknowns. However, it was an inadequate descriptor of leaves with variable leaf serrations. Aggregate boundaries of multiple leaves could not be adequately identified by this technique. Rather, these regions required prior partitioning. Homogeneous transformations of two-dimensional model leaves were partially successful in identifying leaves oriented at angles greater than 30°C.},
	keywords = {Abutilon theophrasti; Alopecurus pratensis; Glycine max; Ipomoea hederacea; Setaria faberi; Biomass--Physical Properties; Image Processing--Image Analysis; Remote Sensing; Leaves; Plant Identification; Shape Analysis; Biomass},
	issn = {00012351},
	coden = {TAAEA},
	language = {English},
	abbrev_source_title = {Trans ASAE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 60}
}

@ARTICLE{Tang20082181,
	author = {Tang, Lie and Tian, Lei F.},
	title = {Plant identification in mosaicked crop row images for automatic emerged corn plant spacing measurement},
	year = {2008},
	journal = {Transactions of the ASABE},
	volume = {51},
	number = {6},
	pages = {2181 – 2191},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149268169&partnerID=40&md5=b5e233219769e30e6de551b1e4cc2c58},
	affiliations = {Department of Agricultural and Biosystems Engineering, Iowa State University, Ames, IA, United States; Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, United States; 203 Davidson Hall, Iowa State University, Ames, IA 50011, United States},
	abstract = {Image processing algorithms for individual corn plant and plant stem center identification were developed. These algorithms were applied to mosaicked crop row image for automatically measuring corn plant spacing at early growth stages. These algorithms utilized multiple sources of information for corn plant detection and plant center location estimation including plant color, plant morphological features, and the crop row centerline. The algorithm was tested over two 41 m (134.5 ft) long corn rows using video acquired two times in both directions. The system had a mean plant misidentification ratio of 3.7%. When compared with manual plant spacing measurements, the system achieved an overall spacing error (RMSE) of 1.7 cm and an overall R2 of 0.96 between manual plant spacing measurement and the system estimates. The developed image processing algorithms were effective in automated corn plant spacing measurement at early growth stages. Interplant spacing errors were mainly due to crop damage and sampling platform vibration that caused mosaicking errors. © 2008 American Society of Agricultural and Biological Engineers.},
	author_keywords = {Corn plant spacing measurement; Image processing; Machine vision; Planters; Robust line fitting},
	keywords = {Zea mays; Computer vision; Crops; Errors; Plants (botany); Center locations; Corn plant spacing measurement; Image processing algorithm; Plant identification; Plant morphological; Planters; Platform vibrations; Robust line fitting; algorithm; automation; color; crop damage; detection method; identification method; image; maize; measurement method; spacing; Image processing},
	correspondence_address = {L. Tang; 203 Davidson Hall, Iowa State University, Ames, IA 50011, United States; email: lietang@iastate.edu},
	publisher = {American Society of Agricultural and Biological Engineers},
	issn = {21510032},
	language = {English},
	abbrev_source_title = {Trans. ASABE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 22}
}

@CONFERENCE{Huang2008871,
	author = {Huang, Lin and He, Peng},
	title = {Machine recognition for broad-leaved trees based on synthetic features of leaves Using Probabilistic Neural Network},
	year = {2008},
	journal = {Proceedings - International Conference on Computer Science and Software Engineering, CSSE 2008},
	volume = {4},
	pages = {871 – 877},
	doi = {10.1109/CSSE.2008.1333},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951492249&doi=10.1109%2fCSSE.2008.1333&partnerID=40&md5=289986e4ae6e6e248aa94c339f47b5dd},
	affiliations = {College of Information Engineering, Northwest A and F University, Yang ling Shaanxi, 712100, China},
	abstract = {This paper is to effectively solve the problem that the objects of traditional plant identification were too broad and the classification features of it were usually not synthetic and the recognition rate was always slightly low. This study gives one recognition approach, in which the shape features and the texture features of the leaves of broad-leaved trees combine, composing a synthetic feature vector of broad leaves and hoping to realize the computer automatic classification towards broad-leaved plants more convenient, rapidly and efficient. Using Probabilistic Neural Networks (PNN) , the rapid recognition for thirty kinds of broad-leaved trees was realized and the average correct recognition rate reached 98.3%. Comparison tests demonstrated that if the shape features of broad leaf solely worked as the recognition features without the texture features, the average correct recognition rate just reached 93.7%. © 2008 IEEE.},
	keywords = {Neural networks; Software engineering; Textures; Automatic classification; Broadleaved trees; Classification features; Comparison test; Feature vectors; Machine recognition; Plant identification; Probabilistic neural networks; Recognition features; Recognition rates; Shape features; Texture features; Feature extraction},
	correspondence_address = {L. Huang; College of Information Engineering, Northwest A and F University, Yang ling Shaanxi, 712100, China; email: hl@nwsuaf.edu.cn},
	isbn = {978-076953336-0},
	language = {English},
	abbrev_source_title = {Proc. - Int. Conf. Comput. Sci. Softw. Eng., CSSE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; Conference name: International Conference on Computer Science and Software Engineering, CSSE 2008; Conference date: 12 December 2008 through 14 December 2008; Conference code: 75355}
}

@CONFERENCE{Runtz199284,
	author = {Runtz, K.J.},
	title = {Electronic recognition of plant species for machine vision sprayer control systems},
	year = {1992},
	pages = {84 – 88},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026720774&partnerID=40&md5=378104e7c2eb1ae0b0801233d14e2859},
	affiliations = {Fac of Eng, Univ of Regina, Sask,, Canada},
	abstract = {It is noted that machine vision systems have the potential as sprayer controllers to reduce farm chemical use and to increase the effectiveness of crop spraying operations. The author examines the issues in developing real-time plant recognition algorithms and associated electronic hardware. A very efficient algorithm for distinguishing between broadleaf and grassy plant species is proposed. Preliminary tests on video images of several types of field crops are reported. These tests show the potential for this and related image processing algorithms as plant classifiers in real-time systems.},
	keywords = {Image Processing; Pattern Recognition; Weed Control; Crop Spraying; Machine Vision Sprayer Control Systems; Plant Classification; Plant Species Recognition; Transition Frequency Index Algorithm; Computer Vision},
	publisher = {Publ by IEEE},
	isbn = {0879425946},
	language = {English},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: Proceedings of the IEEE Western Canada Conference on Computer, Power, and Communications Systems in a Rural Environment - Wescanex '91; Conference date: 29 May 1991 through 30 May 1991; Conference code: 16679}
}

@CONFERENCE{Aoyama20065478,
	author = {Aoyama, Tomoo and Akashi, Ryo and Umeno, Hidenori and Nagshima, Umpei},
	title = {An approach to plant identification technology: Development of lignin pyrolysis gas chromatography and pattern recognition},
	year = {2006},
	journal = {2006 SICE-ICASE International Joint Conference},
	pages = {5478 – 5483},
	doi = {10.1109/SICE.2006.315682},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250723899&doi=10.1109%2fSICE.2006.315682&partnerID=40&md5=9f1bf24ce2345b31c032c2a5873ca5a4},
	affiliations = {Faculty of Engineering, University of Miyazaki, Japan; Faculty of Agriculture, University of Miyazaki, Japan; Faculty of Engineering, Kumamoto University, Japan; National Institute of Advanced Industrial Science and Technology, Japan},
	abstract = {We discovered a technique to extract a characteristic spectrum pattern from Lignin pyrolysis gas chromatography. The Lignin is a kind of methoxyphenol polymer that is often found among cell walls of plants. Using the spectrum, we can determine the species from fragments of 1mg samples. The technique is useful for criminal investigation. The determination of species depends on distances calculated by species and a database of the spectra. We use the distances and can calculate family-locations in Plantae. The location shows the evolution/end of families. We evaluated 88 kinds of plants and discussed the end of ginkgo family and the branch of monocotyledon and a relation between C3/C4 metabolisms of Gramineae (family of the rice). © 2006 ICASE.},
	author_keywords = {Gas chromatography; Ginkgo; Lignin; Oryza sativa; Pyrolysis; Species-location},
	keywords = {Biodiversity; Ginkgo; Lignins; Pyrolysis; Biodiversity; Gas chromatography; Plants (botany); Pyrolysis; Methoxyphenol polymer; Oryza sativa; Plant identification technology; Lignin},
	correspondence_address = {T. Aoyama; Faculty of Engineering, University of Miyazaki, Japan; email: t0b217u@cc.miyazaki-u.ac.jp},
	isbn = {8995003855; 978-899500385-5},
	language = {English},
	abbrev_source_title = {SICE-ICASE Int. Joint Conf.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2006 SICE-ICASE International Joint Conference; Conference date: 18 October 2006 through 21 October 2006; Conference code: 69815}
}

@ARTICLE{Jia1991,
	author = {Jia, Jiancheng and Krutz, Gary W. and Gibson, Harry G.},
	title = {Evaluation of machine vision algorithms for locating corn plants},
	year = {1991},
	journal = {SAE Technical Papers},
	doi = {10.4271/911794},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072357696&doi=10.4271%2f911794&partnerID=40&md5=f0bb9e203a5ae5e29ee086ced40a91d9},
	affiliations = {Agricultural Engrg. Dept., Purdue University, United States},
	abstract = {The feasibility study of using machine vision technology to locate corn plants was conducted to determine its potential in the development of an intelligent detasseling machine. A corn plant feature, the main vein of leaf, was used and the method of feature detection was developed for corn plant identification. Experimental results showed that the leaf feature and the center of the plant can be detected and located using image processing techniques when an image is taken from above of a plant. This research showed that it is possible to identify and locate the corn plant using machine vision technology. Copyright © 1991 Society of Automotive Engineers, Inc.},
	keywords = {Feature extraction; Plants (botany); Corn plant; Feasibility studies; Feature detection; Image processing technique; Machine vision algorithm; Machine vision technologies; Computer vision},
	publisher = {SAE International},
	issn = {01487191},
	language = {English},
	abbrev_source_title = {SAE Techni. Paper.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: International Off-Highway and Powerplant Congress and Exposition; Conference date: 9 September 1991 through 12 September 1991; Conference code: 90062}
}

@ARTICLE{Shearer19902037,
	author = {Shearer, S.A. and Holmes, R.G.},
	title = {Plant identification using color co-occurrence matrices},
	year = {1990},
	journal = {Transactions of the American Society of Agricultural Engineers},
	volume = {33},
	number = {6},
	pages = {2037 – 2044},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025511038&partnerID=40&md5=0c579d887d4733edf951e2222fb1d6b7},
	affiliations = {University of Kentucky, Lexington, United States},
	abstract = {A method of identifying plants based on color texture characterization of canopy sections was developed. Color co-occurrence matrices were derived from image matrices, one for each color attribute: intensity, saturation, and hue. Eleven texture features were calculated from each of the co-occurrence matrices. The 33 total color texture features were used in a discriminant analysis model to identify plants. Overall classification accuracy of 91% was achieved when this method was used to identify seven common cultivars of nursery stock. A total of 350 observations were used in the investigation. This method exhibited a significant improvement over previous methods which used intensity data only.},
	keywords = {Color - Textures; Computer Vision; Image Processing - Image Analysis; Pattern Recognition; Co-Occurrence Matrices; Plant Canopies; Plant Identification; Biomass},
	issn = {00012351},
	coden = {TAAEA},
	language = {English},
	abbrev_source_title = {Trans ASAE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 137}
}

@CONFERENCE{Dickson1997,
	author = {Dickson, M.A. and Bausch, W.C.},
	title = {Plant recognition using a neural network classifier with size and shape descriptors},
	year = {1997},
	journal = {Paper - American Society of Agricultural Engineers},
	volume = {1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031347176&partnerID=40&md5=7c4036be3d26f07c3658c93f6e2476d2},
	affiliations = {Case Corp, Burr Ridge, United States},
	abstract = {A pattern recognition algorithm was developed to successfully classify a broadleaf weed (velvetleaf, Abutilon theophrasti), a grassy weed (wild proso millet, Panicum miliacem), and corn (Zea mays, L). Color images were evaluated over time periods and growth stages when most post emergence herbicides would be applied to corn. Neural network classifiers were trained and tested to classify the three plant species using three size and four shape descriptors as inputs. The optimal feed forward neural network classifier resulted in an overall classification accuracy of 94% when field and greenhouse data were combined.},
	keywords = {Algorithms; Color; Computer vision; Feedforward neural networks; Herbicides; Pattern recognition; Neural network classifiers; Plants (botany)},
	publisher = {ASAE},
	issn = {01450166},
	coden = {AAEPC},
	language = {English},
	abbrev_source_title = {Pap ASAE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; Conference name: Proceedings of the 1997 ASAE Annual International Meeting. Part 1 (of 3); Conference date: 10 August 1997 through 14 August 1997; Conference code: 48254}
}

@CONFERENCE{Ericson2007287,
	author = {Ericson, Stefan and Åstrand, Björn},
	title = {Algorithms for visual odometry in outdoor field environment},
	year = {2007},
	journal = {Proceedings of the 13th IASTED International Conference on Robotics and Applications, RA 2007 and Proceedings of the IASTED International Conference on Telematics},
	pages = {287 – 292},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149093160&partnerID=40&md5=ecf0280a27d3d6c1498115d180e6b0a0},
	affiliations = {University of Skövde, 54128 Skövde, Box 408, Sweden; Halmstad University, 30118 Halmstad, Box 823, Sweden},
	abstract = {In this paper different algorithms for visual odometry are evaluated for navigating an agricultural weeding robot in outdoor field environment. Today there is an encoder wheel that keeps track of the weeding tools position relative the camera, but the system suffers from wheel slippage and errors caused by the uneven terrain. To overcome these difficulties the aim is to replace the encoders with visual odometry using the plant recognition camera. Four different optical flow algorithms are tested on four different surfaces, indoor carpet, outdoor asphalt, grass and soil. The tests are performed on an experimental platform. The result shows that the errors consist mainly of dropouts caused by overriding maximum speed, and of calibration error due to uneven ground. The number of dropouts can be reduced by limiting the maximum speed and detection of missing frames. The calibration problem can be solved using stereo cameras. This gives a height measurement and the calibration will be given by camera mounting. The algorithm using normalized crosscorrelation shows the best result concerning number of dropouts, accuracy and calculation time.},
	author_keywords = {Agricultural applications; Computer vision; Optical flow; Visual odometry},
	keywords = {Applications; Calibration; Cameras; Computer vision; Errors; Image processing; Optical flows; Robotics; Robots; Wheels; Agricultural applications; And detections; Calculation times; Calibration errors; Calibration problems; Experimental platforms; Height measurements; Maximum speeds; Normalized cross correlations; Optical flow algorithms; Plant recognitions; Stereo cameras; Uneven terrains; Visual odometry; Wheel slippages; Computer applications},
	correspondence_address = {S. Ericson; University of Skövde, 54128 Skövde, Box 408, Sweden; email: stefan.ericson@his.se},
	isbn = {978-088986685-0},
	language = {English},
	abbrev_source_title = {Proc. IASTED Int. Conf. Rob. Appl., RA Proc. IASTED Int. Conf. Telematics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: 13th IASTED International Conference on Robotics and Applications, RA 2007 and Proceedings of the IASTED International Conference on Telematics; Conference date: 29 August 2007 through 31 August 2007; Conference code: 74038}
}

@ARTICLE{Guyer19861500,
	author = {Guyer, D.E. and Miles, G.E. and Schreiber, M.M. and Mitchell, O.R. and Vanderbilt, V.C.},
	title = {MACHINE VISION AND IMAGE PROCESSING FOR PLANT IDENTIFICATION.},
	year = {1986},
	journal = {Transactions of the American Society of Agricultural Engineers},
	volume = {29},
	number = {6},
	pages = {1500 – 1507},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022813808&partnerID=40&md5=26c993d45d4df066e37e26cbbc02d35b},
	affiliations = {Purdue Univ, West Lafayette, IN, USA, Purdue Univ, West Lafayette, IN, USA},
	abstract = {A machine vision system coupled with image processing algorithms has successfully recorded and identified images of the juvenile stages of corn, soybeans, tomatoes, Johnsongrass, Jimsonweed, velvetleaf, giant foxtail, and lambsquarters. Plants were grown in containers and images were obtained in a laboratory setting with the camera and light source positioned directly above the plants. Spatial parameters of the plants within the digital images were computed and used in a classification scheme for identification.},
	keywords = {COMPUTER PROGRAMMING - Algorithms; IMAGE PROCESSING - Image Analysis; VISION - Artificial; MACHINE VISION; PLANT IDENTIFICATION; BIOMASS},
	issn = {00012351},
	coden = {TAAEA},
	language = {English},
	abbrev_source_title = {Trans ASAE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 133}
}

@ARTICLE{Kavdir2004153,
	author = {Kavdir, Ismail},
	title = {Discrimination of sunflower, weed and soil by artificial neural networks},
	year = {2004},
	journal = {Computers and Electronics in Agriculture},
	volume = {44},
	number = {2},
	pages = {153 – 160},
	doi = {10.1016/j.compag.2004.03.006},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-3342897624&doi=10.1016%2fj.compag.2004.03.006&partnerID=40&md5=f78c33e3bab4cf0d58fe56bd7d919cac},
	affiliations = {Department of Agricultural Machinery, College of Agriculture, Canakkale Onsekiz Mart Univ., 17020, Canakkale, Turkey},
	abstract = {Selective application of herbicide to weeds at an early stage in crop growth is an important aspect of site-specific management of field crops, both economically and environmentally. This paper describes the application of a neural network classifier to differentiate between 2 and 3 weeks old sunflower plants and common cocklebur weeds of similar size, shape and colour. Colour images were obtained by a digital camera, in natural sunlight. A specific objective was to minimise the subsequent image processing operations needed to enhance the images and to extract the features needed by a back propagation neural network classifier. Neural network structures with different numbers of hidden layers and neurons in them were tested to find the optimal classifier. The maximum number of correctly recognised images in distinguishing weeds from sunflower plants was 71 (out of 86), while it was 82 and 74 in separating sunflower and weed images from bare soil images, respectively. © 2004 Elsevier B.V. All rights reserved.},
	author_keywords = {Artificial neural networks; Image processing; Plant recognition; Site-specific applications; Sunflower; Weed recognition},
	keywords = {Helianthus; Xanthium; Xanthium strumarium; Cameras; Herbicides; Image analysis; Plants (botany); Weed control; agricultural technology; artificial neural network; herbicide; image processing; weed control; Neural network classifiers; Site specific management; Neural networks},
	correspondence_address = {I. Kavdir; Department of Agricultural Machinery, College of Agriculture, Canakkale Onsekiz Mart Univ., 17020, Canakkale, Turkey; email: kavdiris@comu.edu.tr},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 41}
}

@ARTICLE{Lehrer1999S51,
	author = {Lehrer, Miriam},
	title = {Shape perception in the honeybee: Symmetry as a global framework},
	year = {1999},
	journal = {International Journal of Plant Sciences},
	volume = {160},
	number = {6 SUPPL.},
	pages = {S51–S65},
	doi = {10.1086/314216},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033392519&doi=10.1086%2f314216&partnerID=40&md5=23bf9404722ef12fa19881f62c3795e6},
	affiliations = {Zoological Institute, University of Zurich, Department of Neurobiology, CH-8057 Zurich, Winterthurerstrasse 190, Switzerland},
	abstract = {This study is concerned with the honeybee's spatial vision in light of the spatial signals that natural flowers display. A large amount of behavioral data shows that bees are perfectly adept at learning and exploiting a variety of spatial cues in the task of recognizing and discriminating between visual stimuli. These cues include spatial frequency, distribution of contrasting areas, orientation of contours, size and distance, different types of edges, and symmetry (or, in a broader sense, geometry). Symmetry constitutes a global feature that is only one of the cues that the target offers. Symmetrical stimuli always contain several further spatial cues that become relevant as the bee comes nearer to the stimuli. The results reviewed here show that the spatial signals used by the bee depend on whether the stimuli are presented on a horizontal or a vertical plane, on whether bees make their choices at a lesser or a greater distance, and on whether the target's image is stationary at the level of the eye, as opposed to moving. Further, it is shown that pattern recognition in the bee does not always require a learning process (i.e., several types of response to visual stimuli are based on hard-wired, innate behavioral programs). Finally, the results show that although it is not a prerequisite for spatial vision, color vision participates in spatial vision, whereas spatial cues extracted from image motion are processed by a color-blind system.},
	author_keywords = {Distance estimation; Edge detection; Flower recognition; Motion vision; Shape perception},
	keywords = {Animalia; Apis mellifera; Apoidea; Insecta; honeybee; learning; plant-insect interaction; recognition; spatial cognition; symmetry; visual cue},
	correspondence_address = {M. Lehrer; Zoological Institute, University of Zurich, Department of Neurobiology, CH-8057 Zurich, Winterthurerstrasse 190, Switzerland; email: miriam@zool.unizh.ch},
	publisher = {University of Chicago Press},
	issn = {10585893},
	coden = {IPLSE},
	language = {English},
	abbrev_source_title = {Int. J. Plant Sci.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26}
}

@CONFERENCE{Fu2003208,
	author = {Fu, Hong and Chi, Zheru},
	title = {A two-stage approach for leaf vein extraction},
	year = {2003},
	journal = {Proceedings of 2003 International Conference on Neural Networks and Signal Processing, ICNNSP'03},
	volume = {1},
	pages = {208 – 211},
	doi = {10.1109/ICNNSP.2003.1279248},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-15944401667&doi=10.1109%2fICNNSP.2003.1279248&partnerID=40&md5=d0e1e704379eaa532d3a059921317ee0},
	affiliations = {Center for Multimedia Signal Processing, Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong},
	abstract = {Living plant recognition is a promising but challenging task in the fields of pattern recognition and computer vision. As an inherent trait, the leaf vein definitely contains the important information for plant species recognition despite of its complex modality. In this paper, an efficient two-stage approach is presented for leaf vein extraction. At the first stage, a preliminary segmentation based on the intensity histogram of the leaf image is carried out to estimate the rough regions of vein pixels. This is followed at the second stage by a fine checking using a trained artificial neural network (ANN) classifier. Ten features distilled from a window centered at the pixel are used as the input to train the ANN classifier. Compared with conventional edge detection methods, experimental results show that the proposed method is capable of extracting more precise venation modality of the leaf for the subsequent leaf recognition. © 2003 IEEE.},
	keywords = {Classifiers; Computer vision; Edge detection; Image segmentation; Pixels; Signal processing; Artificial Neural Network; Edge detection methods; Leaf images; Leaf recognition; Plant recognition; Plant species; Two stage; Neural networks},
	correspondence_address = {Z. Chi; Center for Multimedia Signal Processing, Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; email: enzheru@polyu.edu.hk},
	isbn = {0780377028; 978-078037702-8},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Neural Networks Signal Process., ICNNSP},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 35; Conference name: 2003 International Conference on Neural Networks and Signal Processing, ICNNSP'03; Conference date: 14 December 2003 through 17 December 2003; Conference code: 82534}
}

@ARTICLE{Keränen200353,
	author = {Keränen, Mika and Aro, Eva-Mari and Tyystjärvi, Esa and Nevalainen, Olli},
	title = {Automatic plant identification with chlorophyll fluorescence fingerprinting},
	year = {2003},
	journal = {Precision Agriculture},
	volume = {4},
	number = {1},
	pages = {53 – 67},
	doi = {10.1023/A:1021863005378},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037370815&doi=10.1023%2fA%3a1021863005378&partnerID=40&md5=7889ea1f747bee3dfa7d59ef92ff26bf},
	affiliations = {University of Turku, Lab. of Plant Physiol./Molec. Biol., FIN-20014, Turku, Finland; University of Turku, Department of Computer Science, FIN-20014, Turku, Finland},
	abstract = {The development of precision farming needs methods for automatic identification of individual plant species. We have earlier shown that chlorophyll fluorescence induction curves can be reliably used for automatical identification of plants (Tyystjärvi et al., 1999). In the present study we show that a high accuracy of recognition can be obtained even if the teaching set for pattern recognition is collected several weeks before identifying a test batch of plants. It is also shown that very simple fluorescence traces can be used for the identification, and that dark pre-incubation of the plants can be shortened to a few seconds without seriously compromising the power of the method. The method is even more powerful if the aim is only to distinguish one crop species from weeds. The data shown here suggest that the fluorescence fingerprint can be developed to a method of practical importance for precision farming.},
	author_keywords = {Automatic plant identification; Chlorophyll a fluorescence; Neural network; Weed detection},
	keywords = {identification method},
	correspondence_address = {M. Keränen; University of Turku, Lab. of Plant Physiol./Molec. Biol., FIN-20014, Turku, Finland; email: esatyy@utu.fi},
	issn = {13852256},
	coden = {PREAF},
	language = {English},
	abbrev_source_title = {Precis. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32}
}

@CONFERENCE{Phillips2008717,
	author = {Phillips, Rhonda D. and Zhang, Jingwei and Watson, Layne T. and Blinn, Christine E. and Wynne, Randolph H.},
	title = {An adaptive noise filtering algorithm based on the maximum noise fraction},
	year = {2008},
	journal = {Proceedings of the 2008 International Conference on Image Processing, Computer Vision, and Pattern Recognition, IPCV 2008},
	pages = {717 – 723},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-62749100759&partnerID=40&md5=70a61b5fdb7dd16a6b4c2c4ea02b2209},
	affiliations = {Departments of Computer Science and Mathematics, MC 0106, Virginia Polytechnic Institute, State University, Blacksburg, VA 24061, United States; Department of Forestry, MC 0324, Virginia Polytechnic Institute, State University, Blacksburg, VA 24061, United States},
	abstract = {This paper describes a new algorithm used to adaptively filter a remote sensing dataset based on signal-to-noise ratios (SNRs) once the maximum noise fraction (MNF) has been applied. This algorithm uses Hermite splines to calculate the approximate area underneath the SNR curve as a function of band number, and that area is used to place bands into "bins" with other bands having similar SNRs. A median filter with a variable sized kernel is then applied to each band, with the same size kernel used for each band in a particular bin. The proposed adaptive filters are applied to a hyperspectral image generated by the AVIRIS sensor, and results are given for the identification of three different pine species located within the study area. The adaptive filtering scheme improves classification accuracies by as much as 10%, indicating that the proposed methods improve the image quality, thereby aiding in species discrimination.},
	author_keywords = {AVIRIS; Classification; Median filter; Noise removal; Tree species identification},
	keywords = {Acoustic intensity; Adaptive filtering; Adaptive filters; Bins; Computer vision; Electric filters; Image quality; Imaging systems; Infrared spectrometers; Remote sensing; Signal to noise ratio; AVIRIS; Classification; Median filter; Noise removal; Tree species identification; Adaptive algorithms},
	isbn = {1601320787; 978-160132078-0},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Image Process., Comput. Vis., Pattern Recogn., IPCV},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 2008 International Conference on Image Processing, Computer Vision, and Pattern Recognition, IPCV 2008; Conference date: 14 July 2008 through 17 July 2008; Conference code: null}
}

@CONFERENCE{Jia1991246,
	author = {Jia, Jiancheng and Krutz, Gary W. and Gibson, Harry G.},
	title = {Corn plant locating by image processing},
	year = {1991},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {1379},
	pages = {246 – 253},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025747760&partnerID=40&md5=45f9a741e6445298a622a51369dcbed7},
	affiliations = {Purdue Univ, West Lafayette, United States},
	abstract = {The feasibility investigation of using machine vision technology to locate corn plants is an important issue for field production automation in the agricultural industry. This paper presents an approach which was developed to locate the center of a corn plant using image processing techniques. Corn plants were first identified using a main vein detection algorithm by detecting a local feature of corn leaves, leaf main veins, based on the spectral difference between mains and leaves, then the center of the plant could be located using a center locating algorithm by tracing and extending each detected vein line and evaluating the center of the plant from intersection points of those lines. The experimental results show the usefulness of the algorithm for machine vision applications related to corn plant identification. Such a technique can be used for precise spraying of pesticides or biotech chemicals.},
	keywords = {Computer Vision; Agricultural Industry Automation; Corn Plant Identification; Machine Vision Technology; Image Processing},
	publisher = {Publ by Int Soc for Optical Engineering},
	issn = {0277786X},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; Conference name: Optics in Agriculture; Conference date: 7 November 1990 through 8 November 1990; Conference code: 14628}
}

@CONFERENCE{Jia1991656,
	author = {Jia, Jiancheng and Krutz, Gary W. and Gibson, Harry G.},
	title = {Image processing to locate corn plants},
	year = {1991},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {1396},
	pages = {656 – 663},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025742038&partnerID=40&md5=f89780384319617eda61b44242c67d2e},
	affiliations = {Purdue Univ, West Lafayette, United States},
	abstract = {This paper presents an image processing algorithm which uses a centripetal self-adaptive line trace approach to locate the center of corn plants based on a detectable local feature of corn leaf. The experimental results show the usefulness of the algorithm for the machine vision applications related to corn plant identification and location.},
	keywords = {Computer Programming - Algorithms; Computer Vision; Corn Plant Identification; Machine Vision; Self-Adaptive Line Trace Approach; Image Processing},
	publisher = {Publ by Int Soc for Optical Engineering},
	issn = {0277786X},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: Applications of Optical Engineering: Proceedings of OE/Midwest '90; Conference date: 27 September 1990 through 28 September 1990; Conference code: 14660}
}

@ARTICLE{Zandonadi2005433,
	author = {Zandonadi, R.S. and Pinto, F.A.C. and Sena Jr., D.G. and Queiroz, D.M. and Viana, P.A. and Mantovani, E.C.},
	title = {Identification of lesser cornstalk borer-attacked maize plants using infrared images},
	year = {2005},
	journal = {Biosystems Engineering},
	volume = {91},
	number = {4},
	pages = {433 – 439},
	doi = {10.1016/j.biosystemseng.2005.05.002},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-23044501270&doi=10.1016%2fj.biosystemseng.2005.05.002&partnerID=40&md5=05794f6a8010279785de40bc3ec04d28},
	affiliations = {Department of Agricultural Engineering, Universidade Federal de Viçosa, 36.570-000, Viçosa, MG, Brazil; Embrapa Milho e Sorgo, 35701-970, Sete Lagoas, MG, Brazil},
	abstract = {The lesser cornstalk borer (Elasmopalpus lignosellus) is a pest that damages the maize plants in the initial growing phase causing stand reduction that can result in yield decrease. The machine vision system could be an alternative for the development of site-specific management of this pest. The objective of this work was to develop and evaluate a machine vision algorithm for identifying maize plants attacked by lesser cornstalk borer based on colour infrared images. To develop the algorithm, images of 40 maize plants were taken on different days after emergency. The plants were grown in pots, and 25 of them were infested with lesser cornstalk borer larvae and 15 were left healthy. The algorithm had three stages: leaf identification, image block classification, and plant classification. In the leaf identification stage, the plant leaves were segmented by thresholding the normalised difference vegetation index image. For the block classification stage, different neural network architectures and block sizes were tested for identification of non-attacked and attacked plant image blocks. Then, in the plant classification stage, discriminating functions were used to classify the scene as either a healthy or an attacked plant. The algorithm performance was compared with the performance of four human experts by using the Kappa coefficient of agreement. The largest size of image block, 9 by 9 pixels, was chosen because of its less computational exigency and because its performance was not significantly different from the other tested block sizes. The algorithm performance was significantly better than just one human expert. The Kappa coefficients for the algorithm and the three best human experts were 63.0 and 49.7%, respectively. The overall accuracy of the algorithm and the best three human experts was 81.6 and 73.4%, respectively. © 2005 Silsoe Research Institute. All rights reserved.},
	keywords = {Algorithms; Biocides; Corn; Images; Infrared Radiation; Neural Networks; Pest Control; Plants; Zea mays; Algorithms; Computer vision; Crops; Infrared imaging; Neural networks; Pest control; Pesticides; Vegetation; Image block classification; Kappa coefficients; Leaf identification; Maize plants; Plants (botany)},
	correspondence_address = {F.A.C. Pinto; Department of Agricultural Engineering, Universidade Federal de Viçosa, 36.570-000, Viçosa, MG, Brazil; email: facpinto@ufv.br},
	issn = {15375110},
	coden = {BEINB},
	language = {English},
	abbrev_source_title = {Biosyst. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access, Green Open Access}
}

@ARTICLE{Ji2007847,
	author = {Ji, Ronghua and Fu, Zetian and Qi, Lijun},
	title = {Real-time plant image segmentation algorithm under natural outdoor light conditions},
	year = {2007},
	journal = {New Zealand Journal of Agricultural Research},
	volume = {50},
	number = {5},
	pages = {847 – 854},
	doi = {10.1080/00288230709510359},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-49249119165&doi=10.1080%2f00288230709510359&partnerID=40&md5=193dae5ac191ec74a37e45cabe18352a},
	affiliations = {College of Information and Electronic Engineering, China Agricultural University, Beijing, 17 Qinghua donglu, Haidian District, China; College of Engineering, China Agricultural University, Beijing, 17 Qinghua donglu, Haidian District, China},
	abstract = {Variable rate pesticide application holds great potential in precision agriculture where application efficiency depends mainly on plant recognition. The segmentation of the plant from the background is key to plant recognition. The work presented in this paper is a real-time segmentation algorithm that was developed to improve the quality of plant segmentation under natural outdoor light conditions. The plant images were greyed with the colour features H, a*, 13 and Cr respectively, and the plant was segmented from the grey images with a threshold that was computed by an iterative method. Experiments showed that segmentation was generally of good quality if the background was bare soil. The quality of segmentation declines if the images are greyed by H and Cr, and the quality remains stable if they are greyed by a* and I3 when the background is complicated, and, especially with heavy shadows. With respect to segmentation speed, that greyed by Cr was the fastest and that greyed by a* was slowest amongst the four. © 2007 Taylor & Francis Group, LLC.},
	author_keywords = {Colour features; Image segmentation algorithm; Threshold},
	keywords = {algorithm; image processing; light intensity; pesticide application; precision agriculture; real time},
	correspondence_address = {L. Qi; College of Engineering, China Agricultural University, Beijing, 17 Qinghua donglu, Haidian District, China; email: qilijun@cau.edu.cn},
	issn = {00288233},
	language = {English},
	abbrev_source_title = {New Zealand J. Agric. Res.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Wu2006127,
	author = {Wu, Qingfeng and Zhou, Changle and Wang, Chaonan},
	title = {Feature extraction and XML representation of plant leaf for image retrieval},
	year = {2006},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {3842 LNCS},
	pages = {127 – 131},
	doi = {10.1007/11610496_16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745643179&doi=10.1007%2f11610496_16&partnerID=40&md5=0221704c2969ef6dd6be82ac92c13067},
	affiliations = {Institute of Artificial Intelligence, Computer Science Department, Xiamen University, 361005, Fujian, China},
	abstract = {Leaf recognition and retrieval plays an important role in plant recognition and retrieval and its key issue lies in whether selected features are stable and have good ability to discriminate different kinds of leaves. From the view of plant leaf morphology, domain-related visual features and semantic features of plant leaf are analyzed and extracted first. Then these features are translated into a hierarchy that is easily represented by XML. On such a basis, the leaf image retrieval system proposed in this paper provides two types of retrieval methods, which could give better precision and flexibility. Experiment results prove the effectiveness of selected features and performance superiority of the leaf image retrieval system. © Springer-Verlag Berlin Heidelberg 2006.},
	keywords = {Feature extraction; Hierarchical systems; Morphology; Semantics; XML; Leaf image retrieval system; Leaf morphology; Performance superiority; Plant leaves; Image retrieval},
	correspondence_address = {Q. Wu; Institute of Artificial Intelligence, Computer Science Department, Xiamen University, 361005, Fujian, China; email: qfwu@xmu.edu.cn},
	publisher = {Springer Verlag},
	issn = {03029743},
	isbn = {3540311580; 978-354031158-4},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; Conference name: APWeb 2006 International Workshops: XRA, IWSN, MEGA, and ICSE; Conference date: 16 January 2006 through 18 January 2006; Conference code: 67605}
}

@ARTICLE{Bruno20082722,
	author = {Bruno, Odemir Martinez and de Oliveira Plotze, Rodrigo and Falvo, Mauricio and de Castro, Mário},
	title = {Fractal dimension applied to plant identification},
	year = {2008},
	journal = {Information Sciences},
	volume = {178},
	number = {12},
	pages = {2722 – 2733},
	doi = {10.1016/j.ins.2008.01.023},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-41749118694&doi=10.1016%2fj.ins.2008.01.023&partnerID=40&md5=e1983a803433192381f85416b8fda9e5},
	affiliations = {Universidade de São Paulo, Instituto de Ciências Matemáticas e de Computação (ICMC), 13560-970 São Carlos, SP, Av. trabalhador Saocarlense - 400, cx668, Brazil},
	abstract = {This article discusses methods to identify plants by analysing leaf complexity based on estimating their fractal dimension. Leaves were analyzed according to the complexity of their internal and external shapes. A computational program was developed to process, analyze and extract the features of leaf images, thereby allowing for automatic plant identification. Results are presented from two experiments, the first to identify plant species from the Brazilian Atlantic forest and Brazilian Cerrado scrublands, using fifty leaf samples from ten different species, and the second to identify four different species from genus Passiflora, using twenty leaf samples for each class. A comparison is made of two methods to estimate fractal dimension (box-counting and multiscale Minkowski). The results are discussed to determine the best approach to analyze shape complexity based on the performance of the technique, when estimating fractal dimension and identifying plants. © 2008 Elsevier Inc. All rights reserved.},
	author_keywords = {biometrics; Fractal dimension; Image processing; Vegetal taxonomy},
	keywords = {Biometrics; Computational complexity; Fractal dimension; Image processing; Computational programs; Plant identification; Vegetal taxonomy; Feature extraction},
	correspondence_address = {O.M. Bruno; Universidade de São Paulo, Instituto de Ciências Matemáticas e de Computação (ICMC), 13560-970 São Carlos, SP, Av. trabalhador Saocarlense - 400, cx668, Brazil; email: bruno@icmc.usp.br},
	issn = {00200255},
	coden = {ISIJB},
	language = {English},
	abbrev_source_title = {Inf Sci},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 211}
}

@ARTICLE{Shiraishi1996382,
	author = {Shiraishi, M. and Sumiya, H.},
	title = {Plant identification from leaves using quasi-sensor fusion},
	year = {1996},
	journal = {Journal of Manufacturing Science and Engineering, Transactions of the ASME},
	volume = {118},
	number = {3},
	pages = {382 – 386},
	doi = {10.1115/1.2831041},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030214739&doi=10.1115%2f1.2831041&partnerID=40&md5=8297793c8f22d27509faa487a232bd74},
	affiliations = {Ibaraki University, Nakanarusawa, Hitachi, 316, Japan},
	abstract = {The method described here identifies plants by using a machine vision technique.This method achieves effective image detection independent of surrounding conditions, dimensionless image detection in each growth stage, and determination of the critical factorfor discriminating individual plants. These are the fundamental factors for successful automatic thinning, cropping, weeding, and harvesting using intelligent agricultural robots. Color, aspect ratio, size, radius permutation in leaf profiles, complexity, and curvature are used to classify each plant. Effective discrimination is obtained by using a quasi-sensor fusion combined with a total occurrence range for decision making. © 1996 by ASME.},
	keywords = {Algorithms; Aspect ratio; Color; Computer vision; Decision making; Harvesting; Intelligent robots; Object recognition; Plants (botany); Sensor data fusion; Cropping; Dimensionless image detection; Quasi sensor fusion; Radius permutation; Thinning; Weeding; Identification (control systems)},
	issn = {10871357},
	language = {English},
	abbrev_source_title = {J Manuf Sci Eng Trans ASME},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21}
}

@ARTICLE{Wilson2005207,
	author = {Wilson, A.D. and Lester, D.G. and Oberle, C.S.},
	title = {Application of conductive polymer analysis for wood and woody plant identifications},
	year = {2005},
	journal = {Forest Ecology and Management},
	volume = {209},
	number = {3},
	pages = {207 – 224},
	doi = {10.1016/j.foreco.2005.01.030},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-15944374810&doi=10.1016%2fj.foreco.2005.01.030&partnerID=40&md5=3486dbd7a064be1d1ad8f7ea36764b0c},
	affiliations = {U. States Department of Agriculture, Ctr. for Bottomland Hardwoods Res., Southern Hardwoods Laboratory, Stoneville, MS 38776-0227, 432 Stoneville Road, United States},
	abstract = {An electronic aroma detection (EAD) technology known as conductive polymer analysis (CPA) was evaluated as a means of identifying and discriminating woody samples of angiosperms and gymnosperms using an analytical instrument (electronic nose) that characterizes the aroma profiles of volatiles released from excised wood into sampled headspace. The instrument measures electrical-resistance changes generated by adsorption of volatiles to the surface of electroactive, polymer-coated sensors. Unique digital electronic fingerprints of wood aromas, derived from multisensor-responses to distinct mixtures of wood volatiles, were obtained from woods of individual tree species. A reference library containing aroma signature patterns for 23 tree species was constructed for identifications of unknown samples using pattern-recognition algorithms. The 32-sensor array used with an Aromascan A32S instrument was sensitive to a wide diversity of organic compounds and produced outputs of distinct electronic aroma signature patterns in response to wood volatiles that effectively identified unknown samples from individual tree species included in the reference library. Some potential applications of CPA methods for research in ecology, forestry, plant taxonomy, and related disciplines were identified with some significant advantages and limitations. Other applications of this technology were discovered for the management of forested stands and ecosystems based on the identification of roles that wood-inhabiting organisms play in stand dynamics and long-term ecosystem functions. Results pertaining to tree systematics and phylogeny are discussed in the context of prevailing opinions of oak taxonomy. © 2005 Elsevier B.V. All rights reserved.},
	author_keywords = {Artificial olfaction; Electronic nose detection; Forest ecology; Forest management; Plant chemotaxonomy; Quercus; Woody sample identification},
	keywords = {Adsorption; Electric Resistance; Electronic Equipment; Pattern Recognition; Plants; Polymers; Sensors; Wood Products; Gymnospermae; Magnoliophyta; Quercus; Adsorption; Algorithms; Electric resistance; Electronic equipment; Pattern recognition; Plants (botany); Polymers; Sensors; chemotaxonomy; forest management; identification method; woody plant; Conductive polymer analysis (CPA); Electronic aroma detection (EAD) technology; Polymer-coated sensors; Wood products},
	correspondence_address = {A.D. Wilson; U. States Department of Agriculture, Ctr. for Bottomland Hardwoods Res., Southern Hardwoods Laboratory, Stoneville, MS 38776-0227, 432 Stoneville Road, United States; email: dwilson02@fs.fed.us},
	issn = {03781127},
	coden = {FECMD},
	language = {English},
	abbrev_source_title = {For. Ecol. Manage.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19}
}

@ARTICLE{Dubrovin2001251,
	author = {Dubrovin, Valeriy and Subbotin, Sergey and Morshchavka, Sergey and Piza, Dmitriy},
	title = {The plant recognition on remote sensing results by the feed-forward neural networks},
	year = {2001},
	journal = {International Journal of Smart Engineering System Design},
	volume = {3},
	number = {4},
	pages = {251 – 256},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035574603&partnerID=40&md5=25e45d8105c7598a444d67bef7c0e5a1},
	affiliations = {Zaporozhye State Technical Univ., Zaporozhye 69063, Zhukovsky Street 64, Ukraine},
	abstract = {Neural networks were used to classify agricultural crops from weeds using spectral reflectance measured over the visible to near-IR range. Reflectance from crops and weeds were used to train and test the neural networks. The results of experiments show that the neural networks are capable of being trained for practically correct plant classifications.},
	author_keywords = {Feature classification; Levenberg-Marquardt algorithm; Neural network; Remote sensing; Training},
	keywords = {Algorithms; Crops; Infrared radiation; Learning systems; Light reflection; Pattern recognition; Remote sensing; Feature classification; Plant recognition; Training Levenberg-Marquardt algorithm; Feedforward neural networks},
	correspondence_address = {V. Dubrovin; Zaporozhye State Technical Univ., Zaporozhye 69063, Zhukovsky Street 64, Ukraine; email: vdubrov@zstu.zaporizhzhe.ug},
	issn = {10255818},
	coden = {IJSDF},
	language = {English},
	abbrev_source_title = {Int J Smart Eng Syst Design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{Li1999193,
	author = {Li, Xiaodong and Purvis, Martin},
	title = {Pattern recognition by an optical thin-film multilayer model},
	year = {1999},
	journal = {Annals of Mathematics and Artificial Intelligence},
	volume = {26},
	number = {1-4},
	pages = {193 – 213},
	doi = {10.1023/a:1018911012905},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033263422&doi=10.1023%2fa%3a1018911012905&partnerID=40&md5=ccaa13ddef9ccd9ab4c48c91cb31ccef},
	affiliations = {Gippsland Sch. Comp. Info. Technol., Monash University, Churchill, Vic., Australia; Computer and Information Science, University of Otago, Dunedin, New Zealand},
	abstract = {This paper describes a computational learning model inspired by the technology of optical thin-film multilayers from the field of optics. With the thicknesses of thin-film layers serving as adjustable "weights" for the computation, the optical thin-film multilayer (OTFM) model is capable of approximating virtually any kind of nonlinear mapping. This paper describes the architecture of the model and how it can be used as a computational learning model. Some sample simulation calculations that are typical of connectionist models, including a pattern recognition of alphabetic characters, iris plant classification, and time series modelling of a gas furnace process, are given to demonstrate the model's learning capability.},
	correspondence_address = {X. Li; Gippsland Sch. Comp. Info. Technol., Monash University, Churchill, Vic., Australia; email: xiaodong.li@infotech.monash.edu.au},
	publisher = {Springer Netherlands},
	issn = {10122443},
	language = {English},
	abbrev_source_title = {Ann. Math. Artif. Intell.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Franz1991682,
	author = {Franz, E. and Gebhardt, M.R. and Unklesbay, K.B.},
	title = {Use of local spectral properties of leaves as an aid for identifying weed seedlings in digital images},
	year = {1991},
	journal = {Transactions of the American Society of Agricultural Engineers},
	volume = {34},
	number = {2},
	pages = {682 – 687},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026124264&partnerID=40&md5=f49daf2bade8aed8cf6ef3e20c43a0d6},
	affiliations = {USDA-ARS, College Station, United States},
	abstract = {An intelligent, plant identification scheme will require multiple types of information to be extracted from digitized scenes. Statistical measures were calculated for reflectance of in situ leaf surfaces in the near-infrared, red, and blue wavebands. Reflectance was quantified by image intensity within a leaf region near the leaf periphery. Mean, variance, and skewness were selected as significant statistical measures. The intensity statistics depended on NIR reflectance, spatial density of veins, and visibility of specular reflections. Three out of forty-eight observations were misclassified when leaf orientation was not a factor. The error rate increased to 16 out of 66 when leaf orientation was a factor. The results indicate that in order to discriminate among individual leaves, the training set must account for leaf orientation with respect to the illumination source.},
	keywords = {Image Processing--Image Analysis; Light--Reflection; Remote Sensing; Spectral Properties; Leaves; Weed Seedlings; Biomass},
	issn = {00012351},
	coden = {TAAEA},
	language = {English},
	abbrev_source_title = {Trans ASAE},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 57}
}

@ARTICLE{Šeatović2008363,
	author = {Šeatović, Dejan},
	title = {A segmentation approach in novel real time 3D plant recognition system},
	year = {2008},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {5008 LNCS},
	pages = {363 – 372},
	doi = {10.1007/978-3-540-79547-6_35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649186217&doi=10.1007%2f978-3-540-79547-6_35&partnerID=40&md5=804c5ceb4d492c9e08aea503d1f1fad0},
	affiliations = {IMS Institute of Mechatronic Systems, Zurich University of Applied Sciences, Winterthur 8401, P.O. Box 805, Switzerland},
	abstract = {One of the most invasive and persistent kind of weed in agriculture is Rumex Obtusifolius L. also called "Broad-leaved Dock". The origin of the plant is Europe and northern Asia, but it has also been reported that this plant occurs in wide parts of Northern America. Eradication of this plant is labour-intensive and hence there is an interest in automatic weed control devices. Some vision systems were proposed that allow to localize and map plants in the meadow. However, these systems were designed and implemented for off-line processing. This paper presents a segmentation approach that allows for real-time recognition and application of herbicides onto the plant leaves. Instead of processing the gray-scale or colour images, our approach relays on 3D point cloud analysis and processing. 3D data processing has several advantages over 2D image processing approaches when it comes to extraction and recognition of plants in their natural environment. © 2008 Springer-Verlag Berlin Heidelberg.},
	author_keywords = {Plant Recognition; Precision Farming; Real-Time Systems; Segmentation},
	keywords = {Computer vision; Data processing; Real time systems; Plant Recognition; Precision Farming; Image segmentation},
	correspondence_address = {D. Šeatović; IMS Institute of Mechatronic Systems, Zurich University of Applied Sciences, Winterthur 8401, P.O. Box 805, Switzerland; email: dejan.seatovic@zhaw.ch},
	issn = {16113349},
	isbn = {3540795464; 978-354079546-9},
	language = {English},
	abbrev_source_title = {Lect. Notes Comput. Sci.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; Conference name: 6th International Conference on Computer Vision Systems, ICVS 2008; Conference date: 12 May 2008 through 15 May 2008; Conference code: 72144}
}

@CONFERENCE{Okamoto200447,
	author = {Okamoto, H. and Murata, T. and Kataoka, T. and Hata, S.},
	title = {Weed detection using hyperspectral imaging},
	year = {2004},
	journal = {Proceedings of the International Conference on Automation Technology for Off-road Equipment, ATOE 2004},
	pages = {47 – 55},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-27844584025&partnerID=40&md5=7db115b8b14b79e812930b80b534bf8e},
	affiliations = {Graduate School of Agriculture, Hokkaido University, Japan},
	abstract = {The goal of this study is to develop the discrimination method between crop and weed which require in the automatic mechanical weeding. In this study, the hyperspectral images were used. As data for analysis and verification, the hyperspectral images were acquired in the field of the university farm. These images consisted of crops, weeds and soil surface. First, the image pixels of the plant (crop or weed) were extracted from the background soil surface. In this process, the difference of spectral patterns between plant and soil was utilized. As a result of the test, the very high-precise segmentation was achieved. Next, the image pixels of crop (sugar beet) and weeds (four species) were classified by the analysis of the difference of spectral characteristics between plant species. In this process, the classification variables were generated using wavelet transform for data compression, noise reduction and feature extraction, and then the stepwise discriminant analysis was done. As a result of the test, the success rate in the plant classification was about 80%. Finally, the technique using the spatial neighbor information (area information) was devised in order to improve the performance of the plant classification.},
	author_keywords = {Discriminant analysis; Hyperspectral imaging; Image processing; Machine vision; Plant classification; Wavelet analysis; Weed control; Weed identification},
	keywords = {Discriminant Analysis; Farm Crops; Image Analysis; Pattern Recognition; Weeds; Biodiversity; Classification (of information); Crops; Data reduction; Feature extraction; Image segmentation; Noise abatement; Pattern recognition; Plants (botany); Wavelet transforms; Weed control; Hyperspectral imaging; Plant classification; Wavelet analysis; Weed identification; Image processing},
	isbn = {189276945X},
	language = {English},
	abbrev_source_title = {Proc. Int. Conf. Autom. Technol. Off-road Equip.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; Conference name: International Conference on Automation Technology for Off-road Equipment, ATOE 2004; Conference date: 7 October 2004 through 8 October 2004; Conference code: 66044}
}

@CONFERENCE{Qi20033039,
	author = {Qi, Heng-Nian and Yang, Jian-Gang},
	title = {Sawtooth feature extraction of leaf edge based on Support Vector Machine},
	year = {2003},
	journal = {International Conference on Machine Learning and Cybernetics},
	volume = {5},
	pages = {3039 – 3044},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542345318&partnerID=40&md5=740cb36ed8da0e151ffe043089786b4f},
	affiliations = {Institute of Artificial Intelligence, Zhejiang University, 310027 Hangzhou, China; School of Information Engineering, Zhejiang Forestry College, 311300 Zhejiang, China},
	abstract = {The focus of Computer-Aided Plant-Identifica- tion (CAPI) is the stable features extraction of plant, such as sawtooth number of leaf edge for some species of plants. Because the shape of the sawteeth varies greatly, they cannot be depicted in rigid mathematic method. However, a trained SVM (Support Vector Machine) with good adaptability can be applied to classify sawtooth and nonsawtooth samples. The samples can be obtained by a rectangular sample window sliding along the edge of the leaf, and then be rotated to a standard pose for decreasing the complexity of identification. By avoiding repeated sampling and counting of the same sawtooth, the algorithm presented in the paper accomplishes automatic counting of the sawtooth number. The results of the experiment show that the SVM-based method works well.},
	author_keywords = {CAPI; Feedforward neural network; Sawtooth of Leaf Edge; SVM},
	keywords = {Computational methods; Data reduction; Feedforward neural networks; Lagrange multipliers; Mathematical models; Matrix algebra; Optimization; Problem solving; Vectors; Computer-aided plant identification (CAPI); Sawtooth of leaf edge; Support vector machine (SVM); Computer aided design},
	correspondence_address = {H.-N. Qi; Institute of Artificial Intelligence, Zhejiang University, 310027 Hangzhou, China; email: qihengnian@zju.edu.cn},
	isbn = {0780378652},
	language = {English},
	abbrev_source_title = {Int. Conf. Mach. Learn. Cybern.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; Conference name: 2003 International Conference on Machine Learning and Cybernetics; Conference date: 2 November 2003 through 5 November 2003; Conference code: 62426}
}

@ARTICLE{Neto200666,
	author = {Neto, João Camargo and Meyer, George E. and Jones, David D.},
	title = {Individual leaf extractions from young canopy images using Gustafson-Kessel clustering and a genetic algorithm},
	year = {2006},
	journal = {Computers and Electronics in Agriculture},
	volume = {51},
	number = {1-2},
	pages = {66 – 85},
	doi = {10.1016/j.compag.2005.11.002},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-32644437469&doi=10.1016%2fj.compag.2005.11.002&partnerID=40&md5=90373898534d34a3058535f354dfe491},
	affiliations = {Embrapa Information Tecnology, Cidade Universitária Zeferino Vaz, 13083-886 Campinas, SP, Av. André Tosello 209, Brazil; Biological Systems Engineering, 250 L.W. Chase Hall, University of Nebraska, Lincoln, NE 68583-0726, United States},
	abstract = {The extraction of individual concealed leaves from images of complex plant canopies is a necessary step for taxonomic feature acquisition, species identification, and mapping using a modern personal computer. A new system for individual leaflet extraction was developed and tested, based on connected components, fuzzy clustering and a genetic optimization algorithm. Color images were taken of young, but sparse green canopies, grown in both greenhouse and field conditions. Some images contained individual leaves as connected components, which were readily apparent after separation of the vegetation from its background. Fragments of all other leaves imbedded in the canopy were obtained using the Gustafson-Kessel (GK) clustering algorithm. Each leaf fragment was labeled and placed in a variable length data structure called a chromosome, which represented selected leaf fragments and its neighbors. A genetic algorithm was then used to systematically reassemble the fragments of non-occluded, individual leaves. System performance was evaluated by comparing the number of individual leaves extracted by the computer per plant or plant canopy connected component for various soil/residue backgrounds and time after emergence. 83.5% of the plants in the second week produced at least one computer-extracted leaf for identification. Ninty-two percent of the plants had at least one computer extracted leaf by the third week. 84.7% had more than three computer extracted leaves for identification in the third week. Images of young field plants in multiple species clusters resulted in a 46% leaf extraction rate, but with at least one leaf per connected canopy component. Soybean and velvetleaf leaflets were the easiest to extract. Once individual leaves are extracted, they can be classified using traditional shape and textural feature methods. Computerized individual leaf extraction could assist plant identification and mapping, needed for weed control and crop management. © 2005 Elsevier B.V. All rights reserved.},
	author_keywords = {Fuzzy clustering; Genetic algorithm; Leaf extraction; Machine vision; Segmentation},
	keywords = {Algorithms; Foliar Analysis; Forest Canopy; Leaves; Abutilon theophrasti; Glycine max; Computer vision; Data structures; Feature extraction; Personal computers; Plants (botany); Crop management; Fuzzy clustering; Leaf extraction; Plant identification; genetic algorithm; image processing; leaf; segmentation; Genetic algorithms},
	correspondence_address = {G.E. Meyer; Biological Systems Engineering, 250 L.W. Chase Hall, University of Nebraska, Lincoln, NE 68583-0726, United States; email: gmeyer1@unl.edu},
	publisher = {Elsevier},
	issn = {01681699},
	coden = {CEAGE},
	language = {English},
	abbrev_source_title = {Comput. Electron. Agric.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 77}
}

@CONFERENCE{Li2005885,
	author = {Li, YunFeng and Zhu, QingSheng and Cao, YuKun and Wang, ChengLiang},
	title = {A leaf vein extraction method based on snakes technique},
	year = {2005},
	journal = {Proceedings of 2005 International Conference on Neural Networks and Brain Proceedings, ICNNB'05},
	volume = {2},
	pages = {885 – 888},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847163613&partnerID=40&md5=dea1117220025106bce3a9c08c966708},
	affiliations = {Department of Computer Science and Engineering, Chongqing University, Chongqing, 400044, China},
	abstract = {Leaf vein extraction is a key step of modeling plant organs and living plant recognition. An efficient leaf vein extraction method is proposed in this paper by combining snakes technique with cellular neural networks (CNN). The active contours technique based on CNN provide high flexibility and control for the contour dynamics of the snakes. This approach has the advantage of applying a priori knowledge, puts similar characteristics from both the implicit and parametric models, to improve the precise and robustness of the segmentation. The experimental results have also shown that the proposed method can obtain satisfactory results of leaf segmentation (to extract leaf veins and outlines), for the subsequent modeling leaf and leaf recognition. © 2005 IEEE.},
	keywords = {Biodiversity; Cellular neural networks; Feature extraction; Image segmentation; Pattern recognition; Robustness (control systems); Active contours technique; Leaf recognition; Leaf vein extraction method; Snakes technique; Plants (botany)},
	correspondence_address = {Y. Li; Department of Computer Science and Engineering, Chongqing University, Chongqing, 400044, China; email: lyf129@126.com},
	isbn = {0780394224; 978-078039422-3},
	language = {English},
	abbrev_source_title = {Proc. 2005 Int. Conf. Neural Netw. Brain Proc. ICCNB'05},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 44; Conference name: 2005 International Conference on Neural Networks and Brain Proceedings, ICNNB'05; Conference date: 13 October 2005 through 15 October 2005; Conference code: 69185}
}

@ARTICLE{Codrea20032663,
	author = {Codrea, C.M. and Aittokallio, T. and Keränen, M. and Tyystjärvi, E. and Nevalainen, O.S.},
	title = {Feature learning with a genetic algorithm for fluorescence fingerprinting of plant species},
	year = {2003},
	journal = {Pattern Recognition Letters},
	volume = {24},
	number = {15},
	pages = {2663 – 2673},
	doi = {10.1016/S0167-8655(03)00109-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041828814&doi=10.1016%2fS0167-8655%2803%2900109-0&partnerID=40&md5=ba39233eec0ed2fb007063ac47342033},
	affiliations = {Turku Centre for Computer Science, Turku FIN-20520, Lemminkäisenkatu 14 A, Finland; Department of Information Technology, University of Turku, Turku FIN-20520, Finland; Department of Mathematics, University of Turku, Turku FIN-20014, Finland; Department of Biology, Lab. of Plant Physiol./Molec. Biol., University of Turku, Turku FIN-20014, Finland},
	abstract = {Proper feature analysis facilitates recognition by focusing the process to those characteristics of observed data that carry the most significant information for the given classification task. In this paper we address the problem of feature selection from a different point of view. Instead of searching for a feature subset out of a large set of predefined candidate features we consider the situation where, given the form of the features and an algorithm for extracting them from the data, the optimizer tunes the feature extraction parameters to improve class separability. This process of feature learning will be solved by the means of a genetic algorithm. The optimized feature set is subsequently used in a neural network classifier. The performance of the feature learning approach is demonstrated with the problem of automatic identification of plant species from their fluorescence induction curves. The general approach should also be useful with other pattern recognition problems where a priori unknown characteristics are extracted from a large feature space. © 2003 Elsevier B.V. All rights reserved.},
	author_keywords = {Classification; Feature analysis; Fluorescence induction; Genetic algorithm; Neural network; Plant identification},
	keywords = {Fluorescence; Genetic algorithms; Neural networks; Problem solving; Feature learning; artificial neural network; classification; fluorescence; image analysis; pattern recognition; plant; remote sensing; Pattern recognition},
	correspondence_address = {C.M. Codrea; Turku Centre for Computer Science, Nanyang Technological University, Turku FIN-20520, Lemminkäisenkatu 14 A, Finland; email: codrea@cs.utu.fi},
	publisher = {Elsevier},
	issn = {01678655},
	language = {English},
	abbrev_source_title = {Pattern Recogn. Lett.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14}
}

@ARTICLE{Lee20041269,
	author = {Lee, W.S. and Slaughter, D.C.},
	title = {Recognition of partially occluded plant leaves using a modified watershed algorithm},
	year = {2004},
	journal = {Transactions of the American Society of Agricultural Engineers},
	volume = {47},
	number = {4},
	pages = {1269 – 1280},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-6344281217&partnerID=40&md5=e782c1ee721c23ec1857afb795c7b7d9},
	affiliations = {Dept. of Agric. and Biol. Eng., University of Florida, Gainesville, FL, United States; Dept. of Biol. and Agric. Eng., University of California, Davis, CA, United States; Dept. of Agric. and Biol. Eng., University of Florida, Frazier Rogers Hall, Gainesville, FL 32611-0570, Museum Road, United States},
	abstract = {Occlusion is an obstacle to two-dimensional machine vision recognition of plants in natural outdoor scenes. Five modifications to the Watershed algorithm were investigated for separating occluded plant leaves, in an attempt to reduce the excessive object fragmentation associated with the original Watershed algorithm. The best modified algorithm improved the recognition of occluded tomato cotyledons and tomato true leaves improved by a factor of 3 and 2, respectively, after it was applied to occluded plant leaves in natural outdoor scenes. Two of the modified Watershed algorithms required about 11% less computation time than the original.},
	author_keywords = {Object segmentation; Occlusion; Plant recognition; Watershed},
	keywords = {Lycopersicon esculentum; Algorithms; Computer vision; Water; Watersheds; computer vision; Object fragmentation; Occlusion; Plant leaves; Plants (botany)},
	correspondence_address = {W.S. Lee; Dept. of Agric. and Biol. Eng., University of Florida, Frazier Rogers Hall, Gainesville, FL 32611-0570, Museum Road, United States; email: wslee@ufl.edu},
	issn = {00012351},
	coden = {TAAEA},
	language = {English},
	abbrev_source_title = {Trans. Am. Soc. Agric. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 29}
}

@CONFERENCE{Zhang2006,
	author = {Zhang, Yun and He, Yong and Fang, Hui},
	title = {Computer-vision-based weed identification of images acquired by 3CCD camera},
	year = {2006},
	journal = {Progress in Biomedical Optics and Imaging - Proceedings of SPIE},
	volume = {6047 I},
	doi = {10.1117/12.710881},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749650339&doi=10.1117%2f12.710881&partnerID=40&md5=dcaa4ab9bb1b287ac970edba529f7e59},
	affiliations = {College of Biosystems Engineering and Food Science, Zhejiang University, 310029 Hangzhou, 268 Kaixuan Road, China},
	abstract = {Selective application of herbicide to weeds at an earlier stage in crop growth is an important aspect of site-specific management of field crops. For approaches more adaptive in developing the on-line weed detecting application, more researchers involves in studies on image processing techniques for intensive computation and feature extraction tasks to identify the weeds from the other crops and soil background. This paper investigated the potentiality of applying the digital images acquired by the MegaPlus™ MS3100 3-CCD camera to segment the background soil from the plants in question and further recognize weeds from the crops using the Matlab script language. The image of the near-infrared waveband (center 800 nm; width 65 nm) was selected principally for segmenting soil and identifying the cottons from the thistles was achieved based on their respective relative area (pixel amount) in the whole image. The results show adequate recognition that the pixel proportion of soil, cotton leaves and thistle leaves were 78.24% (- 0.20% deviation), 16.66% (+ 2.71% S.D) and 4.68% (- 4.19% S.D). However, problems still exists by separating and allocating single plants for their clustering in the images. The information in the images acquired via the other two channels, i.e., the green and the red bands, need to be extracted to help the crop/weed discrimination. More optical specimens should be acquired for calibration and validation to establish the weed-detection model that could be effectively applied in fields.},
	author_keywords = {3CCD camera; Leaf area; Matlab; Plant recognition; Soil background segmentation; Weed discrimination},
	keywords = {Farm Crops; Image Analysis; Weed Control; Computer vision; Image analysis; Image processing; Weed control; 3CCD camera; Leaf area; Plant recognition; Soil background segmentation; Weed discrimination; Crops},
	issn = {16057422},
	isbn = {081946080X; 978-081946080-6},
	language = {English},
	abbrev_source_title = {Progr. Biomed. Opt. Imaging Proc. SPIE},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; Conference name: 4th International Conference on Photonics and Imaging in Biology and Medicine; Conference date: 3 September 2005 through 6 September 2005; Conference code: 68311}
}

@CONFERENCE{Meyer1999327,
	author = {Meyer, George E. and Hindman, Tim and Laksmi, Koppolu},
	title = {Machine vision detection parameters for plant species identification},
	year = {1999},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {3543},
	pages = {327 – 335},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032636094&partnerID=40&md5=34630f1675107f6dc64809d2e2e26d08},
	affiliations = {Univ of Nebraska, Lincoln, United States},
	abstract = {Machine vision based on classical image processing techniques has the potential to be a useful tool for plant detection and identification. Plant identification is needed for weed detection, herbicide application or other efficient chemical spot spraying operations. The key to successful detection and identification of plants as species types is the segmentation of plants from background pixel regions. In particular, it would be beneficial to segment individual leaves from tops of canopies as well. The segmentation process yields an edge or binary image which contains shape feature information. Results indicate that red-green-blue (RGB) formats might provide the best segmentation criteria, based on models of human color perception. The binary image can be also used as a template to investigate textural features of the plant pixel region, using gray image co-occurrence matrices. Texture features considers leaf venation, colors, or additional canopy structure that might be used to identify various type of grasses or broadleaf plants.},
	keywords = {Color image processing; Feature extraction; Herbicides; Image analysis; Image segmentation; Mathematical models; Matrix algebra; Plants (botany); Vegetation; Weed control; Gray image co-occurrence matrices; Plant species identification; Computer vision},
	publisher = {Society of Photo-Optical Instrumentation Engineers},
	issn = {0277786X},
	coden = {PSISD},
	language = {English},
	abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 126; Conference name: Proceedings of 1998 Precision Agriculture and Biological Quality; Conference date: ; Conference code: 55065}
}